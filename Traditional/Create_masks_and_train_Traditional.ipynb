{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Qtpe_CYHnTqU",
        "f9sKW8iq7Tny",
        "S-0aDh52nO-S",
        "SUqyo3PodvRh",
        "HHo1UacopxwC",
        "QrRkgxKu2Wiw",
        "bXGrNtvp_pvm",
        "olhv0tQ8Skgj",
        "_oj2BlyrsCdP",
        "LZQnu2Drr55D",
        "-pincp8wI-yV",
        "KIe8Dr1oxMFW",
        "cn4axcr5xPl7",
        "NFek9qfWohIT",
        "BWZkHOPxowxa",
        "dUJedOC0owxt",
        "Hfodj4ygo4Yq",
        "n8LRhhipo4Y-",
        "eyOMuSd28UWx",
        "tkaPhP3p8NBt",
        "Y_u1h2Busrbs",
        "Wk90E6ENuoaS",
        "gzJ0JEIzuoaZ",
        "71eypyB-tAcN",
        "n46FQu3BuuB-",
        "ACEhql2iuuCF",
        "CWUrgueftApV",
        "kO6hxpXQuwq7",
        "UzXESjDCuwrC",
        "-iAL8bp_tA0f",
        "MjS4kz7EtAQO",
        "ZjjcX6Wku7ay",
        "dQn_F5KEXNpC",
        "j5eCUuXkeQpg",
        "s_uP4E2seQph",
        "7wvV-vByeQpi",
        "NpojxPL5eQpk",
        "J4aNru07eQpl",
        "KgiblP-UeQpm",
        "RKQJ0GpkeQpn",
        "tfzXBr_VKnn0",
        "7llnVmNBKnn1",
        "SDD7euBLKnn1",
        "RZ99EfkAKnn2",
        "LF2GhyGqKnn2",
        "vCZztommtuWY",
        "Oywnn6k1ezSj",
        "7bwipeK4ezSl",
        "WmkD7gXqezSo",
        "fPZtNmU3ezSr",
        "OzuE-CP9ezSt",
        "IhPvsB_-ezSu",
        "Bvu3pgTlezSw",
        "psEyYySbezSy",
        "JjRru0uaKlm5",
        "2oVJvbBwKlm5",
        "loYWk3alKlm6",
        "BlYuEz3bKlm6",
        "mfmM0bw3Klm7",
        "t0Ou_DUXtx37"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# \"Dmoz_Computers\",\"Dmoz_Health\",\"Dmoz_Science\",\"Dmoz_Sports\"\n",
        "# \"classic4\",\"NSF\",\"re8\",\"review_polarity\", \"Industry_Sector\"\n",
        "# \"SyskillWebert\",\"webkb_parsed\", \"CSTR\"\n",
        "dataset_name = \"Dmoz_Computers\""
      ],
      "metadata": {
        "id": "DDgn0_O42cvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, load data , training functions and database creation"
      ],
      "metadata": {
        "id": "Qtpe_CYHnTqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "f9sKW8iq7Tny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2f5c6d-ee86-4d7a-eca8-87a8ba21b1fe",
        "id": "39ccaX7dnIjZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RqdT47FJJn3",
        "outputId": "1010bfdf-69c0-4969-8dfe-856c9b8def45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.2\n",
            "time: 558 µs (started: 2024-12-26 17:19:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import os, errno\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "import json\n",
        "import yaml\n",
        "import ast\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4Ut43TXtnIjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acea81ee-ef8e-45c2-8c2b-cb2b59c96e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 11.3 s (started: 2024-12-26 17:19:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifuE7A2bn9vp",
        "outputId": "e375ac87-37b0-4ed8-c4e1-677652d694b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "time: 35.8 s (started: 2024-12-26 17:20:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "from torch_geometric.nn import SAGEConv, GATConv, Linear, to_hetero"
      ],
      "metadata": {
        "id": "441Hk5OlRRIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b72e3b-e51c-490f-ab8d-90b6a36aa3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.7 s (started: 2024-12-26 17:20:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "D53m3GFKNaBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f7a53f-21f2-4048-a153-6a88ceb0f4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.69 ms (started: 2024-12-26 17:20:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data"
      ],
      "metadata": {
        "id": "S-0aDh52nO-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/\"+dataset_name+\"/\"+dataset_name+\".csv\")"
      ],
      "metadata": {
        "id": "-mntPGsInN7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5d27e1-a087-4faf-f6af-06a60058d691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 31.1 s (started: 2024-12-26 17:20:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "_9OxRWYh64rn",
        "outputId": "6e890d82-c1c8-4256-b411-0e9b341bbf2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                            file_name  \\\n",
              "0   0  Artificial_Intelligence_1434272.txt   \n",
              "1   1  Artificial_Intelligence_1433615.txt   \n",
              "2   2  Artificial_Intelligence_1434400.txt   \n",
              "3   3  Artificial_Intelligence_1434281.txt   \n",
              "4   4  Artificial_Intelligence_1434141.txt   \n",
              "\n",
              "                                                text                    class  \\\n",
              "0  Principal Curves Page Introduction to principa...  Artificial_Intelligence   \n",
              "1  Iowa State University - Artificial Intelligenc...  Artificial_Intelligence   \n",
              "2  Paccanaro, Alberto Learning distributed repres...  Artificial_Intelligence   \n",
              "3  Attrasoft Provider of a number of neural netwo...  Artificial_Intelligence   \n",
              "4  LibML A machine learning library. New implemen...  Artificial_Intelligence   \n",
              "\n",
              "                                          keyphrase2  \\\n",
              "0  [(principal curves, 0.7236), (curves summary, ...   \n",
              "1  [(university artificial, 0.5531), (intelligenc...   \n",
              "2  [(concepts relational, 0.5298), (distributed r...   \n",
              "3  [(sound recognition, 0.5352), (attrasoft provi...   \n",
              "4  [(machine learning, 0.614), (libml machine, 0....   \n",
              "\n",
              "                                          keyphrase3  \\\n",
              "0  [(introduction principal curves, 0.8212), (pri...   \n",
              "1  [(university artificial intelligence, 0.6874),...   \n",
              "2  [(distributed representation concepts, 0.5994)...   \n",
              "3  [(image sound recognition, 0.5388), (neural ne...   \n",
              "4  [(libml machine learning, 0.8631), (machine le...   \n",
              "\n",
              "                                         keyphrase23  \\\n",
              "0  [(introduction principal curves, 0.8212), (pri...   \n",
              "1  [(university artificial intelligence, 0.6874),...   \n",
              "2  [(distributed representation concepts, 0.5994)...   \n",
              "3  [(image sound recognition, 0.5388), (sound rec...   \n",
              "4  [(libml machine learning, 0.8631), (machine le...   \n",
              "\n",
              "                                     text_embeddings  \\\n",
              "0  [-0.034250993, -0.054697827, 0.02771881, -0.00...   \n",
              "1  [-0.037128627, -0.054665048, -0.023520134, -0....   \n",
              "2  [0.017933754, -0.093935154, -0.07233604, 0.049...   \n",
              "3  [-0.09700832, -0.123844825, -0.025894646, 0.02...   \n",
              "4  [-0.08543352, -0.08932488, -0.019404357, -0.04...   \n",
              "\n",
              "                               keyphrase2_embeddings  \\\n",
              "0  [[-0.035950914, -0.084732965, -0.01326547, -0....   \n",
              "1  [[0.013373093, -0.042429943, 0.0018845485, -0....   \n",
              "2  [[0.09489803, -0.010703894, -0.07612236, 0.007...   \n",
              "3  [[-0.043651912, -0.09779582, -0.024362028, -0....   \n",
              "4  [[-0.024391863, 0.0032444268, 0.054267664, -0....   \n",
              "\n",
              "                               keyphrase3_embeddings  \\\n",
              "0  [[-0.068310104, -0.07682108, -0.0070457095, -0...   \n",
              "1  [[-0.03555484, -0.020074962, 0.039645746, -0.0...   \n",
              "2  [[-0.055929314, -0.041734546, -0.06326631, -0....   \n",
              "3  [[-0.02410375, -0.06722981, -0.016101753, -0.0...   \n",
              "4  [[-0.06194022, -0.10829034, 0.034314997, -0.02...   \n",
              "\n",
              "                              keyphrase23_embeddings  \\\n",
              "0  [[-0.068310104, -0.07682108, -0.0070457095, -0...   \n",
              "1  [[-0.03555484, -0.020074962, 0.039645746, -0.0...   \n",
              "2  [[-0.055929314, -0.041734546, -0.06326631, -0....   \n",
              "3  [[-0.02410375, -0.06722981, -0.016101753, -0.0...   \n",
              "4  [[-0.06194022, -0.10829034, 0.034314997, -0.02...   \n",
              "\n",
              "                                sentences_embeddings  \n",
              "0  [[-0.034250993, -0.054697827, 0.02771881, -0.0...  \n",
              "1  [[-0.037128627, -0.054665048, -0.023520134, -0...  \n",
              "2  [[0.017933754, -0.093935154, -0.07233604, 0.04...  \n",
              "3  [[-0.09700832, -0.123844825, -0.025894646, 0.0...  \n",
              "4  [[-0.064178735, -0.12423259, -0.01962825, -0.0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a73e3212-5447-4bdd-81a2-8d9c274449eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "      <th>keyphrase2</th>\n",
              "      <th>keyphrase3</th>\n",
              "      <th>keyphrase23</th>\n",
              "      <th>text_embeddings</th>\n",
              "      <th>keyphrase2_embeddings</th>\n",
              "      <th>keyphrase3_embeddings</th>\n",
              "      <th>keyphrase23_embeddings</th>\n",
              "      <th>sentences_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Artificial_Intelligence_1434272.txt</td>\n",
              "      <td>Principal Curves Page Introduction to principa...</td>\n",
              "      <td>Artificial_Intelligence</td>\n",
              "      <td>[(principal curves, 0.7236), (curves summary, ...</td>\n",
              "      <td>[(introduction principal curves, 0.8212), (pri...</td>\n",
              "      <td>[(introduction principal curves, 0.8212), (pri...</td>\n",
              "      <td>[-0.034250993, -0.054697827, 0.02771881, -0.00...</td>\n",
              "      <td>[[-0.035950914, -0.084732965, -0.01326547, -0....</td>\n",
              "      <td>[[-0.068310104, -0.07682108, -0.0070457095, -0...</td>\n",
              "      <td>[[-0.068310104, -0.07682108, -0.0070457095, -0...</td>\n",
              "      <td>[[-0.034250993, -0.054697827, 0.02771881, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Artificial_Intelligence_1433615.txt</td>\n",
              "      <td>Iowa State University - Artificial Intelligenc...</td>\n",
              "      <td>Artificial_Intelligence</td>\n",
              "      <td>[(university artificial, 0.5531), (intelligenc...</td>\n",
              "      <td>[(university artificial intelligence, 0.6874),...</td>\n",
              "      <td>[(university artificial intelligence, 0.6874),...</td>\n",
              "      <td>[-0.037128627, -0.054665048, -0.023520134, -0....</td>\n",
              "      <td>[[0.013373093, -0.042429943, 0.0018845485, -0....</td>\n",
              "      <td>[[-0.03555484, -0.020074962, 0.039645746, -0.0...</td>\n",
              "      <td>[[-0.03555484, -0.020074962, 0.039645746, -0.0...</td>\n",
              "      <td>[[-0.037128627, -0.054665048, -0.023520134, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Artificial_Intelligence_1434400.txt</td>\n",
              "      <td>Paccanaro, Alberto Learning distributed repres...</td>\n",
              "      <td>Artificial_Intelligence</td>\n",
              "      <td>[(concepts relational, 0.5298), (distributed r...</td>\n",
              "      <td>[(distributed representation concepts, 0.5994)...</td>\n",
              "      <td>[(distributed representation concepts, 0.5994)...</td>\n",
              "      <td>[0.017933754, -0.093935154, -0.07233604, 0.049...</td>\n",
              "      <td>[[0.09489803, -0.010703894, -0.07612236, 0.007...</td>\n",
              "      <td>[[-0.055929314, -0.041734546, -0.06326631, -0....</td>\n",
              "      <td>[[-0.055929314, -0.041734546, -0.06326631, -0....</td>\n",
              "      <td>[[0.017933754, -0.093935154, -0.07233604, 0.04...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Artificial_Intelligence_1434281.txt</td>\n",
              "      <td>Attrasoft Provider of a number of neural netwo...</td>\n",
              "      <td>Artificial_Intelligence</td>\n",
              "      <td>[(sound recognition, 0.5352), (attrasoft provi...</td>\n",
              "      <td>[(image sound recognition, 0.5388), (neural ne...</td>\n",
              "      <td>[(image sound recognition, 0.5388), (sound rec...</td>\n",
              "      <td>[-0.09700832, -0.123844825, -0.025894646, 0.02...</td>\n",
              "      <td>[[-0.043651912, -0.09779582, -0.024362028, -0....</td>\n",
              "      <td>[[-0.02410375, -0.06722981, -0.016101753, -0.0...</td>\n",
              "      <td>[[-0.02410375, -0.06722981, -0.016101753, -0.0...</td>\n",
              "      <td>[[-0.09700832, -0.123844825, -0.025894646, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Artificial_Intelligence_1434141.txt</td>\n",
              "      <td>LibML A machine learning library. New implemen...</td>\n",
              "      <td>Artificial_Intelligence</td>\n",
              "      <td>[(machine learning, 0.614), (libml machine, 0....</td>\n",
              "      <td>[(libml machine learning, 0.8631), (machine le...</td>\n",
              "      <td>[(libml machine learning, 0.8631), (machine le...</td>\n",
              "      <td>[-0.08543352, -0.08932488, -0.019404357, -0.04...</td>\n",
              "      <td>[[-0.024391863, 0.0032444268, 0.054267664, -0....</td>\n",
              "      <td>[[-0.06194022, -0.10829034, 0.034314997, -0.02...</td>\n",
              "      <td>[[-0.06194022, -0.10829034, 0.034314997, -0.02...</td>\n",
              "      <td>[[-0.064178735, -0.12423259, -0.01962825, -0.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a73e3212-5447-4bdd-81a2-8d9c274449eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a73e3212-5447-4bdd-81a2-8d9c274449eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a73e3212-5447-4bdd-81a2-8d9c274449eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bcb26096-9145-49fa-aff4-200acdc410fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bcb26096-9145-49fa-aff4-200acdc410fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bcb26096-9145-49fa-aff4-200acdc410fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 9500,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2742,\n        \"min\": 0,\n        \"max\": 9499,\n        \"num_unique_values\": 9500,\n        \"samples\": [\n          952,\n          6599,\n          4220\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9500,\n        \"samples\": [\n          \"CAD_and_CAM_1435132.txt\",\n          \"Open_Source_1481662.txt\",\n          \"Graphics_1444597.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9494,\n        \"samples\": [\n          \"Instar Corporation Provides high performance storage solutions. Focusing on MO, DVD, and CD-R optical technologies. \",\n          \"Downloading Ada95 Files and compilers. \",\n          \"Formtec GmbH NCspeed NC optimization software to speed up the machining process for manufacturing dies and molds. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"Artificial_Intelligence\",\n          \"Data_Formats\",\n          \"Mobile_Computing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase2\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase3\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase23\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase2_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase3_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase23_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 150 ms (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training functions and Model Definitions"
      ],
      "metadata": {
        "id": "SUqyo3PodvRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    mask = data['document'].train_mask\n",
        "    loss = F.cross_entropy(out['document'][mask], data['document'].y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)"
      ],
      "metadata": {
        "id": "L1LBCeLIWlFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba46e86-4253-4c4c-95d6-2b465513ae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 910 µs (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "    model.eval()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = out['document'].argmax(dim=1)\n",
        "    # print(\"PREDICTIONS ->\", pred)\n",
        "    accs = []\n",
        "    for mask in [data['document'].train_mask, data['document'].test_mask]:\n",
        "        accs.append(int((pred[mask] == data['document'].y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs"
      ],
      "metadata": {
        "id": "GgvEx3HBXCc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7dfe3a-29a4-47d4-dbb6-052f7fde4d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 836 µs (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y(model):\n",
        "    model.eval()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = out['document'].argmax(dim=1)\n",
        "    print(\"PREDICTIONS ->\", pred)\n",
        "\n",
        "    # y_pred, y_true\n",
        "    return pred[data['document'].test_mask],data['document'].y[data['document'].test_mask]"
      ],
      "metadata": {
        "id": "o7a82kzNCt90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c1f191-b69c-456d-c598-8e03958bcc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 357 µs (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNN Model"
      ],
      "metadata": {
        "id": "xtTe3rAQrhG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First Network\n",
        "from torch_geometric.nn import GraphConv\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GraphConv(-1, hidden_channels)\n",
        "        self.conv2 = GraphConv(-1, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PUVqN5z0q80O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a388943e-8a25-4446-b7a2-c72f19b248ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 483 µs (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAT Model"
      ],
      "metadata": {
        "id": "Pws3dFR9rkOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Network\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATv2Conv((-1, -1), hidden_channels, add_self_loops=False)\n",
        "        self.lin1 = Linear(-1, hidden_channels)\n",
        "        self.conv2 = GATv2Conv((-1, -1), out_channels, add_self_loops=False)\n",
        "        self.lin2 = Linear(-1, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DAYkObIpq80Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd55117-d725-4e8a-9c01-9e9db25a6ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 556 µs (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iteration"
      ],
      "metadata": {
        "id": "HHo1UacopxwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iteration(i,GCN_or_GAT,rotulated):\n",
        "    # for i in range(10):\n",
        "    print(f\"\\n===============================================\")\n",
        "    print(f\"=================== MODEL {i} ===================\")\n",
        "    print(f\"===============================================\\n\")\n",
        "\n",
        "    # Defining masks\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'rb') as f:\n",
        "        aux = pickle.load(f)\n",
        "        train_mask, val_mask, test_mask = aux[0],aux[1],aux[2]\n",
        "\n",
        "    data['document'].train_mask = train_mask\n",
        "    data['document'].val_mask = val_mask\n",
        "    data['document'].test_mask = test_mask\n",
        "\n",
        "    # Generate MODEL and Optmizer\n",
        "    if GCN_or_GAT == \"GCN\":\n",
        "        model = GNN(hidden_channels=64, out_channels=class_number)\n",
        "    elif GCN_or_GAT ==\"GAT\":\n",
        "        model = GAT(hidden_channels=64, out_channels=class_number)\n",
        "\n",
        "    model = to_hetero(model, data.metadata(), aggr='sum')\n",
        "    model.to(device) # GPU\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "    #contagem de tempo\n",
        "    start = time.time()\n",
        "\n",
        "    # Training\n",
        "    L_loss = []\n",
        "    for epoch in range(1, 1000):\n",
        "        loss = train(model,optimizer)\n",
        "        L_loss.append(loss)\n",
        "        train_acc, test_acc = test(model)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Test: {test_acc:.4f}')\n",
        "        df_temp = pd.DataFrame(L_loss)\n",
        "        if len(df_temp >=5):\n",
        "          loss_es =  pd.DataFrame(L_loss).tail(5)[0].std()\n",
        "          print('Early stopping: ',loss_es)\n",
        "          if loss_es <= 0.01:\n",
        "            break\n",
        "\n",
        "    #contagem de tempo\n",
        "    end = time.time()\n",
        "\n",
        "    # Creating Classification Report\n",
        "    y_pred,y_true = get_y(model)\n",
        "    y_pred = y_pred.cpu()\n",
        "    y_true = y_true.cpu()\n",
        "    target_names = df[\"class\"].unique()\n",
        "\n",
        "    class_dit = classification_report(y_true, y_pred, target_names=target_names, output_dict = True)\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "\n",
        "    #L_results.append([model_name,'GCN',rotulated,class_dit['macro avg']['f1-score']])\n",
        "    # Updating Dataframes\n",
        "    df_list[i][0].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['accuracy']               # acuracia\n",
        "    df_list[i][1].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['precision'] # precision\n",
        "    df_list[i][2].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['recall']    # recall\n",
        "    df_list[i][3].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['f1-score']  # f1-score\n",
        "    df_list[i][4].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['support']   # support\n",
        "    df_list[i][5].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = (end - start)                       # time"
      ],
      "metadata": {
        "id": "Ia8rnSQFmssN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52db21a-af3a-45cb-c76e-ff3e28eee68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.19 ms (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Database"
      ],
      "metadata": {
        "id": "VQWCTrxz7atg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Only execude if dafaframes do not exist YET\n",
        "\n",
        "# df_list = []\n",
        "\n",
        "# for i in range(10):\n",
        "#     aux_list = []\n",
        "\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # acuracia\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # precision\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # recall\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # f1-score\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # support\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # time\n",
        "\n",
        "#     df_list.append(aux_list)\n",
        "\n",
        "#     try:\n",
        "#         os.makedirs(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i))\n",
        "#     except OSError as e:\n",
        "#         if e.errno != errno.EEXIST:\n",
        "#             raise\n",
        "\n",
        "#     df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "#     df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "#     df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "#     df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "#     df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "#     df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "99RJ3y7cm9AG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700b5c41-5c12-4db8-ea08-902f246fc842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 241 µs (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = []\n",
        "for i in range(10):\n",
        "    aux_list = []\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\")) # acuracia\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\")) # precision\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\")) # recall\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\"))  # f1-score\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\")) # support\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\")) # time\n",
        "    df_list.append(aux_list)"
      ],
      "metadata": {
        "id": "HnYLm6iznJq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04090d41-2dbc-4053-cf08-f0796a9ff9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 35 s (started: 2024-12-26 17:21:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_list[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Dxz4VW7-iy3C",
        "outputId": "8ae642bf-8d85-4d62-bb5c-eb3a862dbd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        1         5        10        20  \\\n",
              "Dmoz_Computers_keyphrase2_GCN    0.180572  0.393195  0.446724  0.489474   \n",
              "Dmoz_Computers_keyphrase2_GAT    0.263685  0.427007  0.479377  0.530044   \n",
              "Dmoz_Computers_keyphrase3_GCN    0.235524  0.436151  0.481633  0.541338   \n",
              "Dmoz_Computers_keyphrase3_GAT    0.244489  0.456034  0.499033  0.567544   \n",
              "Dmoz_Computers_keyphrase23_GCN   0.200506   0.42488  0.495811  0.545285   \n",
              "...                                   ...       ...       ...       ...   \n",
              "Industry_Sector_keyphrase2_GAT    0.10971  0.192075  0.264229  0.400956   \n",
              "Industry_Sector_keyphrase3_GCN   0.126178  0.143885  0.246637  0.236446   \n",
              "Industry_Sector_keyphrase3_GAT   0.108007  0.208176  0.338278  0.397575   \n",
              "Industry_Sector_keyphrase23_GCN  0.117547  0.174832   0.22732  0.306168   \n",
              "Industry_Sector_keyphrase23_GAT  0.055196  0.209889  0.320225   0.42066   \n",
              "\n",
              "                                       30  \n",
              "Dmoz_Computers_keyphrase2_GCN     0.51299  \n",
              "Dmoz_Computers_keyphrase2_GAT    0.540202  \n",
              "Dmoz_Computers_keyphrase3_GCN     0.54009  \n",
              "Dmoz_Computers_keyphrase3_GAT    0.569317  \n",
              "Dmoz_Computers_keyphrase23_GCN   0.556215  \n",
              "...                                   ...  \n",
              "Industry_Sector_keyphrase2_GAT   0.272674  \n",
              "Industry_Sector_keyphrase3_GCN   0.170864  \n",
              "Industry_Sector_keyphrase3_GAT   0.410666  \n",
              "Industry_Sector_keyphrase23_GCN  0.370344  \n",
              "Industry_Sector_keyphrase23_GAT  0.418233  \n",
              "\n",
              "[72 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc5522cd-c08c-4c62-8727-4c2cd8f4a89a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>5</th>\n",
              "      <th>10</th>\n",
              "      <th>20</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase2_GCN</th>\n",
              "      <td>0.180572</td>\n",
              "      <td>0.393195</td>\n",
              "      <td>0.446724</td>\n",
              "      <td>0.489474</td>\n",
              "      <td>0.51299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase2_GAT</th>\n",
              "      <td>0.263685</td>\n",
              "      <td>0.427007</td>\n",
              "      <td>0.479377</td>\n",
              "      <td>0.530044</td>\n",
              "      <td>0.540202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase3_GCN</th>\n",
              "      <td>0.235524</td>\n",
              "      <td>0.436151</td>\n",
              "      <td>0.481633</td>\n",
              "      <td>0.541338</td>\n",
              "      <td>0.54009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase3_GAT</th>\n",
              "      <td>0.244489</td>\n",
              "      <td>0.456034</td>\n",
              "      <td>0.499033</td>\n",
              "      <td>0.567544</td>\n",
              "      <td>0.569317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase23_GCN</th>\n",
              "      <td>0.200506</td>\n",
              "      <td>0.42488</td>\n",
              "      <td>0.495811</td>\n",
              "      <td>0.545285</td>\n",
              "      <td>0.556215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase2_GAT</th>\n",
              "      <td>0.10971</td>\n",
              "      <td>0.192075</td>\n",
              "      <td>0.264229</td>\n",
              "      <td>0.400956</td>\n",
              "      <td>0.272674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase3_GCN</th>\n",
              "      <td>0.126178</td>\n",
              "      <td>0.143885</td>\n",
              "      <td>0.246637</td>\n",
              "      <td>0.236446</td>\n",
              "      <td>0.170864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase3_GAT</th>\n",
              "      <td>0.108007</td>\n",
              "      <td>0.208176</td>\n",
              "      <td>0.338278</td>\n",
              "      <td>0.397575</td>\n",
              "      <td>0.410666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase23_GCN</th>\n",
              "      <td>0.117547</td>\n",
              "      <td>0.174832</td>\n",
              "      <td>0.22732</td>\n",
              "      <td>0.306168</td>\n",
              "      <td>0.370344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase23_GAT</th>\n",
              "      <td>0.055196</td>\n",
              "      <td>0.209889</td>\n",
              "      <td>0.320225</td>\n",
              "      <td>0.42066</td>\n",
              "      <td>0.418233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc5522cd-c08c-4c62-8727-4c2cd8f4a89a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc5522cd-c08c-4c62-8727-4c2cd8f4a89a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc5522cd-c08c-4c62-8727-4c2cd8f4a89a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b1cb5eac-6b16-455b-91e4-7ea30b40309d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b1cb5eac-6b16-455b-91e4-7ea30b40309d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b1cb5eac-6b16-455b-91e4-7ea30b40309d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_list[0][0]\",\n  \"rows\": 72,\n  \"fields\": [\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.05519591141396934,\n        \"max\": 0.7347341700747426,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          0.3466191642544348,\n          0.18057166965509966,\n          0.42166126075306726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.14388489208633093,\n        \"max\": 0.8759010600706714,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          0.6006734006734007,\n          0.39319510898458265,\n          0.8541342756183745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.09011203117389187,\n        \"max\": 0.8803685329553508,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          0.6872260015117158,\n          0.44672395273899035,\n          0.8776754075124026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"20\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.20781134856300662,\n        \"max\": 0.9125632153313814,\n        \"num_unique_values\": 71,\n        \"samples\": [\n          0.7217592592592592,\n          0.48947368421052634,\n          0.8930862437633642\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"30\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.10062669977533405,\n        \"max\": 0.9263082437275986,\n        \"num_unique_values\": 65,\n        \"samples\": [\n          0.4374380574826561,\n          0.27267352489062313,\n          0.512989921612542\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21 ms (started: 2024-12-26 17:21:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "# Hetrogenous Graph for Keyphrase = 2\n",
        "--------------------------------------"
      ],
      "metadata": {
        "id": "F42AmIbTm_Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"class\"].unique())\n",
        "class_number = len(df[\"class\"].unique())\n",
        "print(class_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN5oB59YhW7s",
        "outputId": "afdba572-fa1f-4205-a122-4266f8bb4296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial_Intelligence' 'CAD_and_CAM' 'Companies' 'Computer_Science'\n",
            " 'Consultants' 'Data_Formats' 'Data_Communications' 'Education' 'Graphics'\n",
            " 'Hardware' 'Internet' 'Mobile_Computing' 'Multimedia' 'Open_Source'\n",
            " 'Programming' 'Robotics' 'Security' 'Software' 'Systems']\n",
            "19\n",
            "time: 3.76 ms (started: 2024-12-26 17:21:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change here to change wich keypharse to use\n",
        "keyphrase = \"keyphrase2\"\n",
        "\n",
        "model_name = dataset_name+\"_\"+keyphrase"
      ],
      "metadata": {
        "id": "_JLOrZczNBsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bae8545-bc0b-4b59-a60d-a453e5dc49f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 320 µs (started: 2024-12-26 17:21:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Graph"
      ],
      "metadata": {
        "id": "H9tMAdQPDowd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Graph Nodes and Edges 👀\n",
        "\n",
        "- `Nodes` - documents and contexts\n",
        "- `Edges`\n",
        "  - document <- has -> context\n",
        "- `Labels` - documents classes\n"
      ],
      "metadata": {
        "id": "QrRkgxKu2Wiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Nodes and Edges 👀"
      ],
      "metadata": {
        "id": "Z_kBUMGf_lWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Defining Docmuent nodes, Context nodes and edges between them"
      ],
      "metadata": {
        "id": "MVfGvWUSs0z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_contexts_list =[]\n",
        "edges1,edges2 = [],[]\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes = []\n",
        "context_nodes = []\n",
        "\n",
        "edges_tuple = []\n",
        "\n",
        "sentences = []\n",
        "cont_sentences = 0\n",
        "dit_sentences = {}\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding document nodes for every doc in df\n",
        "    document_nodes.append(df[\"text_embeddings\"][i])\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list: # if NOT\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes.append(df[keyphrase+\"_embeddings\"][i][j])\n",
        "            # add a new edge between doc and new context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(new_edge_cont)\n",
        "            edges_tuple.append((df[\"id\"][i],new_edge_cont))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(all_contexts_list.index(key[0]))\n",
        "            edges_tuple.append((df[\"id\"][i],all_contexts_list.index(key[0])))\n",
        "            cont+=1\n",
        "\n",
        "    # organize sentences, sentences_embeddings, and a dict with the corresponding document for each sentence\n",
        "    aux = df['sentences_embeddings'][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        sentences.append(key)\n",
        "        dit_sentences[cont_sentences] = df[\"id\"][i]\n",
        "        cont_sentences += 1\n",
        "\n",
        "\n",
        "document_nodes = np.array(document_nodes)\n",
        "context_nodes = np.array(context_nodes)"
      ],
      "metadata": {
        "id": "OY-ZzJGr96m9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270e22e5-b8a7-4bac-ff73-e7cb07953d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18.9 s (started: 2024-12-26 17:21:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dit_sentences[9500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrU13Asr-yl2",
        "outputId": "ac0442b8-72b2-4612-de23-88d0f58f078c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6538"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.66 ms (started: 2024-12-26 17:23:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CtAStd_-xEX",
        "outputId": "01ac8f07-5e92-4487-aaf9-7f9895c5e137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14131"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.46 ms (started: 2024-12-26 17:24:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cont_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIQt36XJDesk",
        "outputId": "175a4cf3-a2d7-4145-94f8-bf14114104aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14131"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14.4 ms (started: 2024-12-26 17:40:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of document nodes:\",len(document_nodes))\n",
        "print(\"number of context nodes:\",len(context_nodes))\n",
        "print(\"number of shared contexts:\",cont)\n",
        "print(\"number of direct edges (first dimension):\",len(edges1))\n",
        "print(\"number of direct edges (second dimension):\",len(edges2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXOWqRQplTad",
        "outputId": "80509b35-88bd-42f1-b1ae-8aae407d1e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 9500\n",
            "number of context nodes: 38955\n",
            "number of shared contexts: 8231\n",
            "number of direct edges (first dimension): 47186\n",
            "number of direct edges (second dimension): 47186\n",
            "time: 4.93 ms (started: 2024-08-17 14:57:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=15, metric='cosine').fit(context_nodes)\n",
        "\n",
        "neighbors_list = nbrs.kneighbors(sentences, return_distance=False)\n",
        "\n",
        "# cria aresta para cada vizinho encontrado\n",
        "for i,neighbors in enumerate(neighbors_list):\n",
        "        for n in neighbors:\n",
        "            edges1.append(dit_sentences[i])\n",
        "            edges2.append(n)\n",
        "            edges_tuple.append((dit_sentences[i],n))"
      ],
      "metadata": {
        "id": "aaRvK2v-9KCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e60c83a-f190-4278-cd35-ecf5c21fd7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13 s (started: 2024-08-17 14:57:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ajusting everything to Tensor Objects"
      ],
      "metadata": {
        "id": "hp2TAceotXLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms egdes to tensor\n",
        "edges = np.array([edges1,edges2])\n",
        "edges = torch.tensor(edges)"
      ],
      "metadata": {
        "id": "HDIlnLOPxgNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6b6b21-be1c-4d85-977e-5a837a723654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 49.4 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms nodes to tensor\n",
        "document_nodes = np.array(document_nodes)\n",
        "document_nodes = torch.tensor(document_nodes)\n",
        "context_nodes = np.array(context_nodes)\n",
        "context_nodes = torch.tensor(context_nodes)"
      ],
      "metadata": {
        "id": "ugk20HV6lZk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d3a337-11b6-4b1a-ba1c-fd3772fdc428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 67.1 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show documents nodes\n",
        "document_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p3qraqHR3hP",
        "outputId": "06fbf043-635f-40e7-a938-8fc001d7b04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0343, -0.0547,  0.0277,  ...,  0.0056, -0.0645,  0.0124],\n",
              "        [-0.0371, -0.0547, -0.0235,  ...,  0.0408,  0.0109, -0.0873],\n",
              "        [ 0.0179, -0.0939, -0.0723,  ..., -0.0052, -0.0241, -0.0033],\n",
              "        ...,\n",
              "        [-0.0668, -0.0605, -0.0190,  ...,  0.1015, -0.0920,  0.1007],\n",
              "        [ 0.0360,  0.0188, -0.0016,  ...,  0.0011,  0.0394,  0.0837],\n",
              "        [-0.0379,  0.0473, -0.0619,  ...,  0.0604,  0.0204, -0.0091]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 83.2 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of document nodes\n",
        "len(document_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVG81JF6RTf6",
        "outputId": "8a8f3cc3-62d5-49f4-a99f-52c44603e751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.83 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show context nodes\n",
        "context_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f6c596-58b2-4f33-bf6f-ae819c0fbb23",
        "id": "ku5cQDCur-y1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0360, -0.0847, -0.0133,  ...,  0.0233,  0.0017, -0.0408],\n",
              "        [-0.0184, -0.0183, -0.0022,  ...,  0.0476,  0.0608, -0.0448],\n",
              "        [-0.0510, -0.0211, -0.0460,  ...,  0.0450,  0.0468, -0.0529],\n",
              "        ...,\n",
              "        [-0.0808,  0.0026, -0.0760,  ...,  0.0095,  0.0288,  0.0107],\n",
              "        [-0.0247,  0.0546, -0.0195,  ...,  0.0589,  0.0089, -0.0067],\n",
              "        [-0.0471,  0.0765, -0.0771,  ..., -0.0453, -0.0008, -0.0039]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.05 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of context nodes\n",
        "len(context_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d477df-9f9b-4041-d235-e052cf2bbb6b",
        "id": "eWiCUWg5r-y4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38955"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.66 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of edges\n",
        "print(len(edges[0]))\n",
        "print(len(edges[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4EeQVr62eVO",
        "outputId": "40ec87ed-d809-45b6-c017-0f696dcfb49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "259151\n",
            "259151\n",
            "time: 623 µs (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing edges\n",
        "print(edges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FOMuN9XC-0D",
        "outputId": "f560b2eb-78e7-4cc7-bceb-62da68cc9727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ...,  9499,  9499,  9499],\n",
            "        [    0,     1,     2,  ..., 38953, 37323, 23098]])\n",
            "time: 869 µs (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class Labels"
      ],
      "metadata": {
        "id": "bXGrNtvp_pvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All different classes\n",
        "print(df[\"class\"].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e565d4a-4ee2-40c1-bc0c-0f4833023057",
        "id": "uRZiISzyRMcZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial_Intelligence' 'CAD_and_CAM' 'Companies' 'Computer_Science'\n",
            " 'Consultants' 'Data_Formats' 'Data_Communications' 'Education' 'Graphics'\n",
            " 'Hardware' 'Internet' 'Mobile_Computing' 'Multimedia' 'Open_Source'\n",
            " 'Programming' 'Robotics' 'Security' 'Software' 'Systems']\n",
            "time: 2.57 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating labels for classification\n",
        "# the dictionary is a numeric representation for each possible \"document\" class\n",
        "\n",
        "dit = {}\n",
        "for i,classe in enumerate(df[\"class\"].unique()):\n",
        "  dit[classe] = i\n",
        "\n",
        "print(dit,'\\n')\n",
        "\n",
        "labels = df[[\"class\"]]\n",
        "for i in range(len(df[[\"class\"]])):\n",
        "    labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjvgRobhDwU_",
        "outputId": "7ce23255-534c-4c13-de15-db3e6f969fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Artificial_Intelligence': 0, 'CAD_and_CAM': 1, 'Companies': 2, 'Computer_Science': 3, 'Consultants': 4, 'Data_Formats': 5, 'Data_Communications': 6, 'Education': 7, 'Graphics': 8, 'Hardware': 9, 'Internet': 10, 'Mobile_Computing': 11, 'Multimedia': 12, 'Open_Source': 13, 'Programming': 14, 'Robotics': 15, 'Security': 16, 'Software': 17, 'Systems': 18} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-a018e46c97dd>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class\n",
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80abad78-18e5-4c7d-8c0f-d94aec0d22a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80abad78-18e5-4c7d-8c0f-d94aec0d22a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80abad78-18e5-4c7d-8c0f-d94aec0d22a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80abad78-18e5-4c7d-8c0f-d94aec0d22a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c440c060-2008-4659-80ff-a904a18789f3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c440c060-2008-4659-80ff-a904a18789f3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c440c060-2008-4659-80ff-a904a18789f3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels",
              "summary": "{\n  \"name\": \"labels\",\n  \"rows\": 9500,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 18,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          0,\n          5,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 873 ms (started: 2024-08-17 14:58:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tranfors class dataframe into tensor\n",
        "y = labels[\"class\"].tolist()\n",
        "y = x_np = torch.tensor(y)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3wqIjcRyV8I",
        "outputId": "e6010bcd-4b3b-43d3-e196-c088dd68678b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  ..., 18, 18, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.97 ms (started: 2024-08-17 14:58:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofxnswq6RlNN",
        "outputId": "7dc49468-af85-40f3-a4ed-0d7c997617d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.51 ms (started: 2024-08-17 14:58:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Graph with Networkx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "olhv0tQ8Skgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining overal graph in networkx maner"
      ],
      "metadata": {
        "id": "_oj2BlyrsCdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run graph Representation for networkx\n",
        "all_contexts_list =[]\n",
        "edges_test = []\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes_test = []\n",
        "context_nodes_test = []\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding new documents for every node\n",
        "    document_nodes_test.append(\"doc_\"+str(i)) # in the actual graph nodes -> documents embeddings\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list:\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes_test.append(\"contx_\"+str(new_edge_cont)) # in the actual graph nodes -> context embeddings\n",
        "\n",
        "            # add a new edge between doc and new context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(new_edge_cont)))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(all_contexts_list.index(key[0]))))\n",
        "            cont+=1"
      ],
      "metadata": {
        "id": "c6cZnmOq-UcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903fd882-3456-4df3-cf94-67eb63db9707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14 s (started: 2024-08-17 14:58:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_test = [(\"doc_\"+str(i[0]),\"contx_\"+str(i[1])) for i in edges_tuple]"
      ],
      "metadata": {
        "id": "p15FEl45BtKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a0d590-62a4-443f-908b-b90a1fa0293a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 532 ms (started: 2024-08-17 14:58:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of document nodes:\",len(document_nodes_test))\n",
        "print(\"number of context nodes:\",len(context_nodes_test))\n",
        "print(\"number of edges:\",len(edges_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfCPWtQ3fk23",
        "outputId": "449308d7-51dc-4867-83db-68f35b5baf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 9500\n",
            "number of context nodes: 38955\n",
            "number of edges: 259151\n",
            "time: 1.42 ms (started: 2024-08-17 14:58:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(edges_test[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1OSvVPAij1s",
        "outputId": "1731325a-3cf2-4507-96b0-6da490bf3929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('doc_0', 'contx_0'), ('doc_0', 'contx_1'), ('doc_0', 'contx_2'), ('doc_0', 'contx_3'), ('doc_0', 'contx_4'), ('doc_1', 'contx_5'), ('doc_1', 'contx_6'), ('doc_1', 'contx_7'), ('doc_1', 'contx_8'), ('doc_1', 'contx_9'), ('doc_2', 'contx_10'), ('doc_2', 'contx_11'), ('doc_2', 'contx_12'), ('doc_2', 'contx_13'), ('doc_2', 'contx_14'), ('doc_3', 'contx_15'), ('doc_3', 'contx_16'), ('doc_3', 'contx_17'), ('doc_3', 'contx_18'), ('doc_3', 'contx_19'), ('doc_4', 'contx_20'), ('doc_4', 'contx_21'), ('doc_4', 'contx_22'), ('doc_4', 'contx_23'), ('doc_4', 'contx_24'), ('doc_5', 'contx_25'), ('doc_5', 'contx_26'), ('doc_5', 'contx_27'), ('doc_5', 'contx_28'), ('doc_5', 'contx_29'), ('doc_6', 'contx_30'), ('doc_6', 'contx_31'), ('doc_6', 'contx_32'), ('doc_6', 'contx_33'), ('doc_6', 'contx_34'), ('doc_7', 'contx_35'), ('doc_7', 'contx_36'), ('doc_7', 'contx_37'), ('doc_7', 'contx_38'), ('doc_7', 'contx_39'), ('doc_8', 'contx_40'), ('doc_8', 'contx_41'), ('doc_8', 'contx_42'), ('doc_8', 'contx_43'), ('doc_8', 'contx_44'), ('doc_9', 'contx_45'), ('doc_9', 'contx_46'), ('doc_9', 'contx_47'), ('doc_9', 'contx_48'), ('doc_9', 'contx_49'), ('doc_10', 'contx_50'), ('doc_10', 'contx_51'), ('doc_10', 'contx_52'), ('doc_10', 'contx_53'), ('doc_10', 'contx_54'), ('doc_11', 'contx_55'), ('doc_11', 'contx_33'), ('doc_11', 'contx_56'), ('doc_11', 'contx_57'), ('doc_11', 'contx_58'), ('doc_12', 'contx_59'), ('doc_12', 'contx_60'), ('doc_12', 'contx_61'), ('doc_12', 'contx_62'), ('doc_12', 'contx_63'), ('doc_13', 'contx_64'), ('doc_13', 'contx_65'), ('doc_13', 'contx_66'), ('doc_13', 'contx_67'), ('doc_13', 'contx_68'), ('doc_14', 'contx_69'), ('doc_14', 'contx_70'), ('doc_14', 'contx_71'), ('doc_14', 'contx_72'), ('doc_14', 'contx_73'), ('doc_15', 'contx_74'), ('doc_15', 'contx_75'), ('doc_15', 'contx_76'), ('doc_15', 'contx_77'), ('doc_15', 'contx_78'), ('doc_16', 'contx_79'), ('doc_16', 'contx_80'), ('doc_16', 'contx_81'), ('doc_16', 'contx_82'), ('doc_16', 'contx_20'), ('doc_17', 'contx_83'), ('doc_17', 'contx_84'), ('doc_17', 'contx_85'), ('doc_17', 'contx_86'), ('doc_17', 'contx_53'), ('doc_18', 'contx_87'), ('doc_18', 'contx_88'), ('doc_18', 'contx_89'), ('doc_18', 'contx_90'), ('doc_18', 'contx_91'), ('doc_19', 'contx_92'), ('doc_19', 'contx_93'), ('doc_19', 'contx_94'), ('doc_19', 'contx_17'), ('doc_19', 'contx_95')]\n",
            "time: 376 µs (started: 2024-08-17 14:58:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test graph Conectivity with networkx"
      ],
      "metadata": {
        "id": "LZQnu2Drr55D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Bipartide graph\n",
        "B = nx.Graph()\n",
        "B.add_nodes_from(document_nodes_test, bipartite=0)\n",
        "B.add_nodes_from(context_nodes_test, bipartite=1)\n",
        "B.add_edges_from(edges_test)"
      ],
      "metadata": {
        "id": "rRZ8fI49WiNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3149f4-6482-488e-da37-28d04c67239b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 398 ms (started: 2024-08-17 14:58:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.is_connected(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdCzkLKRjtWj",
        "outputId": "5c743745-00fb-46da-e9ba-f3c835abfcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 124 ms (started: 2024-08-17 14:58:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of conected elements\n",
        "# if == 1 -> all elements of the graph are conected\n",
        "print(nx.number_connected_components(B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW3DrKQTd0lR",
        "outputId": "0f5bc78e-ec40-40e7-ff2a-5f3397be86c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "time: 120 ms (started: 2024-08-17 14:58:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size of a cluster\n",
        "print(len(nx.node_connected_component(B,'doc_0')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yywkADdJ002y",
        "outputId": "b3ebb287-1aa5-4fa2-c4ed-1d67ad9bc59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48455\n",
            "time: 99.7 ms (started: 2024-08-17 14:58:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Graph HeteroData Object"
      ],
      "metadata": {
        "id": "-pincp8wI-yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining nodes, edges and class labels\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# nodes\n",
        "data['document'].x = document_nodes\n",
        "data['concept'].x = context_nodes\n",
        "\n",
        "# edges\n",
        "data['document', 'has', 'concept'].edge_index = edges\n",
        "\n",
        "#class labels\n",
        "data['document'].y = y\n"
      ],
      "metadata": {
        "id": "PKzQhBQYLPvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414dd9ab-c162-426f-c8c4-5594c2bd0a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.18 ms (started: 2024-08-17 14:58:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting graph to undirected\n",
        "data = T.ToUndirected()(data)"
      ],
      "metadata": {
        "id": "IplyJlwsUvDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f611bc60-87a5-4c82-a66a-de9a121e33e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.34 ms (started: 2024-08-17 14:58:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate edges\n",
        "data = T.RemoveDuplicatedEdges()(data)"
      ],
      "metadata": {
        "id": "6KZTlEWJAP-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee525f1-5728-4ba7-916b-6d1c66e85387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 67.3 ms (started: 2024-08-17 14:58:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date in using gpu\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "dn0v0rKXRMWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22727dfb-c10f-43f4-b205-e767a8e24687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 182 ms (started: 2024-08-17 14:58:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAOpJJKEFYt4",
        "outputId": "331efdd1-fc96-4826-ba59-bf2185931ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  document={\n",
              "    x=[9500, 384],\n",
              "    y=[9500],\n",
              "  },\n",
              "  concept={ x=[38955, 384] },\n",
              "  (document, has, concept)={ edge_index=[2, 223512] },\n",
              "  (concept, rev_has, document)={ edge_index=[2, 223512] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.82 ms (started: 2024-08-17 14:58:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating all masks"
      ],
      "metadata": {
        "id": "AsFgg19dw8eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rotulated = 1"
      ],
      "metadata": {
        "id": "KIe8Dr1oxMFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotulated = 1"
      ],
      "metadata": {
        "id": "9Ap8LV_yxPlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b30dc1-20ac-48b8-83f9-039d5c273917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 377 µs (started: 2024-08-16 14:10:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ],
      "metadata": {
        "id": "cn4axcr5xPl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomNodeSplit to create the training, validation and testing masks\n",
        "# RandomNodeSplit performs a randomic separtion of the dataset and generates the masks for the Data object\n",
        "# by selecting validation and tesing sizes, RandomNodeSplit\n",
        "\n",
        "# RandomNodeSplit: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomNodeSplit.html#torch_geometric.transforms.RandomNodeSplit\n",
        "\n",
        "for i in range(10):\n",
        "    transform = T.RandomNodeSplit(split = 'test_rest', num_train_per_class = rotulated, num_val=0, num_test=1000)\n",
        "\n",
        "    data = transform(data)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([data['document'].train_mask, data['document'].val_mask,data['document'].test_mask], f)"
      ],
      "metadata": {
        "id": "oavHS83SxPl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e8ea71-c50b-44f4-ae90-a164d35e17d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.86 s (started: 2024-08-16 14:10:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rotulated = 5"
      ],
      "metadata": {
        "id": "NFek9qfWohIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotulated = 5"
      ],
      "metadata": {
        "id": "X8kSltuxohIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c6957f-b243-465d-9868-7fbd418dbbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 430 µs (started: 2024-08-16 14:10:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ],
      "metadata": {
        "id": "SeVGrrYdohIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomNodeSplit to create the training, validation and testing masks\n",
        "# RandomNodeSplit performs a randomic separtion of the dataset and generates the masks for the Data object\n",
        "# by selecting validation and tesing sizes, RandomNodeSplit\n",
        "\n",
        "# RandomNodeSplit: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomNodeSplit.html#torch_geometric.transforms.RandomNodeSplit\n",
        "\n",
        "for i in range(10):\n",
        "    transform = T.RandomNodeSplit(split = 'test_rest', num_train_per_class = rotulated, num_val=0, num_test=1000)\n",
        "\n",
        "    data = transform(data)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([data['document'].train_mask, data['document'].val_mask,data['document'].test_mask], f)"
      ],
      "metadata": {
        "id": "_Ha4l0u_ohIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63c5a6a-0a18-4ddf-9fac-341339729437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.17 s (started: 2024-08-16 14:10:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Rotulated = 10"
      ],
      "metadata": {
        "id": "BWZkHOPxowxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotulated = 10"
      ],
      "metadata": {
        "id": "XuX-yjKvowxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6217194f-04d0-4e93-dc22-0e87553152c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 593 µs (started: 2024-08-16 14:10:59 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ],
      "metadata": {
        "id": "gPgRPOGqowxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomNodeSplit to create the training, validation and testing masks\n",
        "# RandomNodeSplit performs a randomic separtion of the dataset and generates the masks for the Data object\n",
        "# by selecting validation and tesing sizes, RandomNodeSplit\n",
        "\n",
        "# RandomNodeSplit: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomNodeSplit.html#torch_geometric.transforms.RandomNodeSplit\n",
        "\n",
        "for i in range(10):\n",
        "    transform = T.RandomNodeSplit(split = 'test_rest', num_train_per_class = rotulated, num_val=0, num_test=1000)\n",
        "\n",
        "    data = transform(data)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([data['document'].train_mask, data['document'].val_mask,data['document'].test_mask], f)"
      ],
      "metadata": {
        "id": "lj2LFubyowxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd88cb6a-27b2-45c4-e370-3a4451e511ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.94 s (started: 2024-08-16 14:10:59 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rotulated = 20"
      ],
      "metadata": {
        "id": "dUJedOC0owxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotulated = 20"
      ],
      "metadata": {
        "id": "6q2Pbg8Eowxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85117db9-9f54-4dbb-ce1c-5c545323e328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 520 µs (started: 2024-08-16 14:11:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ],
      "metadata": {
        "id": "Q-JEan1uowxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomNodeSplit to create the training, validation and testing masks\n",
        "# RandomNodeSplit performs a randomic separtion of the dataset and generates the masks for the Data object\n",
        "# by selecting validation and tesing sizes, RandomNodeSplit\n",
        "\n",
        "# RandomNodeSplit: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomNodeSplit.html#torch_geometric.transforms.RandomNodeSplit\n",
        "\n",
        "for i in range(10):\n",
        "    transform = T.RandomNodeSplit(split = 'test_rest', num_train_per_class = rotulated, num_val=0, num_test=1000)\n",
        "\n",
        "    data = transform(data)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([data['document'].train_mask, data['document'].val_mask,data['document'].test_mask], f)"
      ],
      "metadata": {
        "id": "RlmrPl0Howxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a898d5a-7cd6-4037-c47d-480289a35240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.02 s (started: 2024-08-16 14:11:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Rotulated = 30"
      ],
      "metadata": {
        "id": "Hfodj4ygo4Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotulated = 30"
      ],
      "metadata": {
        "id": "FewBjlEfo4Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b248cac6-0814-423e-d7ab-0944eafa00d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 319 µs (started: 2024-08-16 14:11:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ],
      "metadata": {
        "id": "bWzx_X8lo4Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomNodeSplit to create the training, validation and testing masks\n",
        "# RandomNodeSplit performs a randomic separtion of the dataset and generates the masks for the Data object\n",
        "# by selecting validation and tesing sizes, RandomNodeSplit\n",
        "\n",
        "# RandomNodeSplit: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomNodeSplit.html#torch_geometric.transforms.RandomNodeSplit\n",
        "\n",
        "for i in range(10):\n",
        "    transform = T.RandomNodeSplit(split = 'test_rest', num_train_per_class = rotulated, num_val=0, num_test=1000)\n",
        "\n",
        "    data = transform(data)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([data['document'].train_mask, data['document'].val_mask,data['document'].test_mask], f)"
      ],
      "metadata": {
        "id": "F-NHcTkyo4Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370fcd18-a365-46d2-98f6-f89c160a229d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.17 s (started: 2024-08-16 14:11:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rotulated = 80%"
      ],
      "metadata": {
        "id": "n8LRhhipo4Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotulated = '80%'"
      ],
      "metadata": {
        "id": "h7lCwfK7o4Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fe1178-3f80-4e13-8d76-e6abcd1c1198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 281 µs (started: 2024-08-16 14:11:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ],
      "metadata": {
        "id": "m84LwF0Io4Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomNodeSplit to create the training, validation and testing masks\n",
        "# RandomNodeSplit performs a randomic separtion of the dataset and generates the masks for the Data object\n",
        "# by selecting validation and tesing sizes, RandomNodeSplit\n",
        "\n",
        "# RandomNodeSplit: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.RandomNodeSplit.html#torch_geometric.transforms.RandomNodeSplit\n",
        "\n",
        "def percentage(percent, whole):\n",
        "    return (percent * whole) / 100.0\n",
        "\n",
        "perc = int(percentage(20,len(df['id'])))\n",
        "\n",
        "print(\"20 % \"+str(len(df['id']))+' = '+str(perc))\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    transform = T.RandomNodeSplit(split = 'train_rest', num_val=0, num_test=perc)\n",
        "\n",
        "    data = transform(data)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([data['document'].train_mask, data['document'].val_mask,data['document'].test_mask], f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru88JX8Vo4Y_",
        "outputId": "7a7c509e-1414-4718-ae10-ea6999bad96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 % 10524 = 2104\n",
            "time: 4.34 s (started: 2024-08-16 14:11:11 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "8sApynSrs_hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 1 ❎"
      ],
      "metadata": {
        "id": "DnIjviAr5gWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "eyOMuSd28UWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZLP8R5Mq1TN",
        "outputId": "46e959ea-d693-4450-a2ba-cca5207c6dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4004, Train: 0.7500, Test: 0.4371\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0565, Train: 0.9375, Test: 0.5059\n",
            "Early stopping:  3.0715911755936705\n",
            "Epoch: 003, Loss: 0.1183, Train: 1.0000, Test: 0.5642\n",
            "Early stopping:  2.818088877542161\n",
            "Epoch: 004, Loss: 0.0080, Train: 0.9375, Test: 0.5950\n",
            "Early stopping:  2.5468750375848677\n",
            "Epoch: 005, Loss: 0.0601, Train: 1.0000, Test: 0.5684\n",
            "Early stopping:  2.316853496770235\n",
            "Epoch: 006, Loss: 0.0002, Train: 1.0000, Test: 0.5540\n",
            "Early stopping:  0.45408106572987206\n",
            "Epoch: 007, Loss: 0.0001, Train: 1.0000, Test: 0.5358\n",
            "Early stopping:  0.05171516836837455\n",
            "Epoch: 008, Loss: 0.0014, Train: 1.0000, Test: 0.5174\n",
            "Early stopping:  0.025999795842934426\n",
            "Epoch: 009, Loss: 0.0003, Train: 1.0000, Test: 0.4929\n",
            "Early stopping:  0.026662325958785243\n",
            "Epoch: 010, Loss: 0.0001, Train: 1.0000, Test: 0.4715\n",
            "Early stopping:  0.0005273933703170028\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  0, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.41      0.72      0.52       401\n",
            "             ecology       0.47      0.47      0.47       888\n",
            "            economic       0.53      0.85      0.66      1408\n",
            "          geophysics       1.00      0.38      0.55      1201\n",
            "  gravitional_theory       0.97      0.51      0.67       129\n",
            "               hydro       0.16      0.81      0.26       354\n",
            "                math       0.44      0.03      0.05      1338\n",
            "              metals       0.82      0.63      0.71       200\n",
            "          networking       0.89      0.82      0.85       344\n",
            "        neuroscience       1.00      0.61      0.76       306\n",
            "        oceanography       0.80      0.23      0.36       989\n",
            "             politic       0.68      0.56      0.62       602\n",
            "           sociology       0.71      0.18      0.29       738\n",
            "software_engineering       0.84      0.44      0.57       523\n",
            "          statistics       0.25      0.80      0.38       646\n",
            "    theory_computing       0.51      0.36      0.42       441\n",
            "\n",
            "            accuracy                           0.47     10508\n",
            "           macro avg       0.66      0.53      0.51     10508\n",
            "        weighted avg       0.63      0.47      0.46     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5157, Train: 0.8750, Test: 0.4762\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.3372, Train: 1.0000, Test: 0.5392\n",
            "Early stopping:  2.954666945912839\n",
            "Epoch: 003, Loss: 0.0643, Train: 1.0000, Test: 0.5551\n",
            "Early stopping:  2.494991662176913\n",
            "Epoch: 004, Loss: 0.0166, Train: 1.0000, Test: 0.5731\n",
            "Early stopping:  2.192724743009704\n",
            "Epoch: 005, Loss: 0.0002, Train: 1.0000, Test: 0.5697\n",
            "Early stopping:  1.9774203556497254\n",
            "Epoch: 006, Loss: 0.0000, Train: 1.0000, Test: 0.5601\n",
            "Early stopping:  0.14412873213353627\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.5501\n",
            "Early stopping:  0.027791446032120193\n",
            "Epoch: 008, Loss: 0.0000, Train: 1.0000, Test: 0.5367\n",
            "Early stopping:  0.007412898806406689\n",
            "PREDICTIONS -> tensor([14, 14, 13,  ...,  7, 15,  7], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.11      0.02      0.04       401\n",
            "             ecology       0.78      0.71      0.74       888\n",
            "            economic       0.64      0.16      0.25      1408\n",
            "          geophysics       0.95      0.77      0.85      1201\n",
            "  gravitional_theory       0.21      0.95      0.34       129\n",
            "               hydro       0.40      0.84      0.54       354\n",
            "                math       0.51      0.14      0.22      1338\n",
            "              metals       0.31      0.90      0.46       200\n",
            "          networking       0.74      0.83      0.78       344\n",
            "        neuroscience       0.81      1.00      0.89       306\n",
            "        oceanography       0.87      0.70      0.78       989\n",
            "             politic       0.53      0.17      0.26       602\n",
            "           sociology       0.31      0.92      0.47       738\n",
            "software_engineering       0.62      0.76      0.68       523\n",
            "          statistics       0.40      0.70      0.51       646\n",
            "    theory_computing       0.56      0.37      0.44       441\n",
            "\n",
            "            accuracy                           0.54     10508\n",
            "           macro avg       0.55      0.62      0.52     10508\n",
            "        weighted avg       0.61      0.54      0.51     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 7.2410, Train: 0.8750, Test: 0.4176\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6061, Train: 1.0000, Test: 0.5453\n",
            "Early stopping:  4.691594986603514\n",
            "Epoch: 003, Loss: 0.0594, Train: 1.0000, Test: 0.5817\n",
            "Early stopping:  3.997831915197736\n",
            "Epoch: 004, Loss: 0.0039, Train: 1.0000, Test: 0.5829\n",
            "Early stopping:  3.5194317465123777\n",
            "Epoch: 005, Loss: 0.0051, Train: 1.0000, Test: 0.5772\n",
            "Early stopping:  3.1730004404388\n",
            "Epoch: 006, Loss: 0.0027, Train: 1.0000, Test: 0.5735\n",
            "Early stopping:  0.26418512379617115\n",
            "Epoch: 007, Loss: 0.0005, Train: 1.0000, Test: 0.5668\n",
            "Early stopping:  0.025257065643762126\n",
            "Epoch: 008, Loss: 0.0002, Train: 1.0000, Test: 0.5627\n",
            "Early stopping:  0.0021345506773790207\n",
            "PREDICTIONS -> tensor([ 0, 11, 13,  ...,  8, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.93      0.17      0.28       401\n",
            "             ecology       0.79      0.74      0.76       888\n",
            "            economic       0.70      0.18      0.29      1408\n",
            "          geophysics       0.82      0.90      0.86      1201\n",
            "  gravitional_theory       0.83      0.91      0.87       129\n",
            "               hydro       0.37      0.31      0.34       354\n",
            "                math       0.89      0.68      0.77      1338\n",
            "              metals       0.61      0.60      0.61       200\n",
            "          networking       0.23      0.99      0.37       344\n",
            "        neuroscience       0.99      0.90      0.94       306\n",
            "        oceanography       0.62      0.76      0.68       989\n",
            "             politic       0.40      0.86      0.55       602\n",
            "           sociology       0.73      0.04      0.08       738\n",
            "software_engineering       0.61      0.85      0.71       523\n",
            "          statistics       0.26      0.24      0.25       646\n",
            "    theory_computing       0.13      0.18      0.15       441\n",
            "\n",
            "            accuracy                           0.56     10508\n",
            "           macro avg       0.62      0.58      0.53     10508\n",
            "        weighted avg       0.66      0.56      0.54     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.2682, Train: 0.8125, Test: 0.3674\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8256, Train: 1.0000, Test: 0.4659\n",
            "Early stopping:  3.141385303056498\n",
            "Epoch: 003, Loss: 0.0172, Train: 0.9375, Test: 0.3894\n",
            "Early stopping:  2.8273315367744583\n",
            "Epoch: 004, Loss: 0.9728, Train: 1.0000, Test: 0.4019\n",
            "Early stopping:  2.3690389415872635\n",
            "Epoch: 005, Loss: 0.0038, Train: 0.9375, Test: 0.4220\n",
            "Early stopping:  2.1985899901072625\n",
            "Epoch: 006, Loss: 0.1457, Train: 0.9375, Test: 0.4252\n",
            "Early stopping:  0.4682692570563143\n",
            "Epoch: 007, Loss: 0.2151, Train: 1.0000, Test: 0.4267\n",
            "Early stopping:  0.40221197758354404\n",
            "Epoch: 008, Loss: 0.0002, Train: 1.0000, Test: 0.4242\n",
            "Early stopping:  0.4049492330343406\n",
            "Epoch: 009, Loss: 0.0001, Train: 1.0000, Test: 0.4219\n",
            "Early stopping:  0.10109516106230555\n",
            "Epoch: 010, Loss: 0.0003, Train: 1.0000, Test: 0.4156\n",
            "Early stopping:  0.1017088916446754\n",
            "Epoch: 011, Loss: 0.0006, Train: 1.0000, Test: 0.4136\n",
            "Early stopping:  0.09606469095988257\n",
            "Epoch: 012, Loss: 0.0002, Train: 1.0000, Test: 0.4117\n",
            "Early stopping:  0.00021313440328249195\n",
            "PREDICTIONS -> tensor([15,  0, 15,  ..., 15, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.10      0.55      0.17       401\n",
            "             ecology       0.84      0.60      0.70       888\n",
            "            economic       0.66      0.14      0.24      1408\n",
            "          geophysics       0.99      0.41      0.58      1201\n",
            "  gravitional_theory       0.25      0.97      0.39       129\n",
            "               hydro       0.87      0.28      0.42       354\n",
            "                math       0.88      0.02      0.04      1338\n",
            "              metals       0.72      0.66      0.69       200\n",
            "          networking       0.74      0.44      0.55       344\n",
            "        neuroscience       0.96      0.95      0.96       306\n",
            "        oceanography       0.51      0.87      0.64       989\n",
            "             politic       0.45      0.11      0.17       602\n",
            "           sociology       0.44      0.47      0.45       738\n",
            "software_engineering       0.90      0.09      0.16       523\n",
            "          statistics       0.25      0.80      0.38       646\n",
            "    theory_computing       0.30      0.47      0.37       441\n",
            "\n",
            "            accuracy                           0.41     10508\n",
            "           macro avg       0.62      0.49      0.43     10508\n",
            "        weighted avg       0.66      0.41      0.40     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.3564, Train: 0.9375, Test: 0.4346\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2609, Train: 1.0000, Test: 0.4329\n",
            "Early stopping:  2.895985951951394\n",
            "Epoch: 003, Loss: 0.0287, Train: 1.0000, Test: 0.4448\n",
            "Early stopping:  2.434350021768808\n",
            "Epoch: 004, Loss: 0.0012, Train: 1.0000, Test: 0.4546\n",
            "Early stopping:  2.1329246327038627\n",
            "Epoch: 005, Loss: 0.0002, Train: 1.0000, Test: 0.4608\n",
            "Early stopping:  1.9188293599222404\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.4647\n",
            "Early stopping:  0.11395041500224334\n",
            "Epoch: 007, Loss: 0.0001, Train: 1.0000, Test: 0.4675\n",
            "Early stopping:  0.012678471743870287\n",
            "Epoch: 008, Loss: 0.0001, Train: 1.0000, Test: 0.4703\n",
            "Early stopping:  0.0004949433535035474\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.36      0.52      0.43       401\n",
            "             ecology       0.81      0.43      0.56       888\n",
            "            economic       0.43      0.39      0.41      1408\n",
            "          geophysics       0.48      0.97      0.64      1201\n",
            "  gravitional_theory       0.94      0.78      0.85       129\n",
            "               hydro       0.46      0.03      0.06       354\n",
            "                math       0.61      0.41      0.49      1338\n",
            "              metals       0.77      0.60      0.68       200\n",
            "          networking       0.62      0.91      0.74       344\n",
            "        neuroscience       0.82      0.97      0.89       306\n",
            "        oceanography       0.52      0.91      0.66       989\n",
            "             politic       0.09      0.08      0.09       602\n",
            "           sociology       0.05      0.02      0.03       738\n",
            "software_engineering       0.67      0.23      0.34       523\n",
            "          statistics       0.75      0.19      0.30       646\n",
            "    theory_computing       0.05      0.09      0.07       441\n",
            "\n",
            "            accuracy                           0.47     10508\n",
            "           macro avg       0.53      0.47      0.45     10508\n",
            "        weighted avg       0.50      0.47      0.44     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.8865, Train: 0.6250, Test: 0.3484\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.4454, Train: 0.9375, Test: 0.5821\n",
            "Early stopping:  1.7261414609715546\n",
            "Epoch: 003, Loss: 0.1226, Train: 0.9375, Test: 0.5314\n",
            "Early stopping:  1.9094312353890621\n",
            "Epoch: 004, Loss: 0.1735, Train: 1.0000, Test: 0.4951\n",
            "Early stopping:  1.7626286206740598\n",
            "Epoch: 005, Loss: 0.0111, Train: 1.0000, Test: 0.4902\n",
            "Early stopping:  1.649193821725932\n",
            "Epoch: 006, Loss: 0.0058, Train: 0.9375, Test: 0.4885\n",
            "Early stopping:  0.6156289043326945\n",
            "Epoch: 007, Loss: 0.0573, Train: 1.0000, Test: 0.4931\n",
            "Early stopping:  0.0727132574372901\n",
            "Epoch: 008, Loss: 0.0051, Train: 1.0000, Test: 0.4992\n",
            "Early stopping:  0.07208657293705531\n",
            "Epoch: 009, Loss: 0.0015, Train: 1.0000, Test: 0.5040\n",
            "Early stopping:  0.02324083427437178\n",
            "Epoch: 010, Loss: 0.0006, Train: 1.0000, Test: 0.5069\n",
            "Early stopping:  0.024265752525061725\n",
            "Epoch: 011, Loss: 0.0004, Train: 1.0000, Test: 0.5073\n",
            "Early stopping:  0.024839454353003713\n",
            "Epoch: 012, Loss: 0.0004, Train: 1.0000, Test: 0.5076\n",
            "Early stopping:  0.0019974982771517437\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.31      0.43       401\n",
            "             ecology       0.53      0.80      0.64       888\n",
            "            economic       0.44      0.22      0.29      1408\n",
            "          geophysics       0.85      0.84      0.84      1201\n",
            "  gravitional_theory       0.89      0.91      0.90       129\n",
            "               hydro       0.32      0.83      0.46       354\n",
            "                math       0.27      0.03      0.05      1338\n",
            "              metals       0.74      0.21      0.33       200\n",
            "          networking       0.79      0.89      0.84       344\n",
            "        neuroscience       0.46      0.98      0.63       306\n",
            "        oceanography       0.85      0.76      0.80       989\n",
            "             politic       0.61      0.45      0.52       602\n",
            "           sociology       0.27      0.01      0.02       738\n",
            "software_engineering       0.77      0.62      0.69       523\n",
            "          statistics       0.24      0.58      0.34       646\n",
            "    theory_computing       0.24      0.81      0.37       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.56      0.58      0.51     10508\n",
            "        weighted avg       0.53      0.51      0.47     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4485, Train: 0.9375, Test: 0.4226\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2910, Train: 1.0000, Test: 0.4736\n",
            "Early stopping:  2.2326865840336456\n",
            "Epoch: 003, Loss: 0.0022, Train: 1.0000, Test: 0.5310\n",
            "Early stopping:  1.9118027023996471\n",
            "Epoch: 004, Loss: 0.0002, Train: 1.0000, Test: 0.5385\n",
            "Early stopping:  1.680914010793882\n",
            "Epoch: 005, Loss: 0.0001, Train: 1.0000, Test: 0.5351\n",
            "Early stopping:  1.5146284190564308\n",
            "Epoch: 006, Loss: 0.0002, Train: 1.0000, Test: 0.5188\n",
            "Early stopping:  0.12985032900044982\n",
            "Epoch: 007, Loss: 0.0003, Train: 1.0000, Test: 0.5083\n",
            "Early stopping:  0.000921871935753247\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.36      0.47       401\n",
            "             ecology       0.53      0.93      0.68       888\n",
            "            economic       0.78      0.61      0.68      1408\n",
            "          geophysics       0.54      0.77      0.63      1201\n",
            "  gravitional_theory       0.26      0.96      0.41       129\n",
            "               hydro       1.00      0.18      0.30       354\n",
            "                math       0.79      0.03      0.05      1338\n",
            "              metals       0.89      0.35      0.51       200\n",
            "          networking       0.38      0.78      0.51       344\n",
            "        neuroscience       0.98      0.88      0.92       306\n",
            "        oceanography       0.84      0.48      0.61       989\n",
            "             politic       0.40      0.20      0.26       602\n",
            "           sociology       0.72      0.17      0.28       738\n",
            "software_engineering       0.45      0.95      0.61       523\n",
            "          statistics       0.22      0.66      0.33       646\n",
            "    theory_computing       0.78      0.25      0.38       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.64      0.54      0.48     10508\n",
            "        weighted avg       0.65      0.51      0.47     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.2988, Train: 0.9375, Test: 0.4375\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2879, Train: 1.0000, Test: 0.4895\n",
            "Early stopping:  2.8361093994116646\n",
            "Epoch: 003, Loss: 0.0043, Train: 1.0000, Test: 0.4715\n",
            "Early stopping:  2.4017443707691757\n",
            "Epoch: 004, Loss: 0.0002, Train: 1.0000, Test: 0.4759\n",
            "Early stopping:  2.1049797351170256\n",
            "Epoch: 005, Loss: 0.0000, Train: 1.0000, Test: 0.4702\n",
            "Early stopping:  1.8938516652589257\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.4635\n",
            "Early stopping:  0.12826774792754214\n",
            "Epoch: 007, Loss: 0.0029, Train: 1.0000, Test: 0.4645\n",
            "Early stopping:  0.001984532688079483\n",
            "PREDICTIONS -> tensor([10,  0,  0,  ..., 15, 10, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.89      0.27      0.41       401\n",
            "             ecology       0.21      0.03      0.05       888\n",
            "            economic       0.56      0.02      0.03      1408\n",
            "          geophysics       0.99      0.54      0.70      1201\n",
            "  gravitional_theory       0.97      0.67      0.79       129\n",
            "               hydro       0.44      0.56      0.50       354\n",
            "                math       0.63      0.73      0.68      1338\n",
            "              metals       0.72      0.85      0.78       200\n",
            "          networking       0.34      0.93      0.50       344\n",
            "        neuroscience       0.98      0.83      0.90       306\n",
            "        oceanography       0.26      0.87      0.40       989\n",
            "             politic       0.72      0.72      0.72       602\n",
            "           sociology       0.42      0.72      0.53       738\n",
            "software_engineering       0.62      0.06      0.11       523\n",
            "          statistics       0.16      0.15      0.15       646\n",
            "    theory_computing       0.82      0.23      0.36       441\n",
            "\n",
            "            accuracy                           0.46     10508\n",
            "           macro avg       0.61      0.51      0.48     10508\n",
            "        weighted avg       0.57      0.46      0.42     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5866, Train: 0.6875, Test: 0.4816\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8926, Train: 1.0000, Test: 0.5134\n",
            "Early stopping:  2.612093294045411\n",
            "Epoch: 003, Loss: 0.0841, Train: 1.0000, Test: 0.5521\n",
            "Early stopping:  2.400456325306715\n",
            "Epoch: 004, Loss: 0.0043, Train: 1.0000, Test: 0.5664\n",
            "Early stopping:  2.167296812488838\n",
            "Epoch: 005, Loss: 0.0018, Train: 1.0000, Test: 0.5458\n",
            "Early stopping:  1.9772090041660255\n",
            "Epoch: 006, Loss: 0.0030, Train: 1.0000, Test: 0.5334\n",
            "Early stopping:  0.3903449143069442\n",
            "Epoch: 007, Loss: 0.0034, Train: 1.0000, Test: 0.5316\n",
            "Early stopping:  0.03620857919362709\n",
            "Epoch: 008, Loss: 0.0017, Train: 1.0000, Test: 0.5326\n",
            "Early stopping:  0.0010979114672187784\n",
            "PREDICTIONS -> tensor([ 0,  0, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.60      0.69       401\n",
            "             ecology       0.76      0.27      0.40       888\n",
            "            economic       1.00      0.02      0.04      1408\n",
            "          geophysics       0.89      0.90      0.89      1201\n",
            "  gravitional_theory       0.43      0.96      0.59       129\n",
            "               hydro       0.26      0.08      0.12       354\n",
            "                math       0.71      0.61      0.66      1338\n",
            "              metals       0.20      0.84      0.32       200\n",
            "          networking       0.23      0.95      0.38       344\n",
            "        neuroscience       0.96      0.87      0.91       306\n",
            "        oceanography       0.47      0.91      0.61       989\n",
            "             politic       0.72      0.65      0.68       602\n",
            "           sociology       0.30      0.29      0.29       738\n",
            "software_engineering       0.88      0.28      0.43       523\n",
            "          statistics       0.54      0.48      0.51       646\n",
            "    theory_computing       0.48      0.71      0.57       441\n",
            "\n",
            "            accuracy                           0.53     10508\n",
            "           macro avg       0.60      0.59      0.51     10508\n",
            "        weighted avg       0.68      0.53      0.50     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7443, Train: 0.8750, Test: 0.4424\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.4867, Train: 1.0000, Test: 0.5079\n",
            "Early stopping:  2.303451922400134\n",
            "Epoch: 003, Loss: 0.0082, Train: 0.9375, Test: 0.4579\n",
            "Early stopping:  2.0330357740033054\n",
            "Epoch: 004, Loss: 0.2845, Train: 1.0000, Test: 0.3795\n",
            "Early stopping:  1.7532508886127742\n",
            "Epoch: 005, Loss: 0.0006, Train: 1.0000, Test: 0.3259\n",
            "Early stopping:  1.6002942003942422\n",
            "Epoch: 006, Loss: 0.0005, Train: 1.0000, Test: 0.3028\n",
            "Early stopping:  0.22139796412881382\n",
            "Epoch: 007, Loss: 0.0037, Train: 0.9375, Test: 0.2970\n",
            "Early stopping:  0.1258304240024385\n",
            "Epoch: 008, Loss: 0.1994, Train: 1.0000, Test: 0.2858\n",
            "Early stopping:  0.13505942719859781\n",
            "Epoch: 009, Loss: 0.0007, Train: 1.0000, Test: 0.2777\n",
            "Early stopping:  0.08858243199784864\n",
            "Epoch: 010, Loss: 0.0003, Train: 1.0000, Test: 0.2740\n",
            "Early stopping:  0.08862073226962218\n",
            "Epoch: 011, Loss: 0.0003, Train: 1.0000, Test: 0.2717\n",
            "Early stopping:  0.08864798697755409\n",
            "Epoch: 012, Loss: 0.0002, Train: 1.0000, Test: 0.2697\n",
            "Early stopping:  0.08902916625064146\n",
            "Epoch: 013, Loss: 0.0001, Train: 1.0000, Test: 0.2689\n",
            "Early stopping:  0.00025103028230517475\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.95      0.37      0.53       401\n",
            "             ecology       0.16      0.13      0.15       888\n",
            "            economic       0.49      0.10      0.16      1408\n",
            "          geophysics       1.00      0.01      0.02      1201\n",
            "  gravitional_theory       0.65      0.80      0.72       129\n",
            "               hydro       0.07      0.93      0.14       354\n",
            "                math       1.00      0.01      0.03      1338\n",
            "              metals       0.64      0.74      0.69       200\n",
            "          networking       0.87      0.69      0.77       344\n",
            "        neuroscience       0.95      0.95      0.95       306\n",
            "        oceanography       0.23      0.14      0.17       989\n",
            "             politic       0.89      0.37      0.52       602\n",
            "           sociology       0.41      0.36      0.39       738\n",
            "software_engineering       0.60      0.78      0.68       523\n",
            "          statistics       0.35      0.26      0.30       646\n",
            "    theory_computing       0.07      0.17      0.10       441\n",
            "\n",
            "            accuracy                           0.27     10508\n",
            "           macro avg       0.58      0.43      0.39     10508\n",
            "        weighted avg       0.59      0.27      0.27     10508\n",
            "\n",
            "time: 5.47 s (started: 2024-08-16 14:11:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "yFtYdJpWa09V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706444c8-4146-4f4d-e9a6-5b7331790e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 434 ms (started: 2024-08-16 14:11:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "tkaPhP3p8NBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ea47b8-9aa2-4033-b257-9c8bd9ebe189",
        "id": "dLzI_TAc8i6r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7853, Train: 0.9375, Test: 0.3421\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4027, Train: 0.9375, Test: 0.4913\n",
            "Early stopping:  0.27057770981811835\n",
            "Epoch: 003, Loss: 1.9648, Train: 1.0000, Test: 0.5223\n",
            "Early stopping:  0.4106037110549708\n",
            "Epoch: 004, Loss: 1.4785, Train: 1.0000, Test: 0.5317\n",
            "Early stopping:  0.5634700178713508\n",
            "Epoch: 005, Loss: 1.0030, Train: 1.0000, Test: 0.5498\n",
            "Early stopping:  0.710515949745173\n",
            "Epoch: 006, Loss: 0.6062, Train: 1.0000, Test: 0.5672\n",
            "Early stopping:  0.7205575660204593\n",
            "Epoch: 007, Loss: 0.3269, Train: 1.0000, Test: 0.5837\n",
            "Early stopping:  0.6593910279309918\n",
            "Epoch: 008, Loss: 0.1638, Train: 1.0000, Test: 0.5913\n",
            "Early stopping:  0.5320258563654219\n",
            "Epoch: 009, Loss: 0.0816, Train: 1.0000, Test: 0.5932\n",
            "Early stopping:  0.3748715333221112\n",
            "Epoch: 010, Loss: 0.0421, Train: 1.0000, Test: 0.5933\n",
            "Early stopping:  0.23001054749166383\n",
            "Epoch: 011, Loss: 0.0228, Train: 1.0000, Test: 0.5939\n",
            "Early stopping:  0.12391699139973963\n",
            "Epoch: 012, Loss: 0.0130, Train: 1.0000, Test: 0.5935\n",
            "Early stopping:  0.06132956308901338\n",
            "Epoch: 013, Loss: 0.0077, Train: 1.0000, Test: 0.5939\n",
            "Early stopping:  0.029932496212307973\n",
            "Epoch: 014, Loss: 0.0047, Train: 1.0000, Test: 0.5937\n",
            "Early stopping:  0.015105939173776185\n",
            "Epoch: 015, Loss: 0.0029, Train: 1.0000, Test: 0.5945\n",
            "Early stopping:  0.008014457714767848\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.39      0.61      0.47       401\n",
            "             ecology       0.42      0.28      0.33       888\n",
            "            economic       0.69      0.43      0.53      1408\n",
            "          geophysics       0.94      0.85      0.89      1201\n",
            "  gravitional_theory       0.73      0.82      0.77       129\n",
            "               hydro       0.18      0.26      0.21       354\n",
            "                math       0.76      0.57      0.65      1338\n",
            "              metals       0.55      0.91      0.69       200\n",
            "          networking       0.72      0.92      0.81       344\n",
            "        neuroscience       0.89      0.87      0.88       306\n",
            "        oceanography       0.49      0.56      0.52       989\n",
            "             politic       0.69      0.77      0.73       602\n",
            "           sociology       0.46      0.56      0.51       738\n",
            "software_engineering       0.68      0.41      0.51       523\n",
            "          statistics       0.42      0.75      0.54       646\n",
            "    theory_computing       0.61      0.60      0.61       441\n",
            "\n",
            "            accuracy                           0.59     10508\n",
            "           macro avg       0.60      0.64      0.60     10508\n",
            "        weighted avg       0.63      0.59      0.60     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7792, Train: 1.0000, Test: 0.3795\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3616, Train: 1.0000, Test: 0.5226\n",
            "Early stopping:  0.2952270412886237\n",
            "Epoch: 003, Loss: 1.8963, Train: 1.0000, Test: 0.5353\n",
            "Early stopping:  0.4416304678050251\n",
            "Epoch: 004, Loss: 1.3849, Train: 1.0000, Test: 0.5359\n",
            "Early stopping:  0.6006847149430778\n",
            "Epoch: 005, Loss: 0.8991, Train: 1.0000, Test: 0.5368\n",
            "Early stopping:  0.7494547700623535\n",
            "Epoch: 006, Loss: 0.5083, Train: 1.0000, Test: 0.5368\n",
            "Early stopping:  0.7444363665024554\n",
            "Epoch: 007, Loss: 0.2497, Train: 1.0000, Test: 0.5384\n",
            "Early stopping:  0.6643908698708113\n",
            "Epoch: 008, Loss: 0.1106, Train: 1.0000, Test: 0.5368\n",
            "Early stopping:  0.5175685797566943\n",
            "Epoch: 009, Loss: 0.0473, Train: 1.0000, Test: 0.5311\n",
            "Early stopping:  0.34813656384055913\n",
            "Epoch: 010, Loss: 0.0207, Train: 1.0000, Test: 0.5265\n",
            "Early stopping:  0.2001272466535671\n",
            "Epoch: 011, Loss: 0.0096, Train: 1.0000, Test: 0.5244\n",
            "Early stopping:  0.09872326488834211\n",
            "Epoch: 012, Loss: 0.0048, Train: 1.0000, Test: 0.5211\n",
            "Early stopping:  0.043490982841074055\n",
            "Epoch: 013, Loss: 0.0025, Train: 1.0000, Test: 0.5198\n",
            "Early stopping:  0.018337153907671995\n",
            "Epoch: 014, Loss: 0.0014, Train: 1.0000, Test: 0.5162\n",
            "Early stopping:  0.007864713931684312\n",
            "PREDICTIONS -> tensor([15, 14, 15,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.25      0.19      0.21       401\n",
            "             ecology       0.87      0.67      0.76       888\n",
            "            economic       0.45      0.12      0.18      1408\n",
            "          geophysics       0.95      0.70      0.81      1201\n",
            "  gravitional_theory       0.15      0.98      0.26       129\n",
            "               hydro       0.62      0.69      0.65       354\n",
            "                math       0.58      0.19      0.28      1338\n",
            "              metals       0.32      0.90      0.48       200\n",
            "          networking       0.60      0.76      0.67       344\n",
            "        neuroscience       0.91      0.95      0.93       306\n",
            "        oceanography       0.75      0.90      0.82       989\n",
            "             politic       0.25      0.89      0.39       602\n",
            "           sociology       0.62      0.11      0.19       738\n",
            "software_engineering       0.86      0.46      0.60       523\n",
            "          statistics       0.47      0.67      0.55       646\n",
            "    theory_computing       0.37      0.50      0.42       441\n",
            "\n",
            "            accuracy                           0.52     10508\n",
            "           macro avg       0.56      0.60      0.51     10508\n",
            "        weighted avg       0.61      0.52      0.50     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7675, Train: 0.9375, Test: 0.3061\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3603, Train: 1.0000, Test: 0.4361\n",
            "Early stopping:  0.2879248469030202\n",
            "Epoch: 003, Loss: 1.9007, Train: 1.0000, Test: 0.4969\n",
            "Early stopping:  0.433671965867638\n",
            "Epoch: 004, Loss: 1.3967, Train: 1.0000, Test: 0.5399\n",
            "Early stopping:  0.5909171061359679\n",
            "Epoch: 005, Loss: 0.9141, Train: 1.0000, Test: 0.5668\n",
            "Early stopping:  0.7390144072234003\n",
            "Epoch: 006, Loss: 0.5219, Train: 1.0000, Test: 0.5838\n",
            "Early stopping:  0.7379577533642273\n",
            "Epoch: 007, Loss: 0.2577, Train: 1.0000, Test: 0.5875\n",
            "Early stopping:  0.6625002583595125\n",
            "Epoch: 008, Loss: 0.1138, Train: 1.0000, Test: 0.5861\n",
            "Early stopping:  0.5207552097882451\n",
            "Epoch: 009, Loss: 0.0487, Train: 1.0000, Test: 0.5873\n",
            "Early stopping:  0.35378351669798763\n",
            "Epoch: 010, Loss: 0.0216, Train: 1.0000, Test: 0.5854\n",
            "Early stopping:  0.20544726542183775\n",
            "Epoch: 011, Loss: 0.0102, Train: 1.0000, Test: 0.5844\n",
            "Early stopping:  0.10178041168427478\n",
            "Epoch: 012, Loss: 0.0052, Train: 1.0000, Test: 0.5829\n",
            "Early stopping:  0.04462010400435911\n",
            "Epoch: 013, Loss: 0.0028, Train: 1.0000, Test: 0.5810\n",
            "Early stopping:  0.0187788183420983\n",
            "Epoch: 014, Loss: 0.0016, Train: 1.0000, Test: 0.5805\n",
            "Early stopping:  0.008135046841579207\n",
            "PREDICTIONS -> tensor([ 0, 11, 13,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.63      0.33      0.43       401\n",
            "             ecology       0.80      0.60      0.68       888\n",
            "            economic       0.76      0.36      0.49      1408\n",
            "          geophysics       0.92      0.80      0.86      1201\n",
            "  gravitional_theory       0.26      0.97      0.41       129\n",
            "               hydro       0.16      0.48      0.24       354\n",
            "                math       0.95      0.37      0.53      1338\n",
            "              metals       0.53      0.64      0.58       200\n",
            "          networking       0.79      0.84      0.82       344\n",
            "        neuroscience       0.94      0.93      0.93       306\n",
            "        oceanography       0.63      0.83      0.71       989\n",
            "             politic       0.43      0.80      0.56       602\n",
            "           sociology       0.67      0.44      0.54       738\n",
            "software_engineering       0.59      0.89      0.71       523\n",
            "          statistics       0.29      0.35      0.32       646\n",
            "    theory_computing       0.31      0.35      0.33       441\n",
            "\n",
            "            accuracy                           0.58     10508\n",
            "           macro avg       0.61      0.62      0.57     10508\n",
            "        weighted avg       0.68      0.58      0.59     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7733, Train: 1.0000, Test: 0.3117\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3670, Train: 1.0000, Test: 0.5007\n",
            "Early stopping:  0.2873560330355172\n",
            "Epoch: 003, Loss: 1.8988, Train: 1.0000, Test: 0.5347\n",
            "Early stopping:  0.4376543698471149\n",
            "Epoch: 004, Loss: 1.3797, Train: 1.0000, Test: 0.5431\n",
            "Early stopping:  0.6010830757374269\n",
            "Epoch: 005, Loss: 0.8902, Train: 1.0000, Test: 0.5443\n",
            "Early stopping:  0.752329733738884\n",
            "Epoch: 006, Loss: 0.5046, Train: 1.0000, Test: 0.5428\n",
            "Early stopping:  0.7492484739908056\n",
            "Epoch: 007, Loss: 0.2534, Train: 1.0000, Test: 0.5354\n",
            "Early stopping:  0.664414894079923\n",
            "Epoch: 008, Loss: 0.1169, Train: 1.0000, Test: 0.5265\n",
            "Early stopping:  0.5124763964403322\n",
            "Epoch: 009, Loss: 0.0531, Train: 1.0000, Test: 0.5178\n",
            "Early stopping:  0.3415320168869628\n",
            "Epoch: 010, Loss: 0.0253, Train: 1.0000, Test: 0.5139\n",
            "Early stopping:  0.19637016747194982\n",
            "Epoch: 011, Loss: 0.0129, Train: 1.0000, Test: 0.5123\n",
            "Early stopping:  0.09859623719938762\n",
            "Epoch: 012, Loss: 0.0071, Train: 1.0000, Test: 0.5100\n",
            "Early stopping:  0.04493859269234892\n",
            "Epoch: 013, Loss: 0.0041, Train: 1.0000, Test: 0.5078\n",
            "Early stopping:  0.019933571093989716\n",
            "Epoch: 014, Loss: 0.0025, Train: 1.0000, Test: 0.5061\n",
            "Early stopping:  0.009223259698939749\n",
            "PREDICTIONS -> tensor([15,  0, 15,  ..., 14, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.19      0.28      0.22       401\n",
            "             ecology       0.54      0.67      0.60       888\n",
            "            economic       0.84      0.56      0.68      1408\n",
            "          geophysics       0.93      0.66      0.78      1201\n",
            "  gravitional_theory       0.18      0.94      0.30       129\n",
            "               hydro       0.58      0.61      0.59       354\n",
            "                math       0.75      0.13      0.22      1338\n",
            "              metals       0.71      0.76      0.73       200\n",
            "          networking       0.86      0.21      0.34       344\n",
            "        neuroscience       0.90      0.94      0.92       306\n",
            "        oceanography       0.48      0.48      0.48       989\n",
            "             politic       0.49      0.73      0.59       602\n",
            "           sociology       0.34      0.34      0.34       738\n",
            "software_engineering       0.88      0.40      0.55       523\n",
            "          statistics       0.31      0.83      0.45       646\n",
            "    theory_computing       0.16      0.18      0.17       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.57      0.55      0.50     10508\n",
            "        weighted avg       0.62      0.51      0.51     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7824, Train: 1.0000, Test: 0.3770\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3497, Train: 1.0000, Test: 0.4681\n",
            "Early stopping:  0.30599792130667375\n",
            "Epoch: 003, Loss: 1.8552, Train: 1.0000, Test: 0.4910\n",
            "Early stopping:  0.4639429345015002\n",
            "Epoch: 004, Loss: 1.3236, Train: 1.0000, Test: 0.5010\n",
            "Early stopping:  0.6294830186449907\n",
            "Epoch: 005, Loss: 0.8366, Train: 1.0000, Test: 0.5053\n",
            "Early stopping:  0.7779846915841462\n",
            "Epoch: 006, Loss: 0.4626, Train: 1.0000, Test: 0.5092\n",
            "Early stopping:  0.7591479655742444\n",
            "Epoch: 007, Loss: 0.2248, Train: 1.0000, Test: 0.5126\n",
            "Early stopping:  0.6585855900417876\n",
            "Epoch: 008, Loss: 0.0988, Train: 1.0000, Test: 0.5168\n",
            "Early stopping:  0.49747610808684006\n",
            "Epoch: 009, Loss: 0.0415, Train: 1.0000, Test: 0.5201\n",
            "Early stopping:  0.3247869980938671\n",
            "Epoch: 010, Loss: 0.0178, Train: 1.0000, Test: 0.5219\n",
            "Early stopping:  0.18257882482461682\n",
            "Epoch: 011, Loss: 0.0081, Train: 1.0000, Test: 0.5216\n",
            "Early stopping:  0.08921137137917087\n",
            "Epoch: 012, Loss: 0.0039, Train: 1.0000, Test: 0.5214\n",
            "Early stopping:  0.039025270173300854\n",
            "Epoch: 013, Loss: 0.0021, Train: 1.0000, Test: 0.5206\n",
            "Early stopping:  0.01618580076375613\n",
            "Epoch: 014, Loss: 0.0012, Train: 1.0000, Test: 0.5178\n",
            "Early stopping:  0.006774697691111465\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.38      0.80      0.51       401\n",
            "             ecology       0.82      0.63      0.71       888\n",
            "            economic       0.33      0.19      0.24      1408\n",
            "          geophysics       0.93      0.74      0.83      1201\n",
            "  gravitional_theory       0.38      0.95      0.54       129\n",
            "               hydro       0.34      0.38      0.35       354\n",
            "                math       0.69      0.47      0.56      1338\n",
            "              metals       0.59      0.85      0.70       200\n",
            "          networking       0.61      0.92      0.73       344\n",
            "        neuroscience       0.81      0.97      0.89       306\n",
            "        oceanography       0.61      0.86      0.71       989\n",
            "             politic       0.21      0.50      0.30       602\n",
            "           sociology       0.16      0.09      0.11       738\n",
            "software_engineering       0.48      0.24      0.32       523\n",
            "          statistics       0.59      0.55      0.57       646\n",
            "    theory_computing       0.12      0.10      0.11       441\n",
            "\n",
            "            accuracy                           0.52     10508\n",
            "           macro avg       0.50      0.58      0.51     10508\n",
            "        weighted avg       0.54      0.52      0.51     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7827, Train: 0.9375, Test: 0.3829\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4436, Train: 0.9375, Test: 0.4474\n",
            "Early stopping:  0.23974627860803513\n",
            "Epoch: 003, Loss: 2.0271, Train: 1.0000, Test: 0.4781\n",
            "Early stopping:  0.37846063191827134\n",
            "Epoch: 004, Loss: 1.5478, Train: 1.0000, Test: 0.5132\n",
            "Early stopping:  0.5335624411521803\n",
            "Epoch: 005, Loss: 1.0700, Train: 1.0000, Test: 0.5428\n",
            "Early stopping:  0.6848455349867127\n",
            "Epoch: 006, Loss: 0.6562, Train: 1.0000, Test: 0.5662\n",
            "Early stopping:  0.7168479200202865\n",
            "Epoch: 007, Loss: 0.3558, Train: 1.0000, Test: 0.5823\n",
            "Early stopping:  0.6721053859387419\n",
            "Epoch: 008, Loss: 0.1756, Train: 1.0000, Test: 0.5968\n",
            "Early stopping:  0.5550951624810418\n",
            "Epoch: 009, Loss: 0.0852, Train: 1.0000, Test: 0.6013\n",
            "Early stopping:  0.4007736892595341\n",
            "Epoch: 010, Loss: 0.0436, Train: 1.0000, Test: 0.6023\n",
            "Early stopping:  0.2502851900356071\n",
            "Epoch: 011, Loss: 0.0235, Train: 1.0000, Test: 0.5976\n",
            "Early stopping:  0.1356805158016021\n",
            "Epoch: 012, Loss: 0.0129, Train: 1.0000, Test: 0.5911\n",
            "Early stopping:  0.06611069033343411\n",
            "Epoch: 013, Loss: 0.0073, Train: 1.0000, Test: 0.5846\n",
            "Early stopping:  0.03153902580616958\n",
            "Epoch: 014, Loss: 0.0043, Train: 1.0000, Test: 0.5790\n",
            "Early stopping:  0.015910567980964\n",
            "Epoch: 015, Loss: 0.0026, Train: 1.0000, Test: 0.5738\n",
            "Early stopping:  0.008454518540954555\n",
            "PREDICTIONS -> tensor([13,  0,  0,  ...,  0, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.48      0.28      0.36       401\n",
            "             ecology       0.52      0.96      0.67       888\n",
            "            economic       0.69      0.27      0.38      1408\n",
            "          geophysics       0.89      0.86      0.87      1201\n",
            "  gravitional_theory       0.86      0.93      0.89       129\n",
            "               hydro       0.37      0.41      0.39       354\n",
            "                math       0.78      0.55      0.65      1338\n",
            "              metals       0.43      0.27      0.33       200\n",
            "          networking       0.61      0.63      0.62       344\n",
            "        neuroscience       0.91      0.90      0.91       306\n",
            "        oceanography       0.92      0.60      0.73       989\n",
            "             politic       0.36      0.78      0.49       602\n",
            "           sociology       0.13      0.10      0.11       738\n",
            "software_engineering       0.55      0.79      0.65       523\n",
            "          statistics       0.49      0.49      0.49       646\n",
            "    theory_computing       0.32      0.55      0.40       441\n",
            "\n",
            "            accuracy                           0.57     10508\n",
            "           macro avg       0.58      0.59      0.56     10508\n",
            "        weighted avg       0.62      0.57      0.56     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7844, Train: 1.0000, Test: 0.4411\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3794, Train: 1.0000, Test: 0.5315\n",
            "Early stopping:  0.2864289709556716\n",
            "Epoch: 003, Loss: 1.9061, Train: 1.0000, Test: 0.5406\n",
            "Early stopping:  0.43961127176458753\n",
            "Epoch: 004, Loss: 1.3835, Train: 1.0000, Test: 0.5493\n",
            "Early stopping:  0.6046534575478513\n",
            "Epoch: 005, Loss: 0.8864, Train: 1.0000, Test: 0.5548\n",
            "Early stopping:  0.7584552987938366\n",
            "Epoch: 006, Loss: 0.4917, Train: 1.0000, Test: 0.5620\n",
            "Early stopping:  0.7589183132672331\n",
            "Epoch: 007, Loss: 0.2355, Train: 1.0000, Test: 0.5682\n",
            "Early stopping:  0.6748938907632003\n",
            "Epoch: 008, Loss: 0.1011, Train: 1.0000, Test: 0.5747\n",
            "Early stopping:  0.5213979207888272\n",
            "Epoch: 009, Loss: 0.0420, Train: 1.0000, Test: 0.5781\n",
            "Early stopping:  0.3455985076063586\n",
            "Epoch: 010, Loss: 0.0179, Train: 1.0000, Test: 0.5748\n",
            "Early stopping:  0.19476612026602194\n",
            "Epoch: 011, Loss: 0.0081, Train: 1.0000, Test: 0.5718\n",
            "Early stopping:  0.09363936750306745\n",
            "Epoch: 012, Loss: 0.0039, Train: 1.0000, Test: 0.5689\n",
            "Early stopping:  0.039998904177926196\n",
            "Epoch: 013, Loss: 0.0021, Train: 1.0000, Test: 0.5659\n",
            "Early stopping:  0.016409072239404732\n",
            "Epoch: 014, Loss: 0.0012, Train: 1.0000, Test: 0.5642\n",
            "Early stopping:  0.006857402092178077\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  9, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.59      0.61      0.60       401\n",
            "             ecology       0.60      0.79      0.69       888\n",
            "            economic       0.74      0.59      0.66      1408\n",
            "          geophysics       0.74      0.15      0.25      1201\n",
            "  gravitional_theory       0.37      0.95      0.54       129\n",
            "               hydro       0.26      0.76      0.38       354\n",
            "                math       0.70      0.37      0.48      1338\n",
            "              metals       0.51      0.92      0.66       200\n",
            "          networking       0.72      0.91      0.80       344\n",
            "        neuroscience       0.79      0.91      0.85       306\n",
            "        oceanography       0.82      0.56      0.67       989\n",
            "             politic       0.43      0.76      0.55       602\n",
            "           sociology       0.63      0.44      0.52       738\n",
            "software_engineering       0.73      0.86      0.79       523\n",
            "          statistics       0.36      0.42      0.39       646\n",
            "    theory_computing       0.36      0.57      0.44       441\n",
            "\n",
            "            accuracy                           0.56     10508\n",
            "           macro avg       0.58      0.66      0.58     10508\n",
            "        weighted avg       0.63      0.56      0.55     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7729, Train: 0.9375, Test: 0.3313\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3561, Train: 1.0000, Test: 0.4545\n",
            "Early stopping:  0.29470425177969456\n",
            "Epoch: 003, Loss: 1.8732, Train: 1.0000, Test: 0.4923\n",
            "Early stopping:  0.45023086888370223\n",
            "Epoch: 004, Loss: 1.3484, Train: 1.0000, Test: 0.5068\n",
            "Early stopping:  0.6148274049977891\n",
            "Epoch: 005, Loss: 0.8583, Train: 1.0000, Test: 0.5265\n",
            "Early stopping:  0.7653492231007055\n",
            "Epoch: 006, Loss: 0.4722, Train: 1.0000, Test: 0.5434\n",
            "Early stopping:  0.7571762589306602\n",
            "Epoch: 007, Loss: 0.2219, Train: 1.0000, Test: 0.5521\n",
            "Early stopping:  0.6666645459582371\n",
            "Epoch: 008, Loss: 0.0930, Train: 1.0000, Test: 0.5490\n",
            "Early stopping:  0.510678256930278\n",
            "Epoch: 009, Loss: 0.0380, Train: 1.0000, Test: 0.5443\n",
            "Early stopping:  0.33624079146638\n",
            "Epoch: 010, Loss: 0.0163, Train: 1.0000, Test: 0.5376\n",
            "Early stopping:  0.18772941532445828\n",
            "Epoch: 011, Loss: 0.0076, Train: 1.0000, Test: 0.5319\n",
            "Early stopping:  0.0883942176542839\n",
            "Epoch: 012, Loss: 0.0039, Train: 1.0000, Test: 0.5274\n",
            "Early stopping:  0.0366916964643629\n",
            "Epoch: 013, Loss: 0.0022, Train: 1.0000, Test: 0.5216\n",
            "Early stopping:  0.014674738038503542\n",
            "Epoch: 014, Loss: 0.0013, Train: 1.0000, Test: 0.5165\n",
            "Early stopping:  0.0061103278622011694\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.63      0.82      0.71       401\n",
            "             ecology       0.31      0.38      0.34       888\n",
            "            economic       0.45      0.05      0.09      1408\n",
            "          geophysics       0.91      0.62      0.74      1201\n",
            "  gravitional_theory       0.91      0.81      0.86       129\n",
            "               hydro       0.41      0.44      0.42       354\n",
            "                math       0.58      0.71      0.64      1338\n",
            "              metals       0.54      0.94      0.69       200\n",
            "          networking       0.66      0.86      0.75       344\n",
            "        neuroscience       0.90      0.93      0.92       306\n",
            "        oceanography       0.27      0.40      0.32       989\n",
            "             politic       0.59      0.82      0.69       602\n",
            "           sociology       0.38      0.58      0.45       738\n",
            "software_engineering       0.73      0.53      0.61       523\n",
            "          statistics       0.26      0.20      0.22       646\n",
            "    theory_computing       0.61      0.58      0.60       441\n",
            "\n",
            "            accuracy                           0.52     10508\n",
            "           macro avg       0.57      0.60      0.57     10508\n",
            "        weighted avg       0.54      0.52      0.49     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.8052, Train: 0.9375, Test: 0.2262\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3994, Train: 1.0000, Test: 0.4097\n",
            "Early stopping:  0.2869129853639707\n",
            "Epoch: 003, Loss: 1.9428, Train: 1.0000, Test: 0.4759\n",
            "Early stopping:  0.4314602040327671\n",
            "Epoch: 004, Loss: 1.4285, Train: 1.0000, Test: 0.4910\n",
            "Early stopping:  0.5929908882172079\n",
            "Epoch: 005, Loss: 0.9339, Train: 1.0000, Test: 0.4995\n",
            "Early stopping:  0.7460495900021316\n",
            "Epoch: 006, Loss: 0.5286, Train: 1.0000, Test: 0.5080\n",
            "Early stopping:  0.7516575699036665\n",
            "Epoch: 007, Loss: 0.2564, Train: 1.0000, Test: 0.5044\n",
            "Early stopping:  0.6801273317124207\n",
            "Epoch: 008, Loss: 0.1110, Train: 1.0000, Test: 0.4953\n",
            "Early stopping:  0.5354268451122967\n",
            "Epoch: 009, Loss: 0.0469, Train: 1.0000, Test: 0.4863\n",
            "Early stopping:  0.3630893512508556\n",
            "Epoch: 010, Loss: 0.0208, Train: 1.0000, Test: 0.4790\n",
            "Early stopping:  0.20880479650770525\n",
            "Epoch: 011, Loss: 0.0099, Train: 1.0000, Test: 0.4748\n",
            "Early stopping:  0.10148526543332102\n",
            "Epoch: 012, Loss: 0.0050, Train: 1.0000, Test: 0.4672\n",
            "Early stopping:  0.04355187814363497\n",
            "Epoch: 013, Loss: 0.0028, Train: 1.0000, Test: 0.4635\n",
            "Early stopping:  0.018076794560467256\n",
            "Epoch: 014, Loss: 0.0016, Train: 1.0000, Test: 0.4598\n",
            "Early stopping:  0.0078001683831453485\n",
            "PREDICTIONS -> tensor([ 0,  8, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.29      0.43       401\n",
            "             ecology       0.39      0.68      0.49       888\n",
            "            economic       0.78      0.15      0.25      1408\n",
            "          geophysics       0.90      0.54      0.68      1201\n",
            "  gravitional_theory       0.72      0.92      0.81       129\n",
            "               hydro       0.17      0.04      0.06       354\n",
            "                math       0.45      0.39      0.42      1338\n",
            "              metals       0.37      0.88      0.52       200\n",
            "          networking       0.37      0.40      0.38       344\n",
            "        neuroscience       0.82      0.95      0.88       306\n",
            "        oceanography       0.42      0.84      0.56       989\n",
            "             politic       0.61      0.78      0.68       602\n",
            "           sociology       0.15      0.16      0.15       738\n",
            "software_engineering       0.77      0.34      0.47       523\n",
            "          statistics       0.43      0.05      0.09       646\n",
            "    theory_computing       0.26      0.83      0.40       441\n",
            "\n",
            "            accuracy                           0.46     10508\n",
            "           macro avg       0.53      0.52      0.46     10508\n",
            "        weighted avg       0.55      0.46      0.43     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7932, Train: 0.8125, Test: 0.3110\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3907, Train: 1.0000, Test: 0.4852\n",
            "Early stopping:  0.2846136218966103\n",
            "Epoch: 003, Loss: 1.9358, Train: 1.0000, Test: 0.5160\n",
            "Early stopping:  0.42899316569503215\n",
            "Epoch: 004, Loss: 1.4283, Train: 1.0000, Test: 0.5217\n",
            "Early stopping:  0.5881570034273554\n",
            "Epoch: 005, Loss: 0.9414, Train: 1.0000, Test: 0.5226\n",
            "Early stopping:  0.738459896700881\n",
            "Epoch: 006, Loss: 0.5426, Train: 1.0000, Test: 0.5166\n",
            "Early stopping:  0.7421852956156062\n",
            "Epoch: 007, Loss: 0.2685, Train: 1.0000, Test: 0.5022\n",
            "Early stopping:  0.671586840765318\n",
            "Epoch: 008, Loss: 0.1172, Train: 1.0000, Test: 0.4797\n",
            "Early stopping:  0.5317835590202667\n",
            "Epoch: 009, Loss: 0.0487, Train: 1.0000, Test: 0.4653\n",
            "Early stopping:  0.36498841304049123\n",
            "Epoch: 010, Loss: 0.0208, Train: 1.0000, Test: 0.4562\n",
            "Early stopping:  0.21445538634734734\n",
            "Epoch: 011, Loss: 0.0095, Train: 1.0000, Test: 0.4491\n",
            "Early stopping:  0.10670038482551246\n",
            "Epoch: 012, Loss: 0.0047, Train: 1.0000, Test: 0.4449\n",
            "Early stopping:  0.046312630600594096\n",
            "Epoch: 013, Loss: 0.0026, Train: 1.0000, Test: 0.4419\n",
            "Early stopping:  0.018935843267422356\n",
            "Epoch: 014, Loss: 0.0015, Train: 1.0000, Test: 0.4379\n",
            "Early stopping:  0.007861336923621084\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.52      0.88      0.66       401\n",
            "             ecology       0.20      0.18      0.19       888\n",
            "            economic       0.60      0.10      0.16      1408\n",
            "          geophysics       0.91      0.71      0.80      1201\n",
            "  gravitional_theory       0.55      0.92      0.69       129\n",
            "               hydro       0.28      0.69      0.40       354\n",
            "                math       0.96      0.24      0.38      1338\n",
            "              metals       0.26      0.97      0.41       200\n",
            "          networking       0.43      0.92      0.59       344\n",
            "        neuroscience       0.77      0.98      0.86       306\n",
            "        oceanography       0.33      0.37      0.35       989\n",
            "             politic       0.42      0.76      0.54       602\n",
            "           sociology       0.37      0.22      0.27       738\n",
            "software_engineering       0.59      0.31      0.40       523\n",
            "          statistics       0.64      0.41      0.50       646\n",
            "    theory_computing       0.16      0.46      0.23       441\n",
            "\n",
            "            accuracy                           0.44     10508\n",
            "           macro avg       0.50      0.57      0.47     10508\n",
            "        weighted avg       0.56      0.44      0.42     10508\n",
            "\n",
            "time: 7.06 s (started: 2024-08-16 14:11:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "KKuelhyAVyh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b1395a-b49e-4508-d18d-61e2f560e6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 421 ms (started: 2024-08-16 14:11:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 5 ❎"
      ],
      "metadata": {
        "id": "Y_u1h2Busrbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "Wk90E6ENuoaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV_E07btuoaZ",
        "outputId": "3d3b54ce-e76d-45a7-a79f-f2f19863832e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.0398, Train: 0.7875, Test: 0.5812\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9149, Train: 0.9500, Test: 0.6975\n",
            "Early stopping:  2.209632152531065\n",
            "Epoch: 003, Loss: 0.2483, Train: 0.9625, Test: 0.7274\n",
            "Early stopping:  2.024229972181255\n",
            "Epoch: 004, Loss: 0.1319, Train: 0.9875, Test: 0.7199\n",
            "Early stopping:  1.836735418721627\n",
            "Epoch: 005, Loss: 0.0841, Train: 0.9875, Test: 0.7160\n",
            "Early stopping:  1.6859697310888826\n",
            "Epoch: 006, Loss: 0.0481, Train: 1.0000, Test: 0.7228\n",
            "Early stopping:  0.35987780383354745\n",
            "Epoch: 007, Loss: 0.0103, Train: 1.0000, Test: 0.7236\n",
            "Early stopping:  0.09204473502296306\n",
            "Epoch: 008, Loss: 0.0083, Train: 0.9875, Test: 0.7237\n",
            "Early stopping:  0.052390527048580306\n",
            "Epoch: 009, Loss: 0.0134, Train: 1.0000, Test: 0.7302\n",
            "Early stopping:  0.0329768010601154\n",
            "Epoch: 010, Loss: 0.0073, Train: 1.0000, Test: 0.7349\n",
            "Early stopping:  0.017258433826258766\n",
            "Epoch: 011, Loss: 0.0021, Train: 1.0000, Test: 0.7342\n",
            "Early stopping:  0.004190019070994583\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.52      0.85      0.65       397\n",
            "             ecology       0.89      0.67      0.77       884\n",
            "            economic       0.81      0.72      0.77      1404\n",
            "          geophysics       0.99      0.65      0.79      1197\n",
            "  gravitional_theory       0.33      0.97      0.49       125\n",
            "               hydro       0.74      0.77      0.76       350\n",
            "                math       0.66      0.60      0.63      1334\n",
            "              metals       0.69      0.93      0.79       196\n",
            "          networking       0.83      0.86      0.84       340\n",
            "        neuroscience       0.82      1.00      0.90       302\n",
            "        oceanography       0.74      0.90      0.81       985\n",
            "             politic       0.80      0.69      0.74       598\n",
            "           sociology       0.61      0.73      0.66       734\n",
            "software_engineering       0.90      0.74      0.81       519\n",
            "          statistics       0.64      0.70      0.67       642\n",
            "    theory_computing       0.67      0.69      0.68       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.73      0.78      0.74     10444\n",
            "        weighted avg       0.77      0.73      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.9145, Train: 0.8000, Test: 0.6352\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.7684, Train: 0.9250, Test: 0.6272\n",
            "Early stopping:  2.224655017361137\n",
            "Epoch: 003, Loss: 1.2730, Train: 0.9625, Test: 0.6756\n",
            "Early stopping:  1.6896861444542846\n",
            "Epoch: 004, Loss: 0.3052, Train: 0.9375, Test: 0.6681\n",
            "Early stopping:  1.6152381268158762\n",
            "Epoch: 005, Loss: 1.0426, Train: 0.9625, Test: 0.6840\n",
            "Early stopping:  1.4182292313286329\n",
            "Epoch: 006, Loss: 0.5845, Train: 0.9875, Test: 0.6764\n",
            "Early stopping:  0.3790525056722922\n",
            "Epoch: 007, Loss: 0.0392, Train: 0.9750, Test: 0.6785\n",
            "Early stopping:  0.5095553593454736\n",
            "Epoch: 008, Loss: 0.0825, Train: 0.9750, Test: 0.6893\n",
            "Early stopping:  0.4142442076842651\n",
            "Epoch: 009, Loss: 0.0497, Train: 1.0000, Test: 0.7088\n",
            "Early stopping:  0.44513157868806197\n",
            "Epoch: 010, Loss: 0.0011, Train: 1.0000, Test: 0.7201\n",
            "Early stopping:  0.2438363151763967\n",
            "Epoch: 011, Loss: 0.0008, Train: 0.9875, Test: 0.7220\n",
            "Early stopping:  0.03469290327999142\n",
            "Epoch: 012, Loss: 0.0167, Train: 1.0000, Test: 0.7236\n",
            "Early stopping:  0.035416349358747025\n",
            "Epoch: 013, Loss: 0.0011, Train: 1.0000, Test: 0.7231\n",
            "Early stopping:  0.021168559334277703\n",
            "Epoch: 014, Loss: 0.0007, Train: 1.0000, Test: 0.7224\n",
            "Early stopping:  0.007040289076481732\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.62      0.70      0.66       397\n",
            "             ecology       0.88      0.66      0.75       884\n",
            "            economic       0.82      0.59      0.69      1404\n",
            "          geophysics       0.88      0.77      0.82      1197\n",
            "  gravitional_theory       0.94      0.86      0.90       125\n",
            "               hydro       0.59      0.82      0.69       350\n",
            "                math       0.86      0.76      0.81      1334\n",
            "              metals       0.53      0.95      0.68       196\n",
            "          networking       0.90      0.74      0.81       340\n",
            "        neuroscience       0.78      0.98      0.87       302\n",
            "        oceanography       0.76      0.81      0.79       985\n",
            "             politic       0.55      0.82      0.66       598\n",
            "           sociology       0.45      0.44      0.45       734\n",
            "software_engineering       0.83      0.82      0.83       519\n",
            "          statistics       0.53      0.79      0.64       642\n",
            "    theory_computing       0.68      0.57      0.62       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.73      0.76      0.73     10444\n",
            "        weighted avg       0.75      0.72      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.1368, Train: 0.6875, Test: 0.5311\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 3.1353, Train: 0.7375, Test: 0.5575\n",
            "Early stopping:  0.7082221553855398\n",
            "Epoch: 003, Loss: 2.0961, Train: 0.8750, Test: 0.6679\n",
            "Early stopping:  1.0204186952998207\n",
            "Epoch: 004, Loss: 0.9711, Train: 0.8875, Test: 0.6944\n",
            "Early stopping:  1.360733006398523\n",
            "Epoch: 005, Loss: 0.5671, Train: 0.9750, Test: 0.7358\n",
            "Early stopping:  1.4842193741639205\n",
            "Epoch: 006, Loss: 0.2748, Train: 0.9750, Test: 0.7012\n",
            "Early stopping:  1.1873285440131458\n",
            "Epoch: 007, Loss: 0.1718, Train: 0.9875, Test: 0.6698\n",
            "Early stopping:  0.7798241521499611\n",
            "Epoch: 008, Loss: 0.0665, Train: 0.9875, Test: 0.6508\n",
            "Early stopping:  0.3648605120774145\n",
            "Epoch: 009, Loss: 0.0405, Train: 0.9875, Test: 0.6397\n",
            "Early stopping:  0.21302648279147943\n",
            "Epoch: 010, Loss: 0.0894, Train: 0.9875, Test: 0.6396\n",
            "Early stopping:  0.09539309542886788\n",
            "Epoch: 011, Loss: 0.0441, Train: 1.0000, Test: 0.6497\n",
            "Early stopping:  0.05367633643292756\n",
            "Epoch: 012, Loss: 0.0111, Train: 1.0000, Test: 0.6548\n",
            "Early stopping:  0.029441833163899523\n",
            "Epoch: 013, Loss: 0.0101, Train: 1.0000, Test: 0.6593\n",
            "Early stopping:  0.03234984103956042\n",
            "Epoch: 014, Loss: 0.0085, Train: 0.9875, Test: 0.6633\n",
            "Early stopping:  0.03504768135178481\n",
            "Epoch: 015, Loss: 0.0208, Train: 0.9875, Test: 0.6646\n",
            "Early stopping:  0.014865881152124371\n",
            "Epoch: 016, Loss: 0.0242, Train: 1.0000, Test: 0.6652\n",
            "Early stopping:  0.007072140946855153\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.73      0.71       397\n",
            "             ecology       0.75      0.66      0.70       884\n",
            "            economic       0.71      0.70      0.71      1404\n",
            "          geophysics       0.93      0.72      0.81      1197\n",
            "  gravitional_theory       0.53      0.94      0.68       125\n",
            "               hydro       0.59      0.76      0.67       350\n",
            "                math       0.70      0.72      0.71      1334\n",
            "              metals       0.50      0.34      0.41       196\n",
            "          networking       0.59      0.91      0.71       340\n",
            "        neuroscience       0.42      1.00      0.59       302\n",
            "        oceanography       0.88      0.67      0.76       985\n",
            "             politic       0.61      0.79      0.69       598\n",
            "           sociology       0.72      0.27      0.39       734\n",
            "software_engineering       0.87      0.23      0.36       519\n",
            "          statistics       0.63      0.71      0.67       642\n",
            "    theory_computing       0.36      0.71      0.48       437\n",
            "\n",
            "            accuracy                           0.67     10444\n",
            "           macro avg       0.66      0.68      0.63     10444\n",
            "        weighted avg       0.71      0.67      0.66     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.8386, Train: 0.8125, Test: 0.5831\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8729, Train: 0.8875, Test: 0.6564\n",
            "Early stopping:  2.8041988402923086\n",
            "Epoch: 003, Loss: 0.4041, Train: 0.9625, Test: 0.6942\n",
            "Early stopping:  2.4362438170514142\n",
            "Epoch: 004, Loss: 0.1754, Train: 0.9750, Test: 0.6985\n",
            "Early stopping:  2.196514564449437\n",
            "Epoch: 005, Loss: 0.0729, Train: 0.9750, Test: 0.6962\n",
            "Early stopping:  2.017026107434051\n",
            "Epoch: 006, Loss: 0.0823, Train: 0.9750, Test: 0.6987\n",
            "Early stopping:  0.3358460896863255\n",
            "Epoch: 007, Loss: 0.0414, Train: 1.0000, Test: 0.6971\n",
            "Early stopping:  0.14780701000922325\n",
            "Epoch: 008, Loss: 0.0132, Train: 0.9875, Test: 0.6914\n",
            "Early stopping:  0.061361569796776774\n",
            "Epoch: 009, Loss: 0.0199, Train: 0.9875, Test: 0.6889\n",
            "Early stopping:  0.03091888575727213\n",
            "Epoch: 010, Loss: 0.0180, Train: 1.0000, Test: 0.6899\n",
            "Early stopping:  0.028603397570533935\n",
            "Epoch: 011, Loss: 0.0076, Train: 1.0000, Test: 0.6902\n",
            "Early stopping:  0.012855053968840792\n",
            "Epoch: 012, Loss: 0.0025, Train: 1.0000, Test: 0.6916\n",
            "Early stopping:  0.0072478083634126556\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.73      0.72       397\n",
            "             ecology       0.84      0.71      0.77       884\n",
            "            economic       0.71      0.50      0.59      1404\n",
            "          geophysics       0.89      0.87      0.88      1197\n",
            "  gravitional_theory       0.90      0.92      0.91       125\n",
            "               hydro       0.46      0.73      0.56       350\n",
            "                math       0.92      0.45      0.60      1334\n",
            "              metals       0.56      0.94      0.70       196\n",
            "          networking       0.91      0.76      0.83       340\n",
            "        neuroscience       0.92      0.97      0.94       302\n",
            "        oceanography       0.87      0.74      0.80       985\n",
            "             politic       0.49      0.86      0.62       598\n",
            "           sociology       0.45      0.50      0.47       734\n",
            "software_engineering       0.67      0.88      0.76       519\n",
            "          statistics       0.49      0.76      0.59       642\n",
            "    theory_computing       0.62      0.69      0.66       437\n",
            "\n",
            "            accuracy                           0.69     10444\n",
            "           macro avg       0.71      0.75      0.71     10444\n",
            "        weighted avg       0.74      0.69      0.69     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.1087, Train: 0.6000, Test: 0.4165\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.7613, Train: 0.8125, Test: 0.6467\n",
            "Early stopping:  2.3669860627473356\n",
            "Epoch: 003, Loss: 0.9688, Train: 0.9000, Test: 0.7072\n",
            "Early stopping:  2.197424242759658\n",
            "Epoch: 004, Loss: 0.3826, Train: 0.9500, Test: 0.7247\n",
            "Early stopping:  2.1125217148238904\n",
            "Epoch: 005, Loss: 0.2032, Train: 0.9500, Test: 0.7031\n",
            "Early stopping:  2.0082751074520178\n",
            "Epoch: 006, Loss: 0.1344, Train: 0.9875, Test: 0.6883\n",
            "Early stopping:  0.6829357621683428\n",
            "Epoch: 007, Loss: 0.0593, Train: 0.9875, Test: 0.6756\n",
            "Early stopping:  0.36625302932083253\n",
            "Epoch: 008, Loss: 0.0308, Train: 1.0000, Test: 0.6711\n",
            "Early stopping:  0.14043715909407872\n",
            "Epoch: 009, Loss: 0.0237, Train: 1.0000, Test: 0.6858\n",
            "Early stopping:  0.07687042093671305\n",
            "Epoch: 010, Loss: 0.0096, Train: 1.0000, Test: 0.6923\n",
            "Early stopping:  0.049736864886981584\n",
            "Epoch: 011, Loss: 0.0036, Train: 1.0000, Test: 0.6967\n",
            "Early stopping:  0.02182110220741089\n",
            "Epoch: 012, Loss: 0.0040, Train: 1.0000, Test: 0.6999\n",
            "Early stopping:  0.012257230784545003\n",
            "Epoch: 013, Loss: 0.0036, Train: 1.0000, Test: 0.7012\n",
            "Early stopping:  0.008630826815781863\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.86      0.57      0.69       397\n",
            "             ecology       0.72      0.84      0.78       884\n",
            "            economic       0.75      0.49      0.59      1404\n",
            "          geophysics       0.90      0.90      0.90      1197\n",
            "  gravitional_theory       0.87      0.88      0.88       125\n",
            "               hydro       0.37      0.88      0.52       350\n",
            "                math       0.85      0.61      0.71      1334\n",
            "              metals       0.82      0.77      0.79       196\n",
            "          networking       0.89      0.80      0.84       340\n",
            "        neuroscience       0.97      0.90      0.94       302\n",
            "        oceanography       0.94      0.61      0.74       985\n",
            "             politic       0.69      0.54      0.60       598\n",
            "           sociology       0.41      0.82      0.55       734\n",
            "software_engineering       0.72      0.84      0.78       519\n",
            "          statistics       0.57      0.62      0.59       642\n",
            "    theory_computing       0.64      0.73      0.68       437\n",
            "\n",
            "            accuracy                           0.70     10444\n",
            "           macro avg       0.75      0.74      0.72     10444\n",
            "        weighted avg       0.76      0.70      0.71     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.7148, Train: 0.5625, Test: 0.4271\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.7458, Train: 0.7375, Test: 0.6362\n",
            "Early stopping:  2.099435255295573\n",
            "Epoch: 003, Loss: 1.1426, Train: 0.8875, Test: 0.6684\n",
            "Early stopping:  1.912231668787286\n",
            "Epoch: 004, Loss: 0.4410, Train: 0.9375, Test: 0.6800\n",
            "Early stopping:  1.8797068421963796\n",
            "Epoch: 005, Loss: 0.2815, Train: 0.9500, Test: 0.6703\n",
            "Early stopping:  1.8022823870193203\n",
            "Epoch: 006, Loss: 0.2421, Train: 0.9625, Test: 0.6761\n",
            "Early stopping:  0.6550639356124488\n",
            "Epoch: 007, Loss: 0.1157, Train: 0.9875, Test: 0.7038\n",
            "Early stopping:  0.4071169706224062\n",
            "Epoch: 008, Loss: 0.0510, Train: 0.9875, Test: 0.7211\n",
            "Early stopping:  0.15197463313158757\n",
            "Epoch: 009, Loss: 0.0441, Train: 0.9875, Test: 0.7266\n",
            "Early stopping:  0.10947381193243048\n",
            "Epoch: 010, Loss: 0.0208, Train: 0.9875, Test: 0.7254\n",
            "Early stopping:  0.0895861686907929\n",
            "Epoch: 011, Loss: 0.0200, Train: 1.0000, Test: 0.7246\n",
            "Early stopping:  0.03906943829375746\n",
            "Epoch: 012, Loss: 0.0081, Train: 1.0000, Test: 0.7198\n",
            "Early stopping:  0.01799666357974925\n",
            "Epoch: 013, Loss: 0.0013, Train: 1.0000, Test: 0.7155\n",
            "Early stopping:  0.016327143527309088\n",
            "Epoch: 014, Loss: 0.0008, Train: 1.0000, Test: 0.7090\n",
            "Early stopping:  0.009748330750081918\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.61      0.87      0.72       397\n",
            "             ecology       0.75      0.72      0.73       884\n",
            "            economic       0.73      0.68      0.70      1404\n",
            "          geophysics       0.95      0.80      0.87      1197\n",
            "  gravitional_theory       0.85      0.92      0.88       125\n",
            "               hydro       0.40      0.89      0.55       350\n",
            "                math       0.83      0.67      0.74      1334\n",
            "              metals       0.83      0.70      0.76       196\n",
            "          networking       0.81      0.91      0.86       340\n",
            "        neuroscience       0.66      0.99      0.79       302\n",
            "        oceanography       0.76      0.73      0.74       985\n",
            "             politic       0.84      0.58      0.68       598\n",
            "           sociology       0.49      0.54      0.51       734\n",
            "software_engineering       0.74      0.61      0.67       519\n",
            "          statistics       0.62      0.79      0.70       642\n",
            "    theory_computing       0.54      0.40      0.46       437\n",
            "\n",
            "            accuracy                           0.71     10444\n",
            "           macro avg       0.71      0.74      0.71     10444\n",
            "        weighted avg       0.73      0.71      0.71     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.2961, Train: 0.6500, Test: 0.5080\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.0552, Train: 0.8125, Test: 0.5712\n",
            "Early stopping:  1.584623048971494\n",
            "Epoch: 003, Loss: 1.1847, Train: 0.9500, Test: 0.6365\n",
            "Early stopping:  1.6052269861399804\n",
            "Epoch: 004, Loss: 0.3317, Train: 0.9625, Test: 0.7009\n",
            "Early stopping:  1.7047872351506148\n",
            "Epoch: 005, Loss: 0.3940, Train: 0.9625, Test: 0.7096\n",
            "Early stopping:  1.6354048476477623\n",
            "Epoch: 006, Loss: 0.3446, Train: 0.9750, Test: 0.7085\n",
            "Early stopping:  0.7575823520876188\n",
            "Epoch: 007, Loss: 0.1835, Train: 0.9875, Test: 0.7040\n",
            "Early stopping:  0.3974853176571822\n",
            "Epoch: 008, Loss: 0.0194, Train: 0.9875, Test: 0.6702\n",
            "Early stopping:  0.15315635616682957\n",
            "Epoch: 009, Loss: 0.1732, Train: 0.9875, Test: 0.6546\n",
            "Early stopping:  0.14958890373523157\n",
            "Epoch: 010, Loss: 0.1524, Train: 1.0000, Test: 0.6557\n",
            "Early stopping:  0.11571015536354608\n",
            "Epoch: 011, Loss: 0.0064, Train: 1.0000, Test: 0.6420\n",
            "Early stopping:  0.08674218552666248\n",
            "Epoch: 012, Loss: 0.0065, Train: 0.9875, Test: 0.6342\n",
            "Early stopping:  0.08376313402201112\n",
            "Epoch: 013, Loss: 0.0375, Train: 1.0000, Test: 0.6332\n",
            "Early stopping:  0.08130296695234164\n",
            "Epoch: 014, Loss: 0.0110, Train: 1.0000, Test: 0.6318\n",
            "Early stopping:  0.06265456772041487\n",
            "Epoch: 015, Loss: 0.0013, Train: 1.0000, Test: 0.6328\n",
            "Early stopping:  0.014391266411355366\n",
            "Epoch: 016, Loss: 0.0007, Train: 1.0000, Test: 0.6345\n",
            "Early stopping:  0.015194137064797527\n",
            "Epoch: 017, Loss: 0.0005, Train: 1.0000, Test: 0.6363\n",
            "Early stopping:  0.015901470818517685\n",
            "Epoch: 018, Loss: 0.0004, Train: 1.0000, Test: 0.6385\n",
            "Early stopping:  0.004605723918058183\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.68      0.67       397\n",
            "             ecology       0.54      0.85      0.66       884\n",
            "            economic       0.81      0.39      0.53      1404\n",
            "          geophysics       0.88      0.85      0.86      1197\n",
            "  gravitional_theory       0.51      0.86      0.64       125\n",
            "               hydro       0.68      0.75      0.71       350\n",
            "                math       0.74      0.63      0.68      1334\n",
            "              metals       0.50      0.94      0.65       196\n",
            "          networking       0.37      0.94      0.53       340\n",
            "        neuroscience       0.98      0.88      0.93       302\n",
            "        oceanography       0.95      0.27      0.42       985\n",
            "             politic       0.44      0.86      0.58       598\n",
            "           sociology       0.54      0.28      0.37       734\n",
            "software_engineering       0.74      0.84      0.79       519\n",
            "          statistics       0.51      0.64      0.57       642\n",
            "    theory_computing       0.76      0.62      0.68       437\n",
            "\n",
            "            accuracy                           0.64     10444\n",
            "           macro avg       0.66      0.70      0.64     10444\n",
            "        weighted avg       0.71      0.64      0.63     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5124, Train: 0.7250, Test: 0.5738\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2479, Train: 0.7375, Test: 0.6373\n",
            "Early stopping:  2.308389046381549\n",
            "Epoch: 003, Loss: 1.0600, Train: 0.9125, Test: 0.7225\n",
            "Early stopping:  1.941306937157141\n",
            "Epoch: 004, Loss: 0.4600, Train: 0.9500, Test: 0.7381\n",
            "Early stopping:  1.8260770360879\n",
            "Epoch: 005, Loss: 0.2595, Train: 0.9375, Test: 0.7165\n",
            "Early stopping:  1.7285918901503348\n",
            "Epoch: 006, Loss: 0.2136, Train: 1.0000, Test: 0.6854\n",
            "Early stopping:  0.4755413408057466\n",
            "Epoch: 007, Loss: 0.0387, Train: 0.9875, Test: 0.6652\n",
            "Early stopping:  0.3949731479259046\n",
            "Epoch: 008, Loss: 0.0604, Train: 0.9875, Test: 0.6588\n",
            "Early stopping:  0.17076854769253175\n",
            "Epoch: 009, Loss: 0.0222, Train: 0.9875, Test: 0.6566\n",
            "Early stopping:  0.10946990749281763\n",
            "Epoch: 010, Loss: 0.0239, Train: 1.0000, Test: 0.6526\n",
            "Early stopping:  0.08076543469558121\n",
            "Epoch: 011, Loss: 0.0107, Train: 1.0000, Test: 0.6519\n",
            "Early stopping:  0.01912157948678237\n",
            "Epoch: 012, Loss: 0.0047, Train: 1.0000, Test: 0.6539\n",
            "Early stopping:  0.021664698400811\n",
            "Epoch: 013, Loss: 0.0119, Train: 1.0000, Test: 0.6597\n",
            "Early stopping:  0.008176085714667061\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.55      0.71      0.62       397\n",
            "             ecology       0.78      0.41      0.54       884\n",
            "            economic       0.80      0.53      0.64      1404\n",
            "          geophysics       0.93      0.84      0.88      1197\n",
            "  gravitional_theory       0.32      0.91      0.47       125\n",
            "               hydro       0.63      0.77      0.69       350\n",
            "                math       0.94      0.55      0.69      1334\n",
            "              metals       0.43      0.96      0.60       196\n",
            "          networking       0.63      0.93      0.75       340\n",
            "        neuroscience       0.53      1.00      0.69       302\n",
            "        oceanography       0.60      0.79      0.68       985\n",
            "             politic       0.62      0.56      0.59       598\n",
            "           sociology       0.55      0.77      0.64       734\n",
            "software_engineering       0.81      0.35      0.49       519\n",
            "          statistics       0.59      0.57      0.58       642\n",
            "    theory_computing       0.51      0.78      0.62       437\n",
            "\n",
            "            accuracy                           0.66     10444\n",
            "           macro avg       0.64      0.72      0.64     10444\n",
            "        weighted avg       0.72      0.66      0.66     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.8914, Train: 0.6625, Test: 0.5833\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.4950, Train: 0.7750, Test: 0.5184\n",
            "Early stopping:  1.6945082281154\n",
            "Epoch: 003, Loss: 1.4143, Train: 0.8875, Test: 0.6793\n",
            "Early stopping:  1.4074419389565331\n",
            "Epoch: 004, Loss: 0.3717, Train: 0.9000, Test: 0.6600\n",
            "Early stopping:  1.489472260184142\n",
            "Epoch: 005, Loss: 0.7366, Train: 0.9375, Test: 0.6935\n",
            "Early stopping:  1.37372815430601\n",
            "Epoch: 006, Loss: 0.4763, Train: 0.9750, Test: 0.7308\n",
            "Early stopping:  0.525307134855526\n",
            "Epoch: 007, Loss: 0.1609, Train: 0.9875, Test: 0.7409\n",
            "Early stopping:  0.4839498951653822\n",
            "Epoch: 008, Loss: 0.0339, Train: 0.9500, Test: 0.6999\n",
            "Early stopping:  0.27451570857057533\n",
            "Epoch: 009, Loss: 0.1615, Train: 0.9625, Test: 0.6956\n",
            "Early stopping:  0.28729285526523424\n",
            "Epoch: 010, Loss: 0.1007, Train: 1.0000, Test: 0.7114\n",
            "Early stopping:  0.17025531348712963\n",
            "Epoch: 011, Loss: 0.0104, Train: 1.0000, Test: 0.7186\n",
            "Early stopping:  0.07014247056515979\n",
            "Epoch: 012, Loss: 0.0020, Train: 1.0000, Test: 0.7166\n",
            "Early stopping:  0.06791570584580968\n",
            "Epoch: 013, Loss: 0.0018, Train: 1.0000, Test: 0.7110\n",
            "Early stopping:  0.07255268368403381\n",
            "Epoch: 014, Loss: 0.0038, Train: 1.0000, Test: 0.7038\n",
            "Early stopping:  0.04317639249609942\n",
            "Epoch: 015, Loss: 0.0074, Train: 0.9875, Test: 0.6918\n",
            "Early stopping:  0.0037202289988057112\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.52      0.64      0.57       397\n",
            "             ecology       0.70      0.90      0.79       884\n",
            "            economic       0.82      0.49      0.61      1404\n",
            "          geophysics       0.88      0.88      0.88      1197\n",
            "  gravitional_theory       0.66      0.94      0.78       125\n",
            "               hydro       0.87      0.67      0.75       350\n",
            "                math       0.80      0.77      0.78      1334\n",
            "              metals       0.53      0.93      0.68       196\n",
            "          networking       0.74      0.89      0.81       340\n",
            "        neuroscience       0.87      0.97      0.92       302\n",
            "        oceanography       0.92      0.51      0.66       985\n",
            "             politic       0.37      0.47      0.42       598\n",
            "           sociology       0.42      0.76      0.54       734\n",
            "software_engineering       0.92      0.47      0.62       519\n",
            "          statistics       0.69      0.57      0.63       642\n",
            "    theory_computing       0.61      0.73      0.67       437\n",
            "\n",
            "            accuracy                           0.69     10444\n",
            "           macro avg       0.71      0.73      0.69     10444\n",
            "        weighted avg       0.74      0.69      0.69     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5617, Train: 0.6250, Test: 0.4582\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.3127, Train: 0.7375, Test: 0.6473\n",
            "Early stopping:  2.297333759429747\n",
            "Epoch: 003, Loss: 1.1239, Train: 0.8625, Test: 0.6488\n",
            "Early stopping:  1.9325950279609894\n",
            "Epoch: 004, Loss: 0.6714, Train: 0.9375, Test: 0.6458\n",
            "Early stopping:  1.7832431772538913\n",
            "Epoch: 005, Loss: 0.5351, Train: 0.9500, Test: 0.6570\n",
            "Early stopping:  1.6634658838928869\n",
            "Epoch: 006, Loss: 0.2273, Train: 1.0000, Test: 0.7178\n",
            "Early stopping:  0.4413336746600274\n",
            "Epoch: 007, Loss: 0.0225, Train: 0.9875, Test: 0.7128\n",
            "Early stopping:  0.4245343552194892\n",
            "Epoch: 008, Loss: 0.0547, Train: 0.9750, Test: 0.7053\n",
            "Early stopping:  0.28969236412478316\n",
            "Epoch: 009, Loss: 0.1994, Train: 1.0000, Test: 0.7074\n",
            "Early stopping:  0.2033308307088649\n",
            "Epoch: 010, Loss: 0.0124, Train: 0.9875, Test: 0.7024\n",
            "Early stopping:  0.10219603357025604\n",
            "Epoch: 011, Loss: 0.0247, Train: 0.9875, Test: 0.6889\n",
            "Early stopping:  0.07801226436309969\n",
            "Epoch: 012, Loss: 0.0260, Train: 0.9875, Test: 0.6754\n",
            "Early stopping:  0.07757900118264113\n",
            "Epoch: 013, Loss: 0.0588, Train: 1.0000, Test: 0.6786\n",
            "Early stopping:  0.07748522197751193\n",
            "Epoch: 014, Loss: 0.0116, Train: 1.0000, Test: 0.6893\n",
            "Early stopping:  0.01914060287069937\n",
            "Epoch: 015, Loss: 0.0017, Train: 1.0000, Test: 0.6998\n",
            "Early stopping:  0.021590036538996066\n",
            "Epoch: 016, Loss: 0.0007, Train: 1.0000, Test: 0.7050\n",
            "Early stopping:  0.024067965388826492\n",
            "Epoch: 017, Loss: 0.0004, Train: 1.0000, Test: 0.7050\n",
            "Early stopping:  0.02510353796910726\n",
            "Epoch: 018, Loss: 0.0004, Train: 1.0000, Test: 0.7046\n",
            "Early stopping:  0.004855932902594788\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.44      0.70      0.54       397\n",
            "             ecology       0.78      0.84      0.81       884\n",
            "            economic       0.59      0.28      0.38      1404\n",
            "          geophysics       0.87      0.94      0.90      1197\n",
            "  gravitional_theory       0.94      0.74      0.83       125\n",
            "               hydro       0.67      0.70      0.68       350\n",
            "                math       0.82      0.78      0.80      1334\n",
            "              metals       0.68      0.82      0.75       196\n",
            "          networking       0.81      0.89      0.85       340\n",
            "        neuroscience       0.89      0.98      0.93       302\n",
            "        oceanography       0.84      0.77      0.80       985\n",
            "             politic       0.52      0.70      0.60       598\n",
            "           sociology       0.47      0.50      0.49       734\n",
            "software_engineering       0.79      0.81      0.80       519\n",
            "          statistics       0.53      0.71      0.61       642\n",
            "    theory_computing       0.78      0.59      0.67       437\n",
            "\n",
            "            accuracy                           0.70     10444\n",
            "           macro avg       0.71      0.73      0.72     10444\n",
            "        weighted avg       0.71      0.70      0.70     10444\n",
            "\n",
            "time: 5.26 s (started: 2024-08-16 14:11:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "b1w-9ObsuoaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e228780-46f1-4055-e65a-af06b6e0907f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 420 ms (started: 2024-08-16 14:11:34 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "gzJ0JEIzuoaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd043b0-68ca-42e9-d74d-404ed3076f38",
        "id": "-qrKNL5luoaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7875, Train: 0.7250, Test: 0.5150\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4887, Train: 0.9625, Test: 0.7444\n",
            "Early stopping:  0.2112764196137143\n",
            "Epoch: 003, Loss: 2.1323, Train: 0.9750, Test: 0.7563\n",
            "Early stopping:  0.3280061939801322\n",
            "Epoch: 004, Loss: 1.7213, Train: 0.9875, Test: 0.7613\n",
            "Early stopping:  0.4601076253367723\n",
            "Epoch: 005, Loss: 1.3070, Train: 0.9875, Test: 0.7703\n",
            "Early stopping:  0.5908299743660604\n",
            "Epoch: 006, Loss: 0.9299, Train: 0.9875, Test: 0.7807\n",
            "Early stopping:  0.6236397428920462\n",
            "Epoch: 007, Loss: 0.6188, Train: 0.9875, Test: 0.7827\n",
            "Early stopping:  0.6046690904124602\n",
            "Epoch: 008, Loss: 0.3888, Train: 1.0000, Test: 0.7814\n",
            "Early stopping:  0.5333736719718948\n",
            "Epoch: 009, Loss: 0.2372, Train: 1.0000, Test: 0.7738\n",
            "Early stopping:  0.4297793452648574\n",
            "Epoch: 010, Loss: 0.1443, Train: 1.0000, Test: 0.7673\n",
            "Early stopping:  0.31636437341703944\n",
            "Epoch: 011, Loss: 0.0889, Train: 1.0000, Test: 0.7640\n",
            "Early stopping:  0.21342042743795528\n",
            "Epoch: 012, Loss: 0.0559, Train: 1.0000, Test: 0.7618\n",
            "Early stopping:  0.13398804978561524\n",
            "Epoch: 013, Loss: 0.0356, Train: 1.0000, Test: 0.7625\n",
            "Early stopping:  0.08096778777957346\n",
            "Epoch: 014, Loss: 0.0231, Train: 1.0000, Test: 0.7656\n",
            "Early stopping:  0.04862531560305351\n",
            "Epoch: 015, Loss: 0.0152, Train: 1.0000, Test: 0.7687\n",
            "Early stopping:  0.029566197855163612\n",
            "Epoch: 016, Loss: 0.0102, Train: 1.0000, Test: 0.7677\n",
            "Early stopping:  0.0183010333662394\n",
            "Epoch: 017, Loss: 0.0071, Train: 1.0000, Test: 0.7673\n",
            "Early stopping:  0.01145494093502172\n",
            "Epoch: 018, Loss: 0.0050, Train: 1.0000, Test: 0.7686\n",
            "Early stopping:  0.007232707439286376\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.75      0.75       397\n",
            "             ecology       0.83      0.80      0.81       884\n",
            "            economic       0.82      0.70      0.75      1404\n",
            "          geophysics       0.95      0.77      0.85      1197\n",
            "  gravitional_theory       0.54      0.96      0.69       125\n",
            "               hydro       0.70      0.81      0.75       350\n",
            "                math       0.75      0.78      0.76      1334\n",
            "              metals       0.68      0.91      0.78       196\n",
            "          networking       0.72      0.81      0.76       340\n",
            "        neuroscience       0.93      0.97      0.95       302\n",
            "        oceanography       0.85      0.83      0.84       985\n",
            "             politic       0.78      0.71      0.74       598\n",
            "           sociology       0.69      0.68      0.68       734\n",
            "software_engineering       0.84      0.76      0.80       519\n",
            "          statistics       0.59      0.78      0.67       642\n",
            "    theory_computing       0.57      0.72      0.64       437\n",
            "\n",
            "            accuracy                           0.77     10444\n",
            "           macro avg       0.75      0.79      0.76     10444\n",
            "        weighted avg       0.78      0.77      0.77     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7837, Train: 0.6625, Test: 0.4715\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4935, Train: 0.9000, Test: 0.6672\n",
            "Early stopping:  0.2051776020467939\n",
            "Epoch: 003, Loss: 2.1393, Train: 0.9500, Test: 0.7113\n",
            "Early stopping:  0.32270871462729755\n",
            "Epoch: 004, Loss: 1.7319, Train: 0.9500, Test: 0.7186\n",
            "Early stopping:  0.45436652659240384\n",
            "Epoch: 005, Loss: 1.3204, Train: 0.9625, Test: 0.7290\n",
            "Early stopping:  0.5846004824681054\n",
            "Epoch: 006, Loss: 0.9450, Train: 0.9875, Test: 0.7383\n",
            "Early stopping:  0.6193831979564801\n",
            "Epoch: 007, Loss: 0.6353, Train: 0.9875, Test: 0.7432\n",
            "Early stopping:  0.6009269858956078\n",
            "Epoch: 008, Loss: 0.4055, Train: 1.0000, Test: 0.7481\n",
            "Early stopping:  0.5309071328571313\n",
            "Epoch: 009, Loss: 0.2509, Train: 1.0000, Test: 0.7525\n",
            "Early stopping:  0.42919595021411316\n",
            "Epoch: 010, Loss: 0.1535, Train: 1.0000, Test: 0.7583\n",
            "Early stopping:  0.318192055791477\n",
            "Epoch: 011, Loss: 0.0940, Train: 1.0000, Test: 0.7601\n",
            "Early stopping:  0.217709323507937\n",
            "Epoch: 012, Loss: 0.0579, Train: 1.0000, Test: 0.7595\n",
            "Early stopping:  0.13973035328926628\n",
            "Epoch: 013, Loss: 0.0359, Train: 1.0000, Test: 0.7595\n",
            "Early stopping:  0.08636292824223121\n",
            "Epoch: 014, Loss: 0.0226, Train: 1.0000, Test: 0.7586\n",
            "Early stopping:  0.05257072919876578\n",
            "Epoch: 015, Loss: 0.0145, Train: 1.0000, Test: 0.7563\n",
            "Early stopping:  0.031935822650911924\n",
            "Epoch: 016, Loss: 0.0097, Train: 1.0000, Test: 0.7547\n",
            "Early stopping:  0.019397809123762048\n",
            "Epoch: 017, Loss: 0.0067, Train: 1.0000, Test: 0.7523\n",
            "Early stopping:  0.011750013130396034\n",
            "Epoch: 018, Loss: 0.0048, Train: 1.0000, Test: 0.7512\n",
            "Early stopping:  0.007124006803753063\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.65      0.71       397\n",
            "             ecology       0.77      0.77      0.77       884\n",
            "            economic       0.81      0.70      0.75      1404\n",
            "          geophysics       0.91      0.82      0.86      1197\n",
            "  gravitional_theory       0.79      0.94      0.86       125\n",
            "               hydro       0.70      0.77      0.73       350\n",
            "                math       0.87      0.67      0.76      1334\n",
            "              metals       0.56      0.88      0.69       196\n",
            "          networking       0.83      0.89      0.86       340\n",
            "        neuroscience       0.66      0.99      0.79       302\n",
            "        oceanography       0.81      0.80      0.80       985\n",
            "             politic       0.68      0.78      0.73       598\n",
            "           sociology       0.62      0.51      0.56       734\n",
            "software_engineering       0.78      0.80      0.79       519\n",
            "          statistics       0.55      0.84      0.67       642\n",
            "    theory_computing       0.66      0.70      0.68       437\n",
            "\n",
            "            accuracy                           0.75     10444\n",
            "           macro avg       0.74      0.78      0.75     10444\n",
            "        weighted avg       0.77      0.75      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7938, Train: 0.8250, Test: 0.5057\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5282, Train: 0.9625, Test: 0.7333\n",
            "Early stopping:  0.1877672446891257\n",
            "Epoch: 003, Loss: 2.1974, Train: 0.9500, Test: 0.7479\n",
            "Early stopping:  0.2987667507725053\n",
            "Epoch: 004, Loss: 1.8051, Train: 0.9500, Test: 0.7469\n",
            "Early stopping:  0.427194970959024\n",
            "Epoch: 005, Loss: 1.3989, Train: 0.9625, Test: 0.7472\n",
            "Early stopping:  0.5574073048559296\n",
            "Epoch: 006, Loss: 1.0179, Train: 1.0000, Test: 0.7439\n",
            "Early stopping:  0.604226473847737\n",
            "Epoch: 007, Loss: 0.6963, Train: 1.0000, Test: 0.7418\n",
            "Early stopping:  0.5997011064100808\n",
            "Epoch: 008, Loss: 0.4525, Train: 1.0000, Test: 0.7361\n",
            "Early stopping:  0.5413306526564945\n",
            "Epoch: 009, Loss: 0.2848, Train: 1.0000, Test: 0.7317\n",
            "Early stopping:  0.44682174395625\n",
            "Epoch: 010, Loss: 0.1767, Train: 1.0000, Test: 0.7331\n",
            "Early stopping:  0.3378543348775092\n",
            "Epoch: 011, Loss: 0.1093, Train: 1.0000, Test: 0.7363\n",
            "Early stopping:  0.23582307629008997\n",
            "Epoch: 012, Loss: 0.0679, Train: 1.0000, Test: 0.7371\n",
            "Early stopping:  0.15449762733857084\n",
            "Epoch: 013, Loss: 0.0427, Train: 1.0000, Test: 0.7375\n",
            "Early stopping:  0.0972763889460588\n",
            "Epoch: 014, Loss: 0.0272, Train: 1.0000, Test: 0.7363\n",
            "Early stopping:  0.06003276450224431\n",
            "Epoch: 015, Loss: 0.0178, Train: 1.0000, Test: 0.7349\n",
            "Early stopping:  0.036767280053463476\n",
            "Epoch: 016, Loss: 0.0119, Train: 1.0000, Test: 0.7333\n",
            "Early stopping:  0.02249169853773734\n",
            "Epoch: 017, Loss: 0.0083, Train: 1.0000, Test: 0.7309\n",
            "Early stopping:  0.01380307750387908\n",
            "Epoch: 018, Loss: 0.0059, Train: 1.0000, Test: 0.7296\n",
            "Early stopping:  0.008530127903949428\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.72      0.74       397\n",
            "             ecology       0.81      0.78      0.79       884\n",
            "            economic       0.78      0.72      0.75      1404\n",
            "          geophysics       0.94      0.63      0.75      1197\n",
            "  gravitional_theory       0.73      0.94      0.82       125\n",
            "               hydro       0.63      0.72      0.67       350\n",
            "                math       0.81      0.74      0.77      1334\n",
            "              metals       0.42      0.80      0.55       196\n",
            "          networking       0.74      0.92      0.82       340\n",
            "        neuroscience       0.62      1.00      0.77       302\n",
            "        oceanography       0.79      0.82      0.80       985\n",
            "             politic       0.70      0.79      0.74       598\n",
            "           sociology       0.73      0.52      0.61       734\n",
            "software_engineering       0.84      0.46      0.59       519\n",
            "          statistics       0.65      0.84      0.73       642\n",
            "    theory_computing       0.44      0.73      0.55       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.71      0.76      0.72     10444\n",
            "        weighted avg       0.76      0.73      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7918, Train: 0.7375, Test: 0.4773\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5310, Train: 0.9250, Test: 0.6351\n",
            "Early stopping:  0.1843863930889758\n",
            "Epoch: 003, Loss: 2.2058, Train: 0.9250, Test: 0.6902\n",
            "Early stopping:  0.2935782401224485\n",
            "Epoch: 004, Loss: 1.8229, Train: 0.9500, Test: 0.7161\n",
            "Early stopping:  0.4187035942337259\n",
            "Epoch: 005, Loss: 1.4286, Train: 0.9500, Test: 0.7271\n",
            "Early stopping:  0.5448219744229964\n",
            "Epoch: 006, Loss: 1.0576, Train: 0.9625, Test: 0.7306\n",
            "Early stopping:  0.5891174013328226\n",
            "Epoch: 007, Loss: 0.7390, Train: 0.9750, Test: 0.7346\n",
            "Early stopping:  0.5852919071392976\n",
            "Epoch: 008, Loss: 0.4925, Train: 0.9875, Test: 0.7385\n",
            "Early stopping:  0.5318420355560353\n",
            "Epoch: 009, Loss: 0.3191, Train: 0.9875, Test: 0.7442\n",
            "Early stopping:  0.4446220770114122\n",
            "Epoch: 010, Loss: 0.2044, Train: 1.0000, Test: 0.7450\n",
            "Early stopping:  0.3422952167851626\n",
            "Epoch: 011, Loss: 0.1306, Train: 1.0000, Test: 0.7432\n",
            "Early stopping:  0.24405271913194998\n",
            "Epoch: 012, Loss: 0.0839, Train: 1.0000, Test: 0.7441\n",
            "Early stopping:  0.16389865972356357\n",
            "Epoch: 013, Loss: 0.0544, Train: 1.0000, Test: 0.7444\n",
            "Early stopping:  0.1061630184676981\n",
            "Epoch: 014, Loss: 0.0357, Train: 1.0000, Test: 0.7437\n",
            "Early stopping:  0.06761669237995444\n",
            "Epoch: 015, Loss: 0.0237, Train: 1.0000, Test: 0.7412\n",
            "Early stopping:  0.04282959192490701\n",
            "Epoch: 016, Loss: 0.0160, Train: 1.0000, Test: 0.7406\n",
            "Early stopping:  0.02717610556309484\n",
            "Epoch: 017, Loss: 0.0111, Train: 1.0000, Test: 0.7396\n",
            "Early stopping:  0.01735031015216264\n",
            "Epoch: 018, Loss: 0.0078, Train: 1.0000, Test: 0.7379\n",
            "Early stopping:  0.011150802042435458\n",
            "Epoch: 019, Loss: 0.0057, Train: 1.0000, Test: 0.7378\n",
            "Early stopping:  0.0072114673125897874\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.77      0.76       397\n",
            "             ecology       0.81      0.72      0.76       884\n",
            "            economic       0.73      0.55      0.63      1404\n",
            "          geophysics       0.90      0.82      0.86      1197\n",
            "  gravitional_theory       0.85      0.94      0.89       125\n",
            "               hydro       0.59      0.71      0.64       350\n",
            "                math       0.91      0.72      0.80      1334\n",
            "              metals       0.51      0.93      0.66       196\n",
            "          networking       0.81      0.86      0.84       340\n",
            "        neuroscience       0.80      0.99      0.88       302\n",
            "        oceanography       0.82      0.76      0.79       985\n",
            "             politic       0.56      0.84      0.68       598\n",
            "           sociology       0.51      0.65      0.57       734\n",
            "software_engineering       0.74      0.91      0.81       519\n",
            "          statistics       0.77      0.66      0.71       642\n",
            "    theory_computing       0.64      0.64      0.64       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.73      0.78      0.75     10444\n",
            "        weighted avg       0.76      0.74      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7771, Train: 0.7000, Test: 0.4904\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4776, Train: 0.8875, Test: 0.6142\n",
            "Early stopping:  0.21178892529160678\n",
            "Epoch: 003, Loss: 2.1262, Train: 0.9250, Test: 0.6806\n",
            "Early stopping:  0.32578041087066434\n",
            "Epoch: 004, Loss: 1.7293, Train: 0.9750, Test: 0.7185\n",
            "Early stopping:  0.4520245025667316\n",
            "Epoch: 005, Loss: 1.3302, Train: 0.9750, Test: 0.7461\n",
            "Early stopping:  0.5768455849840431\n",
            "Epoch: 006, Loss: 0.9684, Train: 0.9750, Test: 0.7553\n",
            "Early stopping:  0.6032455288506242\n",
            "Epoch: 007, Loss: 0.6695, Train: 0.9875, Test: 0.7551\n",
            "Early stopping:  0.5818882748089509\n",
            "Epoch: 008, Loss: 0.4460, Train: 0.9875, Test: 0.7487\n",
            "Early stopping:  0.5133062170628482\n",
            "Epoch: 009, Loss: 0.2942, Train: 0.9875, Test: 0.7375\n",
            "Early stopping:  0.41551538477248884\n",
            "Epoch: 010, Loss: 0.1945, Train: 1.0000, Test: 0.7266\n",
            "Early stopping:  0.31051796364364287\n",
            "Epoch: 011, Loss: 0.1273, Train: 1.0000, Test: 0.7219\n",
            "Early stopping:  0.21686958898359643\n",
            "Epoch: 012, Loss: 0.0819, Train: 1.0000, Test: 0.7178\n",
            "Early stopping:  0.14535730589709928\n",
            "Epoch: 013, Loss: 0.0529, Train: 1.0000, Test: 0.7160\n",
            "Early stopping:  0.09661850835887487\n",
            "Epoch: 014, Loss: 0.0347, Train: 1.0000, Test: 0.7173\n",
            "Early stopping:  0.06416046057114042\n",
            "Epoch: 015, Loss: 0.0231, Train: 1.0000, Test: 0.7186\n",
            "Early stopping:  0.04175625743720339\n",
            "Epoch: 016, Loss: 0.0156, Train: 1.0000, Test: 0.7227\n",
            "Early stopping:  0.026551590955902347\n",
            "Epoch: 017, Loss: 0.0108, Train: 1.0000, Test: 0.7244\n",
            "Early stopping:  0.016877235733474077\n",
            "Epoch: 018, Loss: 0.0077, Train: 1.0000, Test: 0.7269\n",
            "Early stopping:  0.010829897845967808\n",
            "Epoch: 019, Loss: 0.0057, Train: 1.0000, Test: 0.7293\n",
            "Early stopping:  0.006965508605143452\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.75      0.71       397\n",
            "             ecology       0.66      0.87      0.75       884\n",
            "            economic       0.88      0.48      0.62      1404\n",
            "          geophysics       0.93      0.90      0.91      1197\n",
            "  gravitional_theory       0.87      0.92      0.89       125\n",
            "               hydro       0.50      0.87      0.64       350\n",
            "                math       0.91      0.67      0.77      1334\n",
            "              metals       0.78      0.76      0.77       196\n",
            "          networking       0.85      0.81      0.83       340\n",
            "        neuroscience       0.96      0.90      0.93       302\n",
            "        oceanography       0.90      0.66      0.76       985\n",
            "             politic       0.60      0.67      0.64       598\n",
            "           sociology       0.48      0.76      0.59       734\n",
            "software_engineering       0.63      0.73      0.68       519\n",
            "          statistics       0.64      0.77      0.70       642\n",
            "    theory_computing       0.64      0.67      0.65       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.75      0.76      0.74     10444\n",
            "        weighted avg       0.77      0.73      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7682, Train: 0.8250, Test: 0.5289\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4643, Train: 0.9375, Test: 0.6655\n",
            "Early stopping:  0.21486682534466034\n",
            "Epoch: 003, Loss: 2.1058, Train: 0.9375, Test: 0.7137\n",
            "Early stopping:  0.3315912657460908\n",
            "Epoch: 004, Loss: 1.6994, Train: 0.9500, Test: 0.7285\n",
            "Early stopping:  0.4611688909878759\n",
            "Epoch: 005, Loss: 1.2893, Train: 0.9625, Test: 0.7370\n",
            "Early stopping:  0.5896795609229659\n",
            "Epoch: 006, Loss: 0.9169, Train: 0.9875, Test: 0.7452\n",
            "Early stopping:  0.6185979664697163\n",
            "Epoch: 007, Loss: 0.6147, Train: 1.0000, Test: 0.7522\n",
            "Early stopping:  0.596284661730436\n",
            "Epoch: 008, Loss: 0.3950, Train: 1.0000, Test: 0.7518\n",
            "Early stopping:  0.5227221383010708\n",
            "Epoch: 009, Loss: 0.2488, Train: 1.0000, Test: 0.7502\n",
            "Early stopping:  0.41772493899164126\n",
            "Epoch: 010, Loss: 0.1564, Train: 1.0000, Test: 0.7485\n",
            "Early stopping:  0.30554623468971503\n",
            "Epoch: 011, Loss: 0.0990, Train: 1.0000, Test: 0.7448\n",
            "Early stopping:  0.20714623974253213\n",
            "Epoch: 012, Loss: 0.0633, Train: 1.0000, Test: 0.7406\n",
            "Early stopping:  0.13313669294396308\n",
            "Epoch: 013, Loss: 0.0411, Train: 1.0000, Test: 0.7383\n",
            "Early stopping:  0.08335657555943228\n",
            "Epoch: 014, Loss: 0.0268, Train: 1.0000, Test: 0.7358\n",
            "Early stopping:  0.051923597720757636\n",
            "Epoch: 015, Loss: 0.0176, Train: 1.0000, Test: 0.7351\n",
            "Early stopping:  0.032568066388227686\n",
            "Epoch: 016, Loss: 0.0117, Train: 1.0000, Test: 0.7339\n",
            "Early stopping:  0.020670245913879445\n",
            "Epoch: 017, Loss: 0.0081, Train: 1.0000, Test: 0.7327\n",
            "Early stopping:  0.013237030515981322\n",
            "Epoch: 018, Loss: 0.0057, Train: 1.0000, Test: 0.7331\n",
            "Early stopping:  0.008443520227695012\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 13, 15, 13], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.85      0.73      0.79       397\n",
            "             ecology       0.74      0.55      0.63       884\n",
            "            economic       0.73      0.75      0.74      1404\n",
            "          geophysics       0.92      0.90      0.91      1197\n",
            "  gravitional_theory       0.77      0.88      0.82       125\n",
            "               hydro       0.56      0.83      0.67       350\n",
            "                math       0.87      0.71      0.79      1334\n",
            "              metals       0.77      0.75      0.76       196\n",
            "          networking       0.75      0.91      0.82       340\n",
            "        neuroscience       0.71      0.99      0.83       302\n",
            "        oceanography       0.67      0.78      0.72       985\n",
            "             politic       0.72      0.63      0.67       598\n",
            "           sociology       0.56      0.50      0.53       734\n",
            "software_engineering       0.62      0.88      0.73       519\n",
            "          statistics       0.71      0.81      0.76       642\n",
            "    theory_computing       0.58      0.32      0.41       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.72      0.75      0.72     10444\n",
            "        weighted avg       0.74      0.73      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7732, Train: 0.6875, Test: 0.4426\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5011, Train: 0.8750, Test: 0.6252\n",
            "Early stopping:  0.19233832329122322\n",
            "Epoch: 003, Loss: 2.1590, Train: 0.9125, Test: 0.6665\n",
            "Early stopping:  0.3077281619713822\n",
            "Epoch: 004, Loss: 1.7638, Train: 0.9500, Test: 0.6918\n",
            "Early stopping:  0.43656274180853033\n",
            "Epoch: 005, Loss: 1.3650, Train: 0.9750, Test: 0.7244\n",
            "Early stopping:  0.5634691536435722\n",
            "Epoch: 006, Loss: 0.9956, Train: 1.0000, Test: 0.7490\n",
            "Early stopping:  0.601827878097305\n",
            "Epoch: 007, Loss: 0.6841, Train: 1.0000, Test: 0.7556\n",
            "Early stopping:  0.5885361247863864\n",
            "Epoch: 008, Loss: 0.4484, Train: 1.0000, Test: 0.7469\n",
            "Early stopping:  0.5261807886821633\n",
            "Epoch: 009, Loss: 0.2872, Train: 1.0000, Test: 0.7404\n",
            "Early stopping:  0.43241479667720717\n",
            "Epoch: 010, Loss: 0.1813, Train: 1.0000, Test: 0.7409\n",
            "Early stopping:  0.32680036324088363\n",
            "Epoch: 011, Loss: 0.1123, Train: 1.0000, Test: 0.7392\n",
            "Early stopping:  0.22910367641542803\n",
            "Epoch: 012, Loss: 0.0690, Train: 1.0000, Test: 0.7389\n",
            "Early stopping:  0.15211642145591397\n",
            "Epoch: 013, Loss: 0.0432, Train: 1.0000, Test: 0.7373\n",
            "Early stopping:  0.09813604916619358\n",
            "Epoch: 014, Loss: 0.0279, Train: 1.0000, Test: 0.7329\n",
            "Early stopping:  0.06176348207734693\n",
            "Epoch: 015, Loss: 0.0186, Train: 1.0000, Test: 0.7300\n",
            "Early stopping:  0.037668730491830234\n",
            "Epoch: 016, Loss: 0.0128, Train: 1.0000, Test: 0.7266\n",
            "Early stopping:  0.022574196751120483\n",
            "Epoch: 017, Loss: 0.0090, Train: 1.0000, Test: 0.7241\n",
            "Early stopping:  0.013686764218268853\n",
            "Epoch: 018, Loss: 0.0066, Train: 1.0000, Test: 0.7223\n",
            "Early stopping:  0.008519690668575013\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.74      0.75       397\n",
            "             ecology       0.63      0.93      0.75       884\n",
            "            economic       0.78      0.69      0.73      1404\n",
            "          geophysics       0.92      0.88      0.90      1197\n",
            "  gravitional_theory       0.24      0.97      0.39       125\n",
            "               hydro       0.81      0.77      0.79       350\n",
            "                math       0.89      0.50      0.64      1334\n",
            "              metals       0.61      0.89      0.72       196\n",
            "          networking       0.60      0.92      0.72       340\n",
            "        neuroscience       0.92      0.96      0.94       302\n",
            "        oceanography       0.92      0.54      0.68       985\n",
            "             politic       0.60      0.82      0.70       598\n",
            "           sociology       0.64      0.42      0.51       734\n",
            "software_engineering       0.83      0.81      0.82       519\n",
            "          statistics       0.67      0.76      0.71       642\n",
            "    theory_computing       0.61      0.76      0.68       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.71      0.77      0.71     10444\n",
            "        weighted avg       0.77      0.72      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7768, Train: 0.7375, Test: 0.4503\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5029, Train: 0.9125, Test: 0.7069\n",
            "Early stopping:  0.19364487559506094\n",
            "Epoch: 003, Loss: 2.1656, Train: 0.9500, Test: 0.7317\n",
            "Early stopping:  0.3061266172818194\n",
            "Epoch: 004, Loss: 1.7787, Train: 0.9500, Test: 0.7399\n",
            "Early stopping:  0.4313371121176817\n",
            "Epoch: 005, Loss: 1.3848, Train: 0.9375, Test: 0.7478\n",
            "Early stopping:  0.556105841214714\n",
            "Epoch: 006, Loss: 1.0177, Train: 0.9375, Test: 0.7553\n",
            "Early stopping:  0.5932976854393015\n",
            "Epoch: 007, Loss: 0.7093, Train: 0.9500, Test: 0.7564\n",
            "Early stopping:  0.5814656157383528\n",
            "Epoch: 008, Loss: 0.4777, Train: 0.9750, Test: 0.7560\n",
            "Early stopping:  0.5208105580486639\n",
            "Epoch: 009, Loss: 0.3186, Train: 0.9875, Test: 0.7555\n",
            "Early stopping:  0.42765190329881936\n",
            "Epoch: 010, Loss: 0.2121, Train: 1.0000, Test: 0.7567\n",
            "Early stopping:  0.3228918734591516\n",
            "Epoch: 011, Loss: 0.1396, Train: 1.0000, Test: 0.7566\n",
            "Early stopping:  0.22768106357224727\n",
            "Epoch: 012, Loss: 0.0908, Train: 1.0000, Test: 0.7553\n",
            "Early stopping:  0.15451987149212984\n",
            "Epoch: 013, Loss: 0.0592, Train: 1.0000, Test: 0.7534\n",
            "Early stopping:  0.1038682439013753\n",
            "Epoch: 014, Loss: 0.0392, Train: 1.0000, Test: 0.7530\n",
            "Early stopping:  0.06934562374161814\n",
            "Epoch: 015, Loss: 0.0263, Train: 1.0000, Test: 0.7534\n",
            "Early stopping:  0.04539001558096145\n",
            "Epoch: 016, Loss: 0.0179, Train: 1.0000, Test: 0.7529\n",
            "Early stopping:  0.02916545141185951\n",
            "Epoch: 017, Loss: 0.0125, Train: 1.0000, Test: 0.7515\n",
            "Early stopping:  0.01872456922083357\n",
            "Epoch: 018, Loss: 0.0088, Train: 1.0000, Test: 0.7497\n",
            "Early stopping:  0.012152758519609601\n",
            "Epoch: 019, Loss: 0.0064, Train: 1.0000, Test: 0.7489\n",
            "Early stopping:  0.007952662255080135\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.56      0.73      0.63       397\n",
            "             ecology       0.82      0.64      0.72       884\n",
            "            economic       0.80      0.72      0.76      1404\n",
            "          geophysics       0.92      0.85      0.88      1197\n",
            "  gravitional_theory       0.90      0.86      0.88       125\n",
            "               hydro       0.67      0.70      0.68       350\n",
            "                math       0.85      0.76      0.80      1334\n",
            "              metals       0.49      0.95      0.65       196\n",
            "          networking       0.65      0.95      0.77       340\n",
            "        neuroscience       0.85      0.99      0.91       302\n",
            "        oceanography       0.72      0.85      0.78       985\n",
            "             politic       0.70      0.68      0.69       598\n",
            "           sociology       0.63      0.73      0.68       734\n",
            "software_engineering       0.83      0.49      0.62       519\n",
            "          statistics       0.78      0.69      0.73       642\n",
            "    theory_computing       0.59      0.66      0.62       437\n",
            "\n",
            "            accuracy                           0.75     10444\n",
            "           macro avg       0.73      0.77      0.74     10444\n",
            "        weighted avg       0.77      0.75      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7781, Train: 0.8250, Test: 0.4606\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5024, Train: 0.9125, Test: 0.6498\n",
            "Early stopping:  0.1949790762315218\n",
            "Epoch: 003, Loss: 2.1732, Train: 0.9250, Test: 0.7003\n",
            "Early stopping:  0.30285636468104454\n",
            "Epoch: 004, Loss: 1.7894, Train: 0.9375, Test: 0.7225\n",
            "Early stopping:  0.4265727043773741\n",
            "Epoch: 005, Loss: 1.3902, Train: 0.9375, Test: 0.7378\n",
            "Early stopping:  0.5531471011511137\n",
            "Epoch: 006, Loss: 1.0197, Train: 0.9375, Test: 0.7486\n",
            "Early stopping:  0.5929716453530601\n",
            "Epoch: 007, Loss: 0.7115, Train: 0.9625, Test: 0.7489\n",
            "Early stopping:  0.5845604155298616\n",
            "Epoch: 008, Loss: 0.4798, Train: 0.9750, Test: 0.7442\n",
            "Early stopping:  0.524185978679843\n",
            "Epoch: 009, Loss: 0.3200, Train: 0.9750, Test: 0.7319\n",
            "Early stopping:  0.42898266199010404\n",
            "Epoch: 010, Loss: 0.2129, Train: 1.0000, Test: 0.7272\n",
            "Early stopping:  0.3233311813997965\n",
            "Epoch: 011, Loss: 0.1398, Train: 1.0000, Test: 0.7280\n",
            "Early stopping:  0.22849368948707605\n",
            "Epoch: 012, Loss: 0.0909, Train: 1.0000, Test: 0.7343\n",
            "Early stopping:  0.1553700314675402\n",
            "Epoch: 013, Loss: 0.0593, Train: 1.0000, Test: 0.7401\n",
            "Early stopping:  0.10442354184443939\n",
            "Epoch: 014, Loss: 0.0392, Train: 1.0000, Test: 0.7425\n",
            "Early stopping:  0.06966065275922777\n",
            "Epoch: 015, Loss: 0.0265, Train: 1.0000, Test: 0.7449\n",
            "Early stopping:  0.04542699157275826\n",
            "Epoch: 016, Loss: 0.0185, Train: 1.0000, Test: 0.7450\n",
            "Early stopping:  0.029018599889399886\n",
            "Epoch: 017, Loss: 0.0131, Train: 1.0000, Test: 0.7435\n",
            "Early stopping:  0.018467456210526003\n",
            "Epoch: 018, Loss: 0.0093, Train: 1.0000, Test: 0.7413\n",
            "Early stopping:  0.011895326351948999\n",
            "Epoch: 019, Loss: 0.0068, Train: 1.0000, Test: 0.7406\n",
            "Early stopping:  0.00786672481397181\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.80      0.71       397\n",
            "             ecology       0.76      0.86      0.80       884\n",
            "            economic       0.75      0.58      0.66      1404\n",
            "          geophysics       0.93      0.83      0.88      1197\n",
            "  gravitional_theory       0.86      0.92      0.89       125\n",
            "               hydro       0.77      0.76      0.76       350\n",
            "                math       0.90      0.66      0.76      1334\n",
            "              metals       0.56      0.90      0.69       196\n",
            "          networking       0.82      0.77      0.79       340\n",
            "        neuroscience       0.89      0.95      0.92       302\n",
            "        oceanography       0.88      0.74      0.80       985\n",
            "             politic       0.53      0.72      0.61       598\n",
            "           sociology       0.50      0.64      0.56       734\n",
            "software_engineering       0.84      0.80      0.82       519\n",
            "          statistics       0.69      0.77      0.73       642\n",
            "    theory_computing       0.56      0.71      0.63       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.74      0.78      0.75     10444\n",
            "        weighted avg       0.77      0.74      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7797, Train: 0.7500, Test: 0.4542\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5256, Train: 0.8625, Test: 0.6764\n",
            "Early stopping:  0.17968179327079578\n",
            "Epoch: 003, Loss: 2.1947, Train: 0.8750, Test: 0.6953\n",
            "Early stopping:  0.2933542867319122\n",
            "Epoch: 004, Loss: 1.8060, Train: 0.9250, Test: 0.7001\n",
            "Early stopping:  0.4216198659221871\n",
            "Epoch: 005, Loss: 1.4079, Train: 0.9375, Test: 0.7138\n",
            "Early stopping:  0.5496295620448151\n",
            "Epoch: 006, Loss: 1.0361, Train: 0.9625, Test: 0.7314\n",
            "Early stopping:  0.5957042753631444\n",
            "Epoch: 007, Loss: 0.7185, Train: 0.9750, Test: 0.7410\n",
            "Early stopping:  0.589076081166035\n",
            "Epoch: 008, Loss: 0.4742, Train: 0.9750, Test: 0.7384\n",
            "Early stopping:  0.5324315429233156\n",
            "Epoch: 009, Loss: 0.3059, Train: 0.9875, Test: 0.7322\n",
            "Early stopping:  0.44202702678207073\n",
            "Epoch: 010, Loss: 0.1966, Train: 0.9875, Test: 0.7279\n",
            "Early stopping:  0.3371872317463638\n",
            "Epoch: 011, Loss: 0.1258, Train: 1.0000, Test: 0.7218\n",
            "Early stopping:  0.23764327395606455\n",
            "Epoch: 012, Loss: 0.0800, Train: 1.0000, Test: 0.7210\n",
            "Early stopping:  0.1578200117278283\n",
            "Epoch: 013, Loss: 0.0512, Train: 1.0000, Test: 0.7218\n",
            "Early stopping:  0.10210353897951797\n",
            "Epoch: 014, Loss: 0.0339, Train: 1.0000, Test: 0.7231\n",
            "Early stopping:  0.06542689183744343\n",
            "Epoch: 015, Loss: 0.0233, Train: 1.0000, Test: 0.7235\n",
            "Early stopping:  0.041219955216730095\n",
            "Epoch: 016, Loss: 0.0164, Train: 1.0000, Test: 0.7215\n",
            "Early stopping:  0.025474537690383166\n",
            "Epoch: 017, Loss: 0.0116, Train: 1.0000, Test: 0.7198\n",
            "Early stopping:  0.015788853899145187\n",
            "Epoch: 018, Loss: 0.0084, Train: 1.0000, Test: 0.7155\n",
            "Early stopping:  0.010195851290436509\n",
            "Epoch: 019, Loss: 0.0062, Train: 1.0000, Test: 0.7133\n",
            "Early stopping:  0.0068355736178509026\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.54      0.59       397\n",
            "             ecology       0.81      0.83      0.82       884\n",
            "            economic       0.67      0.33      0.45      1404\n",
            "          geophysics       0.87      0.91      0.89      1197\n",
            "  gravitional_theory       0.92      0.87      0.90       125\n",
            "               hydro       0.65      0.80      0.72       350\n",
            "                math       0.87      0.74      0.80      1334\n",
            "              metals       0.60      0.82      0.69       196\n",
            "          networking       0.71      0.92      0.80       340\n",
            "        neuroscience       0.90      0.96      0.93       302\n",
            "        oceanography       0.83      0.80      0.81       985\n",
            "             politic       0.64      0.82      0.72       598\n",
            "           sociology       0.58      0.51      0.54       734\n",
            "software_engineering       0.78      0.72      0.75       519\n",
            "          statistics       0.40      0.73      0.52       642\n",
            "    theory_computing       0.60      0.73      0.66       437\n",
            "\n",
            "            accuracy                           0.71     10444\n",
            "           macro avg       0.72      0.75      0.72     10444\n",
            "        weighted avg       0.73      0.71      0.71     10444\n",
            "\n",
            "time: 8.54 s (started: 2024-08-16 14:11:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "bGroweTluoaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0f414f-55cb-4e33-a09f-f2d740279341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 404 ms (started: 2024-08-16 14:11:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 10 ❎"
      ],
      "metadata": {
        "id": "71eypyB-tAcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "n46FQu3BuuB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmHaBmMbuuCE",
        "outputId": "d6f40154-9116-414a-d99f-e66e0b6d4783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.7920, Train: 0.6250, Test: 0.5451\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5343, Train: 0.7375, Test: 0.6108\n",
            "Early stopping:  1.5964754171223337\n",
            "Epoch: 003, Loss: 1.2421, Train: 0.9062, Test: 0.7360\n",
            "Early stopping:  1.7967398700262387\n",
            "Epoch: 004, Loss: 0.4119, Train: 0.9187, Test: 0.7469\n",
            "Early stopping:  1.909373999572375\n",
            "Epoch: 005, Loss: 0.3919, Train: 0.9000, Test: 0.7214\n",
            "Early stopping:  1.849624808943473\n",
            "Epoch: 006, Loss: 0.3520, Train: 0.9437, Test: 0.7836\n",
            "Early stopping:  0.941697142460063\n",
            "Epoch: 007, Loss: 0.1570, Train: 0.9625, Test: 0.7837\n",
            "Early stopping:  0.42102660103833894\n",
            "Epoch: 008, Loss: 0.1567, Train: 0.9812, Test: 0.7750\n",
            "Early stopping:  0.12696667045238932\n",
            "Epoch: 009, Loss: 0.1341, Train: 0.9750, Test: 0.7619\n",
            "Early stopping:  0.12313510703427856\n",
            "Epoch: 010, Loss: 0.1027, Train: 0.9938, Test: 0.7532\n",
            "Early stopping:  0.09839461089058668\n",
            "Epoch: 011, Loss: 0.0548, Train: 1.0000, Test: 0.7495\n",
            "Early stopping:  0.0431916106533058\n",
            "Epoch: 012, Loss: 0.0182, Train: 1.0000, Test: 0.7434\n",
            "Early stopping:  0.05678296255841171\n",
            "Epoch: 013, Loss: 0.0111, Train: 1.0000, Test: 0.7446\n",
            "Early stopping:  0.053371755960372795\n",
            "Epoch: 014, Loss: 0.0101, Train: 1.0000, Test: 0.7479\n",
            "Early stopping:  0.03985516901326984\n",
            "Epoch: 015, Loss: 0.0092, Train: 1.0000, Test: 0.7464\n",
            "Early stopping:  0.019388102446564054\n",
            "Epoch: 016, Loss: 0.0112, Train: 0.9875, Test: 0.7447\n",
            "Early stopping:  0.00360078139328371\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.80      0.76       392\n",
            "             ecology       0.77      0.81      0.79       879\n",
            "            economic       0.75      0.66      0.70      1399\n",
            "          geophysics       0.92      0.90      0.91      1192\n",
            "  gravitional_theory       0.55      0.97      0.70       120\n",
            "               hydro       0.50      0.87      0.63       345\n",
            "                math       0.74      0.74      0.74      1329\n",
            "              metals       0.81      0.82      0.82       191\n",
            "          networking       0.63      0.94      0.76       335\n",
            "        neuroscience       0.92      0.95      0.93       297\n",
            "        oceanography       0.88      0.72      0.79       980\n",
            "             politic       0.63      0.82      0.71       593\n",
            "           sociology       0.63      0.55      0.59       729\n",
            "software_engineering       0.83      0.78      0.80       514\n",
            "          statistics       0.76      0.51      0.62       637\n",
            "    theory_computing       0.64      0.51      0.57       432\n",
            "\n",
            "            accuracy                           0.74     10364\n",
            "           macro avg       0.73      0.77      0.74     10364\n",
            "        weighted avg       0.76      0.74      0.74     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5737, Train: 0.6188, Test: 0.5868\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.3336, Train: 0.8625, Test: 0.7066\n",
            "Early stopping:  2.2910637413641757\n",
            "Epoch: 003, Loss: 0.5088, Train: 0.8625, Test: 0.7073\n",
            "Early stopping:  2.148689952368428\n",
            "Epoch: 004, Loss: 0.5539, Train: 0.9000, Test: 0.7324\n",
            "Early stopping:  1.925042415171386\n",
            "Epoch: 005, Loss: 0.3222, Train: 0.9125, Test: 0.6947\n",
            "Early stopping:  1.7840422666448186\n",
            "Epoch: 006, Loss: 0.2338, Train: 0.9563, Test: 0.7216\n",
            "Early stopping:  0.43573769027003906\n",
            "Epoch: 007, Loss: 0.1791, Train: 0.9563, Test: 0.7471\n",
            "Early stopping:  0.1657044644045975\n",
            "Epoch: 008, Loss: 0.1345, Train: 0.9688, Test: 0.7482\n",
            "Early stopping:  0.16595907892752945\n",
            "Epoch: 009, Loss: 0.1068, Train: 0.9750, Test: 0.7547\n",
            "Early stopping:  0.08568508251096714\n",
            "Epoch: 010, Loss: 0.0837, Train: 0.9812, Test: 0.7448\n",
            "Early stopping:  0.05987602586600091\n",
            "Epoch: 011, Loss: 0.0632, Train: 0.9938, Test: 0.7407\n",
            "Early stopping:  0.04529784510306619\n",
            "Epoch: 012, Loss: 0.0238, Train: 0.9938, Test: 0.7285\n",
            "Early stopping:  0.042182911617805394\n",
            "Epoch: 013, Loss: 0.0224, Train: 0.9938, Test: 0.7293\n",
            "Early stopping:  0.03703034239244219\n",
            "Epoch: 014, Loss: 0.0108, Train: 0.9875, Test: 0.7324\n",
            "Early stopping:  0.03112324877019685\n",
            "Epoch: 015, Loss: 0.0120, Train: 1.0000, Test: 0.7307\n",
            "Early stopping:  0.0214063051164749\n",
            "Epoch: 016, Loss: 0.0081, Train: 1.0000, Test: 0.7324\n",
            "Early stopping:  0.0071807219558992835\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 14, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.72      0.74       392\n",
            "             ecology       0.76      0.85      0.80       879\n",
            "            economic       0.77      0.57      0.66      1399\n",
            "          geophysics       0.92      0.86      0.89      1192\n",
            "  gravitional_theory       0.56      0.91      0.69       120\n",
            "               hydro       0.63      0.85      0.72       345\n",
            "                math       0.82      0.59      0.68      1329\n",
            "              metals       0.56      0.94      0.71       191\n",
            "          networking       0.91      0.72      0.81       335\n",
            "        neuroscience       0.95      0.94      0.94       297\n",
            "        oceanography       0.92      0.74      0.82       980\n",
            "             politic       0.73      0.69      0.71       593\n",
            "           sociology       0.52      0.74      0.61       729\n",
            "software_engineering       0.84      0.70      0.77       514\n",
            "          statistics       0.48      0.84      0.61       637\n",
            "    theory_computing       0.65      0.65      0.65       432\n",
            "\n",
            "            accuracy                           0.73     10364\n",
            "           macro avg       0.74      0.77      0.74     10364\n",
            "        weighted avg       0.77      0.73      0.74     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.0986, Train: 0.6750, Test: 0.5859\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1961, Train: 0.8438, Test: 0.7270\n",
            "Early stopping:  2.052386981183953\n",
            "Epoch: 003, Loss: 0.9049, Train: 0.8750, Test: 0.7017\n",
            "Early stopping:  1.7658571786717763\n",
            "Epoch: 004, Loss: 0.9637, Train: 0.9125, Test: 0.7371\n",
            "Early stopping:  1.543667336069582\n",
            "Epoch: 005, Loss: 0.5671, Train: 0.9187, Test: 0.7530\n",
            "Early stopping:  1.4445350829907944\n",
            "Epoch: 006, Loss: 0.2714, Train: 0.9375, Test: 0.7318\n",
            "Early stopping:  0.3627821310953682\n",
            "Epoch: 007, Loss: 0.1717, Train: 0.9313, Test: 0.7325\n",
            "Early stopping:  0.35871920323969475\n",
            "Epoch: 008, Loss: 0.1752, Train: 0.9812, Test: 0.7529\n",
            "Early stopping:  0.33924495054730186\n",
            "Epoch: 009, Loss: 0.0497, Train: 0.9812, Test: 0.7568\n",
            "Early stopping:  0.1954503068814598\n",
            "Epoch: 010, Loss: 0.0569, Train: 0.9812, Test: 0.7553\n",
            "Early stopping:  0.09281125250043544\n",
            "Epoch: 011, Loss: 0.0662, Train: 0.9875, Test: 0.7515\n",
            "Early stopping:  0.06374779064477652\n",
            "Epoch: 012, Loss: 0.0443, Train: 0.9938, Test: 0.7486\n",
            "Early stopping:  0.054709180747980986\n",
            "Epoch: 013, Loss: 0.0152, Train: 1.0000, Test: 0.7430\n",
            "Early stopping:  0.01932469166793243\n",
            "Epoch: 014, Loss: 0.0073, Train: 1.0000, Test: 0.7326\n",
            "Early stopping:  0.02576510770796637\n",
            "Epoch: 015, Loss: 0.0160, Train: 0.9875, Test: 0.7301\n",
            "Early stopping:  0.024728722739863165\n",
            "Epoch: 016, Loss: 0.0236, Train: 1.0000, Test: 0.7361\n",
            "Early stopping:  0.014107659184887577\n",
            "Epoch: 017, Loss: 0.0076, Train: 1.0000, Test: 0.7394\n",
            "Early stopping:  0.006767147093959342\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.73      0.72       392\n",
            "             ecology       0.76      0.69      0.72       879\n",
            "            economic       0.79      0.68      0.73      1399\n",
            "          geophysics       0.93      0.81      0.86      1192\n",
            "  gravitional_theory       0.90      0.90      0.90       120\n",
            "               hydro       0.58      0.74      0.65       345\n",
            "                math       0.87      0.64      0.74      1329\n",
            "              metals       0.34      0.95      0.50       191\n",
            "          networking       0.89      0.81      0.85       335\n",
            "        neuroscience       0.95      0.94      0.94       297\n",
            "        oceanography       0.74      0.76      0.75       980\n",
            "             politic       0.71      0.74      0.72       593\n",
            "           sociology       0.57      0.60      0.59       729\n",
            "software_engineering       0.79      0.89      0.84       514\n",
            "          statistics       0.59      0.87      0.71       637\n",
            "    theory_computing       0.81      0.66      0.73       432\n",
            "\n",
            "            accuracy                           0.74     10364\n",
            "           macro avg       0.75      0.78      0.75     10364\n",
            "        weighted avg       0.77      0.74      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.8681, Train: 0.6438, Test: 0.6318\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.9316, Train: 0.7937, Test: 0.6195\n",
            "Early stopping:  1.3693638364671974\n",
            "Epoch: 003, Loss: 1.5751, Train: 0.8562, Test: 0.6482\n",
            "Early stopping:  1.2339315766525525\n",
            "Epoch: 004, Loss: 0.8193, Train: 0.8812, Test: 0.7298\n",
            "Early stopping:  1.2986985367521673\n",
            "Epoch: 005, Loss: 0.3182, Train: 0.9313, Test: 0.7063\n",
            "Early stopping:  1.365186990807573\n",
            "Epoch: 006, Loss: 0.2977, Train: 0.9625, Test: 0.7322\n",
            "Early stopping:  0.7396762094009957\n",
            "Epoch: 007, Loss: 0.1657, Train: 0.9563, Test: 0.7360\n",
            "Early stopping:  0.5813885888329335\n",
            "Epoch: 008, Loss: 0.1077, Train: 0.9750, Test: 0.7485\n",
            "Early stopping:  0.281197345707919\n",
            "Epoch: 009, Loss: 0.0683, Train: 0.9812, Test: 0.7558\n",
            "Early stopping:  0.11203890796794663\n",
            "Epoch: 010, Loss: 0.0434, Train: 0.9875, Test: 0.7552\n",
            "Early stopping:  0.10123119043692179\n",
            "Epoch: 011, Loss: 0.0435, Train: 0.9938, Test: 0.7495\n",
            "Early stopping:  0.05184398774748242\n",
            "Epoch: 012, Loss: 0.0265, Train: 0.9875, Test: 0.7432\n",
            "Early stopping:  0.03158883958337299\n",
            "Epoch: 013, Loss: 0.0251, Train: 0.9875, Test: 0.7428\n",
            "Early stopping:  0.01745442041829267\n",
            "Epoch: 014, Loss: 0.0201, Train: 1.0000, Test: 0.7464\n",
            "Early stopping:  0.010980185104657774\n",
            "Epoch: 015, Loss: 0.0117, Train: 1.0000, Test: 0.7491\n",
            "Early stopping:  0.01167257208634637\n",
            "Epoch: 016, Loss: 0.0067, Train: 1.0000, Test: 0.7493\n",
            "Early stopping:  0.008575718045926725\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.65      0.64      0.64       392\n",
            "             ecology       0.76      0.80      0.78       879\n",
            "            economic       0.76      0.62      0.68      1399\n",
            "          geophysics       0.94      0.81      0.87      1192\n",
            "  gravitional_theory       0.64      0.93      0.76       120\n",
            "               hydro       0.86      0.70      0.77       345\n",
            "                math       0.86      0.67      0.75      1329\n",
            "              metals       0.51      0.91      0.66       191\n",
            "          networking       0.84      0.87      0.86       335\n",
            "        neuroscience       0.89      0.97      0.93       297\n",
            "        oceanography       0.84      0.74      0.79       980\n",
            "             politic       0.83      0.68      0.75       593\n",
            "           sociology       0.62      0.75      0.68       729\n",
            "software_engineering       0.73      0.80      0.76       514\n",
            "          statistics       0.56      0.89      0.69       637\n",
            "    theory_computing       0.56      0.73      0.64       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.74      0.78      0.75     10364\n",
            "        weighted avg       0.77      0.75      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.3760, Train: 0.6750, Test: 0.6773\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2437, Train: 0.7375, Test: 0.5541\n",
            "Early stopping:  2.214915850047785\n",
            "Epoch: 003, Loss: 2.7996, Train: 0.8625, Test: 0.6736\n",
            "Early stopping:  1.5661932861401746\n",
            "Epoch: 004, Loss: 0.5549, Train: 0.8750, Test: 0.7136\n",
            "Early stopping:  1.70370335912898\n",
            "Epoch: 005, Loss: 1.1070, Train: 0.8812, Test: 0.7398\n",
            "Early stopping:  1.5605404601515336\n",
            "Epoch: 006, Loss: 0.9528, Train: 0.9187, Test: 0.7544\n",
            "Early stopping:  0.8601977465923977\n",
            "Epoch: 007, Loss: 0.4976, Train: 0.9563, Test: 0.7688\n",
            "Early stopping:  0.9402492311176109\n",
            "Epoch: 008, Loss: 0.1045, Train: 0.9625, Test: 0.7384\n",
            "Early stopping:  0.3969191687842532\n",
            "Epoch: 009, Loss: 0.2229, Train: 0.9625, Test: 0.7208\n",
            "Early stopping:  0.4407666622175224\n",
            "Epoch: 010, Loss: 0.2996, Train: 0.9625, Test: 0.7121\n",
            "Early stopping:  0.33266702496319955\n",
            "Epoch: 011, Loss: 0.2219, Train: 0.9750, Test: 0.7146\n",
            "Early stopping:  0.14542144868522874\n",
            "Epoch: 012, Loss: 0.1173, Train: 0.9875, Test: 0.7260\n",
            "Early stopping:  0.0816523185082435\n",
            "Epoch: 013, Loss: 0.0315, Train: 0.9938, Test: 0.7391\n",
            "Early stopping:  0.10474602016090169\n",
            "Epoch: 014, Loss: 0.0183, Train: 0.9812, Test: 0.7386\n",
            "Early stopping:  0.12171727639811504\n",
            "Epoch: 015, Loss: 0.0352, Train: 0.9938, Test: 0.7370\n",
            "Early stopping:  0.08598376290101772\n",
            "Epoch: 016, Loss: 0.0498, Train: 0.9938, Test: 0.7350\n",
            "Early stopping:  0.03900954753938828\n",
            "Epoch: 017, Loss: 0.0452, Train: 0.9938, Test: 0.7356\n",
            "Early stopping:  0.012332279302579751\n",
            "Epoch: 018, Loss: 0.0288, Train: 1.0000, Test: 0.7361\n",
            "Early stopping:  0.01263824096695162\n",
            "Epoch: 019, Loss: 0.0096, Train: 1.0000, Test: 0.7339\n",
            "Early stopping:  0.015796445644356415\n",
            "Epoch: 020, Loss: 0.0112, Train: 1.0000, Test: 0.7325\n",
            "Early stopping:  0.01862734147822896\n",
            "Epoch: 021, Loss: 0.0106, Train: 1.0000, Test: 0.7298\n",
            "Early stopping:  0.015633534850912276\n",
            "Epoch: 022, Loss: 0.0072, Train: 1.0000, Test: 0.7277\n",
            "Early stopping:  0.008692385917002856\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.81      0.68      0.74       392\n",
            "             ecology       0.71      0.90      0.79       879\n",
            "            economic       0.68      0.39      0.49      1399\n",
            "          geophysics       0.92      0.93      0.93      1192\n",
            "  gravitional_theory       0.83      0.92      0.87       120\n",
            "               hydro       0.87      0.72      0.79       345\n",
            "                math       0.95      0.57      0.72      1329\n",
            "              metals       0.60      0.93      0.73       191\n",
            "          networking       0.79      0.88      0.83       335\n",
            "        neuroscience       0.95      0.97      0.96       297\n",
            "        oceanography       0.86      0.74      0.79       980\n",
            "             politic       0.49      0.89      0.63       593\n",
            "           sociology       0.48      0.65      0.55       729\n",
            "software_engineering       0.74      0.83      0.78       514\n",
            "          statistics       0.61      0.76      0.68       637\n",
            "    theory_computing       0.67      0.74      0.70       432\n",
            "\n",
            "            accuracy                           0.73     10364\n",
            "           macro avg       0.75      0.78      0.75     10364\n",
            "        weighted avg       0.76      0.73      0.72     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5327, Train: 0.6750, Test: 0.5207\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3286, Train: 0.8562, Test: 0.7119\n",
            "Early stopping:  1.5584938573560743\n",
            "Epoch: 003, Loss: 0.4880, Train: 0.8500, Test: 0.7346\n",
            "Early stopping:  2.025041547058064\n",
            "Epoch: 004, Loss: 0.7802, Train: 0.9125, Test: 0.7416\n",
            "Early stopping:  1.852219267622252\n",
            "Epoch: 005, Loss: 0.3459, Train: 0.9688, Test: 0.7216\n",
            "Early stopping:  1.7725391653965998\n",
            "Epoch: 006, Loss: 0.2028, Train: 0.9688, Test: 0.7018\n",
            "Early stopping:  0.8650326602527695\n",
            "Epoch: 007, Loss: 0.2115, Train: 0.9812, Test: 0.7140\n",
            "Early stopping:  0.2395481725413165\n",
            "Epoch: 008, Loss: 0.1788, Train: 0.9812, Test: 0.7283\n",
            "Early stopping:  0.25254625903325256\n",
            "Epoch: 009, Loss: 0.1229, Train: 0.9938, Test: 0.7459\n",
            "Early stopping:  0.08224636865117861\n",
            "Epoch: 010, Loss: 0.0451, Train: 1.0000, Test: 0.7586\n",
            "Early stopping:  0.0691170026549649\n",
            "Epoch: 011, Loss: 0.0095, Train: 0.9938, Test: 0.7608\n",
            "Early stopping:  0.08578652235279735\n",
            "Epoch: 012, Loss: 0.0217, Train: 0.9875, Test: 0.7589\n",
            "Early stopping:  0.07258176844469276\n",
            "Epoch: 013, Loss: 0.0349, Train: 1.0000, Test: 0.7575\n",
            "Early stopping:  0.044588598234870803\n",
            "Epoch: 014, Loss: 0.0108, Train: 1.0000, Test: 0.7569\n",
            "Early stopping:  0.015410221202472065\n",
            "Epoch: 015, Loss: 0.0042, Train: 1.0000, Test: 0.7558\n",
            "Early stopping:  0.01222820968361751\n",
            "Epoch: 016, Loss: 0.0024, Train: 1.0000, Test: 0.7541\n",
            "Early stopping:  0.013543285767749472\n",
            "Epoch: 017, Loss: 0.0016, Train: 1.0000, Test: 0.7514\n",
            "Early stopping:  0.01396155150256465\n",
            "Epoch: 018, Loss: 0.0014, Train: 1.0000, Test: 0.7482\n",
            "Early stopping:  0.003936559233494737\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.69      0.75       392\n",
            "             ecology       0.74      0.81      0.78       879\n",
            "            economic       0.83      0.65      0.73      1399\n",
            "          geophysics       0.96      0.85      0.91      1192\n",
            "  gravitional_theory       0.67      0.95      0.79       120\n",
            "               hydro       0.72      0.77      0.74       345\n",
            "                math       0.95      0.56      0.71      1329\n",
            "              metals       0.33      0.95      0.48       191\n",
            "          networking       0.91      0.77      0.83       335\n",
            "        neuroscience       0.95      0.94      0.94       297\n",
            "        oceanography       0.85      0.73      0.78       980\n",
            "             politic       0.69      0.75      0.72       593\n",
            "           sociology       0.56      0.72      0.63       729\n",
            "software_engineering       0.71      0.92      0.80       514\n",
            "          statistics       0.61      0.87      0.72       637\n",
            "    theory_computing       0.64      0.68      0.66       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.75      0.79      0.75     10364\n",
            "        weighted avg       0.79      0.75      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.3298, Train: 0.5875, Test: 0.5404\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.6221, Train: 0.7250, Test: 0.6087\n",
            "Early stopping:  1.9145698211264222\n",
            "Epoch: 003, Loss: 1.4715, Train: 0.8250, Test: 0.6627\n",
            "Early stopping:  1.6084871826639573\n",
            "Epoch: 004, Loss: 0.9739, Train: 0.8750, Test: 0.7148\n",
            "Early stopping:  1.5125392118869854\n",
            "Epoch: 005, Loss: 0.6362, Train: 0.9000, Test: 0.7439\n",
            "Early stopping:  1.4642271433945457\n",
            "Epoch: 006, Loss: 0.5991, Train: 0.9000, Test: 0.7569\n",
            "Early stopping:  0.4703261025093897\n",
            "Epoch: 007, Loss: 0.6727, Train: 0.9187, Test: 0.7602\n",
            "Early stopping:  0.3672821203609454\n",
            "Epoch: 008, Loss: 0.3845, Train: 0.9563, Test: 0.7318\n",
            "Early stopping:  0.2113271731386891\n",
            "Epoch: 009, Loss: 0.2187, Train: 0.9563, Test: 0.7102\n",
            "Early stopping:  0.19407995734211936\n",
            "Epoch: 010, Loss: 0.2067, Train: 0.9437, Test: 0.7043\n",
            "Early stopping:  0.21397382119644162\n",
            "Epoch: 011, Loss: 0.2850, Train: 0.9625, Test: 0.7049\n",
            "Early stopping:  0.191887176325589\n",
            "Epoch: 012, Loss: 0.2431, Train: 0.9625, Test: 0.7119\n",
            "Early stopping:  0.07187229851953286\n",
            "Epoch: 013, Loss: 0.1629, Train: 0.9688, Test: 0.7214\n",
            "Early stopping:  0.045087855577387675\n",
            "Epoch: 014, Loss: 0.1248, Train: 0.9750, Test: 0.7278\n",
            "Early stopping:  0.0633229070777265\n",
            "Epoch: 015, Loss: 0.1022, Train: 0.9938, Test: 0.7360\n",
            "Early stopping:  0.0779698820301527\n",
            "Epoch: 016, Loss: 0.0475, Train: 0.9875, Test: 0.7408\n",
            "Early stopping:  0.07296493159725329\n",
            "Epoch: 017, Loss: 0.0370, Train: 0.9750, Test: 0.7435\n",
            "Early stopping:  0.05285972099539751\n",
            "Epoch: 018, Loss: 0.0517, Train: 0.9812, Test: 0.7479\n",
            "Early stopping:  0.03853016418135068\n",
            "Epoch: 019, Loss: 0.0346, Train: 0.9875, Test: 0.7497\n",
            "Early stopping:  0.027537271950998997\n",
            "Epoch: 020, Loss: 0.0499, Train: 0.9875, Test: 0.7525\n",
            "Early stopping:  0.007795998050704361\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  6, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.78      0.78       392\n",
            "             ecology       0.81      0.81      0.81       879\n",
            "            economic       0.75      0.71      0.73      1399\n",
            "          geophysics       0.86      0.91      0.88      1192\n",
            "  gravitional_theory       0.62      0.95      0.75       120\n",
            "               hydro       0.52      0.89      0.66       345\n",
            "                math       0.84      0.60      0.70      1329\n",
            "              metals       0.86      0.71      0.78       191\n",
            "          networking       0.83      0.90      0.86       335\n",
            "        neuroscience       0.93      0.96      0.94       297\n",
            "        oceanography       0.86      0.77      0.81       980\n",
            "             politic       0.61      0.74      0.67       593\n",
            "           sociology       0.64      0.51      0.57       729\n",
            "software_engineering       0.83      0.75      0.79       514\n",
            "          statistics       0.62      0.78      0.69       637\n",
            "    theory_computing       0.64      0.72      0.68       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.75      0.78      0.76     10364\n",
            "        weighted avg       0.77      0.75      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.5580, Train: 0.7188, Test: 0.6585\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9613, Train: 0.8375, Test: 0.6919\n",
            "Early stopping:  2.5432529919112334\n",
            "Epoch: 003, Loss: 0.6368, Train: 0.8938, Test: 0.7418\n",
            "Early stopping:  2.1762922214956206\n",
            "Epoch: 004, Loss: 0.3085, Train: 0.9313, Test: 0.7542\n",
            "Early stopping:  1.979273698538078\n",
            "Epoch: 005, Loss: 0.1698, Train: 0.9812, Test: 0.7682\n",
            "Early stopping:  1.8320867241095602\n",
            "Epoch: 006, Loss: 0.0810, Train: 0.9875, Test: 0.7675\n",
            "Early stopping:  0.3637868488955954\n",
            "Epoch: 007, Loss: 0.0461, Train: 0.9938, Test: 0.7556\n",
            "Early stopping:  0.23958375060520762\n",
            "Epoch: 008, Loss: 0.0350, Train: 1.0000, Test: 0.7528\n",
            "Early stopping:  0.11388107608031094\n",
            "Epoch: 009, Loss: 0.0184, Train: 0.9938, Test: 0.7498\n",
            "Early stopping:  0.06027965133133696\n",
            "Epoch: 010, Loss: 0.0219, Train: 1.0000, Test: 0.7459\n",
            "Early stopping:  0.02517247220115812\n",
            "Epoch: 011, Loss: 0.0176, Train: 0.9938, Test: 0.7490\n",
            "Early stopping:  0.012393471949065869\n",
            "Epoch: 012, Loss: 0.0188, Train: 1.0000, Test: 0.7547\n",
            "Early stopping:  0.007261453090977617\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.75      0.77       392\n",
            "             ecology       0.83      0.72      0.77       879\n",
            "            economic       0.75      0.53      0.62      1399\n",
            "          geophysics       0.96      0.86      0.91      1192\n",
            "  gravitional_theory       0.94      0.84      0.89       120\n",
            "               hydro       0.67      0.77      0.72       345\n",
            "                math       0.85      0.77      0.81      1329\n",
            "              metals       0.56      0.94      0.70       191\n",
            "          networking       0.87      0.87      0.87       335\n",
            "        neuroscience       0.89      0.98      0.93       297\n",
            "        oceanography       0.87      0.85      0.86       980\n",
            "             politic       0.60      0.85      0.70       593\n",
            "           sociology       0.47      0.56      0.51       729\n",
            "software_engineering       0.79      0.84      0.82       514\n",
            "          statistics       0.60      0.82      0.70       637\n",
            "    theory_computing       0.69      0.63      0.66       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.76      0.79      0.76     10364\n",
            "        weighted avg       0.77      0.75      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7455, Train: 0.6625, Test: 0.5173\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2015, Train: 0.8500, Test: 0.7124\n",
            "Early stopping:  1.798852782554064\n",
            "Epoch: 003, Loss: 0.5329, Train: 0.9062, Test: 0.7300\n",
            "Early stopping:  1.6950573025488576\n",
            "Epoch: 004, Loss: 0.3023, Train: 0.9750, Test: 0.7612\n",
            "Early stopping:  1.5799837387385511\n",
            "Epoch: 005, Loss: 0.1266, Train: 0.9938, Test: 0.7542\n",
            "Early stopping:  1.490019490794992\n",
            "Epoch: 006, Loss: 0.0742, Train: 0.9875, Test: 0.7393\n",
            "Early stopping:  0.45802896509518365\n",
            "Epoch: 007, Loss: 0.0513, Train: 0.9938, Test: 0.7274\n",
            "Early stopping:  0.20190212797865192\n",
            "Epoch: 008, Loss: 0.0331, Train: 1.0000, Test: 0.7307\n",
            "Early stopping:  0.10910660396951546\n",
            "Epoch: 009, Loss: 0.0170, Train: 1.0000, Test: 0.7321\n",
            "Early stopping:  0.04269020814498074\n",
            "Epoch: 010, Loss: 0.0111, Train: 1.0000, Test: 0.7290\n",
            "Early stopping:  0.025861561276328576\n",
            "Epoch: 011, Loss: 0.0081, Train: 1.0000, Test: 0.7271\n",
            "Early stopping:  0.0180126771620023\n",
            "Epoch: 012, Loss: 0.0063, Train: 1.0000, Test: 0.7252\n",
            "Early stopping:  0.010822949961910074\n",
            "Epoch: 013, Loss: 0.0040, Train: 1.0000, Test: 0.7232\n",
            "Early stopping:  0.0050238006607819415\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.86      0.61      0.72       392\n",
            "             ecology       0.89      0.48      0.62       879\n",
            "            economic       0.75      0.74      0.75      1399\n",
            "          geophysics       0.93      0.72      0.81      1192\n",
            "  gravitional_theory       0.87      0.92      0.89       120\n",
            "               hydro       0.32      0.93      0.47       345\n",
            "                math       0.89      0.55      0.68      1329\n",
            "              metals       0.80      0.85      0.82       191\n",
            "          networking       0.88      0.84      0.86       335\n",
            "        neuroscience       0.93      0.97      0.95       297\n",
            "        oceanography       0.77      0.86      0.81       980\n",
            "             politic       0.77      0.74      0.76       593\n",
            "           sociology       0.58      0.71      0.64       729\n",
            "software_engineering       0.70      0.87      0.77       514\n",
            "          statistics       0.55      0.75      0.64       637\n",
            "    theory_computing       0.68      0.73      0.70       432\n",
            "\n",
            "            accuracy                           0.72     10364\n",
            "           macro avg       0.76      0.77      0.74     10364\n",
            "        weighted avg       0.78      0.72      0.73     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.0250, Train: 0.5687, Test: 0.6148\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.1764, Train: 0.8313, Test: 0.6799\n",
            "Early stopping:  1.3071971506199316\n",
            "Epoch: 003, Loss: 0.9690, Train: 0.9062, Test: 0.7405\n",
            "Early stopping:  1.5391681512543982\n",
            "Epoch: 004, Loss: 0.3468, Train: 0.9187, Test: 0.7417\n",
            "Early stopping:  1.6196140817613787\n",
            "Epoch: 005, Loss: 0.2677, Train: 0.9875, Test: 0.7546\n",
            "Early stopping:  1.5769720580713453\n",
            "Epoch: 006, Loss: 0.0752, Train: 0.9938, Test: 0.7390\n",
            "Early stopping:  0.8561001696862506\n",
            "Epoch: 007, Loss: 0.0817, Train: 0.9875, Test: 0.7345\n",
            "Early stopping:  0.36656382609031274\n",
            "Epoch: 008, Loss: 0.0793, Train: 0.9875, Test: 0.7375\n",
            "Early stopping:  0.12827870829137758\n",
            "Epoch: 009, Loss: 0.0440, Train: 1.0000, Test: 0.7380\n",
            "Early stopping:  0.08968309853216858\n",
            "Epoch: 010, Loss: 0.0210, Train: 1.0000, Test: 0.7381\n",
            "Early stopping:  0.026707869982468754\n",
            "Epoch: 011, Loss: 0.0108, Train: 1.0000, Test: 0.7365\n",
            "Early stopping:  0.032562992819242846\n",
            "Epoch: 012, Loss: 0.0087, Train: 1.0000, Test: 0.7346\n",
            "Early stopping:  0.02954002637520483\n",
            "Epoch: 013, Loss: 0.0063, Train: 1.0000, Test: 0.7317\n",
            "Early stopping:  0.015497528309237936\n",
            "Epoch: 014, Loss: 0.0084, Train: 1.0000, Test: 0.7283\n",
            "Early stopping:  0.005780659994061588\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.59      0.73      0.65       392\n",
            "             ecology       0.86      0.71      0.78       879\n",
            "            economic       0.81      0.63      0.71      1399\n",
            "          geophysics       0.90      0.79      0.84      1192\n",
            "  gravitional_theory       0.62      0.96      0.75       120\n",
            "               hydro       0.43      0.88      0.58       345\n",
            "                math       0.87      0.70      0.77      1329\n",
            "              metals       0.66      0.87      0.75       191\n",
            "          networking       0.55      0.60      0.58       335\n",
            "        neuroscience       0.81      0.98      0.89       297\n",
            "        oceanography       0.80      0.82      0.81       980\n",
            "             politic       0.75      0.72      0.73       593\n",
            "           sociology       0.55      0.75      0.63       729\n",
            "software_engineering       0.85      0.69      0.76       514\n",
            "          statistics       0.70      0.57      0.63       637\n",
            "    theory_computing       0.55      0.72      0.62       432\n",
            "\n",
            "            accuracy                           0.73     10364\n",
            "           macro avg       0.71      0.76      0.72     10364\n",
            "        weighted avg       0.76      0.73      0.73     10364\n",
            "\n",
            "time: 6.04 s (started: 2024-08-16 14:11:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "vxt2CQkCuuCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4522ec6b-2d9e-4796-b3c8-6d0110c04b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 451 ms (started: 2024-08-16 14:11:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "ACEhql2iuuCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095933cf-1fe3-495a-ce2e-a59414b2fe70",
        "id": "z2XALkewuuCF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7737, Train: 0.6250, Test: 0.5635\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4945, Train: 0.8063, Test: 0.6664\n",
            "Early stopping:  0.19739813674865306\n",
            "Epoch: 003, Loss: 2.1585, Train: 0.8938, Test: 0.7205\n",
            "Early stopping:  0.3080349188391454\n",
            "Epoch: 004, Loss: 1.7799, Train: 0.9313, Test: 0.7493\n",
            "Early stopping:  0.42926132631503666\n",
            "Epoch: 005, Loss: 1.4028, Train: 0.9437, Test: 0.7617\n",
            "Early stopping:  0.547535317911406\n",
            "Epoch: 006, Loss: 1.0584, Train: 0.9500, Test: 0.7663\n",
            "Early stopping:  0.5737655378564829\n",
            "Epoch: 007, Loss: 0.7710, Train: 0.9625, Test: 0.7723\n",
            "Early stopping:  0.5536564737413016\n",
            "Epoch: 008, Loss: 0.5506, Train: 0.9688, Test: 0.7734\n",
            "Early stopping:  0.49114231859929\n",
            "Epoch: 009, Loss: 0.3934, Train: 0.9750, Test: 0.7746\n",
            "Early stopping:  0.40380389794874166\n",
            "Epoch: 010, Loss: 0.2834, Train: 0.9750, Test: 0.7752\n",
            "Early stopping:  0.309871338128894\n",
            "Epoch: 011, Loss: 0.2036, Train: 0.9938, Test: 0.7756\n",
            "Early stopping:  0.22603413676223194\n",
            "Epoch: 012, Loss: 0.1453, Train: 0.9938, Test: 0.7728\n",
            "Early stopping:  0.16114188047455646\n",
            "Epoch: 013, Loss: 0.1039, Train: 0.9938, Test: 0.7746\n",
            "Early stopping:  0.11537426267785708\n",
            "Epoch: 014, Loss: 0.0744, Train: 1.0000, Test: 0.7762\n",
            "Early stopping:  0.08335760496511024\n",
            "Epoch: 015, Loss: 0.0530, Train: 1.0000, Test: 0.7749\n",
            "Early stopping:  0.059956497438352165\n",
            "Epoch: 016, Loss: 0.0380, Train: 1.0000, Test: 0.7741\n",
            "Early stopping:  0.04275591180125501\n",
            "Epoch: 017, Loss: 0.0280, Train: 1.0000, Test: 0.7750\n",
            "Early stopping:  0.030363559694207384\n",
            "Epoch: 018, Loss: 0.0212, Train: 1.0000, Test: 0.7739\n",
            "Early stopping:  0.0212771084688629\n",
            "Epoch: 019, Loss: 0.0162, Train: 1.0000, Test: 0.7730\n",
            "Early stopping:  0.014635863531372912\n",
            "Epoch: 020, Loss: 0.0125, Train: 1.0000, Test: 0.7712\n",
            "Early stopping:  0.010121996563944444\n",
            "Epoch: 021, Loss: 0.0099, Train: 1.0000, Test: 0.7707\n",
            "Early stopping:  0.007220123362015135\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.83      0.77       392\n",
            "             ecology       0.82      0.79      0.81       879\n",
            "            economic       0.75      0.63      0.69      1399\n",
            "          geophysics       0.87      0.92      0.90      1192\n",
            "  gravitional_theory       0.62      0.96      0.75       120\n",
            "               hydro       0.67      0.78      0.72       345\n",
            "                math       0.86      0.78      0.82      1329\n",
            "              metals       0.75      0.89      0.82       191\n",
            "          networking       0.71      0.93      0.81       335\n",
            "        neuroscience       0.92      0.97      0.95       297\n",
            "        oceanography       0.85      0.77      0.81       980\n",
            "             politic       0.61      0.84      0.71       593\n",
            "           sociology       0.61      0.60      0.61       729\n",
            "software_engineering       0.78      0.89      0.83       514\n",
            "          statistics       0.76      0.61      0.68       637\n",
            "    theory_computing       0.73      0.59      0.66       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.75      0.80      0.77     10364\n",
            "        weighted avg       0.78      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7610, Train: 0.8187, Test: 0.6685\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4768, Train: 0.9000, Test: 0.7412\n",
            "Early stopping:  0.20101147036518904\n",
            "Epoch: 003, Loss: 2.1215, Train: 0.9125, Test: 0.7546\n",
            "Early stopping:  0.3204476300503586\n",
            "Epoch: 004, Loss: 1.7269, Train: 0.9062, Test: 0.7558\n",
            "Early stopping:  0.44753358233681956\n",
            "Epoch: 005, Loss: 1.3468, Train: 0.9313, Test: 0.7686\n",
            "Early stopping:  0.566802379018549\n",
            "Epoch: 006, Loss: 1.0142, Train: 0.9375, Test: 0.7790\n",
            "Early stopping:  0.5852237730253136\n",
            "Epoch: 007, Loss: 0.7416, Train: 0.9313, Test: 0.7792\n",
            "Early stopping:  0.55046809400053\n",
            "Epoch: 008, Loss: 0.5348, Train: 0.9437, Test: 0.7831\n",
            "Early stopping:  0.4757747648381066\n",
            "Epoch: 009, Loss: 0.3900, Train: 0.9625, Test: 0.7808\n",
            "Early stopping:  0.3829155002319487\n",
            "Epoch: 010, Loss: 0.2896, Train: 0.9688, Test: 0.7816\n",
            "Early stopping:  0.2898856248633808\n",
            "Epoch: 011, Loss: 0.2169, Train: 0.9875, Test: 0.7816\n",
            "Early stopping:  0.20900167351350935\n",
            "Epoch: 012, Loss: 0.1619, Train: 0.9938, Test: 0.7813\n",
            "Early stopping:  0.14798992591508672\n",
            "Epoch: 013, Loss: 0.1185, Train: 0.9938, Test: 0.7807\n",
            "Early stopping:  0.10752775176002596\n",
            "Epoch: 014, Loss: 0.0855, Train: 1.0000, Test: 0.7770\n",
            "Early stopping:  0.08103403545043242\n",
            "Epoch: 015, Loss: 0.0647, Train: 0.9938, Test: 0.7736\n",
            "Early stopping:  0.06113364999787378\n",
            "Epoch: 016, Loss: 0.0498, Train: 0.9938, Test: 0.7701\n",
            "Early stopping:  0.04492170296567555\n",
            "Epoch: 017, Loss: 0.0363, Train: 1.0000, Test: 0.7697\n",
            "Early stopping:  0.03224645136336953\n",
            "Epoch: 018, Loss: 0.0265, Train: 1.0000, Test: 0.7679\n",
            "Early stopping:  0.023351961676255202\n",
            "Epoch: 019, Loss: 0.0216, Train: 0.9938, Test: 0.7671\n",
            "Early stopping:  0.01758545209932277\n",
            "Epoch: 020, Loss: 0.0185, Train: 1.0000, Test: 0.7659\n",
            "Early stopping:  0.012677371715941937\n",
            "Epoch: 021, Loss: 0.0149, Train: 1.0000, Test: 0.7648\n",
            "Early stopping:  0.008296496754150604\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.76      0.74       392\n",
            "             ecology       0.82      0.83      0.83       879\n",
            "            economic       0.81      0.60      0.69      1399\n",
            "          geophysics       0.93      0.82      0.87      1192\n",
            "  gravitional_theory       0.66      0.89      0.76       120\n",
            "               hydro       0.64      0.83      0.72       345\n",
            "                math       0.82      0.82      0.82      1329\n",
            "              metals       0.66      0.93      0.78       191\n",
            "          networking       0.93      0.70      0.80       335\n",
            "        neuroscience       0.96      0.92      0.94       297\n",
            "        oceanography       0.89      0.75      0.82       980\n",
            "             politic       0.65      0.78      0.71       593\n",
            "           sociology       0.57      0.71      0.63       729\n",
            "software_engineering       0.87      0.72      0.79       514\n",
            "          statistics       0.61      0.82      0.70       637\n",
            "    theory_computing       0.60      0.71      0.65       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.76      0.79      0.76     10364\n",
            "        weighted avg       0.78      0.76      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7847, Train: 0.7438, Test: 0.6218\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5182, Train: 0.8500, Test: 0.7142\n",
            "Early stopping:  0.18845845300470435\n",
            "Epoch: 003, Loss: 2.1903, Train: 0.8750, Test: 0.7157\n",
            "Early stopping:  0.29773422163978713\n",
            "Epoch: 004, Loss: 1.8137, Train: 0.8750, Test: 0.7262\n",
            "Early stopping:  0.41961642154032264\n",
            "Epoch: 005, Loss: 1.4370, Train: 0.8938, Test: 0.7450\n",
            "Early stopping:  0.5388611443880021\n",
            "Epoch: 006, Loss: 1.0913, Train: 0.9000, Test: 0.7622\n",
            "Early stopping:  0.5704786147149996\n",
            "Epoch: 007, Loss: 0.7988, Train: 0.8938, Test: 0.7752\n",
            "Early stopping:  0.5549458341804928\n",
            "Epoch: 008, Loss: 0.5722, Train: 0.9125, Test: 0.7828\n",
            "Early stopping:  0.49577468723198304\n",
            "Epoch: 009, Loss: 0.4112, Train: 0.9187, Test: 0.7862\n",
            "Early stopping:  0.4106290979994452\n",
            "Epoch: 010, Loss: 0.3016, Train: 0.9375, Test: 0.7861\n",
            "Early stopping:  0.31634035859301557\n",
            "Epoch: 011, Loss: 0.2237, Train: 0.9688, Test: 0.7829\n",
            "Early stopping:  0.22950455342191464\n",
            "Epoch: 012, Loss: 0.1640, Train: 0.9938, Test: 0.7800\n",
            "Early stopping:  0.1618858359820578\n",
            "Epoch: 013, Loss: 0.1178, Train: 0.9938, Test: 0.7767\n",
            "Early stopping:  0.11619096486144061\n",
            "Epoch: 014, Loss: 0.0837, Train: 1.0000, Test: 0.7718\n",
            "Early stopping:  0.0867123277945353\n",
            "Epoch: 015, Loss: 0.0596, Train: 1.0000, Test: 0.7680\n",
            "Early stopping:  0.06556116463590443\n",
            "Epoch: 016, Loss: 0.0426, Train: 1.0000, Test: 0.7644\n",
            "Early stopping:  0.04847016068300518\n",
            "Epoch: 017, Loss: 0.0308, Train: 1.0000, Test: 0.7621\n",
            "Early stopping:  0.03470324287077938\n",
            "Epoch: 018, Loss: 0.0227, Train: 1.0000, Test: 0.7611\n",
            "Early stopping:  0.024357142671219627\n",
            "Epoch: 019, Loss: 0.0171, Train: 1.0000, Test: 0.7616\n",
            "Early stopping:  0.016952000073421764\n",
            "Epoch: 020, Loss: 0.0131, Train: 1.0000, Test: 0.7617\n",
            "Early stopping:  0.011752004312859336\n",
            "Epoch: 021, Loss: 0.0103, Train: 1.0000, Test: 0.7620\n",
            "Early stopping:  0.008170113471182428\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.70      0.72       392\n",
            "             ecology       0.81      0.68      0.74       879\n",
            "            economic       0.82      0.70      0.75      1399\n",
            "          geophysics       0.91      0.86      0.88      1192\n",
            "  gravitional_theory       0.91      0.89      0.90       120\n",
            "               hydro       0.63      0.79      0.70       345\n",
            "                math       0.88      0.70      0.78      1329\n",
            "              metals       0.34      0.95      0.50       191\n",
            "          networking       0.80      0.92      0.85       335\n",
            "        neuroscience       0.98      0.85      0.91       297\n",
            "        oceanography       0.75      0.86      0.80       980\n",
            "             politic       0.78      0.68      0.73       593\n",
            "           sociology       0.60      0.69      0.64       729\n",
            "software_engineering       0.77      0.90      0.83       514\n",
            "          statistics       0.72      0.75      0.74       637\n",
            "    theory_computing       0.69      0.64      0.66       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.76      0.79      0.76     10364\n",
            "        weighted avg       0.79      0.76      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7800, Train: 0.8000, Test: 0.5949\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5154, Train: 0.8812, Test: 0.6816\n",
            "Early stopping:  0.18706052633329504\n",
            "Epoch: 003, Loss: 2.1952, Train: 0.8938, Test: 0.6778\n",
            "Early stopping:  0.2928357118906371\n",
            "Epoch: 004, Loss: 1.8252, Train: 0.8875, Test: 0.6977\n",
            "Early stopping:  0.4122466792224149\n",
            "Epoch: 005, Loss: 1.4499, Train: 0.9125, Test: 0.7298\n",
            "Early stopping:  0.5310417958536975\n",
            "Epoch: 006, Loss: 1.1032, Train: 0.9437, Test: 0.7606\n",
            "Early stopping:  0.5646313100797435\n",
            "Epoch: 007, Loss: 0.8064, Train: 0.9500, Test: 0.7832\n",
            "Early stopping:  0.5538947443816964\n",
            "Epoch: 008, Loss: 0.5731, Train: 0.9437, Test: 0.7936\n",
            "Early stopping:  0.4997212723215011\n",
            "Epoch: 009, Loss: 0.4068, Train: 0.9500, Test: 0.7940\n",
            "Early stopping:  0.41754410795682395\n",
            "Epoch: 010, Loss: 0.2950, Train: 0.9500, Test: 0.7921\n",
            "Early stopping:  0.3240641396049136\n",
            "Epoch: 011, Loss: 0.2167, Train: 0.9625, Test: 0.7892\n",
            "Early stopping:  0.23561781817431054\n",
            "Epoch: 012, Loss: 0.1593, Train: 0.9812, Test: 0.7825\n",
            "Early stopping:  0.16446910306368645\n",
            "Epoch: 013, Loss: 0.1174, Train: 0.9812, Test: 0.7766\n",
            "Early stopping:  0.11501441217760827\n",
            "Epoch: 014, Loss: 0.0877, Train: 0.9875, Test: 0.7706\n",
            "Early stopping:  0.08263798482002005\n",
            "Epoch: 015, Loss: 0.0654, Train: 1.0000, Test: 0.7703\n",
            "Early stopping:  0.06020525665999503\n",
            "Epoch: 016, Loss: 0.0478, Train: 1.0000, Test: 0.7731\n",
            "Early stopping:  0.04414619464073768\n",
            "Epoch: 017, Loss: 0.0346, Train: 1.0000, Test: 0.7746\n",
            "Early stopping:  0.03290779302629521\n",
            "Epoch: 018, Loss: 0.0254, Train: 1.0000, Test: 0.7743\n",
            "Early stopping:  0.02489329844646175\n",
            "Epoch: 019, Loss: 0.0194, Train: 1.0000, Test: 0.7721\n",
            "Early stopping:  0.01844484208820119\n",
            "Epoch: 020, Loss: 0.0152, Train: 1.0000, Test: 0.7704\n",
            "Early stopping:  0.013011630335336605\n",
            "Epoch: 021, Loss: 0.0123, Train: 1.0000, Test: 0.7686\n",
            "Early stopping:  0.008872282389740483\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.78      0.73       392\n",
            "             ecology       0.79      0.84      0.81       879\n",
            "            economic       0.77      0.70      0.74      1399\n",
            "          geophysics       0.96      0.84      0.90      1192\n",
            "  gravitional_theory       0.80      0.92      0.85       120\n",
            "               hydro       0.74      0.78      0.76       345\n",
            "                math       0.91      0.67      0.77      1329\n",
            "              metals       0.41      0.97      0.57       191\n",
            "          networking       0.81      0.91      0.86       335\n",
            "        neuroscience       0.86      0.97      0.91       297\n",
            "        oceanography       0.87      0.80      0.83       980\n",
            "             politic       0.75      0.75      0.75       593\n",
            "           sociology       0.67      0.63      0.65       729\n",
            "software_engineering       0.71      0.73      0.72       514\n",
            "          statistics       0.63      0.88      0.73       637\n",
            "    theory_computing       0.64      0.61      0.62       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.75      0.80      0.76     10364\n",
            "        weighted avg       0.79      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7884, Train: 0.6500, Test: 0.5045\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5103, Train: 0.8875, Test: 0.7081\n",
            "Early stopping:  0.19662752377047987\n",
            "Epoch: 003, Loss: 2.1791, Train: 0.8812, Test: 0.7529\n",
            "Early stopping:  0.3049987456299879\n",
            "Epoch: 004, Loss: 1.7967, Train: 0.9000, Test: 0.7627\n",
            "Early stopping:  0.4278983064667353\n",
            "Epoch: 005, Loss: 1.4119, Train: 0.9000, Test: 0.7664\n",
            "Early stopping:  0.5493041178505194\n",
            "Epoch: 006, Loss: 1.0632, Train: 0.9250, Test: 0.7728\n",
            "Early stopping:  0.5791043850872624\n",
            "Epoch: 007, Loss: 0.7751, Train: 0.9437, Test: 0.7751\n",
            "Early stopping:  0.5608717805060254\n",
            "Epoch: 008, Loss: 0.5557, Train: 0.9437, Test: 0.7734\n",
            "Early stopping:  0.49592616136775297\n",
            "Epoch: 009, Loss: 0.3988, Train: 0.9625, Test: 0.7728\n",
            "Early stopping:  0.405178843360677\n",
            "Epoch: 010, Loss: 0.2886, Train: 0.9812, Test: 0.7747\n",
            "Early stopping:  0.3095473278328832\n",
            "Epoch: 011, Loss: 0.2101, Train: 0.9812, Test: 0.7749\n",
            "Early stopping:  0.22526203762676017\n",
            "Epoch: 012, Loss: 0.1530, Train: 0.9875, Test: 0.7746\n",
            "Early stopping:  0.1602345940565516\n",
            "Epoch: 013, Loss: 0.1113, Train: 1.0000, Test: 0.7742\n",
            "Early stopping:  0.11434746602764631\n",
            "Epoch: 014, Loss: 0.0807, Train: 1.0000, Test: 0.7716\n",
            "Early stopping:  0.08274044230727692\n",
            "Epoch: 015, Loss: 0.0583, Train: 1.0000, Test: 0.7696\n",
            "Early stopping:  0.06040013771657759\n",
            "Epoch: 016, Loss: 0.0422, Train: 1.0000, Test: 0.7682\n",
            "Early stopping:  0.04413456091191453\n",
            "Epoch: 017, Loss: 0.0309, Train: 1.0000, Test: 0.7676\n",
            "Early stopping:  0.03208634521412474\n",
            "Epoch: 018, Loss: 0.0230, Train: 1.0000, Test: 0.7668\n",
            "Early stopping:  0.02303262867845836\n",
            "Epoch: 019, Loss: 0.0175, Train: 1.0000, Test: 0.7668\n",
            "Early stopping:  0.016293441535387157\n",
            "Epoch: 020, Loss: 0.0137, Train: 1.0000, Test: 0.7670\n",
            "Early stopping:  0.011400655572280736\n",
            "Epoch: 021, Loss: 0.0110, Train: 1.0000, Test: 0.7657\n",
            "Early stopping:  0.007925237883052967\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.77      0.78       392\n",
            "             ecology       0.73      0.90      0.81       879\n",
            "            economic       0.81      0.58      0.68      1399\n",
            "          geophysics       0.94      0.89      0.91      1192\n",
            "  gravitional_theory       0.92      0.93      0.93       120\n",
            "               hydro       0.77      0.80      0.79       345\n",
            "                math       0.93      0.67      0.77      1329\n",
            "              metals       0.53      0.91      0.67       191\n",
            "          networking       0.75      0.91      0.82       335\n",
            "        neuroscience       0.95      0.97      0.96       297\n",
            "        oceanography       0.91      0.69      0.79       980\n",
            "             politic       0.62      0.85      0.72       593\n",
            "           sociology       0.54      0.68      0.60       729\n",
            "software_engineering       0.82      0.80      0.81       514\n",
            "          statistics       0.62      0.82      0.70       637\n",
            "    theory_computing       0.66      0.75      0.70       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.77      0.81      0.78     10364\n",
            "        weighted avg       0.79      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7761, Train: 0.7625, Test: 0.5387\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4868, Train: 0.8438, Test: 0.6951\n",
            "Early stopping:  0.20455956066023015\n",
            "Epoch: 003, Loss: 2.1458, Train: 0.8938, Test: 0.7513\n",
            "Early stopping:  0.3155355137936169\n",
            "Epoch: 004, Loss: 1.7589, Train: 0.9062, Test: 0.7656\n",
            "Early stopping:  0.4389247231726341\n",
            "Epoch: 005, Loss: 1.3742, Train: 0.9125, Test: 0.7726\n",
            "Early stopping:  0.5594097062522103\n",
            "Epoch: 006, Loss: 1.0282, Train: 0.9500, Test: 0.7818\n",
            "Early stopping:  0.5834148996766595\n",
            "Epoch: 007, Loss: 0.7423, Train: 0.9750, Test: 0.7906\n",
            "Early stopping:  0.560338693929112\n",
            "Epoch: 008, Loss: 0.5230, Train: 0.9812, Test: 0.7891\n",
            "Early stopping:  0.49352431441360317\n",
            "Epoch: 009, Loss: 0.3673, Train: 0.9812, Test: 0.7824\n",
            "Early stopping:  0.40274368421474266\n",
            "Epoch: 010, Loss: 0.2612, Train: 0.9938, Test: 0.7786\n",
            "Early stopping:  0.3071180998408714\n",
            "Epoch: 011, Loss: 0.1865, Train: 0.9938, Test: 0.7772\n",
            "Early stopping:  0.22189635953333195\n",
            "Epoch: 012, Loss: 0.1313, Train: 1.0000, Test: 0.7791\n",
            "Early stopping:  0.15566105233802072\n",
            "Epoch: 013, Loss: 0.0912, Train: 1.0000, Test: 0.7823\n",
            "Early stopping:  0.10978246555960619\n",
            "Epoch: 014, Loss: 0.0632, Train: 1.0000, Test: 0.7815\n",
            "Early stopping:  0.07901169584105183\n",
            "Epoch: 015, Loss: 0.0442, Train: 1.0000, Test: 0.7795\n",
            "Early stopping:  0.05688903191148058\n",
            "Epoch: 016, Loss: 0.0310, Train: 1.0000, Test: 0.7773\n",
            "Early stopping:  0.040034270485651696\n",
            "Epoch: 017, Loss: 0.0221, Train: 1.0000, Test: 0.7747\n",
            "Early stopping:  0.027590557164260718\n",
            "Epoch: 018, Loss: 0.0161, Train: 1.0000, Test: 0.7734\n",
            "Early stopping:  0.018856457646105985\n",
            "Epoch: 019, Loss: 0.0121, Train: 1.0000, Test: 0.7717\n",
            "Early stopping:  0.01281941736975027\n",
            "Epoch: 020, Loss: 0.0094, Train: 1.0000, Test: 0.7716\n",
            "Early stopping:  0.008640650685623587\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.83      0.80       392\n",
            "             ecology       0.82      0.81      0.82       879\n",
            "            economic       0.85      0.63      0.72      1399\n",
            "          geophysics       0.93      0.88      0.90      1192\n",
            "  gravitional_theory       0.64      0.96      0.76       120\n",
            "               hydro       0.74      0.77      0.76       345\n",
            "                math       0.97      0.59      0.74      1329\n",
            "              metals       0.39      0.97      0.55       191\n",
            "          networking       0.87      0.87      0.87       335\n",
            "        neuroscience       0.95      0.93      0.94       297\n",
            "        oceanography       0.84      0.85      0.85       980\n",
            "             politic       0.69      0.75      0.72       593\n",
            "           sociology       0.56      0.73      0.64       729\n",
            "software_engineering       0.81      0.85      0.83       514\n",
            "          statistics       0.61      0.88      0.72       637\n",
            "    theory_computing       0.74      0.66      0.70       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.76      0.81      0.77     10364\n",
            "        weighted avg       0.81      0.77      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7858, Train: 0.6312, Test: 0.5052\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5334, Train: 0.7688, Test: 0.6304\n",
            "Early stopping:  0.17844200157499931\n",
            "Epoch: 003, Loss: 2.2268, Train: 0.8313, Test: 0.6796\n",
            "Early stopping:  0.27991361134099296\n",
            "Epoch: 004, Loss: 1.8634, Train: 0.8500, Test: 0.7147\n",
            "Early stopping:  0.39808648955090276\n",
            "Epoch: 005, Loss: 1.4894, Train: 0.8562, Test: 0.7436\n",
            "Early stopping:  0.517475737882164\n",
            "Epoch: 006, Loss: 1.1437, Train: 0.8750, Test: 0.7702\n",
            "Early stopping:  0.5563496912034149\n",
            "Epoch: 007, Loss: 0.8528, Train: 0.8812, Test: 0.7798\n",
            "Early stopping:  0.5489042075067152\n",
            "Epoch: 008, Loss: 0.6291, Train: 0.9062, Test: 0.7830\n",
            "Early stopping:  0.4933274060094363\n",
            "Epoch: 009, Loss: 0.4704, Train: 0.9313, Test: 0.7823\n",
            "Early stopping:  0.4078872788840861\n",
            "Epoch: 010, Loss: 0.3590, Train: 0.9437, Test: 0.7841\n",
            "Early stopping:  0.3137625815953387\n",
            "Epoch: 011, Loss: 0.2761, Train: 0.9688, Test: 0.7837\n",
            "Early stopping:  0.22939503674196363\n",
            "Epoch: 012, Loss: 0.2117, Train: 0.9750, Test: 0.7805\n",
            "Early stopping:  0.16536455973933203\n",
            "Epoch: 013, Loss: 0.1612, Train: 0.9875, Test: 0.7786\n",
            "Early stopping:  0.1225492049134404\n",
            "Epoch: 014, Loss: 0.1209, Train: 0.9938, Test: 0.7768\n",
            "Early stopping:  0.09438966856921387\n",
            "Epoch: 015, Loss: 0.0887, Train: 1.0000, Test: 0.7748\n",
            "Early stopping:  0.074271733559447\n",
            "Epoch: 016, Loss: 0.0640, Train: 1.0000, Test: 0.7731\n",
            "Early stopping:  0.05870824022190512\n",
            "Epoch: 017, Loss: 0.0460, Train: 1.0000, Test: 0.7713\n",
            "Early stopping:  0.04595464213201337\n",
            "Epoch: 018, Loss: 0.0334, Train: 1.0000, Test: 0.7675\n",
            "Early stopping:  0.034977527550871954\n",
            "Epoch: 019, Loss: 0.0248, Train: 1.0000, Test: 0.7655\n",
            "Early stopping:  0.02556580870491231\n",
            "Epoch: 020, Loss: 0.0188, Train: 1.0000, Test: 0.7634\n",
            "Early stopping:  0.01804775620927747\n",
            "Epoch: 021, Loss: 0.0147, Train: 1.0000, Test: 0.7610\n",
            "Early stopping:  0.012484639824182162\n",
            "Epoch: 022, Loss: 0.0118, Train: 1.0000, Test: 0.7600\n",
            "Early stopping:  0.008597242358857511\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.78      0.79       392\n",
            "             ecology       0.82      0.75      0.78       879\n",
            "            economic       0.74      0.72      0.73      1399\n",
            "          geophysics       0.90      0.89      0.90      1192\n",
            "  gravitional_theory       0.63      0.96      0.76       120\n",
            "               hydro       0.53      0.89      0.66       345\n",
            "                math       0.91      0.64      0.75      1329\n",
            "              metals       0.65      0.83      0.73       191\n",
            "          networking       0.78      0.89      0.83       335\n",
            "        neuroscience       0.86      0.98      0.92       297\n",
            "        oceanography       0.84      0.78      0.81       980\n",
            "             politic       0.65      0.76      0.70       593\n",
            "           sociology       0.64      0.54      0.59       729\n",
            "software_engineering       0.79      0.79      0.79       514\n",
            "          statistics       0.61      0.86      0.72       637\n",
            "    theory_computing       0.77      0.60      0.68       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.74      0.79      0.76     10364\n",
            "        weighted avg       0.78      0.76      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7721, Train: 0.8125, Test: 0.6397\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4947, Train: 0.9000, Test: 0.7222\n",
            "Early stopping:  0.19612327887489556\n",
            "Epoch: 003, Loss: 2.1626, Train: 0.8938, Test: 0.7370\n",
            "Early stopping:  0.3051463608672153\n",
            "Epoch: 004, Loss: 1.7870, Train: 0.8938, Test: 0.7493\n",
            "Early stopping:  0.425334712593824\n",
            "Epoch: 005, Loss: 1.4108, Train: 0.9062, Test: 0.7651\n",
            "Early stopping:  0.5434007216773648\n",
            "Epoch: 006, Loss: 1.0666, Train: 0.9250, Test: 0.7804\n",
            "Early stopping:  0.5706150700896693\n",
            "Epoch: 007, Loss: 0.7799, Train: 0.9375, Test: 0.7916\n",
            "Early stopping:  0.5519492370668616\n",
            "Epoch: 008, Loss: 0.5623, Train: 0.9563, Test: 0.7985\n",
            "Early stopping:  0.48963616101284607\n",
            "Epoch: 009, Loss: 0.4073, Train: 0.9750, Test: 0.7979\n",
            "Early stopping:  0.401546524588256\n",
            "Epoch: 010, Loss: 0.2988, Train: 0.9812, Test: 0.7980\n",
            "Early stopping:  0.30688910251884416\n",
            "Epoch: 011, Loss: 0.2218, Train: 0.9812, Test: 0.7982\n",
            "Early stopping:  0.2225546937233496\n",
            "Epoch: 012, Loss: 0.1652, Train: 0.9812, Test: 0.7961\n",
            "Early stopping:  0.15793820495831365\n",
            "Epoch: 013, Loss: 0.1221, Train: 0.9938, Test: 0.7957\n",
            "Early stopping:  0.11313958930706862\n",
            "Epoch: 014, Loss: 0.0895, Train: 1.0000, Test: 0.7955\n",
            "Early stopping:  0.08307975043256002\n",
            "Epoch: 015, Loss: 0.0653, Train: 1.0000, Test: 0.7950\n",
            "Early stopping:  0.06227609718284907\n",
            "Epoch: 016, Loss: 0.0477, Train: 1.0000, Test: 0.7928\n",
            "Early stopping:  0.04682357708680399\n",
            "Epoch: 017, Loss: 0.0352, Train: 1.0000, Test: 0.7923\n",
            "Early stopping:  0.034688304907442805\n",
            "Epoch: 018, Loss: 0.0264, Train: 1.0000, Test: 0.7904\n",
            "Early stopping:  0.025204776358389587\n",
            "Epoch: 019, Loss: 0.0202, Train: 1.0000, Test: 0.7897\n",
            "Early stopping:  0.018010615952130864\n",
            "Epoch: 020, Loss: 0.0158, Train: 1.0000, Test: 0.7896\n",
            "Early stopping:  0.012715417182132562\n",
            "Epoch: 021, Loss: 0.0127, Train: 1.0000, Test: 0.7895\n",
            "Early stopping:  0.00894403870728666\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.76      0.79       392\n",
            "             ecology       0.77      0.83      0.80       879\n",
            "            economic       0.80      0.68      0.74      1399\n",
            "          geophysics       0.95      0.89      0.92      1192\n",
            "  gravitional_theory       0.88      0.93      0.90       120\n",
            "               hydro       0.72      0.78      0.75       345\n",
            "                math       0.87      0.77      0.82      1329\n",
            "              metals       0.71      0.90      0.79       191\n",
            "          networking       0.73      0.91      0.81       335\n",
            "        neuroscience       0.91      0.96      0.94       297\n",
            "        oceanography       0.87      0.82      0.84       980\n",
            "             politic       0.71      0.81      0.76       593\n",
            "           sociology       0.58      0.66      0.62       729\n",
            "software_engineering       0.86      0.78      0.82       514\n",
            "          statistics       0.69      0.76      0.73       637\n",
            "    theory_computing       0.63      0.78      0.70       432\n",
            "\n",
            "            accuracy                           0.79     10364\n",
            "           macro avg       0.78      0.81      0.79     10364\n",
            "        weighted avg       0.80      0.79      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7763, Train: 0.7375, Test: 0.5205\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5134, Train: 0.8938, Test: 0.7290\n",
            "Early stopping:  0.18590587127149058\n",
            "Epoch: 003, Loss: 2.1824, Train: 0.8938, Test: 0.7501\n",
            "Early stopping:  0.2976447941244377\n",
            "Epoch: 004, Loss: 1.7989, Train: 0.9187, Test: 0.7628\n",
            "Early stopping:  0.4227409972235792\n",
            "Epoch: 005, Loss: 1.4114, Train: 0.9375, Test: 0.7795\n",
            "Early stopping:  0.5462032861413387\n",
            "Epoch: 006, Loss: 1.0565, Train: 0.9375, Test: 0.7951\n",
            "Early stopping:  0.5828257283169106\n",
            "Epoch: 007, Loss: 0.7586, Train: 0.9500, Test: 0.7943\n",
            "Early stopping:  0.568350979366165\n",
            "Epoch: 008, Loss: 0.5304, Train: 0.9625, Test: 0.7891\n",
            "Early stopping:  0.5068648763695436\n",
            "Epoch: 009, Loss: 0.3706, Train: 0.9688, Test: 0.7810\n",
            "Early stopping:  0.4168578345969681\n",
            "Epoch: 010, Loss: 0.2623, Train: 0.9812, Test: 0.7786\n",
            "Early stopping:  0.3181853912822142\n",
            "Epoch: 011, Loss: 0.1859, Train: 0.9875, Test: 0.7780\n",
            "Early stopping:  0.22856273261558427\n",
            "Epoch: 012, Loss: 0.1302, Train: 1.0000, Test: 0.7789\n",
            "Early stopping:  0.15911203608960806\n",
            "Epoch: 013, Loss: 0.0905, Train: 1.0000, Test: 0.7792\n",
            "Early stopping:  0.11149817789566126\n",
            "Epoch: 014, Loss: 0.0629, Train: 1.0000, Test: 0.7808\n",
            "Early stopping:  0.07960191158727883\n",
            "Epoch: 015, Loss: 0.0439, Train: 1.0000, Test: 0.7823\n",
            "Early stopping:  0.05672139703073625\n",
            "Epoch: 016, Loss: 0.0308, Train: 1.0000, Test: 0.7823\n",
            "Early stopping:  0.03968721152806387\n",
            "Epoch: 017, Loss: 0.0219, Train: 1.0000, Test: 0.7809\n",
            "Early stopping:  0.027398999617036863\n",
            "Epoch: 018, Loss: 0.0159, Train: 1.0000, Test: 0.7803\n",
            "Early stopping:  0.01879561736222456\n",
            "Epoch: 019, Loss: 0.0119, Train: 1.0000, Test: 0.7793\n",
            "Early stopping:  0.012805674772806011\n",
            "Epoch: 020, Loss: 0.0092, Train: 1.0000, Test: 0.7784\n",
            "Early stopping:  0.008632539282507642\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.80      0.76       392\n",
            "             ecology       0.86      0.73      0.79       879\n",
            "            economic       0.75      0.73      0.74      1399\n",
            "          geophysics       0.94      0.89      0.91      1192\n",
            "  gravitional_theory       0.87      0.92      0.89       120\n",
            "               hydro       0.57      0.92      0.70       345\n",
            "                math       0.93      0.63      0.75      1329\n",
            "              metals       0.77      0.88      0.82       191\n",
            "          networking       0.87      0.86      0.86       335\n",
            "        neuroscience       0.90      0.98      0.94       297\n",
            "        oceanography       0.80      0.84      0.82       980\n",
            "             politic       0.73      0.79      0.76       593\n",
            "           sociology       0.63      0.62      0.63       729\n",
            "software_engineering       0.75      0.89      0.81       514\n",
            "          statistics       0.64      0.78      0.70       637\n",
            "    theory_computing       0.71      0.71      0.71       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.78      0.81      0.79     10364\n",
            "        weighted avg       0.79      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7769, Train: 0.7875, Test: 0.6442\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5145, Train: 0.8688, Test: 0.7201\n",
            "Early stopping:  0.1855391936894458\n",
            "Epoch: 003, Loss: 2.1944, Train: 0.8750, Test: 0.7231\n",
            "Early stopping:  0.2917616887221781\n",
            "Epoch: 004, Loss: 1.8223, Train: 0.8938, Test: 0.7290\n",
            "Early stopping:  0.4122699123410083\n",
            "Epoch: 005, Loss: 1.4457, Train: 0.9125, Test: 0.7486\n",
            "Early stopping:  0.5318199671800059\n",
            "Epoch: 006, Loss: 1.0981, Train: 0.9250, Test: 0.7719\n",
            "Early stopping:  0.5665051151251661\n",
            "Epoch: 007, Loss: 0.8025, Train: 0.9375, Test: 0.7849\n",
            "Early stopping:  0.555269196631278\n",
            "Epoch: 008, Loss: 0.5735, Train: 0.9500, Test: 0.7865\n",
            "Early stopping:  0.4988186765952986\n",
            "Epoch: 009, Loss: 0.4102, Train: 0.9563, Test: 0.7850\n",
            "Early stopping:  0.41449120451835997\n",
            "Epoch: 010, Loss: 0.2960, Train: 0.9688, Test: 0.7812\n",
            "Early stopping:  0.32082036247694856\n",
            "Epoch: 011, Loss: 0.2140, Train: 0.9750, Test: 0.7793\n",
            "Early stopping:  0.2345630349873697\n",
            "Epoch: 012, Loss: 0.1543, Train: 0.9938, Test: 0.7792\n",
            "Early stopping:  0.16673997844355673\n",
            "Epoch: 013, Loss: 0.1107, Train: 1.0000, Test: 0.7821\n",
            "Early stopping:  0.11917371148261814\n",
            "Epoch: 014, Loss: 0.0788, Train: 1.0000, Test: 0.7816\n",
            "Early stopping:  0.0864365175975086\n",
            "Epoch: 015, Loss: 0.0561, Train: 1.0000, Test: 0.7822\n",
            "Early stopping:  0.06290400549286529\n",
            "Epoch: 016, Loss: 0.0405, Train: 1.0000, Test: 0.7813\n",
            "Early stopping:  0.04546837795376028\n",
            "Epoch: 017, Loss: 0.0295, Train: 1.0000, Test: 0.7791\n",
            "Early stopping:  0.03242498080926313\n",
            "Epoch: 018, Loss: 0.0215, Train: 1.0000, Test: 0.7766\n",
            "Early stopping:  0.022793384126053827\n",
            "Epoch: 019, Loss: 0.0159, Train: 1.0000, Test: 0.7750\n",
            "Early stopping:  0.016030704760025435\n",
            "Epoch: 020, Loss: 0.0120, Train: 1.0000, Test: 0.7734\n",
            "Early stopping:  0.011376402426438066\n",
            "Epoch: 021, Loss: 0.0093, Train: 1.0000, Test: 0.7716\n",
            "Early stopping:  0.008055852498055145\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.69      0.68       392\n",
            "             ecology       0.81      0.81      0.81       879\n",
            "            economic       0.84      0.63      0.72      1399\n",
            "          geophysics       0.91      0.86      0.88      1192\n",
            "  gravitional_theory       0.90      0.91      0.90       120\n",
            "               hydro       0.63      0.80      0.71       345\n",
            "                math       0.91      0.71      0.80      1329\n",
            "              metals       0.54      0.93      0.68       191\n",
            "          networking       0.80      0.81      0.81       335\n",
            "        neuroscience       0.90      0.96      0.93       297\n",
            "        oceanography       0.85      0.85      0.85       980\n",
            "             politic       0.72      0.74      0.73       593\n",
            "           sociology       0.55      0.71      0.62       729\n",
            "software_engineering       0.83      0.84      0.83       514\n",
            "          statistics       0.64      0.75      0.69       637\n",
            "    theory_computing       0.68      0.80      0.74       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.76      0.80      0.77     10364\n",
            "        weighted avg       0.79      0.77      0.77     10364\n",
            "\n",
            "time: 9.57 s (started: 2024-08-16 14:11:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "npy4wP2uuuCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3f29e9-ad61-442e-e384-ac89f75d608c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 419 ms (started: 2024-08-16 14:12:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 20 ❎"
      ],
      "metadata": {
        "id": "CWUrgueftApV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "kO6hxpXQuwq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycTOAdm4uwrC",
        "outputId": "06ab7285-7363-4b1a-d059-ddc098158a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7326, Train: 0.6156, Test: 0.5724\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.4317, Train: 0.7688, Test: 0.6531\n",
            "Early stopping:  1.6269482629452863\n",
            "Epoch: 003, Loss: 0.9967, Train: 0.8219, Test: 0.7488\n",
            "Early stopping:  1.4701592605030074\n",
            "Epoch: 004, Loss: 0.6394, Train: 0.8562, Test: 0.7496\n",
            "Early stopping:  1.3931768761189545\n",
            "Epoch: 005, Loss: 0.5610, Train: 0.8719, Test: 0.7308\n",
            "Early stopping:  1.3096682531375576\n",
            "Epoch: 006, Loss: 0.5034, Train: 0.9187, Test: 0.7628\n",
            "Early stopping:  0.388963047345332\n",
            "Epoch: 007, Loss: 0.3068, Train: 0.9219, Test: 0.7819\n",
            "Early stopping:  0.2528485931318276\n",
            "Epoch: 008, Loss: 0.2602, Train: 0.9375, Test: 0.7927\n",
            "Early stopping:  0.16393435989873603\n",
            "Epoch: 009, Loss: 0.2342, Train: 0.9375, Test: 0.7939\n",
            "Early stopping:  0.1489263953374671\n",
            "Epoch: 010, Loss: 0.1595, Train: 0.9563, Test: 0.7770\n",
            "Early stopping:  0.1292349331581183\n",
            "Epoch: 011, Loss: 0.1498, Train: 0.9625, Test: 0.7679\n",
            "Early stopping:  0.06690929023201668\n",
            "Epoch: 012, Loss: 0.1273, Train: 0.9719, Test: 0.7671\n",
            "Early stopping:  0.05760437315954256\n",
            "Epoch: 013, Loss: 0.0786, Train: 0.9781, Test: 0.7717\n",
            "Early stopping:  0.05653870077105849\n",
            "Epoch: 014, Loss: 0.0537, Train: 0.9812, Test: 0.7719\n",
            "Early stopping:  0.045889178908006326\n",
            "Epoch: 015, Loss: 0.0710, Train: 0.9844, Test: 0.7695\n",
            "Early stopping:  0.040607839076237946\n",
            "Epoch: 016, Loss: 0.0644, Train: 0.9938, Test: 0.7745\n",
            "Early stopping:  0.02852425177538568\n",
            "Epoch: 017, Loss: 0.0282, Train: 0.9969, Test: 0.7728\n",
            "Early stopping:  0.019586127096245146\n",
            "Epoch: 018, Loss: 0.0202, Train: 0.9969, Test: 0.7714\n",
            "Early stopping:  0.022320505203690402\n",
            "Epoch: 019, Loss: 0.0191, Train: 0.9969, Test: 0.7715\n",
            "Early stopping:  0.025108004774194805\n",
            "Epoch: 020, Loss: 0.0170, Train: 1.0000, Test: 0.7708\n",
            "Early stopping:  0.01979398212159458\n",
            "Epoch: 021, Loss: 0.0146, Train: 1.0000, Test: 0.7696\n",
            "Early stopping:  0.005148266780243386\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.73      0.73       382\n",
            "             ecology       0.82      0.83      0.83       869\n",
            "            economic       0.82      0.68      0.74      1389\n",
            "          geophysics       0.95      0.90      0.92      1182\n",
            "  gravitional_theory       0.78      0.95      0.85       110\n",
            "               hydro       0.61      0.79      0.68       335\n",
            "                math       0.88      0.66      0.75      1319\n",
            "              metals       0.68      0.86      0.76       181\n",
            "          networking       0.73      0.75      0.74       325\n",
            "        neuroscience       0.93      0.96      0.95       287\n",
            "        oceanography       0.88      0.78      0.83       970\n",
            "             politic       0.78      0.65      0.71       583\n",
            "           sociology       0.60      0.68      0.63       719\n",
            "software_engineering       0.80      0.86      0.83       504\n",
            "          statistics       0.50      0.90      0.64       627\n",
            "    theory_computing       0.74      0.77      0.75       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.76      0.80      0.77     10204\n",
            "        weighted avg       0.79      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.1384, Train: 0.4375, Test: 0.4081\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 3.2503, Train: 0.7469, Test: 0.6425\n",
            "Early stopping:  1.3350855516921962\n",
            "Epoch: 003, Loss: 1.2091, Train: 0.7531, Test: 0.6486\n",
            "Early stopping:  1.9651573966590932\n",
            "Epoch: 004, Loss: 1.1405, Train: 0.8063, Test: 0.6371\n",
            "Early stopping:  1.906363466165915\n",
            "Epoch: 005, Loss: 0.6747, Train: 0.8063, Test: 0.6608\n",
            "Early stopping:  1.879782578121895\n",
            "Epoch: 006, Loss: 0.6730, Train: 0.8719, Test: 0.7046\n",
            "Early stopping:  1.0702307235867277\n",
            "Epoch: 007, Loss: 0.5139, Train: 0.9281, Test: 0.7431\n",
            "Early stopping:  0.311473845706596\n",
            "Epoch: 008, Loss: 0.3343, Train: 0.9437, Test: 0.7720\n",
            "Early stopping:  0.2993554888652705\n",
            "Epoch: 009, Loss: 0.2301, Train: 0.9219, Test: 0.7748\n",
            "Early stopping:  0.19993358918810908\n",
            "Epoch: 010, Loss: 0.3053, Train: 0.9219, Test: 0.7552\n",
            "Early stopping:  0.17960319386271326\n",
            "Epoch: 011, Loss: 0.2256, Train: 0.9656, Test: 0.7648\n",
            "Early stopping:  0.11727090947861998\n",
            "Epoch: 012, Loss: 0.1256, Train: 0.9781, Test: 0.7537\n",
            "Early stopping:  0.08131776573819469\n",
            "Epoch: 013, Loss: 0.1427, Train: 0.9750, Test: 0.7499\n",
            "Early stopping:  0.07295499533446317\n",
            "Epoch: 014, Loss: 0.1505, Train: 0.9812, Test: 0.7525\n",
            "Early stopping:  0.07501151489631011\n",
            "Epoch: 015, Loss: 0.1312, Train: 0.9875, Test: 0.7581\n",
            "Early stopping:  0.040580678505212284\n",
            "Epoch: 016, Loss: 0.0882, Train: 0.9906, Test: 0.7727\n",
            "Early stopping:  0.024073068488262593\n",
            "Epoch: 017, Loss: 0.0409, Train: 0.9844, Test: 0.7829\n",
            "Early stopping:  0.045831530211968154\n",
            "Epoch: 018, Loss: 0.0586, Train: 0.9812, Test: 0.7829\n",
            "Early stopping:  0.04657552615625387\n",
            "Epoch: 019, Loss: 0.0923, Train: 0.9906, Test: 0.7792\n",
            "Early stopping:  0.034647035059958115\n",
            "Epoch: 020, Loss: 0.0456, Train: 0.9938, Test: 0.7656\n",
            "Early stopping:  0.023903410616532356\n",
            "Epoch: 021, Loss: 0.0224, Train: 0.9938, Test: 0.7500\n",
            "Early stopping:  0.026038577738301684\n",
            "Epoch: 022, Loss: 0.0403, Train: 0.9906, Test: 0.7455\n",
            "Early stopping:  0.026108866463034636\n",
            "Epoch: 023, Loss: 0.0454, Train: 0.9938, Test: 0.7475\n",
            "Early stopping:  0.02591902583467475\n",
            "Epoch: 024, Loss: 0.0326, Train: 1.0000, Test: 0.7550\n",
            "Early stopping:  0.009853944385497889\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.78      0.70       382\n",
            "             ecology       0.79      0.83      0.81       869\n",
            "            economic       0.79      0.67      0.72      1389\n",
            "          geophysics       0.92      0.78      0.84      1182\n",
            "  gravitional_theory       0.48      0.95      0.63       110\n",
            "               hydro       0.57      0.83      0.67       335\n",
            "                math       0.93      0.62      0.74      1319\n",
            "              metals       0.51      0.94      0.67       181\n",
            "          networking       0.74      0.92      0.82       325\n",
            "        neuroscience       0.93      0.96      0.95       287\n",
            "        oceanography       0.89      0.79      0.84       970\n",
            "             politic       0.74      0.71      0.73       583\n",
            "           sociology       0.59      0.66      0.62       719\n",
            "software_engineering       0.79      0.81      0.80       504\n",
            "          statistics       0.62      0.83      0.71       627\n",
            "    theory_computing       0.69      0.72      0.71       422\n",
            "\n",
            "            accuracy                           0.75     10204\n",
            "           macro avg       0.73      0.80      0.75     10204\n",
            "        weighted avg       0.78      0.75      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.0605, Train: 0.7375, Test: 0.6887\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1579, Train: 0.7781, Test: 0.6763\n",
            "Early stopping:  2.0524872063897117\n",
            "Epoch: 003, Loss: 0.9689, Train: 0.8406, Test: 0.7116\n",
            "Early stopping:  1.7329705713344947\n",
            "Epoch: 004, Loss: 0.6755, Train: 0.8656, Test: 0.7770\n",
            "Early stopping:  1.5757747336172045\n",
            "Epoch: 005, Loss: 0.3971, Train: 0.8906, Test: 0.7844\n",
            "Early stopping:  1.486618432912006\n",
            "Epoch: 006, Loss: 0.3063, Train: 0.9375, Test: 0.7917\n",
            "Early stopping:  0.36375988258835534\n",
            "Epoch: 007, Loss: 0.1997, Train: 0.9594, Test: 0.7745\n",
            "Early stopping:  0.3116512857910296\n",
            "Epoch: 008, Loss: 0.1581, Train: 0.9594, Test: 0.7649\n",
            "Early stopping:  0.20581879123482058\n",
            "Epoch: 009, Loss: 0.1327, Train: 0.9781, Test: 0.7736\n",
            "Early stopping:  0.110600728123181\n",
            "Epoch: 010, Loss: 0.0925, Train: 0.9812, Test: 0.7706\n",
            "Early stopping:  0.08171581592149833\n",
            "Epoch: 011, Loss: 0.0742, Train: 0.9844, Test: 0.7722\n",
            "Early stopping:  0.05040511805903631\n",
            "Epoch: 012, Loss: 0.0579, Train: 0.9938, Test: 0.7801\n",
            "Early stopping:  0.041500863315764636\n",
            "Epoch: 013, Loss: 0.0358, Train: 0.9969, Test: 0.7765\n",
            "Early stopping:  0.03673033339529267\n",
            "Epoch: 014, Loss: 0.0284, Train: 0.9969, Test: 0.7714\n",
            "Early stopping:  0.0265377950611121\n",
            "Epoch: 015, Loss: 0.0237, Train: 0.9938, Test: 0.7645\n",
            "Early stopping:  0.021373770897978685\n",
            "Epoch: 016, Loss: 0.0189, Train: 0.9969, Test: 0.7605\n",
            "Early stopping:  0.01527238233905187\n",
            "Epoch: 017, Loss: 0.0167, Train: 0.9969, Test: 0.7613\n",
            "Early stopping:  0.0076767821752538505\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.73      0.74       382\n",
            "             ecology       0.82      0.84      0.83       869\n",
            "            economic       0.84      0.69      0.76      1389\n",
            "          geophysics       0.88      0.82      0.85      1182\n",
            "  gravitional_theory       0.81      0.91      0.86       110\n",
            "               hydro       0.56      0.81      0.66       335\n",
            "                math       0.88      0.63      0.73      1319\n",
            "              metals       0.55      0.91      0.69       181\n",
            "          networking       0.74      0.84      0.79       325\n",
            "        neuroscience       0.87      0.95      0.91       287\n",
            "        oceanography       0.78      0.86      0.81       970\n",
            "             politic       0.74      0.77      0.75       583\n",
            "           sociology       0.64      0.68      0.66       719\n",
            "software_engineering       0.69      0.76      0.72       504\n",
            "          statistics       0.67      0.83      0.74       627\n",
            "    theory_computing       0.64      0.58      0.61       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.74      0.79      0.76     10204\n",
            "        weighted avg       0.77      0.76      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.2220, Train: 0.6344, Test: 0.6224\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.4906, Train: 0.7250, Test: 0.6697\n",
            "Early stopping:  1.9313645816147402\n",
            "Epoch: 003, Loss: 1.5656, Train: 0.8250, Test: 0.7401\n",
            "Early stopping:  1.5557703964303364\n",
            "Epoch: 004, Loss: 0.6838, Train: 0.8344, Test: 0.6963\n",
            "Early stopping:  1.5402682629906022\n",
            "Epoch: 005, Loss: 0.6511, Train: 0.8313, Test: 0.6687\n",
            "Early stopping:  1.4622379469453166\n",
            "Epoch: 006, Loss: 0.6035, Train: 0.8812, Test: 0.6988\n",
            "Early stopping:  0.48463858815146105\n",
            "Epoch: 007, Loss: 0.3766, Train: 0.9094, Test: 0.7354\n",
            "Early stopping:  0.45737349726350013\n",
            "Epoch: 008, Loss: 0.2667, Train: 0.9219, Test: 0.7593\n",
            "Early stopping:  0.18413388165723468\n",
            "Epoch: 009, Loss: 0.2289, Train: 0.9313, Test: 0.7622\n",
            "Early stopping:  0.1928729465587271\n",
            "Epoch: 010, Loss: 0.2164, Train: 0.9406, Test: 0.7521\n",
            "Early stopping:  0.16104515303968045\n",
            "Epoch: 011, Loss: 0.1751, Train: 0.9625, Test: 0.7461\n",
            "Early stopping:  0.07657850856289015\n",
            "Epoch: 012, Loss: 0.1184, Train: 0.9844, Test: 0.7383\n",
            "Early stopping:  0.05663156883743164\n",
            "Epoch: 013, Loss: 0.0744, Train: 0.9875, Test: 0.7236\n",
            "Early stopping:  0.06552642595026621\n",
            "Epoch: 014, Loss: 0.0627, Train: 0.9844, Test: 0.7101\n",
            "Early stopping:  0.0656658425431425\n",
            "Epoch: 015, Loss: 0.0719, Train: 0.9875, Test: 0.7024\n",
            "Early stopping:  0.046956001537044656\n",
            "Epoch: 016, Loss: 0.0645, Train: 0.9938, Test: 0.7139\n",
            "Early stopping:  0.02289720828117463\n",
            "Epoch: 017, Loss: 0.0459, Train: 0.9969, Test: 0.7237\n",
            "Early stopping:  0.011187382440816523\n",
            "Epoch: 018, Loss: 0.0317, Train: 0.9969, Test: 0.7299\n",
            "Early stopping:  0.0162833792354572\n",
            "Epoch: 019, Loss: 0.0233, Train: 0.9969, Test: 0.7356\n",
            "Early stopping:  0.020737189621694102\n",
            "Epoch: 020, Loss: 0.0186, Train: 0.9969, Test: 0.7416\n",
            "Early stopping:  0.01863165925636308\n",
            "Epoch: 021, Loss: 0.0148, Train: 0.9969, Test: 0.7449\n",
            "Early stopping:  0.01236669329123286\n",
            "Epoch: 022, Loss: 0.0116, Train: 1.0000, Test: 0.7455\n",
            "Early stopping:  0.007849447103610506\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.79      0.71       382\n",
            "             ecology       0.80      0.77      0.78       869\n",
            "            economic       0.83      0.61      0.70      1389\n",
            "          geophysics       0.96      0.82      0.88      1182\n",
            "  gravitional_theory       0.42      0.97      0.58       110\n",
            "               hydro       0.57      0.84      0.68       335\n",
            "                math       0.83      0.74      0.79      1319\n",
            "              metals       0.42      0.93      0.58       181\n",
            "          networking       0.87      0.68      0.76       325\n",
            "        neuroscience       0.85      0.98      0.91       287\n",
            "        oceanography       0.84      0.85      0.84       970\n",
            "             politic       0.60      0.70      0.64       583\n",
            "           sociology       0.59      0.65      0.62       719\n",
            "software_engineering       0.84      0.73      0.78       504\n",
            "          statistics       0.77      0.67      0.72       627\n",
            "    theory_computing       0.57      0.70      0.63       422\n",
            "\n",
            "            accuracy                           0.75     10204\n",
            "           macro avg       0.71      0.78      0.73     10204\n",
            "        weighted avg       0.77      0.75      0.75     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.4923, Train: 0.6188, Test: 0.5651\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.3986, Train: 0.8031, Test: 0.7010\n",
            "Early stopping:  2.1875571505676055\n",
            "Epoch: 003, Loss: 0.7846, Train: 0.8281, Test: 0.7387\n",
            "Early stopping:  1.9872605885204189\n",
            "Epoch: 004, Loss: 0.5381, Train: 0.8906, Test: 0.7711\n",
            "Early stopping:  1.8287717027336081\n",
            "Epoch: 005, Loss: 0.3776, Train: 0.9000, Test: 0.7760\n",
            "Early stopping:  1.7073013123185876\n",
            "Epoch: 006, Loss: 0.2673, Train: 0.9437, Test: 0.7826\n",
            "Early stopping:  0.44974484638007556\n",
            "Epoch: 007, Loss: 0.1806, Train: 0.9594, Test: 0.7712\n",
            "Early stopping:  0.2391779729084016\n",
            "Epoch: 008, Loss: 0.1271, Train: 0.9594, Test: 0.7589\n",
            "Early stopping:  0.164228259600518\n",
            "Epoch: 009, Loss: 0.1171, Train: 0.9563, Test: 0.7536\n",
            "Early stopping:  0.10915801954408759\n",
            "Epoch: 010, Loss: 0.1004, Train: 0.9875, Test: 0.7604\n",
            "Early stopping:  0.06781459865338546\n",
            "Epoch: 011, Loss: 0.0701, Train: 0.9906, Test: 0.7710\n",
            "Early stopping:  0.04062891060187613\n",
            "Epoch: 012, Loss: 0.0350, Train: 0.9969, Test: 0.7731\n",
            "Early stopping:  0.03754481102083507\n",
            "Epoch: 013, Loss: 0.0278, Train: 0.9906, Test: 0.7730\n",
            "Early stopping:  0.0391964235493822\n",
            "Epoch: 014, Loss: 0.0337, Train: 0.9938, Test: 0.7740\n",
            "Early stopping:  0.031105943744147706\n",
            "Epoch: 015, Loss: 0.0286, Train: 0.9969, Test: 0.7744\n",
            "Early stopping:  0.017634139594075723\n",
            "Epoch: 016, Loss: 0.0191, Train: 1.0000, Test: 0.7738\n",
            "Early stopping:  0.006265848877220739\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.85      0.75       382\n",
            "             ecology       0.80      0.79      0.80       869\n",
            "            economic       0.77      0.66      0.71      1389\n",
            "          geophysics       0.96      0.87      0.91      1182\n",
            "  gravitional_theory       0.80      0.90      0.85       110\n",
            "               hydro       0.69      0.77      0.72       335\n",
            "                math       0.92      0.79      0.85      1319\n",
            "              metals       0.46      0.97      0.62       181\n",
            "          networking       0.70      0.93      0.80       325\n",
            "        neuroscience       0.88      0.99      0.93       287\n",
            "        oceanography       0.83      0.79      0.81       970\n",
            "             politic       0.70      0.77      0.73       583\n",
            "           sociology       0.58      0.52      0.55       719\n",
            "software_engineering       0.82      0.86      0.84       504\n",
            "          statistics       0.72      0.79      0.75       627\n",
            "    theory_computing       0.63      0.63      0.63       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.77     10204\n",
            "        weighted avg       0.79      0.77      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.9997, Train: 0.5437, Test: 0.5333\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.6982, Train: 0.7250, Test: 0.6832\n",
            "Early stopping:  1.6273632408157246\n",
            "Epoch: 003, Loss: 1.5787, Train: 0.8438, Test: 0.7770\n",
            "Early stopping:  1.3645564213258878\n",
            "Epoch: 004, Loss: 0.6658, Train: 0.8469, Test: 0.7338\n",
            "Early stopping:  1.4197035198808376\n",
            "Epoch: 005, Loss: 0.6928, Train: 0.8469, Test: 0.7321\n",
            "Early stopping:  1.3586512069342398\n",
            "Epoch: 006, Loss: 0.5765, Train: 0.8750, Test: 0.7612\n",
            "Early stopping:  0.5474646060578875\n",
            "Epoch: 007, Loss: 0.3852, Train: 0.8938, Test: 0.7596\n",
            "Early stopping:  0.4625566541854054\n",
            "Epoch: 008, Loss: 0.3183, Train: 0.9156, Test: 0.7642\n",
            "Early stopping:  0.16795616350770137\n",
            "Epoch: 009, Loss: 0.2408, Train: 0.9531, Test: 0.7734\n",
            "Early stopping:  0.1870833219689823\n",
            "Epoch: 010, Loss: 0.1400, Train: 0.9688, Test: 0.7586\n",
            "Early stopping:  0.16431464616773936\n",
            "Epoch: 011, Loss: 0.1328, Train: 0.9563, Test: 0.7421\n",
            "Early stopping:  0.11029046230937248\n",
            "Epoch: 012, Loss: 0.1522, Train: 0.9594, Test: 0.7418\n",
            "Early stopping:  0.08064725254537342\n",
            "Epoch: 013, Loss: 0.1171, Train: 0.9844, Test: 0.7484\n",
            "Early stopping:  0.04876071998834241\n",
            "Epoch: 014, Loss: 0.0831, Train: 0.9812, Test: 0.7508\n",
            "Early stopping:  0.026653921594219854\n",
            "Epoch: 015, Loss: 0.0727, Train: 0.9875, Test: 0.7542\n",
            "Early stopping:  0.03337496193331989\n",
            "Epoch: 016, Loss: 0.0541, Train: 0.9781, Test: 0.7522\n",
            "Early stopping:  0.03894701949299698\n",
            "Epoch: 017, Loss: 0.0704, Train: 0.9844, Test: 0.7532\n",
            "Early stopping:  0.023449096881972872\n",
            "Epoch: 018, Loss: 0.0566, Train: 0.9812, Test: 0.7546\n",
            "Early stopping:  0.012019834698655918\n",
            "Epoch: 019, Loss: 0.0577, Train: 0.9938, Test: 0.7582\n",
            "Early stopping:  0.008559011451409566\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.77      0.76       382\n",
            "             ecology       0.88      0.69      0.77       869\n",
            "            economic       0.79      0.58      0.67      1389\n",
            "          geophysics       0.87      0.92      0.89      1182\n",
            "  gravitional_theory       0.71      0.95      0.81       110\n",
            "               hydro       0.58      0.75      0.66       335\n",
            "                math       0.84      0.71      0.77      1319\n",
            "              metals       0.50      0.92      0.65       181\n",
            "          networking       0.77      0.87      0.82       325\n",
            "        neuroscience       0.91      0.96      0.94       287\n",
            "        oceanography       0.80      0.85      0.83       970\n",
            "             politic       0.64      0.78      0.70       583\n",
            "           sociology       0.57      0.70      0.63       719\n",
            "software_engineering       0.89      0.75      0.81       504\n",
            "          statistics       0.75      0.72      0.74       627\n",
            "    theory_computing       0.65      0.78      0.71       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.74      0.79      0.76     10204\n",
            "        weighted avg       0.77      0.76      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.6407, Train: 0.6375, Test: 0.5823\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.6879, Train: 0.7688, Test: 0.6417\n",
            "Early stopping:  2.0879199776391233\n",
            "Epoch: 003, Loss: 0.9361, Train: 0.8375, Test: 0.7136\n",
            "Early stopping:  1.9582282470813472\n",
            "Epoch: 004, Loss: 0.7357, Train: 0.8500, Test: 0.7190\n",
            "Early stopping:  1.807489691392462\n",
            "Epoch: 005, Loss: 0.5517, Train: 0.8688, Test: 0.7063\n",
            "Early stopping:  1.694059436753889\n",
            "Epoch: 006, Loss: 0.3916, Train: 0.8969, Test: 0.7240\n",
            "Early stopping:  0.5052726578814118\n",
            "Epoch: 007, Loss: 0.3359, Train: 0.9344, Test: 0.7562\n",
            "Early stopping:  0.2481983024841016\n",
            "Epoch: 008, Loss: 0.2129, Train: 0.9437, Test: 0.7692\n",
            "Early stopping:  0.20279370077527278\n",
            "Epoch: 009, Loss: 0.1532, Train: 0.9406, Test: 0.7691\n",
            "Early stopping:  0.15649084021851048\n",
            "Epoch: 010, Loss: 0.1867, Train: 0.9406, Test: 0.7558\n",
            "Early stopping:  0.10248073657192164\n",
            "Epoch: 011, Loss: 0.1518, Train: 0.9750, Test: 0.7593\n",
            "Early stopping:  0.07585064342010779\n",
            "Epoch: 012, Loss: 0.0753, Train: 0.9844, Test: 0.7600\n",
            "Early stopping:  0.051736347336850765\n",
            "Epoch: 013, Loss: 0.0615, Train: 0.9938, Test: 0.7624\n",
            "Early stopping:  0.054344975360385836\n",
            "Epoch: 014, Loss: 0.0541, Train: 0.9875, Test: 0.7591\n",
            "Early stopping:  0.05961642863560867\n",
            "Epoch: 015, Loss: 0.0393, Train: 0.9875, Test: 0.7542\n",
            "Early stopping:  0.04409340309915803\n",
            "Epoch: 016, Loss: 0.0431, Train: 0.9906, Test: 0.7508\n",
            "Early stopping:  0.014515491504440472\n",
            "Epoch: 017, Loss: 0.0399, Train: 0.9969, Test: 0.7510\n",
            "Early stopping:  0.009774338473777445\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.80      0.74       382\n",
            "             ecology       0.86      0.82      0.84       869\n",
            "            economic       0.83      0.61      0.70      1389\n",
            "          geophysics       0.86      0.87      0.86      1182\n",
            "  gravitional_theory       0.76      0.94      0.84       110\n",
            "               hydro       0.52      0.86      0.65       335\n",
            "                math       0.93      0.54      0.68      1319\n",
            "              metals       0.59      0.92      0.72       181\n",
            "          networking       0.80      0.91      0.85       325\n",
            "        neuroscience       0.95      0.92      0.93       287\n",
            "        oceanography       0.86      0.81      0.83       970\n",
            "             politic       0.65      0.70      0.68       583\n",
            "           sociology       0.57      0.78      0.65       719\n",
            "software_engineering       0.74      0.88      0.80       504\n",
            "          statistics       0.59      0.83      0.69       627\n",
            "    theory_computing       0.70      0.55      0.62       422\n",
            "\n",
            "            accuracy                           0.75     10204\n",
            "           macro avg       0.74      0.80      0.76     10204\n",
            "        weighted avg       0.78      0.75      0.75     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.2730, Train: 0.5813, Test: 0.5054\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.8919, Train: 0.7312, Test: 0.6609\n",
            "Early stopping:  1.6837034620311473\n",
            "Epoch: 003, Loss: 2.0389, Train: 0.8469, Test: 0.7201\n",
            "Early stopping:  1.3343235363794113\n",
            "Epoch: 004, Loss: 0.7890, Train: 0.8531, Test: 0.6960\n",
            "Early stopping:  1.46055907939346\n",
            "Epoch: 005, Loss: 0.8735, Train: 0.8375, Test: 0.6654\n",
            "Early stopping:  1.4063633036195138\n",
            "Epoch: 006, Loss: 0.9852, Train: 0.8656, Test: 0.6995\n",
            "Early stopping:  0.5993833032557103\n",
            "Epoch: 007, Loss: 0.6011, Train: 0.9187, Test: 0.7539\n",
            "Early stopping:  0.5662434921131395\n",
            "Epoch: 008, Loss: 0.2629, Train: 0.9031, Test: 0.7423\n",
            "Early stopping:  0.28293333552088534\n",
            "Epoch: 009, Loss: 0.3610, Train: 0.9031, Test: 0.7304\n",
            "Early stopping:  0.31328534016770676\n",
            "Epoch: 010, Loss: 0.3939, Train: 0.9281, Test: 0.7516\n",
            "Early stopping:  0.28736924141820724\n",
            "Epoch: 011, Loss: 0.2773, Train: 0.9531, Test: 0.7594\n",
            "Early stopping:  0.13574182926825046\n",
            "Epoch: 012, Loss: 0.1836, Train: 0.9500, Test: 0.7534\n",
            "Early stopping:  0.08349933993054898\n",
            "Epoch: 013, Loss: 0.1416, Train: 0.9594, Test: 0.7525\n",
            "Early stopping:  0.10909739833945219\n",
            "Epoch: 014, Loss: 0.0948, Train: 0.9625, Test: 0.7478\n",
            "Early stopping:  0.11899016282593182\n",
            "Epoch: 015, Loss: 0.0878, Train: 0.9750, Test: 0.7427\n",
            "Early stopping:  0.07756303817062661\n",
            "Epoch: 016, Loss: 0.0883, Train: 0.9875, Test: 0.7510\n",
            "Early stopping:  0.04236465227728205\n",
            "Epoch: 017, Loss: 0.0669, Train: 0.9844, Test: 0.7566\n",
            "Early stopping:  0.027647006529548032\n",
            "Epoch: 018, Loss: 0.0490, Train: 0.9812, Test: 0.7591\n",
            "Early stopping:  0.019033175083717085\n",
            "Epoch: 019, Loss: 0.0498, Train: 0.9875, Test: 0.7593\n",
            "Early stopping:  0.01935489124519336\n",
            "Epoch: 020, Loss: 0.0458, Train: 0.9906, Test: 0.7586\n",
            "Early stopping:  0.01786458511273575\n",
            "Epoch: 021, Loss: 0.0395, Train: 0.9938, Test: 0.7572\n",
            "Early stopping:  0.01018862395070635\n",
            "Epoch: 022, Loss: 0.0316, Train: 0.9969, Test: 0.7545\n",
            "Early stopping:  0.007635549159951505\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.57      0.77      0.66       382\n",
            "             ecology       0.81      0.77      0.79       869\n",
            "            economic       0.71      0.71      0.71      1389\n",
            "          geophysics       0.96      0.86      0.91      1182\n",
            "  gravitional_theory       0.48      0.92      0.63       110\n",
            "               hydro       0.74      0.79      0.76       335\n",
            "                math       0.86      0.70      0.77      1319\n",
            "              metals       0.65      0.88      0.75       181\n",
            "          networking       0.84      0.82      0.83       325\n",
            "        neuroscience       0.93      0.92      0.93       287\n",
            "        oceanography       0.75      0.85      0.80       970\n",
            "             politic       0.73      0.70      0.72       583\n",
            "           sociology       0.54      0.60      0.57       719\n",
            "software_engineering       0.78      0.69      0.73       504\n",
            "          statistics       0.79      0.65      0.71       627\n",
            "    theory_computing       0.66      0.76      0.71       422\n",
            "\n",
            "            accuracy                           0.75     10204\n",
            "           macro avg       0.74      0.78      0.75     10204\n",
            "        weighted avg       0.77      0.75      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.7065, Train: 0.4250, Test: 0.4515\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 3.1718, Train: 0.7188, Test: 0.5821\n",
            "Early stopping:  1.0852624673673885\n",
            "Epoch: 003, Loss: 1.3177, Train: 0.8063, Test: 0.6906\n",
            "Early stopping:  1.6969276046339838\n",
            "Epoch: 004, Loss: 0.9246, Train: 0.8094, Test: 0.7136\n",
            "Early stopping:  1.7508304250907272\n",
            "Epoch: 005, Loss: 0.8331, Train: 0.8562, Test: 0.7248\n",
            "Early stopping:  1.6956055960399332\n",
            "Epoch: 006, Loss: 0.6677, Train: 0.8750, Test: 0.7375\n",
            "Early stopping:  1.028117340383015\n",
            "Epoch: 007, Loss: 0.6055, Train: 0.8750, Test: 0.7360\n",
            "Early stopping:  0.2809144882736472\n",
            "Epoch: 008, Loss: 0.5453, Train: 0.9062, Test: 0.7475\n",
            "Early stopping:  0.1588144237486003\n",
            "Epoch: 009, Loss: 0.4270, Train: 0.9219, Test: 0.7599\n",
            "Early stopping:  0.15050692598671345\n",
            "Epoch: 010, Loss: 0.3240, Train: 0.9313, Test: 0.7579\n",
            "Early stopping:  0.13842261057927657\n",
            "Epoch: 011, Loss: 0.2750, Train: 0.9375, Test: 0.7637\n",
            "Early stopping:  0.1407261264068376\n",
            "Epoch: 012, Loss: 0.2238, Train: 0.9625, Test: 0.7634\n",
            "Early stopping:  0.12835287633169182\n",
            "Epoch: 013, Loss: 0.1685, Train: 0.9563, Test: 0.7551\n",
            "Early stopping:  0.0988737512437128\n",
            "Epoch: 014, Loss: 0.1429, Train: 0.9688, Test: 0.7538\n",
            "Early stopping:  0.07452535101285492\n",
            "Epoch: 015, Loss: 0.1158, Train: 0.9625, Test: 0.7630\n",
            "Early stopping:  0.06410914263116241\n",
            "Epoch: 016, Loss: 0.0939, Train: 0.9625, Test: 0.7719\n",
            "Early stopping:  0.05035919565608189\n",
            "Epoch: 017, Loss: 0.0798, Train: 0.9719, Test: 0.7735\n",
            "Early stopping:  0.03603044612427219\n",
            "Epoch: 018, Loss: 0.0761, Train: 0.9844, Test: 0.7743\n",
            "Early stopping:  0.027811576619780422\n",
            "Epoch: 019, Loss: 0.0580, Train: 0.9906, Test: 0.7712\n",
            "Early stopping:  0.021586813170834662\n",
            "Epoch: 020, Loss: 0.0421, Train: 0.9906, Test: 0.7669\n",
            "Early stopping:  0.02016870368489139\n",
            "Epoch: 021, Loss: 0.0355, Train: 0.9938, Test: 0.7710\n",
            "Early stopping:  0.019744455457421355\n",
            "Epoch: 022, Loss: 0.0283, Train: 0.9906, Test: 0.7686\n",
            "Early stopping:  0.01915297241747952\n",
            "Epoch: 023, Loss: 0.0286, Train: 0.9906, Test: 0.7681\n",
            "Early stopping:  0.012286970867503962\n",
            "Epoch: 024, Loss: 0.0243, Train: 0.9969, Test: 0.7709\n",
            "Early stopping:  0.007073281817609481\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.84      0.79       382\n",
            "             ecology       0.74      0.82      0.78       869\n",
            "            economic       0.73      0.69      0.71      1389\n",
            "          geophysics       0.92      0.91      0.91      1182\n",
            "  gravitional_theory       0.54      0.97      0.69       110\n",
            "               hydro       0.78      0.78      0.78       335\n",
            "                math       0.89      0.73      0.80      1319\n",
            "              metals       0.57      0.94      0.71       181\n",
            "          networking       0.85      0.82      0.83       325\n",
            "        neuroscience       0.87      0.97      0.91       287\n",
            "        oceanography       0.83      0.80      0.82       970\n",
            "             politic       0.62      0.74      0.68       583\n",
            "           sociology       0.66      0.55      0.60       719\n",
            "software_engineering       0.84      0.78      0.81       504\n",
            "          statistics       0.74      0.77      0.75       627\n",
            "    theory_computing       0.66      0.67      0.66       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.76     10204\n",
            "        weighted avg       0.78      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.3786, Train: 0.4844, Test: 0.5263\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.6141, Train: 0.7469, Test: 0.6323\n",
            "Early stopping:  1.2477136174425665\n",
            "Epoch: 003, Loss: 1.4855, Train: 0.8219, Test: 0.7250\n",
            "Early stopping:  1.4581527774178369\n",
            "Epoch: 004, Loss: 0.7866, Train: 0.8500, Test: 0.7441\n",
            "Early stopping:  1.5675820159494345\n",
            "Epoch: 005, Loss: 0.6813, Train: 0.8594, Test: 0.7661\n",
            "Early stopping:  1.5419377590349885\n",
            "Epoch: 006, Loss: 0.4784, Train: 0.8875, Test: 0.7779\n",
            "Early stopping:  0.8719739131631058\n",
            "Epoch: 007, Loss: 0.3653, Train: 0.9156, Test: 0.7752\n",
            "Early stopping:  0.4382956695295001\n",
            "Epoch: 008, Loss: 0.2743, Train: 0.9406, Test: 0.7695\n",
            "Early stopping:  0.21396259395336698\n",
            "Epoch: 009, Loss: 0.2261, Train: 0.9500, Test: 0.7742\n",
            "Early stopping:  0.1819536386432132\n",
            "Epoch: 010, Loss: 0.1870, Train: 0.9469, Test: 0.7735\n",
            "Early stopping:  0.1170466951199979\n",
            "Epoch: 011, Loss: 0.1674, Train: 0.9688, Test: 0.7774\n",
            "Early stopping:  0.07915904997533162\n",
            "Epoch: 012, Loss: 0.1271, Train: 0.9812, Test: 0.7774\n",
            "Early stopping:  0.05631240901330374\n",
            "Epoch: 013, Loss: 0.0887, Train: 0.9906, Test: 0.7770\n",
            "Early stopping:  0.05317027445235717\n",
            "Epoch: 014, Loss: 0.0589, Train: 0.9969, Test: 0.7720\n",
            "Early stopping:  0.053211347445698234\n",
            "Epoch: 015, Loss: 0.0439, Train: 0.9906, Test: 0.7705\n",
            "Early stopping:  0.05047628241163602\n",
            "Epoch: 016, Loss: 0.0417, Train: 0.9938, Test: 0.7713\n",
            "Early stopping:  0.036017060149036266\n",
            "Epoch: 017, Loss: 0.0331, Train: 0.9969, Test: 0.7694\n",
            "Early stopping:  0.021884743369918126\n",
            "Epoch: 018, Loss: 0.0258, Train: 0.9938, Test: 0.7654\n",
            "Early stopping:  0.012475346536846621\n",
            "Epoch: 019, Loss: 0.0276, Train: 0.9938, Test: 0.7619\n",
            "Early stopping:  0.008170652895800077\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.65      0.81      0.72       382\n",
            "             ecology       0.89      0.70      0.79       869\n",
            "            economic       0.78      0.58      0.67      1389\n",
            "          geophysics       0.93      0.93      0.93      1182\n",
            "  gravitional_theory       0.80      0.86      0.83       110\n",
            "               hydro       0.59      0.85      0.69       335\n",
            "                math       0.91      0.77      0.83      1319\n",
            "              metals       0.72      0.90      0.80       181\n",
            "          networking       0.84      0.88      0.86       325\n",
            "        neuroscience       0.94      0.95      0.95       287\n",
            "        oceanography       0.79      0.83      0.81       970\n",
            "             politic       0.53      0.86      0.66       583\n",
            "           sociology       0.54      0.50      0.52       719\n",
            "software_engineering       0.84      0.75      0.79       504\n",
            "          statistics       0.69      0.78      0.73       627\n",
            "    theory_computing       0.66      0.71      0.68       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.76      0.79      0.77     10204\n",
            "        weighted avg       0.78      0.76      0.76     10204\n",
            "\n",
            "time: 7.59 s (started: 2024-08-16 14:12:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "3v3kkGZtuwrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaca2ea-9acc-4f04-838b-d3817296308e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 440 ms (started: 2024-08-16 14:12:08 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "UzXESjDCuwrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ced886-a3cd-4b69-e3f6-b53efe275e1d",
        "id": "KsRUWH9-uwrC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7886, Train: 0.6875, Test: 0.5631\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5296, Train: 0.8344, Test: 0.7245\n",
            "Early stopping:  0.1831494673788781\n",
            "Epoch: 003, Loss: 2.2169, Train: 0.8187, Test: 0.7222\n",
            "Early stopping:  0.28630885094650904\n",
            "Epoch: 004, Loss: 1.8589, Train: 0.8406, Test: 0.7378\n",
            "Early stopping:  0.4014999036127426\n",
            "Epoch: 005, Loss: 1.4996, Train: 0.8844, Test: 0.7595\n",
            "Early stopping:  0.5148209682582936\n",
            "Epoch: 006, Loss: 1.1702, Train: 0.8906, Test: 0.7826\n",
            "Early stopping:  0.5434541823379269\n",
            "Epoch: 007, Loss: 0.8917, Train: 0.8969, Test: 0.7921\n",
            "Early stopping:  0.5286180013141202\n",
            "Epoch: 008, Loss: 0.6766, Train: 0.9031, Test: 0.7968\n",
            "Early stopping:  0.4721872481064351\n",
            "Epoch: 009, Loss: 0.5228, Train: 0.9125, Test: 0.7981\n",
            "Early stopping:  0.3908737690074459\n",
            "Epoch: 010, Loss: 0.4135, Train: 0.9250, Test: 0.8009\n",
            "Early stopping:  0.30239315906968045\n",
            "Epoch: 011, Loss: 0.3341, Train: 0.9406, Test: 0.8037\n",
            "Early stopping:  0.22204135457312826\n",
            "Epoch: 012, Loss: 0.2749, Train: 0.9563, Test: 0.8053\n",
            "Early stopping:  0.1596434353069982\n",
            "Epoch: 013, Loss: 0.2268, Train: 0.9594, Test: 0.8068\n",
            "Early stopping:  0.11712677374218362\n",
            "Epoch: 014, Loss: 0.1860, Train: 0.9688, Test: 0.8081\n",
            "Early stopping:  0.08969548868833088\n",
            "Epoch: 015, Loss: 0.1524, Train: 0.9719, Test: 0.8087\n",
            "Early stopping:  0.07191212840737625\n",
            "Epoch: 016, Loss: 0.1238, Train: 0.9844, Test: 0.8101\n",
            "Early stopping:  0.05983158855517728\n",
            "Epoch: 017, Loss: 0.0993, Train: 0.9875, Test: 0.8101\n",
            "Early stopping:  0.05038512211750817\n",
            "Epoch: 018, Loss: 0.0792, Train: 0.9938, Test: 0.8059\n",
            "Early stopping:  0.04237548317212568\n",
            "Epoch: 019, Loss: 0.0633, Train: 0.9969, Test: 0.8046\n",
            "Early stopping:  0.03545904165279059\n",
            "Epoch: 020, Loss: 0.0509, Train: 1.0000, Test: 0.8043\n",
            "Early stopping:  0.029001157086437288\n",
            "Epoch: 021, Loss: 0.0413, Train: 1.0000, Test: 0.8034\n",
            "Early stopping:  0.023058977029512318\n",
            "Epoch: 022, Loss: 0.0338, Train: 1.0000, Test: 0.8030\n",
            "Early stopping:  0.018043246719207155\n",
            "Epoch: 023, Loss: 0.0279, Train: 1.0000, Test: 0.8028\n",
            "Early stopping:  0.01405088885932006\n",
            "Epoch: 024, Loss: 0.0235, Train: 1.0000, Test: 0.8035\n",
            "Early stopping:  0.010910391302204931\n",
            "Epoch: 025, Loss: 0.0201, Train: 1.0000, Test: 0.8036\n",
            "Early stopping:  0.0084362763952082\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.81      0.78       382\n",
            "             ecology       0.81      0.86      0.83       869\n",
            "            economic       0.84      0.69      0.76      1389\n",
            "          geophysics       0.95      0.89      0.92      1182\n",
            "  gravitional_theory       0.76      0.94      0.84       110\n",
            "               hydro       0.75      0.82      0.78       335\n",
            "                math       0.89      0.77      0.83      1319\n",
            "              metals       0.64      0.90      0.75       181\n",
            "          networking       0.84      0.88      0.86       325\n",
            "        neuroscience       0.92      0.95      0.94       287\n",
            "        oceanography       0.87      0.83      0.85       970\n",
            "             politic       0.80      0.66      0.72       583\n",
            "           sociology       0.58      0.75      0.65       719\n",
            "software_engineering       0.86      0.86      0.86       504\n",
            "          statistics       0.69      0.82      0.75       627\n",
            "    theory_computing       0.68      0.81      0.74       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.79      0.83      0.80     10204\n",
            "        weighted avg       0.82      0.80      0.81     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7768, Train: 0.7250, Test: 0.5899\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5161, Train: 0.8250, Test: 0.7331\n",
            "Early stopping:  0.18434239177913045\n",
            "Epoch: 003, Loss: 2.1966, Train: 0.8187, Test: 0.7535\n",
            "Early stopping:  0.2905765963801281\n",
            "Epoch: 004, Loss: 1.8259, Train: 0.8344, Test: 0.7651\n",
            "Early stopping:  0.4107577026899702\n",
            "Epoch: 005, Loss: 1.4545, Train: 0.8531, Test: 0.7756\n",
            "Early stopping:  0.5286080668741031\n",
            "Epoch: 006, Loss: 1.1219, Train: 0.8594, Test: 0.7870\n",
            "Early stopping:  0.5583936234371681\n",
            "Epoch: 007, Loss: 0.8518, Train: 0.8844, Test: 0.7964\n",
            "Early stopping:  0.537627173491998\n",
            "Epoch: 008, Loss: 0.6506, Train: 0.9000, Test: 0.7969\n",
            "Early stopping:  0.4700844439203777\n",
            "Epoch: 009, Loss: 0.5088, Train: 0.9156, Test: 0.7974\n",
            "Early stopping:  0.37843196940141144\n",
            "Epoch: 010, Loss: 0.4089, Train: 0.9250, Test: 0.7974\n",
            "Early stopping:  0.28480524631057363\n",
            "Epoch: 011, Loss: 0.3347, Train: 0.9344, Test: 0.8023\n",
            "Early stopping:  0.20563110216631672\n",
            "Epoch: 012, Loss: 0.2765, Train: 0.9406, Test: 0.8043\n",
            "Early stopping:  0.14814113442395746\n",
            "Epoch: 013, Loss: 0.2302, Train: 0.9656, Test: 0.8056\n",
            "Early stopping:  0.11030132605477123\n",
            "Epoch: 014, Loss: 0.1911, Train: 0.9750, Test: 0.8060\n",
            "Early stopping:  0.0861384276496174\n",
            "Epoch: 015, Loss: 0.1568, Train: 0.9781, Test: 0.8027\n",
            "Early stopping:  0.07015054548109584\n",
            "Epoch: 016, Loss: 0.1281, Train: 0.9875, Test: 0.7999\n",
            "Early stopping:  0.05875648806505552\n",
            "Epoch: 017, Loss: 0.1048, Train: 0.9875, Test: 0.7999\n",
            "Early stopping:  0.04984748521330161\n",
            "Epoch: 018, Loss: 0.0854, Train: 0.9906, Test: 0.7992\n",
            "Early stopping:  0.041899719085334276\n",
            "Epoch: 019, Loss: 0.0696, Train: 0.9906, Test: 0.7997\n",
            "Early stopping:  0.03455619873004265\n",
            "Epoch: 020, Loss: 0.0572, Train: 0.9938, Test: 0.7990\n",
            "Early stopping:  0.02819618350104051\n",
            "Epoch: 021, Loss: 0.0474, Train: 0.9969, Test: 0.7989\n",
            "Early stopping:  0.022804656073906672\n",
            "Epoch: 022, Loss: 0.0396, Train: 0.9969, Test: 0.7982\n",
            "Early stopping:  0.018167631653067184\n",
            "Epoch: 023, Loss: 0.0332, Train: 0.9969, Test: 0.7971\n",
            "Early stopping:  0.014412084423355932\n",
            "Epoch: 024, Loss: 0.0281, Train: 1.0000, Test: 0.7961\n",
            "Early stopping:  0.01154435959421972\n",
            "Epoch: 025, Loss: 0.0241, Train: 1.0000, Test: 0.7955\n",
            "Early stopping:  0.009273534433568573\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.80      0.76       382\n",
            "             ecology       0.81      0.85      0.83       869\n",
            "            economic       0.80      0.70      0.74      1389\n",
            "          geophysics       0.93      0.87      0.90      1182\n",
            "  gravitional_theory       0.72      0.95      0.82       110\n",
            "               hydro       0.67      0.80      0.73       335\n",
            "                math       0.91      0.73      0.81      1319\n",
            "              metals       0.61      0.91      0.73       181\n",
            "          networking       0.79      0.91      0.84       325\n",
            "        neuroscience       0.91      0.97      0.94       287\n",
            "        oceanography       0.87      0.83      0.85       970\n",
            "             politic       0.74      0.78      0.76       583\n",
            "           sociology       0.70      0.62      0.66       719\n",
            "software_engineering       0.78      0.88      0.83       504\n",
            "          statistics       0.69      0.85      0.76       627\n",
            "    theory_computing       0.66      0.74      0.70       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.77      0.82      0.79     10204\n",
            "        weighted avg       0.80      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7880, Train: 0.7250, Test: 0.5979\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5394, Train: 0.8125, Test: 0.7066\n",
            "Early stopping:  0.17580698060609823\n",
            "Epoch: 003, Loss: 2.2269, Train: 0.8250, Test: 0.7209\n",
            "Early stopping:  0.28116753537679806\n",
            "Epoch: 004, Loss: 1.8652, Train: 0.8531, Test: 0.7460\n",
            "Early stopping:  0.3991049229246301\n",
            "Epoch: 005, Loss: 1.4991, Train: 0.8688, Test: 0.7704\n",
            "Early stopping:  0.515703913952876\n",
            "Epoch: 006, Loss: 1.1635, Train: 0.8906, Test: 0.7868\n",
            "Early stopping:  0.5503660856442567\n",
            "Epoch: 007, Loss: 0.8835, Train: 0.9094, Test: 0.7958\n",
            "Early stopping:  0.5364679533812879\n",
            "Epoch: 008, Loss: 0.6713, Train: 0.8969, Test: 0.7987\n",
            "Early stopping:  0.47738801629716265\n",
            "Epoch: 009, Loss: 0.5249, Train: 0.9031, Test: 0.7981\n",
            "Early stopping:  0.39046114042460073\n",
            "Epoch: 010, Loss: 0.4275, Train: 0.9094, Test: 0.8015\n",
            "Early stopping:  0.2951412946017045\n",
            "Epoch: 011, Loss: 0.3579, Train: 0.9219, Test: 0.8037\n",
            "Early stopping:  0.20966440891751548\n",
            "Epoch: 012, Loss: 0.3040, Train: 0.9344, Test: 0.8035\n",
            "Early stopping:  0.14543866620328724\n",
            "Epoch: 013, Loss: 0.2589, Train: 0.9469, Test: 0.8041\n",
            "Early stopping:  0.1049371848093881\n",
            "Epoch: 014, Loss: 0.2180, Train: 0.9625, Test: 0.8036\n",
            "Early stopping:  0.0823940512267444\n",
            "Epoch: 015, Loss: 0.1808, Train: 0.9688, Test: 0.8015\n",
            "Early stopping:  0.06978828293883056\n",
            "Epoch: 016, Loss: 0.1476, Train: 0.9812, Test: 0.8017\n",
            "Early stopping:  0.061910856452507664\n",
            "Epoch: 017, Loss: 0.1190, Train: 0.9906, Test: 0.7999\n",
            "Early stopping:  0.05548088732637804\n",
            "Epoch: 018, Loss: 0.0955, Train: 0.9938, Test: 0.7988\n",
            "Early stopping:  0.048693812322694076\n",
            "Epoch: 019, Loss: 0.0765, Train: 0.9969, Test: 0.7981\n",
            "Early stopping:  0.041479031093852446\n",
            "Epoch: 020, Loss: 0.0612, Train: 0.9969, Test: 0.7964\n",
            "Early stopping:  0.03431722095223158\n",
            "Epoch: 021, Loss: 0.0496, Train: 0.9969, Test: 0.7938\n",
            "Early stopping:  0.027627735025028053\n",
            "Epoch: 022, Loss: 0.0412, Train: 1.0000, Test: 0.7920\n",
            "Early stopping:  0.02168031436040035\n",
            "Epoch: 023, Loss: 0.0350, Train: 1.0000, Test: 0.7911\n",
            "Early stopping:  0.016525754959028593\n",
            "Epoch: 024, Loss: 0.0305, Train: 1.0000, Test: 0.7886\n",
            "Early stopping:  0.012230466047373758\n",
            "Epoch: 025, Loss: 0.0270, Train: 1.0000, Test: 0.7889\n",
            "Early stopping:  0.008970993460514115\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.86      0.78       382\n",
            "             ecology       0.79      0.84      0.81       869\n",
            "            economic       0.82      0.73      0.77      1389\n",
            "          geophysics       0.88      0.87      0.88      1182\n",
            "  gravitional_theory       0.71      0.95      0.81       110\n",
            "               hydro       0.71      0.82      0.76       335\n",
            "                math       0.91      0.68      0.78      1319\n",
            "              metals       0.55      0.93      0.69       181\n",
            "          networking       0.80      0.89      0.84       325\n",
            "        neuroscience       0.89      0.97      0.93       287\n",
            "        oceanography       0.84      0.83      0.83       970\n",
            "             politic       0.68      0.78      0.73       583\n",
            "           sociology       0.69      0.67      0.68       719\n",
            "software_engineering       0.85      0.86      0.85       504\n",
            "          statistics       0.71      0.78      0.74       627\n",
            "    theory_computing       0.70      0.68      0.69       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.82      0.79     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7875, Train: 0.6875, Test: 0.5195\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5390, Train: 0.8125, Test: 0.7106\n",
            "Early stopping:  0.17573212780314287\n",
            "Epoch: 003, Loss: 2.2344, Train: 0.8406, Test: 0.7524\n",
            "Early stopping:  0.27703388635647813\n",
            "Epoch: 004, Loss: 1.8793, Train: 0.8500, Test: 0.7682\n",
            "Early stopping:  0.3922768647556753\n",
            "Epoch: 005, Loss: 1.5190, Train: 0.8656, Test: 0.7840\n",
            "Early stopping:  0.5068409753648121\n",
            "Epoch: 006, Loss: 1.1882, Train: 0.8625, Test: 0.7934\n",
            "Early stopping:  0.540475737864619\n",
            "Epoch: 007, Loss: 0.9113, Train: 0.8688, Test: 0.7956\n",
            "Early stopping:  0.5283217910509709\n",
            "Epoch: 008, Loss: 0.6997, Train: 0.8781, Test: 0.7966\n",
            "Early stopping:  0.47147902286716853\n",
            "Epoch: 009, Loss: 0.5507, Train: 0.8938, Test: 0.7990\n",
            "Early stopping:  0.3876907213527796\n",
            "Epoch: 010, Loss: 0.4478, Train: 0.9031, Test: 0.7995\n",
            "Early stopping:  0.29629755254691464\n",
            "Epoch: 011, Loss: 0.3721, Train: 0.9062, Test: 0.7962\n",
            "Early stopping:  0.21466949122940707\n",
            "Epoch: 012, Loss: 0.3126, Train: 0.9219, Test: 0.7933\n",
            "Early stopping:  0.15322747564519232\n",
            "Epoch: 013, Loss: 0.2631, Train: 0.9344, Test: 0.7922\n",
            "Early stopping:  0.1135605835344863\n",
            "Epoch: 014, Loss: 0.2202, Train: 0.9500, Test: 0.7905\n",
            "Early stopping:  0.08978464187880289\n",
            "Epoch: 015, Loss: 0.1817, Train: 0.9531, Test: 0.7868\n",
            "Early stopping:  0.07512243066955969\n",
            "Epoch: 016, Loss: 0.1469, Train: 0.9719, Test: 0.7838\n",
            "Early stopping:  0.0654363078741155\n",
            "Epoch: 017, Loss: 0.1163, Train: 0.9875, Test: 0.7808\n",
            "Early stopping:  0.058136559148405646\n",
            "Epoch: 018, Loss: 0.0913, Train: 0.9906, Test: 0.7801\n",
            "Early stopping:  0.05125978263340892\n",
            "Epoch: 019, Loss: 0.0720, Train: 0.9938, Test: 0.7786\n",
            "Early stopping:  0.043744780897173464\n",
            "Epoch: 020, Loss: 0.0574, Train: 0.9938, Test: 0.7779\n",
            "Early stopping:  0.03566437954476968\n",
            "Epoch: 021, Loss: 0.0464, Train: 1.0000, Test: 0.7785\n",
            "Early stopping:  0.0278281776584895\n",
            "Epoch: 022, Loss: 0.0380, Train: 1.0000, Test: 0.7781\n",
            "Early stopping:  0.021172669547594173\n",
            "Epoch: 023, Loss: 0.0315, Train: 1.0000, Test: 0.7777\n",
            "Early stopping:  0.01604208419706528\n",
            "Epoch: 024, Loss: 0.0266, Train: 1.0000, Test: 0.7767\n",
            "Early stopping:  0.0122297061050214\n",
            "Epoch: 025, Loss: 0.0229, Train: 1.0000, Test: 0.7750\n",
            "Early stopping:  0.009363138615012322\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.86      0.79       382\n",
            "             ecology       0.79      0.76      0.78       869\n",
            "            economic       0.81      0.69      0.75      1389\n",
            "          geophysics       0.95      0.82      0.88      1182\n",
            "  gravitional_theory       0.43      0.97      0.60       110\n",
            "               hydro       0.70      0.84      0.77       335\n",
            "                math       0.86      0.73      0.79      1319\n",
            "              metals       0.44      0.94      0.60       181\n",
            "          networking       0.88      0.83      0.86       325\n",
            "        neuroscience       0.88      0.96      0.92       287\n",
            "        oceanography       0.80      0.89      0.84       970\n",
            "             politic       0.69      0.69      0.69       583\n",
            "           sociology       0.62      0.64      0.63       719\n",
            "software_engineering       0.91      0.79      0.84       504\n",
            "          statistics       0.81      0.75      0.78       627\n",
            "    theory_computing       0.64      0.78      0.71       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.81      0.76     10204\n",
            "        weighted avg       0.80      0.77      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7825, Train: 0.6719, Test: 0.6254\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5228, Train: 0.8656, Test: 0.7671\n",
            "Early stopping:  0.18362286078135245\n",
            "Epoch: 003, Loss: 2.2140, Train: 0.8812, Test: 0.7649\n",
            "Early stopping:  0.2846055465780644\n",
            "Epoch: 004, Loss: 1.8573, Train: 0.8906, Test: 0.7634\n",
            "Early stopping:  0.39918741349459924\n",
            "Epoch: 005, Loss: 1.4912, Train: 0.9000, Test: 0.7713\n",
            "Early stopping:  0.5147801388142248\n",
            "Epoch: 006, Loss: 1.1523, Train: 0.9125, Test: 0.7786\n",
            "Early stopping:  0.5478767116792682\n",
            "Epoch: 007, Loss: 0.8674, Train: 0.9156, Test: 0.7868\n",
            "Early stopping:  0.5378682907155271\n",
            "Epoch: 008, Loss: 0.6490, Train: 0.9281, Test: 0.7935\n",
            "Early stopping:  0.4830320454289513\n",
            "Epoch: 009, Loss: 0.4936, Train: 0.9313, Test: 0.7980\n",
            "Early stopping:  0.3992991281328898\n",
            "Epoch: 010, Loss: 0.3871, Train: 0.9469, Test: 0.8020\n",
            "Early stopping:  0.3062720358610407\n",
            "Epoch: 011, Loss: 0.3118, Train: 0.9469, Test: 0.8030\n",
            "Early stopping:  0.2217368581985091\n",
            "Epoch: 012, Loss: 0.2546, Train: 0.9594, Test: 0.8019\n",
            "Early stopping:  0.15650887570528038\n",
            "Epoch: 013, Loss: 0.2084, Train: 0.9719, Test: 0.7991\n",
            "Early stopping:  0.11270622830250321\n",
            "Epoch: 014, Loss: 0.1692, Train: 0.9750, Test: 0.7947\n",
            "Early stopping:  0.08598249258274678\n",
            "Epoch: 015, Loss: 0.1364, Train: 0.9875, Test: 0.7913\n",
            "Early stopping:  0.06935016939885012\n",
            "Epoch: 016, Loss: 0.1103, Train: 0.9938, Test: 0.7906\n",
            "Early stopping:  0.057368137780710704\n",
            "Epoch: 017, Loss: 0.0887, Train: 0.9969, Test: 0.7917\n",
            "Early stopping:  0.047501881847048254\n",
            "Epoch: 018, Loss: 0.0713, Train: 0.9969, Test: 0.7906\n",
            "Early stopping:  0.03881315104226806\n",
            "Epoch: 019, Loss: 0.0577, Train: 0.9969, Test: 0.7911\n",
            "Early stopping:  0.031312390177504784\n",
            "Epoch: 020, Loss: 0.0470, Train: 1.0000, Test: 0.7913\n",
            "Early stopping:  0.025157879422722972\n",
            "Epoch: 021, Loss: 0.0385, Train: 1.0000, Test: 0.7896\n",
            "Early stopping:  0.019936854848854997\n",
            "Epoch: 022, Loss: 0.0319, Train: 1.0000, Test: 0.7882\n",
            "Early stopping:  0.015645394483105143\n",
            "Epoch: 023, Loss: 0.0270, Train: 1.0000, Test: 0.7884\n",
            "Early stopping:  0.012228671253072225\n",
            "Epoch: 024, Loss: 0.0232, Train: 1.0000, Test: 0.7868\n",
            "Early stopping:  0.009456159922754804\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.87      0.78       382\n",
            "             ecology       0.79      0.82      0.80       869\n",
            "            economic       0.80      0.67      0.73      1389\n",
            "          geophysics       0.92      0.89      0.90      1182\n",
            "  gravitional_theory       0.82      0.95      0.88       110\n",
            "               hydro       0.69      0.77      0.73       335\n",
            "                math       0.90      0.77      0.83      1319\n",
            "              metals       0.56      0.96      0.71       181\n",
            "          networking       0.71      0.88      0.79       325\n",
            "        neuroscience       0.91      0.97      0.93       287\n",
            "        oceanography       0.84      0.80      0.82       970\n",
            "             politic       0.71      0.74      0.72       583\n",
            "           sociology       0.63      0.61      0.62       719\n",
            "software_engineering       0.88      0.86      0.87       504\n",
            "          statistics       0.75      0.80      0.78       627\n",
            "    theory_computing       0.61      0.72      0.66       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.76      0.82      0.78     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7735, Train: 0.7625, Test: 0.6097\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5315, Train: 0.8187, Test: 0.7071\n",
            "Early stopping:  0.17109513152997433\n",
            "Epoch: 003, Loss: 2.2332, Train: 0.8156, Test: 0.7336\n",
            "Early stopping:  0.27065153715266677\n",
            "Epoch: 004, Loss: 1.8847, Train: 0.8281, Test: 0.7517\n",
            "Early stopping:  0.38399804846939706\n",
            "Epoch: 005, Loss: 1.5303, Train: 0.8500, Test: 0.7689\n",
            "Early stopping:  0.49683986802167124\n",
            "Epoch: 006, Loss: 1.2040, Train: 0.8656, Test: 0.7802\n",
            "Early stopping:  0.5311388211093138\n",
            "Epoch: 007, Loss: 0.9279, Train: 0.8750, Test: 0.7862\n",
            "Early stopping:  0.5209757761983967\n",
            "Epoch: 008, Loss: 0.7131, Train: 0.8812, Test: 0.7896\n",
            "Early stopping:  0.4678300130591058\n",
            "Epoch: 009, Loss: 0.5578, Train: 0.8969, Test: 0.7897\n",
            "Early stopping:  0.3888940630362855\n",
            "Epoch: 010, Loss: 0.4486, Train: 0.9125, Test: 0.7871\n",
            "Early stopping:  0.3020156416038557\n",
            "Epoch: 011, Loss: 0.3678, Train: 0.9281, Test: 0.7872\n",
            "Early stopping:  0.2229901448537407\n",
            "Epoch: 012, Loss: 0.3042, Train: 0.9344, Test: 0.7879\n",
            "Early stopping:  0.16193501199474167\n",
            "Epoch: 013, Loss: 0.2514, Train: 0.9500, Test: 0.7899\n",
            "Early stopping:  0.12101787852245105\n",
            "Epoch: 014, Loss: 0.2060, Train: 0.9688, Test: 0.7884\n",
            "Early stopping:  0.0957301040685499\n",
            "Epoch: 015, Loss: 0.1673, Train: 0.9812, Test: 0.7875\n",
            "Early stopping:  0.07926930482184139\n",
            "Epoch: 016, Loss: 0.1353, Train: 0.9875, Test: 0.7865\n",
            "Early stopping:  0.06700936419207695\n",
            "Epoch: 017, Loss: 0.1090, Train: 0.9875, Test: 0.7835\n",
            "Early stopping:  0.05655494273819998\n",
            "Epoch: 018, Loss: 0.0877, Train: 0.9906, Test: 0.7816\n",
            "Early stopping:  0.04696478710546605\n",
            "Epoch: 019, Loss: 0.0709, Train: 0.9906, Test: 0.7807\n",
            "Early stopping:  0.038323407206064586\n",
            "Epoch: 020, Loss: 0.0575, Train: 0.9969, Test: 0.7794\n",
            "Early stopping:  0.030893721419963233\n",
            "Epoch: 021, Loss: 0.0465, Train: 0.9969, Test: 0.7809\n",
            "Early stopping:  0.0247506290140019\n",
            "Epoch: 022, Loss: 0.0377, Train: 1.0000, Test: 0.7816\n",
            "Early stopping:  0.019821627637325208\n",
            "Epoch: 023, Loss: 0.0311, Train: 1.0000, Test: 0.7824\n",
            "Early stopping:  0.015842260448697586\n",
            "Epoch: 024, Loss: 0.0261, Train: 1.0000, Test: 0.7827\n",
            "Early stopping:  0.012481026181014323\n",
            "Epoch: 025, Loss: 0.0223, Train: 1.0000, Test: 0.7840\n",
            "Early stopping:  0.009597346783945937\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.83      0.79      0.81       382\n",
            "             ecology       0.87      0.75      0.80       869\n",
            "            economic       0.79      0.68      0.73      1389\n",
            "          geophysics       0.92      0.88      0.90      1182\n",
            "  gravitional_theory       0.59      0.96      0.73       110\n",
            "               hydro       0.72      0.79      0.75       335\n",
            "                math       0.84      0.69      0.76      1319\n",
            "              metals       0.48      0.92      0.63       181\n",
            "          networking       0.72      0.90      0.80       325\n",
            "        neuroscience       0.90      0.97      0.94       287\n",
            "        oceanography       0.80      0.90      0.85       970\n",
            "             politic       0.71      0.81      0.75       583\n",
            "           sociology       0.60      0.70      0.65       719\n",
            "software_engineering       0.90      0.80      0.85       504\n",
            "          statistics       0.83      0.78      0.80       627\n",
            "    theory_computing       0.67      0.74      0.70       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.76      0.82      0.78     10204\n",
            "        weighted avg       0.80      0.78      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7766, Train: 0.7312, Test: 0.6789\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5373, Train: 0.8250, Test: 0.7389\n",
            "Early stopping:  0.16919633171086151\n",
            "Epoch: 003, Loss: 2.2399, Train: 0.8250, Test: 0.7433\n",
            "Early stopping:  0.26886326009157624\n",
            "Epoch: 004, Loss: 1.8934, Train: 0.8344, Test: 0.7524\n",
            "Early stopping:  0.3817351996709606\n",
            "Epoch: 005, Loss: 1.5404, Train: 0.8562, Test: 0.7699\n",
            "Early stopping:  0.4941943646214899\n",
            "Epoch: 006, Loss: 1.2102, Train: 0.8781, Test: 0.7877\n",
            "Early stopping:  0.5304884515060668\n",
            "Epoch: 007, Loss: 0.9260, Train: 0.8688, Test: 0.7957\n",
            "Early stopping:  0.5239545233090525\n",
            "Epoch: 008, Loss: 0.7064, Train: 0.8688, Test: 0.7940\n",
            "Early stopping:  0.47439485709533924\n",
            "Epoch: 009, Loss: 0.5521, Train: 0.8688, Test: 0.7910\n",
            "Early stopping:  0.39613776704269243\n",
            "Epoch: 010, Loss: 0.4455, Train: 0.8812, Test: 0.7916\n",
            "Early stopping:  0.3061433662266614\n",
            "Epoch: 011, Loss: 0.3673, Train: 0.9031, Test: 0.7903\n",
            "Early stopping:  0.2224318620752058\n",
            "Epoch: 012, Loss: 0.3074, Train: 0.9250, Test: 0.7920\n",
            "Early stopping:  0.15812918683003516\n",
            "Epoch: 013, Loss: 0.2601, Train: 0.9313, Test: 0.7924\n",
            "Early stopping:  0.11566240490605409\n",
            "Epoch: 014, Loss: 0.2191, Train: 0.9500, Test: 0.7923\n",
            "Early stopping:  0.08932970555262729\n",
            "Epoch: 015, Loss: 0.1820, Train: 0.9656, Test: 0.7938\n",
            "Early stopping:  0.07288526033030295\n",
            "Epoch: 016, Loss: 0.1497, Train: 0.9719, Test: 0.7951\n",
            "Early stopping:  0.062387441395440335\n",
            "Epoch: 017, Loss: 0.1222, Train: 0.9844, Test: 0.7937\n",
            "Early stopping:  0.05474567574247727\n",
            "Epoch: 018, Loss: 0.0985, Train: 0.9906, Test: 0.7910\n",
            "Early stopping:  0.047812900102841664\n",
            "Epoch: 019, Loss: 0.0790, Train: 0.9938, Test: 0.7895\n",
            "Early stopping:  0.04088207389732475\n",
            "Epoch: 020, Loss: 0.0641, Train: 0.9969, Test: 0.7896\n",
            "Early stopping:  0.0341373557856601\n",
            "Epoch: 021, Loss: 0.0528, Train: 0.9969, Test: 0.7893\n",
            "Early stopping:  0.027663064102066287\n",
            "Epoch: 022, Loss: 0.0438, Train: 0.9969, Test: 0.7902\n",
            "Early stopping:  0.021683995098329842\n",
            "Epoch: 023, Loss: 0.0366, Train: 1.0000, Test: 0.7905\n",
            "Early stopping:  0.016772034872445423\n",
            "Epoch: 024, Loss: 0.0311, Train: 1.0000, Test: 0.7897\n",
            "Early stopping:  0.01309819188682625\n",
            "Epoch: 025, Loss: 0.0269, Train: 1.0000, Test: 0.7899\n",
            "Early stopping:  0.010295678769748575\n",
            "Epoch: 026, Loss: 0.0236, Train: 1.0000, Test: 0.7897\n",
            "Early stopping:  0.008011005351403687\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.77      0.74       382\n",
            "             ecology       0.83      0.82      0.83       869\n",
            "            economic       0.83      0.65      0.73      1389\n",
            "          geophysics       0.95      0.89      0.92      1182\n",
            "  gravitional_theory       0.70      0.94      0.80       110\n",
            "               hydro       0.63      0.86      0.73       335\n",
            "                math       0.91      0.77      0.83      1319\n",
            "              metals       0.56      0.92      0.69       181\n",
            "          networking       0.76      0.92      0.83       325\n",
            "        neuroscience       0.96      0.93      0.95       287\n",
            "        oceanography       0.84      0.81      0.82       970\n",
            "             politic       0.67      0.77      0.71       583\n",
            "           sociology       0.61      0.70      0.65       719\n",
            "software_engineering       0.78      0.88      0.83       504\n",
            "          statistics       0.73      0.82      0.77       627\n",
            "    theory_computing       0.74      0.66      0.70       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.76      0.82      0.78     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7849, Train: 0.7875, Test: 0.7002\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5131, Train: 0.8187, Test: 0.7025\n",
            "Early stopping:  0.1922385195539494\n",
            "Epoch: 003, Loss: 2.1878, Train: 0.8313, Test: 0.6997\n",
            "Early stopping:  0.29894600690291506\n",
            "Epoch: 004, Loss: 1.8174, Train: 0.8438, Test: 0.7079\n",
            "Early stopping:  0.4176899380215196\n",
            "Epoch: 005, Loss: 1.4482, Train: 0.8562, Test: 0.7288\n",
            "Early stopping:  0.5337573956256088\n",
            "Epoch: 006, Loss: 1.1177, Train: 0.8844, Test: 0.7649\n",
            "Early stopping:  0.5583597232940748\n",
            "Epoch: 007, Loss: 0.8457, Train: 0.9031, Test: 0.7893\n",
            "Early stopping:  0.5360601234879923\n",
            "Epoch: 008, Loss: 0.6371, Train: 0.9156, Test: 0.7982\n",
            "Early stopping:  0.4712693454584037\n",
            "Epoch: 009, Loss: 0.4898, Train: 0.9281, Test: 0.8015\n",
            "Early stopping:  0.3833944076320743\n",
            "Epoch: 010, Loss: 0.3935, Train: 0.9219, Test: 0.8005\n",
            "Early stopping:  0.2905751163731981\n",
            "Epoch: 011, Loss: 0.3280, Train: 0.9313, Test: 0.8016\n",
            "Early stopping:  0.20722780313492437\n",
            "Epoch: 012, Loss: 0.2759, Train: 0.9469, Test: 0.8012\n",
            "Early stopping:  0.1430075410454354\n",
            "Epoch: 013, Loss: 0.2311, Train: 0.9437, Test: 0.7971\n",
            "Early stopping:  0.10166391358843133\n",
            "Epoch: 014, Loss: 0.1941, Train: 0.9469, Test: 0.7930\n",
            "Early stopping:  0.07886499731170261\n",
            "Epoch: 015, Loss: 0.1635, Train: 0.9531, Test: 0.7880\n",
            "Early stopping:  0.06531547622637777\n",
            "Epoch: 016, Loss: 0.1365, Train: 0.9594, Test: 0.7859\n",
            "Early stopping:  0.05505642949467676\n",
            "Epoch: 017, Loss: 0.1122, Train: 0.9781, Test: 0.7836\n",
            "Early stopping:  0.04687282517504791\n",
            "Epoch: 018, Loss: 0.0907, Train: 0.9875, Test: 0.7827\n",
            "Early stopping:  0.04088580848878808\n",
            "Epoch: 019, Loss: 0.0735, Train: 0.9906, Test: 0.7799\n",
            "Early stopping:  0.0358176854628114\n",
            "Epoch: 020, Loss: 0.0607, Train: 0.9969, Test: 0.7794\n",
            "Early stopping:  0.030302885883322892\n",
            "Epoch: 021, Loss: 0.0510, Train: 1.0000, Test: 0.7760\n",
            "Early stopping:  0.02437187714119727\n",
            "Epoch: 022, Loss: 0.0427, Train: 1.0000, Test: 0.7749\n",
            "Early stopping:  0.0189579756329819\n",
            "Epoch: 023, Loss: 0.0354, Train: 1.0000, Test: 0.7712\n",
            "Early stopping:  0.015006081285154139\n",
            "Epoch: 024, Loss: 0.0295, Train: 1.0000, Test: 0.7709\n",
            "Early stopping:  0.012411497898369548\n",
            "Epoch: 025, Loss: 0.0251, Train: 1.0000, Test: 0.7685\n",
            "Early stopping:  0.010344243513810183\n",
            "Epoch: 026, Loss: 0.0221, Train: 1.0000, Test: 0.7678\n",
            "Early stopping:  0.008247898968580965\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.74      0.71       382\n",
            "             ecology       0.83      0.82      0.83       869\n",
            "            economic       0.79      0.65      0.72      1389\n",
            "          geophysics       0.97      0.82      0.89      1182\n",
            "  gravitional_theory       0.50      0.95      0.65       110\n",
            "               hydro       0.68      0.80      0.74       335\n",
            "                math       0.84      0.70      0.76      1319\n",
            "              metals       0.58      0.91      0.71       181\n",
            "          networking       0.85      0.88      0.86       325\n",
            "        neuroscience       0.91      0.95      0.93       287\n",
            "        oceanography       0.81      0.88      0.84       970\n",
            "             politic       0.73      0.73      0.73       583\n",
            "           sociology       0.51      0.71      0.59       719\n",
            "software_engineering       0.89      0.76      0.82       504\n",
            "          statistics       0.79      0.69      0.74       627\n",
            "    theory_computing       0.62      0.77      0.68       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.76     10204\n",
            "        weighted avg       0.79      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7845, Train: 0.6500, Test: 0.6381\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5382, Train: 0.8187, Test: 0.7422\n",
            "Early stopping:  0.1741989940417106\n",
            "Epoch: 003, Loss: 2.2313, Train: 0.8469, Test: 0.7623\n",
            "Early stopping:  0.2771848206815576\n",
            "Epoch: 004, Loss: 1.8732, Train: 0.8469, Test: 0.7715\n",
            "Early stopping:  0.39389708920270156\n",
            "Epoch: 005, Loss: 1.5058, Train: 0.8719, Test: 0.7837\n",
            "Early stopping:  0.5110702573481672\n",
            "Epoch: 006, Loss: 1.1686, Train: 0.8656, Test: 0.7933\n",
            "Early stopping:  0.5480120221352364\n",
            "Epoch: 007, Loss: 0.8906, Train: 0.8750, Test: 0.7987\n",
            "Early stopping:  0.5360794756047559\n",
            "Epoch: 008, Loss: 0.6823, Train: 0.8875, Test: 0.8025\n",
            "Early stopping:  0.47658858631444534\n",
            "Epoch: 009, Loss: 0.5359, Train: 0.9031, Test: 0.8062\n",
            "Early stopping:  0.38831777715613713\n",
            "Epoch: 010, Loss: 0.4326, Train: 0.9062, Test: 0.8074\n",
            "Early stopping:  0.29403810449482315\n",
            "Epoch: 011, Loss: 0.3560, Train: 0.9125, Test: 0.8079\n",
            "Early stopping:  0.21258721539743802\n",
            "Epoch: 012, Loss: 0.2963, Train: 0.9281, Test: 0.8077\n",
            "Early stopping:  0.15293518294148395\n",
            "Epoch: 013, Loss: 0.2478, Train: 0.9375, Test: 0.8064\n",
            "Early stopping:  0.11394251533806331\n",
            "Epoch: 014, Loss: 0.2067, Train: 0.9531, Test: 0.8066\n",
            "Early stopping:  0.08924139709929986\n",
            "Epoch: 015, Loss: 0.1710, Train: 0.9656, Test: 0.8085\n",
            "Early stopping:  0.07306172775946752\n",
            "Epoch: 016, Loss: 0.1406, Train: 0.9719, Test: 0.8089\n",
            "Early stopping:  0.06166580351419351\n",
            "Epoch: 017, Loss: 0.1155, Train: 0.9875, Test: 0.8066\n",
            "Early stopping:  0.05253926878966341\n",
            "Epoch: 018, Loss: 0.0946, Train: 0.9875, Test: 0.8045\n",
            "Early stopping:  0.0444422858018957\n",
            "Epoch: 019, Loss: 0.0772, Train: 0.9906, Test: 0.8020\n",
            "Early stopping:  0.037139562400926925\n",
            "Epoch: 020, Loss: 0.0627, Train: 1.0000, Test: 0.8005\n",
            "Early stopping:  0.030865574623748355\n",
            "Epoch: 021, Loss: 0.0510, Train: 1.0000, Test: 0.7960\n",
            "Early stopping:  0.025609789689574145\n",
            "Epoch: 022, Loss: 0.0419, Train: 1.0000, Test: 0.7933\n",
            "Early stopping:  0.020990847655653743\n",
            "Epoch: 023, Loss: 0.0349, Train: 1.0000, Test: 0.7915\n",
            "Early stopping:  0.016832658325410903\n",
            "Epoch: 024, Loss: 0.0295, Train: 1.0000, Test: 0.7892\n",
            "Early stopping:  0.013184545915740163\n",
            "Epoch: 025, Loss: 0.0254, Train: 1.0000, Test: 0.7866\n",
            "Early stopping:  0.010161521585025663\n",
            "Epoch: 026, Loss: 0.0222, Train: 1.0000, Test: 0.7859\n",
            "Early stopping:  0.00780804269066844\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.88      0.78       382\n",
            "             ecology       0.77      0.89      0.83       869\n",
            "            economic       0.79      0.73      0.76      1389\n",
            "          geophysics       0.93      0.86      0.89      1182\n",
            "  gravitional_theory       0.42      0.97      0.59       110\n",
            "               hydro       0.69      0.84      0.75       335\n",
            "                math       0.90      0.67      0.77      1319\n",
            "              metals       0.51      0.96      0.67       181\n",
            "          networking       0.81      0.88      0.84       325\n",
            "        neuroscience       0.94      0.94      0.94       287\n",
            "        oceanography       0.88      0.79      0.83       970\n",
            "             politic       0.77      0.73      0.75       583\n",
            "           sociology       0.66      0.64      0.65       719\n",
            "software_engineering       0.86      0.83      0.85       504\n",
            "          statistics       0.71      0.78      0.74       627\n",
            "    theory_computing       0.74      0.74      0.74       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.76      0.82      0.77     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7688, Train: 0.7625, Test: 0.6310\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5046, Train: 0.8469, Test: 0.7170\n",
            "Early stopping:  0.1868777775981518\n",
            "Epoch: 003, Loss: 2.1827, Train: 0.8562, Test: 0.7229\n",
            "Early stopping:  0.29354827911935266\n",
            "Epoch: 004, Loss: 1.8154, Train: 0.8594, Test: 0.7328\n",
            "Early stopping:  0.4119111018668445\n",
            "Epoch: 005, Loss: 1.4511, Train: 0.8719, Test: 0.7507\n",
            "Early stopping:  0.5268060734460399\n",
            "Epoch: 006, Loss: 1.1246, Train: 0.8875, Test: 0.7704\n",
            "Early stopping:  0.5522335773006525\n",
            "Epoch: 007, Loss: 0.8574, Train: 0.8969, Test: 0.7873\n",
            "Early stopping:  0.5293664295416675\n",
            "Epoch: 008, Loss: 0.6564, Train: 0.9000, Test: 0.7993\n",
            "Early stopping:  0.46328554262619936\n",
            "Epoch: 009, Loss: 0.5154, Train: 0.9156, Test: 0.8038\n",
            "Early stopping:  0.3745079399042302\n",
            "Epoch: 010, Loss: 0.4174, Train: 0.9281, Test: 0.8041\n",
            "Early stopping:  0.2827875377211449\n",
            "Epoch: 011, Loss: 0.3446, Train: 0.9437, Test: 0.8033\n",
            "Early stopping:  0.20397358902136686\n",
            "Epoch: 012, Loss: 0.2875, Train: 0.9469, Test: 0.8011\n",
            "Early stopping:  0.14600457833289937\n",
            "Epoch: 013, Loss: 0.2419, Train: 0.9563, Test: 0.8018\n",
            "Early stopping:  0.1082250515804839\n",
            "Epoch: 014, Loss: 0.2031, Train: 0.9625, Test: 0.8024\n",
            "Early stopping:  0.08467231416348345\n",
            "Epoch: 015, Loss: 0.1680, Train: 0.9688, Test: 0.8028\n",
            "Early stopping:  0.0695457045791028\n",
            "Epoch: 016, Loss: 0.1366, Train: 0.9812, Test: 0.8036\n",
            "Early stopping:  0.05957703912216913\n",
            "Epoch: 017, Loss: 0.1096, Train: 0.9906, Test: 0.8019\n",
            "Early stopping:  0.05250428613403476\n",
            "Epoch: 018, Loss: 0.0867, Train: 1.0000, Test: 0.8006\n",
            "Early stopping:  0.04619456100718564\n",
            "Epoch: 019, Loss: 0.0686, Train: 1.0000, Test: 0.7965\n",
            "Early stopping:  0.03952776305972517\n",
            "Epoch: 020, Loss: 0.0550, Train: 1.0000, Test: 0.7939\n",
            "Early stopping:  0.03256678023492727\n",
            "Epoch: 021, Loss: 0.0448, Train: 1.0000, Test: 0.7914\n",
            "Early stopping:  0.025804707219630015\n",
            "Epoch: 022, Loss: 0.0372, Train: 1.0000, Test: 0.7898\n",
            "Early stopping:  0.019694580590098563\n",
            "Epoch: 023, Loss: 0.0313, Train: 1.0000, Test: 0.7882\n",
            "Early stopping:  0.014787585399074031\n",
            "Epoch: 024, Loss: 0.0268, Train: 1.0000, Test: 0.7860\n",
            "Early stopping:  0.011187935707181136\n",
            "Epoch: 025, Loss: 0.0232, Train: 1.0000, Test: 0.7853\n",
            "Early stopping:  0.00857244366262971\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.79      0.77       382\n",
            "             ecology       0.86      0.75      0.80       869\n",
            "            economic       0.84      0.63      0.72      1389\n",
            "          geophysics       0.91      0.92      0.91      1182\n",
            "  gravitional_theory       0.79      0.93      0.85       110\n",
            "               hydro       0.73      0.78      0.75       335\n",
            "                math       0.89      0.79      0.84      1319\n",
            "              metals       0.63      0.93      0.75       181\n",
            "          networking       0.82      0.87      0.85       325\n",
            "        neuroscience       0.88      0.98      0.93       287\n",
            "        oceanography       0.80      0.86      0.83       970\n",
            "             politic       0.57      0.85      0.68       583\n",
            "           sociology       0.61      0.58      0.59       719\n",
            "software_engineering       0.85      0.75      0.79       504\n",
            "          statistics       0.75      0.84      0.79       627\n",
            "    theory_computing       0.66      0.74      0.70       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.81      0.78     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "time: 11.5 s (started: 2024-08-16 14:12:08 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "r5mO1xZouwrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7be62b-2f80-4341-fd1e-0e520d26836e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 455 ms (started: 2024-08-16 14:12:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 30 ❎"
      ],
      "metadata": {
        "id": "-iAL8bp_tA0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "zrd2CZSFu57F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji2mi3z0u57M",
        "outputId": "88b9be2c-47bc-4fe0-fe59-02cdec3aa01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.2296, Train: 0.5104, Test: 0.4516\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.6893, Train: 0.6583, Test: 0.6624\n",
            "Early stopping:  1.796272889663015\n",
            "Epoch: 003, Loss: 2.1617, Train: 0.7875, Test: 0.7386\n",
            "Early stopping:  1.6403131160804936\n",
            "Epoch: 004, Loss: 1.1101, Train: 0.8583, Test: 0.7609\n",
            "Early stopping:  1.74913772023399\n",
            "Epoch: 005, Loss: 0.6282, Train: 0.8521, Test: 0.7371\n",
            "Early stopping:  1.7988697519550108\n",
            "Epoch: 006, Loss: 0.8400, Train: 0.8667, Test: 0.7545\n",
            "Early stopping:  0.8942760773448944\n",
            "Epoch: 007, Loss: 0.6156, Train: 0.8854, Test: 0.7810\n",
            "Early stopping:  0.6418596336448062\n",
            "Epoch: 008, Loss: 0.4209, Train: 0.8750, Test: 0.7803\n",
            "Early stopping:  0.2623426299756426\n",
            "Epoch: 009, Loss: 0.4496, Train: 0.9021, Test: 0.7933\n",
            "Early stopping:  0.16800215866113555\n",
            "Epoch: 010, Loss: 0.4099, Train: 0.9146, Test: 0.8036\n",
            "Early stopping:  0.18351469423803254\n",
            "Epoch: 011, Loss: 0.3413, Train: 0.9250, Test: 0.8062\n",
            "Early stopping:  0.1020558375683968\n",
            "Epoch: 012, Loss: 0.2647, Train: 0.9437, Test: 0.8013\n",
            "Early stopping:  0.07444566119712631\n",
            "Epoch: 013, Loss: 0.1829, Train: 0.9479, Test: 0.7809\n",
            "Early stopping:  0.10806167200186176\n",
            "Epoch: 014, Loss: 0.2083, Train: 0.9500, Test: 0.7733\n",
            "Early stopping:  0.09409924617806834\n",
            "Epoch: 015, Loss: 0.2151, Train: 0.9646, Test: 0.7813\n",
            "Early stopping:  0.06267558034545864\n",
            "Epoch: 016, Loss: 0.1526, Train: 0.9771, Test: 0.7903\n",
            "Early stopping:  0.04156495285520091\n",
            "Epoch: 017, Loss: 0.1117, Train: 0.9708, Test: 0.7914\n",
            "Early stopping:  0.04268688973950624\n",
            "Epoch: 018, Loss: 0.1168, Train: 0.9792, Test: 0.7912\n",
            "Early stopping:  0.04906102200749938\n",
            "Epoch: 019, Loss: 0.1084, Train: 0.9729, Test: 0.7899\n",
            "Early stopping:  0.04510856774611038\n",
            "Epoch: 020, Loss: 0.0924, Train: 0.9854, Test: 0.7884\n",
            "Early stopping:  0.02220788636761787\n",
            "Epoch: 021, Loss: 0.0712, Train: 0.9896, Test: 0.7834\n",
            "Early stopping:  0.018560839839714773\n",
            "Epoch: 022, Loss: 0.0705, Train: 0.9896, Test: 0.7777\n",
            "Early stopping:  0.021088658249143907\n",
            "Epoch: 023, Loss: 0.0738, Train: 0.9917, Test: 0.7760\n",
            "Early stopping:  0.016696748586774963\n",
            "Epoch: 024, Loss: 0.0651, Train: 0.9958, Test: 0.7787\n",
            "Early stopping:  0.010441449528688782\n",
            "Epoch: 025, Loss: 0.0499, Train: 0.9979, Test: 0.7849\n",
            "Early stopping:  0.009565673576909241\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.77      0.74       372\n",
            "             ecology       0.81      0.76      0.78       859\n",
            "            economic       0.81      0.66      0.72      1379\n",
            "          geophysics       0.92      0.90      0.91      1172\n",
            "  gravitional_theory       0.76      0.93      0.84       100\n",
            "               hydro       0.68      0.82      0.74       325\n",
            "                math       0.93      0.77      0.84      1309\n",
            "              metals       0.58      0.90      0.71       171\n",
            "          networking       0.74      0.93      0.82       315\n",
            "        neuroscience       0.93      0.92      0.92       277\n",
            "        oceanography       0.80      0.80      0.80       960\n",
            "             politic       0.69      0.74      0.71       573\n",
            "           sociology       0.63      0.70      0.66       709\n",
            "software_engineering       0.78      0.85      0.82       494\n",
            "          statistics       0.75      0.81      0.78       617\n",
            "    theory_computing       0.68      0.78      0.73       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.76      0.81      0.78     10044\n",
            "        weighted avg       0.80      0.78      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.1529, Train: 0.5896, Test: 0.5085\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.1352, Train: 0.7875, Test: 0.7260\n",
            "Early stopping:  1.4267154977531846\n",
            "Epoch: 003, Loss: 0.9059, Train: 0.8208, Test: 0.7360\n",
            "Early stopping:  1.6393745069201913\n",
            "Epoch: 004, Loss: 0.8199, Train: 0.8167, Test: 0.7289\n",
            "Early stopping:  1.553819768549377\n",
            "Epoch: 005, Loss: 0.7282, Train: 0.8542, Test: 0.7454\n",
            "Early stopping:  1.4615228811274843\n",
            "Epoch: 006, Loss: 0.5684, Train: 0.8896, Test: 0.7741\n",
            "Early stopping:  0.6295385822732651\n",
            "Epoch: 007, Loss: 0.4407, Train: 0.8958, Test: 0.7718\n",
            "Early stopping:  0.18832531626823362\n",
            "Epoch: 008, Loss: 0.4002, Train: 0.8938, Test: 0.7729\n",
            "Early stopping:  0.18078129780956834\n",
            "Epoch: 009, Loss: 0.3445, Train: 0.9042, Test: 0.7699\n",
            "Early stopping:  0.15359251537660612\n",
            "Epoch: 010, Loss: 0.3157, Train: 0.9146, Test: 0.7768\n",
            "Early stopping:  0.0990542443234073\n",
            "Epoch: 011, Loss: 0.2631, Train: 0.9208, Test: 0.7732\n",
            "Early stopping:  0.06975056397961944\n",
            "Epoch: 012, Loss: 0.2336, Train: 0.9292, Test: 0.7704\n",
            "Early stopping:  0.06591566841212584\n",
            "Epoch: 013, Loss: 0.1857, Train: 0.9437, Test: 0.7704\n",
            "Early stopping:  0.06343923843440485\n",
            "Epoch: 014, Loss: 0.1374, Train: 0.9563, Test: 0.7590\n",
            "Early stopping:  0.0688207639040652\n",
            "Epoch: 015, Loss: 0.1284, Train: 0.9688, Test: 0.7595\n",
            "Early stopping:  0.058783257611988216\n",
            "Epoch: 016, Loss: 0.1162, Train: 0.9750, Test: 0.7706\n",
            "Early stopping:  0.04873515378536755\n",
            "Epoch: 017, Loss: 0.0954, Train: 0.9750, Test: 0.7744\n",
            "Early stopping:  0.033584173949359954\n",
            "Epoch: 018, Loss: 0.0813, Train: 0.9771, Test: 0.7731\n",
            "Early stopping:  0.023171078966325084\n",
            "Epoch: 019, Loss: 0.0746, Train: 0.9833, Test: 0.7725\n",
            "Early stopping:  0.02281353368322974\n",
            "Epoch: 020, Loss: 0.0668, Train: 0.9792, Test: 0.7686\n",
            "Early stopping:  0.01950278659938275\n",
            "Epoch: 021, Loss: 0.0606, Train: 0.9875, Test: 0.7630\n",
            "Early stopping:  0.013495939246368953\n",
            "Epoch: 022, Loss: 0.0534, Train: 0.9979, Test: 0.7623\n",
            "Early stopping:  0.011056723173731608\n",
            "Epoch: 023, Loss: 0.0418, Train: 1.0000, Test: 0.7630\n",
            "Early stopping:  0.01256491002020074\n",
            "Epoch: 024, Loss: 0.0334, Train: 0.9979, Test: 0.7636\n",
            "Early stopping:  0.013600608755701175\n",
            "Epoch: 025, Loss: 0.0299, Train: 0.9979, Test: 0.7642\n",
            "Early stopping:  0.013036750851160964\n",
            "Epoch: 026, Loss: 0.0261, Train: 0.9979, Test: 0.7633\n",
            "Early stopping:  0.010892763161717578\n",
            "Epoch: 027, Loss: 0.0216, Train: 1.0000, Test: 0.7636\n",
            "Early stopping:  0.007676641458120235\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.74      0.71       372\n",
            "             ecology       0.77      0.71      0.74       859\n",
            "            economic       0.81      0.64      0.72      1379\n",
            "          geophysics       0.94      0.86      0.90      1172\n",
            "  gravitional_theory       0.84      0.92      0.88       100\n",
            "               hydro       0.63      0.86      0.73       325\n",
            "                math       0.88      0.80      0.84      1309\n",
            "              metals       0.55      0.95      0.69       171\n",
            "          networking       0.81      0.86      0.83       315\n",
            "        neuroscience       0.84      0.98      0.91       277\n",
            "        oceanography       0.78      0.80      0.79       960\n",
            "             politic       0.54      0.60      0.57       573\n",
            "           sociology       0.59      0.61      0.60       709\n",
            "software_engineering       0.79      0.81      0.80       494\n",
            "          statistics       0.74      0.83      0.78       617\n",
            "    theory_computing       0.73      0.75      0.74       412\n",
            "\n",
            "            accuracy                           0.76     10044\n",
            "           macro avg       0.74      0.80      0.76     10044\n",
            "        weighted avg       0.77      0.76      0.77     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.7798, Train: 0.6375, Test: 0.5257\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.3909, Train: 0.6958, Test: 0.6768\n",
            "Early stopping:  2.3962846130770776\n",
            "Epoch: 003, Loss: 2.3519, Train: 0.7833, Test: 0.7451\n",
            "Early stopping:  1.7465342227902159\n",
            "Epoch: 004, Loss: 1.2642, Train: 0.8146, Test: 0.6800\n",
            "Early stopping:  1.6294367937010892\n",
            "Epoch: 005, Loss: 0.7843, Train: 0.8375, Test: 0.6947\n",
            "Early stopping:  1.5950022751047983\n",
            "Epoch: 006, Loss: 1.0930, Train: 0.8625, Test: 0.7275\n",
            "Early stopping:  0.5906332389886417\n",
            "Epoch: 007, Loss: 0.9003, Train: 0.8771, Test: 0.7636\n",
            "Early stopping:  0.6273279335162043\n",
            "Epoch: 008, Loss: 0.4728, Train: 0.8792, Test: 0.7739\n",
            "Early stopping:  0.30237748339811094\n",
            "Epoch: 009, Loss: 0.5244, Train: 0.8917, Test: 0.7806\n",
            "Early stopping:  0.2593397925790749\n",
            "Epoch: 010, Loss: 0.5977, Train: 0.9042, Test: 0.7813\n",
            "Early stopping:  0.26736675284854383\n",
            "Epoch: 011, Loss: 0.4144, Train: 0.9292, Test: 0.7819\n",
            "Early stopping:  0.19033470245483664\n",
            "Epoch: 012, Loss: 0.2493, Train: 0.9437, Test: 0.7709\n",
            "Early stopping:  0.13174649043129338\n",
            "Epoch: 013, Loss: 0.1995, Train: 0.9354, Test: 0.7520\n",
            "Early stopping:  0.17150934488407338\n",
            "Epoch: 014, Loss: 0.2412, Train: 0.9437, Test: 0.7614\n",
            "Early stopping:  0.1655987857193103\n",
            "Epoch: 015, Loss: 0.2052, Train: 0.9688, Test: 0.7778\n",
            "Early stopping:  0.08795359022070237\n",
            "Epoch: 016, Loss: 0.1086, Train: 0.9771, Test: 0.7818\n",
            "Early stopping:  0.055891354336067296\n",
            "Epoch: 017, Loss: 0.1031, Train: 0.9688, Test: 0.7770\n",
            "Early stopping:  0.062053007004301594\n",
            "Epoch: 018, Loss: 0.1132, Train: 0.9750, Test: 0.7782\n",
            "Early stopping:  0.064298525277194\n",
            "Epoch: 019, Loss: 0.1007, Train: 0.9854, Test: 0.7731\n",
            "Early stopping:  0.0444614181635332\n",
            "Epoch: 020, Loss: 0.0975, Train: 0.9854, Test: 0.7671\n",
            "Early stopping:  0.006254267926955312\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.81      0.75       372\n",
            "             ecology       0.86      0.79      0.83       859\n",
            "            economic       0.85      0.61      0.71      1379\n",
            "          geophysics       0.98      0.80      0.88      1172\n",
            "  gravitional_theory       0.28      0.96      0.43       100\n",
            "               hydro       0.63      0.86      0.72       325\n",
            "                math       0.85      0.76      0.80      1309\n",
            "              metals       0.78      0.85      0.81       171\n",
            "          networking       0.72      0.95      0.82       315\n",
            "        neuroscience       0.95      0.97      0.96       277\n",
            "        oceanography       0.83      0.87      0.85       960\n",
            "             politic       0.72      0.68      0.70       573\n",
            "           sociology       0.53      0.77      0.63       709\n",
            "software_engineering       0.80      0.75      0.77       494\n",
            "          statistics       0.75      0.75      0.75       617\n",
            "    theory_computing       0.71      0.66      0.69       412\n",
            "\n",
            "            accuracy                           0.77     10044\n",
            "           macro avg       0.75      0.80      0.76     10044\n",
            "        weighted avg       0.80      0.77      0.77     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.6180, Train: 0.5958, Test: 0.6035\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.7283, Train: 0.7562, Test: 0.7341\n",
            "Early stopping:  2.04333080355072\n",
            "Epoch: 003, Loss: 1.0571, Train: 0.8021, Test: 0.6820\n",
            "Early stopping:  1.892150794317443\n",
            "Epoch: 004, Loss: 1.0695, Train: 0.8208, Test: 0.7120\n",
            "Early stopping:  1.6957790046967338\n",
            "Epoch: 005, Loss: 0.6315, Train: 0.8229, Test: 0.7428\n",
            "Early stopping:  1.6120890084005344\n",
            "Epoch: 006, Loss: 0.7405, Train: 0.8521, Test: 0.7583\n",
            "Early stopping:  0.4276088239949238\n",
            "Epoch: 007, Loss: 0.5202, Train: 0.8771, Test: 0.7603\n",
            "Early stopping:  0.24942408568469365\n",
            "Epoch: 008, Loss: 0.4065, Train: 0.8896, Test: 0.7497\n",
            "Early stopping:  0.2538773545055272\n",
            "Epoch: 009, Loss: 0.3591, Train: 0.9104, Test: 0.7419\n",
            "Early stopping:  0.1574528028698287\n",
            "Epoch: 010, Loss: 0.3431, Train: 0.9313, Test: 0.7493\n",
            "Early stopping:  0.16436941319010023\n",
            "Epoch: 011, Loss: 0.2791, Train: 0.9437, Test: 0.7651\n",
            "Early stopping:  0.08989938901393288\n",
            "Epoch: 012, Loss: 0.2252, Train: 0.9542, Test: 0.7694\n",
            "Early stopping:  0.07101200953983247\n",
            "Epoch: 013, Loss: 0.1826, Train: 0.9646, Test: 0.7766\n",
            "Early stopping:  0.07532186006144082\n",
            "Epoch: 014, Loss: 0.1425, Train: 0.9563, Test: 0.7694\n",
            "Early stopping:  0.07911218634641513\n",
            "Epoch: 015, Loss: 0.1252, Train: 0.9583, Test: 0.7571\n",
            "Early stopping:  0.06264359890596666\n",
            "Epoch: 016, Loss: 0.1123, Train: 0.9729, Test: 0.7574\n",
            "Early stopping:  0.04616453812623934\n",
            "Epoch: 017, Loss: 0.0851, Train: 0.9875, Test: 0.7576\n",
            "Early stopping:  0.03632583684796522\n",
            "Epoch: 018, Loss: 0.0620, Train: 0.9833, Test: 0.7518\n",
            "Early stopping:  0.03206005412018384\n",
            "Epoch: 019, Loss: 0.0623, Train: 0.9875, Test: 0.7489\n",
            "Early stopping:  0.028781998957746007\n",
            "Epoch: 020, Loss: 0.0509, Train: 0.9958, Test: 0.7467\n",
            "Early stopping:  0.024517052921631755\n",
            "Epoch: 021, Loss: 0.0368, Train: 0.9917, Test: 0.7429\n",
            "Early stopping:  0.017725225974509567\n",
            "Epoch: 022, Loss: 0.0380, Train: 0.9958, Test: 0.7436\n",
            "Early stopping:  0.01237547988644227\n",
            "Epoch: 023, Loss: 0.0323, Train: 0.9979, Test: 0.7486\n",
            "Early stopping:  0.012299073734544006\n",
            "Epoch: 024, Loss: 0.0242, Train: 0.9979, Test: 0.7533\n",
            "Early stopping:  0.009748144299344964\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.79      0.75       372\n",
            "             ecology       0.87      0.70      0.78       859\n",
            "            economic       0.83      0.62      0.71      1379\n",
            "          geophysics       0.90      0.88      0.89      1172\n",
            "  gravitional_theory       0.41      0.96      0.57       100\n",
            "               hydro       0.64      0.79      0.71       325\n",
            "                math       0.93      0.60      0.73      1309\n",
            "              metals       0.57      0.94      0.70       171\n",
            "          networking       0.83      0.82      0.83       315\n",
            "        neuroscience       0.86      0.96      0.90       277\n",
            "        oceanography       0.78      0.86      0.82       960\n",
            "             politic       0.67      0.74      0.70       573\n",
            "           sociology       0.59      0.67      0.63       709\n",
            "software_engineering       0.86      0.84      0.85       494\n",
            "          statistics       0.61      0.83      0.71       617\n",
            "    theory_computing       0.57      0.73      0.64       412\n",
            "\n",
            "            accuracy                           0.75     10044\n",
            "           macro avg       0.73      0.80      0.74     10044\n",
            "        weighted avg       0.78      0.75      0.76     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.1299, Train: 0.6104, Test: 0.5230\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.5651, Train: 0.7750, Test: 0.6521\n",
            "Early stopping:  1.8135558792444832\n",
            "Epoch: 003, Loss: 1.1765, Train: 0.8229, Test: 0.7131\n",
            "Early stopping:  1.6047665011268075\n",
            "Epoch: 004, Loss: 0.9398, Train: 0.8604, Test: 0.7652\n",
            "Early stopping:  1.474099981659502\n",
            "Epoch: 005, Loss: 0.4669, Train: 0.8708, Test: 0.7618\n",
            "Early stopping:  1.4392191428596044\n",
            "Epoch: 006, Loss: 0.5932, Train: 0.8792, Test: 0.7642\n",
            "Early stopping:  0.4445595769853612\n",
            "Epoch: 007, Loss: 0.5275, Train: 0.9062, Test: 0.7761\n",
            "Early stopping:  0.3048510528263915\n",
            "Epoch: 008, Loss: 0.3312, Train: 0.9271, Test: 0.7869\n",
            "Early stopping:  0.22734435831822733\n",
            "Epoch: 009, Loss: 0.2404, Train: 0.9313, Test: 0.7748\n",
            "Early stopping:  0.1442195337070542\n",
            "Epoch: 010, Loss: 0.2431, Train: 0.9375, Test: 0.7628\n",
            "Early stopping:  0.16400055314309447\n",
            "Epoch: 011, Loss: 0.2479, Train: 0.9604, Test: 0.7639\n",
            "Early stopping:  0.12310468997369857\n",
            "Epoch: 012, Loss: 0.2062, Train: 0.9688, Test: 0.7745\n",
            "Early stopping:  0.046308371549081066\n",
            "Epoch: 013, Loss: 0.1282, Train: 0.9729, Test: 0.7862\n",
            "Early stopping:  0.05029692691000903\n",
            "Epoch: 014, Loss: 0.0898, Train: 0.9750, Test: 0.7814\n",
            "Early stopping:  0.07079937422911134\n",
            "Epoch: 015, Loss: 0.0886, Train: 0.9771, Test: 0.7734\n",
            "Early stopping:  0.07175122885151787\n",
            "Epoch: 016, Loss: 0.0985, Train: 0.9812, Test: 0.7723\n",
            "Early stopping:  0.04958273621144318\n",
            "Epoch: 017, Loss: 0.0921, Train: 0.9896, Test: 0.7711\n",
            "Early stopping:  0.01649808074285004\n",
            "Epoch: 018, Loss: 0.0660, Train: 0.9917, Test: 0.7742\n",
            "Early stopping:  0.012336947371825573\n",
            "Epoch: 019, Loss: 0.0384, Train: 0.9938, Test: 0.7728\n",
            "Early stopping:  0.024656222958280547\n",
            "Epoch: 020, Loss: 0.0245, Train: 0.9938, Test: 0.7709\n",
            "Early stopping:  0.03238770037676667\n",
            "Epoch: 021, Loss: 0.0247, Train: 0.9896, Test: 0.7621\n",
            "Early stopping:  0.029369899861795563\n",
            "Epoch: 022, Loss: 0.0339, Train: 0.9896, Test: 0.7638\n",
            "Early stopping:  0.017027721354529634\n",
            "Epoch: 023, Loss: 0.0334, Train: 0.9979, Test: 0.7722\n",
            "Early stopping:  0.006149641147107844\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.73      0.72       372\n",
            "             ecology       0.77      0.77      0.77       859\n",
            "            economic       0.79      0.64      0.71      1379\n",
            "          geophysics       0.96      0.89      0.92      1172\n",
            "  gravitional_theory       0.79      0.96      0.87       100\n",
            "               hydro       0.66      0.82      0.73       325\n",
            "                math       0.88      0.75      0.81      1309\n",
            "              metals       0.64      0.91      0.75       171\n",
            "          networking       0.74      0.96      0.84       315\n",
            "        neuroscience       0.94      0.96      0.95       277\n",
            "        oceanography       0.86      0.79      0.82       960\n",
            "             politic       0.67      0.81      0.74       573\n",
            "           sociology       0.57      0.58      0.58       709\n",
            "software_engineering       0.83      0.76      0.79       494\n",
            "          statistics       0.60      0.88      0.72       617\n",
            "    theory_computing       0.73      0.69      0.71       412\n",
            "\n",
            "            accuracy                           0.77     10044\n",
            "           macro avg       0.76      0.81      0.78     10044\n",
            "        weighted avg       0.79      0.77      0.77     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.7350, Train: 0.6333, Test: 0.5724\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2715, Train: 0.6687, Test: 0.5806\n",
            "Early stopping:  1.7419063210696604\n",
            "Epoch: 003, Loss: 1.5433, Train: 0.7875, Test: 0.7006\n",
            "Early stopping:  1.6725952241760114\n",
            "Epoch: 004, Loss: 1.1472, Train: 0.8083, Test: 0.7322\n",
            "Early stopping:  1.6093063489532786\n",
            "Epoch: 005, Loss: 0.7702, Train: 0.8375, Test: 0.7630\n",
            "Early stopping:  1.5778302109128726\n",
            "Epoch: 006, Loss: 0.6238, Train: 0.8500, Test: 0.7822\n",
            "Early stopping:  0.663364094027991\n",
            "Epoch: 007, Loss: 0.5532, Train: 0.8708, Test: 0.7869\n",
            "Early stopping:  0.41369728150399476\n",
            "Epoch: 008, Loss: 0.4191, Train: 0.8917, Test: 0.7739\n",
            "Early stopping:  0.27891525167836195\n",
            "Epoch: 009, Loss: 0.3113, Train: 0.9146, Test: 0.7618\n",
            "Early stopping:  0.17818804928671936\n",
            "Epoch: 010, Loss: 0.2676, Train: 0.9042, Test: 0.7386\n",
            "Early stopping:  0.15261687371503885\n",
            "Epoch: 011, Loss: 0.2982, Train: 0.9229, Test: 0.7402\n",
            "Early stopping:  0.11731565663452813\n",
            "Epoch: 012, Loss: 0.2405, Train: 0.9437, Test: 0.7524\n",
            "Early stopping:  0.06825042327484074\n",
            "Epoch: 013, Loss: 0.1913, Train: 0.9479, Test: 0.7774\n",
            "Early stopping:  0.048051241969576504\n",
            "Epoch: 014, Loss: 0.1553, Train: 0.9542, Test: 0.7785\n",
            "Early stopping:  0.05754223554883126\n",
            "Epoch: 015, Loss: 0.1545, Train: 0.9604, Test: 0.7747\n",
            "Early stopping:  0.06144737202760785\n",
            "Epoch: 016, Loss: 0.1261, Train: 0.9625, Test: 0.7679\n",
            "Early stopping:  0.04399781903327972\n",
            "Epoch: 017, Loss: 0.1199, Train: 0.9729, Test: 0.7805\n",
            "Early stopping:  0.028416029757755187\n",
            "Epoch: 018, Loss: 0.0816, Train: 0.9771, Test: 0.7871\n",
            "Early stopping:  0.03030628864697391\n",
            "Epoch: 019, Loss: 0.0658, Train: 0.9896, Test: 0.7940\n",
            "Early stopping:  0.035715097343062795\n",
            "Epoch: 020, Loss: 0.0501, Train: 0.9979, Test: 0.7906\n",
            "Early stopping:  0.03330679439691912\n",
            "Epoch: 021, Loss: 0.0357, Train: 0.9979, Test: 0.7791\n",
            "Early stopping:  0.032429232642555324\n",
            "Epoch: 022, Loss: 0.0337, Train: 0.9958, Test: 0.7750\n",
            "Early stopping:  0.02035485261223993\n",
            "Epoch: 023, Loss: 0.0300, Train: 0.9979, Test: 0.7743\n",
            "Early stopping:  0.014813494527663718\n",
            "Epoch: 024, Loss: 0.0268, Train: 0.9938, Test: 0.7746\n",
            "Early stopping:  0.008976912709986792\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.84      0.78       372\n",
            "             ecology       0.79      0.86      0.82       859\n",
            "            economic       0.82      0.63      0.72      1379\n",
            "          geophysics       0.90      0.91      0.91      1172\n",
            "  gravitional_theory       0.88      0.92      0.90       100\n",
            "               hydro       0.68      0.75      0.71       325\n",
            "                math       0.94      0.68      0.79      1309\n",
            "              metals       0.70      0.87      0.77       171\n",
            "          networking       0.87      0.86      0.86       315\n",
            "        neuroscience       0.93      0.96      0.95       277\n",
            "        oceanography       0.91      0.76      0.83       960\n",
            "             politic       0.71      0.68      0.69       573\n",
            "           sociology       0.51      0.73      0.60       709\n",
            "software_engineering       0.87      0.79      0.83       494\n",
            "          statistics       0.56      0.90      0.69       617\n",
            "    theory_computing       0.73      0.71      0.72       412\n",
            "\n",
            "            accuracy                           0.77     10044\n",
            "           macro avg       0.78      0.80      0.79     10044\n",
            "        weighted avg       0.80      0.77      0.78     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.0206, Train: 0.4917, Test: 0.5372\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4271, Train: 0.7292, Test: 0.6219\n",
            "Early stopping:  1.1268037499588779\n",
            "Epoch: 003, Loss: 1.4651, Train: 0.8063, Test: 0.7404\n",
            "Early stopping:  1.2906955386538914\n",
            "Epoch: 004, Loss: 0.8138, Train: 0.8313, Test: 0.7758\n",
            "Early stopping:  1.3936226801160811\n",
            "Epoch: 005, Loss: 0.5753, Train: 0.8604, Test: 0.7638\n",
            "Early stopping:  1.404539553899629\n",
            "Epoch: 006, Loss: 0.5308, Train: 0.8833, Test: 0.7828\n",
            "Early stopping:  0.7994198451325718\n",
            "Epoch: 007, Loss: 0.3982, Train: 0.8958, Test: 0.7684\n",
            "Early stopping:  0.4235661717019584\n",
            "Epoch: 008, Loss: 0.3757, Train: 0.9083, Test: 0.7687\n",
            "Early stopping:  0.17562991749442217\n",
            "Epoch: 009, Loss: 0.3247, Train: 0.9271, Test: 0.7818\n",
            "Early stopping:  0.10689805671694996\n",
            "Epoch: 010, Loss: 0.2382, Train: 0.9292, Test: 0.7873\n",
            "Early stopping:  0.10727958876433018\n",
            "Epoch: 011, Loss: 0.2090, Train: 0.9437, Test: 0.7840\n",
            "Early stopping:  0.08316440517129278\n",
            "Epoch: 012, Loss: 0.1888, Train: 0.9604, Test: 0.7815\n",
            "Early stopping:  0.07978718950751813\n",
            "Epoch: 013, Loss: 0.1456, Train: 0.9708, Test: 0.7760\n",
            "Early stopping:  0.06694061380653286\n",
            "Epoch: 014, Loss: 0.1035, Train: 0.9750, Test: 0.7762\n",
            "Early stopping:  0.05316199220864493\n",
            "Epoch: 015, Loss: 0.0889, Train: 0.9833, Test: 0.7732\n",
            "Early stopping:  0.05211939165580573\n",
            "Epoch: 016, Loss: 0.0756, Train: 0.9854, Test: 0.7750\n",
            "Early stopping:  0.046375431742538324\n",
            "Epoch: 017, Loss: 0.0604, Train: 0.9917, Test: 0.7773\n",
            "Early stopping:  0.03256617847757852\n",
            "Epoch: 018, Loss: 0.0446, Train: 0.9979, Test: 0.7807\n",
            "Early stopping:  0.023122279728662903\n",
            "Epoch: 019, Loss: 0.0334, Train: 0.9979, Test: 0.7831\n",
            "Early stopping:  0.022459910873818767\n",
            "Epoch: 020, Loss: 0.0282, Train: 0.9979, Test: 0.7841\n",
            "Early stopping:  0.01955009218731345\n",
            "Epoch: 021, Loss: 0.0242, Train: 1.0000, Test: 0.7849\n",
            "Early stopping:  0.01459245518551447\n",
            "Epoch: 022, Loss: 0.0205, Train: 1.0000, Test: 0.7854\n",
            "Early stopping:  0.009380001417334465\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.57      0.65       372\n",
            "             ecology       0.80      0.83      0.82       859\n",
            "            economic       0.79      0.74      0.76      1379\n",
            "          geophysics       0.94      0.86      0.90      1172\n",
            "  gravitional_theory       0.83      0.95      0.88       100\n",
            "               hydro       0.68      0.82      0.74       325\n",
            "                math       0.90      0.76      0.82      1309\n",
            "              metals       0.55      0.91      0.69       171\n",
            "          networking       0.76      0.87      0.81       315\n",
            "        neuroscience       0.96      0.95      0.95       277\n",
            "        oceanography       0.87      0.79      0.83       960\n",
            "             politic       0.71      0.65      0.68       573\n",
            "           sociology       0.65      0.71      0.68       709\n",
            "software_engineering       0.79      0.84      0.81       494\n",
            "          statistics       0.72      0.82      0.77       617\n",
            "    theory_computing       0.58      0.78      0.66       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.77      0.80      0.78     10044\n",
            "        weighted avg       0.80      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.8459, Train: 0.6333, Test: 0.6075\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.5874, Train: 0.7250, Test: 0.7147\n",
            "Early stopping:  1.5969850568145274\n",
            "Epoch: 003, Loss: 1.7480, Train: 0.8000, Test: 0.7356\n",
            "Early stopping:  1.2601463478364756\n",
            "Epoch: 004, Loss: 0.9428, Train: 0.8354, Test: 0.7170\n",
            "Early stopping:  1.2589639108012025\n",
            "Epoch: 005, Loss: 0.7888, Train: 0.8438, Test: 0.7105\n",
            "Early stopping:  1.2236674654621074\n",
            "Epoch: 006, Loss: 0.8681, Train: 0.8562, Test: 0.7151\n",
            "Early stopping:  0.445787878003427\n",
            "Epoch: 007, Loss: 0.7049, Train: 0.8979, Test: 0.7619\n",
            "Early stopping:  0.42168216232836586\n",
            "Epoch: 008, Loss: 0.4328, Train: 0.8917, Test: 0.7711\n",
            "Early stopping:  0.19702746847192804\n",
            "Epoch: 009, Loss: 0.4099, Train: 0.9000, Test: 0.7712\n",
            "Early stopping:  0.2087529209810686\n",
            "Epoch: 010, Loss: 0.3943, Train: 0.9021, Test: 0.7722\n",
            "Early stopping:  0.2133666708054196\n",
            "Epoch: 011, Loss: 0.3232, Train: 0.9333, Test: 0.7884\n",
            "Early stopping:  0.1466325661153687\n",
            "Epoch: 012, Loss: 0.2004, Train: 0.9375, Test: 0.7818\n",
            "Early stopping:  0.094187517549644\n",
            "Epoch: 013, Loss: 0.1780, Train: 0.9396, Test: 0.7736\n",
            "Early stopping:  0.10762202023346314\n",
            "Epoch: 014, Loss: 0.1729, Train: 0.9667, Test: 0.7735\n",
            "Early stopping:  0.09965412661059798\n",
            "Epoch: 015, Loss: 0.1302, Train: 0.9792, Test: 0.7677\n",
            "Early stopping:  0.07291930877923687\n",
            "Epoch: 016, Loss: 0.0959, Train: 0.9812, Test: 0.7637\n",
            "Early stopping:  0.04187932302150368\n",
            "Epoch: 017, Loss: 0.0784, Train: 0.9875, Test: 0.7587\n",
            "Early stopping:  0.04462968599082043\n",
            "Epoch: 018, Loss: 0.0754, Train: 0.9833, Test: 0.7538\n",
            "Early stopping:  0.041110246579693734\n",
            "Epoch: 019, Loss: 0.0696, Train: 0.9896, Test: 0.7556\n",
            "Early stopping:  0.024572525263540642\n",
            "Epoch: 020, Loss: 0.0476, Train: 0.9917, Test: 0.7506\n",
            "Early stopping:  0.017408398499862577\n",
            "Epoch: 021, Loss: 0.0351, Train: 0.9958, Test: 0.7445\n",
            "Early stopping:  0.01892870550471803\n",
            "Epoch: 022, Loss: 0.0382, Train: 0.9938, Test: 0.7450\n",
            "Early stopping:  0.01834671927172236\n",
            "Epoch: 023, Loss: 0.0364, Train: 0.9938, Test: 0.7470\n",
            "Early stopping:  0.014406878253670126\n",
            "Epoch: 024, Loss: 0.0281, Train: 0.9979, Test: 0.7499\n",
            "Early stopping:  0.007035310395292984\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.83      0.76       372\n",
            "             ecology       0.69      0.85      0.76       859\n",
            "            economic       0.77      0.61      0.68      1379\n",
            "          geophysics       0.96      0.88      0.92      1172\n",
            "  gravitional_theory       0.44      0.93      0.59       100\n",
            "               hydro       0.69      0.75      0.72       325\n",
            "                math       0.84      0.67      0.74      1309\n",
            "              metals       0.46      0.90      0.61       171\n",
            "          networking       0.77      0.90      0.83       315\n",
            "        neuroscience       0.93      0.92      0.93       277\n",
            "        oceanography       0.84      0.80      0.82       960\n",
            "             politic       0.73      0.71      0.72       573\n",
            "           sociology       0.61      0.60      0.61       709\n",
            "software_engineering       0.76      0.80      0.78       494\n",
            "          statistics       0.64      0.78      0.70       617\n",
            "    theory_computing       0.72      0.59      0.65       412\n",
            "\n",
            "            accuracy                           0.75     10044\n",
            "           macro avg       0.72      0.78      0.74     10044\n",
            "        weighted avg       0.77      0.75      0.75     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7139, Train: 0.6875, Test: 0.5371\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.7838, Train: 0.7917, Test: 0.7207\n",
            "Early stopping:  1.3647533927085906\n",
            "Epoch: 003, Loss: 0.9107, Train: 0.8354, Test: 0.7682\n",
            "Early stopping:  1.4343909986403043\n",
            "Epoch: 004, Loss: 0.7138, Train: 0.8750, Test: 0.7860\n",
            "Early stopping:  1.3702012131642824\n",
            "Epoch: 005, Loss: 0.4626, Train: 0.8708, Test: 0.7733\n",
            "Early stopping:  1.3249496432052088\n",
            "Epoch: 006, Loss: 0.4362, Train: 0.8896, Test: 0.7667\n",
            "Early stopping:  0.5511171294001953\n",
            "Epoch: 007, Loss: 0.3463, Train: 0.9146, Test: 0.7656\n",
            "Early stopping:  0.23247201514050728\n",
            "Epoch: 008, Loss: 0.2481, Train: 0.9229, Test: 0.7682\n",
            "Early stopping:  0.17396215585140493\n",
            "Epoch: 009, Loss: 0.2108, Train: 0.9458, Test: 0.7612\n",
            "Early stopping:  0.11117888566340828\n",
            "Epoch: 010, Loss: 0.1809, Train: 0.9604, Test: 0.7661\n",
            "Early stopping:  0.10526195714838674\n",
            "Epoch: 011, Loss: 0.1353, Train: 0.9583, Test: 0.7713\n",
            "Early stopping:  0.07975715121107835\n",
            "Epoch: 012, Loss: 0.1103, Train: 0.9604, Test: 0.7728\n",
            "Early stopping:  0.05569983664986355\n",
            "Epoch: 013, Loss: 0.1111, Train: 0.9708, Test: 0.7733\n",
            "Early stopping:  0.04462066176676828\n",
            "Epoch: 014, Loss: 0.0765, Train: 0.9896, Test: 0.7703\n",
            "Early stopping:  0.03862810583260127\n",
            "Epoch: 015, Loss: 0.0545, Train: 0.9896, Test: 0.7624\n",
            "Early stopping:  0.03187704271301106\n",
            "Epoch: 016, Loss: 0.0521, Train: 0.9875, Test: 0.7567\n",
            "Early stopping:  0.028816375714702535\n",
            "Epoch: 017, Loss: 0.0508, Train: 0.9896, Test: 0.7512\n",
            "Early stopping:  0.025760057508350885\n",
            "Epoch: 018, Loss: 0.0502, Train: 0.9938, Test: 0.7568\n",
            "Early stopping:  0.011124417406366504\n",
            "Epoch: 019, Loss: 0.0378, Train: 0.9958, Test: 0.7617\n",
            "Early stopping:  0.006505730436744564\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.59      0.67       372\n",
            "             ecology       0.83      0.83      0.83       859\n",
            "            economic       0.83      0.53      0.64      1379\n",
            "          geophysics       0.94      0.89      0.91      1172\n",
            "  gravitional_theory       0.51      0.96      0.67       100\n",
            "               hydro       0.78      0.71      0.74       325\n",
            "                math       0.89      0.74      0.80      1309\n",
            "              metals       0.76      0.88      0.81       171\n",
            "          networking       0.79      0.93      0.85       315\n",
            "        neuroscience       0.86      0.99      0.92       277\n",
            "        oceanography       0.79      0.87      0.82       960\n",
            "             politic       0.53      0.86      0.66       573\n",
            "           sociology       0.64      0.61      0.63       709\n",
            "software_engineering       0.87      0.70      0.77       494\n",
            "          statistics       0.59      0.83      0.69       617\n",
            "    theory_computing       0.59      0.78      0.68       412\n",
            "\n",
            "            accuracy                           0.76     10044\n",
            "           macro avg       0.75      0.79      0.76     10044\n",
            "        weighted avg       0.79      0.76      0.76     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.5735, Train: 0.5521, Test: 0.5434\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2913, Train: 0.7812, Test: 0.6800\n",
            "Early stopping:  2.320905733158617\n",
            "Epoch: 003, Loss: 1.2248, Train: 0.8333, Test: 0.7338\n",
            "Early stopping:  2.2664822016608515\n",
            "Epoch: 004, Loss: 1.0611, Train: 0.8729, Test: 0.7775\n",
            "Early stopping:  2.096113796018823\n",
            "Epoch: 005, Loss: 0.9218, Train: 0.8833, Test: 0.7910\n",
            "Early stopping:  1.9538340949319577\n",
            "Epoch: 006, Loss: 0.7883, Train: 0.9021, Test: 0.7929\n",
            "Early stopping:  0.600249585354981\n",
            "Epoch: 007, Loss: 0.6760, Train: 0.9104, Test: 0.7937\n",
            "Early stopping:  0.2171835323001941\n",
            "Epoch: 008, Loss: 0.5402, Train: 0.9250, Test: 0.7852\n",
            "Early stopping:  0.20369145552952525\n",
            "Epoch: 009, Loss: 0.4411, Train: 0.9104, Test: 0.7580\n",
            "Early stopping:  0.19145416731094642\n",
            "Epoch: 010, Loss: 0.4198, Train: 0.9208, Test: 0.7633\n",
            "Early stopping:  0.15723993074018028\n",
            "Epoch: 011, Loss: 0.2932, Train: 0.9458, Test: 0.7780\n",
            "Early stopping:  0.14308911530438734\n",
            "Epoch: 012, Loss: 0.1799, Train: 0.9396, Test: 0.7703\n",
            "Early stopping:  0.14003007008622015\n",
            "Epoch: 013, Loss: 0.2389, Train: 0.9583, Test: 0.7747\n",
            "Early stopping:  0.1133727410301353\n",
            "Epoch: 014, Loss: 0.2091, Train: 0.9521, Test: 0.7805\n",
            "Early stopping:  0.09454957112322483\n",
            "Epoch: 015, Loss: 0.1349, Train: 0.9583, Test: 0.7631\n",
            "Early stopping:  0.05978298250022914\n",
            "Epoch: 016, Loss: 0.1536, Train: 0.9812, Test: 0.7897\n",
            "Early stopping:  0.04179724079875428\n",
            "Epoch: 017, Loss: 0.0752, Train: 0.9792, Test: 0.7864\n",
            "Early stopping:  0.06415213099142197\n",
            "Epoch: 018, Loss: 0.0868, Train: 0.9750, Test: 0.7846\n",
            "Early stopping:  0.05406990214376984\n",
            "Epoch: 019, Loss: 0.0937, Train: 0.9792, Test: 0.7819\n",
            "Early stopping:  0.03365832040704823\n",
            "Epoch: 020, Loss: 0.0878, Train: 0.9833, Test: 0.7801\n",
            "Early stopping:  0.031033625308822907\n",
            "Epoch: 021, Loss: 0.0788, Train: 0.9854, Test: 0.7777\n",
            "Early stopping:  0.007390680674869645\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.80      0.78       372\n",
            "             ecology       0.83      0.84      0.84       859\n",
            "            economic       0.74      0.69      0.71      1379\n",
            "          geophysics       0.86      0.92      0.89      1172\n",
            "  gravitional_theory       0.81      0.93      0.87       100\n",
            "               hydro       0.70      0.77      0.73       325\n",
            "                math       0.89      0.67      0.76      1309\n",
            "              metals       0.67      0.90      0.77       171\n",
            "          networking       0.80      0.82      0.81       315\n",
            "        neuroscience       0.91      0.98      0.94       277\n",
            "        oceanography       0.85      0.78      0.81       960\n",
            "             politic       0.72      0.74      0.73       573\n",
            "           sociology       0.68      0.64      0.66       709\n",
            "software_engineering       0.85      0.85      0.85       494\n",
            "          statistics       0.63      0.78      0.70       617\n",
            "    theory_computing       0.60      0.82      0.69       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.77      0.81      0.78     10044\n",
            "        weighted avg       0.79      0.78      0.78     10044\n",
            "\n",
            "time: 8.47 s (started: 2024-08-16 14:12:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "nPjNrqkUu57N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778ef0d4-a6a8-4f6c-fdae-e7d94a029afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 403 ms (started: 2024-08-16 14:12:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "o5yEru2cu57N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVv1z8wGu57N",
        "outputId": "87d18b9c-e9ab-4d34-a728-1514f1e20861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7771, Train: 0.7375, Test: 0.5742\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5240, Train: 0.8333, Test: 0.7305\n",
            "Early stopping:  0.17892095836147712\n",
            "Epoch: 003, Loss: 2.2173, Train: 0.8479, Test: 0.7551\n",
            "Early stopping:  0.2803310560762672\n",
            "Epoch: 004, Loss: 1.8566, Train: 0.8625, Test: 0.7670\n",
            "Early stopping:  0.39729504336522065\n",
            "Epoch: 005, Loss: 1.4904, Train: 0.8771, Test: 0.7767\n",
            "Early stopping:  0.5138214588327777\n",
            "Epoch: 006, Loss: 1.1568, Train: 0.8812, Test: 0.7868\n",
            "Early stopping:  0.5474924705230528\n",
            "Epoch: 007, Loss: 0.8810, Train: 0.8729, Test: 0.7961\n",
            "Early stopping:  0.5339850755254507\n",
            "Epoch: 008, Loss: 0.6734, Train: 0.8771, Test: 0.7999\n",
            "Early stopping:  0.4732411864176396\n",
            "Epoch: 009, Loss: 0.5291, Train: 0.8833, Test: 0.8042\n",
            "Early stopping:  0.3851156697297833\n",
            "Epoch: 010, Loss: 0.4317, Train: 0.8917, Test: 0.8079\n",
            "Early stopping:  0.29045954142230473\n",
            "Epoch: 011, Loss: 0.3629, Train: 0.9062, Test: 0.8135\n",
            "Early stopping:  0.20671694261852883\n",
            "Epoch: 012, Loss: 0.3109, Train: 0.9167, Test: 0.8162\n",
            "Early stopping:  0.14382195888062788\n",
            "Epoch: 013, Loss: 0.2694, Train: 0.9229, Test: 0.8173\n",
            "Early stopping:  0.10269565257038252\n",
            "Epoch: 014, Loss: 0.2346, Train: 0.9313, Test: 0.8191\n",
            "Early stopping:  0.0778368378042617\n",
            "Epoch: 015, Loss: 0.2041, Train: 0.9375, Test: 0.8209\n",
            "Early stopping:  0.06264185709261513\n",
            "Epoch: 016, Loss: 0.1771, Train: 0.9479, Test: 0.8206\n",
            "Early stopping:  0.05281484168156897\n",
            "Epoch: 017, Loss: 0.1532, Train: 0.9667, Test: 0.8198\n",
            "Early stopping:  0.04598739192506116\n",
            "Epoch: 018, Loss: 0.1315, Train: 0.9729, Test: 0.8201\n",
            "Early stopping:  0.04077559403890569\n",
            "Epoch: 019, Loss: 0.1120, Train: 0.9854, Test: 0.8202\n",
            "Early stopping:  0.036413486131231874\n",
            "Epoch: 020, Loss: 0.0953, Train: 0.9896, Test: 0.8192\n",
            "Early stopping:  0.03245267589135253\n",
            "Epoch: 021, Loss: 0.0811, Train: 0.9958, Test: 0.8187\n",
            "Early stopping:  0.028598702654306786\n",
            "Epoch: 022, Loss: 0.0693, Train: 0.9958, Test: 0.8182\n",
            "Early stopping:  0.024648407059639006\n",
            "Epoch: 023, Loss: 0.0597, Train: 0.9979, Test: 0.8172\n",
            "Early stopping:  0.020772344901337243\n",
            "Epoch: 024, Loss: 0.0519, Train: 0.9979, Test: 0.8167\n",
            "Early stopping:  0.017234270445907446\n",
            "Epoch: 025, Loss: 0.0455, Train: 0.9979, Test: 0.8151\n",
            "Early stopping:  0.014142928189226061\n",
            "Epoch: 026, Loss: 0.0403, Train: 1.0000, Test: 0.8136\n",
            "Early stopping:  0.011520985741012627\n",
            "Epoch: 027, Loss: 0.0361, Train: 1.0000, Test: 0.8129\n",
            "Early stopping:  0.009373578427112408\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.79      0.79       372\n",
            "             ecology       0.84      0.83      0.83       859\n",
            "            economic       0.83      0.70      0.76      1379\n",
            "          geophysics       0.95      0.89      0.92      1172\n",
            "  gravitional_theory       0.82      0.94      0.87       100\n",
            "               hydro       0.71      0.83      0.77       325\n",
            "                math       0.92      0.79      0.85      1309\n",
            "              metals       0.60      0.93      0.73       171\n",
            "          networking       0.79      0.90      0.84       315\n",
            "        neuroscience       0.93      0.92      0.93       277\n",
            "        oceanography       0.84      0.88      0.85       960\n",
            "             politic       0.70      0.79      0.74       573\n",
            "           sociology       0.70      0.70      0.70       709\n",
            "software_engineering       0.83      0.88      0.85       494\n",
            "          statistics       0.73      0.83      0.78       617\n",
            "    theory_computing       0.68      0.83      0.75       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.84      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7686, Train: 0.7250, Test: 0.6441\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5139, Train: 0.7917, Test: 0.6798\n",
            "Early stopping:  0.18009449521141446\n",
            "Epoch: 003, Loss: 2.1971, Train: 0.7854, Test: 0.6841\n",
            "Early stopping:  0.2863076513230633\n",
            "Epoch: 004, Loss: 1.8362, Train: 0.7875, Test: 0.6984\n",
            "Early stopping:  0.40318776145564084\n",
            "Epoch: 005, Loss: 1.4792, Train: 0.8104, Test: 0.7290\n",
            "Early stopping:  0.5160735033778188\n",
            "Epoch: 006, Loss: 1.1592, Train: 0.8479, Test: 0.7691\n",
            "Early stopping:  0.542039162000909\n",
            "Epoch: 007, Loss: 0.8938, Train: 0.8667, Test: 0.7953\n",
            "Early stopping:  0.5201207698231974\n",
            "Epoch: 008, Loss: 0.6905, Train: 0.8875, Test: 0.8060\n",
            "Early stopping:  0.45743243792230404\n",
            "Epoch: 009, Loss: 0.5503, Train: 0.8812, Test: 0.8085\n",
            "Early stopping:  0.3721556177895373\n",
            "Epoch: 010, Loss: 0.4585, Train: 0.8917, Test: 0.8084\n",
            "Early stopping:  0.28132395564821744\n",
            "Epoch: 011, Loss: 0.3920, Train: 0.9000, Test: 0.8081\n",
            "Early stopping:  0.20014956031979705\n",
            "Epoch: 012, Loss: 0.3403, Train: 0.9021, Test: 0.8064\n",
            "Early stopping:  0.13854884446182028\n",
            "Epoch: 013, Loss: 0.3007, Train: 0.9062, Test: 0.8030\n",
            "Early stopping:  0.09891319837974229\n",
            "Epoch: 014, Loss: 0.2683, Train: 0.9208, Test: 0.8022\n",
            "Early stopping:  0.07534753293635178\n",
            "Epoch: 015, Loss: 0.2371, Train: 0.9292, Test: 0.8021\n",
            "Early stopping:  0.06071999803461884\n",
            "Epoch: 016, Loss: 0.2070, Train: 0.9354, Test: 0.8027\n",
            "Early stopping:  0.05228826324707066\n",
            "Epoch: 017, Loss: 0.1806, Train: 0.9458, Test: 0.8021\n",
            "Early stopping:  0.047714935868467305\n",
            "Epoch: 018, Loss: 0.1580, Train: 0.9563, Test: 0.8017\n",
            "Early stopping:  0.043910929838097267\n",
            "Epoch: 019, Loss: 0.1367, Train: 0.9667, Test: 0.8009\n",
            "Early stopping:  0.03960360453100958\n",
            "Epoch: 020, Loss: 0.1176, Train: 0.9812, Test: 0.7997\n",
            "Early stopping:  0.03526104075546416\n",
            "Epoch: 021, Loss: 0.1015, Train: 0.9875, Test: 0.7981\n",
            "Early stopping:  0.03145716205005038\n",
            "Epoch: 022, Loss: 0.0885, Train: 0.9917, Test: 0.7970\n",
            "Early stopping:  0.02764902141694477\n",
            "Epoch: 023, Loss: 0.0774, Train: 0.9896, Test: 0.7973\n",
            "Early stopping:  0.023507475261645656\n",
            "Epoch: 024, Loss: 0.0672, Train: 0.9958, Test: 0.7943\n",
            "Early stopping:  0.01983564068229187\n",
            "Epoch: 025, Loss: 0.0588, Train: 0.9958, Test: 0.7940\n",
            "Early stopping:  0.016931046436433082\n",
            "Epoch: 026, Loss: 0.0524, Train: 0.9979, Test: 0.7950\n",
            "Early stopping:  0.014448451201031918\n",
            "Epoch: 027, Loss: 0.0470, Train: 1.0000, Test: 0.7952\n",
            "Early stopping:  0.01205792866371681\n",
            "Epoch: 028, Loss: 0.0422, Train: 1.0000, Test: 0.7954\n",
            "Early stopping:  0.009864193183316263\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.81      0.77       372\n",
            "             ecology       0.81      0.79      0.80       859\n",
            "            economic       0.82      0.70      0.75      1379\n",
            "          geophysics       0.94      0.85      0.89      1172\n",
            "  gravitional_theory       0.85      0.93      0.89       100\n",
            "               hydro       0.68      0.88      0.77       325\n",
            "                math       0.91      0.77      0.83      1309\n",
            "              metals       0.56      0.94      0.70       171\n",
            "          networking       0.77      0.87      0.82       315\n",
            "        neuroscience       0.86      0.97      0.91       277\n",
            "        oceanography       0.83      0.87      0.85       960\n",
            "             politic       0.67      0.70      0.68       573\n",
            "           sociology       0.65      0.66      0.66       709\n",
            "software_engineering       0.85      0.83      0.84       494\n",
            "          statistics       0.75      0.84      0.80       617\n",
            "    theory_computing       0.68      0.80      0.74       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.77      0.83      0.79     10044\n",
            "        weighted avg       0.80      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7797, Train: 0.6479, Test: 0.5648\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5207, Train: 0.8063, Test: 0.7186\n",
            "Early stopping:  0.18314626421839128\n",
            "Epoch: 003, Loss: 2.2009, Train: 0.8208, Test: 0.7269\n",
            "Early stopping:  0.28989203353143383\n",
            "Epoch: 004, Loss: 1.8324, Train: 0.8271, Test: 0.7342\n",
            "Early stopping:  0.40938811313543616\n",
            "Epoch: 005, Loss: 1.4661, Train: 0.8500, Test: 0.7446\n",
            "Early stopping:  0.5254885675312398\n",
            "Epoch: 006, Loss: 1.1378, Train: 0.8604, Test: 0.7644\n",
            "Early stopping:  0.5536457937137057\n",
            "Epoch: 007, Loss: 0.8684, Train: 0.8771, Test: 0.7892\n",
            "Early stopping:  0.5322147724423817\n",
            "Epoch: 008, Loss: 0.6663, Train: 0.8917, Test: 0.8012\n",
            "Early stopping:  0.4661394797633968\n",
            "Epoch: 009, Loss: 0.5274, Train: 0.8958, Test: 0.8046\n",
            "Early stopping:  0.37614615308737925\n",
            "Epoch: 010, Loss: 0.4360, Train: 0.8958, Test: 0.8072\n",
            "Early stopping:  0.2815095721401908\n",
            "Epoch: 011, Loss: 0.3713, Train: 0.8958, Test: 0.8106\n",
            "Early stopping:  0.19844084710236948\n",
            "Epoch: 012, Loss: 0.3205, Train: 0.9104, Test: 0.8114\n",
            "Early stopping:  0.13687568161387503\n",
            "Epoch: 013, Loss: 0.2797, Train: 0.9229, Test: 0.8134\n",
            "Early stopping:  0.09784539445536417\n",
            "Epoch: 014, Loss: 0.2448, Train: 0.9313, Test: 0.8134\n",
            "Early stopping:  0.07552734803819422\n",
            "Epoch: 015, Loss: 0.2128, Train: 0.9417, Test: 0.8127\n",
            "Early stopping:  0.062377708010321616\n",
            "Epoch: 016, Loss: 0.1843, Train: 0.9563, Test: 0.8133\n",
            "Early stopping:  0.05377509950634192\n",
            "Epoch: 017, Loss: 0.1593, Train: 0.9646, Test: 0.8128\n",
            "Early stopping:  0.04774091308663014\n",
            "Epoch: 018, Loss: 0.1373, Train: 0.9729, Test: 0.8125\n",
            "Early stopping:  0.04254519232808583\n",
            "Epoch: 019, Loss: 0.1187, Train: 0.9792, Test: 0.8116\n",
            "Early stopping:  0.03731688963024674\n",
            "Epoch: 020, Loss: 0.1024, Train: 0.9812, Test: 0.8115\n",
            "Early stopping:  0.03244567504954602\n",
            "Epoch: 021, Loss: 0.0878, Train: 0.9833, Test: 0.8108\n",
            "Early stopping:  0.028233322040303443\n",
            "Epoch: 022, Loss: 0.0746, Train: 0.9896, Test: 0.8122\n",
            "Early stopping:  0.024797635932914307\n",
            "Epoch: 023, Loss: 0.0631, Train: 0.9938, Test: 0.8120\n",
            "Early stopping:  0.022033822322012998\n",
            "Epoch: 024, Loss: 0.0535, Train: 0.9979, Test: 0.8095\n",
            "Early stopping:  0.019435373158615894\n",
            "Epoch: 025, Loss: 0.0459, Train: 0.9979, Test: 0.8089\n",
            "Early stopping:  0.016644956909117176\n",
            "Epoch: 026, Loss: 0.0401, Train: 0.9979, Test: 0.8083\n",
            "Early stopping:  0.013714436749436574\n",
            "Epoch: 027, Loss: 0.0356, Train: 1.0000, Test: 0.8081\n",
            "Early stopping:  0.010916930209533401\n",
            "Epoch: 028, Loss: 0.0321, Train: 1.0000, Test: 0.8080\n",
            "Early stopping:  0.0084883782677255\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.84      0.81       372\n",
            "             ecology       0.86      0.84      0.85       859\n",
            "            economic       0.83      0.70      0.76      1379\n",
            "          geophysics       0.97      0.84      0.90      1172\n",
            "  gravitional_theory       0.77      0.93      0.84       100\n",
            "               hydro       0.65      0.86      0.74       325\n",
            "                math       0.89      0.81      0.85      1309\n",
            "              metals       0.66      0.89      0.76       171\n",
            "          networking       0.76      0.93      0.84       315\n",
            "        neuroscience       0.93      0.97      0.95       277\n",
            "        oceanography       0.86      0.88      0.87       960\n",
            "             politic       0.64      0.78      0.70       573\n",
            "           sociology       0.62      0.68      0.64       709\n",
            "software_engineering       0.87      0.81      0.84       494\n",
            "          statistics       0.74      0.82      0.78       617\n",
            "    theory_computing       0.80      0.71      0.76       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7891, Train: 0.7167, Test: 0.6432\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5414, Train: 0.8042, Test: 0.7146\n",
            "Early stopping:  0.1751491525945402\n",
            "Epoch: 003, Loss: 2.2477, Train: 0.8271, Test: 0.7125\n",
            "Early stopping:  0.27101891661493466\n",
            "Epoch: 004, Loss: 1.9052, Train: 0.8521, Test: 0.7261\n",
            "Early stopping:  0.38123832032718336\n",
            "Epoch: 005, Loss: 1.5536, Train: 0.8625, Test: 0.7460\n",
            "Early stopping:  0.49253878977074733\n",
            "Epoch: 006, Loss: 1.2260, Train: 0.8771, Test: 0.7635\n",
            "Early stopping:  0.5259383288301085\n",
            "Epoch: 007, Loss: 0.9480, Train: 0.8750, Test: 0.7793\n",
            "Early stopping:  0.5188827478374093\n",
            "Epoch: 008, Loss: 0.7341, Train: 0.8792, Test: 0.7914\n",
            "Early stopping:  0.4681523618056353\n",
            "Epoch: 009, Loss: 0.5833, Train: 0.8833, Test: 0.7983\n",
            "Early stopping:  0.3886365645401249\n",
            "Epoch: 010, Loss: 0.4832, Train: 0.8938, Test: 0.8002\n",
            "Early stopping:  0.2978698550250266\n",
            "Epoch: 011, Loss: 0.4142, Train: 0.9021, Test: 0.7984\n",
            "Early stopping:  0.21341361778488466\n",
            "Epoch: 012, Loss: 0.3610, Train: 0.9104, Test: 0.7952\n",
            "Early stopping:  0.14793943334799417\n",
            "Epoch: 013, Loss: 0.3165, Train: 0.9146, Test: 0.7946\n",
            "Early stopping:  0.10514285999169168\n",
            "Epoch: 014, Loss: 0.2775, Train: 0.9187, Test: 0.7958\n",
            "Early stopping:  0.08104274000958309\n",
            "Epoch: 015, Loss: 0.2416, Train: 0.9375, Test: 0.7975\n",
            "Early stopping:  0.06799758165330352\n",
            "Epoch: 016, Loss: 0.2086, Train: 0.9563, Test: 0.7986\n",
            "Early stopping:  0.06012427771911357\n",
            "Epoch: 017, Loss: 0.1787, Train: 0.9604, Test: 0.7986\n",
            "Early stopping:  0.05453111468238417\n",
            "Epoch: 018, Loss: 0.1520, Train: 0.9625, Test: 0.7977\n",
            "Early stopping:  0.04971548575010486\n",
            "Epoch: 019, Loss: 0.1280, Train: 0.9750, Test: 0.7974\n",
            "Early stopping:  0.04498938719376534\n",
            "Epoch: 020, Loss: 0.1068, Train: 0.9854, Test: 0.7963\n",
            "Early stopping:  0.04030203221637332\n",
            "Epoch: 021, Loss: 0.0890, Train: 0.9917, Test: 0.7935\n",
            "Early stopping:  0.03561161860267735\n",
            "Epoch: 022, Loss: 0.0746, Train: 0.9979, Test: 0.7922\n",
            "Early stopping:  0.03076480313750677\n",
            "Epoch: 023, Loss: 0.0631, Train: 1.0000, Test: 0.7929\n",
            "Early stopping:  0.025772060479225218\n",
            "Epoch: 024, Loss: 0.0541, Train: 1.0000, Test: 0.7909\n",
            "Early stopping:  0.020939882063061716\n",
            "Epoch: 025, Loss: 0.0471, Train: 1.0000, Test: 0.7909\n",
            "Early stopping:  0.016671151711614665\n",
            "Epoch: 026, Loss: 0.0416, Train: 1.0000, Test: 0.7906\n",
            "Early stopping:  0.01312817214524063\n",
            "Epoch: 027, Loss: 0.0372, Train: 1.0000, Test: 0.7893\n",
            "Early stopping:  0.010305718583665016\n",
            "Epoch: 028, Loss: 0.0336, Train: 1.0000, Test: 0.7898\n",
            "Early stopping:  0.008143672803170328\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.85      0.79       372\n",
            "             ecology       0.84      0.79      0.81       859\n",
            "            economic       0.86      0.67      0.76      1379\n",
            "          geophysics       0.95      0.84      0.89      1172\n",
            "  gravitional_theory       0.71      0.97      0.82       100\n",
            "               hydro       0.73      0.83      0.78       325\n",
            "                math       0.91      0.69      0.79      1309\n",
            "              metals       0.48      0.94      0.63       171\n",
            "          networking       0.77      0.90      0.83       315\n",
            "        neuroscience       0.86      0.97      0.91       277\n",
            "        oceanography       0.82      0.90      0.86       960\n",
            "             politic       0.75      0.78      0.77       573\n",
            "           sociology       0.64      0.69      0.66       709\n",
            "software_engineering       0.87      0.84      0.85       494\n",
            "          statistics       0.66      0.85      0.74       617\n",
            "    theory_computing       0.62      0.73      0.67       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.76      0.83      0.79     10044\n",
            "        weighted avg       0.81      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7830, Train: 0.7021, Test: 0.5935\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5205, Train: 0.8375, Test: 0.7439\n",
            "Early stopping:  0.18557645150352942\n",
            "Epoch: 003, Loss: 2.2052, Train: 0.8500, Test: 0.7673\n",
            "Early stopping:  0.2893043572500257\n",
            "Epoch: 004, Loss: 1.8455, Train: 0.8667, Test: 0.7780\n",
            "Early stopping:  0.4047840326004211\n",
            "Epoch: 005, Loss: 1.4838, Train: 0.8854, Test: 0.7867\n",
            "Early stopping:  0.5186562914727904\n",
            "Epoch: 006, Loss: 1.1551, Train: 0.8854, Test: 0.7944\n",
            "Early stopping:  0.5459839327542934\n",
            "Epoch: 007, Loss: 0.8822, Train: 0.8938, Test: 0.7994\n",
            "Early stopping:  0.5283191072041004\n",
            "Epoch: 008, Loss: 0.6746, Train: 0.8958, Test: 0.8006\n",
            "Early stopping:  0.46794953870752026\n",
            "Epoch: 009, Loss: 0.5290, Train: 0.8917, Test: 0.8014\n",
            "Early stopping:  0.38228837932858145\n",
            "Epoch: 010, Loss: 0.4297, Train: 0.9000, Test: 0.8023\n",
            "Early stopping:  0.29045172845370526\n",
            "Epoch: 011, Loss: 0.3600, Train: 0.9104, Test: 0.8036\n",
            "Early stopping:  0.20839329129796888\n",
            "Epoch: 012, Loss: 0.3081, Train: 0.9187, Test: 0.8038\n",
            "Early stopping:  0.14559166058397222\n",
            "Epoch: 013, Loss: 0.2674, Train: 0.9250, Test: 0.8042\n",
            "Early stopping:  0.10356908930896784\n",
            "Epoch: 014, Loss: 0.2326, Train: 0.9375, Test: 0.8059\n",
            "Early stopping:  0.07774472073225193\n",
            "Epoch: 015, Loss: 0.2011, Train: 0.9458, Test: 0.8076\n",
            "Early stopping:  0.06251167561539028\n",
            "Epoch: 016, Loss: 0.1724, Train: 0.9625, Test: 0.8087\n",
            "Early stopping:  0.053536367851870464\n",
            "Epoch: 017, Loss: 0.1463, Train: 0.9646, Test: 0.8104\n",
            "Early stopping:  0.047900101097121464\n",
            "Epoch: 018, Loss: 0.1237, Train: 0.9771, Test: 0.8087\n",
            "Early stopping:  0.043199887765217465\n",
            "Epoch: 019, Loss: 0.1048, Train: 0.9854, Test: 0.8083\n",
            "Early stopping:  0.03828809113715122\n",
            "Epoch: 020, Loss: 0.0887, Train: 0.9875, Test: 0.8058\n",
            "Early stopping:  0.033179011623125435\n",
            "Epoch: 021, Loss: 0.0751, Train: 0.9917, Test: 0.8064\n",
            "Early stopping:  0.02820292772842095\n",
            "Epoch: 022, Loss: 0.0636, Train: 0.9979, Test: 0.8052\n",
            "Early stopping:  0.02381379674138411\n",
            "Epoch: 023, Loss: 0.0546, Train: 0.9979, Test: 0.8055\n",
            "Early stopping:  0.0199642307796154\n",
            "Epoch: 024, Loss: 0.0475, Train: 0.9979, Test: 0.8048\n",
            "Early stopping:  0.016399750955324897\n",
            "Epoch: 025, Loss: 0.0419, Train: 0.9979, Test: 0.8032\n",
            "Early stopping:  0.013165692174609915\n",
            "Epoch: 026, Loss: 0.0373, Train: 1.0000, Test: 0.8019\n",
            "Early stopping:  0.010432194242283725\n",
            "Epoch: 027, Loss: 0.0335, Train: 1.0000, Test: 0.8017\n",
            "Early stopping:  0.008357571749246876\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.76      0.76       372\n",
            "             ecology       0.80      0.86      0.83       859\n",
            "            economic       0.82      0.68      0.74      1379\n",
            "          geophysics       0.96      0.87      0.92      1172\n",
            "  gravitional_theory       0.85      0.91      0.88       100\n",
            "               hydro       0.73      0.83      0.78       325\n",
            "                math       0.86      0.84      0.85      1309\n",
            "              metals       0.60      0.93      0.73       171\n",
            "          networking       0.72      0.96      0.82       315\n",
            "        neuroscience       0.91      0.97      0.94       277\n",
            "        oceanography       0.88      0.81      0.84       960\n",
            "             politic       0.69      0.81      0.75       573\n",
            "           sociology       0.63      0.62      0.63       709\n",
            "software_engineering       0.86      0.81      0.83       494\n",
            "          statistics       0.78      0.82      0.80       617\n",
            "    theory_computing       0.68      0.74      0.71       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.78      0.83      0.80     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7943, Train: 0.5583, Test: 0.4171\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5555, Train: 0.7833, Test: 0.6747\n",
            "Early stopping:  0.1688755098999966\n",
            "Epoch: 003, Loss: 2.2725, Train: 0.8042, Test: 0.7063\n",
            "Early stopping:  0.2612157564681125\n",
            "Epoch: 004, Loss: 1.9360, Train: 0.8250, Test: 0.7245\n",
            "Early stopping:  0.37002062168580213\n",
            "Epoch: 005, Loss: 1.5854, Train: 0.8438, Test: 0.7464\n",
            "Early stopping:  0.48170362389107185\n",
            "Epoch: 006, Loss: 1.2564, Train: 0.8500, Test: 0.7661\n",
            "Early stopping:  0.5197816089148646\n",
            "Epoch: 007, Loss: 0.9766, Train: 0.8688, Test: 0.7843\n",
            "Early stopping:  0.517661232840376\n",
            "Epoch: 008, Loss: 0.7607, Train: 0.8750, Test: 0.7979\n",
            "Early stopping:  0.4698898219901226\n",
            "Epoch: 009, Loss: 0.6061, Train: 0.8708, Test: 0.8070\n",
            "Early stopping:  0.39194579551564446\n",
            "Epoch: 010, Loss: 0.4982, Train: 0.8771, Test: 0.8122\n",
            "Early stopping:  0.30323974490937994\n",
            "Epoch: 011, Loss: 0.4218, Train: 0.9000, Test: 0.8127\n",
            "Early stopping:  0.22135138054412237\n",
            "Epoch: 012, Loss: 0.3645, Train: 0.9104, Test: 0.8119\n",
            "Early stopping:  0.15744998006363248\n",
            "Epoch: 013, Loss: 0.3188, Train: 0.9187, Test: 0.8111\n",
            "Early stopping:  0.11367417146215071\n",
            "Epoch: 014, Loss: 0.2808, Train: 0.9271, Test: 0.8111\n",
            "Early stopping:  0.08585197235645495\n",
            "Epoch: 015, Loss: 0.2472, Train: 0.9313, Test: 0.8129\n",
            "Early stopping:  0.0688240536336942\n",
            "Epoch: 016, Loss: 0.2159, Train: 0.9458, Test: 0.8169\n",
            "Early stopping:  0.058477440241456194\n",
            "Epoch: 017, Loss: 0.1872, Train: 0.9479, Test: 0.8186\n",
            "Early stopping:  0.051952924663837996\n",
            "Epoch: 018, Loss: 0.1615, Train: 0.9604, Test: 0.8185\n",
            "Early stopping:  0.047289511230535736\n",
            "Epoch: 019, Loss: 0.1380, Train: 0.9688, Test: 0.8150\n",
            "Early stopping:  0.04319612409991199\n",
            "Epoch: 020, Loss: 0.1171, Train: 0.9812, Test: 0.8135\n",
            "Early stopping:  0.039088799359300444\n",
            "Epoch: 021, Loss: 0.0991, Train: 0.9875, Test: 0.8117\n",
            "Early stopping:  0.03495625549445859\n",
            "Epoch: 022, Loss: 0.0842, Train: 0.9896, Test: 0.8118\n",
            "Early stopping:  0.030723693641294327\n",
            "Epoch: 023, Loss: 0.0720, Train: 0.9938, Test: 0.8111\n",
            "Early stopping:  0.02622880853008184\n",
            "Epoch: 024, Loss: 0.0623, Train: 0.9979, Test: 0.8095\n",
            "Early stopping:  0.021754126173270983\n",
            "Epoch: 025, Loss: 0.0544, Train: 0.9979, Test: 0.8081\n",
            "Early stopping:  0.017728878681861862\n",
            "Epoch: 026, Loss: 0.0479, Train: 1.0000, Test: 0.8057\n",
            "Early stopping:  0.01435125784216441\n",
            "Epoch: 027, Loss: 0.0426, Train: 1.0000, Test: 0.8053\n",
            "Early stopping:  0.01166888082116964\n",
            "Epoch: 028, Loss: 0.0383, Train: 1.0000, Test: 0.8059\n",
            "Early stopping:  0.009556387174289253\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.83      0.80       372\n",
            "             ecology       0.83      0.83      0.83       859\n",
            "            economic       0.84      0.65      0.74      1379\n",
            "          geophysics       0.92      0.91      0.92      1172\n",
            "  gravitional_theory       0.83      0.92      0.87       100\n",
            "               hydro       0.75      0.84      0.79       325\n",
            "                math       0.91      0.79      0.84      1309\n",
            "              metals       0.73      0.89      0.80       171\n",
            "          networking       0.83      0.90      0.87       315\n",
            "        neuroscience       0.89      0.98      0.93       277\n",
            "        oceanography       0.86      0.84      0.85       960\n",
            "             politic       0.71      0.73      0.72       573\n",
            "           sociology       0.58      0.77      0.66       709\n",
            "software_engineering       0.84      0.82      0.83       494\n",
            "          statistics       0.71      0.85      0.78       617\n",
            "    theory_computing       0.75      0.72      0.74       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.80      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7730, Train: 0.7271, Test: 0.6164\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4999, Train: 0.8167, Test: 0.7651\n",
            "Early stopping:  0.19312242326091988\n",
            "Epoch: 003, Loss: 2.1840, Train: 0.8396, Test: 0.7786\n",
            "Early stopping:  0.2947759797858842\n",
            "Epoch: 004, Loss: 1.8269, Train: 0.8583, Test: 0.7801\n",
            "Early stopping:  0.4079343837030186\n",
            "Epoch: 005, Loss: 1.4703, Train: 0.8625, Test: 0.7849\n",
            "Early stopping:  0.5191765382290195\n",
            "Epoch: 006, Loss: 1.1465, Train: 0.8646, Test: 0.7918\n",
            "Early stopping:  0.5409638193212286\n",
            "Epoch: 007, Loss: 0.8798, Train: 0.8667, Test: 0.7999\n",
            "Early stopping:  0.5208493172031556\n",
            "Epoch: 008, Loss: 0.6808, Train: 0.8792, Test: 0.8044\n",
            "Early stopping:  0.45852097659277224\n",
            "Epoch: 009, Loss: 0.5423, Train: 0.8771, Test: 0.8094\n",
            "Early stopping:  0.3717063476826237\n",
            "Epoch: 010, Loss: 0.4464, Train: 0.8938, Test: 0.8128\n",
            "Early stopping:  0.2799796853792944\n",
            "Epoch: 011, Loss: 0.3768, Train: 0.9000, Test: 0.8133\n",
            "Early stopping:  0.20030662317111753\n",
            "Epoch: 012, Loss: 0.3241, Train: 0.9104, Test: 0.8132\n",
            "Early stopping:  0.14152345935842298\n",
            "Epoch: 013, Loss: 0.2824, Train: 0.9250, Test: 0.8125\n",
            "Early stopping:  0.102936447471819\n",
            "Epoch: 014, Loss: 0.2468, Train: 0.9333, Test: 0.8104\n",
            "Early stopping:  0.0787737841670047\n",
            "Epoch: 015, Loss: 0.2154, Train: 0.9479, Test: 0.8116\n",
            "Early stopping:  0.06359365989599236\n",
            "Epoch: 016, Loss: 0.1868, Train: 0.9563, Test: 0.8111\n",
            "Early stopping:  0.054176911787178546\n",
            "Epoch: 017, Loss: 0.1602, Train: 0.9667, Test: 0.8117\n",
            "Early stopping:  0.04819981836640178\n",
            "Epoch: 018, Loss: 0.1362, Train: 0.9771, Test: 0.8105\n",
            "Early stopping:  0.04375664470031185\n",
            "Epoch: 019, Loss: 0.1152, Train: 0.9812, Test: 0.8083\n",
            "Early stopping:  0.0397613664897152\n",
            "Epoch: 020, Loss: 0.0975, Train: 0.9875, Test: 0.8072\n",
            "Early stopping:  0.035474897018469675\n",
            "Epoch: 021, Loss: 0.0825, Train: 0.9896, Test: 0.8055\n",
            "Early stopping:  0.030835370186204303\n",
            "Epoch: 022, Loss: 0.0702, Train: 0.9958, Test: 0.8054\n",
            "Early stopping:  0.026166109265659897\n",
            "Epoch: 023, Loss: 0.0603, Train: 0.9958, Test: 0.8064\n",
            "Early stopping:  0.021798833522786817\n",
            "Epoch: 024, Loss: 0.0522, Train: 0.9979, Test: 0.8066\n",
            "Early stopping:  0.017950117090383196\n",
            "Epoch: 025, Loss: 0.0456, Train: 0.9979, Test: 0.8065\n",
            "Early stopping:  0.014641375643653642\n",
            "Epoch: 026, Loss: 0.0402, Train: 1.0000, Test: 0.8076\n",
            "Early stopping:  0.011907552704409078\n",
            "Epoch: 027, Loss: 0.0359, Train: 1.0000, Test: 0.8077\n",
            "Early stopping:  0.009682732385755306\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.69      0.75       372\n",
            "             ecology       0.78      0.87      0.82       859\n",
            "            economic       0.81      0.76      0.78      1379\n",
            "          geophysics       0.94      0.87      0.90      1172\n",
            "  gravitional_theory       0.81      0.93      0.87       100\n",
            "               hydro       0.72      0.83      0.77       325\n",
            "                math       0.92      0.78      0.84      1309\n",
            "              metals       0.58      0.96      0.73       171\n",
            "          networking       0.80      0.88      0.84       315\n",
            "        neuroscience       0.97      0.92      0.94       277\n",
            "        oceanography       0.85      0.82      0.84       960\n",
            "             politic       0.73      0.72      0.72       573\n",
            "           sociology       0.66      0.73      0.69       709\n",
            "software_engineering       0.83      0.84      0.84       494\n",
            "          statistics       0.80      0.82      0.81       617\n",
            "    theory_computing       0.65      0.81      0.72       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7704, Train: 0.7312, Test: 0.5656\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5023, Train: 0.8167, Test: 0.6820\n",
            "Early stopping:  0.18956185749871957\n",
            "Epoch: 003, Loss: 2.1747, Train: 0.8333, Test: 0.7151\n",
            "Early stopping:  0.29833171296534616\n",
            "Epoch: 004, Loss: 1.8048, Train: 0.8354, Test: 0.7415\n",
            "Early stopping:  0.41730039414512077\n",
            "Epoch: 005, Loss: 1.4384, Train: 0.8479, Test: 0.7688\n",
            "Early stopping:  0.532559318982052\n",
            "Epoch: 006, Loss: 1.1100, Train: 0.8562, Test: 0.7890\n",
            "Early stopping:  0.5568580317619265\n",
            "Epoch: 007, Loss: 0.8444, Train: 0.8667, Test: 0.7969\n",
            "Early stopping:  0.531644484758608\n",
            "Epoch: 008, Loss: 0.6515, Train: 0.8708, Test: 0.8017\n",
            "Early stopping:  0.46193071580660017\n",
            "Epoch: 009, Loss: 0.5214, Train: 0.8854, Test: 0.8035\n",
            "Early stopping:  0.36786935529010717\n",
            "Epoch: 010, Loss: 0.4343, Train: 0.8938, Test: 0.8040\n",
            "Early stopping:  0.2706864518944137\n",
            "Epoch: 011, Loss: 0.3736, Train: 0.8958, Test: 0.8070\n",
            "Early stopping:  0.18786495441196036\n",
            "Epoch: 012, Loss: 0.3278, Train: 0.9042, Test: 0.8078\n",
            "Early stopping:  0.12847020612869095\n",
            "Epoch: 013, Loss: 0.2888, Train: 0.9104, Test: 0.8088\n",
            "Early stopping:  0.09164234344601976\n",
            "Epoch: 014, Loss: 0.2533, Train: 0.9208, Test: 0.8068\n",
            "Early stopping:  0.0710805721297434\n",
            "Epoch: 015, Loss: 0.2202, Train: 0.9354, Test: 0.8067\n",
            "Early stopping:  0.06042006515080975\n",
            "Epoch: 016, Loss: 0.1896, Train: 0.9542, Test: 0.8055\n",
            "Early stopping:  0.05459370770390598\n",
            "Epoch: 017, Loss: 0.1617, Train: 0.9625, Test: 0.8042\n",
            "Early stopping:  0.05033365575474889\n",
            "Epoch: 018, Loss: 0.1367, Train: 0.9771, Test: 0.8021\n",
            "Early stopping:  0.0461695206234252\n",
            "Epoch: 019, Loss: 0.1152, Train: 0.9812, Test: 0.8011\n",
            "Early stopping:  0.04165330907167299\n",
            "Epoch: 020, Loss: 0.0968, Train: 0.9875, Test: 0.7989\n",
            "Early stopping:  0.03680905714623357\n",
            "Epoch: 021, Loss: 0.0808, Train: 0.9938, Test: 0.7988\n",
            "Early stopping:  0.03200386860246782\n",
            "Epoch: 022, Loss: 0.0671, Train: 0.9979, Test: 0.7995\n",
            "Early stopping:  0.02757540829120128\n",
            "Epoch: 023, Loss: 0.0561, Train: 1.0000, Test: 0.7990\n",
            "Early stopping:  0.0235228433480613\n",
            "Epoch: 024, Loss: 0.0476, Train: 1.0000, Test: 0.7994\n",
            "Early stopping:  0.019621406621792046\n",
            "Epoch: 025, Loss: 0.0411, Train: 1.0000, Test: 0.7988\n",
            "Early stopping:  0.01579795300514812\n",
            "Epoch: 026, Loss: 0.0363, Train: 1.0000, Test: 0.7968\n",
            "Early stopping:  0.012270890681841213\n",
            "Epoch: 027, Loss: 0.0327, Train: 1.0000, Test: 0.7960\n",
            "Early stopping:  0.00931457697857354\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.84      0.80       372\n",
            "             ecology       0.80      0.86      0.83       859\n",
            "            economic       0.81      0.69      0.75      1379\n",
            "          geophysics       0.95      0.88      0.92      1172\n",
            "  gravitional_theory       0.72      0.95      0.82       100\n",
            "               hydro       0.69      0.83      0.75       325\n",
            "                math       0.87      0.77      0.82      1309\n",
            "              metals       0.60      0.91      0.72       171\n",
            "          networking       0.77      0.90      0.83       315\n",
            "        neuroscience       0.94      0.95      0.94       277\n",
            "        oceanography       0.86      0.85      0.86       960\n",
            "             politic       0.69      0.78      0.73       573\n",
            "           sociology       0.68      0.64      0.66       709\n",
            "software_engineering       0.82      0.85      0.83       494\n",
            "          statistics       0.70      0.71      0.70       617\n",
            "    theory_computing       0.68      0.73      0.70       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.77      0.82      0.79     10044\n",
            "        weighted avg       0.80      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7785, Train: 0.6646, Test: 0.5565\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5108, Train: 0.8167, Test: 0.7051\n",
            "Early stopping:  0.18926194052471604\n",
            "Epoch: 003, Loss: 2.1772, Train: 0.8479, Test: 0.7457\n",
            "Early stopping:  0.30122805172187644\n",
            "Epoch: 004, Loss: 1.8010, Train: 0.8479, Test: 0.7673\n",
            "Early stopping:  0.4228080107716534\n",
            "Epoch: 005, Loss: 1.4351, Train: 0.8542, Test: 0.7817\n",
            "Early stopping:  0.5381347482810557\n",
            "Epoch: 006, Loss: 1.1114, Train: 0.8688, Test: 0.7929\n",
            "Early stopping:  0.5600559414590844\n",
            "Epoch: 007, Loss: 0.8472, Train: 0.8646, Test: 0.8022\n",
            "Early stopping:  0.5308800538419468\n",
            "Epoch: 008, Loss: 0.6491, Train: 0.8750, Test: 0.8048\n",
            "Early stopping:  0.4602780437234253\n",
            "Epoch: 009, Loss: 0.5115, Train: 0.8812, Test: 0.8075\n",
            "Early stopping:  0.3698165880844817\n",
            "Epoch: 010, Loss: 0.4192, Train: 0.8896, Test: 0.8081\n",
            "Early stopping:  0.2772901809227355\n",
            "Epoch: 011, Loss: 0.3554, Train: 0.9000, Test: 0.8089\n",
            "Early stopping:  0.19646428218887188\n",
            "Epoch: 012, Loss: 0.3071, Train: 0.9146, Test: 0.8093\n",
            "Early stopping:  0.13576972401422813\n",
            "Epoch: 013, Loss: 0.2673, Train: 0.9271, Test: 0.8119\n",
            "Early stopping:  0.09636907514621318\n",
            "Epoch: 014, Loss: 0.2325, Train: 0.9396, Test: 0.8115\n",
            "Early stopping:  0.0735275762706578\n",
            "Epoch: 015, Loss: 0.2009, Train: 0.9583, Test: 0.8106\n",
            "Early stopping:  0.06087530686401086\n",
            "Epoch: 016, Loss: 0.1720, Train: 0.9708, Test: 0.8100\n",
            "Early stopping:  0.053326371659716834\n",
            "Epoch: 017, Loss: 0.1463, Train: 0.9708, Test: 0.8092\n",
            "Early stopping:  0.04791912303906274\n",
            "Epoch: 018, Loss: 0.1236, Train: 0.9833, Test: 0.8087\n",
            "Early stopping:  0.04315877571827566\n",
            "Epoch: 019, Loss: 0.1039, Train: 0.9875, Test: 0.8068\n",
            "Early stopping:  0.038422734087715904\n",
            "Epoch: 020, Loss: 0.0871, Train: 0.9875, Test: 0.8053\n",
            "Early stopping:  0.033663176600652496\n",
            "Epoch: 021, Loss: 0.0731, Train: 0.9896, Test: 0.8040\n",
            "Early stopping:  0.029034894974997973\n",
            "Epoch: 022, Loss: 0.0618, Train: 0.9958, Test: 0.8004\n",
            "Early stopping:  0.024544796170706883\n",
            "Epoch: 023, Loss: 0.0530, Train: 0.9979, Test: 0.7989\n",
            "Early stopping:  0.020243906663871786\n",
            "Epoch: 024, Loss: 0.0461, Train: 0.9979, Test: 0.7976\n",
            "Early stopping:  0.01629284704899513\n",
            "Epoch: 025, Loss: 0.0407, Train: 1.0000, Test: 0.7965\n",
            "Early stopping:  0.01287294183498042\n",
            "Epoch: 026, Loss: 0.0362, Train: 1.0000, Test: 0.7965\n",
            "Early stopping:  0.010150988370022788\n",
            "Epoch: 027, Loss: 0.0324, Train: 1.0000, Test: 0.7963\n",
            "Early stopping:  0.008135297979451608\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.77      0.78       372\n",
            "             ecology       0.85      0.81      0.83       859\n",
            "            economic       0.85      0.62      0.72      1379\n",
            "          geophysics       0.95      0.91      0.93      1172\n",
            "  gravitional_theory       0.73      0.95      0.82       100\n",
            "               hydro       0.73      0.82      0.77       325\n",
            "                math       0.89      0.79      0.84      1309\n",
            "              metals       0.66      0.90      0.76       171\n",
            "          networking       0.75      0.93      0.83       315\n",
            "        neuroscience       0.87      0.97      0.92       277\n",
            "        oceanography       0.82      0.87      0.85       960\n",
            "             politic       0.60      0.84      0.70       573\n",
            "           sociology       0.65      0.62      0.64       709\n",
            "software_engineering       0.91      0.81      0.86       494\n",
            "          statistics       0.69      0.82      0.75       617\n",
            "    theory_computing       0.68      0.78      0.73       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.78      0.83      0.79     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7687, Train: 0.7125, Test: 0.5397\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5183, Train: 0.7625, Test: 0.6505\n",
            "Early stopping:  0.1770503126371696\n",
            "Epoch: 003, Loss: 2.2111, Train: 0.7937, Test: 0.6862\n",
            "Early stopping:  0.2792599276763627\n",
            "Epoch: 004, Loss: 1.8553, Train: 0.8208, Test: 0.7185\n",
            "Early stopping:  0.3945746610622484\n",
            "Epoch: 005, Loss: 1.4921, Train: 0.8479, Test: 0.7509\n",
            "Early stopping:  0.5099050481951595\n",
            "Epoch: 006, Loss: 1.1588, Train: 0.8646, Test: 0.7689\n",
            "Early stopping:  0.5437907489955939\n",
            "Epoch: 007, Loss: 0.8844, Train: 0.8771, Test: 0.7803\n",
            "Early stopping:  0.5303981357468402\n",
            "Epoch: 008, Loss: 0.6812, Train: 0.8875, Test: 0.7892\n",
            "Early stopping:  0.4701508682244389\n",
            "Epoch: 009, Loss: 0.5402, Train: 0.8896, Test: 0.8001\n",
            "Early stopping:  0.3814253835715727\n",
            "Epoch: 010, Loss: 0.4425, Train: 0.8938, Test: 0.8082\n",
            "Early stopping:  0.28640467729658664\n",
            "Epoch: 011, Loss: 0.3747, Train: 0.9104, Test: 0.8110\n",
            "Early stopping:  0.20334314449081645\n",
            "Epoch: 012, Loss: 0.3258, Train: 0.9146, Test: 0.8110\n",
            "Early stopping:  0.1415231792919958\n",
            "Epoch: 013, Loss: 0.2860, Train: 0.9208, Test: 0.8106\n",
            "Early stopping:  0.10053183163713772\n",
            "Epoch: 014, Loss: 0.2513, Train: 0.9375, Test: 0.8092\n",
            "Early stopping:  0.07523854873346751\n",
            "Epoch: 015, Loss: 0.2208, Train: 0.9500, Test: 0.8078\n",
            "Early stopping:  0.06072699776290643\n",
            "Epoch: 016, Loss: 0.1931, Train: 0.9583, Test: 0.8102\n",
            "Early stopping:  0.0523779182098789\n",
            "Epoch: 017, Loss: 0.1672, Train: 0.9646, Test: 0.8120\n",
            "Early stopping:  0.0468353255933857\n",
            "Epoch: 018, Loss: 0.1436, Train: 0.9729, Test: 0.8136\n",
            "Early stopping:  0.042572744260220015\n",
            "Epoch: 019, Loss: 0.1230, Train: 0.9771, Test: 0.8127\n",
            "Early stopping:  0.038809695775062616\n",
            "Epoch: 020, Loss: 0.1054, Train: 0.9854, Test: 0.8102\n",
            "Early stopping:  0.034817742326091305\n",
            "Epoch: 021, Loss: 0.0906, Train: 0.9875, Test: 0.8082\n",
            "Early stopping:  0.03037700169868529\n",
            "Epoch: 022, Loss: 0.0780, Train: 0.9938, Test: 0.8053\n",
            "Early stopping:  0.026007902215247285\n",
            "Epoch: 023, Loss: 0.0674, Train: 0.9938, Test: 0.8038\n",
            "Early stopping:  0.022034153086431415\n",
            "Epoch: 024, Loss: 0.0591, Train: 0.9958, Test: 0.8034\n",
            "Early stopping:  0.018410036814761548\n",
            "Epoch: 025, Loss: 0.0525, Train: 1.0000, Test: 0.8014\n",
            "Early stopping:  0.015153656255086217\n",
            "Epoch: 026, Loss: 0.0471, Train: 1.0000, Test: 0.8005\n",
            "Early stopping:  0.012240857234717413\n",
            "Epoch: 027, Loss: 0.0426, Train: 1.0000, Test: 0.7988\n",
            "Early stopping:  0.009833810796354578\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.78      0.78       372\n",
            "             ecology       0.83      0.84      0.83       859\n",
            "            economic       0.77      0.68      0.73      1379\n",
            "          geophysics       0.90      0.90      0.90      1172\n",
            "  gravitional_theory       0.82      0.86      0.84       100\n",
            "               hydro       0.71      0.83      0.77       325\n",
            "                math       0.91      0.74      0.82      1309\n",
            "              metals       0.69      0.91      0.78       171\n",
            "          networking       0.79      0.93      0.85       315\n",
            "        neuroscience       0.96      0.97      0.97       277\n",
            "        oceanography       0.86      0.82      0.84       960\n",
            "             politic       0.71      0.76      0.73       573\n",
            "           sociology       0.68      0.73      0.71       709\n",
            "software_engineering       0.88      0.84      0.86       494\n",
            "          statistics       0.69      0.77      0.73       617\n",
            "    theory_computing       0.62      0.82      0.71       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.79      0.82      0.80     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "time: 12.8 s (started: 2024-08-16 14:12:29 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "VuUjpAYvu57N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106ae139-1632-4fc0-ef8a-63e425becdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 429 ms (started: 2024-08-16 14:12:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 80% ❎"
      ],
      "metadata": {
        "id": "MjS4kz7EtAQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "05GqmUt6u7ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN','80%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNx5taa1u7ay",
        "outputId": "9e70edd2-7c57-44da-e9b3-2b5c32e85b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4239, Train: 0.1653, Test: 0.1721\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 12.5963, Train: 0.1701, Test: 0.1716\n",
            "Early stopping:  5.0716402002967955\n",
            "Epoch: 003, Loss: 12.6517, Train: 0.2808, Test: 0.2779\n",
            "Early stopping:  4.157081515254105\n",
            "Epoch: 004, Loss: 9.7003, Train: 0.3666, Test: 0.3700\n",
            "Early stopping:  3.4043249504935438\n",
            "Epoch: 005, Loss: 6.1940, Train: 0.3171, Test: 0.3063\n",
            "Early stopping:  3.4252941513867863\n",
            "Epoch: 006, Loss: 6.8460, Train: 0.3470, Test: 0.3395\n",
            "Early stopping:  3.061305589056082\n",
            "Epoch: 007, Loss: 5.9098, Train: 0.3689, Test: 0.3489\n",
            "Early stopping:  2.8789463899110386\n",
            "Epoch: 008, Loss: 4.6041, Train: 0.4345, Test: 0.4068\n",
            "Early stopping:  1.8897503744647153\n",
            "Epoch: 009, Loss: 3.3294, Train: 0.4780, Test: 0.4658\n",
            "Early stopping:  1.4052968555629086\n",
            "Epoch: 010, Loss: 2.5441, Train: 0.4861, Test: 0.4742\n",
            "Early stopping:  1.7741348707177251\n",
            "Epoch: 011, Loss: 2.1945, Train: 0.4786, Test: 0.4742\n",
            "Early stopping:  1.5361664649930546\n",
            "Epoch: 012, Loss: 2.1153, Train: 0.4884, Test: 0.4884\n",
            "Early stopping:  1.0382682358694975\n",
            "Epoch: 013, Loss: 2.0185, Train: 0.5059, Test: 0.5058\n",
            "Early stopping:  0.5350744034233902\n",
            "Epoch: 014, Loss: 1.9285, Train: 0.5261, Test: 0.5263\n",
            "Early stopping:  0.236830303491132\n",
            "Epoch: 015, Loss: 1.8811, Train: 0.5350, Test: 0.5305\n",
            "Early stopping:  0.12932903921228825\n",
            "Epoch: 016, Loss: 1.8514, Train: 0.5363, Test: 0.5284\n",
            "Early stopping:  0.10785576630700999\n",
            "Epoch: 017, Loss: 1.8195, Train: 0.5354, Test: 0.5168\n",
            "Early stopping:  0.07755088704364024\n",
            "Epoch: 018, Loss: 1.7841, Train: 0.5366, Test: 0.5174\n",
            "Early stopping:  0.0555964886155765\n",
            "Epoch: 019, Loss: 1.7418, Train: 0.5397, Test: 0.5263\n",
            "Early stopping:  0.054821297976712625\n",
            "Epoch: 020, Loss: 1.6967, Train: 0.5466, Test: 0.5374\n",
            "Early stopping:  0.06135692955299135\n",
            "Epoch: 021, Loss: 1.6570, Train: 0.5588, Test: 0.5426\n",
            "Early stopping:  0.0652464804202298\n",
            "Epoch: 022, Loss: 1.6251, Train: 0.5614, Test: 0.5505\n",
            "Early stopping:  0.0638124747128677\n",
            "Epoch: 023, Loss: 1.6043, Train: 0.5637, Test: 0.5558\n",
            "Early stopping:  0.05532674466113909\n",
            "Epoch: 024, Loss: 1.5740, Train: 0.5722, Test: 0.5658\n",
            "Early stopping:  0.04740354960759753\n",
            "Epoch: 025, Loss: 1.5304, Train: 0.5862, Test: 0.5732\n",
            "Early stopping:  0.04847202464756509\n",
            "Epoch: 026, Loss: 1.4832, Train: 0.6005, Test: 0.5884\n",
            "Early stopping:  0.05724978422253327\n",
            "Epoch: 027, Loss: 1.4394, Train: 0.6064, Test: 0.5968\n",
            "Early stopping:  0.0666775130726855\n",
            "Epoch: 028, Loss: 1.4055, Train: 0.6084, Test: 0.6032\n",
            "Early stopping:  0.06776952358042676\n",
            "Epoch: 029, Loss: 1.3797, Train: 0.6100, Test: 0.6095\n",
            "Early stopping:  0.06036474469905804\n",
            "Epoch: 030, Loss: 1.3561, Train: 0.6184, Test: 0.6158\n",
            "Early stopping:  0.05008137574560487\n",
            "Epoch: 031, Loss: 1.3315, Train: 0.6284, Test: 0.6200\n",
            "Early stopping:  0.04206559863247094\n",
            "Epoch: 032, Loss: 1.3079, Train: 0.6358, Test: 0.6237\n",
            "Early stopping:  0.03849524940905926\n",
            "Epoch: 033, Loss: 1.2856, Train: 0.6436, Test: 0.6284\n",
            "Early stopping:  0.03736403055476659\n",
            "Epoch: 034, Loss: 1.2641, Train: 0.6467, Test: 0.6342\n",
            "Early stopping:  0.036363395513895634\n",
            "Epoch: 035, Loss: 1.2449, Train: 0.6522, Test: 0.6358\n",
            "Early stopping:  0.03432456443909158\n",
            "Epoch: 036, Loss: 1.2284, Train: 0.6571, Test: 0.6342\n",
            "Early stopping:  0.03164643925943185\n",
            "Epoch: 037, Loss: 1.2139, Train: 0.6596, Test: 0.6305\n",
            "Early stopping:  0.02842707638177374\n",
            "Epoch: 038, Loss: 1.1999, Train: 0.6641, Test: 0.6358\n",
            "Early stopping:  0.02527757007331365\n",
            "Epoch: 039, Loss: 1.1845, Train: 0.6687, Test: 0.6342\n",
            "Early stopping:  0.023613604012597846\n",
            "Epoch: 040, Loss: 1.1680, Train: 0.6729, Test: 0.6379\n",
            "Early stopping:  0.023758674791758357\n",
            "Epoch: 041, Loss: 1.1521, Train: 0.6754, Test: 0.6447\n",
            "Early stopping:  0.024603882984558716\n",
            "Epoch: 042, Loss: 1.1386, Train: 0.6787, Test: 0.6474\n",
            "Early stopping:  0.024509142899299435\n",
            "Epoch: 043, Loss: 1.1267, Train: 0.6814, Test: 0.6484\n",
            "Early stopping:  0.022974579760014876\n",
            "Epoch: 044, Loss: 1.1146, Train: 0.6839, Test: 0.6505\n",
            "Early stopping:  0.02090978643820986\n",
            "Epoch: 045, Loss: 1.1010, Train: 0.6874, Test: 0.6500\n",
            "Early stopping:  0.019941110464412406\n",
            "Epoch: 046, Loss: 1.0867, Train: 0.6922, Test: 0.6516\n",
            "Early stopping:  0.020512335716433395\n",
            "Epoch: 047, Loss: 1.0733, Train: 0.6963, Test: 0.6505\n",
            "Early stopping:  0.02132595677813182\n",
            "Epoch: 048, Loss: 1.0606, Train: 0.6976, Test: 0.6511\n",
            "Early stopping:  0.021477737856859302\n",
            "Epoch: 049, Loss: 1.0478, Train: 0.7000, Test: 0.6521\n",
            "Early stopping:  0.020945661118540838\n",
            "Epoch: 050, Loss: 1.0353, Train: 0.7049, Test: 0.6521\n",
            "Early stopping:  0.020267012409659356\n",
            "Epoch: 051, Loss: 1.0233, Train: 0.7075, Test: 0.6532\n",
            "Early stopping:  0.019798668723245347\n",
            "Epoch: 052, Loss: 1.0120, Train: 0.7096, Test: 0.6542\n",
            "Early stopping:  0.019248298592190856\n",
            "Epoch: 053, Loss: 1.0013, Train: 0.7121, Test: 0.6621\n",
            "Early stopping:  0.01841133510047832\n",
            "Epoch: 054, Loss: 0.9909, Train: 0.7146, Test: 0.6626\n",
            "Early stopping:  0.017532566699218734\n",
            "Epoch: 055, Loss: 0.9806, Train: 0.7178, Test: 0.6626\n",
            "Early stopping:  0.016844812623615354\n",
            "Epoch: 056, Loss: 0.9706, Train: 0.7203, Test: 0.6616\n",
            "Early stopping:  0.016352640094311838\n",
            "Epoch: 057, Loss: 0.9607, Train: 0.7233, Test: 0.6589\n",
            "Early stopping:  0.01603096270012212\n",
            "Epoch: 058, Loss: 0.9509, Train: 0.7242, Test: 0.6574\n",
            "Early stopping:  0.015786022167001812\n",
            "Epoch: 059, Loss: 0.9412, Train: 0.7274, Test: 0.6589\n",
            "Early stopping:  0.015572815168514064\n",
            "Epoch: 060, Loss: 0.9309, Train: 0.7312, Test: 0.6605\n",
            "Early stopping:  0.015652151253346434\n",
            "Epoch: 061, Loss: 0.9204, Train: 0.7351, Test: 0.6632\n",
            "Early stopping:  0.015926041812441668\n",
            "Epoch: 062, Loss: 0.9102, Train: 0.7382, Test: 0.6600\n",
            "Early stopping:  0.016166887271059752\n",
            "Epoch: 063, Loss: 0.9005, Train: 0.7417, Test: 0.6626\n",
            "Early stopping:  0.016156446042344622\n",
            "Epoch: 064, Loss: 0.8909, Train: 0.7439, Test: 0.6616\n",
            "Early stopping:  0.015799009773888168\n",
            "Epoch: 065, Loss: 0.8812, Train: 0.7445, Test: 0.6637\n",
            "Early stopping:  0.015466555144039251\n",
            "Epoch: 066, Loss: 0.8713, Train: 0.7486, Test: 0.6642\n",
            "Early stopping:  0.015361889479492147\n",
            "Epoch: 067, Loss: 0.8620, Train: 0.7511, Test: 0.6621\n",
            "Early stopping:  0.015270509499633149\n",
            "Epoch: 068, Loss: 0.8530, Train: 0.7526, Test: 0.6632\n",
            "Early stopping:  0.015011758069938753\n",
            "Epoch: 069, Loss: 0.8441, Train: 0.7561, Test: 0.6653\n",
            "Early stopping:  0.014612497527103446\n",
            "Epoch: 070, Loss: 0.8352, Train: 0.7591, Test: 0.6658\n",
            "Early stopping:  0.0142489003532951\n",
            "Epoch: 071, Loss: 0.8265, Train: 0.7607, Test: 0.6647\n",
            "Early stopping:  0.014046312518466036\n",
            "Epoch: 072, Loss: 0.8179, Train: 0.7634, Test: 0.6668\n",
            "Early stopping:  0.013904503686034731\n",
            "Epoch: 073, Loss: 0.8096, Train: 0.7634, Test: 0.6668\n",
            "Early stopping:  0.013633304260739054\n",
            "Epoch: 074, Loss: 0.8021, Train: 0.7692, Test: 0.6689\n",
            "Early stopping:  0.013139044506301444\n",
            "Epoch: 075, Loss: 0.7955, Train: 0.7676, Test: 0.6684\n",
            "Early stopping:  0.012316018204185115\n",
            "Epoch: 076, Loss: 0.7877, Train: 0.7746, Test: 0.6663\n",
            "Early stopping:  0.011781906658248175\n",
            "Epoch: 077, Loss: 0.7788, Train: 0.7780, Test: 0.6684\n",
            "Early stopping:  0.012036764427484126\n",
            "Epoch: 078, Loss: 0.7718, Train: 0.7774, Test: 0.6695\n",
            "Early stopping:  0.012233818650007722\n",
            "Epoch: 079, Loss: 0.7660, Train: 0.7812, Test: 0.6705\n",
            "Early stopping:  0.011885652988251167\n",
            "Epoch: 080, Loss: 0.7589, Train: 0.7826, Test: 0.6695\n",
            "Early stopping:  0.011169581185853383\n",
            "Epoch: 081, Loss: 0.7509, Train: 0.7828, Test: 0.6716\n",
            "Early stopping:  0.010894971410284865\n",
            "Epoch: 082, Loss: 0.7447, Train: 0.7849, Test: 0.6737\n",
            "Early stopping:  0.010968593175075462\n",
            "Epoch: 083, Loss: 0.7392, Train: 0.7875, Test: 0.6721\n",
            "Early stopping:  0.010753373509270275\n",
            "Epoch: 084, Loss: 0.7311, Train: 0.7895, Test: 0.6747\n",
            "Early stopping:  0.010662799593181997\n",
            "Epoch: 085, Loss: 0.7255, Train: 0.7887, Test: 0.6758\n",
            "Early stopping:  0.010174042834463666\n",
            "Epoch: 086, Loss: 0.7218, Train: 0.7911, Test: 0.6779\n",
            "Early stopping:  0.009477208247899155\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.80      0.74      0.77        96\n",
            "            CAD_and_CAM       0.70      0.79      0.74        92\n",
            "              Companies       0.47      0.47      0.47        85\n",
            "       Computer_Science       0.67      0.73      0.70        88\n",
            "            Consultants       0.58      0.69      0.63        96\n",
            "           Data_Formats       0.74      0.76      0.75       103\n",
            "    Data_Communications       0.78      0.74      0.76       110\n",
            "              Education       0.81      0.79      0.80        97\n",
            "               Graphics       0.80      0.84      0.82       109\n",
            "               Hardware       0.60      0.67      0.63        93\n",
            "               Internet       0.67      0.53      0.59       110\n",
            "       Mobile_Computing       0.78      0.70      0.74        87\n",
            "             Multimedia       0.67      0.76      0.71        98\n",
            "            Open_Source       0.70      0.60      0.64       114\n",
            "            Programming       0.53      0.53      0.53        98\n",
            "               Robotics       0.92      0.90      0.91       105\n",
            "               Security       0.77      0.86      0.81       107\n",
            "               Software       0.30      0.26      0.28       109\n",
            "                Systems       0.55      0.55      0.55       103\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.68      0.68      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.3125, Train: 0.1188, Test: 0.1121\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 7.7659, Train: 0.1670, Test: 0.1716\n",
            "Early stopping:  1.7347841780208955\n",
            "Epoch: 003, Loss: 11.6801, Train: 0.3338, Test: 0.3453\n",
            "Early stopping:  3.2115713074603778\n",
            "Epoch: 004, Loss: 9.9249, Train: 0.3617, Test: 0.3553\n",
            "Early stopping:  2.752287925847119\n",
            "Epoch: 005, Loss: 8.3579, Train: 0.3314, Test: 0.3311\n",
            "Early stopping:  2.387657144502172\n",
            "Epoch: 006, Loss: 7.6708, Train: 0.3718, Test: 0.3558\n",
            "Early stopping:  1.7108873112488998\n",
            "Epoch: 007, Loss: 6.0698, Train: 0.3980, Test: 0.3932\n",
            "Early stopping:  2.148690713528631\n",
            "Epoch: 008, Loss: 4.5321, Train: 0.4450, Test: 0.4395\n",
            "Early stopping:  2.080904812773993\n",
            "Epoch: 009, Loss: 3.1492, Train: 0.4891, Test: 0.4795\n",
            "Early stopping:  2.158001719367549\n",
            "Epoch: 010, Loss: 2.3812, Train: 0.4980, Test: 0.4932\n",
            "Early stopping:  2.15018554256216\n",
            "Epoch: 011, Loss: 2.1093, Train: 0.4595, Test: 0.4521\n",
            "Early stopping:  1.6483091469911257\n",
            "Epoch: 012, Loss: 2.0410, Train: 0.4575, Test: 0.4584\n",
            "Early stopping:  1.041831025372623\n",
            "Epoch: 013, Loss: 1.9590, Train: 0.4808, Test: 0.4932\n",
            "Early stopping:  0.48568736479251573\n",
            "Epoch: 014, Loss: 1.8446, Train: 0.5084, Test: 0.5179\n",
            "Early stopping:  0.20146573778453664\n",
            "Epoch: 015, Loss: 1.7452, Train: 0.5370, Test: 0.5284\n",
            "Early stopping:  0.1468968419030819\n",
            "Epoch: 016, Loss: 1.6882, Train: 0.5503, Test: 0.5463\n",
            "Early stopping:  0.14612060376217995\n",
            "Epoch: 017, Loss: 1.6779, Train: 0.5503, Test: 0.5437\n",
            "Early stopping:  0.11855348497974041\n",
            "Epoch: 018, Loss: 1.6739, Train: 0.5464, Test: 0.5421\n",
            "Early stopping:  0.07225536018510861\n",
            "Epoch: 019, Loss: 1.6552, Train: 0.5561, Test: 0.5484\n",
            "Early stopping:  0.03406065036290205\n",
            "Epoch: 020, Loss: 1.6215, Train: 0.5663, Test: 0.5579\n",
            "Early stopping:  0.026247387276168765\n",
            "Epoch: 021, Loss: 1.5822, Train: 0.5812, Test: 0.5658\n",
            "Early stopping:  0.04023454061575514\n",
            "Epoch: 022, Loss: 1.5407, Train: 0.5928, Test: 0.5742\n",
            "Early stopping:  0.054141811078869805\n",
            "Epoch: 023, Loss: 1.4985, Train: 0.6054, Test: 0.5889\n",
            "Early stopping:  0.06239227789874312\n",
            "Epoch: 024, Loss: 1.4592, Train: 0.6132, Test: 0.5968\n",
            "Early stopping:  0.06456989383021466\n",
            "Epoch: 025, Loss: 1.4255, Train: 0.6175, Test: 0.6000\n",
            "Early stopping:  0.06250890376596473\n",
            "Epoch: 026, Loss: 1.3973, Train: 0.6232, Test: 0.6021\n",
            "Early stopping:  0.057089902526369786\n",
            "Epoch: 027, Loss: 1.3707, Train: 0.6266, Test: 0.6032\n",
            "Early stopping:  0.05038145735111513\n",
            "Epoch: 028, Loss: 1.3448, Train: 0.6345, Test: 0.6074\n",
            "Early stopping:  0.044895283132226946\n",
            "Epoch: 029, Loss: 1.3220, Train: 0.6412, Test: 0.6126\n",
            "Early stopping:  0.041039820093397204\n",
            "Epoch: 030, Loss: 1.3011, Train: 0.6468, Test: 0.6205\n",
            "Early stopping:  0.038151994916122874\n",
            "Epoch: 031, Loss: 1.2796, Train: 0.6514, Test: 0.6268\n",
            "Early stopping:  0.0357675852787645\n",
            "Epoch: 032, Loss: 1.2582, Train: 0.6534, Test: 0.6300\n",
            "Early stopping:  0.03408853080068282\n",
            "Epoch: 033, Loss: 1.2393, Train: 0.6593, Test: 0.6274\n",
            "Early stopping:  0.03295547388472625\n",
            "Epoch: 034, Loss: 1.2225, Train: 0.6614, Test: 0.6316\n",
            "Early stopping:  0.03127014675890157\n",
            "Epoch: 035, Loss: 1.2061, Train: 0.6680, Test: 0.6332\n",
            "Early stopping:  0.02892749595030283\n",
            "Epoch: 036, Loss: 1.1891, Train: 0.6729, Test: 0.6358\n",
            "Early stopping:  0.027116892118380653\n",
            "Epoch: 037, Loss: 1.1730, Train: 0.6774, Test: 0.6395\n",
            "Early stopping:  0.02623351886614585\n",
            "Epoch: 038, Loss: 1.1582, Train: 0.6786, Test: 0.6447\n",
            "Early stopping:  0.02556129607982264\n",
            "Epoch: 039, Loss: 1.1437, Train: 0.6816, Test: 0.6474\n",
            "Early stopping:  0.024611268106999262\n",
            "Epoch: 040, Loss: 1.1293, Train: 0.6850, Test: 0.6458\n",
            "Early stopping:  0.02355623836453409\n",
            "Epoch: 041, Loss: 1.1150, Train: 0.6874, Test: 0.6500\n",
            "Early stopping:  0.022915354409022023\n",
            "Epoch: 042, Loss: 1.1009, Train: 0.6909, Test: 0.6505\n",
            "Early stopping:  0.022653821127505615\n",
            "Epoch: 043, Loss: 1.0875, Train: 0.6945, Test: 0.6511\n",
            "Early stopping:  0.02225686084047582\n",
            "Epoch: 044, Loss: 1.0752, Train: 0.6946, Test: 0.6516\n",
            "Early stopping:  0.02146067860892689\n",
            "Epoch: 045, Loss: 1.0631, Train: 0.6987, Test: 0.6521\n",
            "Early stopping:  0.02049242831112441\n",
            "Epoch: 046, Loss: 1.0510, Train: 0.7001, Test: 0.6505\n",
            "Early stopping:  0.01966331289408762\n",
            "Epoch: 047, Loss: 1.0393, Train: 0.7030, Test: 0.6505\n",
            "Early stopping:  0.01909779587269283\n",
            "Epoch: 048, Loss: 1.0282, Train: 0.7046, Test: 0.6558\n",
            "Early stopping:  0.01865653147974514\n",
            "Epoch: 049, Loss: 1.0172, Train: 0.7086, Test: 0.6605\n",
            "Early stopping:  0.018140282071794495\n",
            "Epoch: 050, Loss: 1.0062, Train: 0.7114, Test: 0.6605\n",
            "Early stopping:  0.017642398416721303\n",
            "Epoch: 051, Loss: 0.9952, Train: 0.7157, Test: 0.6642\n",
            "Early stopping:  0.017387864070060682\n",
            "Epoch: 052, Loss: 0.9846, Train: 0.7186, Test: 0.6663\n",
            "Early stopping:  0.01724652489837393\n",
            "Epoch: 053, Loss: 0.9746, Train: 0.7201, Test: 0.6674\n",
            "Early stopping:  0.016879087352495045\n",
            "Epoch: 054, Loss: 0.9653, Train: 0.7222, Test: 0.6668\n",
            "Early stopping:  0.016203654146711384\n",
            "Epoch: 055, Loss: 0.9556, Train: 0.7246, Test: 0.6705\n",
            "Early stopping:  0.015581893284925971\n",
            "Epoch: 056, Loss: 0.9457, Train: 0.7283, Test: 0.6732\n",
            "Early stopping:  0.015321390994560452\n",
            "Epoch: 057, Loss: 0.9362, Train: 0.7305, Test: 0.6742\n",
            "Early stopping:  0.015264801591080481\n",
            "Epoch: 058, Loss: 0.9274, Train: 0.7314, Test: 0.6732\n",
            "Early stopping:  0.015059269231482802\n",
            "Epoch: 059, Loss: 0.9190, Train: 0.7338, Test: 0.6737\n",
            "Early stopping:  0.014465918843380783\n",
            "Epoch: 060, Loss: 0.9107, Train: 0.7383, Test: 0.6763\n",
            "Early stopping:  0.013771431650504422\n",
            "Epoch: 061, Loss: 0.9024, Train: 0.7392, Test: 0.6774\n",
            "Early stopping:  0.013344372601041068\n",
            "Epoch: 062, Loss: 0.8942, Train: 0.7413, Test: 0.6753\n",
            "Early stopping:  0.013136676012804922\n",
            "Epoch: 063, Loss: 0.8860, Train: 0.7447, Test: 0.6768\n",
            "Early stopping:  0.013060157088032355\n",
            "Epoch: 064, Loss: 0.8780, Train: 0.7462, Test: 0.6811\n",
            "Early stopping:  0.012918500822510868\n",
            "Epoch: 065, Loss: 0.8702, Train: 0.7479, Test: 0.6847\n",
            "Early stopping:  0.012726335787471604\n",
            "Epoch: 066, Loss: 0.8626, Train: 0.7511, Test: 0.6853\n",
            "Early stopping:  0.012487325811831944\n",
            "Epoch: 067, Loss: 0.8547, Train: 0.7528, Test: 0.6863\n",
            "Early stopping:  0.012343776691030506\n",
            "Epoch: 068, Loss: 0.8470, Train: 0.7557, Test: 0.6847\n",
            "Early stopping:  0.012259123254479582\n",
            "Epoch: 069, Loss: 0.8393, Train: 0.7570, Test: 0.6874\n",
            "Early stopping:  0.012238323052719347\n",
            "Epoch: 070, Loss: 0.8319, Train: 0.7576, Test: 0.6868\n",
            "Early stopping:  0.012133800649019241\n",
            "Epoch: 071, Loss: 0.8250, Train: 0.7604, Test: 0.6879\n",
            "Early stopping:  0.011768838680821833\n",
            "Epoch: 072, Loss: 0.8184, Train: 0.7638, Test: 0.6858\n",
            "Early stopping:  0.011320448175476316\n",
            "Epoch: 073, Loss: 0.8124, Train: 0.7650, Test: 0.6879\n",
            "Early stopping:  0.010660995325172984\n",
            "Epoch: 074, Loss: 0.8046, Train: 0.7658, Test: 0.6879\n",
            "Early stopping:  0.010657704302219483\n",
            "Epoch: 075, Loss: 0.7985, Train: 0.7691, Test: 0.6858\n",
            "Early stopping:  0.010584900087438507\n",
            "Epoch: 076, Loss: 0.7931, Train: 0.7721, Test: 0.6895\n",
            "Early stopping:  0.010228561661411228\n",
            "Epoch: 077, Loss: 0.7847, Train: 0.7699, Test: 0.6884\n",
            "Early stopping:  0.010593130852659635\n",
            "Epoch: 078, Loss: 0.7811, Train: 0.7757, Test: 0.6863\n",
            "Early stopping:  0.009655050657762142\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 14, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.73      0.76      0.74       109\n",
            "            CAD_and_CAM       0.68      0.76      0.72        94\n",
            "              Companies       0.49      0.46      0.48        95\n",
            "       Computer_Science       0.76      0.73      0.75       119\n",
            "            Consultants       0.47      0.52      0.49        94\n",
            "           Data_Formats       0.82      0.71      0.76       112\n",
            "    Data_Communications       0.75      0.75      0.75       111\n",
            "              Education       0.91      0.92      0.91       106\n",
            "               Graphics       0.81      0.85      0.83       104\n",
            "               Hardware       0.63      0.68      0.66        94\n",
            "               Internet       0.56      0.65      0.60        82\n",
            "       Mobile_Computing       0.76      0.72      0.74        85\n",
            "             Multimedia       0.66      0.72      0.69        83\n",
            "            Open_Source       0.67      0.77      0.72       101\n",
            "            Programming       0.54      0.60      0.57        97\n",
            "               Robotics       0.96      0.88      0.92       108\n",
            "               Security       0.75      0.78      0.77       103\n",
            "               Software       0.28      0.22      0.25        94\n",
            "                Systems       0.66      0.48      0.55       109\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.69      0.69      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.9366, Train: 0.2466, Test: 0.2489\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 10.6697, Train: 0.2059, Test: 0.2195\n",
            "Early stopping:  4.053901367516104\n",
            "Epoch: 003, Loss: 9.7448, Train: 0.2667, Test: 0.2705\n",
            "Early stopping:  3.077939232890603\n",
            "Epoch: 004, Loss: 8.1261, Train: 0.3279, Test: 0.3337\n",
            "Early stopping:  2.5183506110143217\n",
            "Epoch: 005, Loss: 6.4897, Train: 0.3659, Test: 0.3595\n",
            "Early stopping:  2.337337448734257\n",
            "Epoch: 006, Loss: 4.6449, Train: 0.3939, Test: 0.3811\n",
            "Early stopping:  2.4343339455662947\n",
            "Epoch: 007, Loss: 3.3945, Train: 0.4395, Test: 0.4316\n",
            "Early stopping:  2.5620572216688777\n",
            "Epoch: 008, Loss: 2.5565, Train: 0.4889, Test: 0.4784\n",
            "Early stopping:  2.2725140570852798\n",
            "Epoch: 009, Loss: 2.0564, Train: 0.5049, Test: 0.4947\n",
            "Early stopping:  1.7815211354050113\n",
            "Epoch: 010, Loss: 1.8829, Train: 0.4914, Test: 0.4974\n",
            "Early stopping:  1.1350451594670299\n",
            "Epoch: 011, Loss: 1.8498, Train: 0.5088, Test: 0.5111\n",
            "Early stopping:  0.6496657059842349\n",
            "Epoch: 012, Loss: 1.7877, Train: 0.5353, Test: 0.5395\n",
            "Early stopping:  0.31252301823958917\n",
            "Epoch: 013, Loss: 1.6936, Train: 0.5550, Test: 0.5574\n",
            "Early stopping:  0.13411535737378713\n",
            "Epoch: 014, Loss: 1.6152, Train: 0.5793, Test: 0.5884\n",
            "Early stopping:  0.1108211967472182\n",
            "Epoch: 015, Loss: 1.5422, Train: 0.6043, Test: 0.6021\n",
            "Early stopping:  0.12472517357948135\n",
            "Epoch: 016, Loss: 1.4783, Train: 0.6228, Test: 0.6126\n",
            "Early stopping:  0.12211438972789689\n",
            "Epoch: 017, Loss: 1.4251, Train: 0.6270, Test: 0.6137\n",
            "Early stopping:  0.10686071359477606\n",
            "Epoch: 018, Loss: 1.3817, Train: 0.6318, Test: 0.6174\n",
            "Early stopping:  0.09285703620837076\n",
            "Epoch: 019, Loss: 1.3499, Train: 0.6339, Test: 0.6205\n",
            "Early stopping:  0.07672893800614218\n",
            "Epoch: 020, Loss: 1.3302, Train: 0.6347, Test: 0.6242\n",
            "Early stopping:  0.05962991869999984\n",
            "Epoch: 021, Loss: 1.3158, Train: 0.6359, Test: 0.6195\n",
            "Early stopping:  0.043714857305763376\n",
            "Epoch: 022, Loss: 1.2971, Train: 0.6416, Test: 0.6211\n",
            "Early stopping:  0.03253146314171232\n",
            "Epoch: 023, Loss: 1.2715, Train: 0.6489, Test: 0.6279\n",
            "Early stopping:  0.030175813489812187\n",
            "Epoch: 024, Loss: 1.2438, Train: 0.6528, Test: 0.6405\n",
            "Early stopping:  0.03461292601760443\n",
            "Epoch: 025, Loss: 1.2179, Train: 0.6638, Test: 0.6426\n",
            "Early stopping:  0.03945313165147818\n",
            "Epoch: 026, Loss: 1.1954, Train: 0.6697, Test: 0.6463\n",
            "Early stopping:  0.04063815823384275\n",
            "Epoch: 027, Loss: 1.1744, Train: 0.6749, Test: 0.6579\n",
            "Early stopping:  0.038433802757444645\n",
            "Epoch: 028, Loss: 1.1535, Train: 0.6786, Test: 0.6611\n",
            "Early stopping:  0.03547389210453638\n",
            "Epoch: 029, Loss: 1.1333, Train: 0.6837, Test: 0.6632\n",
            "Early stopping:  0.033394990743432613\n",
            "Epoch: 030, Loss: 1.1137, Train: 0.6847, Test: 0.6584\n",
            "Early stopping:  0.03233791946057076\n",
            "Epoch: 031, Loss: 1.0953, Train: 0.6862, Test: 0.6589\n",
            "Early stopping:  0.03129524569124821\n",
            "Epoch: 032, Loss: 1.0772, Train: 0.6930, Test: 0.6600\n",
            "Early stopping:  0.030131868636136733\n",
            "Epoch: 033, Loss: 1.0602, Train: 0.6978, Test: 0.6637\n",
            "Early stopping:  0.028904266782078455\n",
            "Epoch: 034, Loss: 1.0437, Train: 0.7021, Test: 0.6689\n",
            "Early stopping:  0.02772742928008084\n",
            "Epoch: 035, Loss: 1.0301, Train: 0.7061, Test: 0.6726\n",
            "Early stopping:  0.025988851935652965\n",
            "Epoch: 036, Loss: 1.0169, Train: 0.7104, Test: 0.6763\n",
            "Early stopping:  0.02387759825807913\n",
            "Epoch: 037, Loss: 1.0028, Train: 0.7158, Test: 0.6758\n",
            "Early stopping:  0.0223869045256054\n",
            "Epoch: 038, Loss: 0.9875, Train: 0.7158, Test: 0.6758\n",
            "Early stopping:  0.022098619049596716\n",
            "Epoch: 039, Loss: 0.9737, Train: 0.7192, Test: 0.6758\n",
            "Early stopping:  0.02248293740135586\n",
            "Epoch: 040, Loss: 0.9613, Train: 0.7228, Test: 0.6816\n",
            "Early stopping:  0.022195300234334917\n",
            "Epoch: 041, Loss: 0.9500, Train: 0.7259, Test: 0.6805\n",
            "Early stopping:  0.020855716711672256\n",
            "Epoch: 042, Loss: 0.9382, Train: 0.7274, Test: 0.6847\n",
            "Early stopping:  0.019346936102995316\n",
            "Epoch: 043, Loss: 0.9255, Train: 0.7303, Test: 0.6863\n",
            "Early stopping:  0.01890402207264053\n",
            "Epoch: 044, Loss: 0.9121, Train: 0.7355, Test: 0.6874\n",
            "Early stopping:  0.019465573764105425\n",
            "Epoch: 045, Loss: 0.8995, Train: 0.7387, Test: 0.6874\n",
            "Early stopping:  0.020117633303636008\n",
            "Epoch: 046, Loss: 0.8878, Train: 0.7420, Test: 0.6916\n",
            "Early stopping:  0.02006544724132574\n",
            "Epoch: 047, Loss: 0.8767, Train: 0.7463, Test: 0.6932\n",
            "Early stopping:  0.019296944313731045\n",
            "Epoch: 048, Loss: 0.8660, Train: 0.7503, Test: 0.6932\n",
            "Early stopping:  0.01816844862499232\n",
            "Epoch: 049, Loss: 0.8550, Train: 0.7526, Test: 0.6900\n",
            "Early stopping:  0.01749023951504551\n",
            "Epoch: 050, Loss: 0.8449, Train: 0.7543, Test: 0.6889\n",
            "Early stopping:  0.0169933331361998\n",
            "Epoch: 051, Loss: 0.8354, Train: 0.7583, Test: 0.6905\n",
            "Early stopping:  0.016414413625365856\n",
            "Epoch: 052, Loss: 0.8260, Train: 0.7604, Test: 0.6889\n",
            "Early stopping:  0.01576395463129064\n",
            "Epoch: 053, Loss: 0.8164, Train: 0.7632, Test: 0.6916\n",
            "Early stopping:  0.01520693242548727\n",
            "Epoch: 054, Loss: 0.8066, Train: 0.7653, Test: 0.6926\n",
            "Early stopping:  0.015118458780804929\n",
            "Epoch: 055, Loss: 0.7971, Train: 0.7693, Test: 0.6932\n",
            "Early stopping:  0.015197590101973085\n",
            "Epoch: 056, Loss: 0.7880, Train: 0.7712, Test: 0.6926\n",
            "Early stopping:  0.015063505240130157\n",
            "Epoch: 057, Loss: 0.7792, Train: 0.7746, Test: 0.6937\n",
            "Early stopping:  0.014686242881540188\n",
            "Epoch: 058, Loss: 0.7701, Train: 0.7776, Test: 0.6921\n",
            "Early stopping:  0.014352309175839982\n",
            "Epoch: 059, Loss: 0.7612, Train: 0.7797, Test: 0.6942\n",
            "Early stopping:  0.01418319749677269\n",
            "Epoch: 060, Loss: 0.7526, Train: 0.7811, Test: 0.6937\n",
            "Early stopping:  0.01405173984585784\n",
            "Epoch: 061, Loss: 0.7444, Train: 0.7838, Test: 0.6958\n",
            "Early stopping:  0.01377614231679562\n",
            "Epoch: 062, Loss: 0.7361, Train: 0.7853, Test: 0.6968\n",
            "Early stopping:  0.013400526524005424\n",
            "Epoch: 063, Loss: 0.7278, Train: 0.7891, Test: 0.6974\n",
            "Early stopping:  0.013173302684002993\n",
            "Epoch: 064, Loss: 0.7195, Train: 0.7901, Test: 0.6984\n",
            "Early stopping:  0.013092400129810689\n",
            "Epoch: 065, Loss: 0.7115, Train: 0.7926, Test: 0.6958\n",
            "Early stopping:  0.013041411943134119\n",
            "Epoch: 066, Loss: 0.7035, Train: 0.7949, Test: 0.6963\n",
            "Early stopping:  0.012905900443030194\n",
            "Epoch: 067, Loss: 0.6955, Train: 0.7987, Test: 0.6979\n",
            "Early stopping:  0.01273785351200539\n",
            "Epoch: 068, Loss: 0.6877, Train: 0.7999, Test: 0.6979\n",
            "Early stopping:  0.01259550614052047\n",
            "Epoch: 069, Loss: 0.6800, Train: 0.8036, Test: 0.6984\n",
            "Early stopping:  0.012444947673917519\n",
            "Epoch: 070, Loss: 0.6728, Train: 0.8045, Test: 0.6968\n",
            "Early stopping:  0.012149734148701078\n",
            "Epoch: 071, Loss: 0.6666, Train: 0.8050, Test: 0.6989\n",
            "Early stopping:  0.011508340869185485\n",
            "Epoch: 072, Loss: 0.6626, Train: 0.8036, Test: 0.6921\n",
            "Early stopping:  0.01011698829401328\n",
            "Epoch: 073, Loss: 0.6564, Train: 0.8083, Test: 0.6984\n",
            "Early stopping:  0.009107562609659186\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.75      0.69      0.72        93\n",
            "            CAD_and_CAM       0.76      0.77      0.76       101\n",
            "              Companies       0.51      0.66      0.58        82\n",
            "       Computer_Science       0.72      0.75      0.74        93\n",
            "            Consultants       0.57      0.67      0.62        98\n",
            "           Data_Formats       0.78      0.83      0.81       110\n",
            "    Data_Communications       0.73      0.73      0.73        98\n",
            "              Education       0.85      0.93      0.89        94\n",
            "               Graphics       0.88      0.86      0.87       111\n",
            "               Hardware       0.69      0.65      0.67       101\n",
            "               Internet       0.67      0.59      0.63       103\n",
            "       Mobile_Computing       0.74      0.71      0.72        94\n",
            "             Multimedia       0.60      0.71      0.65        89\n",
            "            Open_Source       0.69      0.66      0.67        92\n",
            "            Programming       0.59      0.56      0.58       109\n",
            "               Robotics       0.90      0.85      0.88       116\n",
            "               Security       0.87      0.79      0.83        95\n",
            "               Software       0.31      0.29      0.30       100\n",
            "                Systems       0.65      0.56      0.60       121\n",
            "\n",
            "               accuracy                           0.70      1900\n",
            "              macro avg       0.70      0.70      0.70      1900\n",
            "           weighted avg       0.70      0.70      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4510, Train: 0.1259, Test: 0.1237\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 15.8626, Train: 0.2059, Test: 0.1937\n",
            "Early stopping:  7.362155864042935\n",
            "Epoch: 003, Loss: 12.6815, Train: 0.1483, Test: 0.1532\n",
            "Early stopping:  5.335460803753879\n",
            "Epoch: 004, Loss: 14.1048, Train: 0.2372, Test: 0.2484\n",
            "Early stopping:  4.571717810773416\n",
            "Epoch: 005, Loss: 8.7252, Train: 0.2799, Test: 0.2795\n",
            "Early stopping:  4.225296553071645\n",
            "Epoch: 006, Loss: 7.7943, Train: 0.3172, Test: 0.3158\n",
            "Early stopping:  3.4673016923918176\n",
            "Epoch: 007, Loss: 7.0640, Train: 0.3416, Test: 0.3300\n",
            "Early stopping:  3.1274246968871955\n",
            "Epoch: 008, Loss: 5.9707, Train: 0.3937, Test: 0.3900\n",
            "Early stopping:  3.1683315339159908\n",
            "Epoch: 009, Loss: 4.1858, Train: 0.4311, Test: 0.4279\n",
            "Early stopping:  1.7516229994792785\n",
            "Epoch: 010, Loss: 3.0142, Train: 0.4278, Test: 0.4205\n",
            "Early stopping:  1.9862957742025766\n",
            "Epoch: 011, Loss: 2.5420, Train: 0.4404, Test: 0.4347\n",
            "Early stopping:  1.9275522908190494\n",
            "Epoch: 012, Loss: 2.2557, Train: 0.4503, Test: 0.4395\n",
            "Early stopping:  1.5192916581349047\n",
            "Epoch: 013, Loss: 2.0680, Train: 0.4586, Test: 0.4458\n",
            "Early stopping:  0.8461926147078235\n",
            "Epoch: 014, Loss: 1.9644, Train: 0.4699, Test: 0.4553\n",
            "Early stopping:  0.42229670466773195\n",
            "Epoch: 015, Loss: 1.8767, Train: 0.5009, Test: 0.4842\n",
            "Early stopping:  0.26471912668840936\n",
            "Epoch: 016, Loss: 1.7898, Train: 0.5349, Test: 0.5068\n",
            "Early stopping:  0.1804477224684668\n",
            "Epoch: 017, Loss: 1.7271, Train: 0.5439, Test: 0.5295\n",
            "Early stopping:  0.13588242265534528\n",
            "Epoch: 018, Loss: 1.6869, Train: 0.5526, Test: 0.5268\n",
            "Early stopping:  0.11259912013271058\n",
            "Epoch: 019, Loss: 1.6486, Train: 0.5564, Test: 0.5284\n",
            "Early stopping:  0.08993322316465069\n",
            "Epoch: 020, Loss: 1.6106, Train: 0.5614, Test: 0.5395\n",
            "Early stopping:  0.0695412407356648\n",
            "Epoch: 021, Loss: 1.5754, Train: 0.5686, Test: 0.5542\n",
            "Early stopping:  0.06006423264325888\n",
            "Epoch: 022, Loss: 1.5380, Train: 0.5833, Test: 0.5647\n",
            "Early stopping:  0.05868258343879888\n",
            "Epoch: 023, Loss: 1.4970, Train: 0.5939, Test: 0.5747\n",
            "Early stopping:  0.05942426297755751\n",
            "Epoch: 024, Loss: 1.4557, Train: 0.6033, Test: 0.5895\n",
            "Early stopping:  0.061403831381676995\n",
            "Epoch: 025, Loss: 1.4166, Train: 0.6116, Test: 0.6011\n",
            "Early stopping:  0.06323326817179029\n",
            "Epoch: 026, Loss: 1.3823, Train: 0.6180, Test: 0.6100\n",
            "Early stopping:  0.06199762114561587\n",
            "Epoch: 027, Loss: 1.3550, Train: 0.6214, Test: 0.6121\n",
            "Early stopping:  0.056701440550763635\n",
            "Epoch: 028, Loss: 1.3342, Train: 0.6292, Test: 0.6089\n",
            "Early stopping:  0.04849911612679764\n",
            "Epoch: 029, Loss: 1.3164, Train: 0.6337, Test: 0.6100\n",
            "Early stopping:  0.03964305872645341\n",
            "Epoch: 030, Loss: 1.2981, Train: 0.6359, Test: 0.6147\n",
            "Early stopping:  0.032863229243603215\n",
            "Epoch: 031, Loss: 1.2782, Train: 0.6401, Test: 0.6158\n",
            "Early stopping:  0.03000269292357295\n",
            "Epoch: 032, Loss: 1.2574, Train: 0.6496, Test: 0.6258\n",
            "Early stopping:  0.03034204362125147\n",
            "Epoch: 033, Loss: 1.2373, Train: 0.6537, Test: 0.6279\n",
            "Early stopping:  0.03144805819997895\n",
            "Epoch: 034, Loss: 1.2187, Train: 0.6584, Test: 0.6321\n",
            "Early stopping:  0.03159616944898387\n",
            "Epoch: 035, Loss: 1.2018, Train: 0.6622, Test: 0.6358\n",
            "Early stopping:  0.030329828411161684\n",
            "Epoch: 036, Loss: 1.1870, Train: 0.6674, Test: 0.6300\n",
            "Early stopping:  0.02792783848128922\n",
            "Epoch: 037, Loss: 1.1741, Train: 0.6704, Test: 0.6326\n",
            "Early stopping:  0.025059060120320675\n",
            "Epoch: 038, Loss: 1.1608, Train: 0.6758, Test: 0.6332\n",
            "Early stopping:  0.022716896804065613\n",
            "Epoch: 039, Loss: 1.1464, Train: 0.6787, Test: 0.6379\n",
            "Early stopping:  0.021646668853413664\n",
            "Epoch: 040, Loss: 1.1321, Train: 0.6838, Test: 0.6405\n",
            "Early stopping:  0.021732523542412438\n",
            "Epoch: 041, Loss: 1.1178, Train: 0.6867, Test: 0.6416\n",
            "Early stopping:  0.02232559273282254\n",
            "Epoch: 042, Loss: 1.1036, Train: 0.6904, Test: 0.6432\n",
            "Early stopping:  0.02262682021187835\n",
            "Epoch: 043, Loss: 1.0899, Train: 0.6924, Test: 0.6432\n",
            "Early stopping:  0.022395966253246725\n",
            "Epoch: 044, Loss: 1.0773, Train: 0.6941, Test: 0.6416\n",
            "Early stopping:  0.021756791866345337\n",
            "Epoch: 045, Loss: 1.0651, Train: 0.6967, Test: 0.6437\n",
            "Early stopping:  0.020820689162498247\n",
            "Epoch: 046, Loss: 1.0527, Train: 0.6983, Test: 0.6474\n",
            "Early stopping:  0.01999775160208762\n",
            "Epoch: 047, Loss: 1.0408, Train: 0.7012, Test: 0.6463\n",
            "Early stopping:  0.019419750260190324\n",
            "Epoch: 048, Loss: 1.0293, Train: 0.7032, Test: 0.6484\n",
            "Early stopping:  0.019017472772138767\n",
            "Epoch: 049, Loss: 1.0179, Train: 0.7084, Test: 0.6516\n",
            "Early stopping:  0.018643950179140546\n",
            "Epoch: 050, Loss: 1.0065, Train: 0.7097, Test: 0.6558\n",
            "Early stopping:  0.018246790827081628\n",
            "Epoch: 051, Loss: 0.9959, Train: 0.7120, Test: 0.6568\n",
            "Early stopping:  0.017798323292834574\n",
            "Epoch: 052, Loss: 0.9859, Train: 0.7143, Test: 0.6589\n",
            "Early stopping:  0.017205566307307754\n",
            "Epoch: 053, Loss: 0.9760, Train: 0.7171, Test: 0.6611\n",
            "Early stopping:  0.016510919441392047\n",
            "Epoch: 054, Loss: 0.9661, Train: 0.7193, Test: 0.6600\n",
            "Early stopping:  0.01593206510870833\n",
            "Epoch: 055, Loss: 0.9563, Train: 0.7221, Test: 0.6595\n",
            "Early stopping:  0.015653320937589758\n",
            "Epoch: 056, Loss: 0.9468, Train: 0.7267, Test: 0.6600\n",
            "Early stopping:  0.015476162162859361\n",
            "Epoch: 057, Loss: 0.9379, Train: 0.7291, Test: 0.6616\n",
            "Early stopping:  0.015077639463445085\n",
            "Epoch: 058, Loss: 0.9295, Train: 0.7305, Test: 0.6616\n",
            "Early stopping:  0.014491592399714003\n",
            "Epoch: 059, Loss: 0.9207, Train: 0.7328, Test: 0.6642\n",
            "Early stopping:  0.01402319694085973\n",
            "Epoch: 060, Loss: 0.9121, Train: 0.7355, Test: 0.6621\n",
            "Early stopping:  0.013688140131856439\n",
            "Epoch: 061, Loss: 0.9040, Train: 0.7368, Test: 0.6626\n",
            "Early stopping:  0.01347236007743364\n",
            "Epoch: 062, Loss: 0.8959, Train: 0.7374, Test: 0.6663\n",
            "Early stopping:  0.013265507834003638\n",
            "Epoch: 063, Loss: 0.8879, Train: 0.7421, Test: 0.6679\n",
            "Early stopping:  0.012944230040937208\n",
            "Epoch: 064, Loss: 0.8799, Train: 0.7451, Test: 0.6689\n",
            "Early stopping:  0.012733195386370747\n",
            "Epoch: 065, Loss: 0.8722, Train: 0.7461, Test: 0.6684\n",
            "Early stopping:  0.012584843818087893\n",
            "Epoch: 066, Loss: 0.8645, Train: 0.7479, Test: 0.6705\n",
            "Early stopping:  0.012398600930653864\n",
            "Epoch: 067, Loss: 0.8570, Train: 0.7513, Test: 0.6732\n",
            "Early stopping:  0.012196060161779645\n",
            "Epoch: 068, Loss: 0.8499, Train: 0.7532, Test: 0.6732\n",
            "Early stopping:  0.011891889343966516\n",
            "Epoch: 069, Loss: 0.8427, Train: 0.7545, Test: 0.6747\n",
            "Early stopping:  0.011629005898457575\n",
            "Epoch: 070, Loss: 0.8356, Train: 0.7557, Test: 0.6753\n",
            "Early stopping:  0.011407082857055833\n",
            "Epoch: 071, Loss: 0.8286, Train: 0.7579, Test: 0.6758\n",
            "Early stopping:  0.01124045518532399\n",
            "Epoch: 072, Loss: 0.8218, Train: 0.7587, Test: 0.6747\n",
            "Early stopping:  0.011122550731838073\n",
            "Epoch: 073, Loss: 0.8150, Train: 0.7618, Test: 0.6721\n",
            "Early stopping:  0.01095696367707344\n",
            "Epoch: 074, Loss: 0.8083, Train: 0.7638, Test: 0.6732\n",
            "Early stopping:  0.01079849979315659\n",
            "Epoch: 075, Loss: 0.8018, Train: 0.7649, Test: 0.6732\n",
            "Early stopping:  0.010609958542568161\n",
            "Epoch: 076, Loss: 0.7957, Train: 0.7650, Test: 0.6732\n",
            "Early stopping:  0.010330495381316791\n",
            "Epoch: 077, Loss: 0.7897, Train: 0.7668, Test: 0.6779\n",
            "Early stopping:  0.00998943311270413\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.77      0.78       106\n",
            "            CAD_and_CAM       0.66      0.76      0.70        94\n",
            "              Companies       0.47      0.47      0.47       112\n",
            "       Computer_Science       0.70      0.69      0.70       106\n",
            "            Consultants       0.54      0.53      0.54       103\n",
            "           Data_Formats       0.79      0.79      0.79        87\n",
            "    Data_Communications       0.65      0.69      0.67        91\n",
            "              Education       0.90      0.87      0.88       100\n",
            "               Graphics       0.84      0.96      0.89        91\n",
            "               Hardware       0.61      0.62      0.62       111\n",
            "               Internet       0.70      0.63      0.66        99\n",
            "       Mobile_Computing       0.88      0.75      0.81       109\n",
            "             Multimedia       0.65      0.72      0.68        89\n",
            "            Open_Source       0.57      0.71      0.63        98\n",
            "            Programming       0.53      0.52      0.53       111\n",
            "               Robotics       0.87      0.90      0.89       103\n",
            "               Security       0.76      0.79      0.78        86\n",
            "               Software       0.43      0.25      0.32       105\n",
            "                Systems       0.55      0.57      0.56        99\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.67      0.68      0.67      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.3091, Train: 0.0517, Test: 0.0563\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 22.4621, Train: 0.1799, Test: 0.1811\n",
            "Early stopping:  11.421914969152477\n",
            "Epoch: 003, Loss: 12.4523, Train: 0.2432, Test: 0.2337\n",
            "Early stopping:  8.153280352260717\n",
            "Epoch: 004, Loss: 10.8130, Train: 0.2154, Test: 0.2084\n",
            "Early stopping:  6.816224367146422\n",
            "Epoch: 005, Loss: 11.4437, Train: 0.2558, Test: 0.2574\n",
            "Early stopping:  5.9443943299769\n",
            "Epoch: 006, Loss: 9.7402, Train: 0.3025, Test: 0.3068\n",
            "Early stopping:  5.170422872373015\n",
            "Epoch: 007, Loss: 8.4065, Train: 0.3296, Test: 0.3442\n",
            "Early stopping:  1.560038617301263\n",
            "Epoch: 008, Loss: 7.2332, Train: 0.3846, Test: 0.4026\n",
            "Early stopping:  1.7240953490409074\n",
            "Epoch: 009, Loss: 5.9888, Train: 0.4199, Test: 0.4342\n",
            "Early stopping:  2.127408296861798\n",
            "Epoch: 010, Loss: 5.0124, Train: 0.4254, Test: 0.4395\n",
            "Early stopping:  1.8796876648658158\n",
            "Epoch: 011, Loss: 4.2799, Train: 0.4149, Test: 0.4363\n",
            "Early stopping:  1.6640769501880053\n",
            "Epoch: 012, Loss: 3.6885, Train: 0.4222, Test: 0.4395\n",
            "Early stopping:  1.4066064232054778\n",
            "Epoch: 013, Loss: 3.1063, Train: 0.4401, Test: 0.4437\n",
            "Early stopping:  1.1283051662948527\n",
            "Epoch: 014, Loss: 2.5972, Train: 0.4604, Test: 0.4516\n",
            "Early stopping:  0.9513756993100394\n",
            "Epoch: 015, Loss: 2.2781, Train: 0.4741, Test: 0.4774\n",
            "Early stopping:  0.8102852012474651\n",
            "Epoch: 016, Loss: 2.1267, Train: 0.4812, Test: 0.4853\n",
            "Early stopping:  0.6406876120238897\n",
            "Epoch: 017, Loss: 2.0403, Train: 0.4937, Test: 0.4879\n",
            "Early stopping:  0.43369459859683074\n",
            "Epoch: 018, Loss: 1.9607, Train: 0.5028, Test: 0.5037\n",
            "Early stopping:  0.2510255492660473\n",
            "Epoch: 019, Loss: 1.8858, Train: 0.5158, Test: 0.5153\n",
            "Early stopping:  0.15217684835332046\n",
            "Epoch: 020, Loss: 1.8349, Train: 0.5307, Test: 0.5237\n",
            "Early stopping:  0.1171603370645654\n",
            "Epoch: 021, Loss: 1.7996, Train: 0.5409, Test: 0.5342\n",
            "Early stopping:  0.09719622240078943\n",
            "Epoch: 022, Loss: 1.7695, Train: 0.5511, Test: 0.5479\n",
            "Early stopping:  0.07547425440067734\n",
            "Epoch: 023, Loss: 1.7360, Train: 0.5574, Test: 0.5542\n",
            "Early stopping:  0.058028451104213355\n",
            "Epoch: 024, Loss: 1.6963, Train: 0.5676, Test: 0.5605\n",
            "Early stopping:  0.0539442853388239\n",
            "Epoch: 025, Loss: 1.6518, Train: 0.5809, Test: 0.5753\n",
            "Early stopping:  0.058491065212123314\n",
            "Epoch: 026, Loss: 1.6045, Train: 0.5918, Test: 0.5847\n",
            "Early stopping:  0.06563639449157797\n",
            "Epoch: 027, Loss: 1.5566, Train: 0.6011, Test: 0.5826\n",
            "Early stopping:  0.07132413001988559\n",
            "Epoch: 028, Loss: 1.5120, Train: 0.6037, Test: 0.5842\n",
            "Early stopping:  0.07335727849093426\n",
            "Epoch: 029, Loss: 1.4757, Train: 0.5986, Test: 0.5826\n",
            "Early stopping:  0.07038837847622025\n",
            "Epoch: 030, Loss: 1.4503, Train: 0.5979, Test: 0.5884\n",
            "Early stopping:  0.06195990910218164\n",
            "Epoch: 031, Loss: 1.4302, Train: 0.6011, Test: 0.5947\n",
            "Early stopping:  0.050345787318373025\n",
            "Epoch: 032, Loss: 1.4061, Train: 0.6103, Test: 0.6079\n",
            "Early stopping:  0.04093823414093384\n",
            "Epoch: 033, Loss: 1.3768, Train: 0.6212, Test: 0.6168\n",
            "Early stopping:  0.03835482495670844\n",
            "Epoch: 034, Loss: 1.3476, Train: 0.6305, Test: 0.6295\n",
            "Early stopping:  0.041063460651510106\n",
            "Epoch: 035, Loss: 1.3225, Train: 0.6380, Test: 0.6326\n",
            "Early stopping:  0.043367482558952876\n",
            "Epoch: 036, Loss: 1.3028, Train: 0.6426, Test: 0.6337\n",
            "Early stopping:  0.04137741888431322\n",
            "Epoch: 037, Loss: 1.2868, Train: 0.6461, Test: 0.6405\n",
            "Early stopping:  0.035775994281702896\n",
            "Epoch: 038, Loss: 1.2717, Train: 0.6501, Test: 0.6432\n",
            "Early stopping:  0.029800498891666943\n",
            "Epoch: 039, Loss: 1.2550, Train: 0.6532, Test: 0.6468\n",
            "Early stopping:  0.026267952502998463\n",
            "Epoch: 040, Loss: 1.2363, Train: 0.6563, Test: 0.6453\n",
            "Early stopping:  0.02608713494189914\n",
            "Epoch: 041, Loss: 1.2170, Train: 0.6570, Test: 0.6547\n",
            "Early stopping:  0.027717843827314986\n",
            "Epoch: 042, Loss: 1.1992, Train: 0.6616, Test: 0.6553\n",
            "Early stopping:  0.028962837168965305\n",
            "Epoch: 043, Loss: 1.1832, Train: 0.6661, Test: 0.6579\n",
            "Early stopping:  0.028595676022632764\n",
            "Epoch: 044, Loss: 1.1682, Train: 0.6717, Test: 0.6600\n",
            "Early stopping:  0.026903456646664884\n",
            "Epoch: 045, Loss: 1.1533, Train: 0.6762, Test: 0.6600\n",
            "Early stopping:  0.02503889973116441\n",
            "Epoch: 046, Loss: 1.1393, Train: 0.6791, Test: 0.6632\n",
            "Early stopping:  0.02365250229875618\n",
            "Epoch: 047, Loss: 1.1269, Train: 0.6832, Test: 0.6611\n",
            "Early stopping:  0.02238409074576175\n",
            "Epoch: 048, Loss: 1.1154, Train: 0.6854, Test: 0.6658\n",
            "Early stopping:  0.02090767914266828\n",
            "Epoch: 049, Loss: 1.1044, Train: 0.6888, Test: 0.6658\n",
            "Early stopping:  0.0192877497671906\n",
            "Epoch: 050, Loss: 1.0943, Train: 0.6903, Test: 0.6679\n",
            "Early stopping:  0.017823728490809248\n",
            "Epoch: 051, Loss: 1.0844, Train: 0.6918, Test: 0.6737\n",
            "Early stopping:  0.016791666218399535\n",
            "Epoch: 052, Loss: 1.0743, Train: 0.6922, Test: 0.6732\n",
            "Early stopping:  0.01616670091311962\n",
            "Epoch: 053, Loss: 1.0638, Train: 0.6950, Test: 0.6711\n",
            "Early stopping:  0.015991104349275607\n",
            "Epoch: 054, Loss: 1.0533, Train: 0.6963, Test: 0.6763\n",
            "Early stopping:  0.01620022076048947\n",
            "Epoch: 055, Loss: 1.0427, Train: 0.6983, Test: 0.6753\n",
            "Early stopping:  0.016503568118345874\n",
            "Epoch: 056, Loss: 1.0319, Train: 0.7020, Test: 0.6747\n",
            "Early stopping:  0.016735460712003613\n",
            "Epoch: 057, Loss: 1.0218, Train: 0.7034, Test: 0.6768\n",
            "Early stopping:  0.016684568548467768\n",
            "Epoch: 058, Loss: 1.0128, Train: 0.7066, Test: 0.6816\n",
            "Early stopping:  0.016138494604910916\n",
            "Epoch: 059, Loss: 1.0043, Train: 0.7078, Test: 0.6858\n",
            "Early stopping:  0.015180986684239294\n",
            "Epoch: 060, Loss: 0.9959, Train: 0.7099, Test: 0.6863\n",
            "Early stopping:  0.014177647928772345\n",
            "Epoch: 061, Loss: 0.9873, Train: 0.7116, Test: 0.6842\n",
            "Early stopping:  0.013590810204719353\n",
            "Epoch: 062, Loss: 0.9787, Train: 0.7133, Test: 0.6863\n",
            "Early stopping:  0.013454096911008141\n",
            "Epoch: 063, Loss: 0.9707, Train: 0.7149, Test: 0.6874\n",
            "Early stopping:  0.013352639962944505\n",
            "Epoch: 064, Loss: 0.9629, Train: 0.7184, Test: 0.6884\n",
            "Early stopping:  0.013041731409706457\n",
            "Epoch: 065, Loss: 0.9554, Train: 0.7205, Test: 0.6884\n",
            "Early stopping:  0.012596117103928488\n",
            "Epoch: 066, Loss: 0.9479, Train: 0.7239, Test: 0.6895\n",
            "Early stopping:  0.012172838869371745\n",
            "Epoch: 067, Loss: 0.9406, Train: 0.7266, Test: 0.6900\n",
            "Early stopping:  0.011886089207622512\n",
            "Epoch: 068, Loss: 0.9332, Train: 0.7280, Test: 0.6916\n",
            "Early stopping:  0.011727688572014313\n",
            "Epoch: 069, Loss: 0.9261, Train: 0.7305, Test: 0.6921\n",
            "Early stopping:  0.01157271485096393\n",
            "Epoch: 070, Loss: 0.9193, Train: 0.7318, Test: 0.6916\n",
            "Early stopping:  0.011326235870178272\n",
            "Epoch: 071, Loss: 0.9127, Train: 0.7342, Test: 0.6942\n",
            "Early stopping:  0.011018134803932727\n",
            "Epoch: 072, Loss: 0.9061, Train: 0.7363, Test: 0.6974\n",
            "Early stopping:  0.010695030229004842\n",
            "Epoch: 073, Loss: 0.8993, Train: 0.7372, Test: 0.6968\n",
            "Early stopping:  0.010549306974293517\n",
            "Epoch: 074, Loss: 0.8924, Train: 0.7368, Test: 0.6947\n",
            "Early stopping:  0.010617758546224876\n",
            "Epoch: 075, Loss: 0.8858, Train: 0.7387, Test: 0.6963\n",
            "Early stopping:  0.010684625393067202\n",
            "Epoch: 076, Loss: 0.8793, Train: 0.7400, Test: 0.6968\n",
            "Early stopping:  0.010632552015208522\n",
            "Epoch: 077, Loss: 0.8729, Train: 0.7422, Test: 0.7011\n",
            "Early stopping:  0.010436205096244846\n",
            "Epoch: 078, Loss: 0.8666, Train: 0.7458, Test: 0.7037\n",
            "Early stopping:  0.010185886634733972\n",
            "Epoch: 079, Loss: 0.8605, Train: 0.7492, Test: 0.7074\n",
            "Early stopping:  0.01000047671009691\n",
            "Epoch: 080, Loss: 0.8544, Train: 0.7518, Test: 0.7063\n",
            "Early stopping:  0.009852538579288316\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.85      0.81       107\n",
            "            CAD_and_CAM       0.71      0.82      0.76       106\n",
            "              Companies       0.44      0.53      0.48        88\n",
            "       Computer_Science       0.72      0.70      0.71        91\n",
            "            Consultants       0.56      0.66      0.61        89\n",
            "           Data_Formats       0.78      0.82      0.80        98\n",
            "    Data_Communications       0.78      0.85      0.81       110\n",
            "              Education       0.86      0.89      0.88        99\n",
            "               Graphics       0.85      0.90      0.87       108\n",
            "               Hardware       0.72      0.57      0.64       101\n",
            "               Internet       0.66      0.63      0.64       100\n",
            "       Mobile_Computing       0.89      0.79      0.84       101\n",
            "             Multimedia       0.72      0.73      0.72       107\n",
            "            Open_Source       0.67      0.69      0.68        97\n",
            "            Programming       0.57      0.52      0.54       123\n",
            "               Robotics       0.94      0.90      0.92        89\n",
            "               Security       0.86      0.75      0.80        88\n",
            "               Software       0.33      0.23      0.27        96\n",
            "                Systems       0.56      0.57      0.57       102\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.70      0.71      0.70      1900\n",
            "           weighted avg       0.70      0.71      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 7.7494, Train: 0.1388, Test: 0.1353\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 11.9659, Train: 0.1195, Test: 0.1095\n",
            "Early stopping:  2.9814626688640344\n",
            "Epoch: 003, Loss: 17.9357, Train: 0.2309, Test: 0.2189\n",
            "Early stopping:  5.118225748661317\n",
            "Epoch: 004, Loss: 17.1732, Train: 0.2597, Test: 0.2558\n",
            "Early stopping:  4.7756649067412935\n",
            "Epoch: 005, Loss: 11.3107, Train: 0.3236, Test: 0.3153\n",
            "Early stopping:  4.272326593571155\n",
            "Epoch: 006, Loss: 8.6427, Train: 0.3163, Test: 0.3179\n",
            "Early stopping:  3.99572653681996\n",
            "Epoch: 007, Loss: 7.2417, Train: 0.3538, Test: 0.3368\n",
            "Early stopping:  4.88161997871805\n",
            "Epoch: 008, Loss: 5.0120, Train: 0.3992, Test: 0.3858\n",
            "Early stopping:  4.675106756754058\n",
            "Epoch: 009, Loss: 3.6361, Train: 0.4183, Test: 0.4132\n",
            "Early stopping:  3.0192964159425983\n",
            "Epoch: 010, Loss: 2.9772, Train: 0.4354, Test: 0.4268\n",
            "Early stopping:  2.3972372077637742\n",
            "Epoch: 011, Loss: 2.5224, Train: 0.4432, Test: 0.4326\n",
            "Early stopping:  1.9045150243414872\n",
            "Epoch: 012, Loss: 2.2515, Train: 0.4697, Test: 0.4626\n",
            "Early stopping:  1.1011634730988287\n",
            "Epoch: 013, Loss: 2.1086, Train: 0.4821, Test: 0.4768\n",
            "Early stopping:  0.6195798867792538\n",
            "Epoch: 014, Loss: 2.0177, Train: 0.4682, Test: 0.4584\n",
            "Early stopping:  0.38673317478832364\n",
            "Epoch: 015, Loss: 1.9698, Train: 0.4709, Test: 0.4526\n",
            "Early stopping:  0.22236023701874827\n",
            "Epoch: 016, Loss: 1.9369, Train: 0.4891, Test: 0.4732\n",
            "Early stopping:  0.12656884703907895\n",
            "Epoch: 017, Loss: 1.8861, Train: 0.5096, Test: 0.4942\n",
            "Early stopping:  0.08467227534272968\n",
            "Epoch: 018, Loss: 1.8334, Train: 0.5153, Test: 0.5053\n",
            "Early stopping:  0.07169659720835996\n",
            "Epoch: 019, Loss: 1.7963, Train: 0.5083, Test: 0.5084\n",
            "Early stopping:  0.07142857745395266\n",
            "Epoch: 020, Loss: 1.7685, Train: 0.5142, Test: 0.5147\n",
            "Early stopping:  0.06796268063365887\n",
            "Epoch: 021, Loss: 1.7373, Train: 0.5286, Test: 0.5305\n",
            "Early stopping:  0.05781757501234689\n",
            "Epoch: 022, Loss: 1.6972, Train: 0.5420, Test: 0.5479\n",
            "Early stopping:  0.052508442750495606\n",
            "Epoch: 023, Loss: 1.6505, Train: 0.5554, Test: 0.5605\n",
            "Early stopping:  0.0577228894594429\n",
            "Epoch: 024, Loss: 1.6038, Train: 0.5661, Test: 0.5642\n",
            "Early stopping:  0.06601422193280596\n",
            "Epoch: 025, Loss: 1.5613, Train: 0.5692, Test: 0.5711\n",
            "Early stopping:  0.07046013646378897\n",
            "Epoch: 026, Loss: 1.5241, Train: 0.5763, Test: 0.5732\n",
            "Early stopping:  0.06893329395581009\n",
            "Epoch: 027, Loss: 1.4898, Train: 0.5883, Test: 0.5821\n",
            "Early stopping:  0.06356120596604911\n",
            "Epoch: 028, Loss: 1.4555, Train: 0.5958, Test: 0.5879\n",
            "Early stopping:  0.05826516365088492\n",
            "Epoch: 029, Loss: 1.4202, Train: 0.6042, Test: 0.5947\n",
            "Early stopping:  0.05544766959684334\n",
            "Epoch: 030, Loss: 1.3855, Train: 0.6171, Test: 0.6111\n",
            "Early stopping:  0.05484365358797087\n",
            "Epoch: 031, Loss: 1.3542, Train: 0.6297, Test: 0.6137\n",
            "Early stopping:  0.05396057906871852\n",
            "Epoch: 032, Loss: 1.3277, Train: 0.6355, Test: 0.6258\n",
            "Early stopping:  0.05093848995031329\n",
            "Epoch: 033, Loss: 1.3073, Train: 0.6382, Test: 0.6316\n",
            "Early stopping:  0.04506571442452865\n",
            "Epoch: 034, Loss: 1.2914, Train: 0.6396, Test: 0.6279\n",
            "Early stopping:  0.03747152033949998\n",
            "Epoch: 035, Loss: 1.2762, Train: 0.6428, Test: 0.6326\n",
            "Early stopping:  0.030639189036399946\n",
            "Epoch: 036, Loss: 1.2593, Train: 0.6466, Test: 0.6305\n",
            "Early stopping:  0.02658666739114848\n",
            "Epoch: 037, Loss: 1.2412, Train: 0.6517, Test: 0.6342\n",
            "Early stopping:  0.025993929927940165\n",
            "Epoch: 038, Loss: 1.2232, Train: 0.6542, Test: 0.6389\n",
            "Early stopping:  0.027101452474503403\n",
            "Epoch: 039, Loss: 1.2062, Train: 0.6601, Test: 0.6453\n",
            "Early stopping:  0.027846810499159772\n",
            "Epoch: 040, Loss: 1.1902, Train: 0.6647, Test: 0.6468\n",
            "Early stopping:  0.027404960183275397\n",
            "Epoch: 041, Loss: 1.1756, Train: 0.6684, Test: 0.6489\n",
            "Early stopping:  0.02599499319302067\n",
            "Epoch: 042, Loss: 1.1627, Train: 0.6691, Test: 0.6547\n",
            "Early stopping:  0.02401677486782636\n",
            "Epoch: 043, Loss: 1.1510, Train: 0.6746, Test: 0.6600\n",
            "Early stopping:  0.02182748997530666\n",
            "Epoch: 044, Loss: 1.1392, Train: 0.6772, Test: 0.6679\n",
            "Early stopping:  0.020032006148255597\n",
            "Epoch: 045, Loss: 1.1263, Train: 0.6818, Test: 0.6668\n",
            "Early stopping:  0.019290956546721652\n",
            "Epoch: 046, Loss: 1.1125, Train: 0.6866, Test: 0.6668\n",
            "Early stopping:  0.019775151096078386\n",
            "Epoch: 047, Loss: 1.0992, Train: 0.6912, Test: 0.6705\n",
            "Early stopping:  0.020619580820930485\n",
            "Epoch: 048, Loss: 1.0875, Train: 0.6925, Test: 0.6742\n",
            "Early stopping:  0.020651123301418176\n",
            "Epoch: 049, Loss: 1.0772, Train: 0.6979, Test: 0.6737\n",
            "Early stopping:  0.019535829661121353\n",
            "Epoch: 050, Loss: 1.0670, Train: 0.7003, Test: 0.6753\n",
            "Early stopping:  0.017898370724291467\n",
            "Epoch: 051, Loss: 1.0570, Train: 0.7017, Test: 0.6753\n",
            "Early stopping:  0.016593956437116763\n",
            "Epoch: 052, Loss: 1.0469, Train: 0.7033, Test: 0.6784\n",
            "Early stopping:  0.01604663137685665\n",
            "Epoch: 053, Loss: 1.0368, Train: 0.7066, Test: 0.6842\n",
            "Early stopping:  0.015967562545205132\n",
            "Epoch: 054, Loss: 1.0271, Train: 0.7092, Test: 0.6837\n",
            "Early stopping:  0.015819020621027805\n",
            "Epoch: 055, Loss: 1.0182, Train: 0.7122, Test: 0.6826\n",
            "Early stopping:  0.0153984030457735\n",
            "Epoch: 056, Loss: 1.0098, Train: 0.7143, Test: 0.6842\n",
            "Early stopping:  0.014669694860784248\n",
            "Epoch: 057, Loss: 1.0016, Train: 0.7166, Test: 0.6863\n",
            "Early stopping:  0.013869226985270587\n",
            "Epoch: 058, Loss: 0.9936, Train: 0.7187, Test: 0.6858\n",
            "Early stopping:  0.013243409612450163\n",
            "Epoch: 059, Loss: 0.9854, Train: 0.7187, Test: 0.6826\n",
            "Early stopping:  0.012931042385740832\n",
            "Epoch: 060, Loss: 0.9770, Train: 0.7217, Test: 0.6826\n",
            "Early stopping:  0.012942470980155883\n",
            "Epoch: 061, Loss: 0.9685, Train: 0.7233, Test: 0.6805\n",
            "Early stopping:  0.013094352981402362\n",
            "Epoch: 062, Loss: 0.9605, Train: 0.7253, Test: 0.6805\n",
            "Early stopping:  0.013114191215946838\n",
            "Epoch: 063, Loss: 0.9531, Train: 0.7280, Test: 0.6826\n",
            "Early stopping:  0.012835462946233679\n",
            "Epoch: 064, Loss: 0.9456, Train: 0.7311, Test: 0.6847\n",
            "Early stopping:  0.01236689325653176\n",
            "Epoch: 065, Loss: 0.9381, Train: 0.7338, Test: 0.6847\n",
            "Early stopping:  0.011967052172151759\n",
            "Epoch: 066, Loss: 0.9309, Train: 0.7353, Test: 0.6868\n",
            "Early stopping:  0.011747501308288924\n",
            "Epoch: 067, Loss: 0.9239, Train: 0.7353, Test: 0.6858\n",
            "Early stopping:  0.011539714019349522\n",
            "Epoch: 068, Loss: 0.9174, Train: 0.7355, Test: 0.6853\n",
            "Early stopping:  0.011169257267787426\n",
            "Epoch: 069, Loss: 0.9110, Train: 0.7378, Test: 0.6895\n",
            "Early stopping:  0.010704225688949983\n",
            "Epoch: 070, Loss: 0.9047, Train: 0.7384, Test: 0.6884\n",
            "Early stopping:  0.010307190111575184\n",
            "Epoch: 071, Loss: 0.8985, Train: 0.7411, Test: 0.6900\n",
            "Early stopping:  0.010039867710102\n",
            "Epoch: 072, Loss: 0.8918, Train: 0.7433, Test: 0.6905\n",
            "Early stopping:  0.010053793810764728\n",
            "Epoch: 073, Loss: 0.8856, Train: 0.7447, Test: 0.6916\n",
            "Early stopping:  0.010065779023615043\n",
            "Epoch: 074, Loss: 0.8800, Train: 0.7461, Test: 0.6932\n",
            "Early stopping:  0.009858528348148222\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.78      0.78        91\n",
            "            CAD_and_CAM       0.71      0.79      0.75       106\n",
            "              Companies       0.50      0.50      0.50       105\n",
            "       Computer_Science       0.75      0.70      0.72       101\n",
            "            Consultants       0.67      0.62      0.65       112\n",
            "           Data_Formats       0.83      0.82      0.83        96\n",
            "    Data_Communications       0.67      0.81      0.73        86\n",
            "              Education       0.87      0.88      0.87       124\n",
            "               Graphics       0.85      0.88      0.86        97\n",
            "               Hardware       0.67      0.70      0.69        89\n",
            "               Internet       0.64      0.64      0.64       103\n",
            "       Mobile_Computing       0.76      0.72      0.74        93\n",
            "             Multimedia       0.66      0.65      0.66       100\n",
            "            Open_Source       0.69      0.65      0.67       100\n",
            "            Programming       0.48      0.55      0.51        89\n",
            "               Robotics       0.92      0.86      0.89       100\n",
            "               Security       0.75      0.79      0.77       104\n",
            "               Software       0.31      0.28      0.30        95\n",
            "                Systems       0.59      0.52      0.56       109\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.69      0.69      0.69      1900\n",
            "           weighted avg       0.69      0.69      0.69      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 8.0573, Train: 0.0821, Test: 0.0695\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 13.9940, Train: 0.1555, Test: 0.1500\n",
            "Early stopping:  4.197887477496478\n",
            "Epoch: 003, Loss: 11.9797, Train: 0.2092, Test: 0.2068\n",
            "Early stopping:  3.0190350450761074\n",
            "Epoch: 004, Loss: 11.3626, Train: 0.2591, Test: 0.2574\n",
            "Early stopping:  2.465050043615335\n",
            "Epoch: 005, Loss: 10.5562, Train: 0.3132, Test: 0.3063\n",
            "Early stopping:  2.1639901487116564\n",
            "Epoch: 006, Loss: 10.3435, Train: 0.3367, Test: 0.3374\n",
            "Early stopping:  1.4653319791386685\n",
            "Epoch: 007, Loss: 9.3042, Train: 0.3451, Test: 0.3563\n",
            "Early stopping:  1.0213347284839374\n",
            "Epoch: 008, Loss: 7.8471, Train: 0.3482, Test: 0.3505\n",
            "Early stopping:  1.3541053099038363\n",
            "Epoch: 009, Loss: 6.4741, Train: 0.3838, Test: 0.3916\n",
            "Early stopping:  1.7308417196845474\n",
            "Epoch: 010, Loss: 4.7220, Train: 0.4207, Test: 0.4211\n",
            "Early stopping:  2.233077039872778\n",
            "Epoch: 011, Loss: 3.5813, Train: 0.4516, Test: 0.4332\n",
            "Early stopping:  2.307224925380533\n",
            "Epoch: 012, Loss: 2.7975, Train: 0.4622, Test: 0.4547\n",
            "Early stopping:  2.0727003933175454\n",
            "Epoch: 013, Loss: 2.2960, Train: 0.4811, Test: 0.4753\n",
            "Early stopping:  1.670646295094943\n",
            "Epoch: 014, Loss: 2.0485, Train: 0.4859, Test: 0.4784\n",
            "Early stopping:  1.0846274460614025\n",
            "Epoch: 015, Loss: 1.9568, Train: 0.4828, Test: 0.4779\n",
            "Early stopping:  0.6693133070733446\n",
            "Epoch: 016, Loss: 1.9338, Train: 0.4745, Test: 0.4653\n",
            "Early stopping:  0.360221242586436\n",
            "Epoch: 017, Loss: 1.9257, Train: 0.4697, Test: 0.4674\n",
            "Early stopping:  0.1553881963387043\n",
            "Epoch: 018, Loss: 1.9203, Train: 0.4670, Test: 0.4632\n",
            "Early stopping:  0.05299849849556903\n",
            "Epoch: 019, Loss: 1.9138, Train: 0.4661, Test: 0.4700\n",
            "Early stopping:  0.01661234644282628\n",
            "Epoch: 020, Loss: 1.9011, Train: 0.4691, Test: 0.4611\n",
            "Early stopping:  0.012357750250969215\n",
            "Epoch: 021, Loss: 1.8815, Train: 0.4762, Test: 0.4647\n",
            "Early stopping:  0.01765629913222398\n",
            "Epoch: 022, Loss: 1.8536, Train: 0.4845, Test: 0.4747\n",
            "Early stopping:  0.027052634388988465\n",
            "Epoch: 023, Loss: 1.8176, Train: 0.4942, Test: 0.4863\n",
            "Early stopping:  0.038644346231603725\n",
            "Epoch: 024, Loss: 1.7749, Train: 0.5063, Test: 0.4942\n",
            "Early stopping:  0.05053165684514199\n",
            "Epoch: 025, Loss: 1.7282, Train: 0.5167, Test: 0.5053\n",
            "Early stopping:  0.06120880345838588\n",
            "Epoch: 026, Loss: 1.6785, Train: 0.5328, Test: 0.5200\n",
            "Early stopping:  0.06962423282611604\n",
            "Epoch: 027, Loss: 1.6268, Train: 0.5503, Test: 0.5384\n",
            "Early stopping:  0.0756353044005876\n",
            "Epoch: 028, Loss: 1.5765, Train: 0.5647, Test: 0.5495\n",
            "Early stopping:  0.07878138328119301\n",
            "Epoch: 029, Loss: 1.5320, Train: 0.5728, Test: 0.5521\n",
            "Early stopping:  0.07818813422235592\n",
            "Epoch: 030, Loss: 1.4959, Train: 0.5791, Test: 0.5621\n",
            "Early stopping:  0.07290570015328238\n",
            "Epoch: 031, Loss: 1.4673, Train: 0.5891, Test: 0.5674\n",
            "Early stopping:  0.06357728607442284\n",
            "Epoch: 032, Loss: 1.4425, Train: 0.5970, Test: 0.5737\n",
            "Early stopping:  0.05300967146776455\n",
            "Epoch: 033, Loss: 1.4174, Train: 0.6024, Test: 0.5884\n",
            "Early stopping:  0.04483949907754147\n",
            "Epoch: 034, Loss: 1.3920, Train: 0.6118, Test: 0.5958\n",
            "Early stopping:  0.04076667052537771\n",
            "Epoch: 035, Loss: 1.3694, Train: 0.6167, Test: 0.5995\n",
            "Early stopping:  0.03894298064916616\n",
            "Epoch: 036, Loss: 1.3505, Train: 0.6257, Test: 0.6026\n",
            "Early stopping:  0.036755290036591115\n",
            "Epoch: 037, Loss: 1.3328, Train: 0.6311, Test: 0.6121\n",
            "Early stopping:  0.033435589696682366\n",
            "Epoch: 038, Loss: 1.3146, Train: 0.6379, Test: 0.6247\n",
            "Early stopping:  0.030283227464988578\n",
            "Epoch: 039, Loss: 1.2969, Train: 0.6430, Test: 0.6211\n",
            "Early stopping:  0.02859404027829825\n",
            "Epoch: 040, Loss: 1.2804, Train: 0.6497, Test: 0.6274\n",
            "Early stopping:  0.027827687708588074\n",
            "Epoch: 041, Loss: 1.2651, Train: 0.6539, Test: 0.6279\n",
            "Early stopping:  0.02683742491079529\n",
            "Epoch: 042, Loss: 1.2499, Train: 0.6566, Test: 0.6342\n",
            "Early stopping:  0.025508294902765127\n",
            "Epoch: 043, Loss: 1.2350, Train: 0.6592, Test: 0.6332\n",
            "Early stopping:  0.02439108607560296\n",
            "Epoch: 044, Loss: 1.2205, Train: 0.6618, Test: 0.6379\n",
            "Early stopping:  0.023697509584710084\n",
            "Epoch: 045, Loss: 1.2064, Train: 0.6654, Test: 0.6395\n",
            "Early stopping:  0.023215601393898187\n",
            "Epoch: 046, Loss: 1.1925, Train: 0.6696, Test: 0.6416\n",
            "Early stopping:  0.02268626589342992\n",
            "Epoch: 047, Loss: 1.1794, Train: 0.6738, Test: 0.6468\n",
            "Early stopping:  0.022018472988661015\n",
            "Epoch: 048, Loss: 1.1671, Train: 0.6761, Test: 0.6516\n",
            "Early stopping:  0.021171436983708804\n",
            "Epoch: 049, Loss: 1.1558, Train: 0.6809, Test: 0.6526\n",
            "Early stopping:  0.02002396791921363\n",
            "Epoch: 050, Loss: 1.1453, Train: 0.6839, Test: 0.6516\n",
            "Early stopping:  0.018670845239502687\n",
            "Epoch: 051, Loss: 1.1351, Train: 0.6859, Test: 0.6589\n",
            "Early stopping:  0.017447001618539293\n",
            "Epoch: 052, Loss: 1.1252, Train: 0.6870, Test: 0.6616\n",
            "Early stopping:  0.016531371315411665\n",
            "Epoch: 053, Loss: 1.1156, Train: 0.6887, Test: 0.6637\n",
            "Early stopping:  0.015899248750865434\n",
            "Epoch: 054, Loss: 1.1065, Train: 0.6896, Test: 0.6658\n",
            "Early stopping:  0.015364207495565933\n",
            "Epoch: 055, Loss: 1.0977, Train: 0.6909, Test: 0.6605\n",
            "Early stopping:  0.014784810679624474\n",
            "Epoch: 056, Loss: 1.0893, Train: 0.6924, Test: 0.6595\n",
            "Early stopping:  0.014176184046942252\n",
            "Epoch: 057, Loss: 1.0813, Train: 0.6943, Test: 0.6642\n",
            "Early stopping:  0.013582353735173856\n",
            "Epoch: 058, Loss: 1.0732, Train: 0.6954, Test: 0.6647\n",
            "Early stopping:  0.0131172025780034\n",
            "Epoch: 059, Loss: 1.0651, Train: 0.6997, Test: 0.6668\n",
            "Early stopping:  0.012865957614901384\n",
            "Epoch: 060, Loss: 1.0568, Train: 0.7020, Test: 0.6679\n",
            "Early stopping:  0.012835280397936301\n",
            "Epoch: 061, Loss: 1.0489, Train: 0.7036, Test: 0.6684\n",
            "Early stopping:  0.012838938340074931\n",
            "Epoch: 062, Loss: 1.0413, Train: 0.7045, Test: 0.6711\n",
            "Early stopping:  0.012673112381665177\n",
            "Epoch: 063, Loss: 1.0339, Train: 0.7084, Test: 0.6726\n",
            "Early stopping:  0.012309443402679227\n",
            "Epoch: 064, Loss: 1.0268, Train: 0.7103, Test: 0.6721\n",
            "Early stopping:  0.011872724018292515\n",
            "Epoch: 065, Loss: 1.0196, Train: 0.7109, Test: 0.6747\n",
            "Early stopping:  0.011569132784316509\n",
            "Epoch: 066, Loss: 1.0124, Train: 0.7130, Test: 0.6732\n",
            "Early stopping:  0.011386706277942001\n",
            "Epoch: 067, Loss: 1.0057, Train: 0.7146, Test: 0.6737\n",
            "Early stopping:  0.011201743368103739\n",
            "Epoch: 068, Loss: 0.9992, Train: 0.7157, Test: 0.6800\n",
            "Early stopping:  0.010910569368662635\n",
            "Epoch: 069, Loss: 0.9930, Train: 0.7174, Test: 0.6779\n",
            "Early stopping:  0.01049725160846406\n",
            "Epoch: 070, Loss: 0.9867, Train: 0.7188, Test: 0.6763\n",
            "Early stopping:  0.010157167443304508\n",
            "Epoch: 071, Loss: 0.9803, Train: 0.7209, Test: 0.6768\n",
            "Early stopping:  0.010003355041092486\n",
            "Epoch: 072, Loss: 0.9741, Train: 0.7226, Test: 0.6753\n",
            "Early stopping:  0.009950230457029944\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.88      0.76      0.82       105\n",
            "            CAD_and_CAM       0.76      0.71      0.73        95\n",
            "              Companies       0.53      0.56      0.55        86\n",
            "       Computer_Science       0.71      0.78      0.74        87\n",
            "            Consultants       0.52      0.55      0.53        91\n",
            "           Data_Formats       0.80      0.77      0.79       105\n",
            "    Data_Communications       0.67      0.77      0.72       114\n",
            "              Education       0.88      0.87      0.87       106\n",
            "               Graphics       0.81      0.89      0.85       113\n",
            "               Hardware       0.55      0.67      0.60        90\n",
            "               Internet       0.59      0.54      0.56       109\n",
            "       Mobile_Computing       0.68      0.72      0.70        85\n",
            "             Multimedia       0.65      0.67      0.66        91\n",
            "            Open_Source       0.67      0.61      0.64        96\n",
            "            Programming       0.46      0.57      0.51        86\n",
            "               Robotics       0.88      0.89      0.88       102\n",
            "               Security       0.81      0.79      0.80       117\n",
            "               Software       0.34      0.23      0.27       108\n",
            "                Systems       0.53      0.45      0.48       114\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.67      0.67      0.67      1900\n",
            "           weighted avg       0.67      0.68      0.67      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.3568, Train: 0.0536, Test: 0.0584\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 16.5997, Train: 0.1987, Test: 0.2095\n",
            "Early stopping:  7.949913222665061\n",
            "Epoch: 003, Loss: 11.4921, Train: 0.2237, Test: 0.2305\n",
            "Early stopping:  5.629260314395797\n",
            "Epoch: 004, Loss: 9.3644, Train: 0.3022, Test: 0.2963\n",
            "Early stopping:  4.682133502201359\n",
            "Epoch: 005, Loss: 7.9344, Train: 0.2934, Test: 0.2826\n",
            "Early stopping:  4.2397071049867785\n",
            "Epoch: 006, Loss: 7.0493, Train: 0.3182, Test: 0.3000\n",
            "Early stopping:  3.8068680424936083\n",
            "Epoch: 007, Loss: 6.0617, Train: 0.3587, Test: 0.3442\n",
            "Early stopping:  2.121207460650554\n",
            "Epoch: 008, Loss: 4.8809, Train: 0.3553, Test: 0.3447\n",
            "Early stopping:  1.7190042112772126\n",
            "Epoch: 009, Loss: 4.0635, Train: 0.4054, Test: 0.3937\n",
            "Early stopping:  1.569148533842885\n",
            "Epoch: 010, Loss: 2.9936, Train: 0.4679, Test: 0.4600\n",
            "Early stopping:  1.600240356362029\n",
            "Epoch: 011, Loss: 2.2853, Train: 0.5159, Test: 0.5089\n",
            "Early stopping:  1.4972729744325126\n",
            "Epoch: 012, Loss: 1.9593, Train: 0.5175, Test: 0.5158\n",
            "Early stopping:  1.2229995632551562\n",
            "Epoch: 013, Loss: 1.8828, Train: 0.4896, Test: 0.4842\n",
            "Early stopping:  0.9101885202741495\n",
            "Epoch: 014, Loss: 1.8676, Train: 0.4853, Test: 0.4768\n",
            "Early stopping:  0.47594230536516774\n",
            "Epoch: 015, Loss: 1.8347, Train: 0.5064, Test: 0.5079\n",
            "Early stopping:  0.18428614997614887\n",
            "Epoch: 016, Loss: 1.7637, Train: 0.5261, Test: 0.5253\n",
            "Early stopping:  0.07132640462577092\n",
            "Epoch: 017, Loss: 1.7013, Train: 0.5454, Test: 0.5363\n",
            "Early stopping:  0.07612803496859068\n",
            "Epoch: 018, Loss: 1.6449, Train: 0.5686, Test: 0.5595\n",
            "Early stopping:  0.09197852666586594\n",
            "Epoch: 019, Loss: 1.5890, Train: 0.5837, Test: 0.5721\n",
            "Early stopping:  0.09660773502258428\n",
            "Epoch: 020, Loss: 1.5413, Train: 0.5930, Test: 0.5784\n",
            "Early stopping:  0.08818407566271456\n",
            "Epoch: 021, Loss: 1.5019, Train: 0.6004, Test: 0.5947\n",
            "Early stopping:  0.0796468219539192\n",
            "Epoch: 022, Loss: 1.4632, Train: 0.6105, Test: 0.6011\n",
            "Early stopping:  0.07146866292036326\n",
            "Epoch: 023, Loss: 1.4276, Train: 0.6205, Test: 0.6074\n",
            "Early stopping:  0.06349706616797259\n",
            "Epoch: 024, Loss: 1.3969, Train: 0.6261, Test: 0.6121\n",
            "Early stopping:  0.05747530204877146\n",
            "Epoch: 025, Loss: 1.3660, Train: 0.6332, Test: 0.6184\n",
            "Early stopping:  0.053520361311254226\n",
            "Epoch: 026, Loss: 1.3336, Train: 0.6380, Test: 0.6242\n",
            "Early stopping:  0.050716633577372695\n",
            "Epoch: 027, Loss: 1.3067, Train: 0.6380, Test: 0.6258\n",
            "Early stopping:  0.048247322246969836\n",
            "Epoch: 028, Loss: 1.2869, Train: 0.6422, Test: 0.6211\n",
            "Early stopping:  0.04434204999878476\n",
            "Epoch: 029, Loss: 1.2666, Train: 0.6457, Test: 0.6247\n",
            "Early stopping:  0.0390683924637447\n",
            "Epoch: 030, Loss: 1.2417, Train: 0.6507, Test: 0.6242\n",
            "Early stopping:  0.03544870791787958\n",
            "Epoch: 031, Loss: 1.2183, Train: 0.6564, Test: 0.6237\n",
            "Early stopping:  0.035153022560697134\n",
            "Epoch: 032, Loss: 1.2004, Train: 0.6596, Test: 0.6316\n",
            "Early stopping:  0.03503177568535156\n",
            "Epoch: 033, Loss: 1.1857, Train: 0.6650, Test: 0.6321\n",
            "Early stopping:  0.032297283677037186\n",
            "Epoch: 034, Loss: 1.1710, Train: 0.6699, Test: 0.6316\n",
            "Early stopping:  0.02765863505311376\n",
            "Epoch: 035, Loss: 1.1552, Train: 0.6750, Test: 0.6400\n",
            "Early stopping:  0.0245989851802415\n",
            "Epoch: 036, Loss: 1.1393, Train: 0.6787, Test: 0.6395\n",
            "Early stopping:  0.02417177656752452\n",
            "Epoch: 037, Loss: 1.1243, Train: 0.6816, Test: 0.6447\n",
            "Early stopping:  0.024453557966355167\n",
            "Epoch: 038, Loss: 1.1112, Train: 0.6846, Test: 0.6437\n",
            "Early stopping:  0.023830113251908636\n",
            "Epoch: 039, Loss: 1.0992, Train: 0.6886, Test: 0.6484\n",
            "Early stopping:  0.02218164808032286\n",
            "Epoch: 040, Loss: 1.0873, Train: 0.6924, Test: 0.6532\n",
            "Early stopping:  0.020405823118409307\n",
            "Epoch: 041, Loss: 1.0750, Train: 0.6946, Test: 0.6516\n",
            "Early stopping:  0.019369690531899925\n",
            "Epoch: 042, Loss: 1.0623, Train: 0.6980, Test: 0.6505\n",
            "Early stopping:  0.01930440067567588\n",
            "Epoch: 043, Loss: 1.0500, Train: 0.7017, Test: 0.6553\n",
            "Early stopping:  0.019536262692795677\n",
            "Epoch: 044, Loss: 1.0380, Train: 0.7036, Test: 0.6579\n",
            "Early stopping:  0.0195442536590369\n",
            "Epoch: 045, Loss: 1.0261, Train: 0.7068, Test: 0.6611\n",
            "Early stopping:  0.01928681083031215\n",
            "Epoch: 046, Loss: 1.0149, Train: 0.7100, Test: 0.6642\n",
            "Early stopping:  0.018754569074838245\n",
            "Epoch: 047, Loss: 1.0045, Train: 0.7121, Test: 0.6658\n",
            "Early stopping:  0.01805428395609109\n",
            "Epoch: 048, Loss: 0.9944, Train: 0.7142, Test: 0.6658\n",
            "Early stopping:  0.017229917823997937\n",
            "Epoch: 049, Loss: 0.9844, Train: 0.7172, Test: 0.6653\n",
            "Early stopping:  0.016433593889651613\n",
            "Epoch: 050, Loss: 0.9746, Train: 0.7178, Test: 0.6668\n",
            "Early stopping:  0.01591991748342506\n",
            "Epoch: 051, Loss: 0.9649, Train: 0.7203, Test: 0.6711\n",
            "Early stopping:  0.01565195318573677\n",
            "Epoch: 052, Loss: 0.9554, Train: 0.7234, Test: 0.6721\n",
            "Early stopping:  0.015420242802701137\n",
            "Epoch: 053, Loss: 0.9466, Train: 0.7267, Test: 0.6700\n",
            "Early stopping:  0.015002183432801028\n",
            "Epoch: 054, Loss: 0.9381, Train: 0.7283, Test: 0.6726\n",
            "Early stopping:  0.014439188093385276\n",
            "Epoch: 055, Loss: 0.9293, Train: 0.7293, Test: 0.6742\n",
            "Early stopping:  0.013997821847357103\n",
            "Epoch: 056, Loss: 0.9204, Train: 0.7305, Test: 0.6711\n",
            "Early stopping:  0.01379657723485016\n",
            "Epoch: 057, Loss: 0.9123, Train: 0.7313, Test: 0.6679\n",
            "Early stopping:  0.013623959102101369\n",
            "Epoch: 058, Loss: 0.9043, Train: 0.7343, Test: 0.6716\n",
            "Early stopping:  0.013382185475245563\n",
            "Epoch: 059, Loss: 0.8960, Train: 0.7367, Test: 0.6742\n",
            "Early stopping:  0.013069259840747274\n",
            "Epoch: 060, Loss: 0.8880, Train: 0.7393, Test: 0.6747\n",
            "Early stopping:  0.012824125789209887\n",
            "Epoch: 061, Loss: 0.8802, Train: 0.7408, Test: 0.6779\n",
            "Early stopping:  0.012740357205620044\n",
            "Epoch: 062, Loss: 0.8722, Train: 0.7442, Test: 0.6789\n",
            "Early stopping:  0.012642657317212987\n",
            "Epoch: 063, Loss: 0.8644, Train: 0.7475, Test: 0.6768\n",
            "Early stopping:  0.01250041570865751\n",
            "Epoch: 064, Loss: 0.8567, Train: 0.7484, Test: 0.6789\n",
            "Early stopping:  0.012405875487390203\n",
            "Epoch: 065, Loss: 0.8491, Train: 0.7500, Test: 0.6837\n",
            "Early stopping:  0.012279456869011123\n",
            "Epoch: 066, Loss: 0.8416, Train: 0.7516, Test: 0.6842\n",
            "Early stopping:  0.012091298262625914\n",
            "Epoch: 067, Loss: 0.8342, Train: 0.7549, Test: 0.6842\n",
            "Early stopping:  0.011926170579609244\n",
            "Epoch: 068, Loss: 0.8270, Train: 0.7564, Test: 0.6853\n",
            "Early stopping:  0.01175391332206198\n",
            "Epoch: 069, Loss: 0.8198, Train: 0.7582, Test: 0.6858\n",
            "Early stopping:  0.011595504936313828\n",
            "Epoch: 070, Loss: 0.8127, Train: 0.7599, Test: 0.6868\n",
            "Early stopping:  0.011425714030628429\n",
            "Epoch: 071, Loss: 0.8059, Train: 0.7624, Test: 0.6874\n",
            "Early stopping:  0.011217566808131756\n",
            "Epoch: 072, Loss: 0.7991, Train: 0.7641, Test: 0.6900\n",
            "Early stopping:  0.011014625476914461\n",
            "Epoch: 073, Loss: 0.7923, Train: 0.7668, Test: 0.6900\n",
            "Early stopping:  0.010858666330468909\n",
            "Epoch: 074, Loss: 0.7855, Train: 0.7692, Test: 0.6900\n",
            "Early stopping:  0.010770157974807121\n",
            "Epoch: 075, Loss: 0.7787, Train: 0.7724, Test: 0.6889\n",
            "Early stopping:  0.01072613762743437\n",
            "Epoch: 076, Loss: 0.7721, Train: 0.7755, Test: 0.6889\n",
            "Early stopping:  0.010667448011533704\n",
            "Epoch: 077, Loss: 0.7655, Train: 0.7771, Test: 0.6895\n",
            "Early stopping:  0.010574891762670516\n",
            "Epoch: 078, Loss: 0.7590, Train: 0.7788, Test: 0.6895\n",
            "Early stopping:  0.010480146017307936\n",
            "Epoch: 079, Loss: 0.7525, Train: 0.7795, Test: 0.6905\n",
            "Early stopping:  0.010395313284162638\n",
            "Epoch: 080, Loss: 0.7461, Train: 0.7816, Test: 0.6900\n",
            "Early stopping:  0.010303941496146498\n",
            "Epoch: 081, Loss: 0.7398, Train: 0.7814, Test: 0.6900\n",
            "Early stopping:  0.010182248337289113\n",
            "Epoch: 082, Loss: 0.7335, Train: 0.7845, Test: 0.6900\n",
            "Early stopping:  0.01007159620818189\n",
            "Epoch: 083, Loss: 0.7273, Train: 0.7864, Test: 0.6884\n",
            "Early stopping:  0.009954356928943671\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 13, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.81      0.76      0.79       106\n",
            "            CAD_and_CAM       0.70      0.73      0.71        92\n",
            "              Companies       0.60      0.51      0.55        98\n",
            "       Computer_Science       0.75      0.83      0.79        99\n",
            "            Consultants       0.60      0.64      0.62       122\n",
            "           Data_Formats       0.82      0.75      0.78       114\n",
            "    Data_Communications       0.69      0.69      0.69       112\n",
            "              Education       0.81      0.87      0.84       103\n",
            "               Graphics       0.84      0.91      0.87       109\n",
            "               Hardware       0.63      0.60      0.61        97\n",
            "               Internet       0.68      0.70      0.69        99\n",
            "       Mobile_Computing       0.81      0.76      0.79       114\n",
            "             Multimedia       0.66      0.66      0.66        95\n",
            "            Open_Source       0.60      0.69      0.64        84\n",
            "            Programming       0.48      0.50      0.49        98\n",
            "               Robotics       0.91      0.91      0.91        91\n",
            "               Security       0.69      0.80      0.74        85\n",
            "               Software       0.36      0.26      0.31       102\n",
            "                Systems       0.47      0.46      0.47        80\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.68      0.69      0.68      1900\n",
            "           weighted avg       0.68      0.69      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.1396, Train: 0.1171, Test: 0.1100\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 12.1429, Train: 0.1738, Test: 0.1632\n",
            "Early stopping:  4.245015071976995\n",
            "Epoch: 003, Loss: 11.0395, Train: 0.2659, Test: 0.2705\n",
            "Early stopping:  3.1954872528135545\n",
            "Epoch: 004, Loss: 10.0546, Train: 0.2926, Test: 0.2774\n",
            "Early stopping:  2.6128729365607066\n",
            "Epoch: 005, Loss: 8.0370, Train: 0.3086, Test: 0.2921\n",
            "Early stopping:  2.402807263099867\n",
            "Epoch: 006, Loss: 6.4811, Train: 0.3434, Test: 0.3374\n",
            "Early stopping:  2.286038874424296\n",
            "Epoch: 007, Loss: 4.9339, Train: 0.3662, Test: 0.3626\n",
            "Early stopping:  2.5054733589124636\n",
            "Epoch: 008, Loss: 3.7900, Train: 0.3737, Test: 0.3611\n",
            "Early stopping:  2.4833138354701356\n",
            "Epoch: 009, Loss: 2.9894, Train: 0.4004, Test: 0.3937\n",
            "Early stopping:  2.038673924256314\n",
            "Epoch: 010, Loss: 2.4201, Train: 0.4314, Test: 0.4226\n",
            "Early stopping:  1.6212668803187686\n",
            "Epoch: 011, Loss: 2.0942, Train: 0.4626, Test: 0.4505\n",
            "Early stopping:  1.1423157481793333\n",
            "Epoch: 012, Loss: 1.9253, Train: 0.4787, Test: 0.4600\n",
            "Early stopping:  0.7585566286267789\n",
            "Epoch: 013, Loss: 1.8873, Train: 0.4887, Test: 0.4732\n",
            "Early stopping:  0.45716432729130074\n",
            "Epoch: 014, Loss: 1.8602, Train: 0.5030, Test: 0.4905\n",
            "Early stopping:  0.2324770329298993\n",
            "Epoch: 015, Loss: 1.8245, Train: 0.5178, Test: 0.5079\n",
            "Early stopping:  0.1050327747580049\n",
            "Epoch: 016, Loss: 1.7786, Train: 0.5284, Test: 0.5232\n",
            "Early stopping:  0.05651919638664663\n",
            "Epoch: 017, Loss: 1.7291, Train: 0.5345, Test: 0.5258\n",
            "Early stopping:  0.0633715071982792\n",
            "Epoch: 018, Loss: 1.6834, Train: 0.5400, Test: 0.5316\n",
            "Early stopping:  0.07111115547361405\n",
            "Epoch: 019, Loss: 1.6445, Train: 0.5499, Test: 0.5374\n",
            "Early stopping:  0.07203473435872866\n",
            "Epoch: 020, Loss: 1.6074, Train: 0.5592, Test: 0.5484\n",
            "Early stopping:  0.06764253576409457\n",
            "Epoch: 021, Loss: 1.5666, Train: 0.5672, Test: 0.5632\n",
            "Early stopping:  0.0634251444740861\n",
            "Epoch: 022, Loss: 1.5226, Train: 0.5766, Test: 0.5747\n",
            "Early stopping:  0.06319459845681896\n",
            "Epoch: 023, Loss: 1.4796, Train: 0.5879, Test: 0.5837\n",
            "Early stopping:  0.06558244162821106\n",
            "Epoch: 024, Loss: 1.4423, Train: 0.5961, Test: 0.5905\n",
            "Early stopping:  0.06597695432112875\n",
            "Epoch: 025, Loss: 1.4112, Train: 0.6043, Test: 0.6000\n",
            "Early stopping:  0.062002231012748585\n",
            "Epoch: 026, Loss: 1.3847, Train: 0.6111, Test: 0.6089\n",
            "Early stopping:  0.05469211402808363\n",
            "Epoch: 027, Loss: 1.3614, Train: 0.6153, Test: 0.6100\n",
            "Early stopping:  0.04670723588569767\n",
            "Epoch: 028, Loss: 1.3419, Train: 0.6191, Test: 0.6068\n",
            "Early stopping:  0.039776242052622805\n",
            "Epoch: 029, Loss: 1.3267, Train: 0.6221, Test: 0.6132\n",
            "Early stopping:  0.033664125306914\n",
            "Epoch: 030, Loss: 1.3135, Train: 0.6245, Test: 0.6179\n",
            "Early stopping:  0.028191883771539722\n",
            "Epoch: 031, Loss: 1.2988, Train: 0.6291, Test: 0.6195\n",
            "Early stopping:  0.024344479684558416\n",
            "Epoch: 032, Loss: 1.2810, Train: 0.6354, Test: 0.6216\n",
            "Early stopping:  0.0237099640059135\n",
            "Epoch: 033, Loss: 1.2614, Train: 0.6420, Test: 0.6263\n",
            "Early stopping:  0.025877849817406963\n",
            "Epoch: 034, Loss: 1.2430, Train: 0.6487, Test: 0.6300\n",
            "Early stopping:  0.028249804924286113\n",
            "Epoch: 035, Loss: 1.2271, Train: 0.6530, Test: 0.6337\n",
            "Early stopping:  0.028712927588884486\n",
            "Epoch: 036, Loss: 1.2116, Train: 0.6562, Test: 0.6374\n",
            "Early stopping:  0.02738830540310995\n",
            "Epoch: 037, Loss: 1.1954, Train: 0.6616, Test: 0.6368\n",
            "Early stopping:  0.025840139175645434\n",
            "Epoch: 038, Loss: 1.1789, Train: 0.6632, Test: 0.6374\n",
            "Early stopping:  0.02527670882521492\n",
            "Epoch: 039, Loss: 1.1634, Train: 0.6649, Test: 0.6426\n",
            "Early stopping:  0.025321037260276822\n",
            "Epoch: 040, Loss: 1.1493, Train: 0.6701, Test: 0.6437\n",
            "Early stopping:  0.02479363773512144\n",
            "Epoch: 041, Loss: 1.1367, Train: 0.6733, Test: 0.6468\n",
            "Early stopping:  0.023283878230999554\n",
            "Epoch: 042, Loss: 1.1243, Train: 0.6770, Test: 0.6500\n",
            "Early stopping:  0.021523021407756714\n",
            "Epoch: 043, Loss: 1.1105, Train: 0.6809, Test: 0.6542\n",
            "Early stopping:  0.020670590853219734\n",
            "Epoch: 044, Loss: 1.0971, Train: 0.6864, Test: 0.6584\n",
            "Early stopping:  0.020638463579761582\n",
            "Epoch: 045, Loss: 1.0856, Train: 0.6887, Test: 0.6574\n",
            "Early stopping:  0.020448030376665986\n",
            "Epoch: 046, Loss: 1.0754, Train: 0.6933, Test: 0.6579\n",
            "Early stopping:  0.019413207934155463\n",
            "Epoch: 047, Loss: 1.0654, Train: 0.6958, Test: 0.6611\n",
            "Early stopping:  0.017729578979632\n",
            "Epoch: 048, Loss: 1.0554, Train: 0.6986, Test: 0.6600\n",
            "Early stopping:  0.016389554139810136\n",
            "Epoch: 049, Loss: 1.0455, Train: 0.7009, Test: 0.6642\n",
            "Early stopping:  0.01584757147140275\n",
            "Epoch: 050, Loss: 1.0356, Train: 0.7030, Test: 0.6647\n",
            "Early stopping:  0.01573965022340292\n",
            "Epoch: 051, Loss: 1.0261, Train: 0.7067, Test: 0.6653\n",
            "Early stopping:  0.01555792180568037\n",
            "Epoch: 052, Loss: 1.0176, Train: 0.7082, Test: 0.6674\n",
            "Early stopping:  0.015050120458039749\n",
            "Epoch: 053, Loss: 1.0092, Train: 0.7124, Test: 0.6679\n",
            "Early stopping:  0.014344941220954128\n",
            "Epoch: 054, Loss: 1.0003, Train: 0.7150, Test: 0.6732\n",
            "Early stopping:  0.013821354133162395\n",
            "Epoch: 055, Loss: 0.9916, Train: 0.7171, Test: 0.6726\n",
            "Early stopping:  0.013632170649891586\n",
            "Epoch: 056, Loss: 0.9837, Train: 0.7209, Test: 0.6705\n",
            "Early stopping:  0.013487072831070887\n",
            "Epoch: 057, Loss: 0.9761, Train: 0.7221, Test: 0.6716\n",
            "Early stopping:  0.01312148038618392\n",
            "Epoch: 058, Loss: 0.9680, Train: 0.7229, Test: 0.6726\n",
            "Early stopping:  0.012690770064339513\n",
            "Epoch: 059, Loss: 0.9600, Train: 0.7241, Test: 0.6716\n",
            "Early stopping:  0.012469057332922475\n",
            "Epoch: 060, Loss: 0.9526, Train: 0.7272, Test: 0.6732\n",
            "Early stopping:  0.012387124491442176\n",
            "Epoch: 061, Loss: 0.9451, Train: 0.7291, Test: 0.6726\n",
            "Early stopping:  0.012216639670723511\n",
            "Epoch: 062, Loss: 0.9377, Train: 0.7296, Test: 0.6779\n",
            "Early stopping:  0.011937323724846549\n",
            "Epoch: 063, Loss: 0.9306, Train: 0.7337, Test: 0.6800\n",
            "Early stopping:  0.011672678744010003\n",
            "Epoch: 064, Loss: 0.9238, Train: 0.7349, Test: 0.6805\n",
            "Early stopping:  0.011401516421315532\n",
            "Epoch: 065, Loss: 0.9169, Train: 0.7349, Test: 0.6800\n",
            "Early stopping:  0.011140219298453186\n",
            "Epoch: 066, Loss: 0.9101, Train: 0.7363, Test: 0.6816\n",
            "Early stopping:  0.010906605486207295\n",
            "Epoch: 067, Loss: 0.9036, Train: 0.7382, Test: 0.6800\n",
            "Early stopping:  0.010689413588352716\n",
            "Epoch: 068, Loss: 0.8974, Train: 0.7395, Test: 0.6768\n",
            "Early stopping:  0.010433503300430962\n",
            "Epoch: 069, Loss: 0.8912, Train: 0.7392, Test: 0.6795\n",
            "Early stopping:  0.010103898717217489\n",
            "Epoch: 070, Loss: 0.8849, Train: 0.7417, Test: 0.6800\n",
            "Early stopping:  0.009901243436400408\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.82      0.80      0.81        94\n",
            "            CAD_and_CAM       0.70      0.70      0.70       105\n",
            "              Companies       0.53      0.50      0.51       102\n",
            "       Computer_Science       0.76      0.71      0.73       102\n",
            "            Consultants       0.60      0.59      0.59       120\n",
            "           Data_Formats       0.79      0.77      0.78        98\n",
            "    Data_Communications       0.64      0.79      0.71        92\n",
            "              Education       0.79      0.88      0.84        92\n",
            "               Graphics       0.87      0.94      0.90       111\n",
            "               Hardware       0.66      0.66      0.66       105\n",
            "               Internet       0.62      0.64      0.63       102\n",
            "       Mobile_Computing       0.84      0.71      0.77       112\n",
            "             Multimedia       0.69      0.73      0.71       106\n",
            "            Open_Source       0.61      0.70      0.65        96\n",
            "            Programming       0.47      0.50      0.48        88\n",
            "               Robotics       0.89      0.84      0.86       104\n",
            "               Security       0.80      0.74      0.77        89\n",
            "               Software       0.27      0.21      0.24        94\n",
            "                Systems       0.48      0.49      0.48        88\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.67      0.68      0.67      1900\n",
            "           weighted avg       0.68      0.68      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.3753, Train: 0.1279, Test: 0.1295\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 11.2331, Train: 0.2043, Test: 0.2026\n",
            "Early stopping:  3.434958375569009\n",
            "Epoch: 003, Loss: 12.9520, Train: 0.2605, Test: 0.2705\n",
            "Early stopping:  3.4109124671007236\n",
            "Epoch: 004, Loss: 13.1174, Train: 0.2778, Test: 0.2847\n",
            "Early stopping:  3.1469472751097376\n",
            "Epoch: 005, Loss: 14.2314, Train: 0.2721, Test: 0.2653\n",
            "Early stopping:  3.10181655005766\n",
            "Epoch: 006, Loss: 12.3199, Train: 0.3132, Test: 0.3089\n",
            "Early stopping:  1.1016143831136973\n",
            "Epoch: 007, Loss: 10.3874, Train: 0.3478, Test: 0.3426\n",
            "Early stopping:  1.4165973154276925\n",
            "Epoch: 008, Loss: 8.3855, Train: 0.4064, Test: 0.4037\n",
            "Early stopping:  2.318903359534125\n",
            "Epoch: 009, Loss: 6.6007, Train: 0.4604, Test: 0.4658\n",
            "Early stopping:  3.0355266569954527\n",
            "Epoch: 010, Loss: 4.9854, Train: 0.4749, Test: 0.4737\n",
            "Early stopping:  2.9206453812192303\n",
            "Epoch: 011, Loss: 4.0761, Train: 0.4764, Test: 0.4726\n",
            "Early stopping:  2.554285006130948\n",
            "Epoch: 012, Loss: 3.4808, Train: 0.4838, Test: 0.4689\n",
            "Early stopping:  1.9941791749292348\n",
            "Epoch: 013, Loss: 2.9492, Train: 0.4933, Test: 0.4874\n",
            "Early stopping:  1.435158066786176\n",
            "Epoch: 014, Loss: 2.4819, Train: 0.4917, Test: 0.4837\n",
            "Early stopping:  0.9790025108020783\n",
            "Epoch: 015, Loss: 2.2035, Train: 0.4779, Test: 0.4784\n",
            "Early stopping:  0.7561972132122291\n",
            "Epoch: 016, Loss: 2.1196, Train: 0.4705, Test: 0.4726\n",
            "Early stopping:  0.5675802120543668\n",
            "Epoch: 017, Loss: 2.0484, Train: 0.4626, Test: 0.4726\n",
            "Early stopping:  0.36789844500980295\n",
            "Epoch: 018, Loss: 2.0113, Train: 0.4711, Test: 0.4832\n",
            "Early stopping:  0.18767424113751213\n",
            "Epoch: 019, Loss: 1.9599, Train: 0.4833, Test: 0.4863\n",
            "Early stopping:  0.09526785728939496\n",
            "Epoch: 020, Loss: 1.8838, Train: 0.5042, Test: 0.5111\n",
            "Early stopping:  0.08912309503687728\n",
            "Epoch: 021, Loss: 1.8042, Train: 0.5263, Test: 0.5321\n",
            "Early stopping:  0.09848049558667767\n",
            "Epoch: 022, Loss: 1.7295, Train: 0.5430, Test: 0.5542\n",
            "Early stopping:  0.11401301482892542\n",
            "Epoch: 023, Loss: 1.6678, Train: 0.5508, Test: 0.5600\n",
            "Early stopping:  0.116890378128573\n",
            "Epoch: 024, Loss: 1.6208, Train: 0.5561, Test: 0.5663\n",
            "Early stopping:  0.1052736373340699\n",
            "Epoch: 025, Loss: 1.5847, Train: 0.5617, Test: 0.5732\n",
            "Early stopping:  0.0874775385308153\n",
            "Epoch: 026, Loss: 1.5531, Train: 0.5676, Test: 0.5816\n",
            "Early stopping:  0.06958243448081823\n",
            "Epoch: 027, Loss: 1.5217, Train: 0.5814, Test: 0.5905\n",
            "Early stopping:  0.057115033006203825\n",
            "Epoch: 028, Loss: 1.4873, Train: 0.5971, Test: 0.6026\n",
            "Early stopping:  0.052209895281442764\n",
            "Epoch: 029, Loss: 1.4512, Train: 0.5992, Test: 0.6011\n",
            "Early stopping:  0.05265987660451284\n",
            "Epoch: 030, Loss: 1.4197, Train: 0.5997, Test: 0.6021\n",
            "Early stopping:  0.05334770962920019\n",
            "Epoch: 031, Loss: 1.3942, Train: 0.6007, Test: 0.6095\n",
            "Early stopping:  0.05112581906717573\n",
            "Epoch: 032, Loss: 1.3673, Train: 0.6117, Test: 0.6079\n",
            "Early stopping:  0.047068249018526916\n",
            "Epoch: 033, Loss: 1.3372, Train: 0.6246, Test: 0.6153\n",
            "Early stopping:  0.04436422284078959\n",
            "Epoch: 034, Loss: 1.3113, Train: 0.6304, Test: 0.6232\n",
            "Early stopping:  0.04332931812119079\n",
            "Epoch: 035, Loss: 1.2960, Train: 0.6324, Test: 0.6274\n",
            "Early stopping:  0.04013948125284451\n",
            "Epoch: 036, Loss: 1.2812, Train: 0.6366, Test: 0.6316\n",
            "Early stopping:  0.03422878650520206\n",
            "Epoch: 037, Loss: 1.2583, Train: 0.6459, Test: 0.6368\n",
            "Early stopping:  0.029871061332105302\n",
            "Epoch: 038, Loss: 1.2328, Train: 0.6538, Test: 0.6442\n",
            "Early stopping:  0.03100937552957437\n",
            "Epoch: 039, Loss: 1.2123, Train: 0.6596, Test: 0.6479\n",
            "Early stopping:  0.03422073837145551\n",
            "Epoch: 040, Loss: 1.1974, Train: 0.6637, Test: 0.6489\n",
            "Early stopping:  0.033889882442558264\n",
            "Epoch: 041, Loss: 1.1853, Train: 0.6654, Test: 0.6537\n",
            "Early stopping:  0.028999935584888923\n",
            "Epoch: 042, Loss: 1.1733, Train: 0.6688, Test: 0.6600\n",
            "Early stopping:  0.023250357927008806\n",
            "Epoch: 043, Loss: 1.1607, Train: 0.6724, Test: 0.6563\n",
            "Early stopping:  0.020151140791823573\n",
            "Epoch: 044, Loss: 1.1475, Train: 0.6771, Test: 0.6595\n",
            "Early stopping:  0.019677920272405204\n",
            "Epoch: 045, Loss: 1.1338, Train: 0.6812, Test: 0.6653\n",
            "Early stopping:  0.020379106007449087\n",
            "Epoch: 046, Loss: 1.1197, Train: 0.6824, Test: 0.6616\n",
            "Early stopping:  0.021214508913573054\n",
            "Epoch: 047, Loss: 1.1062, Train: 0.6837, Test: 0.6584\n",
            "Early stopping:  0.021638856837172926\n",
            "Epoch: 048, Loss: 1.0933, Train: 0.6861, Test: 0.6563\n",
            "Early stopping:  0.021522901915007792\n",
            "Epoch: 049, Loss: 1.0813, Train: 0.6892, Test: 0.6553\n",
            "Early stopping:  0.02077149969225494\n",
            "Epoch: 050, Loss: 1.0701, Train: 0.6945, Test: 0.6563\n",
            "Early stopping:  0.019630980840174016\n",
            "Epoch: 051, Loss: 1.0580, Train: 0.6983, Test: 0.6542\n",
            "Early stopping:  0.01888658639837806\n",
            "Epoch: 052, Loss: 1.0454, Train: 0.6988, Test: 0.6600\n",
            "Early stopping:  0.018829306711763152\n",
            "Epoch: 053, Loss: 1.0337, Train: 0.7018, Test: 0.6663\n",
            "Early stopping:  0.018955882329409476\n",
            "Epoch: 054, Loss: 1.0231, Train: 0.7058, Test: 0.6637\n",
            "Early stopping:  0.018705457445782784\n",
            "Epoch: 055, Loss: 1.0126, Train: 0.7088, Test: 0.6684\n",
            "Early stopping:  0.017908050725213225\n",
            "Epoch: 056, Loss: 1.0024, Train: 0.7114, Test: 0.6689\n",
            "Early stopping:  0.016932798532849083\n",
            "Epoch: 057, Loss: 0.9928, Train: 0.7143, Test: 0.6737\n",
            "Early stopping:  0.01620530276148719\n",
            "Epoch: 058, Loss: 0.9828, Train: 0.7166, Test: 0.6747\n",
            "Early stopping:  0.01588293352411218\n",
            "Epoch: 059, Loss: 0.9723, Train: 0.7201, Test: 0.6763\n",
            "Early stopping:  0.01585848479217436\n",
            "Epoch: 060, Loss: 0.9622, Train: 0.7233, Test: 0.6800\n",
            "Early stopping:  0.01598145996929016\n",
            "Epoch: 061, Loss: 0.9525, Train: 0.7261, Test: 0.6800\n",
            "Early stopping:  0.016025011608794583\n",
            "Epoch: 062, Loss: 0.9432, Train: 0.7270, Test: 0.6774\n",
            "Early stopping:  0.01566059687515869\n",
            "Epoch: 063, Loss: 0.9339, Train: 0.7297, Test: 0.6811\n",
            "Early stopping:  0.015136567852282734\n",
            "Epoch: 064, Loss: 0.9247, Train: 0.7314, Test: 0.6805\n",
            "Early stopping:  0.01478142417760752\n",
            "Epoch: 065, Loss: 0.9157, Train: 0.7359, Test: 0.6800\n",
            "Early stopping:  0.014532480658054407\n",
            "Epoch: 066, Loss: 0.9069, Train: 0.7371, Test: 0.6774\n",
            "Early stopping:  0.014351264726812932\n",
            "Epoch: 067, Loss: 0.8982, Train: 0.7393, Test: 0.6784\n",
            "Early stopping:  0.01412774627584642\n",
            "Epoch: 068, Loss: 0.8894, Train: 0.7425, Test: 0.6768\n",
            "Early stopping:  0.01395852582938559\n",
            "Epoch: 069, Loss: 0.8804, Train: 0.7454, Test: 0.6768\n",
            "Early stopping:  0.01394444520549371\n",
            "Epoch: 070, Loss: 0.8718, Train: 0.7489, Test: 0.6789\n",
            "Early stopping:  0.013913149811276584\n",
            "Epoch: 071, Loss: 0.8632, Train: 0.7495, Test: 0.6795\n",
            "Early stopping:  0.013834836261516012\n",
            "Epoch: 072, Loss: 0.8549, Train: 0.7518, Test: 0.6800\n",
            "Early stopping:  0.01361985125492737\n",
            "Epoch: 073, Loss: 0.8467, Train: 0.7536, Test: 0.6805\n",
            "Early stopping:  0.013333800065614368\n",
            "Epoch: 074, Loss: 0.8386, Train: 0.7564, Test: 0.6816\n",
            "Early stopping:  0.013091244670876473\n",
            "Epoch: 075, Loss: 0.8304, Train: 0.7584, Test: 0.6805\n",
            "Early stopping:  0.012939003657816526\n",
            "Epoch: 076, Loss: 0.8223, Train: 0.7620, Test: 0.6811\n",
            "Early stopping:  0.012869408171079314\n",
            "Epoch: 077, Loss: 0.8145, Train: 0.7633, Test: 0.6811\n",
            "Early stopping:  0.012758762467546607\n",
            "Epoch: 078, Loss: 0.8068, Train: 0.7651, Test: 0.6795\n",
            "Early stopping:  0.012589029426022634\n",
            "Epoch: 079, Loss: 0.7991, Train: 0.7674, Test: 0.6811\n",
            "Early stopping:  0.01235040445845912\n",
            "Epoch: 080, Loss: 0.7915, Train: 0.7691, Test: 0.6821\n",
            "Early stopping:  0.012164748072661116\n",
            "Epoch: 081, Loss: 0.7839, Train: 0.7724, Test: 0.6826\n",
            "Early stopping:  0.012093390316259257\n",
            "Epoch: 082, Loss: 0.7764, Train: 0.7758, Test: 0.6811\n",
            "Early stopping:  0.012024620978799206\n",
            "Epoch: 083, Loss: 0.7691, Train: 0.7768, Test: 0.6800\n",
            "Early stopping:  0.0119001265083275\n",
            "Epoch: 084, Loss: 0.7619, Train: 0.7799, Test: 0.6826\n",
            "Early stopping:  0.011722856283797066\n",
            "Epoch: 085, Loss: 0.7548, Train: 0.7818, Test: 0.6816\n",
            "Early stopping:  0.011484710914943734\n",
            "Epoch: 086, Loss: 0.7476, Train: 0.7837, Test: 0.6821\n",
            "Early stopping:  0.011373047229828178\n",
            "Epoch: 087, Loss: 0.7407, Train: 0.7858, Test: 0.6811\n",
            "Early stopping:  0.01124308489909527\n",
            "Epoch: 088, Loss: 0.7342, Train: 0.7883, Test: 0.6826\n",
            "Early stopping:  0.010985834356150694\n",
            "Epoch: 089, Loss: 0.7273, Train: 0.7916, Test: 0.6779\n",
            "Early stopping:  0.010832543335341492\n",
            "Epoch: 090, Loss: 0.7203, Train: 0.7920, Test: 0.6763\n",
            "Early stopping:  0.010744657306071855\n",
            "Epoch: 091, Loss: 0.7134, Train: 0.7951, Test: 0.6789\n",
            "Early stopping:  0.01083034634551382\n",
            "Epoch: 092, Loss: 0.7070, Train: 0.7958, Test: 0.6800\n",
            "Early stopping:  0.010806845385418718\n",
            "Epoch: 093, Loss: 0.7008, Train: 0.7982, Test: 0.6826\n",
            "Early stopping:  0.010487936156677868\n",
            "Epoch: 094, Loss: 0.6945, Train: 0.7987, Test: 0.6816\n",
            "Early stopping:  0.010157247284480954\n",
            "Epoch: 095, Loss: 0.6883, Train: 0.8026, Test: 0.6842\n",
            "Early stopping:  0.009904465191378768\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.84      0.81        85\n",
            "            CAD_and_CAM       0.71      0.70      0.70       100\n",
            "              Companies       0.50      0.49      0.49       104\n",
            "       Computer_Science       0.73      0.73      0.73        89\n",
            "            Consultants       0.61      0.59      0.60       111\n",
            "           Data_Formats       0.78      0.78      0.78       103\n",
            "    Data_Communications       0.79      0.75      0.77       112\n",
            "              Education       0.90      0.84      0.87       115\n",
            "               Graphics       0.85      0.92      0.88       103\n",
            "               Hardware       0.65      0.67      0.66       105\n",
            "               Internet       0.57      0.66      0.61        77\n",
            "       Mobile_Computing       0.78      0.71      0.75       101\n",
            "             Multimedia       0.72      0.76      0.74        92\n",
            "            Open_Source       0.61      0.64      0.62       113\n",
            "            Programming       0.49      0.45      0.47       106\n",
            "               Robotics       0.91      0.87      0.89        93\n",
            "               Security       0.76      0.77      0.76       107\n",
            "               Software       0.24      0.24      0.24        92\n",
            "                Systems       0.57      0.59      0.58        92\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.69      0.68      0.68      1900\n",
            "\n",
            "time: 44.5 s (started: 2024-08-17 14:13:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "id": "-LyuKjoou7ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7a2eb3-ed8b-4a02-cf81-3e5a77dfc0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 396 ms (started: 2024-08-17 14:17:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "ZjjcX6Wku7ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT','80%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndDQH_Z8u7ay",
        "outputId": "899399b2-2444-456e-c562-06c4ea1436d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9516, Train: 0.3708, Test: 0.3642\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8218, Train: 0.5242, Test: 0.5100\n",
            "Early stopping:  0.09182567602547291\n",
            "Epoch: 003, Loss: 2.6574, Train: 0.5462, Test: 0.5168\n",
            "Early stopping:  0.1474472939959434\n",
            "Epoch: 004, Loss: 2.4685, Train: 0.5671, Test: 0.5389\n",
            "Early stopping:  0.20904086312435768\n",
            "Epoch: 005, Loss: 2.2699, Train: 0.5899, Test: 0.5705\n",
            "Early stopping:  0.27232267987651354\n",
            "Epoch: 006, Loss: 2.0711, Train: 0.6104, Test: 0.5905\n",
            "Early stopping:  0.29885855196152794\n",
            "Epoch: 007, Loss: 1.8796, Train: 0.6309, Test: 0.6184\n",
            "Early stopping:  0.3088029222987719\n",
            "Epoch: 008, Loss: 1.7019, Train: 0.6404, Test: 0.6263\n",
            "Early stopping:  0.30419721714024717\n",
            "Epoch: 009, Loss: 1.5475, Train: 0.6449, Test: 0.6311\n",
            "Early stopping:  0.2871609024585015\n",
            "Epoch: 010, Loss: 1.4215, Train: 0.6512, Test: 0.6321\n",
            "Early stopping:  0.25878232986934796\n",
            "Epoch: 011, Loss: 1.3216, Train: 0.6562, Test: 0.6411\n",
            "Early stopping:  0.22216628590195142\n",
            "Epoch: 012, Loss: 1.2469, Train: 0.6624, Test: 0.6463\n",
            "Early stopping:  0.1813135164486129\n",
            "Epoch: 013, Loss: 1.1921, Train: 0.6670, Test: 0.6458\n",
            "Early stopping:  0.14178203801841696\n",
            "Epoch: 014, Loss: 1.1515, Train: 0.6709, Test: 0.6521\n",
            "Early stopping:  0.107460981114981\n",
            "Epoch: 015, Loss: 1.1214, Train: 0.6784, Test: 0.6563\n",
            "Early stopping:  0.07962488725270865\n",
            "Epoch: 016, Loss: 1.0954, Train: 0.6839, Test: 0.6616\n",
            "Early stopping:  0.0598101394881908\n",
            "Epoch: 017, Loss: 1.0721, Train: 0.6897, Test: 0.6658\n",
            "Early stopping:  0.047121103113732576\n",
            "Epoch: 018, Loss: 1.0496, Train: 0.6947, Test: 0.6726\n",
            "Early stopping:  0.04011127827823064\n",
            "Epoch: 019, Loss: 1.0275, Train: 0.7012, Test: 0.6753\n",
            "Early stopping:  0.03695207907389025\n",
            "Epoch: 020, Loss: 1.0060, Train: 0.7064, Test: 0.6816\n",
            "Early stopping:  0.0353250541711163\n",
            "Epoch: 021, Loss: 0.9847, Train: 0.7134, Test: 0.6805\n",
            "Early stopping:  0.03453561015337304\n",
            "Epoch: 022, Loss: 0.9642, Train: 0.7188, Test: 0.6800\n",
            "Early stopping:  0.033766452486883224\n",
            "Epoch: 023, Loss: 0.9447, Train: 0.7214, Test: 0.6816\n",
            "Early stopping:  0.03277652852062915\n",
            "Epoch: 024, Loss: 0.9267, Train: 0.7276, Test: 0.6805\n",
            "Early stopping:  0.031402368950015365\n",
            "Epoch: 025, Loss: 0.9097, Train: 0.7307, Test: 0.6832\n",
            "Early stopping:  0.02968016006460772\n",
            "Epoch: 026, Loss: 0.8943, Train: 0.7368, Test: 0.6847\n",
            "Early stopping:  0.02766837743899011\n",
            "Epoch: 027, Loss: 0.8800, Train: 0.7411, Test: 0.6900\n",
            "Early stopping:  0.025601973397657574\n",
            "Epoch: 028, Loss: 0.8668, Train: 0.7453, Test: 0.6905\n",
            "Early stopping:  0.023662767038170607\n",
            "Epoch: 029, Loss: 0.8543, Train: 0.7508, Test: 0.6911\n",
            "Early stopping:  0.021891941161106376\n",
            "Epoch: 030, Loss: 0.8425, Train: 0.7555, Test: 0.6947\n",
            "Early stopping:  0.020488822878023012\n",
            "Epoch: 031, Loss: 0.8313, Train: 0.7593, Test: 0.6974\n",
            "Early stopping:  0.019269841601609584\n",
            "Epoch: 032, Loss: 0.8207, Train: 0.7642, Test: 0.7021\n",
            "Early stopping:  0.018213324653789997\n",
            "Epoch: 033, Loss: 0.8106, Train: 0.7687, Test: 0.7011\n",
            "Early stopping:  0.017254925811888006\n",
            "Epoch: 034, Loss: 0.8013, Train: 0.7742, Test: 0.7037\n",
            "Early stopping:  0.0162840354238815\n",
            "Epoch: 035, Loss: 0.7927, Train: 0.7774, Test: 0.7063\n",
            "Early stopping:  0.015283274711368372\n",
            "Epoch: 036, Loss: 0.7845, Train: 0.7784, Test: 0.7079\n",
            "Early stopping:  0.014299750159083517\n",
            "Epoch: 037, Loss: 0.7766, Train: 0.7812, Test: 0.7121\n",
            "Early stopping:  0.013422193997884387\n",
            "Epoch: 038, Loss: 0.7688, Train: 0.7855, Test: 0.7153\n",
            "Early stopping:  0.012842288300946208\n",
            "Epoch: 039, Loss: 0.7609, Train: 0.7875, Test: 0.7184\n",
            "Early stopping:  0.012540174040411458\n",
            "Epoch: 040, Loss: 0.7529, Train: 0.7911, Test: 0.7211\n",
            "Early stopping:  0.01246915696772053\n",
            "Epoch: 041, Loss: 0.7450, Train: 0.7938, Test: 0.7263\n",
            "Early stopping:  0.0125087416157174\n",
            "Epoch: 042, Loss: 0.7370, Train: 0.7972, Test: 0.7253\n",
            "Early stopping:  0.012557362989496241\n",
            "Epoch: 043, Loss: 0.7290, Train: 0.7992, Test: 0.7279\n",
            "Early stopping:  0.012609435898871068\n",
            "Epoch: 044, Loss: 0.7210, Train: 0.8009, Test: 0.7274\n",
            "Early stopping:  0.012623270065262347\n",
            "Epoch: 045, Loss: 0.7130, Train: 0.8029, Test: 0.7284\n",
            "Early stopping:  0.012644651661993018\n",
            "Epoch: 046, Loss: 0.7049, Train: 0.8057, Test: 0.7268\n",
            "Early stopping:  0.012672082004577135\n",
            "Epoch: 047, Loss: 0.6969, Train: 0.8061, Test: 0.7274\n",
            "Early stopping:  0.012704561046410018\n",
            "Epoch: 048, Loss: 0.6888, Train: 0.8087, Test: 0.7279\n",
            "Early stopping:  0.012738999949210772\n",
            "Epoch: 049, Loss: 0.6808, Train: 0.8093, Test: 0.7242\n",
            "Early stopping:  0.012726131701791741\n",
            "Epoch: 050, Loss: 0.6729, Train: 0.8116, Test: 0.7216\n",
            "Early stopping:  0.012677999581430392\n",
            "Epoch: 051, Loss: 0.6649, Train: 0.8125, Test: 0.7205\n",
            "Early stopping:  0.012614547275915895\n",
            "Epoch: 052, Loss: 0.6573, Train: 0.8150, Test: 0.7195\n",
            "Early stopping:  0.012485978768550334\n",
            "Epoch: 053, Loss: 0.6499, Train: 0.8164, Test: 0.7216\n",
            "Early stopping:  0.012250148067519045\n",
            "Epoch: 054, Loss: 0.6427, Train: 0.8191, Test: 0.7232\n",
            "Early stopping:  0.01192858154572874\n",
            "Epoch: 055, Loss: 0.6348, Train: 0.8238, Test: 0.7258\n",
            "Early stopping:  0.011853593646126703\n",
            "Epoch: 056, Loss: 0.6267, Train: 0.8234, Test: 0.7274\n",
            "Early stopping:  0.012069726960726277\n",
            "Epoch: 057, Loss: 0.6197, Train: 0.8274, Test: 0.7232\n",
            "Early stopping:  0.012087991241407412\n",
            "Epoch: 058, Loss: 0.6128, Train: 0.8299, Test: 0.7226\n",
            "Early stopping:  0.011845588908391231\n",
            "Epoch: 059, Loss: 0.6049, Train: 0.8339, Test: 0.7242\n",
            "Early stopping:  0.011641239558616274\n",
            "Epoch: 060, Loss: 0.5971, Train: 0.8349, Test: 0.7226\n",
            "Early stopping:  0.011678198210566234\n",
            "Epoch: 061, Loss: 0.5906, Train: 0.8376, Test: 0.7279\n",
            "Early stopping:  0.0116815137308487\n",
            "Epoch: 062, Loss: 0.5842, Train: 0.8414, Test: 0.7242\n",
            "Early stopping:  0.011330262477110435\n",
            "Epoch: 063, Loss: 0.5768, Train: 0.8414, Test: 0.7242\n",
            "Early stopping:  0.010958339002251597\n",
            "Epoch: 064, Loss: 0.5697, Train: 0.8455, Test: 0.7284\n",
            "Early stopping:  0.010877873050434736\n",
            "Epoch: 065, Loss: 0.5631, Train: 0.8470, Test: 0.7258\n",
            "Early stopping:  0.010976641565610219\n",
            "Epoch: 066, Loss: 0.5571, Train: 0.8484, Test: 0.7289\n",
            "Early stopping:  0.010738011761412907\n",
            "Epoch: 067, Loss: 0.5508, Train: 0.8518, Test: 0.7221\n",
            "Early stopping:  0.010222878966566144\n",
            "Epoch: 068, Loss: 0.5435, Train: 0.8533, Test: 0.7247\n",
            "Early stopping:  0.01021699926934684\n",
            "Epoch: 069, Loss: 0.5370, Train: 0.8546, Test: 0.7232\n",
            "Early stopping:  0.010386546661363082\n",
            "Epoch: 070, Loss: 0.5311, Train: 0.8582, Test: 0.7237\n",
            "Early stopping:  0.010392165466425105\n",
            "Epoch: 071, Loss: 0.5238, Train: 0.8589, Test: 0.7226\n",
            "Early stopping:  0.010507558397059675\n",
            "Epoch: 072, Loss: 0.5159, Train: 0.8629, Test: 0.7232\n",
            "Early stopping:  0.010870515153598683\n",
            "Epoch: 073, Loss: 0.5095, Train: 0.8621, Test: 0.7237\n",
            "Early stopping:  0.011123233589638865\n",
            "Epoch: 074, Loss: 0.5038, Train: 0.8658, Test: 0.7205\n",
            "Early stopping:  0.010908456770365883\n",
            "Epoch: 075, Loss: 0.4968, Train: 0.8688, Test: 0.7232\n",
            "Early stopping:  0.01046106432249451\n",
            "Epoch: 076, Loss: 0.4898, Train: 0.8687, Test: 0.7200\n",
            "Early stopping:  0.010250813318318017\n",
            "Epoch: 077, Loss: 0.4838, Train: 0.8741, Test: 0.7211\n",
            "Early stopping:  0.010352729942781047\n",
            "Epoch: 078, Loss: 0.4783, Train: 0.8718, Test: 0.7179\n",
            "Early stopping:  0.010132834472605153\n",
            "Epoch: 079, Loss: 0.4729, Train: 0.8761, Test: 0.7189\n",
            "Early stopping:  0.009381889600343977\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.86      0.80      0.83        96\n",
            "            CAD_and_CAM       0.78      0.83      0.80        92\n",
            "              Companies       0.50      0.61      0.55        85\n",
            "       Computer_Science       0.74      0.76      0.75        88\n",
            "            Consultants       0.63      0.65      0.64        96\n",
            "           Data_Formats       0.78      0.83      0.81       103\n",
            "    Data_Communications       0.75      0.68      0.71       110\n",
            "              Education       0.81      0.80      0.81        97\n",
            "               Graphics       0.84      0.93      0.88       109\n",
            "               Hardware       0.69      0.69      0.69        93\n",
            "               Internet       0.65      0.61      0.63       110\n",
            "       Mobile_Computing       0.75      0.76      0.75        87\n",
            "             Multimedia       0.68      0.81      0.73        98\n",
            "            Open_Source       0.76      0.68      0.72       114\n",
            "            Programming       0.57      0.50      0.53        98\n",
            "               Robotics       0.92      0.93      0.93       105\n",
            "               Security       0.79      0.88      0.83       107\n",
            "               Software       0.38      0.31      0.34       109\n",
            "                Systems       0.69      0.61      0.65       103\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.71      0.72      0.72      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9559, Train: 0.3533, Test: 0.3579\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8366, Train: 0.5284, Test: 0.5211\n",
            "Early stopping:  0.0843715843978362\n",
            "Epoch: 003, Loss: 2.6814, Train: 0.5387, Test: 0.5332\n",
            "Early stopping:  0.1376297558091337\n",
            "Epoch: 004, Loss: 2.4977, Train: 0.5718, Test: 0.5632\n",
            "Early stopping:  0.19835482407790547\n",
            "Epoch: 005, Loss: 2.3008, Train: 0.5891, Test: 0.5774\n",
            "Early stopping:  0.26193024496645095\n",
            "Epoch: 006, Loss: 2.0982, Train: 0.6075, Test: 0.5968\n",
            "Early stopping:  0.2940683016459043\n",
            "Epoch: 007, Loss: 1.9025, Train: 0.6239, Test: 0.6200\n",
            "Early stopping:  0.3095352219163954\n",
            "Epoch: 008, Loss: 1.7232, Train: 0.6395, Test: 0.6311\n",
            "Early stopping:  0.30798411462496145\n",
            "Epoch: 009, Loss: 1.5643, Train: 0.6513, Test: 0.6395\n",
            "Early stopping:  0.2925138389192938\n",
            "Epoch: 010, Loss: 1.4336, Train: 0.6525, Test: 0.6389\n",
            "Early stopping:  0.26439084321971945\n",
            "Epoch: 011, Loss: 1.3330, Train: 0.6559, Test: 0.6421\n",
            "Early stopping:  0.22723172233419076\n",
            "Epoch: 012, Loss: 1.2551, Train: 0.6618, Test: 0.6463\n",
            "Early stopping:  0.18637108171946018\n",
            "Epoch: 013, Loss: 1.1977, Train: 0.6675, Test: 0.6532\n",
            "Early stopping:  0.14594302181618948\n",
            "Epoch: 014, Loss: 1.1540, Train: 0.6741, Test: 0.6526\n",
            "Early stopping:  0.11128064071545656\n",
            "Epoch: 015, Loss: 1.1207, Train: 0.6789, Test: 0.6600\n",
            "Early stopping:  0.08425674148333191\n",
            "Epoch: 016, Loss: 1.0938, Train: 0.6847, Test: 0.6637\n",
            "Early stopping:  0.06390270797967622\n",
            "Epoch: 017, Loss: 1.0689, Train: 0.6929, Test: 0.6679\n",
            "Early stopping:  0.05060175392306804\n",
            "Epoch: 018, Loss: 1.0460, Train: 0.6999, Test: 0.6684\n",
            "Early stopping:  0.04247405393945998\n",
            "Epoch: 019, Loss: 1.0242, Train: 0.7055, Test: 0.6679\n",
            "Early stopping:  0.038132152230265615\n",
            "Epoch: 020, Loss: 1.0022, Train: 0.7130, Test: 0.6742\n",
            "Early stopping:  0.03605967104630082\n",
            "Epoch: 021, Loss: 0.9808, Train: 0.7196, Test: 0.6779\n",
            "Early stopping:  0.03479494868918641\n",
            "Epoch: 022, Loss: 0.9607, Train: 0.7239, Test: 0.6832\n",
            "Early stopping:  0.03382772037762324\n",
            "Epoch: 023, Loss: 0.9419, Train: 0.7276, Test: 0.6837\n",
            "Early stopping:  0.03260074845439215\n",
            "Epoch: 024, Loss: 0.9238, Train: 0.7349, Test: 0.6853\n",
            "Early stopping:  0.030975303778285685\n",
            "Epoch: 025, Loss: 0.9072, Train: 0.7380, Test: 0.6874\n",
            "Early stopping:  0.029114519598581787\n",
            "Epoch: 026, Loss: 0.8921, Train: 0.7416, Test: 0.6900\n",
            "Early stopping:  0.02720532904345904\n",
            "Epoch: 027, Loss: 0.8775, Train: 0.7457, Test: 0.6926\n",
            "Early stopping:  0.025393805152014906\n",
            "Epoch: 028, Loss: 0.8636, Train: 0.7493, Test: 0.6942\n",
            "Early stopping:  0.023757343561947777\n",
            "Epoch: 029, Loss: 0.8508, Train: 0.7539, Test: 0.6953\n",
            "Early stopping:  0.022380394993317183\n",
            "Epoch: 030, Loss: 0.8388, Train: 0.7605, Test: 0.6953\n",
            "Early stopping:  0.021096419254286364\n",
            "Epoch: 031, Loss: 0.8273, Train: 0.7620, Test: 0.6963\n",
            "Early stopping:  0.019822885549266707\n",
            "Epoch: 032, Loss: 0.8166, Train: 0.7657, Test: 0.6979\n",
            "Early stopping:  0.018572332554967357\n",
            "Epoch: 033, Loss: 0.8066, Train: 0.7697, Test: 0.7016\n",
            "Early stopping:  0.017467889312391804\n",
            "Epoch: 034, Loss: 0.7971, Train: 0.7730, Test: 0.7068\n",
            "Early stopping:  0.016449875028389356\n",
            "Epoch: 035, Loss: 0.7881, Train: 0.7757, Test: 0.7079\n",
            "Early stopping:  0.015485004851095621\n",
            "Epoch: 036, Loss: 0.7796, Train: 0.7784, Test: 0.7111\n",
            "Early stopping:  0.014656049492363491\n",
            "Epoch: 037, Loss: 0.7715, Train: 0.7816, Test: 0.7116\n",
            "Early stopping:  0.013903640981679725\n",
            "Epoch: 038, Loss: 0.7637, Train: 0.7867, Test: 0.7105\n",
            "Early stopping:  0.013205860510193504\n",
            "Epoch: 039, Loss: 0.7560, Train: 0.7895, Test: 0.7084\n",
            "Early stopping:  0.012648180512063088\n",
            "Epoch: 040, Loss: 0.7484, Train: 0.7908, Test: 0.7126\n",
            "Early stopping:  0.012305316589724943\n",
            "Epoch: 041, Loss: 0.7405, Train: 0.7924, Test: 0.7137\n",
            "Early stopping:  0.012205926066380603\n",
            "Epoch: 042, Loss: 0.7326, Train: 0.7968, Test: 0.7126\n",
            "Early stopping:  0.012285997716707282\n",
            "Epoch: 043, Loss: 0.7246, Train: 0.7989, Test: 0.7137\n",
            "Early stopping:  0.012452769338202264\n",
            "Epoch: 044, Loss: 0.7165, Train: 0.8012, Test: 0.7137\n",
            "Early stopping:  0.012605251292556578\n",
            "Epoch: 045, Loss: 0.7085, Train: 0.8036, Test: 0.7132\n",
            "Early stopping:  0.012664601822771458\n",
            "Epoch: 046, Loss: 0.7006, Train: 0.8058, Test: 0.7147\n",
            "Early stopping:  0.012648678250407636\n",
            "Epoch: 047, Loss: 0.6927, Train: 0.8067, Test: 0.7126\n",
            "Early stopping:  0.012601524495315765\n",
            "Epoch: 048, Loss: 0.6847, Train: 0.8105, Test: 0.7111\n",
            "Early stopping:  0.01257031575385351\n",
            "Epoch: 049, Loss: 0.6766, Train: 0.8117, Test: 0.7126\n",
            "Early stopping:  0.012609621809589997\n",
            "Epoch: 050, Loss: 0.6685, Train: 0.8129, Test: 0.7116\n",
            "Early stopping:  0.012687638864489183\n",
            "Epoch: 051, Loss: 0.6604, Train: 0.8162, Test: 0.7105\n",
            "Early stopping:  0.01274181394514593\n",
            "Epoch: 052, Loss: 0.6524, Train: 0.8193, Test: 0.7116\n",
            "Early stopping:  0.012752377518641738\n",
            "Epoch: 053, Loss: 0.6444, Train: 0.8213, Test: 0.7153\n",
            "Early stopping:  0.012739408661448317\n",
            "Epoch: 054, Loss: 0.6363, Train: 0.8250, Test: 0.7168\n",
            "Early stopping:  0.012720481171470219\n",
            "Epoch: 055, Loss: 0.6283, Train: 0.8270, Test: 0.7153\n",
            "Early stopping:  0.012715919645192447\n",
            "Epoch: 056, Loss: 0.6203, Train: 0.8293, Test: 0.7147\n",
            "Early stopping:  0.012687180809936449\n",
            "Epoch: 057, Loss: 0.6125, Train: 0.8328, Test: 0.7158\n",
            "Early stopping:  0.012612299477906286\n",
            "Epoch: 058, Loss: 0.6046, Train: 0.8346, Test: 0.7200\n",
            "Early stopping:  0.012526841800594877\n",
            "Epoch: 059, Loss: 0.5968, Train: 0.8392, Test: 0.7158\n",
            "Early stopping:  0.01243747895502642\n",
            "Epoch: 060, Loss: 0.5893, Train: 0.8393, Test: 0.7147\n",
            "Early stopping:  0.01227743938493364\n",
            "Epoch: 061, Loss: 0.5824, Train: 0.8442, Test: 0.7153\n",
            "Early stopping:  0.011924743115224448\n",
            "Epoch: 062, Loss: 0.5765, Train: 0.8420, Test: 0.7116\n",
            "Early stopping:  0.011190508140719972\n",
            "Epoch: 063, Loss: 0.5700, Train: 0.8499, Test: 0.7142\n",
            "Early stopping:  0.01053473179709125\n",
            "Epoch: 064, Loss: 0.5613, Train: 0.8488, Test: 0.7158\n",
            "Early stopping:  0.010851622699109876\n",
            "Epoch: 065, Loss: 0.5527, Train: 0.8520, Test: 0.7147\n",
            "Early stopping:  0.011824078742901162\n",
            "Epoch: 066, Loss: 0.5470, Train: 0.8546, Test: 0.7142\n",
            "Early stopping:  0.012078544885802655\n",
            "Epoch: 067, Loss: 0.5410, Train: 0.8578, Test: 0.7132\n",
            "Early stopping:  0.011471411745145751\n",
            "Epoch: 068, Loss: 0.5337, Train: 0.8583, Test: 0.7100\n",
            "Early stopping:  0.010599546228192869\n",
            "Epoch: 069, Loss: 0.5275, Train: 0.8626, Test: 0.7095\n",
            "Early stopping:  0.010090544971973701\n",
            "Epoch: 070, Loss: 0.5200, Train: 0.8641, Test: 0.7142\n",
            "Early stopping:  0.010675372327597884\n",
            "Epoch: 071, Loss: 0.5119, Train: 0.8672, Test: 0.7074\n",
            "Early stopping:  0.011394409258811068\n",
            "Epoch: 072, Loss: 0.5071, Train: 0.8699, Test: 0.7132\n",
            "Early stopping:  0.010915250761460283\n",
            "Epoch: 073, Loss: 0.4997, Train: 0.8717, Test: 0.7132\n",
            "Early stopping:  0.010865123085151238\n",
            "Epoch: 074, Loss: 0.4918, Train: 0.8736, Test: 0.7095\n",
            "Early stopping:  0.010884810378358612\n",
            "Epoch: 075, Loss: 0.4880, Train: 0.8749, Test: 0.7142\n",
            "Early stopping:  0.010019456529430575\n",
            "Epoch: 076, Loss: 0.4819, Train: 0.8772, Test: 0.7100\n",
            "Early stopping:  0.009886717206648016\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.77      0.80      0.78       109\n",
            "            CAD_and_CAM       0.74      0.83      0.78        94\n",
            "              Companies       0.56      0.53      0.54        95\n",
            "       Computer_Science       0.84      0.79      0.81       119\n",
            "            Consultants       0.58      0.52      0.55        94\n",
            "           Data_Formats       0.76      0.76      0.76       112\n",
            "    Data_Communications       0.74      0.73      0.74       111\n",
            "              Education       0.90      0.90      0.90       106\n",
            "               Graphics       0.80      0.90      0.85       104\n",
            "               Hardware       0.64      0.73      0.69        94\n",
            "               Internet       0.58      0.65      0.61        82\n",
            "       Mobile_Computing       0.69      0.72      0.71        85\n",
            "             Multimedia       0.67      0.70      0.68        83\n",
            "            Open_Source       0.73      0.74      0.74       101\n",
            "            Programming       0.60      0.57      0.58        97\n",
            "               Robotics       0.95      0.94      0.94       108\n",
            "               Security       0.78      0.79      0.78       103\n",
            "               Software       0.34      0.28      0.31        94\n",
            "                Systems       0.59      0.52      0.55       109\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.70      0.70      0.70      1900\n",
            "           weighted avg       0.71      0.71      0.71      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9526, Train: 0.3528, Test: 0.3526\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8248, Train: 0.4964, Test: 0.4826\n",
            "Early stopping:  0.09042134303308992\n",
            "Epoch: 003, Loss: 2.6527, Train: 0.5453, Test: 0.5342\n",
            "Early stopping:  0.1504940453483352\n",
            "Epoch: 004, Loss: 2.4506, Train: 0.5637, Test: 0.5526\n",
            "Early stopping:  0.21770247345808807\n",
            "Epoch: 005, Loss: 2.2401, Train: 0.5924, Test: 0.5821\n",
            "Early stopping:  0.28573434188729124\n",
            "Epoch: 006, Loss: 2.0301, Train: 0.6166, Test: 0.6068\n",
            "Early stopping:  0.3167750275407988\n",
            "Epoch: 007, Loss: 1.8304, Train: 0.6330, Test: 0.6153\n",
            "Early stopping:  0.3265423956463792\n",
            "Epoch: 008, Loss: 1.6521, Train: 0.6388, Test: 0.6268\n",
            "Early stopping:  0.3174635917596645\n",
            "Epoch: 009, Loss: 1.5018, Train: 0.6450, Test: 0.6337\n",
            "Early stopping:  0.2938620436638376\n",
            "Epoch: 010, Loss: 1.3816, Train: 0.6517, Test: 0.6332\n",
            "Early stopping:  0.2582606117401852\n",
            "Epoch: 011, Loss: 1.2904, Train: 0.6587, Test: 0.6411\n",
            "Early stopping:  0.21528939344960304\n",
            "Epoch: 012, Loss: 1.2238, Train: 0.6632, Test: 0.6447\n",
            "Early stopping:  0.17090297332617893\n",
            "Epoch: 013, Loss: 1.1757, Train: 0.6689, Test: 0.6447\n",
            "Early stopping:  0.13003089882951008\n",
            "Epoch: 014, Loss: 1.1414, Train: 0.6751, Test: 0.6547\n",
            "Early stopping:  0.09574919041604758\n",
            "Epoch: 015, Loss: 1.1147, Train: 0.6784, Test: 0.6579\n",
            "Early stopping:  0.06973360911449963\n",
            "Epoch: 016, Loss: 1.0926, Train: 0.6833, Test: 0.6616\n",
            "Early stopping:  0.051780984664062925\n",
            "Epoch: 017, Loss: 1.0710, Train: 0.6878, Test: 0.6711\n",
            "Early stopping:  0.04104460262584819\n",
            "Epoch: 018, Loss: 1.0502, Train: 0.6947, Test: 0.6774\n",
            "Early stopping:  0.03576259908503665\n",
            "Epoch: 019, Loss: 1.0284, Train: 0.6999, Test: 0.6816\n",
            "Early stopping:  0.03398408345786859\n",
            "Epoch: 020, Loss: 1.0061, Train: 0.7043, Test: 0.6837\n",
            "Early stopping:  0.0340984373123067\n",
            "Epoch: 021, Loss: 0.9838, Train: 0.7114, Test: 0.6879\n",
            "Early stopping:  0.0345828589040015\n",
            "Epoch: 022, Loss: 0.9621, Train: 0.7164, Test: 0.6921\n",
            "Early stopping:  0.034935493181796305\n",
            "Epoch: 023, Loss: 0.9417, Train: 0.7224, Test: 0.6937\n",
            "Early stopping:  0.03436789627975015\n",
            "Epoch: 024, Loss: 0.9232, Train: 0.7286, Test: 0.6911\n",
            "Early stopping:  0.0328626801069908\n",
            "Epoch: 025, Loss: 0.9066, Train: 0.7314, Test: 0.6942\n",
            "Early stopping:  0.030605397094648004\n",
            "Epoch: 026, Loss: 0.8913, Train: 0.7361, Test: 0.6984\n",
            "Early stopping:  0.02799204744150073\n",
            "Epoch: 027, Loss: 0.8775, Train: 0.7407, Test: 0.7011\n",
            "Early stopping:  0.025406558889136\n",
            "Epoch: 028, Loss: 0.8645, Train: 0.7441, Test: 0.7068\n",
            "Early stopping:  0.023202261526150455\n",
            "Epoch: 029, Loss: 0.8524, Train: 0.7491, Test: 0.7147\n",
            "Early stopping:  0.021391058033571785\n",
            "Epoch: 030, Loss: 0.8409, Train: 0.7526, Test: 0.7142\n",
            "Early stopping:  0.019926315308468962\n",
            "Epoch: 031, Loss: 0.8301, Train: 0.7584, Test: 0.7158\n",
            "Early stopping:  0.018729053038584017\n",
            "Epoch: 032, Loss: 0.8197, Train: 0.7599, Test: 0.7179\n",
            "Early stopping:  0.017694525502959035\n",
            "Epoch: 033, Loss: 0.8099, Train: 0.7634, Test: 0.7200\n",
            "Early stopping:  0.016790689000884842\n",
            "Epoch: 034, Loss: 0.8005, Train: 0.7676, Test: 0.7216\n",
            "Early stopping:  0.015957794371135252\n",
            "Epoch: 035, Loss: 0.7915, Train: 0.7700, Test: 0.7247\n",
            "Early stopping:  0.01524984356438826\n",
            "Epoch: 036, Loss: 0.7827, Train: 0.7747, Test: 0.7253\n",
            "Early stopping:  0.014633553499658199\n",
            "Epoch: 037, Loss: 0.7739, Train: 0.7788, Test: 0.7253\n",
            "Early stopping:  0.01421291217272343\n",
            "Epoch: 038, Loss: 0.7653, Train: 0.7825, Test: 0.7242\n",
            "Early stopping:  0.013927338271459285\n",
            "Epoch: 039, Loss: 0.7568, Train: 0.7850, Test: 0.7263\n",
            "Early stopping:  0.013716554206062402\n",
            "Epoch: 040, Loss: 0.7484, Train: 0.7870, Test: 0.7279\n",
            "Early stopping:  0.013534655420886037\n",
            "Epoch: 041, Loss: 0.7401, Train: 0.7897, Test: 0.7289\n",
            "Early stopping:  0.01334604427718446\n",
            "Epoch: 042, Loss: 0.7319, Train: 0.7934, Test: 0.7295\n",
            "Early stopping:  0.013176703146766614\n",
            "Epoch: 043, Loss: 0.7238, Train: 0.7966, Test: 0.7316\n",
            "Early stopping:  0.013049407771519764\n",
            "Epoch: 044, Loss: 0.7155, Train: 0.7967, Test: 0.7347\n",
            "Early stopping:  0.012983857711357022\n",
            "Epoch: 045, Loss: 0.7074, Train: 0.7999, Test: 0.7368\n",
            "Early stopping:  0.012943928135182537\n",
            "Epoch: 046, Loss: 0.6993, Train: 0.8017, Test: 0.7358\n",
            "Early stopping:  0.012921169301663418\n",
            "Epoch: 047, Loss: 0.6911, Train: 0.8039, Test: 0.7347\n",
            "Early stopping:  0.01288887799555754\n",
            "Epoch: 048, Loss: 0.6830, Train: 0.8054, Test: 0.7337\n",
            "Early stopping:  0.012855977014989584\n",
            "Epoch: 049, Loss: 0.6749, Train: 0.8066, Test: 0.7342\n",
            "Early stopping:  0.012843138437636051\n",
            "Epoch: 050, Loss: 0.6668, Train: 0.8082, Test: 0.7347\n",
            "Early stopping:  0.012826288310643436\n",
            "Epoch: 051, Loss: 0.6589, Train: 0.8111, Test: 0.7316\n",
            "Early stopping:  0.012768130494637393\n",
            "Epoch: 052, Loss: 0.6513, Train: 0.8145, Test: 0.7374\n",
            "Early stopping:  0.012582788185821682\n",
            "Epoch: 053, Loss: 0.6436, Train: 0.8153, Test: 0.7342\n",
            "Early stopping:  0.012347192415690345\n",
            "Epoch: 054, Loss: 0.6359, Train: 0.8200, Test: 0.7395\n",
            "Early stopping:  0.012189982518961187\n",
            "Epoch: 055, Loss: 0.6274, Train: 0.8213, Test: 0.7395\n",
            "Early stopping:  0.012399168212733756\n",
            "Epoch: 056, Loss: 0.6193, Train: 0.8239, Test: 0.7363\n",
            "Early stopping:  0.012689222352887003\n",
            "Epoch: 057, Loss: 0.6119, Train: 0.8253, Test: 0.7411\n",
            "Early stopping:  0.012662425490188972\n",
            "Epoch: 058, Loss: 0.6047, Train: 0.8270, Test: 0.7374\n",
            "Early stopping:  0.012303710299257725\n",
            "Epoch: 059, Loss: 0.5973, Train: 0.8301, Test: 0.7426\n",
            "Early stopping:  0.01180345038390427\n",
            "Epoch: 060, Loss: 0.5893, Train: 0.8336, Test: 0.7411\n",
            "Early stopping:  0.011774165648958054\n",
            "Epoch: 061, Loss: 0.5816, Train: 0.8366, Test: 0.7395\n",
            "Early stopping:  0.012028089959943764\n",
            "Epoch: 062, Loss: 0.5742, Train: 0.8399, Test: 0.7405\n",
            "Early stopping:  0.012129826596681482\n",
            "Epoch: 063, Loss: 0.5673, Train: 0.8418, Test: 0.7395\n",
            "Early stopping:  0.011888884237378616\n",
            "Epoch: 064, Loss: 0.5605, Train: 0.8470, Test: 0.7405\n",
            "Early stopping:  0.011381323203331285\n",
            "Epoch: 065, Loss: 0.5538, Train: 0.8463, Test: 0.7432\n",
            "Early stopping:  0.01096470618841327\n",
            "Epoch: 066, Loss: 0.5474, Train: 0.8525, Test: 0.7389\n",
            "Early stopping:  0.010636730094729123\n",
            "Epoch: 067, Loss: 0.5408, Train: 0.8521, Test: 0.7437\n",
            "Early stopping:  0.010458122707855108\n",
            "Epoch: 068, Loss: 0.5337, Train: 0.8564, Test: 0.7389\n",
            "Early stopping:  0.010514875164944428\n",
            "Epoch: 069, Loss: 0.5258, Train: 0.8629, Test: 0.7416\n",
            "Early stopping:  0.011007531198250296\n",
            "Epoch: 070, Loss: 0.5187, Train: 0.8607, Test: 0.7395\n",
            "Early stopping:  0.011432718231150371\n",
            "Epoch: 071, Loss: 0.5128, Train: 0.8675, Test: 0.7321\n",
            "Early stopping:  0.01124798007129107\n",
            "Epoch: 072, Loss: 0.5059, Train: 0.8686, Test: 0.7400\n",
            "Early stopping:  0.010868895990012088\n",
            "Epoch: 073, Loss: 0.4982, Train: 0.8701, Test: 0.7379\n",
            "Early stopping:  0.010774524917188439\n",
            "Epoch: 074, Loss: 0.4913, Train: 0.8742, Test: 0.7379\n",
            "Early stopping:  0.010982380311275942\n",
            "Epoch: 075, Loss: 0.4860, Train: 0.8729, Test: 0.7384\n",
            "Early stopping:  0.010790216225197527\n",
            "Epoch: 076, Loss: 0.4801, Train: 0.8768, Test: 0.7363\n",
            "Early stopping:  0.010125446258894508\n",
            "Epoch: 077, Loss: 0.4729, Train: 0.8793, Test: 0.7368\n",
            "Early stopping:  0.009777109291950655\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.83      0.83      0.83        93\n",
            "            CAD_and_CAM       0.82      0.83      0.83       101\n",
            "              Companies       0.54      0.63      0.58        82\n",
            "       Computer_Science       0.79      0.77      0.78        93\n",
            "            Consultants       0.70      0.68      0.69        98\n",
            "           Data_Formats       0.81      0.88      0.84       110\n",
            "    Data_Communications       0.68      0.80      0.73        98\n",
            "              Education       0.88      0.91      0.90        94\n",
            "               Graphics       0.89      0.94      0.91       111\n",
            "               Hardware       0.69      0.72      0.71       101\n",
            "               Internet       0.69      0.65      0.67       103\n",
            "       Mobile_Computing       0.80      0.68      0.74        94\n",
            "             Multimedia       0.67      0.73      0.70        89\n",
            "            Open_Source       0.69      0.74      0.71        92\n",
            "            Programming       0.58      0.55      0.57       109\n",
            "               Robotics       0.93      0.88      0.90       116\n",
            "               Security       0.88      0.79      0.83        95\n",
            "               Software       0.40      0.40      0.40       100\n",
            "                Systems       0.72      0.57      0.64       121\n",
            "\n",
            "               accuracy                           0.74      1900\n",
            "              macro avg       0.74      0.74      0.73      1900\n",
            "           weighted avg       0.74      0.74      0.74      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9507, Train: 0.5125, Test: 0.4979\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8042, Train: 0.5209, Test: 0.5158\n",
            "Early stopping:  0.10362477055979472\n",
            "Epoch: 003, Loss: 2.6301, Train: 0.5366, Test: 0.5342\n",
            "Early stopping:  0.16050560052420584\n",
            "Epoch: 004, Loss: 2.4255, Train: 0.5630, Test: 0.5537\n",
            "Early stopping:  0.22650214937044416\n",
            "Epoch: 005, Loss: 2.2099, Train: 0.5967, Test: 0.5695\n",
            "Early stopping:  0.2950112791417871\n",
            "Epoch: 006, Loss: 1.9993, Train: 0.6236, Test: 0.6079\n",
            "Early stopping:  0.3212221017964812\n",
            "Epoch: 007, Loss: 1.8024, Train: 0.6403, Test: 0.6253\n",
            "Early stopping:  0.3291880781889459\n",
            "Epoch: 008, Loss: 1.6256, Train: 0.6474, Test: 0.6289\n",
            "Early stopping:  0.3176569824371903\n",
            "Epoch: 009, Loss: 1.4778, Train: 0.6538, Test: 0.6363\n",
            "Early stopping:  0.2912846101436007\n",
            "Epoch: 010, Loss: 1.3617, Train: 0.6592, Test: 0.6447\n",
            "Early stopping:  0.25424351820862395\n",
            "Epoch: 011, Loss: 1.2726, Train: 0.6670, Test: 0.6463\n",
            "Early stopping:  0.211074014222303\n",
            "Epoch: 012, Loss: 1.2073, Train: 0.6692, Test: 0.6526\n",
            "Early stopping:  0.16669508361690477\n",
            "Epoch: 013, Loss: 1.1609, Train: 0.6749, Test: 0.6568\n",
            "Early stopping:  0.12650717199289316\n",
            "Epoch: 014, Loss: 1.1260, Train: 0.6801, Test: 0.6611\n",
            "Early stopping:  0.09376533221386898\n",
            "Epoch: 015, Loss: 1.0991, Train: 0.6846, Test: 0.6632\n",
            "Early stopping:  0.06876112304884693\n",
            "Epoch: 016, Loss: 1.0766, Train: 0.6899, Test: 0.6653\n",
            "Early stopping:  0.051639820711987854\n",
            "Epoch: 017, Loss: 1.0547, Train: 0.6991, Test: 0.6721\n",
            "Early stopping:  0.04159879958279852\n",
            "Epoch: 018, Loss: 1.0339, Train: 0.7049, Test: 0.6784\n",
            "Early stopping:  0.036184589116150044\n",
            "Epoch: 019, Loss: 1.0123, Train: 0.7095, Test: 0.6821\n",
            "Early stopping:  0.0342175793594026\n",
            "Epoch: 020, Loss: 0.9908, Train: 0.7112, Test: 0.6847\n",
            "Early stopping:  0.03384949069208448\n",
            "Epoch: 021, Loss: 0.9700, Train: 0.7193, Test: 0.6868\n",
            "Early stopping:  0.03362449469652095\n",
            "Epoch: 022, Loss: 0.9500, Train: 0.7258, Test: 0.6895\n",
            "Early stopping:  0.0332373553195799\n",
            "Epoch: 023, Loss: 0.9314, Train: 0.7295, Test: 0.6905\n",
            "Early stopping:  0.03203431200208414\n",
            "Epoch: 024, Loss: 0.9140, Train: 0.7333, Test: 0.6947\n",
            "Early stopping:  0.030380769941945517\n",
            "Epoch: 025, Loss: 0.8982, Train: 0.7371, Test: 0.6947\n",
            "Early stopping:  0.02841701187685132\n",
            "Epoch: 026, Loss: 0.8831, Train: 0.7421, Test: 0.6937\n",
            "Early stopping:  0.026428886421516442\n",
            "Epoch: 027, Loss: 0.8693, Train: 0.7454, Test: 0.6958\n",
            "Early stopping:  0.02455083699753835\n",
            "Epoch: 028, Loss: 0.8560, Train: 0.7517, Test: 0.6974\n",
            "Early stopping:  0.022920355832103584\n",
            "Epoch: 029, Loss: 0.8435, Train: 0.7557, Test: 0.7016\n",
            "Early stopping:  0.021577637918944657\n",
            "Epoch: 030, Loss: 0.8320, Train: 0.7595, Test: 0.7021\n",
            "Early stopping:  0.020267230604387496\n",
            "Epoch: 031, Loss: 0.8210, Train: 0.7637, Test: 0.7032\n",
            "Early stopping:  0.019110745694362004\n",
            "Epoch: 032, Loss: 0.8108, Train: 0.7695, Test: 0.7026\n",
            "Early stopping:  0.017896224103631165\n",
            "Epoch: 033, Loss: 0.8011, Train: 0.7728, Test: 0.7074\n",
            "Early stopping:  0.016768801842211477\n",
            "Epoch: 034, Loss: 0.7922, Train: 0.7763, Test: 0.7079\n",
            "Early stopping:  0.015720843417809126\n",
            "Epoch: 035, Loss: 0.7836, Train: 0.7812, Test: 0.7084\n",
            "Early stopping:  0.014743840351819682\n",
            "Epoch: 036, Loss: 0.7754, Train: 0.7846, Test: 0.7111\n",
            "Early stopping:  0.013963231254866622\n",
            "Epoch: 037, Loss: 0.7673, Train: 0.7879, Test: 0.7100\n",
            "Early stopping:  0.013374894979122878\n",
            "Epoch: 038, Loss: 0.7594, Train: 0.7916, Test: 0.7116\n",
            "Early stopping:  0.01298123531487609\n",
            "Epoch: 039, Loss: 0.7515, Train: 0.7932, Test: 0.7132\n",
            "Early stopping:  0.012700667004846646\n",
            "Epoch: 040, Loss: 0.7436, Train: 0.7953, Test: 0.7079\n",
            "Early stopping:  0.012556597805327531\n",
            "Epoch: 041, Loss: 0.7357, Train: 0.7983, Test: 0.7079\n",
            "Early stopping:  0.012464321363198588\n",
            "Epoch: 042, Loss: 0.7279, Train: 0.7999, Test: 0.7100\n",
            "Early stopping:  0.012425005084003501\n",
            "Epoch: 043, Loss: 0.7201, Train: 0.8008, Test: 0.7079\n",
            "Early stopping:  0.012384950941493129\n",
            "Epoch: 044, Loss: 0.7122, Train: 0.8025, Test: 0.7063\n",
            "Early stopping:  0.012385499023406119\n",
            "Epoch: 045, Loss: 0.7042, Train: 0.8037, Test: 0.7084\n",
            "Early stopping:  0.01246649248868108\n",
            "Epoch: 046, Loss: 0.6961, Train: 0.8057, Test: 0.7079\n",
            "Early stopping:  0.01259794773133784\n",
            "Epoch: 047, Loss: 0.6879, Train: 0.8083, Test: 0.7042\n",
            "Early stopping:  0.01273319149125172\n",
            "Epoch: 048, Loss: 0.6797, Train: 0.8096, Test: 0.7063\n",
            "Early stopping:  0.012837557715948772\n",
            "Epoch: 049, Loss: 0.6716, Train: 0.8113, Test: 0.7047\n",
            "Early stopping:  0.012876775104141727\n",
            "Epoch: 050, Loss: 0.6635, Train: 0.8125, Test: 0.7058\n",
            "Early stopping:  0.012874524364585601\n",
            "Epoch: 051, Loss: 0.6555, Train: 0.8157, Test: 0.7063\n",
            "Early stopping:  0.012833281802768673\n",
            "Epoch: 052, Loss: 0.6475, Train: 0.8179, Test: 0.7079\n",
            "Early stopping:  0.012757202432414362\n",
            "Epoch: 053, Loss: 0.6396, Train: 0.8200, Test: 0.7068\n",
            "Early stopping:  0.012667422726687886\n",
            "Epoch: 054, Loss: 0.6318, Train: 0.8225, Test: 0.7089\n",
            "Early stopping:  0.012526291541664679\n",
            "Epoch: 055, Loss: 0.6242, Train: 0.8255, Test: 0.7089\n",
            "Early stopping:  0.012371815873990737\n",
            "Epoch: 056, Loss: 0.6167, Train: 0.8264, Test: 0.7074\n",
            "Early stopping:  0.012176028605185211\n",
            "Epoch: 057, Loss: 0.6094, Train: 0.8287, Test: 0.7089\n",
            "Early stopping:  0.011931183724449346\n",
            "Epoch: 058, Loss: 0.6030, Train: 0.8292, Test: 0.7063\n",
            "Early stopping:  0.011448495338572843\n",
            "Epoch: 059, Loss: 0.5976, Train: 0.8317, Test: 0.7089\n",
            "Early stopping:  0.010574647988395363\n",
            "Epoch: 060, Loss: 0.5928, Train: 0.8346, Test: 0.7084\n",
            "Early stopping:  0.009467797313381389\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.83      0.81       106\n",
            "            CAD_and_CAM       0.73      0.80      0.76        94\n",
            "              Companies       0.52      0.56      0.54       112\n",
            "       Computer_Science       0.77      0.75      0.76       106\n",
            "            Consultants       0.57      0.58      0.58       103\n",
            "           Data_Formats       0.80      0.87      0.84        87\n",
            "    Data_Communications       0.68      0.70      0.69        91\n",
            "              Education       0.91      0.88      0.89       100\n",
            "               Graphics       0.83      0.92      0.87        91\n",
            "               Hardware       0.69      0.62      0.65       111\n",
            "               Internet       0.68      0.53      0.59        99\n",
            "       Mobile_Computing       0.82      0.77      0.80       109\n",
            "             Multimedia       0.66      0.74      0.70        89\n",
            "            Open_Source       0.63      0.79      0.70        98\n",
            "            Programming       0.59      0.55      0.57       111\n",
            "               Robotics       0.90      0.93      0.91       103\n",
            "               Security       0.74      0.81      0.78        86\n",
            "               Software       0.48      0.28      0.35       105\n",
            "                Systems       0.64      0.66      0.65        99\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.71      0.71      0.71      1900\n",
            "           weighted avg       0.70      0.71      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9472, Train: 0.3855, Test: 0.3753\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8209, Train: 0.4976, Test: 0.4932\n",
            "Early stopping:  0.08932198463653176\n",
            "Epoch: 003, Loss: 2.6613, Train: 0.5437, Test: 0.5553\n",
            "Early stopping:  0.14326582218744668\n",
            "Epoch: 004, Loss: 2.4789, Train: 0.5792, Test: 0.5753\n",
            "Early stopping:  0.202595420735739\n",
            "Epoch: 005, Loss: 2.2823, Train: 0.5979, Test: 0.5984\n",
            "Early stopping:  0.26522917461216344\n",
            "Epoch: 006, Loss: 2.0800, Train: 0.6170, Test: 0.6158\n",
            "Early stopping:  0.29452316487752844\n",
            "Epoch: 007, Loss: 1.8840, Train: 0.6312, Test: 0.6384\n",
            "Early stopping:  0.3089387892771784\n",
            "Epoch: 008, Loss: 1.7058, Train: 0.6405, Test: 0.6453\n",
            "Early stopping:  0.3075408107970936\n",
            "Epoch: 009, Loss: 1.5521, Train: 0.6430, Test: 0.6526\n",
            "Early stopping:  0.29048909675959766\n",
            "Epoch: 010, Loss: 1.4272, Train: 0.6497, Test: 0.6516\n",
            "Early stopping:  0.2598541189614128\n",
            "Epoch: 011, Loss: 1.3310, Train: 0.6558, Test: 0.6611\n",
            "Early stopping:  0.22043496592779271\n",
            "Epoch: 012, Loss: 1.2576, Train: 0.6611, Test: 0.6611\n",
            "Early stopping:  0.17852402103870552\n",
            "Epoch: 013, Loss: 1.2030, Train: 0.6659, Test: 0.6684\n",
            "Early stopping:  0.1389659263617282\n",
            "Epoch: 014, Loss: 1.1620, Train: 0.6729, Test: 0.6732\n",
            "Early stopping:  0.10553637700527206\n",
            "Epoch: 015, Loss: 1.1287, Train: 0.6789, Test: 0.6784\n",
            "Early stopping:  0.08008312951095904\n",
            "Epoch: 016, Loss: 1.1011, Train: 0.6839, Test: 0.6805\n",
            "Early stopping:  0.06180093529847986\n",
            "Epoch: 017, Loss: 1.0761, Train: 0.6900, Test: 0.6853\n",
            "Early stopping:  0.0500213701418422\n",
            "Epoch: 018, Loss: 1.0524, Train: 0.6926, Test: 0.6905\n",
            "Early stopping:  0.043092516945482266\n",
            "Epoch: 019, Loss: 1.0299, Train: 0.6979, Test: 0.6921\n",
            "Early stopping:  0.03895479503179055\n",
            "Epoch: 020, Loss: 1.0078, Train: 0.7030, Test: 0.6953\n",
            "Early stopping:  0.036820300042953245\n",
            "Epoch: 021, Loss: 0.9868, Train: 0.7082, Test: 0.7021\n",
            "Early stopping:  0.0353176782626956\n",
            "Epoch: 022, Loss: 0.9665, Train: 0.7163, Test: 0.7016\n",
            "Early stopping:  0.03397999346990762\n",
            "Epoch: 023, Loss: 0.9472, Train: 0.7208, Test: 0.7074\n",
            "Early stopping:  0.0326895124677112\n",
            "Epoch: 024, Loss: 0.9292, Train: 0.7241, Test: 0.7063\n",
            "Early stopping:  0.031104727818809918\n",
            "Epoch: 025, Loss: 0.9122, Train: 0.7295, Test: 0.7100\n",
            "Early stopping:  0.029487339345826348\n",
            "Epoch: 026, Loss: 0.8965, Train: 0.7354, Test: 0.7105\n",
            "Early stopping:  0.027690829263894275\n",
            "Epoch: 027, Loss: 0.8817, Train: 0.7420, Test: 0.7137\n",
            "Early stopping:  0.025902820382761524\n",
            "Epoch: 028, Loss: 0.8677, Train: 0.7484, Test: 0.7168\n",
            "Early stopping:  0.024288704027169712\n",
            "Epoch: 029, Loss: 0.8547, Train: 0.7514, Test: 0.7153\n",
            "Early stopping:  0.022760083891308985\n",
            "Epoch: 030, Loss: 0.8423, Train: 0.7559, Test: 0.7174\n",
            "Early stopping:  0.021435855799812515\n",
            "Epoch: 031, Loss: 0.8308, Train: 0.7603, Test: 0.7163\n",
            "Early stopping:  0.02014118510594603\n",
            "Epoch: 032, Loss: 0.8199, Train: 0.7637, Test: 0.7216\n",
            "Early stopping:  0.018907687195575455\n",
            "Epoch: 033, Loss: 0.8098, Train: 0.7674, Test: 0.7237\n",
            "Early stopping:  0.017732042334596628\n",
            "Epoch: 034, Loss: 0.8001, Train: 0.7726, Test: 0.7258\n",
            "Early stopping:  0.01664904078473388\n",
            "Epoch: 035, Loss: 0.7910, Train: 0.7762, Test: 0.7232\n",
            "Early stopping:  0.015715728183901954\n",
            "Epoch: 036, Loss: 0.7822, Train: 0.7800, Test: 0.7221\n",
            "Early stopping:  0.014917419827522357\n",
            "Epoch: 037, Loss: 0.7736, Train: 0.7822, Test: 0.7263\n",
            "Early stopping:  0.014293884542682326\n",
            "Epoch: 038, Loss: 0.7653, Train: 0.7847, Test: 0.7284\n",
            "Early stopping:  0.013780792586091032\n",
            "Epoch: 039, Loss: 0.7569, Train: 0.7876, Test: 0.7295\n",
            "Early stopping:  0.013452686206424274\n",
            "Epoch: 040, Loss: 0.7488, Train: 0.7903, Test: 0.7300\n",
            "Early stopping:  0.013182831189034445\n",
            "Epoch: 041, Loss: 0.7407, Train: 0.7913, Test: 0.7284\n",
            "Early stopping:  0.013006797923829689\n",
            "Epoch: 042, Loss: 0.7327, Train: 0.7949, Test: 0.7274\n",
            "Early stopping:  0.012878327135195499\n",
            "Epoch: 043, Loss: 0.7246, Train: 0.7959, Test: 0.7311\n",
            "Early stopping:  0.012775786293840921\n",
            "Epoch: 044, Loss: 0.7169, Train: 0.7974, Test: 0.7326\n",
            "Early stopping:  0.012636287752441971\n",
            "Epoch: 045, Loss: 0.7096, Train: 0.8013, Test: 0.7321\n",
            "Early stopping:  0.012339200230192048\n",
            "Epoch: 046, Loss: 0.7007, Train: 0.8049, Test: 0.7321\n",
            "Early stopping:  0.012499067848589309\n",
            "Epoch: 047, Loss: 0.6916, Train: 0.8058, Test: 0.7300\n",
            "Early stopping:  0.013014030554432093\n",
            "Epoch: 048, Loss: 0.6838, Train: 0.8071, Test: 0.7300\n",
            "Early stopping:  0.013322123346969398\n",
            "Epoch: 049, Loss: 0.6764, Train: 0.8093, Test: 0.7342\n",
            "Early stopping:  0.013183033140394691\n",
            "Epoch: 050, Loss: 0.6683, Train: 0.8120, Test: 0.7363\n",
            "Early stopping:  0.01265673919409083\n",
            "Epoch: 051, Loss: 0.6597, Train: 0.8149, Test: 0.7389\n",
            "Early stopping:  0.01253792725487627\n",
            "Epoch: 052, Loss: 0.6521, Train: 0.8159, Test: 0.7353\n",
            "Early stopping:  0.012647289355144972\n",
            "Epoch: 053, Loss: 0.6448, Train: 0.8195, Test: 0.7389\n",
            "Early stopping:  0.012543984452297322\n",
            "Epoch: 054, Loss: 0.6367, Train: 0.8214, Test: 0.7353\n",
            "Early stopping:  0.012346339131739532\n",
            "Epoch: 055, Loss: 0.6286, Train: 0.8249, Test: 0.7342\n",
            "Early stopping:  0.0122632482150304\n",
            "Epoch: 056, Loss: 0.6213, Train: 0.8249, Test: 0.7337\n",
            "Early stopping:  0.012293444448131276\n",
            "Epoch: 057, Loss: 0.6144, Train: 0.8279, Test: 0.7321\n",
            "Early stopping:  0.0120754467286583\n",
            "Epoch: 058, Loss: 0.6078, Train: 0.8274, Test: 0.7326\n",
            "Early stopping:  0.011416738686772102\n",
            "Epoch: 059, Loss: 0.6024, Train: 0.8322, Test: 0.7316\n",
            "Early stopping:  0.010457120558010367\n",
            "Epoch: 060, Loss: 0.5975, Train: 0.8321, Test: 0.7311\n",
            "Early stopping:  0.00946391926248485\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.84      0.85      0.85       107\n",
            "            CAD_and_CAM       0.75      0.87      0.81       106\n",
            "              Companies       0.58      0.51      0.54        88\n",
            "       Computer_Science       0.70      0.74      0.72        91\n",
            "            Consultants       0.54      0.73      0.62        89\n",
            "           Data_Formats       0.77      0.87      0.81        98\n",
            "    Data_Communications       0.78      0.80      0.79       110\n",
            "              Education       0.85      0.86      0.85        99\n",
            "               Graphics       0.83      0.94      0.88       108\n",
            "               Hardware       0.73      0.59      0.66       101\n",
            "               Internet       0.67      0.68      0.68       100\n",
            "       Mobile_Computing       0.82      0.83      0.82       101\n",
            "             Multimedia       0.68      0.72      0.70       107\n",
            "            Open_Source       0.69      0.77      0.73        97\n",
            "            Programming       0.62      0.49      0.55       123\n",
            "               Robotics       0.97      0.94      0.95        89\n",
            "               Security       0.87      0.77      0.82        88\n",
            "               Software       0.43      0.29      0.35        96\n",
            "                Systems       0.68      0.64      0.66       102\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.73      0.73      0.73      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9531, Train: 0.3882, Test: 0.3826\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8282, Train: 0.5370, Test: 0.5279\n",
            "Early stopping:  0.08831113462184648\n",
            "Epoch: 003, Loss: 2.6547, Train: 0.5446, Test: 0.5484\n",
            "Early stopping:  0.1498345993932378\n",
            "Epoch: 004, Loss: 2.4527, Train: 0.5618, Test: 0.5658\n",
            "Early stopping:  0.21737257712663113\n",
            "Epoch: 005, Loss: 2.2438, Train: 0.5950, Test: 0.5858\n",
            "Early stopping:  0.2849776676685221\n",
            "Epoch: 006, Loss: 2.0318, Train: 0.6218, Test: 0.6142\n",
            "Early stopping:  0.3170503363794623\n",
            "Epoch: 007, Loss: 1.8296, Train: 0.6384, Test: 0.6274\n",
            "Early stopping:  0.3274767517098677\n",
            "Epoch: 008, Loss: 1.6482, Train: 0.6422, Test: 0.6447\n",
            "Early stopping:  0.32001084084150005\n",
            "Epoch: 009, Loss: 1.4949, Train: 0.6463, Test: 0.6400\n",
            "Early stopping:  0.2980616492425498\n",
            "Epoch: 010, Loss: 1.3750, Train: 0.6517, Test: 0.6437\n",
            "Early stopping:  0.26191470238513215\n",
            "Epoch: 011, Loss: 1.2851, Train: 0.6607, Test: 0.6479\n",
            "Early stopping:  0.21734166375428096\n",
            "Epoch: 012, Loss: 1.2197, Train: 0.6630, Test: 0.6516\n",
            "Early stopping:  0.17092097729209207\n",
            "Epoch: 013, Loss: 1.1743, Train: 0.6691, Test: 0.6516\n",
            "Early stopping:  0.1280626134914338\n",
            "Epoch: 014, Loss: 1.1407, Train: 0.6732, Test: 0.6553\n",
            "Early stopping:  0.09332423419917778\n",
            "Epoch: 015, Loss: 1.1145, Train: 0.6766, Test: 0.6632\n",
            "Early stopping:  0.06754556762670398\n",
            "Epoch: 016, Loss: 1.0916, Train: 0.6818, Test: 0.6689\n",
            "Early stopping:  0.05044806601571917\n",
            "Epoch: 017, Loss: 1.0696, Train: 0.6870, Test: 0.6737\n",
            "Early stopping:  0.04101971407539372\n",
            "Epoch: 018, Loss: 1.0484, Train: 0.6938, Test: 0.6779\n",
            "Early stopping:  0.03630606085562599\n",
            "Epoch: 019, Loss: 1.0267, Train: 0.7003, Test: 0.6842\n",
            "Early stopping:  0.03459331604394732\n",
            "Epoch: 020, Loss: 1.0051, Train: 0.7054, Test: 0.6874\n",
            "Early stopping:  0.034138575286957684\n",
            "Epoch: 021, Loss: 0.9843, Train: 0.7125, Test: 0.6895\n",
            "Early stopping:  0.03381347784045233\n",
            "Epoch: 022, Loss: 0.9642, Train: 0.7191, Test: 0.6911\n",
            "Early stopping:  0.03334833823694975\n",
            "Epoch: 023, Loss: 0.9454, Train: 0.7245, Test: 0.6947\n",
            "Early stopping:  0.03219606046570571\n",
            "Epoch: 024, Loss: 0.9279, Train: 0.7280, Test: 0.7011\n",
            "Early stopping:  0.030585283402244173\n",
            "Epoch: 025, Loss: 0.9118, Train: 0.7312, Test: 0.7026\n",
            "Early stopping:  0.02869516178911834\n",
            "Epoch: 026, Loss: 0.8970, Train: 0.7380, Test: 0.7047\n",
            "Early stopping:  0.026592245597673748\n",
            "Epoch: 027, Loss: 0.8832, Train: 0.7416, Test: 0.7068\n",
            "Early stopping:  0.024607291599012544\n",
            "Epoch: 028, Loss: 0.8703, Train: 0.7443, Test: 0.7121\n",
            "Early stopping:  0.022777093085583153\n",
            "Epoch: 029, Loss: 0.8578, Train: 0.7493, Test: 0.7121\n",
            "Early stopping:  0.021302862349748358\n",
            "Epoch: 030, Loss: 0.8460, Train: 0.7518, Test: 0.7132\n",
            "Early stopping:  0.02013131436592991\n",
            "Epoch: 031, Loss: 0.8347, Train: 0.7561, Test: 0.7163\n",
            "Early stopping:  0.019172384669583552\n",
            "Epoch: 032, Loss: 0.8239, Train: 0.7601, Test: 0.7147\n",
            "Early stopping:  0.018319087576160728\n",
            "Epoch: 033, Loss: 0.8141, Train: 0.7638, Test: 0.7163\n",
            "Early stopping:  0.017332949871308644\n",
            "Epoch: 034, Loss: 0.8049, Train: 0.7676, Test: 0.7226\n",
            "Early stopping:  0.016271900326262925\n",
            "Epoch: 035, Loss: 0.7966, Train: 0.7717, Test: 0.7258\n",
            "Early stopping:  0.015073816959378792\n",
            "Epoch: 036, Loss: 0.7886, Train: 0.7730, Test: 0.7274\n",
            "Early stopping:  0.013949130194778003\n",
            "Epoch: 037, Loss: 0.7809, Train: 0.7755, Test: 0.7289\n",
            "Early stopping:  0.013079185782476986\n",
            "Epoch: 038, Loss: 0.7731, Train: 0.7792, Test: 0.7274\n",
            "Early stopping:  0.012550344179199396\n",
            "Epoch: 039, Loss: 0.7652, Train: 0.7816, Test: 0.7295\n",
            "Early stopping:  0.012370896927088102\n",
            "Epoch: 040, Loss: 0.7573, Train: 0.7857, Test: 0.7332\n",
            "Early stopping:  0.012377115886500882\n",
            "Epoch: 041, Loss: 0.7492, Train: 0.7883, Test: 0.7337\n",
            "Early stopping:  0.012521664347710173\n",
            "Epoch: 042, Loss: 0.7410, Train: 0.7922, Test: 0.7342\n",
            "Early stopping:  0.012690765351399812\n",
            "Epoch: 043, Loss: 0.7326, Train: 0.7946, Test: 0.7326\n",
            "Early stopping:  0.012893501851009069\n",
            "Epoch: 044, Loss: 0.7243, Train: 0.7984, Test: 0.7363\n",
            "Early stopping:  0.013069664113638371\n",
            "Epoch: 045, Loss: 0.7159, Train: 0.8004, Test: 0.7358\n",
            "Early stopping:  0.0131855985405747\n",
            "Epoch: 046, Loss: 0.7075, Train: 0.8018, Test: 0.7358\n",
            "Early stopping:  0.013247960529716102\n",
            "Epoch: 047, Loss: 0.6990, Train: 0.8051, Test: 0.7342\n",
            "Early stopping:  0.0132746027094465\n",
            "Epoch: 048, Loss: 0.6905, Train: 0.8066, Test: 0.7347\n",
            "Early stopping:  0.01332175402300176\n",
            "Epoch: 049, Loss: 0.6821, Train: 0.8095, Test: 0.7342\n",
            "Early stopping:  0.013343579799069633\n",
            "Epoch: 050, Loss: 0.6739, Train: 0.8118, Test: 0.7337\n",
            "Early stopping:  0.013296422959490677\n",
            "Epoch: 051, Loss: 0.6659, Train: 0.8142, Test: 0.7332\n",
            "Early stopping:  0.013124013480346362\n",
            "Epoch: 052, Loss: 0.6583, Train: 0.8175, Test: 0.7316\n",
            "Early stopping:  0.012780764225465173\n",
            "Epoch: 053, Loss: 0.6508, Train: 0.8176, Test: 0.7363\n",
            "Early stopping:  0.012362833072776847\n",
            "Epoch: 054, Loss: 0.6430, Train: 0.8205, Test: 0.7342\n",
            "Early stopping:  0.012140824046807396\n",
            "Epoch: 055, Loss: 0.6344, Train: 0.8229, Test: 0.7358\n",
            "Early stopping:  0.012370254617089505\n",
            "Epoch: 056, Loss: 0.6264, Train: 0.8270, Test: 0.7353\n",
            "Early stopping:  0.01268043501003039\n",
            "Epoch: 057, Loss: 0.6192, Train: 0.8303, Test: 0.7342\n",
            "Early stopping:  0.012637912888098133\n",
            "Epoch: 058, Loss: 0.6118, Train: 0.8322, Test: 0.7347\n",
            "Early stopping:  0.012286813356771594\n",
            "Epoch: 059, Loss: 0.6041, Train: 0.8354, Test: 0.7326\n",
            "Early stopping:  0.011885764067243761\n",
            "Epoch: 060, Loss: 0.5968, Train: 0.8378, Test: 0.7332\n",
            "Early stopping:  0.011757458754118776\n",
            "Epoch: 061, Loss: 0.5897, Train: 0.8408, Test: 0.7353\n",
            "Early stopping:  0.011698393212677762\n",
            "Epoch: 062, Loss: 0.5827, Train: 0.8432, Test: 0.7316\n",
            "Early stopping:  0.011468752463305356\n",
            "Epoch: 063, Loss: 0.5752, Train: 0.8493, Test: 0.7337\n",
            "Early stopping:  0.011365313307289371\n",
            "Epoch: 064, Loss: 0.5677, Train: 0.8479, Test: 0.7353\n",
            "Early stopping:  0.011491790490671196\n",
            "Epoch: 065, Loss: 0.5608, Train: 0.8537, Test: 0.7316\n",
            "Early stopping:  0.011511625036450058\n",
            "Epoch: 066, Loss: 0.5546, Train: 0.8520, Test: 0.7347\n",
            "Early stopping:  0.011180773323994468\n",
            "Epoch: 067, Loss: 0.5485, Train: 0.8562, Test: 0.7263\n",
            "Early stopping:  0.010524650314172755\n",
            "Epoch: 068, Loss: 0.5428, Train: 0.8538, Test: 0.7347\n",
            "Early stopping:  0.00982246409780903\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.82      0.80        91\n",
            "            CAD_and_CAM       0.85      0.86      0.85       106\n",
            "              Companies       0.67      0.42      0.51       105\n",
            "       Computer_Science       0.77      0.78      0.77       101\n",
            "            Consultants       0.57      0.73      0.64       112\n",
            "           Data_Formats       0.90      0.82      0.86        96\n",
            "    Data_Communications       0.72      0.81      0.77        86\n",
            "              Education       0.88      0.86      0.87       124\n",
            "               Graphics       0.81      0.88      0.84        97\n",
            "               Hardware       0.77      0.67      0.72        89\n",
            "               Internet       0.70      0.68      0.69       103\n",
            "       Mobile_Computing       0.75      0.84      0.79        93\n",
            "             Multimedia       0.66      0.65      0.66       100\n",
            "            Open_Source       0.69      0.81      0.74       100\n",
            "            Programming       0.61      0.58      0.60        89\n",
            "               Robotics       0.94      0.91      0.92       100\n",
            "               Security       0.78      0.86      0.82       104\n",
            "               Software       0.36      0.33      0.34        95\n",
            "                Systems       0.71      0.61      0.66       109\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.73      0.73      0.73      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9472, Train: 0.3857, Test: 0.3674\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8102, Train: 0.4100, Test: 0.3968\n",
            "Early stopping:  0.09690909171806887\n",
            "Epoch: 003, Loss: 2.6376, Train: 0.4862, Test: 0.4758\n",
            "Early stopping:  0.1551868153779312\n",
            "Epoch: 004, Loss: 2.4369, Train: 0.5512, Test: 0.5442\n",
            "Early stopping:  0.2207235803184367\n",
            "Epoch: 005, Loss: 2.2273, Train: 0.5899, Test: 0.5758\n",
            "Early stopping:  0.2876625727444257\n",
            "Epoch: 006, Loss: 2.0207, Train: 0.6151, Test: 0.6021\n",
            "Early stopping:  0.3147435914749436\n",
            "Epoch: 007, Loss: 1.8254, Train: 0.6305, Test: 0.6226\n",
            "Early stopping:  0.3226418936111478\n",
            "Epoch: 008, Loss: 1.6503, Train: 0.6376, Test: 0.6247\n",
            "Early stopping:  0.3124749258430127\n",
            "Epoch: 009, Loss: 1.5016, Train: 0.6453, Test: 0.6337\n",
            "Early stopping:  0.28863846319145736\n",
            "Epoch: 010, Loss: 1.3827, Train: 0.6516, Test: 0.6468\n",
            "Early stopping:  0.2540832886446053\n",
            "Epoch: 011, Loss: 1.2921, Train: 0.6603, Test: 0.6579\n",
            "Early stopping:  0.2126189508951778\n",
            "Epoch: 012, Loss: 1.2260, Train: 0.6649, Test: 0.6621\n",
            "Early stopping:  0.1692600885928723\n",
            "Epoch: 013, Loss: 1.1782, Train: 0.6684, Test: 0.6595\n",
            "Early stopping:  0.12897969019154495\n",
            "Epoch: 014, Loss: 1.1436, Train: 0.6732, Test: 0.6637\n",
            "Early stopping:  0.09524844870982796\n",
            "Epoch: 015, Loss: 1.1163, Train: 0.6772, Test: 0.6705\n",
            "Early stopping:  0.06972924329199104\n",
            "Epoch: 016, Loss: 1.0927, Train: 0.6854, Test: 0.6758\n",
            "Early stopping:  0.05249441249143064\n",
            "Epoch: 017, Loss: 1.0702, Train: 0.6936, Test: 0.6763\n",
            "Early stopping:  0.04237852887176429\n",
            "Epoch: 018, Loss: 1.0480, Train: 0.6987, Test: 0.6779\n",
            "Early stopping:  0.03756333035241403\n",
            "Epoch: 019, Loss: 1.0257, Train: 0.7057, Test: 0.6826\n",
            "Early stopping:  0.03572923857403476\n",
            "Epoch: 020, Loss: 1.0037, Train: 0.7107, Test: 0.6879\n",
            "Early stopping:  0.03519982941127694\n",
            "Epoch: 021, Loss: 0.9823, Train: 0.7171, Test: 0.6921\n",
            "Early stopping:  0.03478756032166093\n",
            "Epoch: 022, Loss: 0.9622, Train: 0.7225, Test: 0.6900\n",
            "Early stopping:  0.034007238518337785\n",
            "Epoch: 023, Loss: 0.9434, Train: 0.7268, Test: 0.6911\n",
            "Early stopping:  0.03261110472652402\n",
            "Epoch: 024, Loss: 0.9258, Train: 0.7322, Test: 0.6932\n",
            "Early stopping:  0.030820868526321037\n",
            "Epoch: 025, Loss: 0.9093, Train: 0.7376, Test: 0.6968\n",
            "Early stopping:  0.028874690272654494\n",
            "Epoch: 026, Loss: 0.8936, Train: 0.7422, Test: 0.6995\n",
            "Early stopping:  0.02709792379969108\n",
            "Epoch: 027, Loss: 0.8790, Train: 0.7462, Test: 0.7005\n",
            "Early stopping:  0.025453498160812455\n",
            "Epoch: 028, Loss: 0.8650, Train: 0.7521, Test: 0.7016\n",
            "Early stopping:  0.024006684030196953\n",
            "Epoch: 029, Loss: 0.8518, Train: 0.7547, Test: 0.7026\n",
            "Early stopping:  0.022686809641386287\n",
            "Epoch: 030, Loss: 0.8394, Train: 0.7574, Test: 0.7053\n",
            "Early stopping:  0.021438255502592103\n",
            "Epoch: 031, Loss: 0.8276, Train: 0.7628, Test: 0.7079\n",
            "Early stopping:  0.020330590625647866\n",
            "Epoch: 032, Loss: 0.8165, Train: 0.7679, Test: 0.7074\n",
            "Early stopping:  0.01920239236706225\n",
            "Epoch: 033, Loss: 0.8063, Train: 0.7711, Test: 0.7068\n",
            "Early stopping:  0.018025688740113906\n",
            "Epoch: 034, Loss: 0.7968, Train: 0.7750, Test: 0.7074\n",
            "Early stopping:  0.01684628572654949\n",
            "Epoch: 035, Loss: 0.7877, Train: 0.7783, Test: 0.7095\n",
            "Early stopping:  0.015750461926691925\n",
            "Epoch: 036, Loss: 0.7788, Train: 0.7811, Test: 0.7126\n",
            "Early stopping:  0.014875038843201708\n",
            "Epoch: 037, Loss: 0.7701, Train: 0.7847, Test: 0.7121\n",
            "Early stopping:  0.014309370637617107\n",
            "Epoch: 038, Loss: 0.7615, Train: 0.7891, Test: 0.7153\n",
            "Early stopping:  0.013945176540287502\n",
            "Epoch: 039, Loss: 0.7528, Train: 0.7912, Test: 0.7168\n",
            "Early stopping:  0.013768583651881175\n",
            "Epoch: 040, Loss: 0.7440, Train: 0.7926, Test: 0.7174\n",
            "Early stopping:  0.013748708180780101\n",
            "Epoch: 041, Loss: 0.7351, Train: 0.7941, Test: 0.7168\n",
            "Early stopping:  0.013837836285243604\n",
            "Epoch: 042, Loss: 0.7260, Train: 0.7980, Test: 0.7189\n",
            "Early stopping:  0.01400315118888029\n",
            "Epoch: 043, Loss: 0.7171, Train: 0.8005, Test: 0.7216\n",
            "Early stopping:  0.014119227762509323\n",
            "Epoch: 044, Loss: 0.7082, Train: 0.8018, Test: 0.7168\n",
            "Early stopping:  0.014171447342972517\n",
            "Epoch: 045, Loss: 0.6992, Train: 0.8029, Test: 0.7205\n",
            "Early stopping:  0.014160285444174232\n",
            "Epoch: 046, Loss: 0.6906, Train: 0.8061, Test: 0.7179\n",
            "Early stopping:  0.014028044198062651\n",
            "Epoch: 047, Loss: 0.6827, Train: 0.8064, Test: 0.7237\n",
            "Early stopping:  0.013656046234349128\n",
            "Epoch: 048, Loss: 0.6751, Train: 0.8087, Test: 0.7200\n",
            "Early stopping:  0.013085896636331102\n",
            "Epoch: 049, Loss: 0.6660, Train: 0.8136, Test: 0.7211\n",
            "Early stopping:  0.012968973744076392\n",
            "Epoch: 050, Loss: 0.6551, Train: 0.8155, Test: 0.7216\n",
            "Early stopping:  0.013896176007500654\n",
            "Epoch: 051, Loss: 0.6482, Train: 0.8171, Test: 0.7237\n",
            "Early stopping:  0.014110007159802002\n",
            "Epoch: 052, Loss: 0.6412, Train: 0.8201, Test: 0.7232\n",
            "Early stopping:  0.013603818460281572\n",
            "Epoch: 053, Loss: 0.6307, Train: 0.8228, Test: 0.7232\n",
            "Early stopping:  0.013414060179717075\n",
            "Epoch: 054, Loss: 0.6236, Train: 0.8237, Test: 0.7242\n",
            "Early stopping:  0.01276462397620781\n",
            "Epoch: 055, Loss: 0.6168, Train: 0.8287, Test: 0.7258\n",
            "Early stopping:  0.012740455541123956\n",
            "Epoch: 056, Loss: 0.6075, Train: 0.8307, Test: 0.7237\n",
            "Early stopping:  0.012880621639592768\n",
            "Epoch: 057, Loss: 0.6004, Train: 0.8321, Test: 0.7226\n",
            "Early stopping:  0.012148873349657665\n",
            "Epoch: 058, Loss: 0.5934, Train: 0.8355, Test: 0.7211\n",
            "Early stopping:  0.01214933135094963\n",
            "Epoch: 059, Loss: 0.5852, Train: 0.8374, Test: 0.7216\n",
            "Early stopping:  0.01223196776106957\n",
            "Epoch: 060, Loss: 0.5780, Train: 0.8404, Test: 0.7211\n",
            "Early stopping:  0.011761213218831123\n",
            "Epoch: 061, Loss: 0.5707, Train: 0.8430, Test: 0.7258\n",
            "Early stopping:  0.01184541931243228\n",
            "Epoch: 062, Loss: 0.5634, Train: 0.8479, Test: 0.7221\n",
            "Early stopping:  0.011791979971380774\n",
            "Epoch: 063, Loss: 0.5561, Train: 0.8495, Test: 0.7216\n",
            "Early stopping:  0.011517290904650356\n",
            "Epoch: 064, Loss: 0.5489, Train: 0.8534, Test: 0.7253\n",
            "Early stopping:  0.011505616504062517\n",
            "Epoch: 065, Loss: 0.5423, Train: 0.8514, Test: 0.7200\n",
            "Early stopping:  0.011265747836079103\n",
            "Epoch: 066, Loss: 0.5352, Train: 0.8566, Test: 0.7200\n",
            "Early stopping:  0.011086752805371905\n",
            "Epoch: 067, Loss: 0.5290, Train: 0.8574, Test: 0.7216\n",
            "Early stopping:  0.010717700773862714\n",
            "Epoch: 068, Loss: 0.5242, Train: 0.8578, Test: 0.7184\n",
            "Early stopping:  0.009916765441129226\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.88      0.82      0.85       105\n",
            "            CAD_and_CAM       0.80      0.78      0.79        95\n",
            "              Companies       0.57      0.62      0.59        86\n",
            "       Computer_Science       0.81      0.78      0.80        87\n",
            "            Consultants       0.61      0.56      0.59        91\n",
            "           Data_Formats       0.81      0.82      0.82       105\n",
            "    Data_Communications       0.73      0.73      0.73       114\n",
            "              Education       0.87      0.92      0.89       106\n",
            "               Graphics       0.81      0.89      0.85       113\n",
            "               Hardware       0.65      0.71      0.68        90\n",
            "               Internet       0.66      0.61      0.63       109\n",
            "       Mobile_Computing       0.69      0.81      0.75        85\n",
            "             Multimedia       0.69      0.75      0.72        91\n",
            "            Open_Source       0.72      0.75      0.73        96\n",
            "            Programming       0.43      0.52      0.47        86\n",
            "               Robotics       0.91      0.94      0.92       102\n",
            "               Security       0.84      0.84      0.84       117\n",
            "               Software       0.40      0.32      0.36       108\n",
            "                Systems       0.65      0.46      0.54       114\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.71      0.72      0.71      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9452, Train: 0.4041, Test: 0.3837\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8044, Train: 0.4733, Test: 0.4595\n",
            "Early stopping:  0.09951697011652895\n",
            "Epoch: 003, Loss: 2.6282, Train: 0.5403, Test: 0.5284\n",
            "Early stopping:  0.15878519143719774\n",
            "Epoch: 004, Loss: 2.4264, Train: 0.5996, Test: 0.5968\n",
            "Early stopping:  0.22435099359797867\n",
            "Epoch: 005, Loss: 2.2160, Train: 0.6217, Test: 0.6168\n",
            "Early stopping:  0.2912209973541165\n",
            "Epoch: 006, Loss: 2.0099, Train: 0.6358, Test: 0.6253\n",
            "Early stopping:  0.31660450445666183\n",
            "Epoch: 007, Loss: 1.8146, Train: 0.6463, Test: 0.6395\n",
            "Early stopping:  0.3231755123069772\n",
            "Epoch: 008, Loss: 1.6377, Train: 0.6512, Test: 0.6474\n",
            "Early stopping:  0.3130569756154879\n",
            "Epoch: 009, Loss: 1.4890, Train: 0.6538, Test: 0.6453\n",
            "Early stopping:  0.2893016934135734\n",
            "Epoch: 010, Loss: 1.3704, Train: 0.6578, Test: 0.6468\n",
            "Early stopping:  0.2548774763849439\n",
            "Epoch: 011, Loss: 1.2789, Train: 0.6624, Test: 0.6537\n",
            "Early stopping:  0.21337413775776423\n",
            "Epoch: 012, Loss: 1.2133, Train: 0.6659, Test: 0.6626\n",
            "Early stopping:  0.16941569976842724\n",
            "Epoch: 013, Loss: 1.1667, Train: 0.6674, Test: 0.6600\n",
            "Early stopping:  0.12878876831286248\n",
            "Epoch: 014, Loss: 1.1335, Train: 0.6726, Test: 0.6642\n",
            "Early stopping:  0.09442012499039118\n",
            "Epoch: 015, Loss: 1.1094, Train: 0.6772, Test: 0.6637\n",
            "Early stopping:  0.0674706333938903\n",
            "Epoch: 016, Loss: 1.0890, Train: 0.6832, Test: 0.6674\n",
            "Early stopping:  0.049072803358137386\n",
            "Epoch: 017, Loss: 1.0692, Train: 0.6907, Test: 0.6689\n",
            "Early stopping:  0.03810589150844805\n",
            "Epoch: 018, Loss: 1.0493, Train: 0.6974, Test: 0.6726\n",
            "Early stopping:  0.03302005449516394\n",
            "Epoch: 019, Loss: 1.0278, Train: 0.7028, Test: 0.6805\n",
            "Early stopping:  0.03206998586626933\n",
            "Epoch: 020, Loss: 1.0059, Train: 0.7087, Test: 0.6821\n",
            "Early stopping:  0.032839875653204\n",
            "Epoch: 021, Loss: 0.9843, Train: 0.7155, Test: 0.6863\n",
            "Early stopping:  0.033704063048400026\n",
            "Epoch: 022, Loss: 0.9634, Train: 0.7205, Test: 0.6863\n",
            "Early stopping:  0.03403243050859932\n",
            "Epoch: 023, Loss: 0.9436, Train: 0.7261, Test: 0.6905\n",
            "Early stopping:  0.03337241354914112\n",
            "Epoch: 024, Loss: 0.9249, Train: 0.7307, Test: 0.6937\n",
            "Early stopping:  0.03205212853221451\n",
            "Epoch: 025, Loss: 0.9081, Train: 0.7334, Test: 0.6963\n",
            "Early stopping:  0.030207099007834855\n",
            "Epoch: 026, Loss: 0.8926, Train: 0.7396, Test: 0.6974\n",
            "Early stopping:  0.028042987929942764\n",
            "Epoch: 027, Loss: 0.8785, Train: 0.7445, Test: 0.6979\n",
            "Early stopping:  0.025732871964855686\n",
            "Epoch: 028, Loss: 0.8656, Train: 0.7470, Test: 0.7047\n",
            "Early stopping:  0.023476327226596566\n",
            "Epoch: 029, Loss: 0.8536, Train: 0.7525, Test: 0.7074\n",
            "Early stopping:  0.0215389435856333\n",
            "Epoch: 030, Loss: 0.8424, Train: 0.7597, Test: 0.7095\n",
            "Early stopping:  0.019825060369003302\n",
            "Epoch: 031, Loss: 0.8316, Train: 0.7649, Test: 0.7105\n",
            "Early stopping:  0.018511429380847554\n",
            "Epoch: 032, Loss: 0.8215, Train: 0.7689, Test: 0.7142\n",
            "Early stopping:  0.017447468350374123\n",
            "Epoch: 033, Loss: 0.8119, Train: 0.7741, Test: 0.7189\n",
            "Early stopping:  0.01650347452870618\n",
            "Epoch: 034, Loss: 0.8027, Train: 0.7784, Test: 0.7184\n",
            "Early stopping:  0.01569379689064044\n",
            "Epoch: 035, Loss: 0.7939, Train: 0.7796, Test: 0.7195\n",
            "Early stopping:  0.014896533153946031\n",
            "Epoch: 036, Loss: 0.7854, Train: 0.7828, Test: 0.7189\n",
            "Early stopping:  0.014237490842215412\n",
            "Epoch: 037, Loss: 0.7771, Train: 0.7837, Test: 0.7200\n",
            "Early stopping:  0.013738611452477852\n",
            "Epoch: 038, Loss: 0.7688, Train: 0.7864, Test: 0.7226\n",
            "Early stopping:  0.013389247056969126\n",
            "Epoch: 039, Loss: 0.7603, Train: 0.7879, Test: 0.7247\n",
            "Early stopping:  0.013254624187540212\n",
            "Epoch: 040, Loss: 0.7520, Train: 0.7912, Test: 0.7268\n",
            "Early stopping:  0.013223745882325107\n",
            "Epoch: 041, Loss: 0.7436, Train: 0.7937, Test: 0.7253\n",
            "Early stopping:  0.013234941235825505\n",
            "Epoch: 042, Loss: 0.7352, Train: 0.7943, Test: 0.7258\n",
            "Early stopping:  0.013270754241269648\n",
            "Epoch: 043, Loss: 0.7269, Train: 0.7971, Test: 0.7289\n",
            "Early stopping:  0.013241715591403587\n",
            "Epoch: 044, Loss: 0.7186, Train: 0.7992, Test: 0.7300\n",
            "Early stopping:  0.0132080397943351\n",
            "Epoch: 045, Loss: 0.7103, Train: 0.8009, Test: 0.7289\n",
            "Early stopping:  0.01314151331446169\n",
            "Epoch: 046, Loss: 0.7021, Train: 0.8036, Test: 0.7295\n",
            "Early stopping:  0.013060052124382687\n",
            "Epoch: 047, Loss: 0.6940, Train: 0.8050, Test: 0.7289\n",
            "Early stopping:  0.013003859874281086\n",
            "Epoch: 048, Loss: 0.6859, Train: 0.8084, Test: 0.7279\n",
            "Early stopping:  0.012917406219649971\n",
            "Epoch: 049, Loss: 0.6779, Train: 0.8113, Test: 0.7284\n",
            "Early stopping:  0.012824732126188253\n",
            "Epoch: 050, Loss: 0.6700, Train: 0.8137, Test: 0.7300\n",
            "Early stopping:  0.012709242089439935\n",
            "Epoch: 051, Loss: 0.6621, Train: 0.8158, Test: 0.7332\n",
            "Early stopping:  0.012586317351605229\n",
            "Epoch: 052, Loss: 0.6544, Train: 0.8180, Test: 0.7321\n",
            "Early stopping:  0.012470050056697081\n",
            "Epoch: 053, Loss: 0.6467, Train: 0.8193, Test: 0.7326\n",
            "Early stopping:  0.01232765266130841\n",
            "Epoch: 054, Loss: 0.6395, Train: 0.8195, Test: 0.7326\n",
            "Early stopping:  0.012069864820136703\n",
            "Epoch: 055, Loss: 0.6333, Train: 0.8191, Test: 0.7311\n",
            "Early stopping:  0.011479772141060055\n",
            "Epoch: 056, Loss: 0.6297, Train: 0.8234, Test: 0.7295\n",
            "Early stopping:  0.010012474234940626\n",
            "Epoch: 057, Loss: 0.6220, Train: 0.8257, Test: 0.7305\n",
            "Early stopping:  0.009438945341118334\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.92      0.82      0.87       106\n",
            "            CAD_and_CAM       0.76      0.73      0.74        92\n",
            "              Companies       0.55      0.43      0.48        98\n",
            "       Computer_Science       0.80      0.91      0.85        99\n",
            "            Consultants       0.62      0.66      0.64       122\n",
            "           Data_Formats       0.81      0.80      0.80       114\n",
            "    Data_Communications       0.78      0.78      0.78       112\n",
            "              Education       0.86      0.89      0.88       103\n",
            "               Graphics       0.83      0.93      0.87       109\n",
            "               Hardware       0.64      0.59      0.61        97\n",
            "               Internet       0.77      0.72      0.74        99\n",
            "       Mobile_Computing       0.81      0.81      0.81       114\n",
            "             Multimedia       0.72      0.75      0.73        95\n",
            "            Open_Source       0.68      0.69      0.69        84\n",
            "            Programming       0.58      0.70      0.64        98\n",
            "               Robotics       0.92      0.96      0.94        91\n",
            "               Security       0.73      0.82      0.77        85\n",
            "               Software       0.48      0.34      0.40       102\n",
            "                Systems       0.48      0.51      0.49        80\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.72      0.73      0.72      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9461, Train: 0.4150, Test: 0.4253\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8237, Train: 0.5186, Test: 0.5332\n",
            "Early stopping:  0.08658125937156534\n",
            "Epoch: 003, Loss: 2.6605, Train: 0.5237, Test: 0.5389\n",
            "Early stopping:  0.14329819125533585\n",
            "Epoch: 004, Loss: 2.4665, Train: 0.5541, Test: 0.5637\n",
            "Early stopping:  0.2078530120840532\n",
            "Epoch: 005, Loss: 2.2611, Train: 0.5938, Test: 0.6026\n",
            "Early stopping:  0.27441515764101415\n",
            "Epoch: 006, Loss: 2.0546, Train: 0.6234, Test: 0.6284\n",
            "Early stopping:  0.3066992331957564\n",
            "Epoch: 007, Loss: 1.8575, Train: 0.6354, Test: 0.6400\n",
            "Early stopping:  0.3190785136513894\n",
            "Epoch: 008, Loss: 1.6814, Train: 0.6426, Test: 0.6405\n",
            "Early stopping:  0.31224531780464293\n",
            "Epoch: 009, Loss: 1.5312, Train: 0.6463, Test: 0.6405\n",
            "Early stopping:  0.29038234612907676\n",
            "Epoch: 010, Loss: 1.4085, Train: 0.6470, Test: 0.6405\n",
            "Early stopping:  0.2569562609056735\n",
            "Epoch: 011, Loss: 1.3145, Train: 0.6537, Test: 0.6453\n",
            "Early stopping:  0.21637789602942548\n",
            "Epoch: 012, Loss: 1.2443, Train: 0.6607, Test: 0.6558\n",
            "Early stopping:  0.17428790338746528\n",
            "Epoch: 013, Loss: 1.1925, Train: 0.6651, Test: 0.6600\n",
            "Early stopping:  0.13488970071357073\n",
            "Epoch: 014, Loss: 1.1532, Train: 0.6709, Test: 0.6584\n",
            "Early stopping:  0.10150260405531636\n",
            "Epoch: 015, Loss: 1.1224, Train: 0.6770, Test: 0.6589\n",
            "Early stopping:  0.07617106056825199\n",
            "Epoch: 016, Loss: 1.0958, Train: 0.6846, Test: 0.6663\n",
            "Early stopping:  0.058618233047102675\n",
            "Epoch: 017, Loss: 1.0723, Train: 0.6912, Test: 0.6726\n",
            "Early stopping:  0.047343513724830665\n",
            "Epoch: 018, Loss: 1.0505, Train: 0.6964, Test: 0.6758\n",
            "Early stopping:  0.04049445411525504\n",
            "Epoch: 019, Loss: 1.0293, Train: 0.7033, Test: 0.6779\n",
            "Early stopping:  0.03661819810192755\n",
            "Epoch: 020, Loss: 1.0085, Train: 0.7093, Test: 0.6821\n",
            "Early stopping:  0.034396485672262736\n",
            "Epoch: 021, Loss: 0.9881, Train: 0.7130, Test: 0.6847\n",
            "Early stopping:  0.03327471826726108\n",
            "Epoch: 022, Loss: 0.9682, Train: 0.7172, Test: 0.6837\n",
            "Early stopping:  0.03253991381836485\n",
            "Epoch: 023, Loss: 0.9492, Train: 0.7232, Test: 0.6889\n",
            "Early stopping:  0.03171867882877413\n",
            "Epoch: 024, Loss: 0.9312, Train: 0.7284, Test: 0.6942\n",
            "Early stopping:  0.0306018956359531\n",
            "Epoch: 025, Loss: 0.9143, Train: 0.7325, Test: 0.6953\n",
            "Early stopping:  0.02921616396962442\n",
            "Epoch: 026, Loss: 0.8988, Train: 0.7380, Test: 0.6921\n",
            "Early stopping:  0.027494702856909352\n",
            "Epoch: 027, Loss: 0.8847, Train: 0.7428, Test: 0.6958\n",
            "Early stopping:  0.025535584380620622\n",
            "Epoch: 028, Loss: 0.8715, Train: 0.7474, Test: 0.7016\n",
            "Early stopping:  0.0235830912056937\n",
            "Epoch: 029, Loss: 0.8587, Train: 0.7524, Test: 0.7053\n",
            "Early stopping:  0.02190302250361358\n",
            "Epoch: 030, Loss: 0.8465, Train: 0.7563, Test: 0.7063\n",
            "Early stopping:  0.02065964478722831\n",
            "Epoch: 031, Loss: 0.8347, Train: 0.7617, Test: 0.7100\n",
            "Early stopping:  0.019781215661948844\n",
            "Epoch: 032, Loss: 0.8234, Train: 0.7662, Test: 0.7089\n",
            "Early stopping:  0.019006876871602497\n",
            "Epoch: 033, Loss: 0.8129, Train: 0.7683, Test: 0.7121\n",
            "Early stopping:  0.018160616197901738\n",
            "Epoch: 034, Loss: 0.8030, Train: 0.7725, Test: 0.7153\n",
            "Early stopping:  0.01722715132040638\n",
            "Epoch: 035, Loss: 0.7939, Train: 0.7747, Test: 0.7195\n",
            "Early stopping:  0.01614259962836199\n",
            "Epoch: 036, Loss: 0.7853, Train: 0.7783, Test: 0.7174\n",
            "Early stopping:  0.015053485700545412\n",
            "Epoch: 037, Loss: 0.7772, Train: 0.7800, Test: 0.7174\n",
            "Early stopping:  0.01409103685078652\n",
            "Epoch: 038, Loss: 0.7693, Train: 0.7832, Test: 0.7158\n",
            "Early stopping:  0.013321815845124674\n",
            "Epoch: 039, Loss: 0.7614, Train: 0.7853, Test: 0.7142\n",
            "Early stopping:  0.012817021948568891\n",
            "Epoch: 040, Loss: 0.7535, Train: 0.7867, Test: 0.7105\n",
            "Early stopping:  0.012560788758734323\n",
            "Epoch: 041, Loss: 0.7454, Train: 0.7897, Test: 0.7137\n",
            "Early stopping:  0.012538351457067328\n",
            "Epoch: 042, Loss: 0.7370, Train: 0.7925, Test: 0.7168\n",
            "Early stopping:  0.012719551312960282\n",
            "Epoch: 043, Loss: 0.7286, Train: 0.7961, Test: 0.7153\n",
            "Early stopping:  0.01297167546957695\n",
            "Epoch: 044, Loss: 0.7202, Train: 0.8004, Test: 0.7184\n",
            "Early stopping:  0.013191960705091248\n",
            "Epoch: 045, Loss: 0.7117, Train: 0.8039, Test: 0.7179\n",
            "Early stopping:  0.013322359420289222\n",
            "Epoch: 046, Loss: 0.7033, Train: 0.8057, Test: 0.7200\n",
            "Early stopping:  0.013348060191018656\n",
            "Epoch: 047, Loss: 0.6949, Train: 0.8089, Test: 0.7205\n",
            "Early stopping:  0.013330402457877545\n",
            "Epoch: 048, Loss: 0.6866, Train: 0.8084, Test: 0.7242\n",
            "Early stopping:  0.013275856623706115\n",
            "Epoch: 049, Loss: 0.6784, Train: 0.8100, Test: 0.7195\n",
            "Early stopping:  0.013177143610871078\n",
            "Epoch: 050, Loss: 0.6704, Train: 0.8109, Test: 0.7205\n",
            "Early stopping:  0.013003075879128188\n",
            "Epoch: 051, Loss: 0.6626, Train: 0.8139, Test: 0.7205\n",
            "Early stopping:  0.012755047095842747\n",
            "Epoch: 052, Loss: 0.6547, Train: 0.8167, Test: 0.7179\n",
            "Early stopping:  0.012565888243557696\n",
            "Epoch: 053, Loss: 0.6466, Train: 0.8192, Test: 0.7216\n",
            "Early stopping:  0.01253430893162882\n",
            "Epoch: 054, Loss: 0.6384, Train: 0.8217, Test: 0.7200\n",
            "Early stopping:  0.0126647797813801\n",
            "Epoch: 055, Loss: 0.6307, Train: 0.8224, Test: 0.7232\n",
            "Early stopping:  0.012695047747879174\n",
            "Epoch: 056, Loss: 0.6234, Train: 0.8255, Test: 0.7221\n",
            "Early stopping:  0.012420740013509627\n",
            "Epoch: 057, Loss: 0.6166, Train: 0.8270, Test: 0.7184\n",
            "Early stopping:  0.011876958721100398\n",
            "Epoch: 058, Loss: 0.6098, Train: 0.8300, Test: 0.7205\n",
            "Early stopping:  0.011285289152677394\n",
            "Epoch: 059, Loss: 0.6028, Train: 0.8309, Test: 0.7179\n",
            "Early stopping:  0.010981086457428367\n",
            "Epoch: 060, Loss: 0.5957, Train: 0.8349, Test: 0.7221\n",
            "Early stopping:  0.010942984116797838\n",
            "Epoch: 061, Loss: 0.5888, Train: 0.8367, Test: 0.7189\n",
            "Early stopping:  0.011009827083394427\n",
            "Epoch: 062, Loss: 0.5824, Train: 0.8397, Test: 0.7221\n",
            "Early stopping:  0.01087789863104184\n",
            "Epoch: 063, Loss: 0.5757, Train: 0.8438, Test: 0.7205\n",
            "Early stopping:  0.010662707763973944\n",
            "Epoch: 064, Loss: 0.5686, Train: 0.8442, Test: 0.7211\n",
            "Early stopping:  0.010625867635154931\n",
            "Epoch: 065, Loss: 0.5614, Train: 0.8483, Test: 0.7200\n",
            "Early stopping:  0.01083343264657807\n",
            "Epoch: 066, Loss: 0.5548, Train: 0.8513, Test: 0.7184\n",
            "Early stopping:  0.01100280275432812\n",
            "Epoch: 067, Loss: 0.5485, Train: 0.8533, Test: 0.7216\n",
            "Early stopping:  0.010800414799312418\n",
            "Epoch: 068, Loss: 0.5420, Train: 0.8554, Test: 0.7184\n",
            "Early stopping:  0.010461886412637462\n",
            "Epoch: 069, Loss: 0.5354, Train: 0.8593, Test: 0.7174\n",
            "Early stopping:  0.010222567980191043\n",
            "Epoch: 070, Loss: 0.5293, Train: 0.8588, Test: 0.7163\n",
            "Early stopping:  0.010122468254610729\n",
            "Epoch: 071, Loss: 0.5241, Train: 0.8626, Test: 0.7142\n",
            "Early stopping:  0.009738946065227612\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.82      0.80      0.81        94\n",
            "            CAD_and_CAM       0.78      0.73      0.75       105\n",
            "              Companies       0.65      0.49      0.56       102\n",
            "       Computer_Science       0.78      0.82      0.80       102\n",
            "            Consultants       0.65      0.64      0.65       120\n",
            "           Data_Formats       0.78      0.79      0.78        98\n",
            "    Data_Communications       0.65      0.77      0.70        92\n",
            "              Education       0.81      0.90      0.86        92\n",
            "               Graphics       0.90      0.98      0.94       111\n",
            "               Hardware       0.74      0.64      0.68       105\n",
            "               Internet       0.66      0.66      0.66       102\n",
            "       Mobile_Computing       0.88      0.77      0.82       112\n",
            "             Multimedia       0.73      0.71      0.72       106\n",
            "            Open_Source       0.69      0.82      0.75        96\n",
            "            Programming       0.55      0.48      0.51        88\n",
            "               Robotics       0.89      0.89      0.89       104\n",
            "               Security       0.79      0.78      0.78        89\n",
            "               Software       0.29      0.37      0.33        94\n",
            "                Systems       0.52      0.47      0.49        88\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.71      0.71      0.71      1900\n",
            "           weighted avg       0.72      0.71      0.71      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9497, Train: 0.3920, Test: 0.3958\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8142, Train: 0.5561, Test: 0.5553\n",
            "Early stopping:  0.095803158413143\n",
            "Epoch: 003, Loss: 2.6451, Train: 0.5791, Test: 0.5958\n",
            "Early stopping:  0.15257526519635942\n",
            "Epoch: 004, Loss: 2.4534, Train: 0.5903, Test: 0.5968\n",
            "Early stopping:  0.21465660721693972\n",
            "Epoch: 005, Loss: 2.2535, Train: 0.6075, Test: 0.6132\n",
            "Early stopping:  0.2779759846966889\n",
            "Epoch: 006, Loss: 2.0497, Train: 0.6205, Test: 0.6289\n",
            "Early stopping:  0.303866336801959\n",
            "Epoch: 007, Loss: 1.8517, Train: 0.6304, Test: 0.6342\n",
            "Early stopping:  0.31473004133375937\n",
            "Epoch: 008, Loss: 1.6716, Train: 0.6380, Test: 0.6458\n",
            "Early stopping:  0.310808814047786\n",
            "Epoch: 009, Loss: 1.5195, Train: 0.6428, Test: 0.6489\n",
            "Early stopping:  0.29234792323143466\n",
            "Epoch: 010, Loss: 1.3979, Train: 0.6507, Test: 0.6463\n",
            "Early stopping:  0.25979823044957684\n",
            "Epoch: 011, Loss: 1.3040, Train: 0.6549, Test: 0.6516\n",
            "Early stopping:  0.21817984968653834\n",
            "Epoch: 012, Loss: 1.2360, Train: 0.6613, Test: 0.6563\n",
            "Early stopping:  0.1738045814514877\n",
            "Epoch: 013, Loss: 1.1851, Train: 0.6672, Test: 0.6679\n",
            "Early stopping:  0.13323369184123923\n",
            "Epoch: 014, Loss: 1.1482, Train: 0.6761, Test: 0.6674\n",
            "Early stopping:  0.0993148246814409\n",
            "Epoch: 015, Loss: 1.1197, Train: 0.6803, Test: 0.6663\n",
            "Early stopping:  0.0732356339266611\n",
            "Epoch: 016, Loss: 1.0960, Train: 0.6845, Test: 0.6711\n",
            "Early stopping:  0.05527564393627781\n",
            "Epoch: 017, Loss: 1.0744, Train: 0.6889, Test: 0.6763\n",
            "Early stopping:  0.04353874667124478\n",
            "Epoch: 018, Loss: 1.0534, Train: 0.6938, Test: 0.6821\n",
            "Early stopping:  0.037236423386699136\n",
            "Epoch: 019, Loss: 1.0321, Train: 0.7012, Test: 0.6816\n",
            "Early stopping:  0.0344627080782938\n",
            "Epoch: 020, Loss: 1.0103, Train: 0.7068, Test: 0.6858\n",
            "Early stopping:  0.033794323265612186\n",
            "Epoch: 021, Loss: 0.9886, Train: 0.7136, Test: 0.6889\n",
            "Early stopping:  0.033959294029138394\n",
            "Epoch: 022, Loss: 0.9676, Train: 0.7179, Test: 0.6905\n",
            "Early stopping:  0.03401632611742777\n",
            "Epoch: 023, Loss: 0.9480, Train: 0.7225, Test: 0.6921\n",
            "Early stopping:  0.03334957553751022\n",
            "Epoch: 024, Loss: 0.9295, Train: 0.7278, Test: 0.6953\n",
            "Early stopping:  0.03197591520319335\n",
            "Epoch: 025, Loss: 0.9129, Train: 0.7333, Test: 0.6989\n",
            "Early stopping:  0.02998255086781049\n",
            "Epoch: 026, Loss: 0.8975, Train: 0.7380, Test: 0.7026\n",
            "Early stopping:  0.02774340281859115\n",
            "Epoch: 027, Loss: 0.8836, Train: 0.7439, Test: 0.7037\n",
            "Early stopping:  0.025467880510651842\n",
            "Epoch: 028, Loss: 0.8705, Train: 0.7500, Test: 0.7089\n",
            "Early stopping:  0.023332172771357983\n",
            "Epoch: 029, Loss: 0.8585, Train: 0.7530, Test: 0.7147\n",
            "Early stopping:  0.02151262613618836\n",
            "Epoch: 030, Loss: 0.8471, Train: 0.7578, Test: 0.7153\n",
            "Early stopping:  0.019932394041317875\n",
            "Epoch: 031, Loss: 0.8364, Train: 0.7607, Test: 0.7163\n",
            "Early stopping:  0.018636561347139147\n",
            "Epoch: 032, Loss: 0.8262, Train: 0.7651, Test: 0.7195\n",
            "Early stopping:  0.017505081433656602\n",
            "Epoch: 033, Loss: 0.8166, Train: 0.7684, Test: 0.7205\n",
            "Early stopping:  0.016555599064114468\n",
            "Epoch: 034, Loss: 0.8074, Train: 0.7701, Test: 0.7237\n",
            "Early stopping:  0.015678114772287786\n",
            "Epoch: 035, Loss: 0.7987, Train: 0.7733, Test: 0.7242\n",
            "Early stopping:  0.014883176915054622\n",
            "Epoch: 036, Loss: 0.7902, Train: 0.7772, Test: 0.7247\n",
            "Early stopping:  0.014209709999408807\n",
            "Epoch: 037, Loss: 0.7818, Train: 0.7800, Test: 0.7232\n",
            "Early stopping:  0.013740950332011772\n",
            "Epoch: 038, Loss: 0.7733, Train: 0.7822, Test: 0.7242\n",
            "Early stopping:  0.013458861700237237\n",
            "Epoch: 039, Loss: 0.7651, Train: 0.7849, Test: 0.7237\n",
            "Early stopping:  0.01330604821305812\n",
            "Epoch: 040, Loss: 0.7568, Train: 0.7892, Test: 0.7237\n",
            "Early stopping:  0.01318764585596929\n",
            "Epoch: 041, Loss: 0.7488, Train: 0.7917, Test: 0.7247\n",
            "Early stopping:  0.01302865386981179\n",
            "Epoch: 042, Loss: 0.7409, Train: 0.7946, Test: 0.7274\n",
            "Early stopping:  0.012830286153819613\n",
            "Epoch: 043, Loss: 0.7329, Train: 0.7964, Test: 0.7237\n",
            "Early stopping:  0.01268040081525156\n",
            "Epoch: 044, Loss: 0.7250, Train: 0.7986, Test: 0.7247\n",
            "Early stopping:  0.012591038391407615\n",
            "Epoch: 045, Loss: 0.7169, Train: 0.8016, Test: 0.7268\n",
            "Early stopping:  0.01261574645640622\n",
            "Epoch: 046, Loss: 0.7087, Train: 0.8014, Test: 0.7263\n",
            "Early stopping:  0.012700942657594144\n",
            "Epoch: 047, Loss: 0.7006, Train: 0.8063, Test: 0.7295\n",
            "Early stopping:  0.012781435405599046\n",
            "Epoch: 048, Loss: 0.6927, Train: 0.8034, Test: 0.7247\n",
            "Early stopping:  0.012787370795882652\n",
            "Epoch: 049, Loss: 0.6850, Train: 0.8092, Test: 0.7289\n",
            "Early stopping:  0.012640618590891795\n",
            "Epoch: 050, Loss: 0.6775, Train: 0.8103, Test: 0.7226\n",
            "Early stopping:  0.012352272259163287\n",
            "Epoch: 051, Loss: 0.6699, Train: 0.8151, Test: 0.7279\n",
            "Early stopping:  0.012105344872300335\n",
            "Epoch: 052, Loss: 0.6614, Train: 0.8158, Test: 0.7263\n",
            "Early stopping:  0.012261472987930793\n",
            "Epoch: 053, Loss: 0.6532, Train: 0.8199, Test: 0.7263\n",
            "Early stopping:  0.012609311410840702\n",
            "Epoch: 054, Loss: 0.6457, Train: 0.8226, Test: 0.7279\n",
            "Early stopping:  0.012704210399839799\n",
            "Epoch: 055, Loss: 0.6384, Train: 0.8245, Test: 0.7253\n",
            "Early stopping:  0.012442564974536361\n",
            "Epoch: 056, Loss: 0.6308, Train: 0.8262, Test: 0.7237\n",
            "Early stopping:  0.012013252517118921\n",
            "Epoch: 057, Loss: 0.6230, Train: 0.8287, Test: 0.7242\n",
            "Early stopping:  0.011901157949223235\n",
            "Epoch: 058, Loss: 0.6155, Train: 0.8307, Test: 0.7221\n",
            "Early stopping:  0.012017473726583515\n",
            "Epoch: 059, Loss: 0.6084, Train: 0.8329, Test: 0.7237\n",
            "Early stopping:  0.011914011430683576\n",
            "Epoch: 060, Loss: 0.6010, Train: 0.8354, Test: 0.7258\n",
            "Early stopping:  0.011714730787576315\n",
            "Epoch: 061, Loss: 0.5934, Train: 0.8378, Test: 0.7232\n",
            "Early stopping:  0.011639480189674416\n",
            "Epoch: 062, Loss: 0.5856, Train: 0.8416, Test: 0.7232\n",
            "Early stopping:  0.011817766262363277\n",
            "Epoch: 063, Loss: 0.5783, Train: 0.8429, Test: 0.7247\n",
            "Early stopping:  0.01196602551595796\n",
            "Epoch: 064, Loss: 0.5714, Train: 0.8464, Test: 0.7216\n",
            "Early stopping:  0.011754260233059883\n",
            "Epoch: 065, Loss: 0.5649, Train: 0.8475, Test: 0.7258\n",
            "Early stopping:  0.011272891493123632\n",
            "Epoch: 066, Loss: 0.5586, Train: 0.8508, Test: 0.7226\n",
            "Early stopping:  0.010682620909272432\n",
            "Epoch: 067, Loss: 0.5519, Train: 0.8530, Test: 0.7226\n",
            "Early stopping:  0.010397348488401981\n",
            "Epoch: 068, Loss: 0.5447, Train: 0.8541, Test: 0.7232\n",
            "Early stopping:  0.010513182308802103\n",
            "Epoch: 069, Loss: 0.5381, Train: 0.8566, Test: 0.7174\n",
            "Early stopping:  0.010660105754729643\n",
            "Epoch: 070, Loss: 0.5324, Train: 0.8553, Test: 0.7221\n",
            "Early stopping:  0.010467893801546425\n",
            "Epoch: 071, Loss: 0.5274, Train: 0.8564, Test: 0.7205\n",
            "Early stopping:  0.00971761357740619\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.83      0.88      0.86        85\n",
            "            CAD_and_CAM       0.75      0.72      0.73       100\n",
            "              Companies       0.59      0.45      0.51       104\n",
            "       Computer_Science       0.79      0.76      0.78        89\n",
            "            Consultants       0.66      0.64      0.65       111\n",
            "           Data_Formats       0.84      0.84      0.84       103\n",
            "    Data_Communications       0.77      0.73      0.75       112\n",
            "              Education       0.88      0.89      0.88       115\n",
            "               Graphics       0.84      0.90      0.87       103\n",
            "               Hardware       0.68      0.82      0.74       105\n",
            "               Internet       0.58      0.70      0.64        77\n",
            "       Mobile_Computing       0.80      0.81      0.81       101\n",
            "             Multimedia       0.71      0.79      0.75        92\n",
            "            Open_Source       0.76      0.56      0.64       113\n",
            "            Programming       0.53      0.59      0.56       106\n",
            "               Robotics       0.90      0.94      0.92        93\n",
            "               Security       0.78      0.79      0.79       107\n",
            "               Software       0.29      0.26      0.27        92\n",
            "                Systems       0.62      0.60      0.61        92\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.72      0.72      0.72      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "time: 36.3 s (started: 2024-08-17 14:17:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d673e761-f977-41d1-d978-fc62a39f847c",
        "id": "IVpqs4LStqU5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 515 ms (started: 2024-08-17 14:18:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "# Hetrogenous Graph for Keyphrase = 3\n",
        "--------------------------------------"
      ],
      "metadata": {
        "id": "dQn_F5KEXNpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"class\"].unique())\n",
        "class_number = len(df[\"class\"].unique())\n",
        "print(class_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfa37ea-3b62-4776-95a2-007526ee58a4",
        "id": "upPyCTFBXNpN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data_management' 'ecology' 'economic' 'geophysics' 'gravitional_theory'\n",
            " 'hydro' 'math' 'metals' 'networking' 'neuroscience' 'oceanography'\n",
            " 'politic' 'sociology' 'software_engineering' 'statistics'\n",
            " 'theory_computing']\n",
            "16\n",
            "time: 5.15 ms (started: 2024-08-16 14:12:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change here to change wich keypharse to use\n",
        "keyphrase = \"keyphrase3\"\n",
        "\n",
        "model_name = dataset_name+\"_\"+keyphrase"
      ],
      "metadata": {
        "id": "38ZkbthBXNpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4aaecc-c13f-4c4d-cc12-54104d193ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 367 µs (started: 2024-08-16 14:12:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Graph"
      ],
      "metadata": {
        "id": "q-p0Ugv8eQpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Graph Nodes and Edges 👀\n",
        "\n",
        "- `Nodes` - documents and contexts\n",
        "- `Edges`\n",
        "  - document <- has -> context\n",
        "- `Labels` - documents classes\n"
      ],
      "metadata": {
        "id": "j5eCUuXkeQpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Nodes and Edges 👀"
      ],
      "metadata": {
        "id": "s_uP4E2seQph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Defining Docmuent nodes, Context nodes and edges between them"
      ],
      "metadata": {
        "id": "DNqpuD_qeQph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_contexts_list =[]\n",
        "edges1,edges2 = [],[]\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes = []\n",
        "context_nodes = []\n",
        "\n",
        "edges_tuple = []\n",
        "\n",
        "sentences = []\n",
        "cont_sentences = 0\n",
        "dit_sentences = {}\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding document nodes for every doc in df\n",
        "    document_nodes.append(df[\"text_embeddings\"][i])\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list: # if NOT\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes.append(df[keyphrase+\"_embeddings\"][i][j])\n",
        "            # add a new edge between doc and new context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(new_edge_cont)\n",
        "            edges_tuple.append((df[\"id\"][i],new_edge_cont))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(all_contexts_list.index(key[0]))\n",
        "            edges_tuple.append((df[\"id\"][i],all_contexts_list.index(key[0])))\n",
        "            cont+=1\n",
        "\n",
        "    # organize sentences, sentences_embeddings, and a dict with the corresponding document for each sentence\n",
        "    aux = df['sentences_embeddings'][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        sentences.append(key)\n",
        "        dit_sentences[cont_sentences] = df[\"id\"][i]\n",
        "        cont_sentences += 1\n",
        "\n",
        "\n",
        "document_nodes = np.array(document_nodes)\n",
        "context_nodes = np.array(context_nodes)"
      ],
      "metadata": {
        "id": "rluwFHD8eQph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0b9ca9-8dc2-4135-9dfd-f66f42481964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14.4 s (started: 2024-08-16 14:12:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of document nodes:\",len(document_nodes))\n",
        "print(\"number of context nodes:\",len(context_nodes))\n",
        "print(\"number of shared contexts:\",cont)\n",
        "print(\"number of direct edges (first dimension):\",len(edges1))\n",
        "print(\"number of direct edges (second dimension):\",len(edges2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEEarV3OeQpi",
        "outputId": "00a12ad2-8e3d-426c-ffa7-c9fea78a899f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 10524\n",
            "number of context nodes: 38370\n",
            "number of shared contexts: 3961\n",
            "number of direct edges (first dimension): 42331\n",
            "number of direct edges (second dimension): 42331\n",
            "time: 5.75 ms (started: 2024-08-16 14:12:57 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=15, metric='cosine').fit(context_nodes)\n",
        "\n",
        "neighbors_list = nbrs.kneighbors(sentences, return_distance=False)\n",
        "\n",
        "# cria aresta para cada vizinho encontrado\n",
        "for i,neighbors in enumerate(neighbors_list):\n",
        "        for n in neighbors:\n",
        "            edges1.append(dit_sentences[i])\n",
        "            edges2.append(n)\n",
        "            edges_tuple.append((dit_sentences[i],n))"
      ],
      "metadata": {
        "id": "SLeSewX5eQpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b5d0ec-f596-443d-c234-5796058d479d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.2 s (started: 2024-08-16 14:12:57 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ajusting everything to Tensor Objects"
      ],
      "metadata": {
        "id": "7wvV-vByeQpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms egdes to tensor\n",
        "edges = np.array([edges1,edges2])\n",
        "edges = torch.tensor(edges)"
      ],
      "metadata": {
        "id": "IcLYlutFeQpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e544a7-c603-4da1-ef38-e237de283a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.1 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms nodes to tensor\n",
        "document_nodes = np.array(document_nodes)\n",
        "document_nodes = torch.tensor(document_nodes)\n",
        "context_nodes = np.array(context_nodes)\n",
        "context_nodes = torch.tensor(context_nodes)"
      ],
      "metadata": {
        "id": "q3EFPPQfeQpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9853a0-f901-4b6e-ca3a-5804f9629706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 67.7 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show documents nodes\n",
        "document_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMc-Rgc4eQpj",
        "outputId": "f8cf0ead-3bfd-4f2c-af83-0968f37ae122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0480, -0.0557,  0.0054,  ..., -0.0087,  0.0842, -0.0266],\n",
              "        [ 0.0155, -0.0795,  0.0167,  ..., -0.0797, -0.0875,  0.0055],\n",
              "        [-0.0026, -0.0006, -0.0163,  ..., -0.0748,  0.0273, -0.0155],\n",
              "        ...,\n",
              "        [-0.0508,  0.0383, -0.0004,  ...,  0.0265, -0.0420, -0.0559],\n",
              "        [-0.0322, -0.0649,  0.0155,  ...,  0.1090,  0.0247, -0.0226],\n",
              "        [ 0.0748,  0.0328,  0.0163,  ..., -0.0506, -0.0447,  0.0613]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.74 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of document nodes\n",
        "len(document_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO-MZZgkeQpj",
        "outputId": "542885c1-783e-4f1b-d9e8-41e200de7c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10524"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.43 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show context nodes\n",
        "context_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PQJp_6oeQpj",
        "outputId": "7ac327a1-c89a-4c3f-8ecb-17700cf6387c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0531, -0.0962,  0.0218,  ..., -0.0255,  0.0040, -0.0138],\n",
              "        [ 0.0517, -0.0748,  0.0175,  ...,  0.0007,  0.0347, -0.0148],\n",
              "        [ 0.0352,  0.0249,  0.0602,  ...,  0.1391,  0.0714, -0.0247],\n",
              "        ...,\n",
              "        [ 0.0899,  0.0222,  0.0036,  ..., -0.0822, -0.0003,  0.0316],\n",
              "        [ 0.0639,  0.0573, -0.0116,  ...,  0.0225, -0.0384,  0.0333],\n",
              "        [ 0.0135,  0.0573,  0.0189,  ..., -0.0285,  0.0146,  0.0765]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.01 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of context nodes\n",
        "len(context_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBB5kzRseQpj",
        "outputId": "f690db34-e37d-4669-e602-11b2dac957a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38370"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.61 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of edges\n",
        "print(len(edges[0]))\n",
        "print(len(edges[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ERr5YqreQpk",
        "outputId": "a8dfc224-a85f-4b9b-a6e2-8500272c0061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "201586\n",
            "201586\n",
            "time: 516 µs (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing edges\n",
        "print(edges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuWKiX2ceQpk",
        "outputId": "4189664b-cdf5-4338-dd2e-f9986bfda497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ..., 10523, 10523, 10523],\n",
            "        [    0,     1,     2,  ..., 38308, 37587, 37571]])\n",
            "time: 1.18 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class Labels"
      ],
      "metadata": {
        "id": "NpojxPL5eQpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All different classes\n",
        "print(df[\"class\"].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z6PwJeteQpk",
        "outputId": "394834bb-068e-4520-db67-ecf1a243e169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data_management' 'ecology' 'economic' 'geophysics' 'gravitional_theory'\n",
            " 'hydro' 'math' 'metals' 'networking' 'neuroscience' 'oceanography'\n",
            " 'politic' 'sociology' 'software_engineering' 'statistics'\n",
            " 'theory_computing']\n",
            "time: 2.13 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating labels for classification\n",
        "# the dictionary is a numeric representation for each possible \"document\" class\n",
        "\n",
        "dit = {}\n",
        "for i,classe in enumerate(df[\"class\"].unique()):\n",
        "  dit[classe] = i\n",
        "\n",
        "print(dit,'\\n')\n",
        "\n",
        "labels = df[[\"class\"]]\n",
        "for i in range(len(df[[\"class\"]])):\n",
        "    labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOPXet4DeQpk",
        "outputId": "962e307b-6a03-4a8c-9d5c-2c68e6865499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data_management': 0, 'ecology': 1, 'economic': 2, 'geophysics': 3, 'gravitional_theory': 4, 'hydro': 5, 'math': 6, 'metals': 7, 'networking': 8, 'neuroscience': 9, 'oceanography': 10, 'politic': 11, 'sociology': 12, 'software_engineering': 13, 'statistics': 14, 'theory_computing': 15} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-a018e46c97dd>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class\n",
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d57fc39-5486-44b0-b24e-64223e12e3c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d57fc39-5486-44b0-b24e-64223e12e3c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d57fc39-5486-44b0-b24e-64223e12e3c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d57fc39-5486-44b0-b24e-64223e12e3c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-63660c4f-3077-41a9-bee5-35eeae78fb6f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63660c4f-3077-41a9-bee5-35eeae78fb6f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-63660c4f-3077-41a9-bee5-35eeae78fb6f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels",
              "summary": "{\n  \"name\": \"labels\",\n  \"rows\": 10524,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 95
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 995 ms (started: 2024-08-16 14:13:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tranfors class dataframe into tensor\n",
        "y = labels[\"class\"].tolist()\n",
        "y = x_np = torch.tensor(y)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za7OEJ9IeQpl",
        "outputId": "8254d2cc-d3f0-4a01-da23-6e59e811285f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  ..., 15, 15, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.46 ms (started: 2024-08-16 14:13:08 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eFpsDwAeQpl",
        "outputId": "3b3ad7bd-affb-4bb4-bd58-09ae3ad0acf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10524"
            ]
          },
          "metadata": {},
          "execution_count": 97
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.47 ms (started: 2024-08-16 14:13:08 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Graph with Networkx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-p97x2PZeQpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining overal graph in networkx maner"
      ],
      "metadata": {
        "id": "J4aNru07eQpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run graph Representation for networkx\n",
        "all_contexts_list =[]\n",
        "edges_test = []\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes_test = []\n",
        "context_nodes_test = []\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding new documents for every node\n",
        "    document_nodes_test.append(\"doc_\"+str(i)) # in the actual graph nodes -> documents embeddings\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list:\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes_test.append(\"contx_\"+str(new_edge_cont)) # in the actual graph nodes -> context embeddings\n",
        "\n",
        "            # add a new edge between doc and new context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(new_edge_cont)))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(all_contexts_list.index(key[0]))))\n",
        "            cont+=1"
      ],
      "metadata": {
        "id": "OerHW_ZueQpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b099ec79-1172-4846-d622-529cd6fb28af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.8 s (started: 2024-08-16 14:13:08 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_test = [(\"doc_\"+str(i[0]),\"contx_\"+str(i[1])) for i in edges_tuple]"
      ],
      "metadata": {
        "id": "wWwW2UhteQpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da52db01-2e4d-46aa-bdbe-7592e071da9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 133 ms (started: 2024-08-16 14:13:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of document nodes:\",len(document_nodes_test))\n",
        "print(\"number of context nodes:\",len(context_nodes_test))\n",
        "print(\"number of edges:\",len(edges_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZOoC3pkeQpm",
        "outputId": "7ea2aaa5-3dae-4f0a-8d17-443c95c57a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 10524\n",
            "number of context nodes: 38370\n",
            "number of edges: 201586\n",
            "time: 4.56 ms (started: 2024-08-16 14:13:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(edges_test[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj_plNzUeQpm",
        "outputId": "84608b0c-6562-4cf3-db9b-c68a18697f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('doc_0', 'contx_0'), ('doc_0', 'contx_1'), ('doc_0', 'contx_2'), ('doc_0', 'contx_3'), ('doc_0', 'contx_4'), ('doc_1', 'contx_5'), ('doc_1', 'contx_6'), ('doc_1', 'contx_7'), ('doc_1', 'contx_8'), ('doc_2', 'contx_9'), ('doc_2', 'contx_10'), ('doc_2', 'contx_11'), ('doc_2', 'contx_12'), ('doc_3', 'contx_13'), ('doc_3', 'contx_14'), ('doc_3', 'contx_15'), ('doc_3', 'contx_16'), ('doc_3', 'contx_17'), ('doc_4', 'contx_18'), ('doc_4', 'contx_19'), ('doc_4', 'contx_20'), ('doc_4', 'contx_21'), ('doc_4', 'contx_22'), ('doc_5', 'contx_23'), ('doc_5', 'contx_24'), ('doc_6', 'contx_25'), ('doc_6', 'contx_26'), ('doc_6', 'contx_27'), ('doc_7', 'contx_28'), ('doc_7', 'contx_29'), ('doc_8', 'contx_30'), ('doc_8', 'contx_31'), ('doc_8', 'contx_32'), ('doc_8', 'contx_33'), ('doc_8', 'contx_34'), ('doc_9', 'contx_35'), ('doc_10', 'contx_36'), ('doc_10', 'contx_37'), ('doc_10', 'contx_38'), ('doc_10', 'contx_39'), ('doc_10', 'contx_40'), ('doc_11', 'contx_41'), ('doc_11', 'contx_42'), ('doc_11', 'contx_43'), ('doc_11', 'contx_44'), ('doc_11', 'contx_45'), ('doc_12', 'contx_46'), ('doc_12', 'contx_47'), ('doc_12', 'contx_48'), ('doc_12', 'contx_49'), ('doc_12', 'contx_50'), ('doc_13', 'contx_51'), ('doc_13', 'contx_52'), ('doc_13', 'contx_53'), ('doc_14', 'contx_54'), ('doc_14', 'contx_55'), ('doc_14', 'contx_56'), ('doc_14', 'contx_57'), ('doc_14', 'contx_58'), ('doc_15', 'contx_59'), ('doc_16', 'contx_60'), ('doc_16', 'contx_61'), ('doc_16', 'contx_62'), ('doc_16', 'contx_63'), ('doc_17', 'contx_64'), ('doc_17', 'contx_65'), ('doc_17', 'contx_66'), ('doc_17', 'contx_67'), ('doc_17', 'contx_68'), ('doc_18', 'contx_69'), ('doc_18', 'contx_70'), ('doc_18', 'contx_71'), ('doc_19', 'contx_72'), ('doc_19', 'contx_73'), ('doc_19', 'contx_74'), ('doc_19', 'contx_75'), ('doc_19', 'contx_76'), ('doc_20', 'contx_77'), ('doc_20', 'contx_78'), ('doc_20', 'contx_79'), ('doc_20', 'contx_80'), ('doc_21', 'contx_81'), ('doc_21', 'contx_82'), ('doc_21', 'contx_83'), ('doc_21', 'contx_84'), ('doc_21', 'contx_85'), ('doc_22', 'contx_86'), ('doc_22', 'contx_87'), ('doc_22', 'contx_88'), ('doc_22', 'contx_89'), ('doc_23', 'contx_90'), ('doc_23', 'contx_52'), ('doc_23', 'contx_53'), ('doc_24', 'contx_91'), ('doc_24', 'contx_92'), ('doc_24', 'contx_93'), ('doc_24', 'contx_94'), ('doc_24', 'contx_95'), ('doc_25', 'contx_96'), ('doc_25', 'contx_97')]\n",
            "time: 523 µs (started: 2024-08-16 14:13:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test graph Conectivity with networkx"
      ],
      "metadata": {
        "id": "KgiblP-UeQpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Bipartide graph\n",
        "B = nx.Graph()\n",
        "B.add_nodes_from(document_nodes_test, bipartite=0)\n",
        "B.add_nodes_from(context_nodes_test, bipartite=1)\n",
        "B.add_edges_from(edges_test)"
      ],
      "metadata": {
        "id": "RuUkEAGLeQpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f62b47-1cb5-42ff-daef-470bd73f98af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 394 ms (started: 2024-08-16 14:13:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.is_connected(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFDpkX2oeQpn",
        "outputId": "6d5fd245-abd0-498e-9623-1a8bec2393fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 103
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 113 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of conected elements\n",
        "# if == 1 -> all elements of the graph are conected\n",
        "print(nx.number_connected_components(B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvLEAfMUeQpn",
        "outputId": "83d5d266-d34c-4f1d-942e-ec0027ef3bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "time: 125 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size of a cluster\n",
        "print(len(nx.node_connected_component(B,'doc_0')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhhBhwAAeQpn",
        "outputId": "6595f371-bf10-4ac4-c3d5-8eed9a0ae1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48894\n",
            "time: 92 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Graph HeteroData Object"
      ],
      "metadata": {
        "id": "RKQJ0GpkeQpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining nodes, edges and class labels\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# nodes\n",
        "data['document'].x = document_nodes\n",
        "data['concept'].x = context_nodes\n",
        "\n",
        "# edges\n",
        "data['document', 'has', 'concept'].edge_index = edges\n",
        "\n",
        "#class labels\n",
        "data['document'].y = y\n"
      ],
      "metadata": {
        "id": "xr8Q3wkreQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb9ff97-4c69-4a5b-921f-1c123aa16ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 664 µs (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting graph to undirected\n",
        "data = T.ToUndirected()(data)"
      ],
      "metadata": {
        "id": "uQpYAViGeQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8957d9d0-5a0f-409f-bfd4-91fe131fb496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.74 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicate edges\n",
        "data = T.RemoveDuplicatedEdges()(data)"
      ],
      "metadata": {
        "id": "hHrIrV2geQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f26f525-37c9-4c10-aa10-bfb962a6bfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 32.9 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date in using gpu\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "PSgA_wDDeQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0133e37-b2b8-4878-98e6-e0728854ddce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.2 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RM4Ioq_eQpo",
        "outputId": "cee8c926-0485-4eb2-da2e-13581347feb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  document={\n",
              "    x=[10524, 384],\n",
              "    y=[10524],\n",
              "  },\n",
              "  concept={ x=[38370, 384] },\n",
              "  (document, has, concept)={ edge_index=[2, 167583] },\n",
              "  (concept, rev_has, document)={ edge_index=[2, 167583] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.01 ms (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "1vulhpj_Knn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 1 ❎"
      ],
      "metadata": {
        "id": "tfzXBr_VKnn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "gqT_pA39Knn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J5coXuoKnn1",
        "outputId": "267a4d9f-3227-4c0e-ddb0-e8fd58d8558a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.0329, Train: 0.9375, Test: 0.4499\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5202, Train: 1.0000, Test: 0.5810\n",
            "Early stopping:  2.4838286329091774\n",
            "Epoch: 003, Loss: 0.0411, Train: 1.0000, Test: 0.5817\n",
            "Early stopping:  2.179561579391559\n",
            "Epoch: 004, Loss: 0.0113, Train: 1.0000, Test: 0.5845\n",
            "Early stopping:  1.9351033586258048\n",
            "Epoch: 005, Loss: 0.0038, Train: 1.0000, Test: 0.5802\n",
            "Early stopping:  1.7526724011720107\n",
            "Epoch: 006, Loss: 0.0002, Train: 1.0000, Test: 0.5729\n",
            "Early stopping:  0.226910760829561\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.5679\n",
            "Early stopping:  0.0172634282066465\n",
            "Epoch: 008, Loss: 0.0000, Train: 1.0000, Test: 0.5652\n",
            "Early stopping:  0.004886694805615043\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 14,  2, 14], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.61      0.57      0.59       401\n",
            "             ecology       0.31      0.26      0.28       888\n",
            "            economic       0.53      0.40      0.45      1408\n",
            "          geophysics       0.97      0.80      0.88      1201\n",
            "  gravitional_theory       0.25      0.95      0.40       129\n",
            "               hydro       0.27      0.24      0.25       354\n",
            "                math       0.62      0.81      0.70      1338\n",
            "              metals       0.67      0.87      0.76       200\n",
            "          networking       0.63      0.91      0.75       344\n",
            "        neuroscience       0.97      0.92      0.94       306\n",
            "        oceanography       0.44      0.58      0.50       989\n",
            "             politic       0.56      0.78      0.65       602\n",
            "           sociology       0.51      0.37      0.43       738\n",
            "software_engineering       0.89      0.36      0.51       523\n",
            "          statistics       0.40      0.44      0.42       646\n",
            "    theory_computing       0.87      0.25      0.39       441\n",
            "\n",
            "            accuracy                           0.57     10508\n",
            "           macro avg       0.59      0.59      0.56     10508\n",
            "        weighted avg       0.60      0.57      0.56     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7466, Train: 1.0000, Test: 0.4610\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2552, Train: 1.0000, Test: 0.5767\n",
            "Early stopping:  2.4688248816749057\n",
            "Epoch: 003, Loss: 0.0071, Train: 1.0000, Test: 0.5717\n",
            "Early stopping:  2.091092610306642\n",
            "Epoch: 004, Loss: 0.0019, Train: 1.0000, Test: 0.5552\n",
            "Early stopping:  1.8331067163762222\n",
            "Epoch: 005, Loss: 0.0006, Train: 1.0000, Test: 0.5389\n",
            "Early stopping:  1.6495606975387576\n",
            "Epoch: 006, Loss: 0.0002, Train: 1.0000, Test: 0.5237\n",
            "Early stopping:  0.11306838502397917\n",
            "Epoch: 007, Loss: 0.0001, Train: 1.0000, Test: 0.5123\n",
            "Early stopping:  0.0029504130377352106\n",
            "PREDICTIONS -> tensor([15,  0, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.10      0.39      0.16       401\n",
            "             ecology       0.93      0.54      0.68       888\n",
            "            economic       0.43      0.69      0.53      1408\n",
            "          geophysics       0.94      0.66      0.78      1201\n",
            "  gravitional_theory       0.78      0.89      0.83       129\n",
            "               hydro       0.61      0.53      0.57       354\n",
            "                math       0.96      0.33      0.49      1338\n",
            "              metals       0.52      0.88      0.66       200\n",
            "          networking       0.93      0.65      0.76       344\n",
            "        neuroscience       0.74      0.98      0.84       306\n",
            "        oceanography       0.70      0.92      0.80       989\n",
            "             politic       0.51      0.15      0.23       602\n",
            "           sociology       0.51      0.03      0.05       738\n",
            "software_engineering       0.96      0.09      0.17       523\n",
            "          statistics       0.15      0.15      0.15       646\n",
            "    theory_computing       0.29      0.87      0.44       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.63      0.55      0.51     10508\n",
            "        weighted avg       0.66      0.51      0.50     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6022, Train: 1.0000, Test: 0.5297\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2939, Train: 1.0000, Test: 0.5981\n",
            "Early stopping:  2.3393125052353554\n",
            "Epoch: 003, Loss: 0.0169, Train: 1.0000, Test: 0.6174\n",
            "Early stopping:  1.994837300559723\n",
            "Epoch: 004, Loss: 0.0008, Train: 1.0000, Test: 0.6025\n",
            "Early stopping:  1.7543404644845049\n",
            "Epoch: 005, Loss: 0.0003, Train: 1.0000, Test: 0.5785\n",
            "Early stopping:  1.581028788792102\n",
            "Epoch: 006, Loss: 0.0010, Train: 1.0000, Test: 0.5593\n",
            "Early stopping:  0.12951872136116666\n",
            "Epoch: 007, Loss: 0.0013, Train: 1.0000, Test: 0.5521\n",
            "Early stopping:  0.0071708644749757\n",
            "PREDICTIONS -> tensor([15, 11,  5,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.94      0.08      0.15       401\n",
            "             ecology       0.60      0.76      0.67       888\n",
            "            economic       0.64      0.54      0.58      1408\n",
            "          geophysics       0.84      0.75      0.79      1201\n",
            "  gravitional_theory       0.75      0.94      0.83       129\n",
            "               hydro       0.27      0.57      0.37       354\n",
            "                math       0.92      0.62      0.74      1338\n",
            "              metals       0.67      0.50      0.57       200\n",
            "          networking       0.88      0.83      0.85       344\n",
            "        neuroscience       0.90      0.98      0.94       306\n",
            "        oceanography       0.78      0.40      0.53       989\n",
            "             politic       0.55      0.52      0.54       602\n",
            "           sociology       0.72      0.25      0.37       738\n",
            "software_engineering       0.87      0.58      0.69       523\n",
            "          statistics       0.09      0.01      0.01       646\n",
            "    theory_computing       0.14      0.89      0.25       441\n",
            "\n",
            "            accuracy                           0.55     10508\n",
            "           macro avg       0.66      0.58      0.56     10508\n",
            "        weighted avg       0.68      0.55      0.57     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3759, Train: 1.0000, Test: 0.5502\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.1505, Train: 1.0000, Test: 0.5785\n",
            "Early stopping:  2.280655492830713\n",
            "Epoch: 003, Loss: 0.0038, Train: 1.0000, Test: 0.5482\n",
            "Early stopping:  1.905922968136223\n",
            "Epoch: 004, Loss: 0.0007, Train: 1.0000, Test: 0.5116\n",
            "Early stopping:  1.6635705289339748\n",
            "Epoch: 005, Loss: 0.0002, Train: 1.0000, Test: 0.4891\n",
            "Early stopping:  1.4937706347743696\n",
            "Epoch: 006, Loss: 0.0002, Train: 1.0000, Test: 0.4675\n",
            "Early stopping:  0.06679060607209447\n",
            "Epoch: 007, Loss: 0.0002, Train: 1.0000, Test: 0.4536\n",
            "Early stopping:  0.0015634391047390723\n",
            "PREDICTIONS -> tensor([15,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.30      0.45      0.36       401\n",
            "             ecology       0.46      0.48      0.47       888\n",
            "            economic       0.76      0.67      0.71      1408\n",
            "          geophysics       0.99      0.26      0.41      1201\n",
            "  gravitional_theory       0.63      0.93      0.75       129\n",
            "               hydro       0.21      0.84      0.34       354\n",
            "                math       0.89      0.26      0.40      1338\n",
            "              metals       0.33      0.90      0.48       200\n",
            "          networking       0.91      0.23      0.37       344\n",
            "        neuroscience       0.94      0.95      0.95       306\n",
            "        oceanography       0.12      0.06      0.08       989\n",
            "             politic       0.50      0.61      0.55       602\n",
            "           sociology       0.45      0.26      0.33       738\n",
            "software_engineering       0.91      0.56      0.70       523\n",
            "          statistics       0.27      0.71      0.39       646\n",
            "    theory_computing       0.28      0.48      0.35       441\n",
            "\n",
            "            accuracy                           0.45     10508\n",
            "           macro avg       0.56      0.54      0.48     10508\n",
            "        weighted avg       0.60      0.45      0.45     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7689, Train: 1.0000, Test: 0.4118\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.1059, Train: 1.0000, Test: 0.4313\n",
            "Early stopping:  2.5901071376386122\n",
            "Epoch: 003, Loss: 0.0015, Train: 1.0000, Test: 0.4315\n",
            "Early stopping:  2.1455873798932705\n",
            "Epoch: 004, Loss: 0.0002, Train: 1.0000, Test: 0.4252\n",
            "Early stopping:  1.8671570525749435\n",
            "Epoch: 005, Loss: 0.0001, Train: 1.0000, Test: 0.4172\n",
            "Early stopping:  1.6740725049803233\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.4094\n",
            "Early stopping:  0.047165617880746254\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.4007\n",
            "Early stopping:  0.0006435542143297581\n",
            "PREDICTIONS -> tensor([0, 0, 0,  ..., 6, 0, 6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.34      0.86      0.49       401\n",
            "             ecology       0.91      0.45      0.61       888\n",
            "            economic       0.38      0.06      0.11      1408\n",
            "          geophysics       0.93      0.23      0.38      1201\n",
            "  gravitional_theory       0.54      0.87      0.66       129\n",
            "               hydro       0.52      0.21      0.30       354\n",
            "                math       0.46      0.26      0.33      1338\n",
            "              metals       0.31      0.90      0.46       200\n",
            "          networking       0.73      0.87      0.79       344\n",
            "        neuroscience       0.95      0.93      0.94       306\n",
            "        oceanography       0.35      0.95      0.51       989\n",
            "             politic       0.22      0.55      0.31       602\n",
            "           sociology       0.14      0.07      0.09       738\n",
            "software_engineering       0.81      0.07      0.13       523\n",
            "          statistics       0.58      0.60      0.59       646\n",
            "    theory_computing       0.05      0.09      0.07       441\n",
            "\n",
            "            accuracy                           0.40     10508\n",
            "           macro avg       0.51      0.50      0.42     10508\n",
            "        weighted avg       0.52      0.40      0.37     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6555, Train: 0.8750, Test: 0.4534\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.3149, Train: 0.9375, Test: 0.5051\n",
            "Early stopping:  2.3621279219600306\n",
            "Epoch: 003, Loss: 0.5603, Train: 1.0000, Test: 0.5477\n",
            "Early stopping:  1.8618937693493716\n",
            "Epoch: 004, Loss: 0.0175, Train: 1.0000, Test: 0.5519\n",
            "Early stopping:  1.6935590320860665\n",
            "Epoch: 005, Loss: 0.0072, Train: 0.9375, Test: 0.5447\n",
            "Early stopping:  1.551269647523834\n",
            "Epoch: 006, Loss: 0.1197, Train: 1.0000, Test: 0.5484\n",
            "Early stopping:  0.23444284310097377\n",
            "Epoch: 007, Loss: 0.0008, Train: 1.0000, Test: 0.5454\n",
            "Early stopping:  0.23929634129074498\n",
            "Epoch: 008, Loss: 0.0006, Train: 1.0000, Test: 0.5404\n",
            "Early stopping:  0.05109196665719627\n",
            "Epoch: 009, Loss: 0.0005, Train: 1.0000, Test: 0.5346\n",
            "Early stopping:  0.052605770883578755\n",
            "Epoch: 010, Loss: 0.0003, Train: 1.0000, Test: 0.5266\n",
            "Early stopping:  0.05330023441418505\n",
            "Epoch: 011, Loss: 0.0002, Train: 1.0000, Test: 0.5195\n",
            "Early stopping:  0.00024948988420859413\n",
            "PREDICTIONS -> tensor([13, 15, 13,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.60      0.01      0.03       401\n",
            "             ecology       0.45      0.98      0.62       888\n",
            "            economic       0.80      0.48      0.60      1408\n",
            "          geophysics       0.67      0.89      0.76      1201\n",
            "  gravitional_theory       0.81      0.95      0.87       129\n",
            "               hydro       0.41      0.45      0.43       354\n",
            "                math       0.97      0.29      0.44      1338\n",
            "              metals       0.48      0.68      0.56       200\n",
            "          networking       0.68      0.92      0.79       344\n",
            "        neuroscience       0.94      0.73      0.82       306\n",
            "        oceanography       0.97      0.14      0.25       989\n",
            "             politic       0.46      0.74      0.57       602\n",
            "           sociology       0.30      0.19      0.23       738\n",
            "software_engineering       0.62      0.68      0.65       523\n",
            "          statistics       0.29      0.04      0.08       646\n",
            "    theory_computing       0.20      0.86      0.32       441\n",
            "\n",
            "            accuracy                           0.52     10508\n",
            "           macro avg       0.60      0.57      0.50     10508\n",
            "        weighted avg       0.65      0.52      0.49     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7414, Train: 1.0000, Test: 0.5133\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.0758, Train: 1.0000, Test: 0.5758\n",
            "Early stopping:  1.8848662395824904\n",
            "Epoch: 003, Loss: 0.0007, Train: 1.0000, Test: 0.5811\n",
            "Early stopping:  1.5611126958679271\n",
            "Epoch: 004, Loss: 0.0010, Train: 1.0000, Test: 0.5779\n",
            "Early stopping:  1.3582387901882342\n",
            "Epoch: 005, Loss: 0.0016, Train: 1.0000, Test: 0.5836\n",
            "Early stopping:  1.217573685772905\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.5847\n",
            "Early stopping:  0.03351962566947803\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.5837\n",
            "Early stopping:  0.0006625012254116622\n",
            "PREDICTIONS -> tensor([13, 13, 13,  ..., 13, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.86      0.08      0.14       401\n",
            "             ecology       0.52      0.90      0.66       888\n",
            "            economic       0.77      0.57      0.66      1408\n",
            "          geophysics       0.77      0.88      0.82      1201\n",
            "  gravitional_theory       0.94      0.85      0.89       129\n",
            "               hydro       0.77      0.65      0.71       354\n",
            "                math       0.76      0.26      0.39      1338\n",
            "              metals       0.78      0.81      0.79       200\n",
            "          networking       0.70      0.86      0.77       344\n",
            "        neuroscience       0.95      0.90      0.92       306\n",
            "        oceanography       0.87      0.24      0.38       989\n",
            "             politic       0.49      0.74      0.59       602\n",
            "           sociology       0.61      0.44      0.51       738\n",
            "software_engineering       0.36      0.98      0.53       523\n",
            "          statistics       0.31      0.74      0.43       646\n",
            "    theory_computing       0.62      0.05      0.10       441\n",
            "\n",
            "            accuracy                           0.58     10508\n",
            "           macro avg       0.69      0.62      0.58     10508\n",
            "        weighted avg       0.68      0.58      0.56     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7025, Train: 1.0000, Test: 0.5124\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2219, Train: 1.0000, Test: 0.5476\n",
            "Early stopping:  2.461156146684515\n",
            "Epoch: 003, Loss: 0.0090, Train: 1.0000, Test: 0.5456\n",
            "Early stopping:  2.0737319037588415\n",
            "Epoch: 004, Loss: 0.0004, Train: 1.0000, Test: 0.5394\n",
            "Early stopping:  1.815601162100802\n",
            "Epoch: 005, Loss: 0.0000, Train: 1.0000, Test: 0.5265\n",
            "Early stopping:  1.6327089872250256\n",
            "Epoch: 006, Loss: 0.0000, Train: 1.0000, Test: 0.5149\n",
            "Early stopping:  0.0982704207929442\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.5051\n",
            "Early stopping:  0.003975097654329166\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.78      0.73       401\n",
            "             ecology       0.36      0.25      0.30       888\n",
            "            economic       0.24      0.05      0.08      1408\n",
            "          geophysics       0.99      0.59      0.73      1201\n",
            "  gravitional_theory       0.54      0.95      0.69       129\n",
            "               hydro       0.31      0.28      0.29       354\n",
            "                math       0.77      0.68      0.72      1338\n",
            "              metals       0.49      0.91      0.64       200\n",
            "          networking       0.37      0.99      0.54       344\n",
            "        neuroscience       0.66      0.98      0.79       306\n",
            "        oceanography       0.37      0.53      0.44       989\n",
            "             politic       0.74      0.70      0.72       602\n",
            "           sociology       0.35      0.54      0.43       738\n",
            "software_engineering       0.43      0.68      0.53       523\n",
            "          statistics       0.28      0.44      0.34       646\n",
            "    theory_computing       0.85      0.15      0.26       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.53      0.59      0.51     10508\n",
            "        weighted avg       0.54      0.51      0.48     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5826, Train: 0.9375, Test: 0.4276\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2499, Train: 1.0000, Test: 0.4829\n",
            "Early stopping:  2.356591406572518\n",
            "Epoch: 003, Loss: 0.0290, Train: 1.0000, Test: 0.4674\n",
            "Early stopping:  1.9909668512368373\n",
            "Epoch: 004, Loss: 0.0016, Train: 1.0000, Test: 0.4377\n",
            "Early stopping:  1.7480754376767262\n",
            "Epoch: 005, Loss: 0.0002, Train: 1.0000, Test: 0.4129\n",
            "Early stopping:  1.5742622889519762\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.3916\n",
            "Early stopping:  0.1089777871383166\n",
            "Epoch: 007, Loss: 0.0002, Train: 1.0000, Test: 0.3790\n",
            "Early stopping:  0.012755175647419896\n",
            "Epoch: 008, Loss: 0.0003, Train: 1.0000, Test: 0.3686\n",
            "Early stopping:  0.0006500460553212257\n",
            "PREDICTIONS -> tensor([ 0,  8, 13,  ...,  2, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.87      0.40      0.55       401\n",
            "             ecology       0.52      0.19      0.28       888\n",
            "            economic       0.70      0.09      0.15      1408\n",
            "          geophysics       0.48      0.37      0.42      1201\n",
            "  gravitional_theory       0.39      0.96      0.55       129\n",
            "               hydro       0.05      0.02      0.03       354\n",
            "                math       0.23      0.11      0.15      1338\n",
            "              metals       0.65      0.21      0.32       200\n",
            "          networking       0.22      0.97      0.36       344\n",
            "        neuroscience       0.22      0.98      0.35       306\n",
            "        oceanography       0.33      0.97      0.49       989\n",
            "             politic       0.54      0.79      0.65       602\n",
            "           sociology       0.21      0.13      0.16       738\n",
            "software_engineering       0.85      0.57      0.68       523\n",
            "          statistics       0.66      0.05      0.09       646\n",
            "    theory_computing       0.90      0.36      0.51       441\n",
            "\n",
            "            accuracy                           0.37     10508\n",
            "           macro avg       0.49      0.45      0.36     10508\n",
            "        weighted avg       0.48      0.37      0.32     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0421, Train: 0.9375, Test: 0.3645\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2028, Train: 1.0000, Test: 0.4395\n",
            "Early stopping:  2.0076889944694063\n",
            "Epoch: 003, Loss: 0.0069, Train: 1.0000, Test: 0.4461\n",
            "Early stopping:  1.6986331289340857\n",
            "Epoch: 004, Loss: 0.0028, Train: 1.0000, Test: 0.4456\n",
            "Early stopping:  1.4885527509003078\n",
            "Epoch: 005, Loss: 0.0026, Train: 1.0000, Test: 0.4429\n",
            "Early stopping:  1.3391728744073403\n",
            "Epoch: 006, Loss: 0.0012, Train: 1.0000, Test: 0.4400\n",
            "Early stopping:  0.08919188317666829\n",
            "Epoch: 007, Loss: 0.0003, Train: 1.0000, Test: 0.4360\n",
            "Early stopping:  0.002539244724680405\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 14, 13, 14], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.79      0.78       401\n",
            "             ecology       0.15      0.13      0.14       888\n",
            "            economic       0.54      0.06      0.11      1408\n",
            "          geophysics       0.99      0.42      0.59      1201\n",
            "  gravitional_theory       0.77      0.85      0.81       129\n",
            "               hydro       0.00      0.00      0.00       354\n",
            "                math       0.80      0.62      0.70      1338\n",
            "              metals       0.16      0.94      0.27       200\n",
            "          networking       0.67      0.90      0.77       344\n",
            "        neuroscience       0.93      0.94      0.93       306\n",
            "        oceanography       0.23      0.37      0.28       989\n",
            "             politic       0.89      0.14      0.25       602\n",
            "           sociology       0.29      0.85      0.43       738\n",
            "software_engineering       0.83      0.47      0.60       523\n",
            "          statistics       0.40      0.77      0.53       646\n",
            "    theory_computing       0.05      0.01      0.02       441\n",
            "\n",
            "            accuracy                           0.44     10508\n",
            "           macro avg       0.53      0.52      0.45     10508\n",
            "        weighted avg       0.55      0.44      0.41     10508\n",
            "\n",
            "time: 3.35 s (started: 2024-08-16 14:13:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnULagO0Knn1",
        "outputId": "9ed5f606-8bf4-4772-d3be-7c99285d063d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 402 ms (started: 2024-08-16 14:13:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "vO3G2e2pKnn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a92KAzqKnn1",
        "outputId": "73f5a0df-f0e0-4742-f0a5-9c1eba320adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7849, Train: 1.0000, Test: 0.3588\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3904, Train: 1.0000, Test: 0.5153\n",
            "Early stopping:  0.2789125019922387\n",
            "Epoch: 003, Loss: 1.9320, Train: 1.0000, Test: 0.5448\n",
            "Early stopping:  0.4268626248222807\n",
            "Epoch: 004, Loss: 1.4255, Train: 1.0000, Test: 0.5674\n",
            "Early stopping:  0.5865529875924703\n",
            "Epoch: 005, Loss: 0.9414, Train: 1.0000, Test: 0.5826\n",
            "Early stopping:  0.736293188638564\n",
            "Epoch: 006, Loss: 0.5463, Train: 1.0000, Test: 0.5938\n",
            "Early stopping:  0.7403746679234149\n",
            "Epoch: 007, Loss: 0.2773, Train: 1.0000, Test: 0.5991\n",
            "Early stopping:  0.666754974312117\n",
            "Epoch: 008, Loss: 0.1275, Train: 1.0000, Test: 0.6038\n",
            "Early stopping:  0.5263336341448823\n",
            "Epoch: 009, Loss: 0.0571, Train: 1.0000, Test: 0.6045\n",
            "Early stopping:  0.3608582587237114\n",
            "Epoch: 010, Loss: 0.0266, Train: 1.0000, Test: 0.5993\n",
            "Early stopping:  0.2129508427916691\n",
            "Epoch: 011, Loss: 0.0134, Train: 1.0000, Test: 0.5954\n",
            "Early stopping:  0.10829469256165235\n",
            "Epoch: 012, Loss: 0.0072, Train: 1.0000, Test: 0.5914\n",
            "Early stopping:  0.04927990591997582\n",
            "Epoch: 013, Loss: 0.0042, Train: 1.0000, Test: 0.5873\n",
            "Early stopping:  0.021587594407345898\n",
            "Epoch: 014, Loss: 0.0026, Train: 1.0000, Test: 0.5837\n",
            "Early stopping:  0.009772081315245514\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ...,  0, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.25      0.68      0.37       401\n",
            "             ecology       0.43      0.17      0.24       888\n",
            "            economic       0.69      0.48      0.57      1408\n",
            "          geophysics       0.98      0.68      0.80      1201\n",
            "  gravitional_theory       0.69      0.90      0.78       129\n",
            "               hydro       0.21      0.31      0.25       354\n",
            "                math       0.81      0.66      0.73      1338\n",
            "              metals       0.47      0.93      0.62       200\n",
            "          networking       0.76      0.88      0.81       344\n",
            "        neuroscience       0.92      0.95      0.93       306\n",
            "        oceanography       0.46      0.72      0.56       989\n",
            "             politic       0.65      0.78      0.71       602\n",
            "           sociology       0.46      0.59      0.52       738\n",
            "software_engineering       0.70      0.60      0.65       523\n",
            "          statistics       0.50      0.40      0.44       646\n",
            "    theory_computing       0.69      0.29      0.41       441\n",
            "\n",
            "            accuracy                           0.58     10508\n",
            "           macro avg       0.60      0.63      0.59     10508\n",
            "        weighted avg       0.64      0.58      0.59     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7918, Train: 1.0000, Test: 0.3105\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3848, Train: 1.0000, Test: 0.4538\n",
            "Early stopping:  0.2877683978013478\n",
            "Epoch: 003, Loss: 1.9331, Train: 1.0000, Test: 0.4579\n",
            "Early stopping:  0.42950422568339575\n",
            "Epoch: 004, Loss: 1.4343, Train: 1.0000, Test: 0.4652\n",
            "Early stopping:  0.5846647480769293\n",
            "Epoch: 005, Loss: 0.9507, Train: 1.0000, Test: 0.4809\n",
            "Early stopping:  0.7330445937466314\n",
            "Epoch: 006, Loss: 0.5546, Train: 1.0000, Test: 0.4974\n",
            "Early stopping:  0.7345982894896669\n",
            "Epoch: 007, Loss: 0.2836, Train: 1.0000, Test: 0.5135\n",
            "Early stopping:  0.6649235176495361\n",
            "Epoch: 008, Loss: 0.1293, Train: 1.0000, Test: 0.5231\n",
            "Early stopping:  0.5286605605167476\n",
            "Epoch: 009, Loss: 0.0546, Train: 1.0000, Test: 0.5266\n",
            "Early stopping:  0.3651059694024742\n",
            "Epoch: 010, Loss: 0.0230, Train: 1.0000, Test: 0.5338\n",
            "Early stopping:  0.21782893108582965\n",
            "Epoch: 011, Loss: 0.0104, Train: 1.0000, Test: 0.5366\n",
            "Early stopping:  0.11245606058541477\n",
            "Epoch: 012, Loss: 0.0052, Train: 1.0000, Test: 0.5395\n",
            "Early stopping:  0.05113021694946043\n",
            "Epoch: 013, Loss: 0.0029, Train: 1.0000, Test: 0.5421\n",
            "Early stopping:  0.021254645928914684\n",
            "Epoch: 014, Loss: 0.0017, Train: 1.0000, Test: 0.5465\n",
            "Early stopping:  0.00871004679164193\n",
            "PREDICTIONS -> tensor([15, 14, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.32      0.06      0.10       401\n",
            "             ecology       0.85      0.65      0.73       888\n",
            "            economic       0.40      0.39      0.40      1408\n",
            "          geophysics       0.90      0.77      0.83      1201\n",
            "  gravitional_theory       0.22      0.94      0.36       129\n",
            "               hydro       0.42      0.84      0.56       354\n",
            "                math       0.85      0.13      0.23      1338\n",
            "              metals       0.38      0.85      0.53       200\n",
            "          networking       0.68      0.88      0.77       344\n",
            "        neuroscience       0.90      0.96      0.93       306\n",
            "        oceanography       0.81      0.86      0.84       989\n",
            "             politic       0.31      0.78      0.45       602\n",
            "           sociology       0.60      0.14      0.23       738\n",
            "software_engineering       0.82      0.56      0.67       523\n",
            "          statistics       0.33      0.43      0.37       646\n",
            "    theory_computing       0.42      0.68      0.52       441\n",
            "\n",
            "            accuracy                           0.55     10508\n",
            "           macro avg       0.57      0.62      0.53     10508\n",
            "        weighted avg       0.63      0.55      0.53     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7935, Train: 1.0000, Test: 0.3907\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4010, Train: 1.0000, Test: 0.5609\n",
            "Early stopping:  0.27756194837855563\n",
            "Epoch: 003, Loss: 1.9462, Train: 1.0000, Test: 0.5725\n",
            "Early stopping:  0.4240619822532584\n",
            "Epoch: 004, Loss: 1.4358, Train: 1.0000, Test: 0.5847\n",
            "Early stopping:  0.5855631611051259\n",
            "Epoch: 005, Loss: 0.9462, Train: 1.0000, Test: 0.6011\n",
            "Early stopping:  0.7376631366880487\n",
            "Epoch: 006, Loss: 0.5486, Train: 1.0000, Test: 0.6166\n",
            "Early stopping:  0.744454120531407\n",
            "Epoch: 007, Loss: 0.2751, Train: 1.0000, Test: 0.6229\n",
            "Early stopping:  0.6731624445122356\n",
            "Epoch: 008, Loss: 0.1205, Train: 1.0000, Test: 0.6290\n",
            "Early stopping:  0.5327415556706905\n",
            "Epoch: 009, Loss: 0.0492, Train: 1.0000, Test: 0.6312\n",
            "Early stopping:  0.3662434851549448\n",
            "Epoch: 010, Loss: 0.0205, Train: 1.0000, Test: 0.6297\n",
            "Early stopping:  0.21705086485772107\n",
            "Epoch: 011, Loss: 0.0092, Train: 1.0000, Test: 0.6274\n",
            "Early stopping:  0.10965991679040929\n",
            "Epoch: 012, Loss: 0.0045, Train: 1.0000, Test: 0.6211\n",
            "Early stopping:  0.047813328968105695\n",
            "Epoch: 013, Loss: 0.0024, Train: 1.0000, Test: 0.6143\n",
            "Early stopping:  0.01921600958097017\n",
            "Epoch: 014, Loss: 0.0014, Train: 1.0000, Test: 0.6107\n",
            "Early stopping:  0.007789896462504017\n",
            "PREDICTIONS -> tensor([ 0, 11, 11,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.90      0.32      0.47       401\n",
            "             ecology       0.72      0.72      0.72       888\n",
            "            economic       0.67      0.29      0.41      1408\n",
            "          geophysics       0.82      0.83      0.82      1201\n",
            "  gravitional_theory       0.64      0.94      0.76       129\n",
            "               hydro       0.26      0.62      0.37       354\n",
            "                math       0.94      0.57      0.71      1338\n",
            "              metals       0.54      0.63      0.58       200\n",
            "          networking       0.65      0.92      0.76       344\n",
            "        neuroscience       0.89      0.97      0.93       306\n",
            "        oceanography       0.75      0.64      0.69       989\n",
            "             politic       0.43      0.80      0.56       602\n",
            "           sociology       0.74      0.24      0.36       738\n",
            "software_engineering       0.86      0.77      0.81       523\n",
            "          statistics       0.29      0.65      0.40       646\n",
            "    theory_computing       0.44      0.65      0.53       441\n",
            "\n",
            "            accuracy                           0.61     10508\n",
            "           macro avg       0.66      0.66      0.62     10508\n",
            "        weighted avg       0.70      0.61      0.61     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7762, Train: 0.9375, Test: 0.3437\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3325, Train: 1.0000, Test: 0.4390\n",
            "Early stopping:  0.31372917061011807\n",
            "Epoch: 003, Loss: 1.8382, Train: 1.0000, Test: 0.4520\n",
            "Early stopping:  0.4692466736330508\n",
            "Epoch: 004, Loss: 1.3075, Train: 1.0000, Test: 0.4731\n",
            "Early stopping:  0.6331508405841927\n",
            "Epoch: 005, Loss: 0.8154, Train: 1.0000, Test: 0.4978\n",
            "Early stopping:  0.7824882829534406\n",
            "Epoch: 006, Loss: 0.4341, Train: 1.0000, Test: 0.5184\n",
            "Early stopping:  0.7632372718916861\n",
            "Epoch: 007, Loss: 0.1992, Train: 1.0000, Test: 0.5293\n",
            "Early stopping:  0.6632922150006196\n",
            "Epoch: 008, Loss: 0.0841, Train: 1.0000, Test: 0.5337\n",
            "Early stopping:  0.4990449308681399\n",
            "Epoch: 009, Loss: 0.0353, Train: 1.0000, Test: 0.5330\n",
            "Early stopping:  0.3199664101226004\n",
            "Epoch: 010, Loss: 0.0155, Train: 1.0000, Test: 0.5308\n",
            "Early stopping:  0.17223873079019228\n",
            "Epoch: 011, Loss: 0.0073, Train: 1.0000, Test: 0.5305\n",
            "Early stopping:  0.07902965735605581\n",
            "Epoch: 012, Loss: 0.0037, Train: 1.0000, Test: 0.5299\n",
            "Early stopping:  0.03304383934258314\n",
            "Epoch: 013, Loss: 0.0020, Train: 1.0000, Test: 0.5282\n",
            "Early stopping:  0.013622328682304353\n",
            "Epoch: 014, Loss: 0.0012, Train: 1.0000, Test: 0.5266\n",
            "Early stopping:  0.005844243778603189\n",
            "PREDICTIONS -> tensor([15,  0, 13,  ...,  6, 15, 14], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.19      0.21      0.20       401\n",
            "             ecology       0.50      0.77      0.60       888\n",
            "            economic       0.83      0.58      0.69      1408\n",
            "          geophysics       0.94      0.74      0.83      1201\n",
            "  gravitional_theory       0.39      0.95      0.56       129\n",
            "               hydro       0.56      0.77      0.65       354\n",
            "                math       0.73      0.23      0.35      1338\n",
            "              metals       0.61      0.86      0.71       200\n",
            "          networking       0.76      0.28      0.41       344\n",
            "        neuroscience       0.81      0.99      0.89       306\n",
            "        oceanography       0.43      0.18      0.25       989\n",
            "             politic       0.47      0.41      0.44       602\n",
            "           sociology       0.32      0.54      0.40       738\n",
            "software_engineering       0.77      0.72      0.75       523\n",
            "          statistics       0.30      0.84      0.44       646\n",
            "    theory_computing       0.16      0.11      0.13       441\n",
            "\n",
            "            accuracy                           0.53     10508\n",
            "           macro avg       0.55      0.57      0.52     10508\n",
            "        weighted avg       0.60      0.53      0.52     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7601, Train: 1.0000, Test: 0.3593\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3317, Train: 1.0000, Test: 0.4349\n",
            "Early stopping:  0.30295525601897527\n",
            "Epoch: 003, Loss: 1.8363, Train: 1.0000, Test: 0.4621\n",
            "Early stopping:  0.4622998063288336\n",
            "Epoch: 004, Loss: 1.3034, Train: 1.0000, Test: 0.4733\n",
            "Early stopping:  0.6288732677869423\n",
            "Epoch: 005, Loss: 0.8096, Train: 1.0000, Test: 0.4872\n",
            "Early stopping:  0.7799162458733037\n",
            "Epoch: 006, Loss: 0.4308, Train: 1.0000, Test: 0.4967\n",
            "Early stopping:  0.7646990329314972\n",
            "Epoch: 007, Loss: 0.1959, Train: 1.0000, Test: 0.5123\n",
            "Early stopping:  0.6637756583142105\n",
            "Epoch: 008, Loss: 0.0805, Train: 1.0000, Test: 0.5192\n",
            "Early stopping:  0.49851070439915507\n",
            "Epoch: 009, Loss: 0.0327, Train: 1.0000, Test: 0.5279\n",
            "Early stopping:  0.31888438786203777\n",
            "Epoch: 010, Loss: 0.0141, Train: 1.0000, Test: 0.5313\n",
            "Early stopping:  0.17177499449766864\n",
            "Epoch: 011, Loss: 0.0065, Train: 1.0000, Test: 0.5369\n",
            "Early stopping:  0.07812956594646318\n",
            "Epoch: 012, Loss: 0.0033, Train: 1.0000, Test: 0.5399\n",
            "Early stopping:  0.03177366428717112\n",
            "Epoch: 013, Loss: 0.0018, Train: 1.0000, Test: 0.5423\n",
            "Early stopping:  0.012662380447609497\n",
            "Epoch: 014, Loss: 0.0011, Train: 1.0000, Test: 0.5444\n",
            "Early stopping:  0.005283204527731343\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.41      0.83      0.55       401\n",
            "             ecology       0.77      0.70      0.73       888\n",
            "            economic       0.48      0.29      0.36      1408\n",
            "          geophysics       0.85      0.80      0.83      1201\n",
            "  gravitional_theory       0.84      0.95      0.89       129\n",
            "               hydro       0.37      0.37      0.37       354\n",
            "                math       0.77      0.45      0.57      1338\n",
            "              metals       0.57      0.81      0.67       200\n",
            "          networking       0.55      0.91      0.69       344\n",
            "        neuroscience       0.84      0.98      0.90       306\n",
            "        oceanography       0.73      0.74      0.74       989\n",
            "             politic       0.27      0.49      0.35       602\n",
            "           sociology       0.09      0.04      0.05       738\n",
            "software_engineering       0.59      0.36      0.45       523\n",
            "          statistics       0.52      0.71      0.60       646\n",
            "    theory_computing       0.07      0.14      0.10       441\n",
            "\n",
            "            accuracy                           0.54     10508\n",
            "           macro avg       0.55      0.60      0.55     10508\n",
            "        weighted avg       0.57      0.54      0.54     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7560, Train: 1.0000, Test: 0.3750\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3128, Train: 1.0000, Test: 0.4833\n",
            "Early stopping:  0.3133926701716071\n",
            "Epoch: 003, Loss: 1.8154, Train: 1.0000, Test: 0.5402\n",
            "Early stopping:  0.47056705178606884\n",
            "Epoch: 004, Loss: 1.2834, Train: 1.0000, Test: 0.5693\n",
            "Early stopping:  0.6350726101923665\n",
            "Epoch: 005, Loss: 0.8037, Train: 1.0000, Test: 0.5862\n",
            "Early stopping:  0.780441971934109\n",
            "Epoch: 006, Loss: 0.4416, Train: 1.0000, Test: 0.6019\n",
            "Early stopping:  0.7532935975500895\n",
            "Epoch: 007, Loss: 0.2178, Train: 1.0000, Test: 0.6093\n",
            "Early stopping:  0.6459247535008297\n",
            "Epoch: 008, Loss: 0.1020, Train: 1.0000, Test: 0.6085\n",
            "Early stopping:  0.4803892711364862\n",
            "Epoch: 009, Loss: 0.0484, Train: 1.0000, Test: 0.6040\n",
            "Early stopping:  0.30841060050996105\n",
            "Epoch: 010, Loss: 0.0241, Train: 1.0000, Test: 0.5970\n",
            "Early stopping:  0.1708150878653812\n",
            "Epoch: 011, Loss: 0.0125, Train: 1.0000, Test: 0.5926\n",
            "Early stopping:  0.08389765886812596\n",
            "Epoch: 012, Loss: 0.0068, Train: 1.0000, Test: 0.5890\n",
            "Early stopping:  0.03877970607833955\n",
            "Epoch: 013, Loss: 0.0039, Train: 1.0000, Test: 0.5852\n",
            "Early stopping:  0.018086566104015985\n",
            "Epoch: 014, Loss: 0.0024, Train: 1.0000, Test: 0.5830\n",
            "Early stopping:  0.008785145062562942\n",
            "PREDICTIONS -> tensor([13,  0, 13,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.54      0.25      0.34       401\n",
            "             ecology       0.49      0.95      0.65       888\n",
            "            economic       0.63      0.31      0.42      1408\n",
            "          geophysics       0.98      0.57      0.72      1201\n",
            "  gravitional_theory       0.90      0.83      0.86       129\n",
            "               hydro       0.41      0.67      0.51       354\n",
            "                math       0.64      0.65      0.64      1338\n",
            "              metals       0.37      0.51      0.43       200\n",
            "          networking       0.70      0.88      0.78       344\n",
            "        neuroscience       0.96      0.84      0.90       306\n",
            "        oceanography       0.84      0.68      0.75       989\n",
            "             politic       0.44      0.78      0.56       602\n",
            "           sociology       0.21      0.03      0.05       738\n",
            "software_engineering       0.47      0.84      0.60       523\n",
            "          statistics       0.56      0.52      0.54       646\n",
            "    theory_computing       0.36      0.58      0.44       441\n",
            "\n",
            "            accuracy                           0.58     10508\n",
            "           macro avg       0.59      0.62      0.57     10508\n",
            "        weighted avg       0.62      0.58      0.56     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7773, Train: 0.9375, Test: 0.3017\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3689, Train: 1.0000, Test: 0.4978\n",
            "Early stopping:  0.2887932405697387\n",
            "Epoch: 003, Loss: 1.9014, Train: 1.0000, Test: 0.5767\n",
            "Early stopping:  0.43832344617243124\n",
            "Epoch: 004, Loss: 1.3840, Train: 1.0000, Test: 0.6171\n",
            "Early stopping:  0.6008292356460283\n",
            "Epoch: 005, Loss: 0.8918, Train: 1.0000, Test: 0.6456\n",
            "Early stopping:  0.7526951860129552\n",
            "Epoch: 006, Loss: 0.4996, Train: 1.0000, Test: 0.6618\n",
            "Early stopping:  0.7514955709013721\n",
            "Epoch: 007, Loss: 0.2429, Train: 1.0000, Test: 0.6664\n",
            "Early stopping:  0.6696889439179092\n",
            "Epoch: 008, Loss: 0.1058, Train: 1.0000, Test: 0.6594\n",
            "Early stopping:  0.5192686346494165\n",
            "Epoch: 009, Loss: 0.0442, Train: 1.0000, Test: 0.6486\n",
            "Early stopping:  0.34651498372048795\n",
            "Epoch: 010, Loss: 0.0192, Train: 1.0000, Test: 0.6362\n",
            "Early stopping:  0.19742403493102498\n",
            "Epoch: 011, Loss: 0.0090, Train: 1.0000, Test: 0.6265\n",
            "Early stopping:  0.09633069871275675\n",
            "Epoch: 012, Loss: 0.0047, Train: 1.0000, Test: 0.6172\n",
            "Early stopping:  0.04161075409585939\n",
            "Epoch: 013, Loss: 0.0026, Train: 1.0000, Test: 0.6093\n",
            "Early stopping:  0.017037078225543126\n",
            "Epoch: 014, Loss: 0.0016, Train: 1.0000, Test: 0.6036\n",
            "Early stopping:  0.007167050909938709\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.49      0.60      0.54       401\n",
            "             ecology       0.55      0.83      0.66       888\n",
            "            economic       0.78      0.40      0.53      1408\n",
            "          geophysics       0.93      0.66      0.77      1201\n",
            "  gravitional_theory       0.90      0.80      0.84       129\n",
            "               hydro       0.37      0.86      0.52       354\n",
            "                math       0.88      0.55      0.68      1338\n",
            "              metals       0.55      0.95      0.69       200\n",
            "          networking       0.80      0.80      0.80       344\n",
            "        neuroscience       0.92      0.94      0.93       306\n",
            "        oceanography       0.90      0.31      0.47       989\n",
            "             politic       0.34      0.94      0.50       602\n",
            "           sociology       0.69      0.36      0.48       738\n",
            "software_engineering       0.69      0.89      0.77       523\n",
            "          statistics       0.39      0.38      0.39       646\n",
            "    theory_computing       0.42      0.60      0.49       441\n",
            "\n",
            "            accuracy                           0.60     10508\n",
            "           macro avg       0.66      0.68      0.63     10508\n",
            "        weighted avg       0.70      0.60      0.60     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7784, Train: 0.9375, Test: 0.2804\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3247, Train: 1.0000, Test: 0.3941\n",
            "Early stopping:  0.32078118130296085\n",
            "Epoch: 003, Loss: 1.8367, Train: 1.0000, Test: 0.4317\n",
            "Early stopping:  0.4709217874534735\n",
            "Epoch: 004, Loss: 1.3162, Train: 1.0000, Test: 0.4636\n",
            "Early stopping:  0.6295940804607727\n",
            "Epoch: 005, Loss: 0.8276, Train: 1.0000, Test: 0.4941\n",
            "Early stopping:  0.7765344134860048\n",
            "Epoch: 006, Loss: 0.4450, Train: 1.0000, Test: 0.5137\n",
            "Early stopping:  0.7549654485688735\n",
            "Epoch: 007, Loss: 0.2053, Train: 1.0000, Test: 0.5325\n",
            "Early stopping:  0.6599253433097286\n",
            "Epoch: 008, Loss: 0.0858, Train: 1.0000, Test: 0.5417\n",
            "Early stopping:  0.5015120239748228\n",
            "Epoch: 009, Loss: 0.0354, Train: 1.0000, Test: 0.5397\n",
            "Early stopping:  0.32493495734308264\n",
            "Epoch: 010, Loss: 0.0155, Train: 1.0000, Test: 0.5381\n",
            "Early stopping:  0.1769102996679887\n",
            "Epoch: 011, Loss: 0.0074, Train: 1.0000, Test: 0.5345\n",
            "Early stopping:  0.08162609225689461\n",
            "Epoch: 012, Loss: 0.0038, Train: 1.0000, Test: 0.5322\n",
            "Early stopping:  0.03370579635035882\n",
            "Epoch: 013, Loss: 0.0022, Train: 1.0000, Test: 0.5285\n",
            "Early stopping:  0.013612638475451504\n",
            "Epoch: 014, Loss: 0.0013, Train: 1.0000, Test: 0.5248\n",
            "Early stopping:  0.005764937624881745\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.82      0.73       401\n",
            "             ecology       0.39      0.34      0.37       888\n",
            "            economic       0.32      0.04      0.07      1408\n",
            "          geophysics       0.64      0.83      0.72      1201\n",
            "  gravitional_theory       0.66      0.94      0.78       129\n",
            "               hydro       0.38      0.35      0.36       354\n",
            "                math       0.84      0.35      0.49      1338\n",
            "              metals       0.58      0.93      0.72       200\n",
            "          networking       0.69      0.92      0.79       344\n",
            "        neuroscience       0.86      0.95      0.90       306\n",
            "        oceanography       0.39      0.55      0.45       989\n",
            "             politic       0.56      0.83      0.67       602\n",
            "           sociology       0.32      0.49      0.39       738\n",
            "software_engineering       0.48      0.89      0.62       523\n",
            "          statistics       0.44      0.54      0.48       646\n",
            "    theory_computing       0.64      0.27      0.38       441\n",
            "\n",
            "            accuracy                           0.52     10508\n",
            "           macro avg       0.55      0.63      0.56     10508\n",
            "        weighted avg       0.53      0.52      0.49     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7746, Train: 1.0000, Test: 0.2501\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3622, Train: 1.0000, Test: 0.3569\n",
            "Early stopping:  0.2916027494914749\n",
            "Epoch: 003, Loss: 1.8770, Train: 1.0000, Test: 0.3891\n",
            "Early stopping:  0.4493047024357883\n",
            "Epoch: 004, Loss: 1.3503, Train: 1.0000, Test: 0.4097\n",
            "Early stopping:  0.6151780043755967\n",
            "Epoch: 005, Loss: 0.8611, Train: 1.0000, Test: 0.4321\n",
            "Early stopping:  0.7657423058056458\n",
            "Epoch: 006, Loss: 0.4768, Train: 1.0000, Test: 0.4577\n",
            "Early stopping:  0.7578702099133506\n",
            "Epoch: 007, Loss: 0.2281, Train: 1.0000, Test: 0.4688\n",
            "Early stopping:  0.6655963625487858\n",
            "Epoch: 008, Loss: 0.0986, Train: 1.0000, Test: 0.4747\n",
            "Early stopping:  0.5089202803595522\n",
            "Epoch: 009, Loss: 0.0417, Train: 1.0000, Test: 0.4789\n",
            "Early stopping:  0.3354497326569379\n",
            "Epoch: 010, Loss: 0.0183, Train: 1.0000, Test: 0.4806\n",
            "Early stopping:  0.18844937460351444\n",
            "Epoch: 011, Loss: 0.0086, Train: 1.0000, Test: 0.4818\n",
            "Early stopping:  0.09034872778764495\n",
            "Epoch: 012, Loss: 0.0043, Train: 1.0000, Test: 0.4819\n",
            "Early stopping:  0.038716433992055946\n",
            "Epoch: 013, Loss: 0.0024, Train: 1.0000, Test: 0.4812\n",
            "Early stopping:  0.016096762891632786\n",
            "Epoch: 014, Loss: 0.0014, Train: 1.0000, Test: 0.4807\n",
            "Early stopping:  0.006921158481182465\n",
            "PREDICTIONS -> tensor([ 0,  8, 15,  ...,  6, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.46      0.55       401\n",
            "             ecology       0.47      0.62      0.54       888\n",
            "            economic       0.78      0.23      0.35      1408\n",
            "          geophysics       0.87      0.58      0.70      1201\n",
            "  gravitional_theory       0.43      0.94      0.59       129\n",
            "               hydro       0.20      0.12      0.15       354\n",
            "                math       0.41      0.27      0.33      1338\n",
            "              metals       0.32      0.70      0.44       200\n",
            "          networking       0.38      0.58      0.46       344\n",
            "        neuroscience       0.72      0.98      0.83       306\n",
            "        oceanography       0.51      0.84      0.63       989\n",
            "             politic       0.54      0.81      0.65       602\n",
            "           sociology       0.12      0.12      0.12       738\n",
            "software_engineering       0.73      0.47      0.57       523\n",
            "          statistics       0.54      0.20      0.30       646\n",
            "    theory_computing       0.28      0.77      0.41       441\n",
            "\n",
            "            accuracy                           0.48     10508\n",
            "           macro avg       0.50      0.54      0.48     10508\n",
            "        weighted avg       0.54      0.48      0.46     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7909, Train: 0.8750, Test: 0.3595\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3827, Train: 1.0000, Test: 0.4820\n",
            "Early stopping:  0.2886747236317261\n",
            "Epoch: 003, Loss: 1.9389, Train: 1.0000, Test: 0.5227\n",
            "Early stopping:  0.42613632662597717\n",
            "Epoch: 004, Loss: 1.4474, Train: 1.0000, Test: 0.5434\n",
            "Early stopping:  0.578133478182202\n",
            "Epoch: 005, Loss: 0.9734, Train: 1.0000, Test: 0.5473\n",
            "Early stopping:  0.7230871847830923\n",
            "Epoch: 006, Loss: 0.5798, Train: 1.0000, Test: 0.5390\n",
            "Early stopping:  0.7232267849939639\n",
            "Epoch: 007, Loss: 0.3022, Train: 1.0000, Test: 0.5296\n",
            "Early stopping:  0.6584533212347862\n",
            "Epoch: 008, Loss: 0.1395, Train: 1.0000, Test: 0.5246\n",
            "Early stopping:  0.5290468637070547\n",
            "Epoch: 009, Loss: 0.0601, Train: 1.0000, Test: 0.5179\n",
            "Early stopping:  0.3719798545511499\n",
            "Epoch: 010, Loss: 0.0261, Train: 1.0000, Test: 0.5099\n",
            "Early stopping:  0.22686232769819123\n",
            "Epoch: 011, Loss: 0.0121, Train: 1.0000, Test: 0.5024\n",
            "Early stopping:  0.11927398830766847\n",
            "Epoch: 012, Loss: 0.0061, Train: 1.0000, Test: 0.4969\n",
            "Early stopping:  0.05487157985819982\n",
            "Epoch: 013, Loss: 0.0034, Train: 1.0000, Test: 0.4919\n",
            "Early stopping:  0.023259299533469703\n",
            "Epoch: 014, Loss: 0.0021, Train: 1.0000, Test: 0.4860\n",
            "Early stopping:  0.009799567601515572\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.50      0.88      0.64       401\n",
            "             ecology       0.25      0.34      0.29       888\n",
            "            economic       0.56      0.22      0.31      1408\n",
            "          geophysics       0.91      0.62      0.74      1201\n",
            "  gravitional_theory       0.88      0.81      0.85       129\n",
            "               hydro       0.30      0.45      0.36       354\n",
            "                math       0.73      0.54      0.62      1338\n",
            "              metals       0.40      0.93      0.56       200\n",
            "          networking       0.68      0.84      0.75       344\n",
            "        neuroscience       0.74      0.98      0.84       306\n",
            "        oceanography       0.26      0.20      0.23       989\n",
            "             politic       0.46      0.73      0.57       602\n",
            "           sociology       0.39      0.18      0.24       738\n",
            "software_engineering       0.85      0.40      0.54       523\n",
            "          statistics       0.63      0.56      0.59       646\n",
            "    theory_computing       0.22      0.69      0.33       441\n",
            "\n",
            "            accuracy                           0.49     10508\n",
            "           macro avg       0.55      0.58      0.53     10508\n",
            "        weighted avg       0.55      0.49      0.48     10508\n",
            "\n",
            "time: 6.88 s (started: 2024-08-16 14:13:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Q0egUaKnn1",
        "outputId": "12322f45-abcd-4c37-fda8-98231cdc35b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 425 ms (started: 2024-08-16 14:13:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 5 ❎"
      ],
      "metadata": {
        "id": "7llnVmNBKnn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "PKImgOc6Knn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXU_KVF2Knn1",
        "outputId": "1002de0a-790e-4bdb-de40-99053c9cbbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5568, Train: 0.7750, Test: 0.6129\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8930, Train: 0.9500, Test: 0.7285\n",
            "Early stopping:  1.8835798756190893\n",
            "Epoch: 003, Loss: 0.2141, Train: 0.9750, Test: 0.7304\n",
            "Early stopping:  1.766848725006492\n",
            "Epoch: 004, Loss: 0.0837, Train: 1.0000, Test: 0.7153\n",
            "Early stopping:  1.619288021087117\n",
            "Epoch: 005, Loss: 0.0285, Train: 1.0000, Test: 0.7073\n",
            "Early stopping:  1.4949737449455156\n",
            "Epoch: 006, Loss: 0.0160, Train: 1.0000, Test: 0.7039\n",
            "Early stopping:  0.3695205251866835\n",
            "Epoch: 007, Loss: 0.0111, Train: 1.0000, Test: 0.7083\n",
            "Early stopping:  0.08522124757679383\n",
            "Epoch: 008, Loss: 0.0106, Train: 1.0000, Test: 0.7176\n",
            "Early stopping:  0.0308848296023774\n",
            "Epoch: 009, Loss: 0.0041, Train: 1.0000, Test: 0.7222\n",
            "Early stopping:  0.009135104955824416\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.81      0.62      0.70       397\n",
            "             ecology       0.81      0.81      0.81       884\n",
            "            economic       0.78      0.74      0.76      1404\n",
            "          geophysics       0.99      0.68      0.80      1197\n",
            "  gravitional_theory       0.64      0.94      0.76       125\n",
            "               hydro       0.56      0.89      0.68       350\n",
            "                math       0.68      0.62      0.65      1334\n",
            "              metals       0.46      0.94      0.61       196\n",
            "          networking       0.65      0.89      0.75       340\n",
            "        neuroscience       0.81      0.99      0.89       302\n",
            "        oceanography       0.91      0.68      0.78       985\n",
            "             politic       0.82      0.64      0.72       598\n",
            "           sociology       0.66      0.65      0.66       734\n",
            "software_engineering       0.96      0.42      0.59       519\n",
            "          statistics       0.57      0.88      0.69       642\n",
            "    theory_computing       0.50      0.85      0.63       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.73      0.77      0.72     10444\n",
            "        weighted avg       0.77      0.72      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2624, Train: 0.8750, Test: 0.6676\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6064, Train: 0.9375, Test: 0.7429\n",
            "Early stopping:  1.8780752443224755\n",
            "Epoch: 003, Loss: 0.1973, Train: 0.9875, Test: 0.7182\n",
            "Early stopping:  1.6641640857007025\n",
            "Epoch: 004, Loss: 0.0670, Train: 1.0000, Test: 0.7106\n",
            "Early stopping:  1.5037522155890435\n",
            "Epoch: 005, Loss: 0.0094, Train: 1.0000, Test: 0.7062\n",
            "Early stopping:  1.3804502812478638\n",
            "Epoch: 006, Loss: 0.0056, Train: 1.0000, Test: 0.6978\n",
            "Early stopping:  0.25218261197875247\n",
            "Epoch: 007, Loss: 0.0094, Train: 1.0000, Test: 0.6962\n",
            "Early stopping:  0.08209829271709022\n",
            "Epoch: 008, Loss: 0.0085, Train: 1.0000, Test: 0.7003\n",
            "Early stopping:  0.02633380004369637\n",
            "Epoch: 009, Loss: 0.0032, Train: 1.0000, Test: 0.7048\n",
            "Early stopping:  0.002715560524100855\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.78      0.72       397\n",
            "             ecology       0.74      0.50      0.60       884\n",
            "            economic       0.92      0.37      0.52      1404\n",
            "          geophysics       0.90      0.89      0.90      1197\n",
            "  gravitional_theory       0.94      0.89      0.91       125\n",
            "               hydro       0.87      0.72      0.78       350\n",
            "                math       0.85      0.61      0.71      1334\n",
            "              metals       0.82      0.67      0.74       196\n",
            "          networking       0.94      0.69      0.80       340\n",
            "        neuroscience       0.50      0.99      0.67       302\n",
            "        oceanography       0.76      0.86      0.81       985\n",
            "             politic       0.69      0.78      0.73       598\n",
            "           sociology       0.44      0.72      0.55       734\n",
            "software_engineering       0.81      0.84      0.82       519\n",
            "          statistics       0.47      0.91      0.62       642\n",
            "    theory_computing       0.66      0.70      0.68       437\n",
            "\n",
            "            accuracy                           0.70     10444\n",
            "           macro avg       0.75      0.75      0.72     10444\n",
            "        weighted avg       0.76      0.70      0.70     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3285, Train: 0.7250, Test: 0.6891\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8963, Train: 0.8875, Test: 0.6477\n",
            "Early stopping:  1.7198271044213493\n",
            "Epoch: 003, Loss: 0.4029, Train: 0.9875, Test: 0.6896\n",
            "Early stopping:  1.566233459245189\n",
            "Epoch: 004, Loss: 0.0961, Train: 0.9875, Test: 0.6802\n",
            "Early stopping:  1.4691768548334463\n",
            "Epoch: 005, Loss: 0.0535, Train: 1.0000, Test: 0.6699\n",
            "Early stopping:  1.3686054322408803\n",
            "Epoch: 006, Loss: 0.0157, Train: 1.0000, Test: 0.6602\n",
            "Early stopping:  0.37050895850839327\n",
            "Epoch: 007, Loss: 0.0078, Train: 1.0000, Test: 0.6537\n",
            "Early stopping:  0.16459565033681486\n",
            "Epoch: 008, Loss: 0.0072, Train: 1.0000, Test: 0.6519\n",
            "Early stopping:  0.03857008160861472\n",
            "Epoch: 009, Loss: 0.0044, Train: 1.0000, Test: 0.6537\n",
            "Early stopping:  0.02043062749758381\n",
            "Epoch: 010, Loss: 0.0020, Train: 1.0000, Test: 0.6533\n",
            "Early stopping:  0.0051562827982953\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.59      0.76      0.66       397\n",
            "             ecology       0.86      0.55      0.67       884\n",
            "            economic       0.86      0.53      0.66      1404\n",
            "          geophysics       0.92      0.51      0.66      1197\n",
            "  gravitional_theory       0.74      0.93      0.82       125\n",
            "               hydro       0.60      0.67      0.64       350\n",
            "                math       0.75      0.77      0.76      1334\n",
            "              metals       0.35      0.47      0.40       196\n",
            "          networking       0.81      0.81      0.81       340\n",
            "        neuroscience       0.35      1.00      0.52       302\n",
            "        oceanography       0.67      0.82      0.74       985\n",
            "             politic       0.76      0.53      0.63       598\n",
            "           sociology       0.63      0.71      0.67       734\n",
            "software_engineering       0.95      0.17      0.29       519\n",
            "          statistics       0.66      0.82      0.73       642\n",
            "    theory_computing       0.33      0.84      0.47       437\n",
            "\n",
            "            accuracy                           0.65     10444\n",
            "           macro avg       0.68      0.68      0.63     10444\n",
            "        weighted avg       0.74      0.65      0.65     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2883, Train: 0.7500, Test: 0.4469\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8576, Train: 0.9250, Test: 0.6957\n",
            "Early stopping:  1.7187896175984052\n",
            "Epoch: 003, Loss: 0.3133, Train: 0.9625, Test: 0.7229\n",
            "Early stopping:  1.584064292513383\n",
            "Epoch: 004, Loss: 0.1252, Train: 1.0000, Test: 0.7005\n",
            "Early stopping:  1.4615309820369615\n",
            "Epoch: 005, Loss: 0.0235, Train: 0.9875, Test: 0.6766\n",
            "Early stopping:  1.3616520091466653\n",
            "Epoch: 006, Loss: 0.0323, Train: 1.0000, Test: 0.6649\n",
            "Early stopping:  0.3483471960446323\n",
            "Epoch: 007, Loss: 0.0116, Train: 1.0000, Test: 0.6607\n",
            "Early stopping:  0.1268620440098559\n",
            "Epoch: 008, Loss: 0.0054, Train: 1.0000, Test: 0.6587\n",
            "Early stopping:  0.04897685064526909\n",
            "Epoch: 009, Loss: 0.0026, Train: 1.0000, Test: 0.6583\n",
            "Early stopping:  0.012543292093209145\n",
            "Epoch: 010, Loss: 0.0013, Train: 1.0000, Test: 0.6588\n",
            "Early stopping:  0.012739428699732396\n",
            "Epoch: 011, Loss: 0.0006, Train: 1.0000, Test: 0.6570\n",
            "Early stopping:  0.004470690845201325\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.72      0.73       397\n",
            "             ecology       0.80      0.51      0.62       884\n",
            "            economic       0.48      0.48      0.48      1404\n",
            "          geophysics       0.74      0.90      0.81      1197\n",
            "  gravitional_theory       0.87      0.92      0.89       125\n",
            "               hydro       0.46      0.62      0.53       350\n",
            "                math       0.96      0.47      0.63      1334\n",
            "              metals       0.51      0.95      0.67       196\n",
            "          networking       0.90      0.77      0.83       340\n",
            "        neuroscience       0.91      0.97      0.94       302\n",
            "        oceanography       0.67      0.74      0.70       985\n",
            "             politic       0.62      0.82      0.71       598\n",
            "           sociology       0.48      0.73      0.58       734\n",
            "software_engineering       0.67      0.88      0.76       519\n",
            "          statistics       0.60      0.29      0.39       642\n",
            "    theory_computing       0.66      0.60      0.63       437\n",
            "\n",
            "            accuracy                           0.66     10444\n",
            "           macro avg       0.69      0.71      0.68     10444\n",
            "        weighted avg       0.69      0.66      0.65     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5087, Train: 0.8125, Test: 0.6246\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6949, Train: 0.9500, Test: 0.7150\n",
            "Early stopping:  1.9896956513664235\n",
            "Epoch: 003, Loss: 0.2154, Train: 0.9875, Test: 0.7490\n",
            "Early stopping:  1.779210609434886\n",
            "Epoch: 004, Loss: 0.0747, Train: 1.0000, Test: 0.7272\n",
            "Early stopping:  1.6122075911337634\n",
            "Epoch: 005, Loss: 0.0376, Train: 1.0000, Test: 0.7187\n",
            "Early stopping:  1.4782547862620954\n",
            "Epoch: 006, Loss: 0.0086, Train: 1.0000, Test: 0.7091\n",
            "Early stopping:  0.2844664122780423\n",
            "Epoch: 007, Loss: 0.0019, Train: 1.0000, Test: 0.6968\n",
            "Early stopping:  0.08748058573298177\n",
            "Epoch: 008, Loss: 0.0009, Train: 1.0000, Test: 0.6853\n",
            "Early stopping:  0.03168339338734342\n",
            "Epoch: 009, Loss: 0.0009, Train: 1.0000, Test: 0.6760\n",
            "Early stopping:  0.01576432878649823\n",
            "Epoch: 010, Loss: 0.0010, Train: 1.0000, Test: 0.6670\n",
            "Early stopping:  0.0033458683568096696\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.73      0.73       397\n",
            "             ecology       0.62      0.86      0.72       884\n",
            "            economic       0.80      0.48      0.60      1404\n",
            "          geophysics       0.79      0.89      0.84      1197\n",
            "  gravitional_theory       0.86      0.91      0.89       125\n",
            "               hydro       0.31      0.93      0.46       350\n",
            "                math       0.95      0.45      0.62      1334\n",
            "              metals       0.75      0.78      0.76       196\n",
            "          networking       0.91      0.72      0.80       340\n",
            "        neuroscience       0.98      0.90      0.94       302\n",
            "        oceanography       0.96      0.44      0.60       985\n",
            "             politic       0.47      0.68      0.55       598\n",
            "           sociology       0.46      0.64      0.54       734\n",
            "software_engineering       0.68      0.81      0.74       519\n",
            "          statistics       0.73      0.66      0.69       642\n",
            "    theory_computing       0.58      0.70      0.63       437\n",
            "\n",
            "            accuracy                           0.67     10444\n",
            "           macro avg       0.72      0.72      0.70     10444\n",
            "        weighted avg       0.75      0.67      0.67     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4937, Train: 0.7000, Test: 0.5342\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9042, Train: 0.8625, Test: 0.6693\n",
            "Early stopping:  1.831041595167289\n",
            "Epoch: 003, Loss: 0.3665, Train: 0.9500, Test: 0.6971\n",
            "Early stopping:  1.6720145312651957\n",
            "Epoch: 004, Loss: 0.1322, Train: 0.9750, Test: 0.6962\n",
            "Early stopping:  1.5471544521037341\n",
            "Epoch: 005, Loss: 0.0919, Train: 0.9875, Test: 0.6841\n",
            "Early stopping:  1.432372651261298\n",
            "Epoch: 006, Loss: 0.0339, Train: 1.0000, Test: 0.6700\n",
            "Early stopping:  0.35764526435823146\n",
            "Epoch: 007, Loss: 0.0134, Train: 0.9875, Test: 0.6691\n",
            "Early stopping:  0.1416110487090661\n",
            "Epoch: 008, Loss: 0.0229, Train: 1.0000, Test: 0.6755\n",
            "Early stopping:  0.051136609471019945\n",
            "Epoch: 009, Loss: 0.0123, Train: 1.0000, Test: 0.6833\n",
            "Early stopping:  0.033035215061728844\n",
            "Epoch: 010, Loss: 0.0038, Train: 1.0000, Test: 0.6892\n",
            "Early stopping:  0.011509239539200354\n",
            "Epoch: 011, Loss: 0.0022, Train: 1.0000, Test: 0.6930\n",
            "Early stopping:  0.008327884678181564\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 15,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.80      0.76       397\n",
            "             ecology       0.71      0.33      0.45       884\n",
            "            economic       0.70      0.76      0.73      1404\n",
            "          geophysics       0.96      0.69      0.80      1197\n",
            "  gravitional_theory       0.95      0.89      0.92       125\n",
            "               hydro       0.34      0.93      0.50       350\n",
            "                math       0.83      0.70      0.76      1334\n",
            "              metals       0.81      0.65      0.72       196\n",
            "          networking       0.77      0.94      0.85       340\n",
            "        neuroscience       0.49      1.00      0.66       302\n",
            "        oceanography       0.63      0.81      0.71       985\n",
            "             politic       0.70      0.75      0.72       598\n",
            "           sociology       0.67      0.40      0.50       734\n",
            "software_engineering       0.73      0.85      0.79       519\n",
            "          statistics       0.75      0.77      0.76       642\n",
            "    theory_computing       0.65      0.33      0.43       437\n",
            "\n",
            "            accuracy                           0.69     10444\n",
            "           macro avg       0.71      0.72      0.69     10444\n",
            "        weighted avg       0.73      0.69      0.69     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4538, Train: 0.8625, Test: 0.6007\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5544, Train: 0.9375, Test: 0.6741\n",
            "Early stopping:  2.0501594780933035\n",
            "Epoch: 003, Loss: 0.2495, Train: 0.9750, Test: 0.7278\n",
            "Early stopping:  1.7685557049431546\n",
            "Epoch: 004, Loss: 0.0708, Train: 0.9875, Test: 0.7309\n",
            "Early stopping:  1.5936485672284706\n",
            "Epoch: 005, Loss: 0.0297, Train: 1.0000, Test: 0.7252\n",
            "Early stopping:  1.4581877351521242\n",
            "Epoch: 006, Loss: 0.0072, Train: 1.0000, Test: 0.7191\n",
            "Early stopping:  0.22876035806944217\n",
            "Epoch: 007, Loss: 0.0039, Train: 1.0000, Test: 0.7076\n",
            "Early stopping:  0.10260806376755685\n",
            "Epoch: 008, Loss: 0.0034, Train: 1.0000, Test: 0.7015\n",
            "Early stopping:  0.0288551370590886\n",
            "Epoch: 009, Loss: 0.0013, Train: 1.0000, Test: 0.6953\n",
            "Early stopping:  0.011692023116371669\n",
            "Epoch: 010, Loss: 0.0004, Train: 1.0000, Test: 0.6885\n",
            "Early stopping:  0.0026500448551731197\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.68      0.73       397\n",
            "             ecology       0.51      0.94      0.67       884\n",
            "            economic       0.74      0.75      0.75      1404\n",
            "          geophysics       0.93      0.88      0.90      1197\n",
            "  gravitional_theory       0.22      0.96      0.35       125\n",
            "               hydro       0.73      0.80      0.77       350\n",
            "                math       0.80      0.49      0.60      1334\n",
            "              metals       0.56      0.90      0.69       196\n",
            "          networking       0.82      0.67      0.74       340\n",
            "        neuroscience       0.97      0.88      0.93       302\n",
            "        oceanography       0.94      0.23      0.37       985\n",
            "             politic       0.74      0.68      0.71       598\n",
            "           sociology       0.53      0.60      0.56       734\n",
            "software_engineering       0.77      0.89      0.82       519\n",
            "          statistics       0.68      0.64      0.66       642\n",
            "    theory_computing       0.64      0.69      0.67       437\n",
            "\n",
            "            accuracy                           0.69     10444\n",
            "           macro avg       0.71      0.73      0.68     10444\n",
            "        weighted avg       0.75      0.69      0.68     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1262, Train: 0.8000, Test: 0.6638\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6706, Train: 0.9500, Test: 0.7151\n",
            "Early stopping:  1.7363656542175698\n",
            "Epoch: 003, Loss: 0.3928, Train: 0.9750, Test: 0.7316\n",
            "Early stopping:  1.5043554879291892\n",
            "Epoch: 004, Loss: 0.0928, Train: 0.9750, Test: 0.7098\n",
            "Early stopping:  1.390567633533405\n",
            "Epoch: 005, Loss: 0.1785, Train: 0.9875, Test: 0.7053\n",
            "Early stopping:  1.26862973300838\n",
            "Epoch: 006, Loss: 0.1603, Train: 0.9875, Test: 0.7055\n",
            "Early stopping:  0.2362128673666269\n",
            "Epoch: 007, Loss: 0.0808, Train: 1.0000, Test: 0.7068\n",
            "Early stopping:  0.1256132692715935\n",
            "Epoch: 008, Loss: 0.0043, Train: 0.9875, Test: 0.7094\n",
            "Early stopping:  0.0694757577088513\n",
            "Epoch: 009, Loss: 0.0672, Train: 0.9875, Test: 0.7110\n",
            "Early stopping:  0.07135994994266344\n",
            "Epoch: 010, Loss: 0.0686, Train: 1.0000, Test: 0.7112\n",
            "Early stopping:  0.05565228277672649\n",
            "Epoch: 011, Loss: 0.0060, Train: 1.0000, Test: 0.7062\n",
            "Early stopping:  0.037125699943651745\n",
            "Epoch: 012, Loss: 0.0010, Train: 0.9875, Test: 0.7034\n",
            "Early stopping:  0.035182926793126505\n",
            "Epoch: 013, Loss: 0.0185, Train: 0.9875, Test: 0.7010\n",
            "Early stopping:  0.033174633429804264\n",
            "Epoch: 014, Loss: 0.0110, Train: 1.0000, Test: 0.7015\n",
            "Early stopping:  0.027384974941189893\n",
            "Epoch: 015, Loss: 0.0005, Train: 1.0000, Test: 0.7009\n",
            "Early stopping:  0.007507266387784915\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.56      0.78      0.65       397\n",
            "             ecology       0.89      0.58      0.71       884\n",
            "            economic       0.85      0.58      0.69      1404\n",
            "          geophysics       0.98      0.75      0.85      1197\n",
            "  gravitional_theory       0.92      0.82      0.87       125\n",
            "               hydro       0.59      0.71      0.65       350\n",
            "                math       0.91      0.62      0.74      1334\n",
            "              metals       0.23      0.97      0.38       196\n",
            "          networking       0.76      0.93      0.84       340\n",
            "        neuroscience       0.71      0.99      0.83       302\n",
            "        oceanography       0.68      0.88      0.77       985\n",
            "             politic       0.59      0.78      0.67       598\n",
            "           sociology       0.55      0.64      0.59       734\n",
            "software_engineering       0.79      0.41      0.54       519\n",
            "          statistics       0.73      0.72      0.73       642\n",
            "    theory_computing       0.59      0.71      0.64       437\n",
            "\n",
            "            accuracy                           0.70     10444\n",
            "           macro avg       0.71      0.74      0.70     10444\n",
            "        weighted avg       0.77      0.70      0.71     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5188, Train: 0.7250, Test: 0.6650\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9148, Train: 0.9125, Test: 0.7093\n",
            "Early stopping:  1.8412796125796158\n",
            "Epoch: 003, Loss: 0.3051, Train: 0.9500, Test: 0.7270\n",
            "Early stopping:  1.7068633077137618\n",
            "Epoch: 004, Loss: 0.1427, Train: 0.9875, Test: 0.7240\n",
            "Early stopping:  1.5679196899377748\n",
            "Epoch: 005, Loss: 0.0510, Train: 1.0000, Test: 0.7250\n",
            "Early stopping:  1.4550747681209923\n",
            "Epoch: 006, Loss: 0.0237, Train: 1.0000, Test: 0.7328\n",
            "Early stopping:  0.3675338988120259\n",
            "Epoch: 007, Loss: 0.0056, Train: 1.0000, Test: 0.7383\n",
            "Early stopping:  0.12335313835434884\n",
            "Epoch: 008, Loss: 0.0026, Train: 1.0000, Test: 0.7405\n",
            "Early stopping:  0.05786934876674208\n",
            "Epoch: 009, Loss: 0.0020, Train: 1.0000, Test: 0.7419\n",
            "Early stopping:  0.021004919576603606\n",
            "Epoch: 010, Loss: 0.0023, Train: 1.0000, Test: 0.7441\n",
            "Early stopping:  0.009300227454348723\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.74      0.74       397\n",
            "             ecology       0.72      0.93      0.81       884\n",
            "            economic       0.76      0.56      0.65      1404\n",
            "          geophysics       0.87      0.89      0.88      1197\n",
            "  gravitional_theory       0.91      0.92      0.92       125\n",
            "               hydro       0.92      0.66      0.77       350\n",
            "                math       0.91      0.67      0.77      1334\n",
            "              metals       0.56      0.94      0.71       196\n",
            "          networking       0.85      0.81      0.83       340\n",
            "        neuroscience       0.89      0.98      0.94       302\n",
            "        oceanography       0.94      0.67      0.78       985\n",
            "             politic       0.57      0.70      0.63       598\n",
            "           sociology       0.47      0.69      0.56       734\n",
            "software_engineering       0.88      0.82      0.85       519\n",
            "          statistics       0.74      0.72      0.73       642\n",
            "    theory_computing       0.49      0.79      0.60       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.77      0.78      0.76     10444\n",
            "        weighted avg       0.78      0.74      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3177, Train: 0.7000, Test: 0.5879\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8303, Train: 0.9375, Test: 0.6831\n",
            "Early stopping:  1.7588475840034024\n",
            "Epoch: 003, Loss: 0.2715, Train: 0.9750, Test: 0.6441\n",
            "Early stopping:  1.6216680202427969\n",
            "Epoch: 004, Loss: 0.1098, Train: 1.0000, Test: 0.6706\n",
            "Early stopping:  1.4892667693955748\n",
            "Epoch: 005, Loss: 0.0181, Train: 1.0000, Test: 0.6906\n",
            "Early stopping:  1.3826645490041467\n",
            "Epoch: 006, Loss: 0.0109, Train: 1.0000, Test: 0.6907\n",
            "Early stopping:  0.34200238880964784\n",
            "Epoch: 007, Loss: 0.0116, Train: 1.0000, Test: 0.6894\n",
            "Early stopping:  0.11265247793580999\n",
            "Epoch: 008, Loss: 0.0053, Train: 1.0000, Test: 0.6844\n",
            "Early stopping:  0.044215430508861606\n",
            "Epoch: 009, Loss: 0.0036, Train: 1.0000, Test: 0.6824\n",
            "Early stopping:  0.005730199664490515\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.46      0.77      0.57       397\n",
            "             ecology       0.74      0.88      0.80       884\n",
            "            economic       0.68      0.56      0.61      1404\n",
            "          geophysics       0.87      0.86      0.86      1197\n",
            "  gravitional_theory       0.95      0.86      0.90       125\n",
            "               hydro       0.63      0.80      0.70       350\n",
            "                math       0.83      0.45      0.58      1334\n",
            "              metals       0.31      0.94      0.46       196\n",
            "          networking       0.58      0.96      0.72       340\n",
            "        neuroscience       0.89      0.98      0.93       302\n",
            "        oceanography       0.84      0.73      0.78       985\n",
            "             politic       0.82      0.58      0.68       598\n",
            "           sociology       0.52      0.47      0.49       734\n",
            "software_engineering       0.89      0.52      0.66       519\n",
            "          statistics       0.56      0.74      0.64       642\n",
            "    theory_computing       0.54      0.64      0.59       437\n",
            "\n",
            "            accuracy                           0.68     10444\n",
            "           macro avg       0.69      0.73      0.69     10444\n",
            "        weighted avg       0.72      0.68      0.68     10444\n",
            "\n",
            "time: 4.14 s (started: 2024-08-16 14:13:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUVcwzKIKnn1",
        "outputId": "d7ac1c65-7127-47a1-9d8a-c09a32d10350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 413 ms (started: 2024-08-16 14:13:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "k4xAtLBIKnn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvAMCiSEKnn1",
        "outputId": "1888e4ca-7904-4e3c-a4a2-a60ae2a15cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7894, Train: 0.9125, Test: 0.6218\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4840, Train: 0.9250, Test: 0.7352\n",
            "Early stopping:  0.2159518538127248\n",
            "Epoch: 003, Loss: 2.1105, Train: 0.9375, Test: 0.7589\n",
            "Early stopping:  0.3400251079935325\n",
            "Epoch: 004, Loss: 1.6795, Train: 0.9375, Test: 0.7712\n",
            "Early stopping:  0.4794505943041632\n",
            "Epoch: 005, Loss: 1.2492, Train: 0.9625, Test: 0.7810\n",
            "Early stopping:  0.6157347889560302\n",
            "Epoch: 006, Loss: 0.8665, Train: 1.0000, Test: 0.7917\n",
            "Early stopping:  0.6478815879585577\n",
            "Epoch: 007, Loss: 0.5632, Train: 1.0000, Test: 0.7941\n",
            "Early stopping:  0.6192819734760274\n",
            "Epoch: 008, Loss: 0.3490, Train: 1.0000, Test: 0.7920\n",
            "Early stopping:  0.5336295782149869\n",
            "Epoch: 009, Loss: 0.2135, Train: 1.0000, Test: 0.7893\n",
            "Early stopping:  0.41668834318836\n",
            "Epoch: 010, Loss: 0.1314, Train: 1.0000, Test: 0.7894\n",
            "Early stopping:  0.2961238160091948\n",
            "Epoch: 011, Loss: 0.0812, Train: 1.0000, Test: 0.7893\n",
            "Early stopping:  0.1937936413357436\n",
            "Epoch: 012, Loss: 0.0508, Train: 1.0000, Test: 0.7889\n",
            "Early stopping:  0.11981534349992135\n",
            "Epoch: 013, Loss: 0.0324, Train: 1.0000, Test: 0.7866\n",
            "Early stopping:  0.07279029950589538\n",
            "Epoch: 014, Loss: 0.0211, Train: 1.0000, Test: 0.7839\n",
            "Early stopping:  0.04429249160839333\n",
            "Epoch: 015, Loss: 0.0140, Train: 1.0000, Test: 0.7812\n",
            "Early stopping:  0.026939426454657325\n",
            "Epoch: 016, Loss: 0.0095, Train: 1.0000, Test: 0.7792\n",
            "Early stopping:  0.01652544838553624\n",
            "Epoch: 017, Loss: 0.0067, Train: 1.0000, Test: 0.7776\n",
            "Early stopping:  0.010314056029736639\n",
            "Epoch: 018, Loss: 0.0049, Train: 1.0000, Test: 0.7762\n",
            "Early stopping:  0.006491875917852374\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.81      0.74       397\n",
            "             ecology       0.84      0.81      0.82       884\n",
            "            economic       0.85      0.68      0.76      1404\n",
            "          geophysics       0.97      0.78      0.87      1197\n",
            "  gravitional_theory       0.53      0.95      0.68       125\n",
            "               hydro       0.74      0.81      0.77       350\n",
            "                math       0.82      0.75      0.79      1334\n",
            "              metals       0.52      0.95      0.67       196\n",
            "          networking       0.81      0.78      0.80       340\n",
            "        neuroscience       0.88      0.99      0.93       302\n",
            "        oceanography       0.83      0.83      0.83       985\n",
            "             politic       0.75      0.76      0.75       598\n",
            "           sociology       0.71      0.69      0.70       734\n",
            "software_engineering       0.90      0.68      0.78       519\n",
            "          statistics       0.65      0.84      0.73       642\n",
            "    theory_computing       0.54      0.79      0.64       437\n",
            "\n",
            "            accuracy                           0.78     10444\n",
            "           macro avg       0.75      0.81      0.77     10444\n",
            "        weighted avg       0.80      0.78      0.78     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7690, Train: 0.9000, Test: 0.4961\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4743, Train: 0.9375, Test: 0.6398\n",
            "Early stopping:  0.20837654784877074\n",
            "Epoch: 003, Loss: 2.1160, Train: 0.9500, Test: 0.6835\n",
            "Early stopping:  0.32701523016519424\n",
            "Epoch: 004, Loss: 1.7062, Train: 0.9875, Test: 0.7160\n",
            "Early stopping:  0.4590848442478295\n",
            "Epoch: 005, Loss: 1.2937, Train: 0.9875, Test: 0.7405\n",
            "Early stopping:  0.5893350654362914\n",
            "Epoch: 006, Loss: 0.9195, Train: 1.0000, Test: 0.7570\n",
            "Early stopping:  0.621891623607942\n",
            "Epoch: 007, Loss: 0.6103, Train: 1.0000, Test: 0.7619\n",
            "Early stopping:  0.6015060533826849\n",
            "Epoch: 008, Loss: 0.3802, Train: 1.0000, Test: 0.7605\n",
            "Early stopping:  0.5305292222322512\n",
            "Epoch: 009, Loss: 0.2275, Train: 1.0000, Test: 0.7579\n",
            "Early stopping:  0.4281310111144009\n",
            "Epoch: 010, Loss: 0.1349, Train: 1.0000, Test: 0.7563\n",
            "Early stopping:  0.316103101110016\n",
            "Epoch: 011, Loss: 0.0803, Train: 1.0000, Test: 0.7566\n",
            "Early stopping:  0.21365382634876073\n",
            "Epoch: 012, Loss: 0.0480, Train: 1.0000, Test: 0.7578\n",
            "Early stopping:  0.13377872270887126\n",
            "Epoch: 013, Loss: 0.0289, Train: 1.0000, Test: 0.7583\n",
            "Early stopping:  0.07991411207662669\n",
            "Epoch: 014, Loss: 0.0179, Train: 1.0000, Test: 0.7580\n",
            "Early stopping:  0.04711333157496045\n",
            "Epoch: 015, Loss: 0.0115, Train: 1.0000, Test: 0.7568\n",
            "Early stopping:  0.027735931435511056\n",
            "Epoch: 016, Loss: 0.0077, Train: 1.0000, Test: 0.7551\n",
            "Early stopping:  0.016203951828148974\n",
            "Epoch: 017, Loss: 0.0054, Train: 1.0000, Test: 0.7524\n",
            "Early stopping:  0.009434956026797918\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.68      0.72       397\n",
            "             ecology       0.81      0.64      0.71       884\n",
            "            economic       0.82      0.63      0.72      1404\n",
            "          geophysics       0.90      0.83      0.87      1197\n",
            "  gravitional_theory       0.89      0.92      0.91       125\n",
            "               hydro       0.77      0.81      0.79       350\n",
            "                math       0.87      0.73      0.79      1334\n",
            "              metals       0.61      0.91      0.73       196\n",
            "          networking       0.83      0.87      0.85       340\n",
            "        neuroscience       0.66      0.98      0.79       302\n",
            "        oceanography       0.77      0.83      0.80       985\n",
            "             politic       0.67      0.76      0.71       598\n",
            "           sociology       0.56      0.57      0.56       734\n",
            "software_engineering       0.75      0.85      0.79       519\n",
            "          statistics       0.57      0.89      0.70       642\n",
            "    theory_computing       0.71      0.68      0.69       437\n",
            "\n",
            "            accuracy                           0.75     10444\n",
            "           macro avg       0.75      0.79      0.76     10444\n",
            "        weighted avg       0.77      0.75      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7941, Train: 0.8125, Test: 0.4209\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5026, Train: 0.9375, Test: 0.6843\n",
            "Early stopping:  0.20607988177971512\n",
            "Epoch: 003, Loss: 2.1498, Train: 0.9875, Test: 0.7267\n",
            "Early stopping:  0.32263935622857925\n",
            "Epoch: 004, Loss: 1.7345, Train: 0.9875, Test: 0.7413\n",
            "Early stopping:  0.45733125412547243\n",
            "Epoch: 005, Loss: 1.3068, Train: 0.9875, Test: 0.7520\n",
            "Early stopping:  0.5935122060903444\n",
            "Epoch: 006, Loss: 0.9163, Train: 1.0000, Test: 0.7569\n",
            "Early stopping:  0.6352169604642617\n",
            "Epoch: 007, Loss: 0.6001, Train: 1.0000, Test: 0.7555\n",
            "Early stopping:  0.6203623861282539\n",
            "Epoch: 008, Loss: 0.3728, Train: 1.0000, Test: 0.7476\n",
            "Early stopping:  0.5461060594892198\n",
            "Epoch: 009, Loss: 0.2264, Train: 1.0000, Test: 0.7468\n",
            "Early stopping:  0.4344955776464645\n",
            "Epoch: 010, Loss: 0.1374, Train: 1.0000, Test: 0.7466\n",
            "Early stopping:  0.31371505482069517\n",
            "Epoch: 011, Loss: 0.0838, Train: 1.0000, Test: 0.7486\n",
            "Early stopping:  0.207771407790553\n",
            "Epoch: 012, Loss: 0.0521, Train: 1.0000, Test: 0.7504\n",
            "Early stopping:  0.12903959256900713\n",
            "Epoch: 013, Loss: 0.0333, Train: 1.0000, Test: 0.7493\n",
            "Early stopping:  0.07773086201366815\n",
            "Epoch: 014, Loss: 0.0218, Train: 1.0000, Test: 0.7471\n",
            "Early stopping:  0.04644952632253805\n",
            "Epoch: 015, Loss: 0.0146, Train: 1.0000, Test: 0.7449\n",
            "Early stopping:  0.027769681998300016\n",
            "Epoch: 016, Loss: 0.0100, Train: 1.0000, Test: 0.7412\n",
            "Early stopping:  0.016863966753826037\n",
            "Epoch: 017, Loss: 0.0070, Train: 1.0000, Test: 0.7388\n",
            "Early stopping:  0.01050476289750425\n",
            "Epoch: 018, Loss: 0.0052, Train: 1.0000, Test: 0.7358\n",
            "Early stopping:  0.006667262294299577\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.70      0.73       397\n",
            "             ecology       0.81      0.79      0.80       884\n",
            "            economic       0.77      0.72      0.74      1404\n",
            "          geophysics       0.94      0.60      0.73      1197\n",
            "  gravitional_theory       0.82      0.94      0.88       125\n",
            "               hydro       0.67      0.69      0.68       350\n",
            "                math       0.85      0.78      0.81      1334\n",
            "              metals       0.38      0.84      0.52       196\n",
            "          networking       0.84      0.88      0.86       340\n",
            "        neuroscience       0.70      1.00      0.82       302\n",
            "        oceanography       0.75      0.83      0.79       985\n",
            "             politic       0.67      0.76      0.71       598\n",
            "           sociology       0.67      0.56      0.61       734\n",
            "software_engineering       0.85      0.46      0.60       519\n",
            "          statistics       0.74      0.83      0.78       642\n",
            "    theory_computing       0.44      0.82      0.58       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.73      0.76      0.73     10444\n",
            "        weighted avg       0.77      0.74      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7543, Train: 0.8875, Test: 0.5593\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4323, Train: 0.9625, Test: 0.6836\n",
            "Early stopping:  0.2277318979715197\n",
            "Epoch: 003, Loss: 2.0406, Train: 0.9625, Test: 0.7124\n",
            "Early stopping:  0.35741629739474584\n",
            "Epoch: 004, Loss: 1.6046, Train: 0.9750, Test: 0.7239\n",
            "Early stopping:  0.4969342128235116\n",
            "Epoch: 005, Loss: 1.1824, Train: 0.9750, Test: 0.7346\n",
            "Early stopping:  0.6289491255342015\n",
            "Epoch: 006, Loss: 0.8172, Train: 0.9750, Test: 0.7431\n",
            "Early stopping:  0.6466983822968727\n",
            "Epoch: 007, Loss: 0.5334, Train: 0.9750, Test: 0.7463\n",
            "Early stopping:  0.6031714223147658\n",
            "Epoch: 008, Loss: 0.3351, Train: 0.9875, Test: 0.7478\n",
            "Early stopping:  0.5090337065767417\n",
            "Epoch: 009, Loss: 0.2087, Train: 0.9875, Test: 0.7462\n",
            "Early stopping:  0.3914274262769518\n",
            "Epoch: 010, Loss: 0.1309, Train: 0.9875, Test: 0.7434\n",
            "Early stopping:  0.27609850798878394\n",
            "Epoch: 011, Loss: 0.0826, Train: 1.0000, Test: 0.7397\n",
            "Early stopping:  0.18106732410542484\n",
            "Epoch: 012, Loss: 0.0526, Train: 1.0000, Test: 0.7360\n",
            "Early stopping:  0.11335346488854892\n",
            "Epoch: 013, Loss: 0.0342, Train: 1.0000, Test: 0.7349\n",
            "Early stopping:  0.07006517347475469\n",
            "Epoch: 014, Loss: 0.0228, Train: 1.0000, Test: 0.7327\n",
            "Early stopping:  0.04341219753348837\n",
            "Epoch: 015, Loss: 0.0153, Train: 1.0000, Test: 0.7311\n",
            "Early stopping:  0.026930188963515175\n",
            "Epoch: 016, Loss: 0.0104, Train: 1.0000, Test: 0.7308\n",
            "Early stopping:  0.016857988147509797\n",
            "Epoch: 017, Loss: 0.0073, Train: 1.0000, Test: 0.7294\n",
            "Early stopping:  0.01079257685840953\n",
            "Epoch: 018, Loss: 0.0053, Train: 1.0000, Test: 0.7294\n",
            "Early stopping:  0.007018214696577849\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.78      0.76       397\n",
            "             ecology       0.78      0.68      0.72       884\n",
            "            economic       0.72      0.51      0.60      1404\n",
            "          geophysics       0.92      0.84      0.88      1197\n",
            "  gravitional_theory       0.91      0.89      0.90       125\n",
            "               hydro       0.58      0.75      0.66       350\n",
            "                math       0.91      0.67      0.77      1334\n",
            "              metals       0.49      0.93      0.64       196\n",
            "          networking       0.80      0.89      0.84       340\n",
            "        neuroscience       0.87      0.99      0.93       302\n",
            "        oceanography       0.79      0.74      0.76       985\n",
            "             politic       0.54      0.87      0.67       598\n",
            "           sociology       0.50      0.65      0.56       734\n",
            "software_engineering       0.74      0.90      0.81       519\n",
            "          statistics       0.72      0.70      0.71       642\n",
            "    theory_computing       0.73      0.65      0.69       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.73      0.78      0.74     10444\n",
            "        weighted avg       0.75      0.73      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7927, Train: 0.8875, Test: 0.5616\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4835, Train: 0.9500, Test: 0.6939\n",
            "Early stopping:  0.2186538039770612\n",
            "Epoch: 003, Loss: 2.1246, Train: 0.9500, Test: 0.6876\n",
            "Early stopping:  0.3343921065039576\n",
            "Epoch: 004, Loss: 1.7140, Train: 0.9625, Test: 0.6857\n",
            "Early stopping:  0.46503823482275225\n",
            "Epoch: 005, Loss: 1.3015, Train: 0.9750, Test: 0.7024\n",
            "Early stopping:  0.5943039923725182\n",
            "Epoch: 006, Loss: 0.9294, Train: 0.9875, Test: 0.7330\n",
            "Early stopping:  0.6217610706619847\n",
            "Epoch: 007, Loss: 0.6264, Train: 0.9875, Test: 0.7589\n",
            "Early stopping:  0.5988880992103606\n",
            "Epoch: 008, Loss: 0.4049, Train: 0.9875, Test: 0.7674\n",
            "Early stopping:  0.524227385382253\n",
            "Epoch: 009, Loss: 0.2591, Train: 1.0000, Test: 0.7658\n",
            "Early stopping:  0.41869118488594564\n",
            "Epoch: 010, Loss: 0.1661, Train: 1.0000, Test: 0.7597\n",
            "Early stopping:  0.3067464072717482\n",
            "Epoch: 011, Loss: 0.1059, Train: 1.0000, Test: 0.7538\n",
            "Early stopping:  0.2086074187091691\n",
            "Epoch: 012, Loss: 0.0672, Train: 1.0000, Test: 0.7466\n",
            "Early stopping:  0.13521398087383066\n",
            "Epoch: 013, Loss: 0.0432, Train: 1.0000, Test: 0.7429\n",
            "Early stopping:  0.08662271549961344\n",
            "Epoch: 014, Loss: 0.0281, Train: 1.0000, Test: 0.7404\n",
            "Early stopping:  0.055396100306310696\n",
            "Epoch: 015, Loss: 0.0183, Train: 1.0000, Test: 0.7432\n",
            "Early stopping:  0.03504012828747643\n",
            "Epoch: 016, Loss: 0.0121, Train: 1.0000, Test: 0.7461\n",
            "Early stopping:  0.022048561882996944\n",
            "Epoch: 017, Loss: 0.0082, Train: 1.0000, Test: 0.7465\n",
            "Early stopping:  0.014030875454638627\n",
            "Epoch: 018, Loss: 0.0058, Train: 1.0000, Test: 0.7471\n",
            "Early stopping:  0.008958786149636884\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.78      0.72       397\n",
            "             ecology       0.75      0.80      0.77       884\n",
            "            economic       0.87      0.52      0.65      1404\n",
            "          geophysics       0.95      0.88      0.91      1197\n",
            "  gravitional_theory       0.90      0.93      0.91       125\n",
            "               hydro       0.48      0.91      0.63       350\n",
            "                math       0.90      0.70      0.79      1334\n",
            "              metals       0.78      0.80      0.79       196\n",
            "          networking       0.83      0.84      0.84       340\n",
            "        neuroscience       0.94      0.93      0.94       302\n",
            "        oceanography       0.86      0.79      0.83       985\n",
            "             politic       0.55      0.68      0.61       598\n",
            "           sociology       0.56      0.69      0.62       734\n",
            "software_engineering       0.66      0.77      0.71       519\n",
            "          statistics       0.63      0.81      0.71       642\n",
            "    theory_computing       0.67      0.64      0.65       437\n",
            "\n",
            "            accuracy                           0.75     10444\n",
            "           macro avg       0.75      0.78      0.76     10444\n",
            "        weighted avg       0.78      0.75      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7759, Train: 0.9125, Test: 0.5480\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4560, Train: 0.9125, Test: 0.6603\n",
            "Early stopping:  0.22617448762534512\n",
            "Epoch: 003, Loss: 2.0812, Train: 0.9250, Test: 0.6843\n",
            "Early stopping:  0.34771806297323304\n",
            "Epoch: 004, Loss: 1.6574, Train: 0.9500, Test: 0.6960\n",
            "Early stopping:  0.48251002501720003\n",
            "Epoch: 005, Loss: 1.2385, Train: 0.9625, Test: 0.7092\n",
            "Early stopping:  0.6133869975820381\n",
            "Epoch: 006, Loss: 0.8694, Train: 0.9750, Test: 0.7276\n",
            "Early stopping:  0.6351502199499133\n",
            "Epoch: 007, Loss: 0.5784, Train: 1.0000, Test: 0.7478\n",
            "Early stopping:  0.6013896255169943\n",
            "Epoch: 008, Loss: 0.3710, Train: 1.0000, Test: 0.7628\n",
            "Early stopping:  0.5155804808861967\n",
            "Epoch: 009, Loss: 0.2348, Train: 1.0000, Test: 0.7624\n",
            "Early stopping:  0.4029793074729588\n",
            "Epoch: 010, Loss: 0.1504, Train: 1.0000, Test: 0.7545\n",
            "Early stopping:  0.2890933010907227\n",
            "Epoch: 011, Loss: 0.0981, Train: 1.0000, Test: 0.7517\n",
            "Early stopping:  0.19301479920153744\n",
            "Epoch: 012, Loss: 0.0644, Train: 1.0000, Test: 0.7493\n",
            "Early stopping:  0.12280280358943074\n",
            "Epoch: 013, Loss: 0.0426, Train: 1.0000, Test: 0.7470\n",
            "Early stopping:  0.07688509938584727\n",
            "Epoch: 014, Loss: 0.0288, Train: 1.0000, Test: 0.7451\n",
            "Early stopping:  0.0487443245971314\n",
            "Epoch: 015, Loss: 0.0202, Train: 1.0000, Test: 0.7408\n",
            "Early stopping:  0.03128511156176297\n",
            "Epoch: 016, Loss: 0.0146, Train: 1.0000, Test: 0.7380\n",
            "Early stopping:  0.019954971615546723\n",
            "Epoch: 017, Loss: 0.0108, Train: 1.0000, Test: 0.7356\n",
            "Early stopping:  0.012683983560851892\n",
            "Epoch: 018, Loss: 0.0080, Train: 1.0000, Test: 0.7347\n",
            "Early stopping:  0.008274970666911704\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 13, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.81      0.77       397\n",
            "             ecology       0.82      0.56      0.66       884\n",
            "            economic       0.70      0.77      0.73      1404\n",
            "          geophysics       0.86      0.91      0.89      1197\n",
            "  gravitional_theory       0.83      0.91      0.87       125\n",
            "               hydro       0.51      0.85      0.64       350\n",
            "                math       0.89      0.63      0.74      1334\n",
            "              metals       0.80      0.69      0.74       196\n",
            "          networking       0.73      0.92      0.81       340\n",
            "        neuroscience       0.80      0.99      0.89       302\n",
            "        oceanography       0.68      0.85      0.76       985\n",
            "             politic       0.71      0.70      0.71       598\n",
            "           sociology       0.60      0.38      0.46       734\n",
            "software_engineering       0.69      0.87      0.77       519\n",
            "          statistics       0.67      0.84      0.75       642\n",
            "    theory_computing       0.71      0.33      0.45       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.73      0.75      0.73     10444\n",
            "        weighted avg       0.75      0.73      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7793, Train: 0.7375, Test: 0.5217\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4922, Train: 0.9500, Test: 0.6714\n",
            "Early stopping:  0.20296034064033278\n",
            "Epoch: 003, Loss: 2.1390, Train: 0.9500, Test: 0.7008\n",
            "Early stopping:  0.32068562735716377\n",
            "Epoch: 004, Loss: 1.7362, Train: 0.9500, Test: 0.7193\n",
            "Early stopping:  0.45080429688080365\n",
            "Epoch: 005, Loss: 1.3331, Train: 0.9500, Test: 0.7359\n",
            "Early stopping:  0.5781616647138891\n",
            "Epoch: 006, Loss: 0.9664, Train: 0.9875, Test: 0.7515\n",
            "Early stopping:  0.6101012738553213\n",
            "Epoch: 007, Loss: 0.6582, Train: 1.0000, Test: 0.7570\n",
            "Early stopping:  0.5908502064010568\n",
            "Epoch: 008, Loss: 0.4225, Train: 1.0000, Test: 0.7495\n",
            "Early stopping:  0.5248323038814509\n",
            "Epoch: 009, Loss: 0.2604, Train: 1.0000, Test: 0.7363\n",
            "Early stopping:  0.43007552950934225\n",
            "Epoch: 010, Loss: 0.1573, Train: 1.0000, Test: 0.7295\n",
            "Early stopping:  0.32525921351836246\n",
            "Epoch: 011, Loss: 0.0939, Train: 1.0000, Test: 0.7273\n",
            "Early stopping:  0.2269555658157414\n",
            "Epoch: 012, Loss: 0.0558, Train: 1.0000, Test: 0.7266\n",
            "Early stopping:  0.14749961638840514\n",
            "Epoch: 013, Loss: 0.0334, Train: 1.0000, Test: 0.7297\n",
            "Early stopping:  0.09138366697952353\n",
            "Epoch: 014, Loss: 0.0207, Train: 1.0000, Test: 0.7329\n",
            "Early stopping:  0.05505420333123785\n",
            "Epoch: 015, Loss: 0.0135, Train: 1.0000, Test: 0.7344\n",
            "Early stopping:  0.03243479020282548\n",
            "Epoch: 016, Loss: 0.0092, Train: 1.0000, Test: 0.7355\n",
            "Early stopping:  0.01874119418048259\n",
            "Epoch: 017, Loss: 0.0066, Train: 1.0000, Test: 0.7362\n",
            "Early stopping:  0.010772175656491046\n",
            "Epoch: 018, Loss: 0.0049, Train: 1.0000, Test: 0.7341\n",
            "Early stopping:  0.006331700400816218\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.76      0.77       397\n",
            "             ecology       0.64      0.90      0.75       884\n",
            "            economic       0.80      0.73      0.76      1404\n",
            "          geophysics       0.94      0.86      0.90      1197\n",
            "  gravitional_theory       0.25      0.96      0.40       125\n",
            "               hydro       0.74      0.86      0.80       350\n",
            "                math       0.83      0.50      0.63      1334\n",
            "              metals       0.48      0.94      0.63       196\n",
            "          networking       0.77      0.89      0.83       340\n",
            "        neuroscience       0.93      0.93      0.93       302\n",
            "        oceanography       0.90      0.58      0.71       985\n",
            "             politic       0.62      0.82      0.71       598\n",
            "           sociology       0.63      0.52      0.57       734\n",
            "software_engineering       0.88      0.79      0.83       519\n",
            "          statistics       0.68      0.74      0.71       642\n",
            "    theory_computing       0.70      0.75      0.72       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.72      0.78      0.73     10444\n",
            "        weighted avg       0.77      0.73      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7809, Train: 0.6750, Test: 0.4076\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4667, Train: 0.9000, Test: 0.6783\n",
            "Early stopping:  0.22218183237221112\n",
            "Epoch: 003, Loss: 2.1033, Train: 0.8875, Test: 0.7147\n",
            "Early stopping:  0.33911758762630373\n",
            "Epoch: 004, Loss: 1.6979, Train: 0.9000, Test: 0.7275\n",
            "Early stopping:  0.46713383523203783\n",
            "Epoch: 005, Loss: 1.2940, Train: 0.9125, Test: 0.7434\n",
            "Early stopping:  0.5925748228069259\n",
            "Epoch: 006, Loss: 0.9321, Train: 0.9375, Test: 0.7608\n",
            "Early stopping:  0.6134040555230602\n",
            "Epoch: 007, Loss: 0.6358, Train: 0.9750, Test: 0.7679\n",
            "Early stopping:  0.5862776354017267\n",
            "Epoch: 008, Loss: 0.4152, Train: 0.9875, Test: 0.7669\n",
            "Early stopping:  0.5129919212810636\n",
            "Epoch: 009, Loss: 0.2658, Train: 1.0000, Test: 0.7566\n",
            "Early stopping:  0.4123474900116258\n",
            "Epoch: 010, Loss: 0.1713, Train: 1.0000, Test: 0.7488\n",
            "Early stopping:  0.3057641453175812\n",
            "Epoch: 011, Loss: 0.1101, Train: 1.0000, Test: 0.7486\n",
            "Early stopping:  0.2108825307148866\n",
            "Epoch: 012, Loss: 0.0696, Train: 1.0000, Test: 0.7463\n",
            "Early stopping:  0.1381499848964862\n",
            "Epoch: 013, Loss: 0.0444, Train: 1.0000, Test: 0.7463\n",
            "Early stopping:  0.08873792841768065\n",
            "Epoch: 014, Loss: 0.0294, Train: 1.0000, Test: 0.7462\n",
            "Early stopping:  0.05712396695751255\n",
            "Epoch: 015, Loss: 0.0201, Train: 1.0000, Test: 0.7435\n",
            "Early stopping:  0.036180683244290815\n",
            "Epoch: 016, Loss: 0.0140, Train: 1.0000, Test: 0.7420\n",
            "Early stopping:  0.02223346177515414\n",
            "Epoch: 017, Loss: 0.0098, Train: 1.0000, Test: 0.7401\n",
            "Early stopping:  0.01377879211086611\n",
            "Epoch: 018, Loss: 0.0070, Train: 1.0000, Test: 0.7389\n",
            "Early stopping:  0.00893504912890982\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.66      0.75      0.70       397\n",
            "             ecology       0.85      0.61      0.71       884\n",
            "            economic       0.83      0.65      0.73      1404\n",
            "          geophysics       0.89      0.87      0.88      1197\n",
            "  gravitional_theory       0.96      0.82      0.88       125\n",
            "               hydro       0.63      0.70      0.66       350\n",
            "                math       0.93      0.63      0.75      1334\n",
            "              metals       0.34      0.97      0.51       196\n",
            "          networking       0.67      0.94      0.78       340\n",
            "        neuroscience       0.83      1.00      0.90       302\n",
            "        oceanography       0.72      0.87      0.79       985\n",
            "             politic       0.75      0.66      0.70       598\n",
            "           sociology       0.64      0.74      0.68       734\n",
            "software_engineering       0.83      0.59      0.69       519\n",
            "          statistics       0.67      0.82      0.74       642\n",
            "    theory_computing       0.54      0.73      0.62       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.73      0.77      0.73     10444\n",
            "        weighted avg       0.77      0.74      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7742, Train: 0.7250, Test: 0.4802\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4666, Train: 0.8250, Test: 0.6669\n",
            "Early stopping:  0.21752848312182033\n",
            "Epoch: 003, Loss: 2.0987, Train: 0.8625, Test: 0.6924\n",
            "Early stopping:  0.3382115052226681\n",
            "Epoch: 004, Loss: 1.6887, Train: 0.8500, Test: 0.7080\n",
            "Early stopping:  0.4688653393785667\n",
            "Epoch: 005, Loss: 1.2889, Train: 0.9000, Test: 0.7378\n",
            "Early stopping:  0.5935983526506468\n",
            "Epoch: 006, Loss: 0.9352, Train: 0.9375, Test: 0.7680\n",
            "Early stopping:  0.61251201066417\n",
            "Epoch: 007, Loss: 0.6473, Train: 0.9750, Test: 0.7688\n",
            "Early stopping:  0.5795013746880964\n",
            "Epoch: 008, Loss: 0.4349, Train: 1.0000, Test: 0.7515\n",
            "Early stopping:  0.5014191536922029\n",
            "Epoch: 009, Loss: 0.2920, Train: 1.0000, Test: 0.7407\n",
            "Early stopping:  0.39989680078572\n",
            "Epoch: 010, Loss: 0.1967, Train: 1.0000, Test: 0.7432\n",
            "Early stopping:  0.2960355418042231\n",
            "Epoch: 011, Loss: 0.1303, Train: 1.0000, Test: 0.7491\n",
            "Early stopping:  0.20629328559071017\n",
            "Epoch: 012, Loss: 0.0856, Train: 1.0000, Test: 0.7511\n",
            "Early stopping:  0.13938135949479913\n",
            "Epoch: 013, Loss: 0.0579, Train: 1.0000, Test: 0.7537\n",
            "Early stopping:  0.09398060060617877\n",
            "Epoch: 014, Loss: 0.0399, Train: 1.0000, Test: 0.7553\n",
            "Early stopping:  0.06293169510697195\n",
            "Epoch: 015, Loss: 0.0271, Train: 1.0000, Test: 0.7565\n",
            "Early stopping:  0.04109819434600519\n",
            "Epoch: 016, Loss: 0.0184, Train: 1.0000, Test: 0.7557\n",
            "Early stopping:  0.02677132893194763\n",
            "Epoch: 017, Loss: 0.0130, Train: 1.0000, Test: 0.7536\n",
            "Early stopping:  0.018025701360091403\n",
            "Epoch: 018, Loss: 0.0097, Train: 1.0000, Test: 0.7516\n",
            "Early stopping:  0.012160475565681055\n",
            "Epoch: 019, Loss: 0.0074, Train: 1.0000, Test: 0.7511\n",
            "Early stopping:  0.007865038508039142\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.76      0.73       397\n",
            "             ecology       0.79      0.85      0.82       884\n",
            "            economic       0.79      0.57      0.66      1404\n",
            "          geophysics       0.90      0.87      0.89      1197\n",
            "  gravitional_theory       0.92      0.88      0.90       125\n",
            "               hydro       0.71      0.80      0.76       350\n",
            "                math       0.89      0.70      0.78      1334\n",
            "              metals       0.55      0.95      0.70       196\n",
            "          networking       0.79      0.84      0.82       340\n",
            "        neuroscience       0.90      0.98      0.94       302\n",
            "        oceanography       0.91      0.71      0.80       985\n",
            "             politic       0.53      0.70      0.60       598\n",
            "           sociology       0.49      0.66      0.56       734\n",
            "software_engineering       0.90      0.78      0.83       519\n",
            "          statistics       0.71      0.79      0.75       642\n",
            "    theory_computing       0.57      0.76      0.65       437\n",
            "\n",
            "            accuracy                           0.75     10444\n",
            "           macro avg       0.75      0.79      0.76     10444\n",
            "        weighted avg       0.78      0.75      0.76     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7817, Train: 0.8750, Test: 0.5347\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4903, Train: 0.9125, Test: 0.6938\n",
            "Early stopping:  0.20606875501170824\n",
            "Epoch: 003, Loss: 2.1310, Train: 0.9250, Test: 0.7222\n",
            "Early stopping:  0.3259783586096084\n",
            "Epoch: 004, Loss: 1.7200, Train: 0.9375, Test: 0.7312\n",
            "Early stopping:  0.458904951018254\n",
            "Epoch: 005, Loss: 1.3050, Train: 0.9500, Test: 0.7345\n",
            "Early stopping:  0.5902262761860918\n",
            "Epoch: 006, Loss: 0.9252, Train: 0.9625, Test: 0.7380\n",
            "Early stopping:  0.6256930676530781\n",
            "Epoch: 007, Loss: 0.6160, Train: 0.9875, Test: 0.7355\n",
            "Early stopping:  0.6056773259057157\n",
            "Epoch: 008, Loss: 0.3923, Train: 0.9875, Test: 0.7300\n",
            "Early stopping:  0.5322870943349711\n",
            "Epoch: 009, Loss: 0.2460, Train: 0.9875, Test: 0.7241\n",
            "Early stopping:  0.42557639091082305\n",
            "Epoch: 010, Loss: 0.1550, Train: 1.0000, Test: 0.7210\n",
            "Early stopping:  0.30980769660565943\n",
            "Epoch: 011, Loss: 0.0976, Train: 1.0000, Test: 0.7202\n",
            "Early stopping:  0.20813338459671055\n",
            "Epoch: 012, Loss: 0.0609, Train: 1.0000, Test: 0.7196\n",
            "Early stopping:  0.13277533828802093\n",
            "Epoch: 013, Loss: 0.0380, Train: 1.0000, Test: 0.7181\n",
            "Early stopping:  0.08342881537079733\n",
            "Epoch: 014, Loss: 0.0242, Train: 1.0000, Test: 0.7178\n",
            "Early stopping:  0.052591614371534756\n",
            "Epoch: 015, Loss: 0.0160, Train: 1.0000, Test: 0.7171\n",
            "Early stopping:  0.03284953738190271\n",
            "Epoch: 016, Loss: 0.0109, Train: 1.0000, Test: 0.7173\n",
            "Early stopping:  0.02006139721628166\n",
            "Epoch: 017, Loss: 0.0077, Train: 1.0000, Test: 0.7180\n",
            "Early stopping:  0.012136673621920991\n",
            "Epoch: 018, Loss: 0.0056, Train: 1.0000, Test: 0.7176\n",
            "Early stopping:  0.007474881728862734\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.62      0.69      0.66       397\n",
            "             ecology       0.80      0.85      0.82       884\n",
            "            economic       0.64      0.31      0.42      1404\n",
            "          geophysics       0.84      0.93      0.88      1197\n",
            "  gravitional_theory       0.91      0.93      0.92       125\n",
            "               hydro       0.73      0.82      0.77       350\n",
            "                math       0.93      0.66      0.77      1334\n",
            "              metals       0.55      0.85      0.67       196\n",
            "          networking       0.67      0.94      0.78       340\n",
            "        neuroscience       0.89      0.97      0.93       302\n",
            "        oceanography       0.86      0.80      0.83       985\n",
            "             politic       0.64      0.78      0.70       598\n",
            "           sociology       0.60      0.51      0.55       734\n",
            "software_engineering       0.83      0.76      0.79       519\n",
            "          statistics       0.42      0.81      0.56       642\n",
            "    theory_computing       0.62      0.72      0.67       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.72      0.77      0.73     10444\n",
            "        weighted avg       0.74      0.72      0.71     10444\n",
            "\n",
            "time: 8.57 s (started: 2024-08-16 14:13:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpk5Z6KMKnn1",
        "outputId": "a8d7b1bf-1399-497d-e650-7158f2c382c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 478 ms (started: 2024-08-16 14:13:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 10 ❎"
      ],
      "metadata": {
        "id": "SDD7euBLKnn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "vi8dgW5hKnn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66eVzFRNKnn1",
        "outputId": "6af5d722-6bca-4ec5-cbce-afb9db00e915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2167, Train: 0.7625, Test: 0.7045\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.7791, Train: 0.8562, Test: 0.7397\n",
            "Early stopping:  1.7236664294396007\n",
            "Epoch: 003, Loss: 0.4374, Train: 0.9437, Test: 0.7667\n",
            "Early stopping:  1.515674895474295\n",
            "Epoch: 004, Loss: 0.1864, Train: 0.9688, Test: 0.7791\n",
            "Early stopping:  1.3958649880313438\n",
            "Epoch: 005, Loss: 0.1320, Train: 0.9750, Test: 0.7788\n",
            "Early stopping:  1.292507159065527\n",
            "Epoch: 006, Loss: 0.1075, Train: 0.9875, Test: 0.7808\n",
            "Early stopping:  0.28397333202377556\n",
            "Epoch: 007, Loss: 0.0642, Train: 0.9875, Test: 0.7808\n",
            "Early stopping:  0.14756869022929436\n",
            "Epoch: 008, Loss: 0.0360, Train: 0.9938, Test: 0.7823\n",
            "Early stopping:  0.05870465799957043\n",
            "Epoch: 009, Loss: 0.0175, Train: 1.0000, Test: 0.7806\n",
            "Early stopping:  0.04793504704416349\n",
            "Epoch: 010, Loss: 0.0065, Train: 1.0000, Test: 0.7755\n",
            "Early stopping:  0.04055840333760713\n",
            "Epoch: 011, Loss: 0.0039, Train: 1.0000, Test: 0.7705\n",
            "Early stopping:  0.024989198580632453\n",
            "Epoch: 012, Loss: 0.0036, Train: 1.0000, Test: 0.7665\n",
            "Early stopping:  0.013766954764379708\n",
            "Epoch: 013, Loss: 0.0027, Train: 1.0000, Test: 0.7624\n",
            "Early stopping:  0.0061170517826680225\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.83      0.77       392\n",
            "             ecology       0.86      0.78      0.82       879\n",
            "            economic       0.86      0.58      0.69      1399\n",
            "          geophysics       0.91      0.93      0.92      1192\n",
            "  gravitional_theory       0.52      0.96      0.67       120\n",
            "               hydro       0.74      0.80      0.76       345\n",
            "                math       0.87      0.62      0.72      1329\n",
            "              metals       0.70      0.93      0.80       191\n",
            "          networking       0.66      0.90      0.76       335\n",
            "        neuroscience       0.91      0.97      0.94       297\n",
            "        oceanography       0.85      0.84      0.84       980\n",
            "             politic       0.61      0.82      0.70       593\n",
            "           sociology       0.55      0.66      0.60       729\n",
            "software_engineering       0.79      0.88      0.84       514\n",
            "          statistics       0.58      0.88      0.69       637\n",
            "    theory_computing       0.85      0.45      0.59       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.75      0.80      0.76     10364\n",
            "        weighted avg       0.79      0.76      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.8370, Train: 0.7438, Test: 0.6420\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0046, Train: 0.8562, Test: 0.7471\n",
            "Early stopping:  2.002817314007017\n",
            "Epoch: 003, Loss: 0.4850, Train: 0.9250, Test: 0.7815\n",
            "Early stopping:  1.804075190711308\n",
            "Epoch: 004, Loss: 0.2355, Train: 0.9500, Test: 0.7655\n",
            "Early stopping:  1.6621268290482993\n",
            "Epoch: 005, Loss: 0.1707, Train: 0.9812, Test: 0.7643\n",
            "Early stopping:  1.539348857839101\n",
            "Epoch: 006, Loss: 0.0774, Train: 0.9875, Test: 0.7659\n",
            "Early stopping:  0.37291066624731284\n",
            "Epoch: 007, Loss: 0.0405, Train: 0.9938, Test: 0.7643\n",
            "Early stopping:  0.17591940667376385\n",
            "Epoch: 008, Loss: 0.0303, Train: 0.9875, Test: 0.7579\n",
            "Early stopping:  0.08899424614179703\n",
            "Epoch: 009, Loss: 0.0356, Train: 1.0000, Test: 0.7513\n",
            "Early stopping:  0.05877137788434696\n",
            "Epoch: 010, Loss: 0.0142, Train: 1.0000, Test: 0.7427\n",
            "Early stopping:  0.023354747010705362\n",
            "Epoch: 011, Loss: 0.0069, Train: 1.0000, Test: 0.7342\n",
            "Early stopping:  0.014375621076159383\n",
            "Epoch: 012, Loss: 0.0059, Train: 1.0000, Test: 0.7284\n",
            "Early stopping:  0.013654779675069945\n",
            "Epoch: 013, Loss: 0.0038, Train: 1.0000, Test: 0.7240\n",
            "Early stopping:  0.013093508652740688\n",
            "Epoch: 014, Loss: 0.0030, Train: 1.0000, Test: 0.7227\n",
            "Early stopping:  0.004436835542616851\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.76      0.73       392\n",
            "             ecology       0.65      0.77      0.70       879\n",
            "            economic       0.77      0.59      0.66      1399\n",
            "          geophysics       0.94      0.86      0.90      1192\n",
            "  gravitional_theory       0.43      0.95      0.59       120\n",
            "               hydro       0.49      0.82      0.61       345\n",
            "                math       0.74      0.80      0.77      1329\n",
            "              metals       0.84      0.77      0.81       191\n",
            "          networking       0.92      0.75      0.83       335\n",
            "        neuroscience       0.94      0.96      0.95       297\n",
            "        oceanography       0.95      0.50      0.65       980\n",
            "             politic       0.65      0.79      0.71       593\n",
            "           sociology       0.51      0.73      0.60       729\n",
            "software_engineering       0.91      0.62      0.74       514\n",
            "          statistics       0.68      0.74      0.71       637\n",
            "    theory_computing       0.64      0.60      0.62       432\n",
            "\n",
            "            accuracy                           0.72     10364\n",
            "           macro avg       0.73      0.75      0.72     10364\n",
            "        weighted avg       0.76      0.72      0.73     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6184, Train: 0.7250, Test: 0.5561\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8888, Train: 0.8750, Test: 0.7331\n",
            "Early stopping:  1.9301559785868418\n",
            "Epoch: 003, Loss: 0.3998, Train: 0.9187, Test: 0.7630\n",
            "Early stopping:  1.734458276360584\n",
            "Epoch: 004, Loss: 0.1983, Train: 0.9437, Test: 0.7327\n",
            "Early stopping:  1.5881030830623213\n",
            "Epoch: 005, Loss: 0.1812, Train: 0.9812, Test: 0.7548\n",
            "Early stopping:  1.459931490480533\n",
            "Epoch: 006, Loss: 0.0532, Train: 0.9875, Test: 0.7464\n",
            "Early stopping:  0.32870635013669947\n",
            "Epoch: 007, Loss: 0.0410, Train: 0.9875, Test: 0.7389\n",
            "Early stopping:  0.14481239601572776\n",
            "Epoch: 008, Loss: 0.0377, Train: 1.0000, Test: 0.7393\n",
            "Early stopping:  0.080282825288355\n",
            "Epoch: 009, Loss: 0.0164, Train: 1.0000, Test: 0.7423\n",
            "Early stopping:  0.06581879676765426\n",
            "Epoch: 010, Loss: 0.0087, Train: 1.0000, Test: 0.7414\n",
            "Early stopping:  0.018354254255091303\n",
            "Epoch: 011, Loss: 0.0063, Train: 1.0000, Test: 0.7393\n",
            "Early stopping:  0.016297595248390067\n",
            "Epoch: 012, Loss: 0.0060, Train: 1.0000, Test: 0.7384\n",
            "Early stopping:  0.013358037317549379\n",
            "Epoch: 013, Loss: 0.0053, Train: 1.0000, Test: 0.7417\n",
            "Early stopping:  0.004560908637513026\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.63      0.79      0.70       392\n",
            "             ecology       0.88      0.55      0.68       879\n",
            "            economic       0.82      0.72      0.77      1399\n",
            "          geophysics       0.85      0.90      0.87      1192\n",
            "  gravitional_theory       0.95      0.87      0.90       120\n",
            "               hydro       0.65      0.80      0.72       345\n",
            "                math       0.92      0.63      0.75      1329\n",
            "              metals       0.32      0.94      0.48       191\n",
            "          networking       0.79      0.93      0.85       335\n",
            "        neuroscience       0.93      0.96      0.95       297\n",
            "        oceanography       0.68      0.78      0.73       980\n",
            "             politic       0.82      0.53      0.64       593\n",
            "           sociology       0.54      0.71      0.62       729\n",
            "software_engineering       0.84      0.84      0.84       514\n",
            "          statistics       0.65      0.80      0.72       637\n",
            "    theory_computing       0.75      0.67      0.71       432\n",
            "\n",
            "            accuracy                           0.74     10364\n",
            "           macro avg       0.75      0.78      0.74     10364\n",
            "        weighted avg       0.78      0.74      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5311, Train: 0.6625, Test: 0.5425\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1357, Train: 0.9125, Test: 0.7335\n",
            "Early stopping:  1.6937977165432034\n",
            "Epoch: 003, Loss: 0.4200, Train: 0.9437, Test: 0.7728\n",
            "Early stopping:  1.6293781489712134\n",
            "Epoch: 004, Loss: 0.2548, Train: 0.9375, Test: 0.7651\n",
            "Early stopping:  1.5129162517295698\n",
            "Epoch: 005, Loss: 0.1855, Train: 0.9750, Test: 0.7724\n",
            "Early stopping:  1.4075357881299597\n",
            "Epoch: 006, Loss: 0.0939, Train: 0.9750, Test: 0.7514\n",
            "Early stopping:  0.41859697548282865\n",
            "Epoch: 007, Loss: 0.0680, Train: 0.9812, Test: 0.7349\n",
            "Early stopping:  0.14160063526963462\n",
            "Epoch: 008, Loss: 0.0533, Train: 0.9938, Test: 0.7298\n",
            "Early stopping:  0.08613027902638598\n",
            "Epoch: 009, Loss: 0.0309, Train: 0.9812, Test: 0.7297\n",
            "Early stopping:  0.05997225101326028\n",
            "Epoch: 010, Loss: 0.0269, Train: 0.9875, Test: 0.7316\n",
            "Early stopping:  0.02762210804548976\n",
            "Epoch: 011, Loss: 0.0216, Train: 1.0000, Test: 0.7356\n",
            "Early stopping:  0.019688990566690715\n",
            "Epoch: 012, Loss: 0.0083, Train: 1.0000, Test: 0.7370\n",
            "Early stopping:  0.01641716204751327\n",
            "Epoch: 013, Loss: 0.0038, Train: 1.0000, Test: 0.7394\n",
            "Early stopping:  0.011761202005926322\n",
            "Epoch: 014, Loss: 0.0056, Train: 0.9938, Test: 0.7423\n",
            "Early stopping:  0.010354694852055365\n",
            "Epoch: 015, Loss: 0.0070, Train: 1.0000, Test: 0.7438\n",
            "Early stopping:  0.007102810243963654\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.78      0.77       392\n",
            "             ecology       0.74      0.80      0.77       879\n",
            "            economic       0.78      0.71      0.74      1399\n",
            "          geophysics       0.99      0.68      0.81      1192\n",
            "  gravitional_theory       0.67      0.93      0.78       120\n",
            "               hydro       0.91      0.68      0.78       345\n",
            "                math       0.90      0.61      0.73      1329\n",
            "              metals       0.24      0.98      0.38       191\n",
            "          networking       0.86      0.87      0.87       335\n",
            "        neuroscience       0.92      0.98      0.95       297\n",
            "        oceanography       0.83      0.74      0.78       980\n",
            "             politic       0.77      0.70      0.73       593\n",
            "           sociology       0.61      0.78      0.68       729\n",
            "software_engineering       0.77      0.80      0.79       514\n",
            "          statistics       0.63      0.87      0.73       637\n",
            "    theory_computing       0.74      0.68      0.71       432\n",
            "\n",
            "            accuracy                           0.74     10364\n",
            "           macro avg       0.76      0.79      0.75     10364\n",
            "        weighted avg       0.80      0.74      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5828, Train: 0.7063, Test: 0.5685\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0171, Train: 0.8625, Test: 0.7131\n",
            "Early stopping:  1.814258720090251\n",
            "Epoch: 003, Loss: 0.3899, Train: 0.9375, Test: 0.7447\n",
            "Early stopping:  1.6917273972408409\n",
            "Epoch: 004, Loss: 0.2165, Train: 0.9688, Test: 0.7826\n",
            "Early stopping:  1.5592556786820233\n",
            "Epoch: 005, Loss: 0.1050, Train: 0.9750, Test: 0.7818\n",
            "Early stopping:  1.4525213098805212\n",
            "Epoch: 006, Loss: 0.0806, Train: 0.9875, Test: 0.7759\n",
            "Early stopping:  0.3861123818970059\n",
            "Epoch: 007, Loss: 0.0387, Train: 1.0000, Test: 0.7649\n",
            "Early stopping:  0.1413074987718703\n",
            "Epoch: 008, Loss: 0.0154, Train: 0.9938, Test: 0.7521\n",
            "Early stopping:  0.07824491431983456\n",
            "Epoch: 009, Loss: 0.0181, Train: 0.9938, Test: 0.7468\n",
            "Early stopping:  0.03965618879163258\n",
            "Epoch: 010, Loss: 0.0206, Train: 0.9938, Test: 0.7477\n",
            "Early stopping:  0.027259069963757637\n",
            "Epoch: 011, Loss: 0.0094, Train: 1.0000, Test: 0.7514\n",
            "Early stopping:  0.011017531152443857\n",
            "Epoch: 012, Loss: 0.0023, Train: 1.0000, Test: 0.7539\n",
            "Early stopping:  0.0073467880513923365\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.65      0.78      0.71       392\n",
            "             ecology       0.68      0.91      0.78       879\n",
            "            economic       0.82      0.51      0.63      1399\n",
            "          geophysics       0.91      0.94      0.93      1192\n",
            "  gravitional_theory       0.93      0.91      0.92       120\n",
            "               hydro       0.86      0.80      0.83       345\n",
            "                math       0.96      0.67      0.79      1329\n",
            "              metals       0.55      0.86      0.67       191\n",
            "          networking       0.75      0.91      0.82       335\n",
            "        neuroscience       0.92      0.98      0.95       297\n",
            "        oceanography       0.92      0.60      0.73       980\n",
            "             politic       0.59      0.87      0.70       593\n",
            "           sociology       0.52      0.69      0.59       729\n",
            "software_engineering       0.85      0.69      0.76       514\n",
            "          statistics       0.65      0.84      0.74       637\n",
            "    theory_computing       0.65      0.80      0.72       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.76      0.80      0.77     10364\n",
            "        weighted avg       0.79      0.75      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2823, Train: 0.7562, Test: 0.6013\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0956, Train: 0.8625, Test: 0.6645\n",
            "Early stopping:  1.5462518837376\n",
            "Epoch: 003, Loss: 0.4598, Train: 0.9563, Test: 0.7461\n",
            "Early stopping:  1.48057648506829\n",
            "Epoch: 004, Loss: 0.2079, Train: 0.9625, Test: 0.7659\n",
            "Early stopping:  1.3981029027786447\n",
            "Epoch: 005, Loss: 0.1230, Train: 0.9812, Test: 0.7872\n",
            "Early stopping:  1.3134810662478444\n",
            "Epoch: 006, Loss: 0.0864, Train: 0.9875, Test: 0.7934\n",
            "Early stopping:  0.4181208322059507\n",
            "Epoch: 007, Loss: 0.0600, Train: 0.9938, Test: 0.7894\n",
            "Early stopping:  0.16219361936945026\n",
            "Epoch: 008, Loss: 0.0289, Train: 1.0000, Test: 0.7807\n",
            "Early stopping:  0.06894094924900857\n",
            "Epoch: 009, Loss: 0.0153, Train: 1.0000, Test: 0.7740\n",
            "Early stopping:  0.04357359324386553\n",
            "Epoch: 010, Loss: 0.0071, Train: 1.0000, Test: 0.7686\n",
            "Early stopping:  0.03303327362526471\n",
            "Epoch: 011, Loss: 0.0040, Train: 1.0000, Test: 0.7624\n",
            "Early stopping:  0.022769174493703616\n",
            "Epoch: 012, Loss: 0.0026, Train: 1.0000, Test: 0.7583\n",
            "Early stopping:  0.010854717270937205\n",
            "Epoch: 013, Loss: 0.0019, Train: 1.0000, Test: 0.7551\n",
            "Early stopping:  0.005469349576676369\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 14, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.80      0.79       392\n",
            "             ecology       0.77      0.87      0.82       879\n",
            "            economic       0.82      0.65      0.72      1399\n",
            "          geophysics       0.98      0.76      0.86      1192\n",
            "  gravitional_theory       0.71      0.93      0.81       120\n",
            "               hydro       0.67      0.89      0.77       345\n",
            "                math       0.97      0.62      0.76      1329\n",
            "              metals       0.39      0.96      0.55       191\n",
            "          networking       0.90      0.79      0.84       335\n",
            "        neuroscience       0.92      0.97      0.95       297\n",
            "        oceanography       0.89      0.74      0.81       980\n",
            "             politic       0.54      0.84      0.66       593\n",
            "           sociology       0.65      0.58      0.61       729\n",
            "software_engineering       0.86      0.84      0.85       514\n",
            "          statistics       0.50      0.94      0.65       637\n",
            "    theory_computing       0.82      0.64      0.72       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.76      0.80      0.76     10364\n",
            "        weighted avg       0.81      0.76      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4579, Train: 0.6625, Test: 0.5768\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2041, Train: 0.8562, Test: 0.7492\n",
            "Early stopping:  1.5936930507710385\n",
            "Epoch: 003, Loss: 0.6140, Train: 0.8875, Test: 0.7666\n",
            "Early stopping:  1.5008930794594761\n",
            "Epoch: 004, Loss: 0.4511, Train: 0.9125, Test: 0.7513\n",
            "Early stopping:  1.3889598586572052\n",
            "Epoch: 005, Loss: 0.3604, Train: 0.9437, Test: 0.7347\n",
            "Early stopping:  1.2947802877681251\n",
            "Epoch: 006, Loss: 0.2133, Train: 0.9563, Test: 0.7366\n",
            "Early stopping:  0.38382422781895076\n",
            "Epoch: 007, Loss: 0.1624, Train: 0.9688, Test: 0.7476\n",
            "Early stopping:  0.18259411713873622\n",
            "Epoch: 008, Loss: 0.1067, Train: 0.9688, Test: 0.7555\n",
            "Early stopping:  0.1430050242098624\n",
            "Epoch: 009, Loss: 0.0999, Train: 0.9688, Test: 0.7602\n",
            "Early stopping:  0.10653032877741156\n",
            "Epoch: 010, Loss: 0.0760, Train: 0.9750, Test: 0.7605\n",
            "Early stopping:  0.05553766348230236\n",
            "Epoch: 011, Loss: 0.0648, Train: 0.9812, Test: 0.7598\n",
            "Early stopping:  0.03785950857734635\n",
            "Epoch: 012, Loss: 0.0565, Train: 0.9812, Test: 0.7595\n",
            "Early stopping:  0.02184227846098948\n",
            "Epoch: 013, Loss: 0.0355, Train: 1.0000, Test: 0.7636\n",
            "Early stopping:  0.023840009526499673\n",
            "Epoch: 014, Loss: 0.0118, Train: 0.9938, Test: 0.7635\n",
            "Early stopping:  0.02549645919127801\n",
            "Epoch: 015, Loss: 0.0143, Train: 1.0000, Test: 0.7634\n",
            "Early stopping:  0.024005516001953817\n",
            "Epoch: 016, Loss: 0.0094, Train: 1.0000, Test: 0.7609\n",
            "Early stopping:  0.020184371218483495\n",
            "Epoch: 017, Loss: 0.0115, Train: 1.0000, Test: 0.7619\n",
            "Early stopping:  0.010742724061227721\n",
            "Epoch: 018, Loss: 0.0110, Train: 1.0000, Test: 0.7623\n",
            "Early stopping:  0.001742787398428446\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.85      0.82      0.83       392\n",
            "             ecology       0.80      0.82      0.81       879\n",
            "            economic       0.76      0.71      0.74      1399\n",
            "          geophysics       0.84      0.96      0.90      1192\n",
            "  gravitional_theory       0.65      0.94      0.77       120\n",
            "               hydro       0.70      0.86      0.77       345\n",
            "                math       0.93      0.59      0.72      1329\n",
            "              metals       0.85      0.75      0.80       191\n",
            "          networking       0.87      0.78      0.82       335\n",
            "        neuroscience       0.88      0.98      0.93       297\n",
            "        oceanography       0.88      0.72      0.79       980\n",
            "             politic       0.60      0.79      0.68       593\n",
            "           sociology       0.65      0.52      0.58       729\n",
            "software_engineering       0.70      0.77      0.73       514\n",
            "          statistics       0.56      0.91      0.70       637\n",
            "    theory_computing       0.73      0.69      0.71       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.77      0.79      0.77     10364\n",
            "        weighted avg       0.78      0.76      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6225, Train: 0.7188, Test: 0.5551\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0403, Train: 0.8187, Test: 0.7237\n",
            "Early stopping:  1.8258855183078602\n",
            "Epoch: 003, Loss: 0.5493, Train: 0.8750, Test: 0.7460\n",
            "Early stopping:  1.6509493297547184\n",
            "Epoch: 004, Loss: 0.3039, Train: 0.9750, Test: 0.7629\n",
            "Early stopping:  1.5266954622282773\n",
            "Epoch: 005, Loss: 0.1279, Train: 0.9750, Test: 0.7627\n",
            "Early stopping:  1.435680954745474\n",
            "Epoch: 006, Loss: 0.0738, Train: 0.9875, Test: 0.7647\n",
            "Early stopping:  0.3937614816734889\n",
            "Epoch: 007, Loss: 0.0436, Train: 1.0000, Test: 0.7707\n",
            "Early stopping:  0.21002590242977281\n",
            "Epoch: 008, Loss: 0.0204, Train: 0.9938, Test: 0.7664\n",
            "Early stopping:  0.11356541536664286\n",
            "Epoch: 009, Loss: 0.0189, Train: 0.9938, Test: 0.7637\n",
            "Early stopping:  0.04547856980887203\n",
            "Epoch: 010, Loss: 0.0119, Train: 1.0000, Test: 0.7653\n",
            "Early stopping:  0.025386823680152854\n",
            "Epoch: 011, Loss: 0.0049, Train: 1.0000, Test: 0.7612\n",
            "Early stopping:  0.01459703633391099\n",
            "Epoch: 012, Loss: 0.0044, Train: 1.0000, Test: 0.7541\n",
            "Early stopping:  0.007546794849691575\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.81      0.72      0.76       392\n",
            "             ecology       0.76      0.57      0.65       879\n",
            "            economic       0.76      0.67      0.71      1399\n",
            "          geophysics       0.97      0.86      0.91      1192\n",
            "  gravitional_theory       0.95      0.88      0.91       120\n",
            "               hydro       0.52      0.81      0.63       345\n",
            "                math       0.86      0.75      0.80      1329\n",
            "              metals       0.80      0.75      0.78       191\n",
            "          networking       0.79      0.91      0.85       335\n",
            "        neuroscience       0.94      0.97      0.95       297\n",
            "        oceanography       0.82      0.81      0.82       980\n",
            "             politic       0.70      0.80      0.75       593\n",
            "           sociology       0.52      0.61      0.56       729\n",
            "software_engineering       0.84      0.77      0.80       514\n",
            "          statistics       0.57      0.87      0.69       637\n",
            "    theory_computing       0.67      0.66      0.66       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.77      0.78      0.76     10364\n",
            "        weighted avg       0.77      0.75      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5319, Train: 0.7875, Test: 0.6376\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8693, Train: 0.9250, Test: 0.7751\n",
            "Early stopping:  1.882748276151121\n",
            "Epoch: 003, Loss: 0.2801, Train: 0.9625, Test: 0.7785\n",
            "Early stopping:  1.7325694049761493\n",
            "Epoch: 004, Loss: 0.1231, Train: 0.9875, Test: 0.7637\n",
            "Early stopping:  1.586732704632196\n",
            "Epoch: 005, Loss: 0.0593, Train: 0.9938, Test: 0.7569\n",
            "Early stopping:  1.4659558721460975\n",
            "Epoch: 006, Loss: 0.0341, Train: 1.0000, Test: 0.7517\n",
            "Early stopping:  0.346720755698798\n",
            "Epoch: 007, Loss: 0.0155, Train: 1.0000, Test: 0.7497\n",
            "Early stopping:  0.10734812962439311\n",
            "Epoch: 008, Loss: 0.0082, Train: 0.9938, Test: 0.7464\n",
            "Early stopping:  0.046366291398922\n",
            "Epoch: 009, Loss: 0.0093, Train: 0.9938, Test: 0.7470\n",
            "Early stopping:  0.02168719866599905\n",
            "Epoch: 010, Loss: 0.0084, Train: 1.0000, Test: 0.7477\n",
            "Early stopping:  0.01104311426954799\n",
            "Epoch: 011, Loss: 0.0028, Train: 1.0000, Test: 0.7482\n",
            "Early stopping:  0.004507610378629194\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.69      0.74       392\n",
            "             ecology       0.84      0.77      0.80       879\n",
            "            economic       0.75      0.69      0.72      1399\n",
            "          geophysics       0.88      0.85      0.87      1192\n",
            "  gravitional_theory       0.94      0.82      0.87       120\n",
            "               hydro       0.44      0.92      0.60       345\n",
            "                math       0.96      0.54      0.69      1329\n",
            "              metals       0.65      0.93      0.77       191\n",
            "          networking       0.84      0.88      0.86       335\n",
            "        neuroscience       0.87      0.99      0.93       297\n",
            "        oceanography       0.88      0.75      0.81       980\n",
            "             politic       0.75      0.76      0.75       593\n",
            "           sociology       0.60      0.73      0.66       729\n",
            "software_engineering       0.68      0.93      0.79       514\n",
            "          statistics       0.52      0.77      0.62       637\n",
            "    theory_computing       0.86      0.55      0.67       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.77      0.79      0.76     10364\n",
            "        weighted avg       0.79      0.75      0.75     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5566, Train: 0.8187, Test: 0.7130\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.7199, Train: 0.9062, Test: 0.7376\n",
            "Early stopping:  2.005886321075035\n",
            "Epoch: 003, Loss: 0.3451, Train: 0.9375, Test: 0.7507\n",
            "Early stopping:  1.7560383929007835\n",
            "Epoch: 004, Loss: 0.1761, Train: 0.9812, Test: 0.7785\n",
            "Early stopping:  1.5878179160653476\n",
            "Epoch: 005, Loss: 0.0801, Train: 0.9812, Test: 0.7784\n",
            "Early stopping:  1.4633731299827537\n",
            "Epoch: 006, Loss: 0.0546, Train: 0.9938, Test: 0.7687\n",
            "Early stopping:  0.2734844813991545\n",
            "Epoch: 007, Loss: 0.0337, Train: 0.9875, Test: 0.7651\n",
            "Early stopping:  0.12797943887412966\n",
            "Epoch: 008, Loss: 0.0265, Train: 1.0000, Test: 0.7630\n",
            "Early stopping:  0.060680794254628655\n",
            "Epoch: 009, Loss: 0.0127, Train: 1.0000, Test: 0.7631\n",
            "Early stopping:  0.02634524424309233\n",
            "Epoch: 010, Loss: 0.0072, Train: 1.0000, Test: 0.7612\n",
            "Early stopping:  0.018731378562070446\n",
            "Epoch: 011, Loss: 0.0076, Train: 1.0000, Test: 0.7583\n",
            "Early stopping:  0.011941598130141444\n",
            "Epoch: 012, Loss: 0.0066, Train: 1.0000, Test: 0.7566\n",
            "Early stopping:  0.008410425839437477\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.67      0.70       392\n",
            "             ecology       0.84      0.81      0.82       879\n",
            "            economic       0.84      0.51      0.63      1399\n",
            "          geophysics       0.93      0.87      0.90      1192\n",
            "  gravitional_theory       0.79      0.93      0.85       120\n",
            "               hydro       0.61      0.85      0.71       345\n",
            "                math       0.93      0.72      0.81      1329\n",
            "              metals       0.57      0.93      0.71       191\n",
            "          networking       0.78      0.85      0.81       335\n",
            "        neuroscience       0.94      0.97      0.95       297\n",
            "        oceanography       0.89      0.86      0.87       980\n",
            "             politic       0.66      0.80      0.72       593\n",
            "           sociology       0.44      0.71      0.54       729\n",
            "software_engineering       0.83      0.83      0.83       514\n",
            "          statistics       0.62      0.72      0.66       637\n",
            "    theory_computing       0.65      0.68      0.67       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.75      0.79      0.76     10364\n",
            "        weighted avg       0.79      0.76      0.76     10364\n",
            "\n",
            "time: 5.21 s (started: 2024-08-16 14:13:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sIidyHtKnn2",
        "outputId": "7537ee20-5eb2-44ed-8f9e-9fc6c4dc45f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 394 ms (started: 2024-08-16 14:13:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "lzle8ACUKnn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSw3m620Knn2",
        "outputId": "45487de3-95e9-4b85-82e3-572f3977f730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7811, Train: 0.7562, Test: 0.5229\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5169, Train: 0.9187, Test: 0.7213\n",
            "Early stopping:  0.18677679374911968\n",
            "Epoch: 003, Loss: 2.1877, Train: 0.9500, Test: 0.7701\n",
            "Early stopping:  0.29728103424651514\n",
            "Epoch: 004, Loss: 1.8045, Train: 0.9437, Test: 0.7782\n",
            "Early stopping:  0.42211150633810185\n",
            "Epoch: 005, Loss: 1.4139, Train: 0.9375, Test: 0.7776\n",
            "Early stopping:  0.5465806113634537\n",
            "Epoch: 006, Loss: 1.0544, Train: 0.9437, Test: 0.7761\n",
            "Early stopping:  0.5850555770858353\n",
            "Epoch: 007, Loss: 0.7564, Train: 0.9500, Test: 0.7725\n",
            "Early stopping:  0.5719570732784492\n",
            "Epoch: 008, Loss: 0.5324, Train: 0.9812, Test: 0.7720\n",
            "Early stopping:  0.5090450094405661\n",
            "Epoch: 009, Loss: 0.3737, Train: 0.9938, Test: 0.7757\n",
            "Early stopping:  0.4163686046926992\n",
            "Epoch: 010, Loss: 0.2629, Train: 1.0000, Test: 0.7770\n",
            "Early stopping:  0.3163483541504974\n",
            "Epoch: 011, Loss: 0.1850, Train: 1.0000, Test: 0.7801\n",
            "Early stopping:  0.2279602184328303\n",
            "Epoch: 012, Loss: 0.1296, Train: 1.0000, Test: 0.7823\n",
            "Early stopping:  0.160503095952791\n",
            "Epoch: 013, Loss: 0.0904, Train: 1.0000, Test: 0.7827\n",
            "Early stopping:  0.11289440697937367\n",
            "Epoch: 014, Loss: 0.0630, Train: 1.0000, Test: 0.7821\n",
            "Early stopping:  0.07971817551522951\n",
            "Epoch: 015, Loss: 0.0441, Train: 1.0000, Test: 0.7817\n",
            "Early stopping:  0.05624151506893854\n",
            "Epoch: 016, Loss: 0.0311, Train: 1.0000, Test: 0.7790\n",
            "Early stopping:  0.03932861083474743\n",
            "Epoch: 017, Loss: 0.0223, Train: 1.0000, Test: 0.7768\n",
            "Early stopping:  0.027222459340381346\n",
            "Epoch: 018, Loss: 0.0163, Train: 1.0000, Test: 0.7755\n",
            "Early stopping:  0.0186695083334984\n",
            "Epoch: 019, Loss: 0.0121, Train: 1.0000, Test: 0.7728\n",
            "Early stopping:  0.012744008820521045\n",
            "Epoch: 020, Loss: 0.0093, Train: 1.0000, Test: 0.7706\n",
            "Early stopping:  0.008693039560280308\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.83      0.76       392\n",
            "             ecology       0.85      0.79      0.82       879\n",
            "            economic       0.79      0.57      0.66      1399\n",
            "          geophysics       0.86      0.93      0.89      1192\n",
            "  gravitional_theory       0.65      0.93      0.77       120\n",
            "               hydro       0.66      0.82      0.73       345\n",
            "                math       0.88      0.76      0.81      1329\n",
            "              metals       0.74      0.87      0.80       191\n",
            "          networking       0.70      0.94      0.81       335\n",
            "        neuroscience       0.90      0.97      0.93       297\n",
            "        oceanography       0.84      0.81      0.83       980\n",
            "             politic       0.58      0.86      0.69       593\n",
            "           sociology       0.57      0.59      0.58       729\n",
            "software_engineering       0.81      0.91      0.86       514\n",
            "          statistics       0.77      0.65      0.71       637\n",
            "    theory_computing       0.78      0.62      0.69       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.76      0.80      0.77     10364\n",
            "        weighted avg       0.78      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7971, Train: 0.7688, Test: 0.5467\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5126, Train: 0.8938, Test: 0.7119\n",
            "Early stopping:  0.2011788776474743\n",
            "Epoch: 003, Loss: 2.1759, Train: 0.8812, Test: 0.7359\n",
            "Early stopping:  0.3109557836302274\n",
            "Epoch: 004, Loss: 1.7873, Train: 0.8750, Test: 0.7443\n",
            "Early stopping:  0.43560167264469274\n",
            "Epoch: 005, Loss: 1.3981, Train: 0.8938, Test: 0.7542\n",
            "Early stopping:  0.5582678876458703\n",
            "Epoch: 006, Loss: 1.0457, Train: 0.9062, Test: 0.7662\n",
            "Early stopping:  0.5870706478319236\n",
            "Epoch: 007, Loss: 0.7532, Train: 0.9125, Test: 0.7811\n",
            "Early stopping:  0.5680849859214959\n",
            "Epoch: 008, Loss: 0.5312, Train: 0.9313, Test: 0.7942\n",
            "Early stopping:  0.5019891954859582\n",
            "Epoch: 009, Loss: 0.3759, Train: 0.9500, Test: 0.7985\n",
            "Early stopping:  0.4093319597060201\n",
            "Epoch: 010, Loss: 0.2724, Train: 0.9688, Test: 0.8001\n",
            "Early stopping:  0.3099503326768383\n",
            "Epoch: 011, Loss: 0.2020, Train: 0.9875, Test: 0.7953\n",
            "Early stopping:  0.22043797126731032\n",
            "Epoch: 012, Loss: 0.1500, Train: 0.9938, Test: 0.7913\n",
            "Early stopping:  0.15154283573114258\n",
            "Epoch: 013, Loss: 0.1092, Train: 0.9938, Test: 0.7889\n",
            "Early stopping:  0.10549368928946574\n",
            "Epoch: 014, Loss: 0.0784, Train: 1.0000, Test: 0.7846\n",
            "Early stopping:  0.07698618859845462\n",
            "Epoch: 015, Loss: 0.0577, Train: 1.0000, Test: 0.7794\n",
            "Early stopping:  0.05779223404831796\n",
            "Epoch: 016, Loss: 0.0445, Train: 1.0000, Test: 0.7778\n",
            "Early stopping:  0.042412497547439816\n",
            "Epoch: 017, Loss: 0.0345, Train: 1.0000, Test: 0.7793\n",
            "Early stopping:  0.029760864230510546\n",
            "Epoch: 018, Loss: 0.0258, Train: 1.0000, Test: 0.7800\n",
            "Early stopping:  0.020655519279804697\n",
            "Epoch: 019, Loss: 0.0190, Train: 1.0000, Test: 0.7802\n",
            "Early stopping:  0.015287856856275843\n",
            "Epoch: 020, Loss: 0.0145, Train: 1.0000, Test: 0.7782\n",
            "Early stopping:  0.0120446445467974\n",
            "Epoch: 021, Loss: 0.0118, Train: 1.0000, Test: 0.7774\n",
            "Early stopping:  0.009140731550742368\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.76      0.75       392\n",
            "             ecology       0.80      0.85      0.83       879\n",
            "            economic       0.80      0.65      0.71      1399\n",
            "          geophysics       0.95      0.85      0.90      1192\n",
            "  gravitional_theory       0.64      0.91      0.75       120\n",
            "               hydro       0.64      0.88      0.74       345\n",
            "                math       0.85      0.81      0.83      1329\n",
            "              metals       0.63      0.94      0.76       191\n",
            "          networking       0.93      0.75      0.83       335\n",
            "        neuroscience       0.95      0.96      0.95       297\n",
            "        oceanography       0.90      0.73      0.81       980\n",
            "             politic       0.66      0.81      0.72       593\n",
            "           sociology       0.59      0.70      0.64       729\n",
            "software_engineering       0.88      0.70      0.78       514\n",
            "          statistics       0.64      0.84      0.72       637\n",
            "    theory_computing       0.66      0.67      0.67       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.80      0.77     10364\n",
            "        weighted avg       0.80      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7728, Train: 0.7937, Test: 0.5920\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5040, Train: 0.8812, Test: 0.6674\n",
            "Early stopping:  0.19008414124546658\n",
            "Epoch: 003, Loss: 2.1809, Train: 0.9000, Test: 0.7138\n",
            "Early stopping:  0.29634158953724365\n",
            "Epoch: 004, Loss: 1.8018, Train: 0.9125, Test: 0.7467\n",
            "Early stopping:  0.41899387693723456\n",
            "Epoch: 005, Loss: 1.4114, Train: 0.9000, Test: 0.7640\n",
            "Early stopping:  0.5430702718419251\n",
            "Epoch: 006, Loss: 1.0538, Train: 0.9125, Test: 0.7767\n",
            "Early stopping:  0.5805279024452386\n",
            "Epoch: 007, Loss: 0.7590, Train: 0.9250, Test: 0.7856\n",
            "Early stopping:  0.5686788396055039\n",
            "Epoch: 008, Loss: 0.5362, Train: 0.9375, Test: 0.7916\n",
            "Early stopping:  0.5061965506484984\n",
            "Epoch: 009, Loss: 0.3804, Train: 0.9500, Test: 0.7911\n",
            "Early stopping:  0.41278029935584604\n",
            "Epoch: 010, Loss: 0.2747, Train: 0.9625, Test: 0.7924\n",
            "Early stopping:  0.31197418817078243\n",
            "Epoch: 011, Loss: 0.2002, Train: 0.9750, Test: 0.7924\n",
            "Early stopping:  0.22298045187510887\n",
            "Epoch: 012, Loss: 0.1455, Train: 0.9812, Test: 0.7931\n",
            "Early stopping:  0.15528236795402886\n",
            "Epoch: 013, Loss: 0.1050, Train: 0.9875, Test: 0.7911\n",
            "Early stopping:  0.1094102110267607\n",
            "Epoch: 014, Loss: 0.0756, Train: 0.9875, Test: 0.7882\n",
            "Early stopping:  0.07926027030030729\n",
            "Epoch: 015, Loss: 0.0545, Train: 1.0000, Test: 0.7854\n",
            "Early stopping:  0.05808637787397681\n",
            "Epoch: 016, Loss: 0.0395, Train: 1.0000, Test: 0.7829\n",
            "Early stopping:  0.042258528034616556\n",
            "Epoch: 017, Loss: 0.0289, Train: 1.0000, Test: 0.7809\n",
            "Early stopping:  0.030361891685335953\n",
            "Epoch: 018, Loss: 0.0214, Train: 1.0000, Test: 0.7806\n",
            "Early stopping:  0.02163738525649292\n",
            "Epoch: 019, Loss: 0.0161, Train: 1.0000, Test: 0.7814\n",
            "Early stopping:  0.015328945301904321\n",
            "Epoch: 020, Loss: 0.0124, Train: 1.0000, Test: 0.7808\n",
            "Early stopping:  0.010824355001487217\n",
            "Epoch: 021, Loss: 0.0099, Train: 1.0000, Test: 0.7795\n",
            "Early stopping:  0.007596009029492301\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.76      0.71       392\n",
            "             ecology       0.84      0.70      0.77       879\n",
            "            economic       0.83      0.68      0.75      1399\n",
            "          geophysics       0.91      0.89      0.90      1192\n",
            "  gravitional_theory       0.94      0.88      0.91       120\n",
            "               hydro       0.62      0.86      0.72       345\n",
            "                math       0.90      0.74      0.81      1329\n",
            "              metals       0.49      0.92      0.64       191\n",
            "          networking       0.79      0.93      0.86       335\n",
            "        neuroscience       0.95      0.91      0.93       297\n",
            "        oceanography       0.76      0.88      0.81       980\n",
            "             politic       0.78      0.67      0.72       593\n",
            "           sociology       0.61      0.71      0.66       729\n",
            "software_engineering       0.80      0.89      0.84       514\n",
            "          statistics       0.74      0.75      0.74       637\n",
            "    theory_computing       0.66      0.70      0.68       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.80      0.78     10364\n",
            "        weighted avg       0.79      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7838, Train: 0.8313, Test: 0.6438\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5055, Train: 0.9250, Test: 0.7369\n",
            "Early stopping:  0.1967637423848671\n",
            "Epoch: 003, Loss: 2.1667, Train: 0.9375, Test: 0.7482\n",
            "Early stopping:  0.30902705240766143\n",
            "Epoch: 004, Loss: 1.7679, Train: 0.9500, Test: 0.7616\n",
            "Early stopping:  0.4385651124145475\n",
            "Epoch: 005, Loss: 1.3624, Train: 0.9625, Test: 0.7755\n",
            "Early stopping:  0.5677299352799303\n",
            "Epoch: 006, Loss: 0.9975, Train: 0.9688, Test: 0.7891\n",
            "Early stopping:  0.6043089679610324\n",
            "Epoch: 007, Loss: 0.7027, Train: 0.9625, Test: 0.7935\n",
            "Early stopping:  0.5858331160777729\n",
            "Epoch: 008, Loss: 0.4861, Train: 0.9688, Test: 0.7918\n",
            "Early stopping:  0.5131811244001807\n",
            "Epoch: 009, Loss: 0.3386, Train: 0.9688, Test: 0.7889\n",
            "Early stopping:  0.4103713035487023\n",
            "Epoch: 010, Loss: 0.2413, Train: 0.9812, Test: 0.7873\n",
            "Early stopping:  0.30313383473535777\n",
            "Epoch: 011, Loss: 0.1755, Train: 0.9812, Test: 0.7864\n",
            "Early stopping:  0.21083236268945457\n",
            "Epoch: 012, Loss: 0.1287, Train: 0.9875, Test: 0.7851\n",
            "Early stopping:  0.1423395216632582\n",
            "Epoch: 013, Loss: 0.0949, Train: 0.9938, Test: 0.7828\n",
            "Early stopping:  0.09690915535987514\n",
            "Epoch: 014, Loss: 0.0702, Train: 0.9938, Test: 0.7815\n",
            "Early stopping:  0.06807498532116524\n",
            "Epoch: 015, Loss: 0.0520, Train: 1.0000, Test: 0.7783\n",
            "Early stopping:  0.04910235562852111\n",
            "Epoch: 016, Loss: 0.0388, Train: 1.0000, Test: 0.7779\n",
            "Early stopping:  0.03580533484247781\n",
            "Epoch: 017, Loss: 0.0291, Train: 1.0000, Test: 0.7766\n",
            "Early stopping:  0.026180867552270434\n",
            "Epoch: 018, Loss: 0.0222, Train: 1.0000, Test: 0.7761\n",
            "Early stopping:  0.019116146894723195\n",
            "Epoch: 019, Loss: 0.0172, Train: 1.0000, Test: 0.7765\n",
            "Early stopping:  0.013865184722954347\n",
            "Epoch: 020, Loss: 0.0137, Train: 1.0000, Test: 0.7769\n",
            "Early stopping:  0.009989665243747897\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.73      0.74       392\n",
            "             ecology       0.76      0.84      0.80       879\n",
            "            economic       0.81      0.68      0.74      1399\n",
            "          geophysics       0.96      0.84      0.89      1192\n",
            "  gravitional_theory       0.76      0.94      0.84       120\n",
            "               hydro       0.82      0.78      0.80       345\n",
            "                math       0.94      0.68      0.78      1329\n",
            "              metals       0.43      0.96      0.59       191\n",
            "          networking       0.77      0.93      0.84       335\n",
            "        neuroscience       0.91      0.97      0.94       297\n",
            "        oceanography       0.88      0.77      0.82       980\n",
            "             politic       0.73      0.79      0.76       593\n",
            "           sociology       0.60      0.69      0.64       729\n",
            "software_engineering       0.78      0.79      0.79       514\n",
            "          statistics       0.66      0.87      0.75       637\n",
            "    theory_computing       0.64      0.75      0.69       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.76      0.81      0.78     10364\n",
            "        weighted avg       0.80      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7758, Train: 0.6813, Test: 0.4930\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4997, Train: 0.8500, Test: 0.6737\n",
            "Early stopping:  0.19523768929398463\n",
            "Epoch: 003, Loss: 2.1593, Train: 0.8938, Test: 0.7002\n",
            "Early stopping:  0.3087911700770911\n",
            "Epoch: 004, Loss: 1.7701, Train: 0.9062, Test: 0.7293\n",
            "Early stopping:  0.43467543643133805\n",
            "Epoch: 005, Loss: 1.3786, Train: 0.9563, Test: 0.7616\n",
            "Early stopping:  0.5585176346510073\n",
            "Epoch: 006, Loss: 1.0218, Train: 0.9625, Test: 0.7871\n",
            "Early stopping:  0.5909394752061352\n",
            "Epoch: 007, Loss: 0.7263, Train: 0.9625, Test: 0.7938\n",
            "Early stopping:  0.5723342773115709\n",
            "Epoch: 008, Loss: 0.5063, Train: 0.9688, Test: 0.7969\n",
            "Early stopping:  0.5057210306133332\n",
            "Epoch: 009, Loss: 0.3546, Train: 0.9812, Test: 0.7942\n",
            "Early stopping:  0.4104915153963269\n",
            "Epoch: 010, Loss: 0.2496, Train: 0.9875, Test: 0.7941\n",
            "Early stopping:  0.30887330301647725\n",
            "Epoch: 011, Loss: 0.1759, Train: 0.9875, Test: 0.7897\n",
            "Early stopping:  0.21943556868336103\n",
            "Epoch: 012, Loss: 0.1247, Train: 1.0000, Test: 0.7867\n",
            "Early stopping:  0.15218900148955522\n",
            "Epoch: 013, Loss: 0.0889, Train: 1.0000, Test: 0.7868\n",
            "Early stopping:  0.10602267961676172\n",
            "Epoch: 014, Loss: 0.0631, Train: 1.0000, Test: 0.7865\n",
            "Early stopping:  0.07426466702878269\n",
            "Epoch: 015, Loss: 0.0448, Train: 1.0000, Test: 0.7858\n",
            "Early stopping:  0.052181737959670904\n",
            "Epoch: 016, Loss: 0.0325, Train: 1.0000, Test: 0.7848\n",
            "Early stopping:  0.03685493836773129\n",
            "Epoch: 017, Loss: 0.0240, Train: 1.0000, Test: 0.7826\n",
            "Early stopping:  0.025948459711077385\n",
            "Epoch: 018, Loss: 0.0179, Train: 1.0000, Test: 0.7792\n",
            "Early stopping:  0.01799422325721976\n",
            "Epoch: 019, Loss: 0.0136, Train: 1.0000, Test: 0.7760\n",
            "Early stopping:  0.012424289921244589\n",
            "Epoch: 020, Loss: 0.0106, Train: 1.0000, Test: 0.7734\n",
            "Early stopping:  0.008732948294884493\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.79      0.75       392\n",
            "             ecology       0.72      0.89      0.80       879\n",
            "            economic       0.77      0.63      0.69      1399\n",
            "          geophysics       0.94      0.89      0.92      1192\n",
            "  gravitional_theory       0.90      0.92      0.91       120\n",
            "               hydro       0.84      0.76      0.80       345\n",
            "                math       0.92      0.70      0.80      1329\n",
            "              metals       0.62      0.90      0.73       191\n",
            "          networking       0.74      0.91      0.81       335\n",
            "        neuroscience       0.93      0.97      0.95       297\n",
            "        oceanography       0.88      0.72      0.79       980\n",
            "             politic       0.59      0.89      0.71       593\n",
            "           sociology       0.59      0.60      0.60       729\n",
            "software_engineering       0.81      0.79      0.80       514\n",
            "          statistics       0.72      0.79      0.75       637\n",
            "    theory_computing       0.68      0.75      0.71       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.77      0.81      0.78     10364\n",
            "        weighted avg       0.79      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7832, Train: 0.7562, Test: 0.5948\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4804, Train: 0.8938, Test: 0.7069\n",
            "Early stopping:  0.21409621236648715\n",
            "Epoch: 003, Loss: 2.1282, Train: 0.9000, Test: 0.7152\n",
            "Early stopping:  0.327762649569316\n",
            "Epoch: 004, Loss: 1.7323, Train: 0.9062, Test: 0.7294\n",
            "Early stopping:  0.45324995586290545\n",
            "Epoch: 005, Loss: 1.3439, Train: 0.9313, Test: 0.7606\n",
            "Early stopping:  0.574212832780046\n",
            "Epoch: 006, Loss: 0.9976, Train: 0.9500, Test: 0.7834\n",
            "Early stopping:  0.5930664284787862\n",
            "Epoch: 007, Loss: 0.7104, Train: 0.9688, Test: 0.7972\n",
            "Early stopping:  0.5656570778213992\n",
            "Epoch: 008, Loss: 0.4924, Train: 0.9750, Test: 0.7993\n",
            "Early stopping:  0.4951600956312387\n",
            "Epoch: 009, Loss: 0.3416, Train: 0.9875, Test: 0.7974\n",
            "Early stopping:  0.40155891273690686\n",
            "Epoch: 010, Loss: 0.2416, Train: 0.9938, Test: 0.7929\n",
            "Early stopping:  0.30320305505641776\n",
            "Epoch: 011, Loss: 0.1730, Train: 0.9938, Test: 0.7910\n",
            "Early stopping:  0.21481476753141346\n",
            "Epoch: 012, Loss: 0.1238, Train: 1.0000, Test: 0.7903\n",
            "Early stopping:  0.1467075386989447\n",
            "Epoch: 013, Loss: 0.0880, Train: 1.0000, Test: 0.7920\n",
            "Early stopping:  0.10081605303553022\n",
            "Epoch: 014, Loss: 0.0622, Train: 1.0000, Test: 0.7929\n",
            "Early stopping:  0.07139739659179026\n",
            "Epoch: 015, Loss: 0.0443, Train: 1.0000, Test: 0.7920\n",
            "Early stopping:  0.051369161371603494\n",
            "Epoch: 016, Loss: 0.0321, Train: 1.0000, Test: 0.7907\n",
            "Early stopping:  0.03668439712098984\n",
            "Epoch: 017, Loss: 0.0235, Train: 1.0000, Test: 0.7884\n",
            "Early stopping:  0.025736573991477378\n",
            "Epoch: 018, Loss: 0.0175, Train: 1.0000, Test: 0.7862\n",
            "Early stopping:  0.01781767905328307\n",
            "Epoch: 019, Loss: 0.0133, Train: 1.0000, Test: 0.7847\n",
            "Early stopping:  0.012366177233184363\n",
            "Epoch: 020, Loss: 0.0103, Train: 1.0000, Test: 0.7847\n",
            "Early stopping:  0.008682928250695205\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.81      0.80       392\n",
            "             ecology       0.83      0.82      0.83       879\n",
            "            economic       0.84      0.68      0.76      1399\n",
            "          geophysics       0.94      0.88      0.91      1192\n",
            "  gravitional_theory       0.61      0.96      0.74       120\n",
            "               hydro       0.66      0.85      0.74       345\n",
            "                math       0.95      0.61      0.75      1329\n",
            "              metals       0.46      0.95      0.62       191\n",
            "          networking       0.92      0.81      0.86       335\n",
            "        neuroscience       0.97      0.93      0.95       297\n",
            "        oceanography       0.85      0.85      0.85       980\n",
            "             politic       0.67      0.76      0.71       593\n",
            "           sociology       0.63      0.74      0.68       729\n",
            "software_engineering       0.79      0.90      0.84       514\n",
            "          statistics       0.63      0.87      0.73       637\n",
            "    theory_computing       0.74      0.69      0.71       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.82      0.78     10364\n",
            "        weighted avg       0.81      0.78      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7804, Train: 0.6250, Test: 0.4741\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5131, Train: 0.8313, Test: 0.6870\n",
            "Early stopping:  0.18901833174032306\n",
            "Epoch: 003, Loss: 2.1877, Train: 0.8375, Test: 0.7409\n",
            "Early stopping:  0.2967944570123139\n",
            "Epoch: 004, Loss: 1.8137, Train: 0.8562, Test: 0.7674\n",
            "Early stopping:  0.4175375583875483\n",
            "Epoch: 005, Loss: 1.4354, Train: 0.8688, Test: 0.7815\n",
            "Early stopping:  0.5371983709939464\n",
            "Epoch: 006, Loss: 1.0911, Train: 0.8875, Test: 0.7887\n",
            "Early stopping:  0.5687894734907757\n",
            "Epoch: 007, Loss: 0.8082, Train: 0.9062, Test: 0.7953\n",
            "Early stopping:  0.5513641816925728\n",
            "Epoch: 008, Loss: 0.5937, Train: 0.9187, Test: 0.7973\n",
            "Early stopping:  0.4877812165455424\n",
            "Epoch: 009, Loss: 0.4398, Train: 0.9375, Test: 0.7967\n",
            "Early stopping:  0.39804148478656576\n",
            "Epoch: 010, Loss: 0.3316, Train: 0.9563, Test: 0.7973\n",
            "Early stopping:  0.30345466943375043\n",
            "Epoch: 011, Loss: 0.2538, Train: 0.9625, Test: 0.7937\n",
            "Early stopping:  0.22094961915484007\n",
            "Epoch: 012, Loss: 0.1951, Train: 0.9812, Test: 0.7939\n",
            "Early stopping:  0.1582777773783613\n",
            "Epoch: 013, Loss: 0.1487, Train: 0.9875, Test: 0.7917\n",
            "Early stopping:  0.11524005663665297\n",
            "Epoch: 014, Loss: 0.1116, Train: 0.9875, Test: 0.7912\n",
            "Early stopping:  0.08709077982943284\n",
            "Epoch: 015, Loss: 0.0825, Train: 0.9938, Test: 0.7901\n",
            "Early stopping:  0.06799417465443201\n",
            "Epoch: 016, Loss: 0.0607, Train: 0.9938, Test: 0.7901\n",
            "Early stopping:  0.053518046913997364\n",
            "Epoch: 017, Loss: 0.0450, Train: 1.0000, Test: 0.7905\n",
            "Early stopping:  0.041389226610497515\n",
            "Epoch: 018, Loss: 0.0337, Train: 1.0000, Test: 0.7897\n",
            "Early stopping:  0.031081107212249905\n",
            "Epoch: 019, Loss: 0.0254, Train: 1.0000, Test: 0.7884\n",
            "Early stopping:  0.022739166327100886\n",
            "Epoch: 020, Loss: 0.0196, Train: 1.0000, Test: 0.7879\n",
            "Early stopping:  0.016382995896101284\n",
            "Epoch: 021, Loss: 0.0157, Train: 1.0000, Test: 0.7863\n",
            "Early stopping:  0.01172571487852386\n",
            "Epoch: 022, Loss: 0.0128, Train: 1.0000, Test: 0.7848\n",
            "Early stopping:  0.008314745002866317\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.85      0.83      0.84       392\n",
            "             ecology       0.83      0.77      0.80       879\n",
            "            economic       0.77      0.75      0.76      1399\n",
            "          geophysics       0.92      0.88      0.90      1192\n",
            "  gravitional_theory       0.69      0.95      0.80       120\n",
            "               hydro       0.60      0.89      0.72       345\n",
            "                math       0.93      0.66      0.77      1329\n",
            "              metals       0.69      0.86      0.76       191\n",
            "          networking       0.85      0.88      0.87       335\n",
            "        neuroscience       0.92      0.99      0.95       297\n",
            "        oceanography       0.83      0.84      0.83       980\n",
            "             politic       0.63      0.79      0.70       593\n",
            "           sociology       0.67      0.55      0.60       729\n",
            "software_engineering       0.83      0.84      0.83       514\n",
            "          statistics       0.62      0.90      0.73       637\n",
            "    theory_computing       0.81      0.67      0.74       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.78      0.82      0.79     10364\n",
            "        weighted avg       0.80      0.78      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7692, Train: 0.7812, Test: 0.6085\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4976, Train: 0.8500, Test: 0.6905\n",
            "Early stopping:  0.1921088758479299\n",
            "Epoch: 003, Loss: 2.1581, Train: 0.8875, Test: 0.7207\n",
            "Early stopping:  0.30616942224476373\n",
            "Epoch: 004, Loss: 1.7700, Train: 0.8938, Test: 0.7370\n",
            "Early stopping:  0.43211848137574965\n",
            "Epoch: 005, Loss: 1.3841, Train: 0.9125, Test: 0.7571\n",
            "Early stopping:  0.5543992269638286\n",
            "Epoch: 006, Loss: 1.0391, Train: 0.9250, Test: 0.7780\n",
            "Early stopping:  0.5837508949445689\n",
            "Epoch: 007, Loss: 0.7575, Train: 0.9313, Test: 0.7942\n",
            "Early stopping:  0.559611881136525\n",
            "Epoch: 008, Loss: 0.5457, Train: 0.9563, Test: 0.8031\n",
            "Early stopping:  0.4893792897310269\n",
            "Epoch: 009, Loss: 0.3956, Train: 0.9688, Test: 0.8033\n",
            "Early stopping:  0.3954152321027338\n",
            "Epoch: 010, Loss: 0.2914, Train: 0.9750, Test: 0.8030\n",
            "Early stopping:  0.29892413611533686\n",
            "Epoch: 011, Loss: 0.2170, Train: 0.9750, Test: 0.8016\n",
            "Early stopping:  0.21546972076094828\n",
            "Epoch: 012, Loss: 0.1609, Train: 0.9812, Test: 0.8001\n",
            "Early stopping:  0.15276131951363087\n",
            "Epoch: 013, Loss: 0.1178, Train: 0.9875, Test: 0.7998\n",
            "Early stopping:  0.11012632862498703\n",
            "Epoch: 014, Loss: 0.0855, Train: 1.0000, Test: 0.7971\n",
            "Early stopping:  0.08185770537105473\n",
            "Epoch: 015, Loss: 0.0623, Train: 1.0000, Test: 0.7965\n",
            "Early stopping:  0.06171723746178469\n",
            "Epoch: 016, Loss: 0.0456, Train: 1.0000, Test: 0.7951\n",
            "Early stopping:  0.046005780941918886\n",
            "Epoch: 017, Loss: 0.0333, Train: 1.0000, Test: 0.7944\n",
            "Early stopping:  0.0336213551605691\n",
            "Epoch: 018, Loss: 0.0244, Train: 1.0000, Test: 0.7947\n",
            "Early stopping:  0.024315178711550256\n",
            "Epoch: 019, Loss: 0.0184, Train: 1.0000, Test: 0.7929\n",
            "Early stopping:  0.01754339645899572\n",
            "Epoch: 020, Loss: 0.0145, Train: 1.0000, Test: 0.7898\n",
            "Early stopping:  0.012451809603583549\n",
            "Epoch: 021, Loss: 0.0119, Train: 1.0000, Test: 0.7875\n",
            "Early stopping:  0.008543798812617227\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.71      0.76       392\n",
            "             ecology       0.74      0.86      0.80       879\n",
            "            economic       0.76      0.69      0.72      1399\n",
            "          geophysics       0.94      0.91      0.92      1192\n",
            "  gravitional_theory       0.88      0.92      0.90       120\n",
            "               hydro       0.76      0.70      0.73       345\n",
            "                math       0.88      0.77      0.82      1329\n",
            "              metals       0.76      0.87      0.81       191\n",
            "          networking       0.75      0.94      0.83       335\n",
            "        neuroscience       0.92      0.97      0.94       297\n",
            "        oceanography       0.87      0.81      0.84       980\n",
            "             politic       0.73      0.77      0.75       593\n",
            "           sociology       0.59      0.65      0.62       729\n",
            "software_engineering       0.85      0.80      0.83       514\n",
            "          statistics       0.67      0.76      0.72       637\n",
            "    theory_computing       0.66      0.75      0.70       432\n",
            "\n",
            "            accuracy                           0.79     10364\n",
            "           macro avg       0.79      0.80      0.79     10364\n",
            "        weighted avg       0.79      0.79      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7835, Train: 0.7875, Test: 0.5534\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5080, Train: 0.9250, Test: 0.7469\n",
            "Early stopping:  0.19480796002656756\n",
            "Epoch: 003, Loss: 2.1722, Train: 0.9375, Test: 0.7689\n",
            "Early stopping:  0.30613912323071785\n",
            "Epoch: 004, Loss: 1.7863, Train: 0.9375, Test: 0.7785\n",
            "Early stopping:  0.43073743345843335\n",
            "Epoch: 005, Loss: 1.3971, Train: 0.9500, Test: 0.7870\n",
            "Early stopping:  0.5538411286988754\n",
            "Epoch: 006, Loss: 1.0427, Train: 0.9563, Test: 0.7912\n",
            "Early stopping:  0.5860898086439531\n",
            "Epoch: 007, Loss: 0.7473, Train: 0.9500, Test: 0.7952\n",
            "Early stopping:  0.5689851294253027\n",
            "Epoch: 008, Loss: 0.5218, Train: 0.9500, Test: 0.7958\n",
            "Early stopping:  0.5052772496588729\n",
            "Epoch: 009, Loss: 0.3622, Train: 0.9625, Test: 0.7908\n",
            "Early stopping:  0.4142016506344119\n",
            "Epoch: 010, Loss: 0.2530, Train: 0.9750, Test: 0.7861\n",
            "Early stopping:  0.31609912718013194\n",
            "Epoch: 011, Loss: 0.1785, Train: 0.9875, Test: 0.7794\n",
            "Early stopping:  0.22733178450376512\n",
            "Epoch: 012, Loss: 0.1268, Train: 0.9938, Test: 0.7771\n",
            "Early stopping:  0.15761659499566008\n",
            "Epoch: 013, Loss: 0.0896, Train: 1.0000, Test: 0.7768\n",
            "Early stopping:  0.10850362456598144\n",
            "Epoch: 014, Loss: 0.0626, Train: 1.0000, Test: 0.7792\n",
            "Early stopping:  0.07572456099361667\n",
            "Epoch: 015, Loss: 0.0436, Train: 1.0000, Test: 0.7795\n",
            "Early stopping:  0.05379160366605322\n",
            "Epoch: 016, Loss: 0.0307, Train: 1.0000, Test: 0.7825\n",
            "Early stopping:  0.038416859490783266\n",
            "Epoch: 017, Loss: 0.0220, Train: 1.0000, Test: 0.7859\n",
            "Early stopping:  0.027039508636631637\n",
            "Epoch: 018, Loss: 0.0162, Train: 1.0000, Test: 0.7880\n",
            "Early stopping:  0.018564118866756286\n",
            "Epoch: 019, Loss: 0.0121, Train: 1.0000, Test: 0.7886\n",
            "Early stopping:  0.012555538524340444\n",
            "Epoch: 020, Loss: 0.0094, Train: 1.0000, Test: 0.7891\n",
            "Early stopping:  0.008508682181365242\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.81      0.77       392\n",
            "             ecology       0.85      0.76      0.80       879\n",
            "            economic       0.75      0.72      0.73      1399\n",
            "          geophysics       0.94      0.90      0.92      1192\n",
            "  gravitional_theory       0.94      0.88      0.91       120\n",
            "               hydro       0.60      0.91      0.72       345\n",
            "                math       0.94      0.66      0.77      1329\n",
            "              metals       0.74      0.89      0.81       191\n",
            "          networking       0.90      0.83      0.86       335\n",
            "        neuroscience       0.90      0.97      0.94       297\n",
            "        oceanography       0.82      0.88      0.85       980\n",
            "             politic       0.73      0.79      0.76       593\n",
            "           sociology       0.62      0.66      0.64       729\n",
            "software_engineering       0.81      0.88      0.85       514\n",
            "          statistics       0.63      0.81      0.71       637\n",
            "    theory_computing       0.79      0.69      0.73       432\n",
            "\n",
            "            accuracy                           0.79     10364\n",
            "           macro avg       0.79      0.82      0.80     10364\n",
            "        weighted avg       0.80      0.79      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7811, Train: 0.7125, Test: 0.5425\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5040, Train: 0.8562, Test: 0.6854\n",
            "Early stopping:  0.19588658217365837\n",
            "Epoch: 003, Loss: 2.1647, Train: 0.8750, Test: 0.7281\n",
            "Early stopping:  0.3086851265027734\n",
            "Epoch: 004, Loss: 1.7731, Train: 0.9187, Test: 0.7449\n",
            "Early stopping:  0.4354482367174636\n",
            "Epoch: 005, Loss: 1.3776, Train: 0.9250, Test: 0.7579\n",
            "Early stopping:  0.5608014940668645\n",
            "Epoch: 006, Loss: 1.0226, Train: 0.9437, Test: 0.7741\n",
            "Early stopping:  0.5931257052171659\n",
            "Epoch: 007, Loss: 0.7352, Train: 0.9375, Test: 0.7867\n",
            "Early stopping:  0.5718055836558513\n",
            "Epoch: 008, Loss: 0.5194, Train: 0.9500, Test: 0.7917\n",
            "Early stopping:  0.5013111915082537\n",
            "Epoch: 009, Loss: 0.3673, Train: 0.9688, Test: 0.7965\n",
            "Early stopping:  0.4041270464675578\n",
            "Epoch: 010, Loss: 0.2641, Train: 0.9688, Test: 0.7946\n",
            "Early stopping:  0.3035881951636271\n",
            "Epoch: 011, Loss: 0.1922, Train: 0.9875, Test: 0.7925\n",
            "Early stopping:  0.21684197153326318\n",
            "Epoch: 012, Loss: 0.1391, Train: 1.0000, Test: 0.7926\n",
            "Early stopping:  0.15113877494893388\n",
            "Epoch: 013, Loss: 0.0997, Train: 1.0000, Test: 0.7922\n",
            "Early stopping:  0.10621753516945334\n",
            "Epoch: 014, Loss: 0.0713, Train: 1.0000, Test: 0.7916\n",
            "Early stopping:  0.07679926752438146\n",
            "Epoch: 015, Loss: 0.0511, Train: 1.0000, Test: 0.7918\n",
            "Early stopping:  0.05630638250595565\n",
            "Epoch: 016, Loss: 0.0369, Train: 1.0000, Test: 0.7914\n",
            "Early stopping:  0.040797310466100196\n",
            "Epoch: 017, Loss: 0.0270, Train: 1.0000, Test: 0.7912\n",
            "Early stopping:  0.029039728628577982\n",
            "Epoch: 018, Loss: 0.0198, Train: 1.0000, Test: 0.7903\n",
            "Early stopping:  0.02050434741624225\n",
            "Epoch: 019, Loss: 0.0147, Train: 1.0000, Test: 0.7888\n",
            "Early stopping:  0.014458847243149745\n",
            "Epoch: 020, Loss: 0.0111, Train: 1.0000, Test: 0.7895\n",
            "Early stopping:  0.010263001878436985\n",
            "Epoch: 021, Loss: 0.0087, Train: 1.0000, Test: 0.7895\n",
            "Early stopping:  0.007302081471875857\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.76      0.73       392\n",
            "             ecology       0.86      0.79      0.82       879\n",
            "            economic       0.81      0.65      0.72      1399\n",
            "          geophysics       0.92      0.88      0.90      1192\n",
            "  gravitional_theory       0.75      0.93      0.83       120\n",
            "               hydro       0.69      0.84      0.76       345\n",
            "                math       0.91      0.77      0.84      1329\n",
            "              metals       0.58      0.92      0.71       191\n",
            "          networking       0.83      0.82      0.83       335\n",
            "        neuroscience       0.93      0.97      0.95       297\n",
            "        oceanography       0.84      0.90      0.87       980\n",
            "             politic       0.71      0.78      0.74       593\n",
            "           sociology       0.55      0.69      0.61       729\n",
            "software_engineering       0.88      0.81      0.85       514\n",
            "          statistics       0.74      0.71      0.72       637\n",
            "    theory_computing       0.67      0.80      0.73       432\n",
            "\n",
            "            accuracy                           0.79     10364\n",
            "           macro avg       0.77      0.81      0.79     10364\n",
            "        weighted avg       0.80      0.79      0.79     10364\n",
            "\n",
            "time: 9.67 s (started: 2024-08-16 14:13:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkXdJHSLKnn2",
        "outputId": "98d79398-3e14-430b-c389-d1046167e22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 408 ms (started: 2024-08-16 14:14:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 20 ❎"
      ],
      "metadata": {
        "id": "RZ99EfkAKnn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "RBk7KEnhKnn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSevKeBMKnn2",
        "outputId": "9fb5c435-1245-4d9d-af0a-9873903bd8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4796, Train: 0.7094, Test: 0.6378\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9156, Train: 0.8531, Test: 0.7708\n",
            "Early stopping:  1.8130280321137329\n",
            "Epoch: 003, Loss: 0.4565, Train: 0.9031, Test: 0.7839\n",
            "Early stopping:  1.6291413628603246\n",
            "Epoch: 004, Loss: 0.3436, Train: 0.9375, Test: 0.8040\n",
            "Early stopping:  1.4747703580101723\n",
            "Epoch: 005, Loss: 0.2236, Train: 0.9281, Test: 0.8047\n",
            "Early stopping:  1.3647168531138527\n",
            "Epoch: 006, Loss: 0.1638, Train: 0.9656, Test: 0.8004\n",
            "Early stopping:  0.298754572435327\n",
            "Epoch: 007, Loss: 0.1207, Train: 0.9812, Test: 0.8002\n",
            "Early stopping:  0.13744078354530176\n",
            "Epoch: 008, Loss: 0.0680, Train: 0.9875, Test: 0.7974\n",
            "Early stopping:  0.10597077899919993\n",
            "Epoch: 009, Loss: 0.0494, Train: 0.9875, Test: 0.7893\n",
            "Early stopping:  0.071082891359334\n",
            "Epoch: 010, Loss: 0.0410, Train: 0.9938, Test: 0.7864\n",
            "Early stopping:  0.05223887179708905\n",
            "Epoch: 011, Loss: 0.0231, Train: 0.9969, Test: 0.7849\n",
            "Early stopping:  0.037326928109152814\n",
            "Epoch: 012, Loss: 0.0165, Train: 0.9969, Test: 0.7822\n",
            "Early stopping:  0.020667642996083035\n",
            "Epoch: 013, Loss: 0.0128, Train: 1.0000, Test: 0.7793\n",
            "Early stopping:  0.015909341504259166\n",
            "Epoch: 014, Loss: 0.0110, Train: 1.0000, Test: 0.7789\n",
            "Early stopping:  0.012177235753952253\n",
            "Epoch: 015, Loss: 0.0099, Train: 1.0000, Test: 0.7790\n",
            "Early stopping:  0.005341706289570225\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.56      0.87      0.68       382\n",
            "             ecology       0.82      0.87      0.84       869\n",
            "            economic       0.83      0.67      0.74      1389\n",
            "          geophysics       0.94      0.91      0.92      1182\n",
            "  gravitional_theory       0.50      0.95      0.66       110\n",
            "               hydro       0.72      0.80      0.76       335\n",
            "                math       0.92      0.72      0.81      1319\n",
            "              metals       0.61      0.92      0.74       181\n",
            "          networking       0.59      0.77      0.67       325\n",
            "        neuroscience       0.93      0.97      0.95       287\n",
            "        oceanography       0.90      0.76      0.82       970\n",
            "             politic       0.73      0.74      0.73       583\n",
            "           sociology       0.63      0.66      0.64       719\n",
            "software_engineering       0.82      0.71      0.76       504\n",
            "          statistics       0.65      0.86      0.74       627\n",
            "    theory_computing       0.80      0.70      0.74       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.75      0.81      0.76     10204\n",
            "        weighted avg       0.80      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3147, Train: 0.6438, Test: 0.6307\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2211, Train: 0.8281, Test: 0.7187\n",
            "Early stopping:  1.4803934711355682\n",
            "Epoch: 003, Loss: 0.5542, Train: 0.8938, Test: 0.7725\n",
            "Early stopping:  1.4403909955839984\n",
            "Epoch: 004, Loss: 0.3544, Train: 0.9156, Test: 0.7735\n",
            "Early stopping:  1.3540995878509012\n",
            "Epoch: 005, Loss: 0.2680, Train: 0.9406, Test: 0.7681\n",
            "Early stopping:  1.2704932747441529\n",
            "Epoch: 006, Loss: 0.2053, Train: 0.9406, Test: 0.7699\n",
            "Early stopping:  0.41311245661262785\n",
            "Epoch: 007, Loss: 0.1512, Train: 0.9656, Test: 0.7846\n",
            "Early stopping:  0.15769247320713162\n",
            "Epoch: 008, Loss: 0.1028, Train: 0.9812, Test: 0.7928\n",
            "Early stopping:  0.09871192596932173\n",
            "Epoch: 009, Loss: 0.0728, Train: 0.9875, Test: 0.7948\n",
            "Early stopping:  0.07854368316298313\n",
            "Epoch: 010, Loss: 0.0477, Train: 0.9938, Test: 0.7902\n",
            "Early stopping:  0.06309149019895281\n",
            "Epoch: 011, Loss: 0.0310, Train: 0.9969, Test: 0.7845\n",
            "Early stopping:  0.047639387405832186\n",
            "Epoch: 012, Loss: 0.0262, Train: 0.9938, Test: 0.7808\n",
            "Early stopping:  0.03184256097624625\n",
            "Epoch: 013, Loss: 0.0225, Train: 1.0000, Test: 0.7778\n",
            "Early stopping:  0.02069796667085554\n",
            "Epoch: 014, Loss: 0.0156, Train: 1.0000, Test: 0.7789\n",
            "Early stopping:  0.012069149007234192\n",
            "Epoch: 015, Loss: 0.0099, Train: 1.0000, Test: 0.7816\n",
            "Early stopping:  0.008376839708908453\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.77      0.77       382\n",
            "             ecology       0.84      0.74      0.79       869\n",
            "            economic       0.76      0.78      0.77      1389\n",
            "          geophysics       0.90      0.91      0.90      1182\n",
            "  gravitional_theory       0.46      0.95      0.62       110\n",
            "               hydro       0.72      0.83      0.77       335\n",
            "                math       0.90      0.63      0.74      1319\n",
            "              metals       0.58      0.90      0.70       181\n",
            "          networking       0.78      0.94      0.85       325\n",
            "        neuroscience       0.86      1.00      0.92       287\n",
            "        oceanography       0.81      0.88      0.84       970\n",
            "             politic       0.75      0.68      0.71       583\n",
            "           sociology       0.66      0.60      0.63       719\n",
            "software_engineering       0.78      0.88      0.83       504\n",
            "          statistics       0.72      0.83      0.77       627\n",
            "    theory_computing       0.72      0.66      0.69       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.75      0.81      0.77     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5134, Train: 0.6562, Test: 0.6052\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1728, Train: 0.8562, Test: 0.7686\n",
            "Early stopping:  1.6549929445318292\n",
            "Epoch: 003, Loss: 0.5970, Train: 0.8344, Test: 0.7280\n",
            "Early stopping:  1.5446144891475146\n",
            "Epoch: 004, Loss: 0.5485, Train: 0.8812, Test: 0.7771\n",
            "Early stopping:  1.3993358265788967\n",
            "Epoch: 005, Loss: 0.3534, Train: 0.9219, Test: 0.8010\n",
            "Early stopping:  1.308656999901432\n",
            "Epoch: 006, Loss: 0.2202, Train: 0.9344, Test: 0.7956\n",
            "Early stopping:  0.3652123920293161\n",
            "Epoch: 007, Loss: 0.1737, Train: 0.9219, Test: 0.7794\n",
            "Early stopping:  0.18990779982824696\n",
            "Epoch: 008, Loss: 0.2103, Train: 0.9625, Test: 0.7901\n",
            "Early stopping:  0.15408541145079196\n",
            "Epoch: 009, Loss: 0.0993, Train: 0.9688, Test: 0.7917\n",
            "Early stopping:  0.09252732524533464\n",
            "Epoch: 010, Loss: 0.0706, Train: 0.9781, Test: 0.7918\n",
            "Early stopping:  0.06687133853609432\n",
            "Epoch: 011, Loss: 0.0536, Train: 0.9812, Test: 0.7840\n",
            "Early stopping:  0.06762612025024842\n",
            "Epoch: 012, Loss: 0.0485, Train: 0.9875, Test: 0.7811\n",
            "Early stopping:  0.06663725333216261\n",
            "Epoch: 013, Loss: 0.0329, Train: 0.9906, Test: 0.7737\n",
            "Early stopping:  0.02528560159897779\n",
            "Epoch: 014, Loss: 0.0299, Train: 0.9969, Test: 0.7695\n",
            "Early stopping:  0.01652877638762459\n",
            "Epoch: 015, Loss: 0.0285, Train: 0.9969, Test: 0.7709\n",
            "Early stopping:  0.011553812512974455\n",
            "Epoch: 016, Loss: 0.0206, Train: 0.9969, Test: 0.7733\n",
            "Early stopping:  0.010233372588242593\n",
            "Epoch: 017, Loss: 0.0130, Train: 1.0000, Test: 0.7774\n",
            "Early stopping:  0.00809349120374732\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.80      0.75       382\n",
            "             ecology       0.76      0.88      0.81       869\n",
            "            economic       0.77      0.75      0.76      1389\n",
            "          geophysics       0.89      0.91      0.90      1182\n",
            "  gravitional_theory       0.83      0.92      0.87       110\n",
            "               hydro       0.69      0.75      0.72       335\n",
            "                math       0.94      0.61      0.74      1319\n",
            "              metals       0.57      0.92      0.70       181\n",
            "          networking       0.79      0.90      0.84       325\n",
            "        neuroscience       0.94      0.95      0.95       287\n",
            "        oceanography       0.85      0.82      0.83       970\n",
            "             politic       0.72      0.78      0.74       583\n",
            "           sociology       0.66      0.63      0.65       719\n",
            "software_engineering       0.84      0.71      0.77       504\n",
            "          statistics       0.61      0.79      0.69       627\n",
            "    theory_computing       0.73      0.70      0.71       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.77      0.80      0.78     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1393, Train: 0.7406, Test: 0.7092\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8934, Train: 0.8500, Test: 0.7781\n",
            "Early stopping:  1.5880428864065166\n",
            "Epoch: 003, Loss: 0.4552, Train: 0.8406, Test: 0.7656\n",
            "Early stopping:  1.4399036997935613\n",
            "Epoch: 004, Loss: 0.4052, Train: 0.9187, Test: 0.7806\n",
            "Early stopping:  1.296006049100701\n",
            "Epoch: 005, Loss: 0.2403, Train: 0.9250, Test: 0.7681\n",
            "Early stopping:  1.2053893404753167\n",
            "Epoch: 006, Loss: 0.1767, Train: 0.9531, Test: 0.7651\n",
            "Early stopping:  0.2811061460260686\n",
            "Epoch: 007, Loss: 0.1273, Train: 0.9656, Test: 0.7403\n",
            "Early stopping:  0.14314212852848626\n",
            "Epoch: 008, Loss: 0.0890, Train: 0.9906, Test: 0.7286\n",
            "Early stopping:  0.12410564681110096\n",
            "Epoch: 009, Loss: 0.0540, Train: 0.9938, Test: 0.7209\n",
            "Early stopping:  0.07338240302641648\n",
            "Epoch: 010, Loss: 0.0371, Train: 0.9906, Test: 0.7214\n",
            "Early stopping:  0.05649694086661816\n",
            "Epoch: 011, Loss: 0.0309, Train: 0.9938, Test: 0.7305\n",
            "Early stopping:  0.040252835075781715\n",
            "Epoch: 012, Loss: 0.0204, Train: 0.9969, Test: 0.7387\n",
            "Early stopping:  0.026806528558219515\n",
            "Epoch: 013, Loss: 0.0142, Train: 1.0000, Test: 0.7437\n",
            "Early stopping:  0.015480120736194631\n",
            "Epoch: 014, Loss: 0.0101, Train: 1.0000, Test: 0.7476\n",
            "Early stopping:  0.011304780280694088\n",
            "Epoch: 015, Loss: 0.0069, Train: 1.0000, Test: 0.7505\n",
            "Early stopping:  0.009523186764643764\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.55      0.88      0.68       382\n",
            "             ecology       0.82      0.76      0.79       869\n",
            "            economic       0.82      0.64      0.72      1389\n",
            "          geophysics       0.95      0.83      0.89      1182\n",
            "  gravitional_theory       0.34      0.98      0.51       110\n",
            "               hydro       0.61      0.86      0.71       335\n",
            "                math       0.86      0.68      0.76      1319\n",
            "              metals       0.45      0.93      0.60       181\n",
            "          networking       0.88      0.68      0.77       325\n",
            "        neuroscience       0.80      0.98      0.88       287\n",
            "        oceanography       0.83      0.87      0.85       970\n",
            "             politic       0.63      0.69      0.66       583\n",
            "           sociology       0.63      0.63      0.63       719\n",
            "software_engineering       0.92      0.62      0.75       504\n",
            "          statistics       0.76      0.80      0.78       627\n",
            "    theory_computing       0.66      0.75      0.71       422\n",
            "\n",
            "            accuracy                           0.75     10204\n",
            "           macro avg       0.72      0.79      0.73     10204\n",
            "        weighted avg       0.78      0.75      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6131, Train: 0.6531, Test: 0.5243\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2618, Train: 0.8375, Test: 0.7445\n",
            "Early stopping:  1.6626355168660067\n",
            "Epoch: 003, Loss: 0.6692, Train: 0.8594, Test: 0.7304\n",
            "Early stopping:  1.5570446877871813\n",
            "Epoch: 004, Loss: 0.5611, Train: 0.9031, Test: 0.7676\n",
            "Early stopping:  1.4248956278256324\n",
            "Epoch: 005, Loss: 0.3844, Train: 0.9187, Test: 0.7801\n",
            "Early stopping:  1.3354948320114477\n",
            "Epoch: 006, Loss: 0.2759, Train: 0.9344, Test: 0.7792\n",
            "Early stopping:  0.3844329994838698\n",
            "Epoch: 007, Loss: 0.1799, Train: 0.9625, Test: 0.7682\n",
            "Early stopping:  0.200959908562574\n",
            "Epoch: 008, Loss: 0.1309, Train: 0.9531, Test: 0.7614\n",
            "Early stopping:  0.17226635519097402\n",
            "Epoch: 009, Loss: 0.1159, Train: 0.9656, Test: 0.7596\n",
            "Early stopping:  0.1123841859361854\n",
            "Epoch: 010, Loss: 0.0963, Train: 0.9750, Test: 0.7657\n",
            "Early stopping:  0.0719230116165709\n",
            "Epoch: 011, Loss: 0.0628, Train: 0.9969, Test: 0.7716\n",
            "Early stopping:  0.043340841163744794\n",
            "Epoch: 012, Loss: 0.0348, Train: 1.0000, Test: 0.7742\n",
            "Early stopping:  0.039211352871223\n",
            "Epoch: 013, Loss: 0.0264, Train: 0.9938, Test: 0.7754\n",
            "Early stopping:  0.0385726164578017\n",
            "Epoch: 014, Loss: 0.0262, Train: 0.9938, Test: 0.7753\n",
            "Early stopping:  0.030248578232465143\n",
            "Epoch: 015, Loss: 0.0197, Train: 0.9969, Test: 0.7750\n",
            "Early stopping:  0.01696517373483911\n",
            "Epoch: 016, Loss: 0.0128, Train: 1.0000, Test: 0.7745\n",
            "Early stopping:  0.008218900794841388\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.66      0.83      0.74       382\n",
            "             ecology       0.77      0.85      0.81       869\n",
            "            economic       0.79      0.66      0.72      1389\n",
            "          geophysics       0.95      0.85      0.89      1182\n",
            "  gravitional_theory       0.71      0.94      0.80       110\n",
            "               hydro       0.72      0.77      0.75       335\n",
            "                math       0.90      0.81      0.85      1319\n",
            "              metals       0.46      0.95      0.62       181\n",
            "          networking       0.72      0.77      0.74       325\n",
            "        neuroscience       0.93      0.97      0.95       287\n",
            "        oceanography       0.87      0.75      0.81       970\n",
            "             politic       0.70      0.77      0.73       583\n",
            "           sociology       0.59      0.54      0.56       719\n",
            "software_engineering       0.84      0.86      0.85       504\n",
            "          statistics       0.77      0.78      0.78       627\n",
            "    theory_computing       0.59      0.72      0.64       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.76     10204\n",
            "        weighted avg       0.79      0.77      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5969, Train: 0.7250, Test: 0.6542\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1797, Train: 0.8219, Test: 0.7527\n",
            "Early stopping:  1.7092113248058194\n",
            "Epoch: 003, Loss: 0.6050, Train: 0.8656, Test: 0.7723\n",
            "Early stopping:  1.5876896788791466\n",
            "Epoch: 004, Loss: 0.4249, Train: 0.9125, Test: 0.7857\n",
            "Early stopping:  1.4659535676305517\n",
            "Epoch: 005, Loss: 0.2911, Train: 0.9219, Test: 0.7740\n",
            "Early stopping:  1.3715402027613977\n",
            "Epoch: 006, Loss: 0.2465, Train: 0.9469, Test: 0.7747\n",
            "Early stopping:  0.3789259383685155\n",
            "Epoch: 007, Loss: 0.1839, Train: 0.9594, Test: 0.7792\n",
            "Early stopping:  0.16764088056463408\n",
            "Epoch: 008, Loss: 0.1289, Train: 0.9750, Test: 0.7774\n",
            "Early stopping:  0.11315798532273734\n",
            "Epoch: 009, Loss: 0.0948, Train: 0.9656, Test: 0.7641\n",
            "Early stopping:  0.08100355714010445\n",
            "Epoch: 010, Loss: 0.0847, Train: 0.9875, Test: 0.7618\n",
            "Early stopping:  0.0674386352925293\n",
            "Epoch: 011, Loss: 0.0574, Train: 0.9906, Test: 0.7549\n",
            "Early stopping:  0.04860618921852167\n",
            "Epoch: 012, Loss: 0.0559, Train: 0.9969, Test: 0.7583\n",
            "Early stopping:  0.03009655971811328\n",
            "Epoch: 013, Loss: 0.0477, Train: 0.9969, Test: 0.7630\n",
            "Early stopping:  0.0204283303736229\n",
            "Epoch: 014, Loss: 0.0428, Train: 0.9875, Test: 0.7681\n",
            "Early stopping:  0.01623015915581739\n",
            "Epoch: 015, Loss: 0.0334, Train: 0.9906, Test: 0.7715\n",
            "Early stopping:  0.009878981716398274\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.84      0.75      0.79       382\n",
            "             ecology       0.86      0.75      0.80       869\n",
            "            economic       0.77      0.62      0.68      1389\n",
            "          geophysics       0.95      0.91      0.93      1182\n",
            "  gravitional_theory       0.59      0.94      0.72       110\n",
            "               hydro       0.63      0.83      0.71       335\n",
            "                math       0.82      0.68      0.74      1319\n",
            "              metals       0.59      0.92      0.72       181\n",
            "          networking       0.71      0.94      0.81       325\n",
            "        neuroscience       0.90      0.96      0.93       287\n",
            "        oceanography       0.82      0.86      0.84       970\n",
            "             politic       0.62      0.81      0.70       583\n",
            "           sociology       0.54      0.69      0.60       719\n",
            "software_engineering       0.91      0.81      0.86       504\n",
            "          statistics       0.84      0.72      0.78       627\n",
            "    theory_computing       0.70      0.74      0.72       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.76      0.81      0.77     10204\n",
            "        weighted avg       0.79      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4673, Train: 0.6844, Test: 0.5803\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0590, Train: 0.8187, Test: 0.7803\n",
            "Early stopping:  1.702919221791628\n",
            "Epoch: 003, Loss: 0.6485, Train: 0.8750, Test: 0.7868\n",
            "Early stopping:  1.5228437938347505\n",
            "Epoch: 004, Loss: 0.3960, Train: 0.9094, Test: 0.7703\n",
            "Early stopping:  1.409808678853003\n",
            "Epoch: 005, Loss: 0.2770, Train: 0.9156, Test: 0.7552\n",
            "Early stopping:  1.3189578232960497\n",
            "Epoch: 006, Loss: 0.2400, Train: 0.9437, Test: 0.7688\n",
            "Early stopping:  0.3390544022238021\n",
            "Epoch: 007, Loss: 0.1455, Train: 0.9594, Test: 0.7799\n",
            "Early stopping:  0.1937022784550627\n",
            "Epoch: 008, Loss: 0.1092, Train: 0.9719, Test: 0.7879\n",
            "Early stopping:  0.11351140736461911\n",
            "Epoch: 009, Loss: 0.0732, Train: 0.9875, Test: 0.7892\n",
            "Early stopping:  0.08661419877671206\n",
            "Epoch: 010, Loss: 0.0431, Train: 0.9906, Test: 0.7784\n",
            "Early stopping:  0.07624090439559708\n",
            "Epoch: 011, Loss: 0.0603, Train: 0.9969, Test: 0.7779\n",
            "Early stopping:  0.0410648231766254\n",
            "Epoch: 012, Loss: 0.0277, Train: 0.9906, Test: 0.7718\n",
            "Early stopping:  0.031156172017568873\n",
            "Epoch: 013, Loss: 0.0355, Train: 0.9938, Test: 0.7701\n",
            "Early stopping:  0.018564301823449474\n",
            "Epoch: 014, Loss: 0.0341, Train: 0.9969, Test: 0.7706\n",
            "Early stopping:  0.01253222267042209\n",
            "Epoch: 015, Loss: 0.0176, Train: 1.0000, Test: 0.7684\n",
            "Early stopping:  0.01579238447438892\n",
            "Epoch: 016, Loss: 0.0117, Train: 0.9969, Test: 0.7658\n",
            "Early stopping:  0.010393088075295905\n",
            "Epoch: 017, Loss: 0.0137, Train: 0.9969, Test: 0.7645\n",
            "Early stopping:  0.011419292951455101\n",
            "Epoch: 018, Loss: 0.0131, Train: 0.9969, Test: 0.7654\n",
            "Early stopping:  0.00924799892663256\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.78      0.73       382\n",
            "             ecology       0.88      0.80      0.84       869\n",
            "            economic       0.80      0.60      0.69      1389\n",
            "          geophysics       0.88      0.88      0.88      1182\n",
            "  gravitional_theory       0.74      0.94      0.82       110\n",
            "               hydro       0.62      0.86      0.72       335\n",
            "                math       0.90      0.69      0.78      1319\n",
            "              metals       0.58      0.93      0.72       181\n",
            "          networking       0.81      0.89      0.85       325\n",
            "        neuroscience       0.99      0.87      0.93       287\n",
            "        oceanography       0.86      0.87      0.87       970\n",
            "             politic       0.54      0.71      0.62       583\n",
            "           sociology       0.61      0.65      0.63       719\n",
            "software_engineering       0.68      0.93      0.79       504\n",
            "          statistics       0.70      0.81      0.75       627\n",
            "    theory_computing       0.75      0.55      0.63       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.76     10204\n",
            "        weighted avg       0.78      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.8292, Train: 0.6813, Test: 0.5544\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2823, Train: 0.8187, Test: 0.6771\n",
            "Early stopping:  1.80095017832336\n",
            "Epoch: 003, Loss: 0.6705, Train: 0.9062, Test: 0.7536\n",
            "Early stopping:  1.675235261641929\n",
            "Epoch: 004, Loss: 0.3437, Train: 0.9062, Test: 0.7622\n",
            "Early stopping:  1.580486843004093\n",
            "Epoch: 005, Loss: 0.3009, Train: 0.9250, Test: 0.7829\n",
            "Early stopping:  1.4752334388746549\n",
            "Epoch: 006, Loss: 0.2181, Train: 0.9469, Test: 0.7757\n",
            "Early stopping:  0.4372356512724647\n",
            "Epoch: 007, Loss: 0.1320, Train: 0.9844, Test: 0.7589\n",
            "Early stopping:  0.20535698399298807\n",
            "Epoch: 008, Loss: 0.0918, Train: 0.9750, Test: 0.7553\n",
            "Early stopping:  0.10724077007224722\n",
            "Epoch: 009, Loss: 0.0718, Train: 0.9750, Test: 0.7485\n",
            "Early stopping:  0.0954131721283965\n",
            "Epoch: 010, Loss: 0.0646, Train: 0.9969, Test: 0.7521\n",
            "Early stopping:  0.06299936246888001\n",
            "Epoch: 011, Loss: 0.0355, Train: 0.9938, Test: 0.7666\n",
            "Early stopping:  0.035787729767734304\n",
            "Epoch: 012, Loss: 0.0224, Train: 0.9906, Test: 0.7715\n",
            "Early stopping:  0.028036479742660592\n",
            "Epoch: 013, Loss: 0.0222, Train: 0.9969, Test: 0.7718\n",
            "Early stopping:  0.023483734058854615\n",
            "Epoch: 014, Loss: 0.0171, Train: 0.9969, Test: 0.7692\n",
            "Early stopping:  0.019258836199360158\n",
            "Epoch: 015, Loss: 0.0139, Train: 1.0000, Test: 0.7666\n",
            "Early stopping:  0.008225589129562297\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.73      0.73       382\n",
            "             ecology       0.81      0.86      0.83       869\n",
            "            economic       0.76      0.63      0.69      1389\n",
            "          geophysics       0.97      0.86      0.91      1182\n",
            "  gravitional_theory       0.43      0.92      0.59       110\n",
            "               hydro       0.75      0.79      0.77       335\n",
            "                math       0.81      0.72      0.76      1319\n",
            "              metals       0.56      0.93      0.70       181\n",
            "          networking       0.88      0.83      0.86       325\n",
            "        neuroscience       0.89      0.98      0.94       287\n",
            "        oceanography       0.86      0.83      0.84       970\n",
            "             politic       0.75      0.67      0.71       583\n",
            "           sociology       0.49      0.68      0.57       719\n",
            "software_engineering       0.89      0.80      0.84       504\n",
            "          statistics       0.78      0.71      0.75       627\n",
            "    theory_computing       0.64      0.81      0.72       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.76     10204\n",
            "        weighted avg       0.79      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3733, Train: 0.7156, Test: 0.6402\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9173, Train: 0.7969, Test: 0.7046\n",
            "Early stopping:  1.736654739451506\n",
            "Epoch: 003, Loss: 0.7760, Train: 0.8906, Test: 0.7861\n",
            "Early stopping:  1.460475131415844\n",
            "Epoch: 004, Loss: 0.3998, Train: 0.9031, Test: 0.7703\n",
            "Early stopping:  1.355498883607427\n",
            "Epoch: 005, Loss: 0.3603, Train: 0.9125, Test: 0.7544\n",
            "Early stopping:  1.2572007830610914\n",
            "Epoch: 006, Loss: 0.3140, Train: 0.9375, Test: 0.7504\n",
            "Early stopping:  0.27392799223881015\n",
            "Epoch: 007, Loss: 0.2323, Train: 0.9531, Test: 0.7523\n",
            "Early stopping:  0.21041608092112066\n",
            "Epoch: 008, Loss: 0.1764, Train: 0.9812, Test: 0.7637\n",
            "Early stopping:  0.0916558532229423\n",
            "Epoch: 009, Loss: 0.1059, Train: 0.9719, Test: 0.7658\n",
            "Early stopping:  0.10248802717570343\n",
            "Epoch: 010, Loss: 0.0925, Train: 0.9812, Test: 0.7691\n",
            "Early stopping:  0.09185761366959967\n",
            "Epoch: 011, Loss: 0.0775, Train: 0.9812, Test: 0.7723\n",
            "Early stopping:  0.06543707724424314\n",
            "Epoch: 012, Loss: 0.0630, Train: 0.9844, Test: 0.7731\n",
            "Early stopping:  0.04405072949762401\n",
            "Epoch: 013, Loss: 0.0396, Train: 0.9906, Test: 0.7733\n",
            "Early stopping:  0.025803339258151592\n",
            "Epoch: 014, Loss: 0.0268, Train: 0.9938, Test: 0.7692\n",
            "Early stopping:  0.026870336817935124\n",
            "Epoch: 015, Loss: 0.0217, Train: 1.0000, Test: 0.7636\n",
            "Early stopping:  0.0238528770899096\n",
            "Epoch: 016, Loss: 0.0154, Train: 0.9906, Test: 0.7547\n",
            "Early stopping:  0.018811680329918208\n",
            "Epoch: 017, Loss: 0.0210, Train: 0.9938, Test: 0.7551\n",
            "Early stopping:  0.00915432210632954\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.59      0.89      0.71       382\n",
            "             ecology       0.63      0.92      0.75       869\n",
            "            economic       0.85      0.65      0.74      1389\n",
            "          geophysics       0.89      0.89      0.89      1182\n",
            "  gravitional_theory       0.38      0.97      0.54       110\n",
            "               hydro       0.82      0.81      0.82       335\n",
            "                math       0.88      0.65      0.75      1319\n",
            "              metals       0.52      0.95      0.67       181\n",
            "          networking       0.89      0.83      0.86       325\n",
            "        neuroscience       0.94      0.95      0.94       287\n",
            "        oceanography       0.87      0.55      0.67       970\n",
            "             politic       0.68      0.76      0.72       583\n",
            "           sociology       0.63      0.67      0.65       719\n",
            "software_engineering       0.87      0.80      0.84       504\n",
            "          statistics       0.71      0.81      0.75       627\n",
            "    theory_computing       0.76      0.71      0.73       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.74      0.80      0.75     10204\n",
            "        weighted avg       0.79      0.76      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2253, Train: 0.7156, Test: 0.6789\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0084, Train: 0.8375, Test: 0.7454\n",
            "Early stopping:  1.5675636905682246\n",
            "Epoch: 003, Loss: 0.5341, Train: 0.8781, Test: 0.7606\n",
            "Early stopping:  1.4365445099675844\n",
            "Epoch: 004, Loss: 0.3651, Train: 0.9344, Test: 0.7919\n",
            "Early stopping:  1.323044787434566\n",
            "Epoch: 005, Loss: 0.2113, Train: 0.9281, Test: 0.7938\n",
            "Early stopping:  1.2420398569488438\n",
            "Epoch: 006, Loss: 0.1774, Train: 0.9625, Test: 0.7924\n",
            "Early stopping:  0.3380129476153288\n",
            "Epoch: 007, Loss: 0.1082, Train: 0.9688, Test: 0.7818\n",
            "Early stopping:  0.17071356355353406\n",
            "Epoch: 008, Loss: 0.0777, Train: 0.9906, Test: 0.7828\n",
            "Early stopping:  0.11241243189450888\n",
            "Epoch: 009, Loss: 0.0539, Train: 0.9938, Test: 0.7808\n",
            "Early stopping:  0.06663952554909429\n",
            "Epoch: 010, Loss: 0.0414, Train: 0.9969, Test: 0.7815\n",
            "Early stopping:  0.05425260717041864\n",
            "Epoch: 011, Loss: 0.0260, Train: 1.0000, Test: 0.7803\n",
            "Early stopping:  0.032277724250483295\n",
            "Epoch: 012, Loss: 0.0151, Train: 0.9938, Test: 0.7791\n",
            "Early stopping:  0.024463689350272235\n",
            "Epoch: 013, Loss: 0.0137, Train: 0.9969, Test: 0.7766\n",
            "Early stopping:  0.017358274554945874\n",
            "Epoch: 014, Loss: 0.0116, Train: 1.0000, Test: 0.7758\n",
            "Early stopping:  0.012418266713915875\n",
            "Epoch: 015, Loss: 0.0074, Train: 1.0000, Test: 0.7768\n",
            "Early stopping:  0.006908080119525395\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.78      0.77       382\n",
            "             ecology       0.88      0.75      0.81       869\n",
            "            economic       0.83      0.58      0.68      1389\n",
            "          geophysics       0.93      0.92      0.92      1182\n",
            "  gravitional_theory       0.62      0.91      0.74       110\n",
            "               hydro       0.65      0.84      0.73       335\n",
            "                math       0.90      0.80      0.85      1319\n",
            "              metals       0.69      0.94      0.79       181\n",
            "          networking       0.84      0.88      0.86       325\n",
            "        neuroscience       0.91      0.98      0.94       287\n",
            "        oceanography       0.80      0.82      0.81       970\n",
            "             politic       0.56      0.87      0.68       583\n",
            "           sociology       0.54      0.56      0.55       719\n",
            "software_engineering       0.83      0.73      0.77       504\n",
            "          statistics       0.72      0.87      0.79       627\n",
            "    theory_computing       0.69      0.70      0.69       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.76      0.81      0.77     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "time: 5.91 s (started: 2024-08-16 14:14:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJTYmRZNKnn2",
        "outputId": "c9ecf865-78c4-4a7d-cb07-456a23974c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 439 ms (started: 2024-08-16 14:14:08 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "2NOxh1cpKnn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1lDdA4-Knn2",
        "outputId": "bad64764-cbca-4da5-a4f8-79499ecf2bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7839, Train: 0.8156, Test: 0.7076\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5159, Train: 0.8250, Test: 0.7453\n",
            "Early stopping:  0.1895503935559246\n",
            "Epoch: 003, Loss: 2.2081, Train: 0.8219, Test: 0.7497\n",
            "Early stopping:  0.28813044581852804\n",
            "Epoch: 004, Loss: 1.8535, Train: 0.8344, Test: 0.7592\n",
            "Early stopping:  0.40085139890285804\n",
            "Epoch: 005, Loss: 1.4898, Train: 0.8469, Test: 0.7707\n",
            "Early stopping:  0.5149703756418026\n",
            "Epoch: 006, Loss: 1.1562, Train: 0.8781, Test: 0.7809\n",
            "Early stopping:  0.5437295800235419\n",
            "Epoch: 007, Loss: 0.8791, Train: 0.9000, Test: 0.7895\n",
            "Early stopping:  0.5311983254137532\n",
            "Epoch: 008, Loss: 0.6651, Train: 0.9187, Test: 0.8021\n",
            "Early stopping:  0.4748002338332647\n",
            "Epoch: 009, Loss: 0.5069, Train: 0.9344, Test: 0.8048\n",
            "Early stopping:  0.3924031164113588\n",
            "Epoch: 010, Loss: 0.3949, Train: 0.9375, Test: 0.8052\n",
            "Early stopping:  0.30402938491923204\n",
            "Epoch: 011, Loss: 0.3181, Train: 0.9500, Test: 0.8047\n",
            "Early stopping:  0.22430050731493048\n",
            "Epoch: 012, Loss: 0.2608, Train: 0.9625, Test: 0.8074\n",
            "Early stopping:  0.16091280666616736\n",
            "Epoch: 013, Loss: 0.2118, Train: 0.9688, Test: 0.8117\n",
            "Early stopping:  0.1162523425621999\n",
            "Epoch: 014, Loss: 0.1709, Train: 0.9781, Test: 0.8120\n",
            "Early stopping:  0.08831697348590846\n",
            "Epoch: 015, Loss: 0.1386, Train: 0.9844, Test: 0.8100\n",
            "Early stopping:  0.07140431982976583\n",
            "Epoch: 016, Loss: 0.1121, Train: 0.9875, Test: 0.8090\n",
            "Early stopping:  0.05903158686819271\n",
            "Epoch: 017, Loss: 0.0894, Train: 0.9875, Test: 0.8089\n",
            "Early stopping:  0.048322489012145864\n",
            "Epoch: 018, Loss: 0.0706, Train: 0.9906, Test: 0.8067\n",
            "Early stopping:  0.03969756030350025\n",
            "Epoch: 019, Loss: 0.0561, Train: 0.9969, Test: 0.8055\n",
            "Early stopping:  0.032850549812033884\n",
            "Epoch: 020, Loss: 0.0455, Train: 1.0000, Test: 0.8033\n",
            "Early stopping:  0.02660131086310287\n",
            "Epoch: 021, Loss: 0.0376, Train: 1.0000, Test: 0.8028\n",
            "Early stopping:  0.020650150634193523\n",
            "Epoch: 022, Loss: 0.0312, Train: 1.0000, Test: 0.8015\n",
            "Early stopping:  0.015579208848474475\n",
            "Epoch: 023, Loss: 0.0261, Train: 1.0000, Test: 0.8013\n",
            "Early stopping:  0.011859791718409965\n",
            "Epoch: 024, Loss: 0.0221, Train: 1.0000, Test: 0.8007\n",
            "Early stopping:  0.009309902581186042\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.80      0.77       382\n",
            "             ecology       0.82      0.86      0.84       869\n",
            "            economic       0.83      0.66      0.74      1389\n",
            "          geophysics       0.94      0.88      0.91      1182\n",
            "  gravitional_theory       0.73      0.93      0.82       110\n",
            "               hydro       0.74      0.81      0.78       335\n",
            "                math       0.92      0.76      0.83      1319\n",
            "              metals       0.64      0.88      0.74       181\n",
            "          networking       0.75      0.90      0.82       325\n",
            "        neuroscience       0.94      0.96      0.95       287\n",
            "        oceanography       0.88      0.82      0.84       970\n",
            "             politic       0.82      0.65      0.72       583\n",
            "           sociology       0.58      0.79      0.67       719\n",
            "software_engineering       0.85      0.88      0.86       504\n",
            "          statistics       0.65      0.87      0.75       627\n",
            "    theory_computing       0.73      0.77      0.75       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.79      0.83      0.80     10204\n",
            "        weighted avg       0.82      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7715, Train: 0.6344, Test: 0.5699\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5175, Train: 0.7688, Test: 0.7018\n",
            "Early stopping:  0.17960188284601913\n",
            "Epoch: 003, Loss: 2.2010, Train: 0.8219, Test: 0.7307\n",
            "Early stopping:  0.28577339675795643\n",
            "Epoch: 004, Loss: 1.8353, Train: 0.8344, Test: 0.7537\n",
            "Early stopping:  0.404725599779582\n",
            "Epoch: 005, Loss: 1.4626, Train: 0.8688, Test: 0.7753\n",
            "Early stopping:  0.5232283030380794\n",
            "Epoch: 006, Loss: 1.1212, Train: 0.9000, Test: 0.7886\n",
            "Early stopping:  0.5584691707838736\n",
            "Epoch: 007, Loss: 0.8378, Train: 0.9125, Test: 0.7991\n",
            "Early stopping:  0.5447050616465099\n",
            "Epoch: 008, Loss: 0.6243, Train: 0.9219, Test: 0.7993\n",
            "Early stopping:  0.48438378581466845\n",
            "Epoch: 009, Loss: 0.4777, Train: 0.9219, Test: 0.7975\n",
            "Early stopping:  0.394849397320735\n",
            "Epoch: 010, Loss: 0.3779, Train: 0.9375, Test: 0.8008\n",
            "Early stopping:  0.29774393738378435\n",
            "Epoch: 011, Loss: 0.3034, Train: 0.9469, Test: 0.8066\n",
            "Early stopping:  0.2125429969440877\n",
            "Epoch: 012, Loss: 0.2448, Train: 0.9563, Test: 0.8066\n",
            "Early stopping:  0.15007524343397494\n",
            "Epoch: 013, Loss: 0.1988, Train: 0.9656, Test: 0.8065\n",
            "Early stopping:  0.11049390963220568\n",
            "Epoch: 014, Loss: 0.1615, Train: 0.9781, Test: 0.8051\n",
            "Early stopping:  0.08577541055009753\n",
            "Epoch: 015, Loss: 0.1303, Train: 0.9812, Test: 0.8027\n",
            "Early stopping:  0.06844908027426434\n",
            "Epoch: 016, Loss: 0.1051, Train: 0.9875, Test: 0.8001\n",
            "Early stopping:  0.05537136807071795\n",
            "Epoch: 017, Loss: 0.0847, Train: 0.9906, Test: 0.7992\n",
            "Early stopping:  0.04530853540573529\n",
            "Epoch: 018, Loss: 0.0678, Train: 0.9938, Test: 0.8003\n",
            "Early stopping:  0.037112665044418076\n",
            "Epoch: 019, Loss: 0.0543, Train: 0.9938, Test: 0.7998\n",
            "Early stopping:  0.030165331488941596\n",
            "Epoch: 020, Loss: 0.0440, Train: 1.0000, Test: 0.8002\n",
            "Early stopping:  0.024350023644794536\n",
            "Epoch: 021, Loss: 0.0359, Train: 1.0000, Test: 0.8016\n",
            "Early stopping:  0.019395417764215796\n",
            "Epoch: 022, Loss: 0.0294, Train: 1.0000, Test: 0.8011\n",
            "Early stopping:  0.015209219626726481\n",
            "Epoch: 023, Loss: 0.0242, Train: 1.0000, Test: 0.7999\n",
            "Early stopping:  0.01191812322843869\n",
            "Epoch: 024, Loss: 0.0204, Train: 1.0000, Test: 0.7995\n",
            "Early stopping:  0.009402200203623343\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.81      0.79       382\n",
            "             ecology       0.83      0.83      0.83       869\n",
            "            economic       0.80      0.73      0.77      1389\n",
            "          geophysics       0.92      0.89      0.91      1182\n",
            "  gravitional_theory       0.75      0.96      0.84       110\n",
            "               hydro       0.74      0.82      0.78       335\n",
            "                math       0.93      0.68      0.79      1319\n",
            "              metals       0.57      0.90      0.70       181\n",
            "          networking       0.77      0.94      0.84       325\n",
            "        neuroscience       0.91      0.99      0.95       287\n",
            "        oceanography       0.86      0.85      0.85       970\n",
            "             politic       0.70      0.70      0.70       583\n",
            "           sociology       0.68      0.67      0.67       719\n",
            "software_engineering       0.84      0.88      0.86       504\n",
            "          statistics       0.67      0.88      0.76       627\n",
            "    theory_computing       0.71      0.75      0.73       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.78      0.83      0.80     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7804, Train: 0.7562, Test: 0.6159\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4989, Train: 0.8187, Test: 0.6750\n",
            "Early stopping:  0.19903933502966728\n",
            "Epoch: 003, Loss: 2.1699, Train: 0.8250, Test: 0.7026\n",
            "Early stopping:  0.3055598727981478\n",
            "Epoch: 004, Loss: 1.7932, Train: 0.8469, Test: 0.7351\n",
            "Early stopping:  0.4257059563032753\n",
            "Epoch: 005, Loss: 1.4164, Train: 0.8625, Test: 0.7650\n",
            "Early stopping:  0.5439263663295645\n",
            "Epoch: 006, Loss: 1.0803, Train: 0.8781, Test: 0.7824\n",
            "Early stopping:  0.5679421229236041\n",
            "Epoch: 007, Loss: 0.8109, Train: 0.8875, Test: 0.7932\n",
            "Early stopping:  0.5436524705939976\n",
            "Epoch: 008, Loss: 0.6148, Train: 0.9031, Test: 0.8015\n",
            "Early stopping:  0.4718853692386894\n",
            "Epoch: 009, Loss: 0.4807, Train: 0.9094, Test: 0.8104\n",
            "Early stopping:  0.37495364595363484\n",
            "Epoch: 010, Loss: 0.3883, Train: 0.9250, Test: 0.8124\n",
            "Early stopping:  0.2767258341132255\n",
            "Epoch: 011, Loss: 0.3220, Train: 0.9344, Test: 0.8141\n",
            "Early stopping:  0.19471491318326864\n",
            "Epoch: 012, Loss: 0.2716, Train: 0.9375, Test: 0.8150\n",
            "Early stopping:  0.13615015776627296\n",
            "Epoch: 013, Loss: 0.2298, Train: 0.9563, Test: 0.8141\n",
            "Early stopping:  0.09904638006707644\n",
            "Epoch: 014, Loss: 0.1928, Train: 0.9594, Test: 0.8122\n",
            "Early stopping:  0.0769357060290579\n",
            "Epoch: 015, Loss: 0.1598, Train: 0.9719, Test: 0.8103\n",
            "Early stopping:  0.06400199739497797\n",
            "Epoch: 016, Loss: 0.1306, Train: 0.9844, Test: 0.8095\n",
            "Early stopping:  0.05583109806668129\n",
            "Epoch: 017, Loss: 0.1054, Train: 0.9875, Test: 0.8083\n",
            "Early stopping:  0.04933510409454091\n",
            "Epoch: 018, Loss: 0.0844, Train: 0.9938, Test: 0.8064\n",
            "Early stopping:  0.04304051969538621\n",
            "Epoch: 019, Loss: 0.0674, Train: 0.9938, Test: 0.8047\n",
            "Early stopping:  0.036691160490880524\n",
            "Epoch: 020, Loss: 0.0546, Train: 0.9969, Test: 0.8028\n",
            "Early stopping:  0.03027512459486927\n",
            "Epoch: 021, Loss: 0.0450, Train: 1.0000, Test: 0.8021\n",
            "Early stopping:  0.024101699212389843\n",
            "Epoch: 022, Loss: 0.0374, Train: 1.0000, Test: 0.8003\n",
            "Early stopping:  0.01865949546405267\n",
            "Epoch: 023, Loss: 0.0315, Train: 1.0000, Test: 0.7994\n",
            "Early stopping:  0.014233628774287703\n",
            "Epoch: 024, Loss: 0.0273, Train: 1.0000, Test: 0.7990\n",
            "Early stopping:  0.010890952501268458\n",
            "Epoch: 025, Loss: 0.0240, Train: 1.0000, Test: 0.7980\n",
            "Early stopping:  0.008346121332793518\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.84      0.80       382\n",
            "             ecology       0.79      0.85      0.82       869\n",
            "            economic       0.82      0.74      0.78      1389\n",
            "          geophysics       0.92      0.87      0.89      1182\n",
            "  gravitional_theory       0.70      0.95      0.80       110\n",
            "               hydro       0.77      0.78      0.77       335\n",
            "                math       0.91      0.71      0.80      1319\n",
            "              metals       0.52      0.97      0.68       181\n",
            "          networking       0.84      0.87      0.86       325\n",
            "        neuroscience       0.91      0.97      0.94       287\n",
            "        oceanography       0.84      0.82      0.83       970\n",
            "             politic       0.69      0.80      0.74       583\n",
            "           sociology       0.66      0.66      0.66       719\n",
            "software_engineering       0.84      0.84      0.84       504\n",
            "          statistics       0.74      0.80      0.77       627\n",
            "    theory_computing       0.70      0.79      0.74       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.78      0.83      0.79     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7738, Train: 0.6156, Test: 0.5563\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5026, Train: 0.8187, Test: 0.7356\n",
            "Early stopping:  0.19179479753282672\n",
            "Epoch: 003, Loss: 2.1670, Train: 0.8344, Test: 0.7602\n",
            "Early stopping:  0.30396573408884714\n",
            "Epoch: 004, Loss: 1.7869, Train: 0.8406, Test: 0.7625\n",
            "Early stopping:  0.4267396555190799\n",
            "Epoch: 005, Loss: 1.4131, Train: 0.8500, Test: 0.7695\n",
            "Early stopping:  0.5446135024559551\n",
            "Epoch: 006, Loss: 1.0840, Train: 0.8625, Test: 0.7779\n",
            "Early stopping:  0.5679829464992083\n",
            "Epoch: 007, Loss: 0.8203, Train: 0.8875, Test: 0.7833\n",
            "Early stopping:  0.5383552781721959\n",
            "Epoch: 008, Loss: 0.6270, Train: 0.8906, Test: 0.7919\n",
            "Early stopping:  0.46402135433861397\n",
            "Epoch: 009, Loss: 0.4945, Train: 0.9125, Test: 0.7942\n",
            "Early stopping:  0.3680032788151874\n",
            "Epoch: 010, Loss: 0.4047, Train: 0.9125, Test: 0.7908\n",
            "Early stopping:  0.271920017339557\n",
            "Epoch: 011, Loss: 0.3398, Train: 0.9156, Test: 0.7892\n",
            "Early stopping:  0.19144340736339088\n",
            "Epoch: 012, Loss: 0.2866, Train: 0.9344, Test: 0.7878\n",
            "Early stopping:  0.13447914509100653\n",
            "Epoch: 013, Loss: 0.2398, Train: 0.9437, Test: 0.7851\n",
            "Early stopping:  0.10011563912899898\n",
            "Epoch: 014, Loss: 0.1988, Train: 0.9594, Test: 0.7823\n",
            "Early stopping:  0.08124698238144945\n",
            "Epoch: 015, Loss: 0.1621, Train: 0.9688, Test: 0.7817\n",
            "Early stopping:  0.07025392327498413\n",
            "Epoch: 016, Loss: 0.1291, Train: 0.9906, Test: 0.7806\n",
            "Early stopping:  0.06223679322634327\n",
            "Epoch: 017, Loss: 0.1019, Train: 0.9875, Test: 0.7783\n",
            "Early stopping:  0.054795823682416867\n",
            "Epoch: 018, Loss: 0.0812, Train: 0.9906, Test: 0.7774\n",
            "Early stopping:  0.0469759889741323\n",
            "Epoch: 019, Loss: 0.0655, Train: 0.9938, Test: 0.7767\n",
            "Early stopping:  0.03849665696745125\n",
            "Epoch: 020, Loss: 0.0530, Train: 0.9938, Test: 0.7749\n",
            "Early stopping:  0.030159381337140006\n",
            "Epoch: 021, Loss: 0.0435, Train: 0.9969, Test: 0.7732\n",
            "Early stopping:  0.02318633191891425\n",
            "Epoch: 022, Loss: 0.0366, Train: 1.0000, Test: 0.7736\n",
            "Early stopping:  0.01782104921830835\n",
            "Epoch: 023, Loss: 0.0314, Train: 1.0000, Test: 0.7716\n",
            "Early stopping:  0.013586589438051182\n",
            "Epoch: 024, Loss: 0.0273, Train: 1.0000, Test: 0.7704\n",
            "Early stopping:  0.01020925054265342\n",
            "Epoch: 025, Loss: 0.0240, Train: 1.0000, Test: 0.7697\n",
            "Early stopping:  0.007731014100290652\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.87      0.76       382\n",
            "             ecology       0.83      0.75      0.79       869\n",
            "            economic       0.82      0.65      0.72      1389\n",
            "          geophysics       0.95      0.84      0.89      1182\n",
            "  gravitional_theory       0.43      0.97      0.60       110\n",
            "               hydro       0.67      0.89      0.77       335\n",
            "                math       0.88      0.70      0.78      1319\n",
            "              metals       0.42      0.93      0.58       181\n",
            "          networking       0.89      0.81      0.85       325\n",
            "        neuroscience       0.85      0.99      0.91       287\n",
            "        oceanography       0.82      0.88      0.85       970\n",
            "             politic       0.67      0.68      0.68       583\n",
            "           sociology       0.62      0.67      0.64       719\n",
            "software_engineering       0.92      0.77      0.84       504\n",
            "          statistics       0.77      0.76      0.77       627\n",
            "    theory_computing       0.60      0.81      0.69       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.74      0.81      0.76     10204\n",
            "        weighted avg       0.80      0.77      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7855, Train: 0.6312, Test: 0.5567\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5178, Train: 0.8594, Test: 0.7473\n",
            "Early stopping:  0.1893426938864629\n",
            "Epoch: 003, Loss: 2.2057, Train: 0.8656, Test: 0.7584\n",
            "Early stopping:  0.290224325333176\n",
            "Epoch: 004, Loss: 1.8444, Train: 0.8625, Test: 0.7611\n",
            "Early stopping:  0.40570424887730727\n",
            "Epoch: 005, Loss: 1.4748, Train: 0.8844, Test: 0.7703\n",
            "Early stopping:  0.5220988183827485\n",
            "Epoch: 006, Loss: 1.1337, Train: 0.9000, Test: 0.7823\n",
            "Early stopping:  0.5534428522268687\n",
            "Epoch: 007, Loss: 0.8475, Train: 0.9125, Test: 0.7951\n",
            "Early stopping:  0.5424605970025631\n",
            "Epoch: 008, Loss: 0.6271, Train: 0.9313, Test: 0.8047\n",
            "Early stopping:  0.48644548698322065\n",
            "Epoch: 009, Loss: 0.4718, Train: 0.9469, Test: 0.8099\n",
            "Early stopping:  0.4015465954066157\n",
            "Epoch: 010, Loss: 0.3686, Train: 0.9469, Test: 0.8091\n",
            "Early stopping:  0.30680579922099205\n",
            "Epoch: 011, Loss: 0.2964, Train: 0.9594, Test: 0.8102\n",
            "Early stopping:  0.22019254325754964\n",
            "Epoch: 012, Loss: 0.2395, Train: 0.9625, Test: 0.8083\n",
            "Early stopping:  0.1534584240406576\n",
            "Epoch: 013, Loss: 0.1927, Train: 0.9719, Test: 0.8068\n",
            "Early stopping:  0.11006230424720145\n",
            "Epoch: 014, Loss: 0.1549, Train: 0.9875, Test: 0.8057\n",
            "Early stopping:  0.0846624663901762\n",
            "Epoch: 015, Loss: 0.1242, Train: 0.9906, Test: 0.8050\n",
            "Early stopping:  0.06834263085654332\n",
            "Epoch: 016, Loss: 0.0994, Train: 0.9938, Test: 0.8022\n",
            "Early stopping:  0.05558381353514396\n",
            "Epoch: 017, Loss: 0.0796, Train: 0.9938, Test: 0.8036\n",
            "Early stopping:  0.04491402901425991\n",
            "Epoch: 018, Loss: 0.0641, Train: 0.9969, Test: 0.8035\n",
            "Early stopping:  0.03606148855555678\n",
            "Epoch: 019, Loss: 0.0521, Train: 0.9969, Test: 0.8026\n",
            "Early stopping:  0.028644993711504467\n",
            "Epoch: 020, Loss: 0.0427, Train: 1.0000, Test: 0.8007\n",
            "Early stopping:  0.022492172972429444\n",
            "Epoch: 021, Loss: 0.0351, Train: 1.0000, Test: 0.7995\n",
            "Early stopping:  0.01760936120360363\n",
            "Epoch: 022, Loss: 0.0292, Train: 1.0000, Test: 0.7986\n",
            "Early stopping:  0.013856767501843474\n",
            "Epoch: 023, Loss: 0.0247, Train: 1.0000, Test: 0.7981\n",
            "Early stopping:  0.010922503292006534\n",
            "Epoch: 024, Loss: 0.0213, Train: 1.0000, Test: 0.7984\n",
            "Early stopping:  0.008527863494270396\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.88      0.78       382\n",
            "             ecology       0.81      0.83      0.82       869\n",
            "            economic       0.79      0.74      0.76      1389\n",
            "          geophysics       0.93      0.87      0.90      1182\n",
            "  gravitional_theory       0.81      0.94      0.87       110\n",
            "               hydro       0.70      0.81      0.75       335\n",
            "                math       0.93      0.77      0.84      1319\n",
            "              metals       0.52      0.96      0.67       181\n",
            "          networking       0.70      0.86      0.77       325\n",
            "        neuroscience       0.91      0.98      0.94       287\n",
            "        oceanography       0.85      0.81      0.83       970\n",
            "             politic       0.74      0.74      0.74       583\n",
            "           sociology       0.66      0.61      0.64       719\n",
            "software_engineering       0.90      0.85      0.87       504\n",
            "          statistics       0.76      0.84      0.79       627\n",
            "    theory_computing       0.69      0.72      0.71       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.78      0.82      0.79     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7848, Train: 0.7344, Test: 0.5657\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5239, Train: 0.8281, Test: 0.7320\n",
            "Early stopping:  0.18454048196713166\n",
            "Epoch: 003, Loss: 2.2152, Train: 0.8406, Test: 0.7525\n",
            "Early stopping:  0.28514549722248017\n",
            "Epoch: 004, Loss: 1.8540, Train: 0.8531, Test: 0.7624\n",
            "Early stopping:  0.40141833825797435\n",
            "Epoch: 005, Loss: 1.4837, Train: 0.8594, Test: 0.7701\n",
            "Early stopping:  0.5186899167526237\n",
            "Epoch: 006, Loss: 1.1446, Train: 0.8719, Test: 0.7809\n",
            "Early stopping:  0.5520462899521865\n",
            "Epoch: 007, Loss: 0.8656, Train: 0.8656, Test: 0.7928\n",
            "Early stopping:  0.5396729494705179\n",
            "Epoch: 008, Loss: 0.6567, Train: 0.8969, Test: 0.8027\n",
            "Early stopping:  0.4791130512442761\n",
            "Epoch: 009, Loss: 0.5110, Train: 0.9000, Test: 0.8061\n",
            "Early stopping:  0.38954686921899967\n",
            "Epoch: 010, Loss: 0.4110, Train: 0.9094, Test: 0.8051\n",
            "Early stopping:  0.293549098841993\n",
            "Epoch: 011, Loss: 0.3375, Train: 0.9187, Test: 0.8058\n",
            "Early stopping:  0.2102406127643293\n",
            "Epoch: 012, Loss: 0.2789, Train: 0.9250, Test: 0.8045\n",
            "Early stopping:  0.14940202056817406\n",
            "Epoch: 013, Loss: 0.2308, Train: 0.9531, Test: 0.8036\n",
            "Early stopping:  0.11066315654800747\n",
            "Epoch: 014, Loss: 0.1898, Train: 0.9688, Test: 0.8037\n",
            "Early stopping:  0.08740340314631236\n",
            "Epoch: 015, Loss: 0.1541, Train: 0.9750, Test: 0.8037\n",
            "Early stopping:  0.07245320136865423\n",
            "Epoch: 016, Loss: 0.1241, Train: 0.9906, Test: 0.8007\n",
            "Early stopping:  0.061358157147647266\n",
            "Epoch: 017, Loss: 0.0998, Train: 0.9906, Test: 0.7976\n",
            "Early stopping:  0.0520824066620186\n",
            "Epoch: 018, Loss: 0.0804, Train: 0.9938, Test: 0.7954\n",
            "Early stopping:  0.043476089708853145\n",
            "Epoch: 019, Loss: 0.0650, Train: 0.9938, Test: 0.7922\n",
            "Early stopping:  0.0353826930581603\n",
            "Epoch: 020, Loss: 0.0528, Train: 0.9969, Test: 0.7921\n",
            "Early stopping:  0.02831367296102099\n",
            "Epoch: 021, Loss: 0.0432, Train: 1.0000, Test: 0.7916\n",
            "Early stopping:  0.022485981282732083\n",
            "Epoch: 022, Loss: 0.0359, Train: 1.0000, Test: 0.7909\n",
            "Early stopping:  0.017699481288608848\n",
            "Epoch: 023, Loss: 0.0304, Train: 1.0000, Test: 0.7895\n",
            "Early stopping:  0.013766252499810656\n",
            "Epoch: 024, Loss: 0.0261, Train: 1.0000, Test: 0.7894\n",
            "Early stopping:  0.010586124563946014\n",
            "Epoch: 025, Loss: 0.0226, Train: 1.0000, Test: 0.7863\n",
            "Early stopping:  0.008131132699241471\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.86      0.79      0.82       382\n",
            "             ecology       0.88      0.74      0.81       869\n",
            "            economic       0.78      0.65      0.71      1389\n",
            "          geophysics       0.92      0.92      0.92      1182\n",
            "  gravitional_theory       0.63      0.93      0.75       110\n",
            "               hydro       0.77      0.82      0.79       335\n",
            "                math       0.86      0.71      0.78      1319\n",
            "              metals       0.58      0.92      0.71       181\n",
            "          networking       0.73      0.93      0.82       325\n",
            "        neuroscience       0.93      0.97      0.95       287\n",
            "        oceanography       0.80      0.90      0.85       970\n",
            "             politic       0.64      0.81      0.71       583\n",
            "           sociology       0.58      0.69      0.63       719\n",
            "software_engineering       0.92      0.79      0.85       504\n",
            "          statistics       0.82      0.76      0.79       627\n",
            "    theory_computing       0.65      0.75      0.70       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.82      0.79     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7796, Train: 0.7438, Test: 0.6396\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5037, Train: 0.8438, Test: 0.7469\n",
            "Early stopping:  0.19511934094336605\n",
            "Epoch: 003, Loss: 2.1673, Train: 0.8688, Test: 0.7617\n",
            "Early stopping:  0.3066515444756399\n",
            "Epoch: 004, Loss: 1.7838, Train: 0.8719, Test: 0.7705\n",
            "Early stopping:  0.4302464347911022\n",
            "Epoch: 005, Loss: 1.4011, Train: 0.8750, Test: 0.7823\n",
            "Early stopping:  0.5509486183817931\n",
            "Epoch: 006, Loss: 1.0591, Train: 0.8688, Test: 0.7939\n",
            "Early stopping:  0.5781369489006333\n",
            "Epoch: 007, Loss: 0.7866, Train: 0.8906, Test: 0.8001\n",
            "Early stopping:  0.5524358485119315\n",
            "Epoch: 008, Loss: 0.5911, Train: 0.8906, Test: 0.8024\n",
            "Early stopping:  0.4780386184653709\n",
            "Epoch: 009, Loss: 0.4592, Train: 0.9000, Test: 0.8033\n",
            "Early stopping:  0.3777347121732868\n",
            "Epoch: 010, Loss: 0.3685, Train: 0.9094, Test: 0.8064\n",
            "Early stopping:  0.2761718706350008\n",
            "Epoch: 011, Loss: 0.3007, Train: 0.9250, Test: 0.8080\n",
            "Early stopping:  0.19306748746211494\n",
            "Epoch: 012, Loss: 0.2470, Train: 0.9469, Test: 0.8080\n",
            "Early stopping:  0.1360999037557759\n",
            "Epoch: 013, Loss: 0.2033, Train: 0.9594, Test: 0.8057\n",
            "Early stopping:  0.10120727212595332\n",
            "Epoch: 014, Loss: 0.1663, Train: 0.9750, Test: 0.8027\n",
            "Early stopping:  0.07993570785248377\n",
            "Epoch: 015, Loss: 0.1338, Train: 0.9844, Test: 0.8014\n",
            "Early stopping:  0.0658748145395659\n",
            "Epoch: 016, Loss: 0.1061, Train: 0.9906, Test: 0.7994\n",
            "Early stopping:  0.055735352663202474\n",
            "Epoch: 017, Loss: 0.0841, Train: 0.9969, Test: 0.7989\n",
            "Early stopping:  0.047423799180870606\n",
            "Epoch: 018, Loss: 0.0670, Train: 1.0000, Test: 0.7976\n",
            "Early stopping:  0.03953660395366872\n",
            "Epoch: 019, Loss: 0.0537, Train: 1.0000, Test: 0.7969\n",
            "Early stopping:  0.03183530129828017\n",
            "Epoch: 020, Loss: 0.0434, Train: 1.0000, Test: 0.7961\n",
            "Early stopping:  0.0249026632329936\n",
            "Epoch: 021, Loss: 0.0358, Train: 1.0000, Test: 0.7934\n",
            "Early stopping:  0.01921426556938259\n",
            "Epoch: 022, Loss: 0.0302, Train: 1.0000, Test: 0.7918\n",
            "Early stopping:  0.014664981153210336\n",
            "Epoch: 023, Loss: 0.0259, Train: 1.0000, Test: 0.7897\n",
            "Early stopping:  0.011030214628558731\n",
            "Epoch: 024, Loss: 0.0225, Train: 1.0000, Test: 0.7899\n",
            "Early stopping:  0.008310702477167501\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.81      0.74       382\n",
            "             ecology       0.85      0.84      0.84       869\n",
            "            economic       0.80      0.63      0.71      1389\n",
            "          geophysics       0.96      0.87      0.91      1182\n",
            "  gravitional_theory       0.74      0.94      0.83       110\n",
            "               hydro       0.61      0.86      0.71       335\n",
            "                math       0.92      0.78      0.84      1319\n",
            "              metals       0.59      0.97      0.74       181\n",
            "          networking       0.79      0.90      0.84       325\n",
            "        neuroscience       0.97      0.91      0.94       287\n",
            "        oceanography       0.87      0.81      0.84       970\n",
            "             politic       0.65      0.68      0.67       583\n",
            "           sociology       0.61      0.71      0.66       719\n",
            "software_engineering       0.82      0.88      0.85       504\n",
            "          statistics       0.69      0.83      0.76       627\n",
            "    theory_computing       0.71      0.72      0.72       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.82      0.79     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7796, Train: 0.6937, Test: 0.4986\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5136, Train: 0.8313, Test: 0.6682\n",
            "Early stopping:  0.18806884753706965\n",
            "Epoch: 003, Loss: 2.1826, Train: 0.8594, Test: 0.7168\n",
            "Early stopping:  0.29909502598257326\n",
            "Epoch: 004, Loss: 1.8016, Train: 0.8812, Test: 0.7453\n",
            "Early stopping:  0.42282970535844583\n",
            "Epoch: 005, Loss: 1.4183, Train: 0.9031, Test: 0.7684\n",
            "Early stopping:  0.5444970404746676\n",
            "Epoch: 006, Loss: 1.0719, Train: 0.9094, Test: 0.7858\n",
            "Early stopping:  0.5769365777962571\n",
            "Epoch: 007, Loss: 0.7897, Train: 0.9156, Test: 0.8020\n",
            "Early stopping:  0.5568259324826709\n",
            "Epoch: 008, Loss: 0.5820, Train: 0.9219, Test: 0.8082\n",
            "Early stopping:  0.4882501364072678\n",
            "Epoch: 009, Loss: 0.4421, Train: 0.9281, Test: 0.8105\n",
            "Early stopping:  0.3916076881568291\n",
            "Epoch: 010, Loss: 0.3506, Train: 0.9281, Test: 0.8112\n",
            "Early stopping:  0.28938478352061886\n",
            "Epoch: 011, Loss: 0.2875, Train: 0.9281, Test: 0.8102\n",
            "Early stopping:  0.20062702117506392\n",
            "Epoch: 012, Loss: 0.2400, Train: 0.9437, Test: 0.8067\n",
            "Early stopping:  0.1357289858428116\n",
            "Epoch: 013, Loss: 0.2015, Train: 0.9406, Test: 0.8058\n",
            "Early stopping:  0.095015979432048\n",
            "Epoch: 014, Loss: 0.1687, Train: 0.9500, Test: 0.8038\n",
            "Early stopping:  0.07173800016018356\n",
            "Epoch: 015, Loss: 0.1403, Train: 0.9688, Test: 0.8027\n",
            "Early stopping:  0.058124976601164606\n",
            "Epoch: 016, Loss: 0.1155, Train: 0.9750, Test: 0.8012\n",
            "Early stopping:  0.04921697884523253\n",
            "Epoch: 017, Loss: 0.0947, Train: 0.9844, Test: 0.7988\n",
            "Early stopping:  0.04234101859747111\n",
            "Epoch: 018, Loss: 0.0783, Train: 0.9844, Test: 0.7951\n",
            "Early stopping:  0.03598043797130929\n",
            "Epoch: 019, Loss: 0.0652, Train: 0.9906, Test: 0.7926\n",
            "Early stopping:  0.02987374851175294\n",
            "Epoch: 020, Loss: 0.0541, Train: 0.9938, Test: 0.7901\n",
            "Early stopping:  0.02429027873230503\n",
            "Epoch: 021, Loss: 0.0449, Train: 0.9969, Test: 0.7890\n",
            "Early stopping:  0.01970023215933335\n",
            "Epoch: 022, Loss: 0.0373, Train: 1.0000, Test: 0.7870\n",
            "Early stopping:  0.016269073424488717\n",
            "Epoch: 023, Loss: 0.0312, Train: 1.0000, Test: 0.7852\n",
            "Early stopping:  0.013487779683830725\n",
            "Epoch: 024, Loss: 0.0266, Train: 1.0000, Test: 0.7838\n",
            "Early stopping:  0.010971166547588482\n",
            "Epoch: 025, Loss: 0.0231, Train: 1.0000, Test: 0.7836\n",
            "Early stopping:  0.008697698829323655\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.73      0.75       382\n",
            "             ecology       0.84      0.83      0.83       869\n",
            "            economic       0.79      0.69      0.73      1389\n",
            "          geophysics       0.95      0.87      0.91      1182\n",
            "  gravitional_theory       0.58      0.95      0.72       110\n",
            "               hydro       0.67      0.83      0.74       335\n",
            "                math       0.88      0.72      0.79      1319\n",
            "              metals       0.65      0.90      0.75       181\n",
            "          networking       0.84      0.89      0.86       325\n",
            "        neuroscience       0.90      0.97      0.93       287\n",
            "        oceanography       0.83      0.86      0.85       970\n",
            "             politic       0.76      0.70      0.73       583\n",
            "           sociology       0.52      0.72      0.60       719\n",
            "software_engineering       0.88      0.82      0.85       504\n",
            "          statistics       0.81      0.70      0.75       627\n",
            "    theory_computing       0.60      0.82      0.69       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.77      0.81      0.78     10204\n",
            "        weighted avg       0.80      0.78      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7711, Train: 0.7781, Test: 0.6556\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5011, Train: 0.8719, Test: 0.7479\n",
            "Early stopping:  0.19096619049110256\n",
            "Epoch: 003, Loss: 2.1769, Train: 0.8844, Test: 0.7605\n",
            "Early stopping:  0.29751280722955287\n",
            "Epoch: 004, Loss: 1.8082, Train: 0.8844, Test: 0.7753\n",
            "Early stopping:  0.41575925236162026\n",
            "Epoch: 005, Loss: 1.4379, Train: 0.8938, Test: 0.7952\n",
            "Early stopping:  0.5322227461396094\n",
            "Epoch: 006, Loss: 1.1009, Train: 0.9000, Test: 0.8071\n",
            "Early stopping:  0.5597631207886572\n",
            "Epoch: 007, Loss: 0.8227, Train: 0.9000, Test: 0.8139\n",
            "Early stopping:  0.5409288103142744\n",
            "Epoch: 008, Loss: 0.6171, Train: 0.9094, Test: 0.8181\n",
            "Early stopping:  0.4768002719777753\n",
            "Epoch: 009, Loss: 0.4776, Train: 0.9187, Test: 0.8190\n",
            "Early stopping:  0.38526939094582474\n",
            "Epoch: 010, Loss: 0.3822, Train: 0.9281, Test: 0.8180\n",
            "Early stopping:  0.2877165965498284\n",
            "Epoch: 011, Loss: 0.3123, Train: 0.9344, Test: 0.8171\n",
            "Early stopping:  0.20306323162185577\n",
            "Epoch: 012, Loss: 0.2586, Train: 0.9375, Test: 0.8168\n",
            "Early stopping:  0.14204608396977128\n",
            "Epoch: 013, Loss: 0.2156, Train: 0.9469, Test: 0.8151\n",
            "Early stopping:  0.10368311264984115\n",
            "Epoch: 014, Loss: 0.1796, Train: 0.9563, Test: 0.8133\n",
            "Early stopping:  0.08004596549497672\n",
            "Epoch: 015, Loss: 0.1493, Train: 0.9625, Test: 0.8116\n",
            "Early stopping:  0.06445385427595485\n",
            "Epoch: 016, Loss: 0.1234, Train: 0.9750, Test: 0.8116\n",
            "Early stopping:  0.053503393713968235\n",
            "Epoch: 017, Loss: 0.1010, Train: 0.9875, Test: 0.8120\n",
            "Early stopping:  0.04533579191729835\n",
            "Epoch: 018, Loss: 0.0819, Train: 0.9969, Test: 0.8118\n",
            "Early stopping:  0.038692572962581155\n",
            "Epoch: 019, Loss: 0.0664, Train: 1.0000, Test: 0.8107\n",
            "Early stopping:  0.03293858971686751\n",
            "Epoch: 020, Loss: 0.0540, Train: 1.0000, Test: 0.8104\n",
            "Early stopping:  0.027615508959231105\n",
            "Epoch: 021, Loss: 0.0442, Train: 1.0000, Test: 0.8084\n",
            "Early stopping:  0.022562479522586512\n",
            "Epoch: 022, Loss: 0.0367, Train: 1.0000, Test: 0.8065\n",
            "Early stopping:  0.01800014286685137\n",
            "Epoch: 023, Loss: 0.0310, Train: 1.0000, Test: 0.8028\n",
            "Early stopping:  0.01409846813038078\n",
            "Epoch: 024, Loss: 0.0266, Train: 1.0000, Test: 0.8000\n",
            "Early stopping:  0.010888425126121364\n",
            "Epoch: 025, Loss: 0.0231, Train: 1.0000, Test: 0.7991\n",
            "Early stopping:  0.008349623251469967\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.86      0.81       382\n",
            "             ecology       0.76      0.90      0.83       869\n",
            "            economic       0.81      0.73      0.77      1389\n",
            "          geophysics       0.93      0.86      0.90      1182\n",
            "  gravitional_theory       0.52      0.98      0.68       110\n",
            "               hydro       0.71      0.86      0.78       335\n",
            "                math       0.91      0.71      0.80      1319\n",
            "              metals       0.49      0.96      0.65       181\n",
            "          networking       0.85      0.84      0.85       325\n",
            "        neuroscience       0.93      0.95      0.94       287\n",
            "        oceanography       0.89      0.79      0.84       970\n",
            "             politic       0.71      0.79      0.75       583\n",
            "           sociology       0.68      0.66      0.67       719\n",
            "software_engineering       0.82      0.90      0.86       504\n",
            "          statistics       0.76      0.81      0.78       627\n",
            "    theory_computing       0.79      0.71      0.75       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.77      0.83      0.79     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7843, Train: 0.6062, Test: 0.5299\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5181, Train: 0.7969, Test: 0.6954\n",
            "Early stopping:  0.1882252966387421\n",
            "Epoch: 003, Loss: 2.2102, Train: 0.8375, Test: 0.7406\n",
            "Early stopping:  0.2873275377634268\n",
            "Epoch: 004, Loss: 1.8535, Train: 0.8594, Test: 0.7645\n",
            "Early stopping:  0.4011129772987269\n",
            "Epoch: 005, Loss: 1.4909, Train: 0.8875, Test: 0.7846\n",
            "Early stopping:  0.5151424506227638\n",
            "Epoch: 006, Loss: 1.1550, Train: 0.9062, Test: 0.7974\n",
            "Early stopping:  0.5449785489434017\n",
            "Epoch: 007, Loss: 0.8695, Train: 0.9062, Test: 0.8015\n",
            "Early stopping:  0.5349511374826612\n",
            "Epoch: 008, Loss: 0.6502, Train: 0.9094, Test: 0.8015\n",
            "Early stopping:  0.4809399370392543\n",
            "Epoch: 009, Loss: 0.4971, Train: 0.9219, Test: 0.8038\n",
            "Early stopping:  0.3983202551994436\n",
            "Epoch: 010, Loss: 0.3940, Train: 0.9313, Test: 0.8078\n",
            "Early stopping:  0.30503644915343936\n",
            "Epoch: 011, Loss: 0.3210, Train: 0.9344, Test: 0.8090\n",
            "Early stopping:  0.2188750701087282\n",
            "Epoch: 012, Loss: 0.2652, Train: 0.9406, Test: 0.8099\n",
            "Early stopping:  0.1526408526299212\n",
            "Epoch: 013, Loss: 0.2195, Train: 0.9531, Test: 0.8091\n",
            "Early stopping:  0.10962232955366864\n",
            "Epoch: 014, Loss: 0.1804, Train: 0.9656, Test: 0.8087\n",
            "Early stopping:  0.08426044463138538\n",
            "Epoch: 015, Loss: 0.1467, Train: 0.9688, Test: 0.8064\n",
            "Early stopping:  0.0688727231601139\n",
            "Epoch: 016, Loss: 0.1177, Train: 0.9844, Test: 0.8062\n",
            "Early stopping:  0.05838738223686475\n",
            "Epoch: 017, Loss: 0.0929, Train: 0.9969, Test: 0.8055\n",
            "Early stopping:  0.050130478655882944\n",
            "Epoch: 018, Loss: 0.0726, Train: 0.9969, Test: 0.8045\n",
            "Early stopping:  0.04280590609241444\n",
            "Epoch: 019, Loss: 0.0567, Train: 1.0000, Test: 0.8031\n",
            "Early stopping:  0.035818111488661035\n",
            "Epoch: 020, Loss: 0.0449, Train: 1.0000, Test: 0.8017\n",
            "Early stopping:  0.029028237501260865\n",
            "Epoch: 021, Loss: 0.0361, Train: 1.0000, Test: 0.7985\n",
            "Early stopping:  0.022638574939210468\n",
            "Epoch: 022, Loss: 0.0297, Train: 1.0000, Test: 0.7956\n",
            "Early stopping:  0.017078148192017955\n",
            "Epoch: 023, Loss: 0.0249, Train: 1.0000, Test: 0.7939\n",
            "Early stopping:  0.012668320013955776\n",
            "Epoch: 024, Loss: 0.0214, Train: 1.0000, Test: 0.7922\n",
            "Early stopping:  0.009359024548836543\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.81      0.79       382\n",
            "             ecology       0.87      0.75      0.80       869\n",
            "            economic       0.84      0.67      0.74      1389\n",
            "          geophysics       0.93      0.91      0.92      1182\n",
            "  gravitional_theory       0.74      0.93      0.83       110\n",
            "               hydro       0.72      0.81      0.76       335\n",
            "                math       0.88      0.82      0.85      1319\n",
            "              metals       0.63      0.92      0.75       181\n",
            "          networking       0.83      0.88      0.86       325\n",
            "        neuroscience       0.92      0.96      0.94       287\n",
            "        oceanography       0.79      0.87      0.83       970\n",
            "             politic       0.60      0.86      0.70       583\n",
            "           sociology       0.64      0.56      0.60       719\n",
            "software_engineering       0.83      0.74      0.78       504\n",
            "          statistics       0.76      0.81      0.79       627\n",
            "    theory_computing       0.63      0.76      0.69       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.82      0.79     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "time: 11.2 s (started: 2024-08-16 14:14:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qBrogQfKnn2",
        "outputId": "a31d91aa-5740-4192-9229-bbe46ae8f1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 431 ms (started: 2024-08-16 14:14:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 30 ❎"
      ],
      "metadata": {
        "id": "LF2GhyGqKnn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "Jt2vU4GxKnn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRu-W3ekKnn2",
        "outputId": "d8963fbf-f0e8-4391-b5ea-b342e3a86d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5498, Train: 0.7063, Test: 0.5860\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9020, Train: 0.8167, Test: 0.7355\n",
            "Early stopping:  1.8722409825236581\n",
            "Epoch: 003, Loss: 0.5234, Train: 0.8604, Test: 0.7768\n",
            "Early stopping:  1.648885292043685\n",
            "Epoch: 004, Loss: 0.4055, Train: 0.8896, Test: 0.7860\n",
            "Early stopping:  1.4849079127435185\n",
            "Epoch: 005, Loss: 0.3434, Train: 0.9104, Test: 0.8019\n",
            "Early stopping:  1.361771650300464\n",
            "Epoch: 006, Loss: 0.2522, Train: 0.9354, Test: 0.8071\n",
            "Early stopping:  0.2529520034005417\n",
            "Epoch: 007, Loss: 0.1894, Train: 0.9500, Test: 0.8073\n",
            "Early stopping:  0.13066340652357367\n",
            "Epoch: 008, Loss: 0.1643, Train: 0.9521, Test: 0.8098\n",
            "Early stopping:  0.10215968140170152\n",
            "Epoch: 009, Loss: 0.1410, Train: 0.9625, Test: 0.8188\n",
            "Early stopping:  0.08142715240295695\n",
            "Epoch: 010, Loss: 0.1096, Train: 0.9729, Test: 0.8186\n",
            "Early stopping:  0.05395142302027506\n",
            "Epoch: 011, Loss: 0.0849, Train: 0.9833, Test: 0.8134\n",
            "Early stopping:  0.04172562010286906\n",
            "Epoch: 012, Loss: 0.0651, Train: 0.9854, Test: 0.8083\n",
            "Early stopping:  0.04032689322039551\n",
            "Epoch: 013, Loss: 0.0528, Train: 0.9938, Test: 0.8053\n",
            "Early stopping:  0.03540423320152341\n",
            "Epoch: 014, Loss: 0.0397, Train: 0.9958, Test: 0.8016\n",
            "Early stopping:  0.02751441494534035\n",
            "Epoch: 015, Loss: 0.0317, Train: 0.9958, Test: 0.7983\n",
            "Early stopping:  0.02111060657044118\n",
            "Epoch: 016, Loss: 0.0284, Train: 0.9979, Test: 0.7939\n",
            "Early stopping:  0.015309220485622155\n",
            "Epoch: 017, Loss: 0.0255, Train: 0.9958, Test: 0.7943\n",
            "Early stopping:  0.010970245691817633\n",
            "Epoch: 018, Loss: 0.0210, Train: 1.0000, Test: 0.7958\n",
            "Early stopping:  0.007036640988098425\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.78      0.78       372\n",
            "             ecology       0.78      0.82      0.80       859\n",
            "            economic       0.76      0.73      0.75      1379\n",
            "          geophysics       0.94      0.89      0.92      1172\n",
            "  gravitional_theory       0.56      0.93      0.70       100\n",
            "               hydro       0.79      0.85      0.82       325\n",
            "                math       0.92      0.75      0.83      1309\n",
            "              metals       0.53      0.92      0.67       171\n",
            "          networking       0.88      0.90      0.89       315\n",
            "        neuroscience       0.95      0.88      0.92       277\n",
            "        oceanography       0.82      0.79      0.80       960\n",
            "             politic       0.66      0.77      0.71       573\n",
            "           sociology       0.68      0.63      0.66       709\n",
            "software_engineering       0.80      0.87      0.83       494\n",
            "          statistics       0.83      0.79      0.81       617\n",
            "    theory_computing       0.69      0.84      0.76       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.77      0.82      0.79     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.1368, Train: 0.6875, Test: 0.6502\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0489, Train: 0.8417, Test: 0.7730\n",
            "Early stopping:  2.1835164479071687\n",
            "Epoch: 003, Loss: 0.5520, Train: 0.8688, Test: 0.7833\n",
            "Early stopping:  1.9422180578843609\n",
            "Epoch: 004, Loss: 0.4373, Train: 0.8812, Test: 0.7824\n",
            "Early stopping:  1.7489811242555404\n",
            "Epoch: 005, Loss: 0.3736, Train: 0.9083, Test: 0.7865\n",
            "Early stopping:  1.602518062527433\n",
            "Epoch: 006, Loss: 0.2871, Train: 0.9208, Test: 0.7842\n",
            "Early stopping:  0.30054355432588037\n",
            "Epoch: 007, Loss: 0.2463, Train: 0.9187, Test: 0.7738\n",
            "Early stopping:  0.1218753962611993\n",
            "Epoch: 008, Loss: 0.2190, Train: 0.9521, Test: 0.7714\n",
            "Early stopping:  0.09092329494221006\n",
            "Epoch: 009, Loss: 0.1473, Train: 0.9625, Test: 0.7656\n",
            "Early stopping:  0.08375730202130494\n",
            "Epoch: 010, Loss: 0.1252, Train: 0.9563, Test: 0.7531\n",
            "Early stopping:  0.06772051735623084\n",
            "Epoch: 011, Loss: 0.1125, Train: 0.9812, Test: 0.7571\n",
            "Early stopping:  0.05927209688227161\n",
            "Epoch: 012, Loss: 0.0759, Train: 0.9854, Test: 0.7681\n",
            "Early stopping:  0.053154128930174904\n",
            "Epoch: 013, Loss: 0.0587, Train: 0.9875, Test: 0.7712\n",
            "Early stopping:  0.03620134190657409\n",
            "Epoch: 014, Loss: 0.0541, Train: 0.9938, Test: 0.7697\n",
            "Early stopping:  0.03201616926270611\n",
            "Epoch: 015, Loss: 0.0406, Train: 0.9958, Test: 0.7663\n",
            "Early stopping:  0.02771667384954568\n",
            "Epoch: 016, Loss: 0.0318, Train: 0.9938, Test: 0.7669\n",
            "Early stopping:  0.017002990654740018\n",
            "Epoch: 017, Loss: 0.0278, Train: 0.9979, Test: 0.7680\n",
            "Early stopping:  0.013509766932568985\n",
            "Epoch: 018, Loss: 0.0245, Train: 0.9979, Test: 0.7672\n",
            "Early stopping:  0.011903602820825173\n",
            "Epoch: 019, Loss: 0.0195, Train: 0.9958, Test: 0.7671\n",
            "Early stopping:  0.007965654837092688\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.58      0.84      0.68       372\n",
            "             ecology       0.81      0.74      0.78       859\n",
            "            economic       0.82      0.61      0.70      1379\n",
            "          geophysics       0.95      0.83      0.88      1172\n",
            "  gravitional_theory       0.85      0.89      0.87       100\n",
            "               hydro       0.58      0.90      0.70       325\n",
            "                math       0.87      0.80      0.83      1309\n",
            "              metals       0.52      0.94      0.67       171\n",
            "          networking       0.79      0.85      0.82       315\n",
            "        neuroscience       0.89      0.98      0.93       277\n",
            "        oceanography       0.85      0.84      0.84       960\n",
            "             politic       0.56      0.65      0.60       573\n",
            "           sociology       0.63      0.64      0.63       709\n",
            "software_engineering       0.86      0.77      0.81       494\n",
            "          statistics       0.71      0.82      0.76       617\n",
            "    theory_computing       0.72      0.76      0.74       412\n",
            "\n",
            "            accuracy                           0.77     10044\n",
            "           macro avg       0.75      0.80      0.77     10044\n",
            "        weighted avg       0.79      0.77      0.77     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3727, Train: 0.6937, Test: 0.6193\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0584, Train: 0.8604, Test: 0.7791\n",
            "Early stopping:  1.6364041612897964\n",
            "Epoch: 003, Loss: 0.4611, Train: 0.8771, Test: 0.7871\n",
            "Early stopping:  1.5378221264389957\n",
            "Epoch: 004, Loss: 0.3910, Train: 0.8896, Test: 0.7992\n",
            "Early stopping:  1.4002967617831845\n",
            "Epoch: 005, Loss: 0.3062, Train: 0.9167, Test: 0.7981\n",
            "Early stopping:  1.2948027561780662\n",
            "Epoch: 006, Loss: 0.2020, Train: 0.9271, Test: 0.7924\n",
            "Early stopping:  0.3355072656050948\n",
            "Epoch: 007, Loss: 0.1701, Train: 0.9500, Test: 0.7970\n",
            "Early stopping:  0.12306175330691392\n",
            "Epoch: 008, Loss: 0.1259, Train: 0.9729, Test: 0.8050\n",
            "Early stopping:  0.1078620904179224\n",
            "Epoch: 009, Loss: 0.0884, Train: 0.9812, Test: 0.8038\n",
            "Early stopping:  0.08339212060131797\n",
            "Epoch: 010, Loss: 0.0709, Train: 0.9875, Test: 0.8049\n",
            "Early stopping:  0.05483970147353864\n",
            "Epoch: 011, Loss: 0.0545, Train: 0.9938, Test: 0.8068\n",
            "Early stopping:  0.046413656891106346\n",
            "Epoch: 012, Loss: 0.0414, Train: 0.9917, Test: 0.8024\n",
            "Early stopping:  0.03290160003235644\n",
            "Epoch: 013, Loss: 0.0312, Train: 0.9958, Test: 0.7946\n",
            "Early stopping:  0.02288561254545448\n",
            "Epoch: 014, Loss: 0.0252, Train: 0.9979, Test: 0.7908\n",
            "Early stopping:  0.018403120888906416\n",
            "Epoch: 015, Loss: 0.0200, Train: 0.9979, Test: 0.7890\n",
            "Early stopping:  0.013731261186737679\n",
            "Epoch: 016, Loss: 0.0139, Train: 1.0000, Test: 0.7892\n",
            "Early stopping:  0.010555212903993815\n",
            "Epoch: 017, Loss: 0.0099, Train: 1.0000, Test: 0.7897\n",
            "Early stopping:  0.008526609862432637\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.81      0.77       372\n",
            "             ecology       0.87      0.84      0.85       859\n",
            "            economic       0.82      0.60      0.69      1379\n",
            "          geophysics       0.97      0.86      0.91      1172\n",
            "  gravitional_theory       0.66      0.94      0.78       100\n",
            "               hydro       0.64      0.91      0.75       325\n",
            "                math       0.91      0.78      0.84      1309\n",
            "              metals       0.64      0.85      0.73       171\n",
            "          networking       0.72      0.95      0.82       315\n",
            "        neuroscience       0.95      0.93      0.94       277\n",
            "        oceanography       0.89      0.88      0.88       960\n",
            "             politic       0.59      0.77      0.67       573\n",
            "           sociology       0.56      0.67      0.61       709\n",
            "software_engineering       0.85      0.78      0.81       494\n",
            "          statistics       0.70      0.84      0.76       617\n",
            "    theory_computing       0.80      0.72      0.75       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.77      0.82      0.79     10044\n",
            "        weighted avg       0.81      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5106, Train: 0.7000, Test: 0.7044\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1033, Train: 0.8333, Test: 0.7365\n",
            "Early stopping:  1.7022448722154537\n",
            "Epoch: 003, Loss: 0.6320, Train: 0.8542, Test: 0.7289\n",
            "Early stopping:  1.5440347544428343\n",
            "Epoch: 004, Loss: 0.5846, Train: 0.8812, Test: 0.7614\n",
            "Early stopping:  1.3885533755555255\n",
            "Epoch: 005, Loss: 0.4620, Train: 0.9021, Test: 0.7890\n",
            "Early stopping:  1.2823080303530685\n",
            "Epoch: 006, Loss: 0.3615, Train: 0.9250, Test: 0.8020\n",
            "Early stopping:  0.28565958121126195\n",
            "Epoch: 007, Loss: 0.2841, Train: 0.9333, Test: 0.8026\n",
            "Early stopping:  0.1463212989316956\n",
            "Epoch: 008, Loss: 0.2270, Train: 0.9354, Test: 0.7991\n",
            "Early stopping:  0.1427344852066652\n",
            "Epoch: 009, Loss: 0.1795, Train: 0.9521, Test: 0.7892\n",
            "Early stopping:  0.11189908617703552\n",
            "Epoch: 010, Loss: 0.1428, Train: 0.9667, Test: 0.7842\n",
            "Early stopping:  0.08656387147011618\n",
            "Epoch: 011, Loss: 0.1015, Train: 0.9792, Test: 0.7766\n",
            "Early stopping:  0.07133041384249612\n",
            "Epoch: 012, Loss: 0.0734, Train: 0.9917, Test: 0.7660\n",
            "Early stopping:  0.06110510256957812\n",
            "Epoch: 013, Loss: 0.0596, Train: 0.9875, Test: 0.7576\n",
            "Early stopping:  0.04961937800904186\n",
            "Epoch: 014, Loss: 0.0555, Train: 0.9854, Test: 0.7527\n",
            "Early stopping:  0.03620877727741925\n",
            "Epoch: 015, Loss: 0.0561, Train: 0.9938, Test: 0.7543\n",
            "Early stopping:  0.019427685698358794\n",
            "Epoch: 016, Loss: 0.0388, Train: 0.9917, Test: 0.7543\n",
            "Early stopping:  0.012344695951962517\n",
            "Epoch: 017, Loss: 0.0309, Train: 0.9917, Test: 0.7563\n",
            "Early stopping:  0.012608309563213838\n",
            "Epoch: 018, Loss: 0.0245, Train: 0.9958, Test: 0.7588\n",
            "Early stopping:  0.014300237463240505\n",
            "Epoch: 019, Loss: 0.0182, Train: 0.9979, Test: 0.7614\n",
            "Early stopping:  0.014666220692689948\n",
            "Epoch: 020, Loss: 0.0154, Train: 0.9979, Test: 0.7617\n",
            "Early stopping:  0.00950376872783731\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.82      0.79       372\n",
            "             ecology       0.87      0.74      0.80       859\n",
            "            economic       0.86      0.63      0.73      1379\n",
            "          geophysics       0.96      0.82      0.88      1172\n",
            "  gravitional_theory       0.62      0.97      0.76       100\n",
            "               hydro       0.72      0.86      0.79       325\n",
            "                math       0.85      0.62      0.72      1309\n",
            "              metals       0.44      0.95      0.60       171\n",
            "          networking       0.84      0.86      0.85       315\n",
            "        neuroscience       0.82      0.98      0.89       277\n",
            "        oceanography       0.76      0.87      0.81       960\n",
            "             politic       0.70      0.80      0.75       573\n",
            "           sociology       0.63      0.71      0.67       709\n",
            "software_engineering       0.79      0.85      0.82       494\n",
            "          statistics       0.65      0.81      0.72       617\n",
            "    theory_computing       0.51      0.67      0.58       412\n",
            "\n",
            "            accuracy                           0.76     10044\n",
            "           macro avg       0.74      0.81      0.76     10044\n",
            "        weighted avg       0.79      0.76      0.76     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3757, Train: 0.7312, Test: 0.6414\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8829, Train: 0.8521, Test: 0.7871\n",
            "Early stopping:  1.7627277493178608\n",
            "Epoch: 003, Loss: 0.4628, Train: 0.8708, Test: 0.7960\n",
            "Early stopping:  1.5746030757579095\n",
            "Epoch: 004, Loss: 0.3659, Train: 0.9104, Test: 0.8012\n",
            "Early stopping:  1.420448016357565\n",
            "Epoch: 005, Loss: 0.2680, Train: 0.9229, Test: 0.7996\n",
            "Early stopping:  1.3094984902742846\n",
            "Epoch: 006, Loss: 0.2134, Train: 0.9437, Test: 0.8020\n",
            "Early stopping:  0.2660098272166334\n",
            "Epoch: 007, Loss: 0.1663, Train: 0.9563, Test: 0.8060\n",
            "Early stopping:  0.11950790656101563\n",
            "Epoch: 008, Loss: 0.1286, Train: 0.9667, Test: 0.8075\n",
            "Early stopping:  0.09289344603851958\n",
            "Epoch: 009, Loss: 0.1004, Train: 0.9688, Test: 0.8079\n",
            "Early stopping:  0.06692296584872888\n",
            "Epoch: 010, Loss: 0.0805, Train: 0.9792, Test: 0.8133\n",
            "Early stopping:  0.05312870451720181\n",
            "Epoch: 011, Loss: 0.0655, Train: 0.9833, Test: 0.8148\n",
            "Early stopping:  0.04010953616327952\n",
            "Epoch: 012, Loss: 0.0504, Train: 0.9854, Test: 0.8151\n",
            "Early stopping:  0.030561449244250954\n",
            "Epoch: 013, Loss: 0.0388, Train: 0.9958, Test: 0.8136\n",
            "Early stopping:  0.024360812445975374\n",
            "Epoch: 014, Loss: 0.0273, Train: 0.9958, Test: 0.8088\n",
            "Early stopping:  0.02110069778828511\n",
            "Epoch: 015, Loss: 0.0223, Train: 0.9938, Test: 0.8084\n",
            "Early stopping:  0.017538947942847313\n",
            "Epoch: 016, Loss: 0.0187, Train: 1.0000, Test: 0.8099\n",
            "Early stopping:  0.013007321659271279\n",
            "Epoch: 017, Loss: 0.0152, Train: 1.0000, Test: 0.8096\n",
            "Early stopping:  0.009197285708232632\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.84      0.74      0.79       372\n",
            "             ecology       0.86      0.82      0.84       859\n",
            "            economic       0.81      0.73      0.77      1379\n",
            "          geophysics       0.95      0.89      0.92      1172\n",
            "  gravitional_theory       0.80      0.88      0.84       100\n",
            "               hydro       0.75      0.84      0.79       325\n",
            "                math       0.84      0.86      0.85      1309\n",
            "              metals       0.67      0.88      0.76       171\n",
            "          networking       0.79      0.94      0.86       315\n",
            "        neuroscience       0.93      0.97      0.95       277\n",
            "        oceanography       0.87      0.89      0.88       960\n",
            "             politic       0.62      0.81      0.70       573\n",
            "           sociology       0.69      0.52      0.59       709\n",
            "software_engineering       0.87      0.82      0.85       494\n",
            "          statistics       0.74      0.79      0.77       617\n",
            "    theory_computing       0.70      0.76      0.73       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.80      0.82      0.81     10044\n",
            "        weighted avg       0.81      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4107, Train: 0.7250, Test: 0.6147\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2802, Train: 0.7542, Test: 0.6987\n",
            "Early stopping:  1.5064903782650285\n",
            "Epoch: 003, Loss: 0.7857, Train: 0.8604, Test: 0.7798\n",
            "Early stopping:  1.3948944672869599\n",
            "Epoch: 004, Loss: 0.4612, Train: 0.8708, Test: 0.7763\n",
            "Early stopping:  1.3275977875182108\n",
            "Epoch: 005, Loss: 0.4033, Train: 0.8938, Test: 0.7941\n",
            "Early stopping:  1.2472729900756354\n",
            "Epoch: 006, Loss: 0.3167, Train: 0.9187, Test: 0.7962\n",
            "Early stopping:  0.3947227297571055\n",
            "Epoch: 007, Loss: 0.2605, Train: 0.9333, Test: 0.7971\n",
            "Early stopping:  0.2052892061564357\n",
            "Epoch: 008, Loss: 0.1997, Train: 0.9458, Test: 0.7909\n",
            "Early stopping:  0.10554429700622701\n",
            "Epoch: 009, Loss: 0.1583, Train: 0.9479, Test: 0.7758\n",
            "Early stopping:  0.0967263845489482\n",
            "Epoch: 010, Loss: 0.1405, Train: 0.9604, Test: 0.7791\n",
            "Early stopping:  0.07316748916485148\n",
            "Epoch: 011, Loss: 0.1096, Train: 0.9708, Test: 0.7897\n",
            "Early stopping:  0.05844881566040402\n",
            "Epoch: 012, Loss: 0.0851, Train: 0.9833, Test: 0.7917\n",
            "Early stopping:  0.044244301713936875\n",
            "Epoch: 013, Loss: 0.0628, Train: 0.9938, Test: 0.7903\n",
            "Early stopping:  0.039082530792178154\n",
            "Epoch: 014, Loss: 0.0509, Train: 0.9979, Test: 0.7871\n",
            "Early stopping:  0.03615057610153118\n",
            "Epoch: 015, Loss: 0.0388, Train: 0.9979, Test: 0.7814\n",
            "Early stopping:  0.028207594910031313\n",
            "Epoch: 016, Loss: 0.0327, Train: 1.0000, Test: 0.7793\n",
            "Early stopping:  0.020828725822427504\n",
            "Epoch: 017, Loss: 0.0257, Train: 1.0000, Test: 0.7814\n",
            "Early stopping:  0.014756571826471103\n",
            "Epoch: 018, Loss: 0.0195, Train: 1.0000, Test: 0.7789\n",
            "Early stopping:  0.01212188237433003\n",
            "Epoch: 019, Loss: 0.0161, Train: 1.0000, Test: 0.7753\n",
            "Early stopping:  0.009308994037693627\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.85      0.77       372\n",
            "             ecology       0.84      0.81      0.83       859\n",
            "            economic       0.81      0.64      0.71      1379\n",
            "          geophysics       0.93      0.90      0.91      1172\n",
            "  gravitional_theory       0.87      0.88      0.88       100\n",
            "               hydro       0.70      0.82      0.75       325\n",
            "                math       0.89      0.72      0.80      1309\n",
            "              metals       0.60      0.88      0.71       171\n",
            "          networking       0.82      0.90      0.85       315\n",
            "        neuroscience       0.92      0.96      0.94       277\n",
            "        oceanography       0.88      0.82      0.85       960\n",
            "             politic       0.62      0.72      0.66       573\n",
            "           sociology       0.52      0.60      0.56       709\n",
            "software_engineering       0.88      0.75      0.81       494\n",
            "          statistics       0.61      0.89      0.73       617\n",
            "    theory_computing       0.70      0.72      0.71       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.77      0.80      0.78     10044\n",
            "        weighted avg       0.79      0.78      0.78     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4464, Train: 0.6104, Test: 0.5658\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.2830, Train: 0.8208, Test: 0.7966\n",
            "Early stopping:  1.5297311620582998\n",
            "Epoch: 003, Loss: 0.5967, Train: 0.8458, Test: 0.7859\n",
            "Early stopping:  1.4873001587780617\n",
            "Epoch: 004, Loss: 0.5233, Train: 0.8854, Test: 0.8017\n",
            "Early stopping:  1.3662404044110017\n",
            "Epoch: 005, Loss: 0.3965, Train: 0.9062, Test: 0.8071\n",
            "Early stopping:  1.275599654129179\n",
            "Epoch: 006, Loss: 0.3111, Train: 0.9208, Test: 0.8016\n",
            "Early stopping:  0.3856417924555714\n",
            "Epoch: 007, Loss: 0.2562, Train: 0.9313, Test: 0.7885\n",
            "Early stopping:  0.14237218482233963\n",
            "Epoch: 008, Loss: 0.2237, Train: 0.9500, Test: 0.7839\n",
            "Early stopping:  0.12058116822795141\n",
            "Epoch: 009, Loss: 0.1776, Train: 0.9667, Test: 0.7912\n",
            "Early stopping:  0.08445008672696329\n",
            "Epoch: 010, Loss: 0.1163, Train: 0.9833, Test: 0.7912\n",
            "Early stopping:  0.07435420286996218\n",
            "Epoch: 011, Loss: 0.0825, Train: 0.9833, Test: 0.7833\n",
            "Early stopping:  0.07227959600871241\n",
            "Epoch: 012, Loss: 0.0681, Train: 0.9875, Test: 0.7819\n",
            "Early stopping:  0.0656662290616036\n",
            "Epoch: 013, Loss: 0.0638, Train: 0.9917, Test: 0.7794\n",
            "Early stopping:  0.047164264050140466\n",
            "Epoch: 014, Loss: 0.0559, Train: 0.9938, Test: 0.7872\n",
            "Early stopping:  0.023841600141734226\n",
            "Epoch: 015, Loss: 0.0349, Train: 0.9938, Test: 0.7899\n",
            "Early stopping:  0.017535271808959668\n",
            "Epoch: 016, Loss: 0.0264, Train: 0.9938, Test: 0.7883\n",
            "Early stopping:  0.018293218409467873\n",
            "Epoch: 017, Loss: 0.0250, Train: 0.9938, Test: 0.7870\n",
            "Early stopping:  0.017665805405539626\n",
            "Epoch: 018, Loss: 0.0244, Train: 0.9958, Test: 0.7855\n",
            "Early stopping:  0.013320736801939318\n",
            "Epoch: 019, Loss: 0.0223, Train: 0.9958, Test: 0.7855\n",
            "Early stopping:  0.004863663861719435\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.56      0.65       372\n",
            "             ecology       0.82      0.85      0.83       859\n",
            "            economic       0.83      0.69      0.75      1379\n",
            "          geophysics       0.93      0.89      0.91      1172\n",
            "  gravitional_theory       0.63      0.95      0.76       100\n",
            "               hydro       0.68      0.86      0.76       325\n",
            "                math       0.93      0.73      0.81      1309\n",
            "              metals       0.54      0.91      0.68       171\n",
            "          networking       0.70      0.90      0.79       315\n",
            "        neuroscience       0.94      0.94      0.94       277\n",
            "        oceanography       0.85      0.79      0.82       960\n",
            "             politic       0.64      0.72      0.68       573\n",
            "           sociology       0.65      0.70      0.67       709\n",
            "software_engineering       0.85      0.83      0.84       494\n",
            "          statistics       0.70      0.88      0.78       617\n",
            "    theory_computing       0.61      0.76      0.68       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.75      0.81      0.77     10044\n",
            "        weighted avg       0.80      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1728, Train: 0.7229, Test: 0.6785\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8649, Train: 0.8187, Test: 0.7288\n",
            "Early stopping:  1.6319164913009019\n",
            "Epoch: 003, Loss: 0.5602, Train: 0.8625, Test: 0.7779\n",
            "Early stopping:  1.4285591104215358\n",
            "Epoch: 004, Loss: 0.4594, Train: 0.8958, Test: 0.7989\n",
            "Early stopping:  1.283919795456715\n",
            "Epoch: 005, Loss: 0.3464, Train: 0.9146, Test: 0.7989\n",
            "Early stopping:  1.1852706837605886\n",
            "Epoch: 006, Loss: 0.2813, Train: 0.9187, Test: 0.7870\n",
            "Early stopping:  0.22902414609874244\n",
            "Epoch: 007, Loss: 0.2278, Train: 0.9271, Test: 0.7901\n",
            "Early stopping:  0.13479087003698834\n",
            "Epoch: 008, Loss: 0.1698, Train: 0.9500, Test: 0.7885\n",
            "Early stopping:  0.11182574732461946\n",
            "Epoch: 009, Loss: 0.1553, Train: 0.9708, Test: 0.7956\n",
            "Early stopping:  0.07935899781772314\n",
            "Epoch: 010, Loss: 0.0824, Train: 0.9771, Test: 0.7847\n",
            "Early stopping:  0.07539399875896205\n",
            "Epoch: 011, Loss: 0.0762, Train: 0.9812, Test: 0.7791\n",
            "Early stopping:  0.06363877941368631\n",
            "Epoch: 012, Loss: 0.0717, Train: 0.9958, Test: 0.7880\n",
            "Early stopping:  0.04741031385428088\n",
            "Epoch: 013, Loss: 0.0393, Train: 0.9958, Test: 0.7881\n",
            "Early stopping:  0.042687147472491235\n",
            "Epoch: 014, Loss: 0.0305, Train: 0.9896, Test: 0.7851\n",
            "Early stopping:  0.023454238449891283\n",
            "Epoch: 015, Loss: 0.0339, Train: 0.9958, Test: 0.7813\n",
            "Early stopping:  0.021863913353829593\n",
            "Epoch: 016, Loss: 0.0235, Train: 1.0000, Test: 0.7756\n",
            "Early stopping:  0.018760828034270684\n",
            "Epoch: 017, Loss: 0.0159, Train: 0.9979, Test: 0.7699\n",
            "Early stopping:  0.009124801574663515\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.85      0.76       372\n",
            "             ecology       0.75      0.88      0.81       859\n",
            "            economic       0.77      0.72      0.74      1379\n",
            "          geophysics       0.94      0.86      0.90      1172\n",
            "  gravitional_theory       0.65      0.93      0.77       100\n",
            "               hydro       0.61      0.87      0.72       325\n",
            "                math       0.91      0.61      0.73      1309\n",
            "              metals       0.52      0.92      0.66       171\n",
            "          networking       0.80      0.92      0.85       315\n",
            "        neuroscience       0.91      0.96      0.93       277\n",
            "        oceanography       0.88      0.78      0.83       960\n",
            "             politic       0.76      0.69      0.72       573\n",
            "           sociology       0.62      0.63      0.62       709\n",
            "software_engineering       0.67      0.86      0.75       494\n",
            "          statistics       0.77      0.79      0.78       617\n",
            "    theory_computing       0.67      0.68      0.67       412\n",
            "\n",
            "            accuracy                           0.77     10044\n",
            "           macro avg       0.74      0.81      0.77     10044\n",
            "        weighted avg       0.79      0.77      0.77     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6964, Train: 0.6396, Test: 0.6184\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1334, Train: 0.8438, Test: 0.7606\n",
            "Early stopping:  1.8123205551146289\n",
            "Epoch: 003, Loss: 0.5156, Train: 0.8500, Test: 0.7563\n",
            "Early stopping:  1.6866212745042148\n",
            "Epoch: 004, Loss: 0.4786, Train: 0.8708, Test: 0.7934\n",
            "Early stopping:  1.5234902617769073\n",
            "Epoch: 005, Loss: 0.3585, Train: 0.8917, Test: 0.8069\n",
            "Early stopping:  1.4077131091407842\n",
            "Epoch: 006, Loss: 0.2739, Train: 0.9104, Test: 0.8040\n",
            "Early stopping:  0.3389310494971973\n",
            "Epoch: 007, Loss: 0.2274, Train: 0.9271, Test: 0.7978\n",
            "Early stopping:  0.125194252764019\n",
            "Epoch: 008, Loss: 0.1921, Train: 0.9563, Test: 0.7941\n",
            "Early stopping:  0.11482502417932494\n",
            "Epoch: 009, Loss: 0.1388, Train: 0.9521, Test: 0.7943\n",
            "Early stopping:  0.08345891644391193\n",
            "Epoch: 010, Loss: 0.1109, Train: 0.9646, Test: 0.7866\n",
            "Early stopping:  0.06578012272941339\n",
            "Epoch: 011, Loss: 0.0987, Train: 0.9792, Test: 0.7836\n",
            "Early stopping:  0.05473355698401239\n",
            "Epoch: 012, Loss: 0.0748, Train: 0.9875, Test: 0.7834\n",
            "Early stopping:  0.04494103903708286\n",
            "Epoch: 013, Loss: 0.0558, Train: 0.9917, Test: 0.7814\n",
            "Early stopping:  0.03212103808870052\n",
            "Epoch: 014, Loss: 0.0467, Train: 0.9938, Test: 0.7833\n",
            "Early stopping:  0.027328493297259615\n",
            "Epoch: 015, Loss: 0.0329, Train: 0.9938, Test: 0.7838\n",
            "Early stopping:  0.02563596659087519\n",
            "Epoch: 016, Loss: 0.0241, Train: 1.0000, Test: 0.7842\n",
            "Early stopping:  0.019831999681781762\n",
            "Epoch: 017, Loss: 0.0175, Train: 0.9958, Test: 0.7839\n",
            "Early stopping:  0.01578171591443803\n",
            "Epoch: 018, Loss: 0.0166, Train: 0.9979, Test: 0.7874\n",
            "Early stopping:  0.012531832471292642\n",
            "Epoch: 019, Loss: 0.0147, Train: 1.0000, Test: 0.7877\n",
            "Early stopping:  0.00747369745471355\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.77      0.79       372\n",
            "             ecology       0.83      0.80      0.81       859\n",
            "            economic       0.83      0.65      0.73      1379\n",
            "          geophysics       0.95      0.87      0.91      1172\n",
            "  gravitional_theory       0.56      0.95      0.70       100\n",
            "               hydro       0.68      0.84      0.75       325\n",
            "                math       0.88      0.79      0.83      1309\n",
            "              metals       0.68      0.89      0.77       171\n",
            "          networking       0.79      0.93      0.85       315\n",
            "        neuroscience       0.94      0.96      0.95       277\n",
            "        oceanography       0.81      0.84      0.83       960\n",
            "             politic       0.58      0.82      0.68       573\n",
            "           sociology       0.69      0.67      0.68       709\n",
            "software_engineering       0.90      0.76      0.82       494\n",
            "          statistics       0.65      0.84      0.73       617\n",
            "    theory_computing       0.73      0.68      0.71       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.77      0.82      0.78     10044\n",
            "        weighted avg       0.80      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6944, Train: 0.7333, Test: 0.6572\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0194, Train: 0.8396, Test: 0.7680\n",
            "Early stopping:  1.8914837162671858\n",
            "Epoch: 003, Loss: 0.5520, Train: 0.8667, Test: 0.7871\n",
            "Early stopping:  1.6955077962901457\n",
            "Epoch: 004, Loss: 0.4297, Train: 0.8917, Test: 0.7684\n",
            "Early stopping:  1.5348500040848154\n",
            "Epoch: 005, Loss: 0.3558, Train: 0.9208, Test: 0.7734\n",
            "Early stopping:  1.4124441170706041\n",
            "Epoch: 006, Loss: 0.3077, Train: 0.9479, Test: 0.7968\n",
            "Early stopping:  0.2871455965524266\n",
            "Epoch: 007, Loss: 0.1956, Train: 0.9250, Test: 0.7659\n",
            "Early stopping:  0.13340279168732927\n",
            "Epoch: 008, Loss: 0.2197, Train: 0.9583, Test: 0.7977\n",
            "Early stopping:  0.09660972547154809\n",
            "Epoch: 009, Loss: 0.1467, Train: 0.9750, Test: 0.8068\n",
            "Early stopping:  0.08506770551488434\n",
            "Epoch: 010, Loss: 0.1125, Train: 0.9729, Test: 0.8019\n",
            "Early stopping:  0.07490491414042418\n",
            "Epoch: 011, Loss: 0.0988, Train: 0.9750, Test: 0.7963\n",
            "Early stopping:  0.052135237972807\n",
            "Epoch: 012, Loss: 0.0819, Train: 0.9729, Test: 0.7901\n",
            "Early stopping:  0.054561651378529384\n",
            "Epoch: 013, Loss: 0.0599, Train: 0.9833, Test: 0.7755\n",
            "Early stopping:  0.03270823780138524\n",
            "Epoch: 014, Loss: 0.0550, Train: 0.9917, Test: 0.7771\n",
            "Early stopping:  0.024646838149430212\n",
            "Epoch: 015, Loss: 0.0410, Train: 0.9979, Test: 0.7880\n",
            "Early stopping:  0.022934529611942867\n",
            "Epoch: 016, Loss: 0.0265, Train: 0.9938, Test: 0.7849\n",
            "Early stopping:  0.020815395020570095\n",
            "Epoch: 017, Loss: 0.0324, Train: 0.9917, Test: 0.7853\n",
            "Early stopping:  0.01430415910850708\n",
            "Epoch: 018, Loss: 0.0294, Train: 1.0000, Test: 0.7891\n",
            "Early stopping:  0.011513204455336129\n",
            "Epoch: 019, Loss: 0.0173, Train: 0.9958, Test: 0.7872\n",
            "Early stopping:  0.008609526908885841\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.80      0.79       372\n",
            "             ecology       0.79      0.85      0.81       859\n",
            "            economic       0.79      0.65      0.71      1379\n",
            "          geophysics       0.87      0.91      0.89      1172\n",
            "  gravitional_theory       0.70      0.89      0.78       100\n",
            "               hydro       0.69      0.82      0.75       325\n",
            "                math       0.93      0.74      0.83      1309\n",
            "              metals       0.64      0.90      0.75       171\n",
            "          networking       0.74      0.83      0.78       315\n",
            "        neuroscience       0.94      0.96      0.95       277\n",
            "        oceanography       0.87      0.75      0.81       960\n",
            "             politic       0.70      0.73      0.71       573\n",
            "           sociology       0.64      0.73      0.68       709\n",
            "software_engineering       0.87      0.83      0.85       494\n",
            "          statistics       0.70      0.81      0.75       617\n",
            "    theory_computing       0.64      0.83      0.72       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.77      0.82      0.79     10044\n",
            "        weighted avg       0.80      0.79      0.79     10044\n",
            "\n",
            "time: 6.94 s (started: 2024-08-16 14:14:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8KdwabUKnn2",
        "outputId": "0d8ffe5c-3300-480b-988d-c91148f8af26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 477 ms (started: 2024-08-16 14:14:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "ZEGuodiPKnn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7d85hnXKnn3",
        "outputId": "adb77992-34f4-48ec-b1fd-53134db4cecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7895, Train: 0.6875, Test: 0.6507\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5234, Train: 0.7979, Test: 0.7442\n",
            "Early stopping:  0.18815735591894253\n",
            "Epoch: 003, Loss: 2.2002, Train: 0.8375, Test: 0.7665\n",
            "Early stopping:  0.29513687383457293\n",
            "Epoch: 004, Loss: 1.8254, Train: 0.8542, Test: 0.7719\n",
            "Early stopping:  0.4163118496928804\n",
            "Epoch: 005, Loss: 1.4530, Train: 0.8604, Test: 0.7814\n",
            "Early stopping:  0.5342854083405189\n",
            "Epoch: 006, Loss: 1.1192, Train: 0.8667, Test: 0.7969\n",
            "Early stopping:  0.5624106934632878\n",
            "Epoch: 007, Loss: 0.8435, Train: 0.8667, Test: 0.8071\n",
            "Early stopping:  0.5417021186916977\n",
            "Epoch: 008, Loss: 0.6361, Train: 0.8729, Test: 0.8127\n",
            "Early stopping:  0.4753286507184025\n",
            "Epoch: 009, Loss: 0.4928, Train: 0.8771, Test: 0.8187\n",
            "Early stopping:  0.384713244783109\n",
            "Epoch: 010, Loss: 0.3956, Train: 0.8917, Test: 0.8195\n",
            "Early stopping:  0.2897947700801969\n",
            "Epoch: 011, Loss: 0.3284, Train: 0.9083, Test: 0.8231\n",
            "Early stopping:  0.20566281849930024\n",
            "Epoch: 012, Loss: 0.2800, Train: 0.9167, Test: 0.8241\n",
            "Early stopping:  0.1417550795942546\n",
            "Epoch: 013, Loss: 0.2411, Train: 0.9271, Test: 0.8265\n",
            "Early stopping:  0.09959116150248984\n",
            "Epoch: 014, Loss: 0.2072, Train: 0.9500, Test: 0.8294\n",
            "Early stopping:  0.0740978094140872\n",
            "Epoch: 015, Loss: 0.1782, Train: 0.9521, Test: 0.8287\n",
            "Early stopping:  0.05930398387191989\n",
            "Epoch: 016, Loss: 0.1529, Train: 0.9604, Test: 0.8283\n",
            "Early stopping:  0.050313529441397456\n",
            "Epoch: 017, Loss: 0.1300, Train: 0.9750, Test: 0.8285\n",
            "Early stopping:  0.043843417011077385\n",
            "Epoch: 018, Loss: 0.1100, Train: 0.9833, Test: 0.8270\n",
            "Early stopping:  0.03844818052770608\n",
            "Epoch: 019, Loss: 0.0932, Train: 0.9917, Test: 0.8255\n",
            "Early stopping:  0.03376555351488701\n",
            "Epoch: 020, Loss: 0.0788, Train: 0.9958, Test: 0.8256\n",
            "Early stopping:  0.02937453643870911\n",
            "Epoch: 021, Loss: 0.0666, Train: 0.9979, Test: 0.8257\n",
            "Early stopping:  0.02511465617054907\n",
            "Epoch: 022, Loss: 0.0566, Train: 0.9979, Test: 0.8263\n",
            "Early stopping:  0.021229489428891336\n",
            "Epoch: 023, Loss: 0.0486, Train: 0.9979, Test: 0.8259\n",
            "Early stopping:  0.017743755324388686\n",
            "Epoch: 024, Loss: 0.0421, Train: 0.9979, Test: 0.8226\n",
            "Early stopping:  0.014561117539729124\n",
            "Epoch: 025, Loss: 0.0369, Train: 0.9979, Test: 0.8194\n",
            "Early stopping:  0.011745949071185037\n",
            "Epoch: 026, Loss: 0.0328, Train: 0.9979, Test: 0.8174\n",
            "Early stopping:  0.009420279306607876\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.80      0.80       372\n",
            "             ecology       0.83      0.81      0.82       859\n",
            "            economic       0.84      0.71      0.77      1379\n",
            "          geophysics       0.94      0.91      0.92      1172\n",
            "  gravitional_theory       0.79      0.92      0.85       100\n",
            "               hydro       0.79      0.85      0.82       325\n",
            "                math       0.93      0.79      0.86      1309\n",
            "              metals       0.59      0.92      0.72       171\n",
            "          networking       0.81      0.92      0.86       315\n",
            "        neuroscience       0.95      0.89      0.92       277\n",
            "        oceanography       0.82      0.87      0.85       960\n",
            "             politic       0.67      0.77      0.72       573\n",
            "           sociology       0.69      0.70      0.70       709\n",
            "software_engineering       0.85      0.87      0.86       494\n",
            "          statistics       0.76      0.84      0.80       617\n",
            "    theory_computing       0.72      0.84      0.78       412\n",
            "\n",
            "            accuracy                           0.82     10044\n",
            "           macro avg       0.80      0.84      0.81     10044\n",
            "        weighted avg       0.83      0.82      0.82     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7785, Train: 0.6417, Test: 0.5920\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5233, Train: 0.7771, Test: 0.7208\n",
            "Early stopping:  0.1804800545815921\n",
            "Epoch: 003, Loss: 2.2053, Train: 0.8292, Test: 0.7677\n",
            "Early stopping:  0.2872008987206719\n",
            "Epoch: 004, Loss: 1.8358, Train: 0.8396, Test: 0.7822\n",
            "Early stopping:  0.4075211535965998\n",
            "Epoch: 005, Loss: 1.4661, Train: 0.8417, Test: 0.7913\n",
            "Early stopping:  0.5251558846859462\n",
            "Epoch: 006, Loss: 1.1358, Train: 0.8417, Test: 0.7990\n",
            "Early stopping:  0.5558285764043616\n",
            "Epoch: 007, Loss: 0.8675, Train: 0.8625, Test: 0.8074\n",
            "Early stopping:  0.534775486740586\n",
            "Epoch: 008, Loss: 0.6679, Train: 0.8792, Test: 0.8112\n",
            "Early stopping:  0.46710036112832365\n",
            "Epoch: 009, Loss: 0.5309, Train: 0.8812, Test: 0.8123\n",
            "Early stopping:  0.37471225689418663\n",
            "Epoch: 010, Loss: 0.4399, Train: 0.8896, Test: 0.8120\n",
            "Early stopping:  0.278936923967956\n",
            "Epoch: 011, Loss: 0.3749, Train: 0.9000, Test: 0.8103\n",
            "Early stopping:  0.19647381751181106\n",
            "Epoch: 012, Loss: 0.3249, Train: 0.9125, Test: 0.8099\n",
            "Early stopping:  0.13585201064874058\n",
            "Epoch: 013, Loss: 0.2853, Train: 0.9187, Test: 0.8079\n",
            "Early stopping:  0.09716882697395997\n",
            "Epoch: 014, Loss: 0.2513, Train: 0.9292, Test: 0.8068\n",
            "Early stopping:  0.07448248210960338\n",
            "Epoch: 015, Loss: 0.2196, Train: 0.9313, Test: 0.8068\n",
            "Early stopping:  0.061029327044725745\n",
            "Epoch: 016, Loss: 0.1909, Train: 0.9375, Test: 0.8076\n",
            "Early stopping:  0.05285772758080572\n",
            "Epoch: 017, Loss: 0.1649, Train: 0.9437, Test: 0.8070\n",
            "Early stopping:  0.04768051302329447\n",
            "Epoch: 018, Loss: 0.1406, Train: 0.9604, Test: 0.8050\n",
            "Early stopping:  0.04371923534721704\n",
            "Epoch: 019, Loss: 0.1190, Train: 0.9812, Test: 0.8010\n",
            "Early stopping:  0.039847200816466505\n",
            "Epoch: 020, Loss: 0.1011, Train: 0.9896, Test: 0.7999\n",
            "Early stopping:  0.03574117933896285\n",
            "Epoch: 021, Loss: 0.0862, Train: 0.9958, Test: 0.7988\n",
            "Early stopping:  0.031265078185989566\n",
            "Epoch: 022, Loss: 0.0735, Train: 0.9958, Test: 0.7976\n",
            "Early stopping:  0.026529924876937602\n",
            "Epoch: 023, Loss: 0.0632, Train: 0.9958, Test: 0.7971\n",
            "Early stopping:  0.02211799177758418\n",
            "Epoch: 024, Loss: 0.0549, Train: 1.0000, Test: 0.7976\n",
            "Early stopping:  0.01837626031302683\n",
            "Epoch: 025, Loss: 0.0478, Train: 1.0000, Test: 0.7978\n",
            "Early stopping:  0.015204278090266535\n",
            "Epoch: 026, Loss: 0.0418, Train: 1.0000, Test: 0.7981\n",
            "Early stopping:  0.012539602219107413\n",
            "Epoch: 027, Loss: 0.0370, Train: 1.0000, Test: 0.7986\n",
            "Early stopping:  0.010411482747380577\n",
            "Epoch: 028, Loss: 0.0332, Train: 1.0000, Test: 0.7996\n",
            "Early stopping:  0.008629994173856952\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.83      0.80       372\n",
            "             ecology       0.83      0.78      0.80       859\n",
            "            economic       0.81      0.69      0.75      1379\n",
            "          geophysics       0.95      0.87      0.91      1172\n",
            "  gravitional_theory       0.86      0.89      0.88       100\n",
            "               hydro       0.74      0.89      0.81       325\n",
            "                math       0.92      0.79      0.85      1309\n",
            "              metals       0.51      0.94      0.66       171\n",
            "          networking       0.82      0.88      0.85       315\n",
            "        neuroscience       0.89      0.99      0.94       277\n",
            "        oceanography       0.85      0.88      0.86       960\n",
            "             politic       0.65      0.72      0.68       573\n",
            "           sociology       0.63      0.65      0.64       709\n",
            "software_engineering       0.86      0.82      0.84       494\n",
            "          statistics       0.71      0.84      0.77       617\n",
            "    theory_computing       0.70      0.80      0.75       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.78      0.83      0.80     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7899, Train: 0.5500, Test: 0.3835\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5454, Train: 0.8354, Test: 0.6862\n",
            "Early stopping:  0.17293239695026122\n",
            "Epoch: 003, Loss: 2.2497, Train: 0.8438, Test: 0.7223\n",
            "Early stopping:  0.27050602198254114\n",
            "Epoch: 004, Loss: 1.8967, Train: 0.8562, Test: 0.7443\n",
            "Early stopping:  0.3854033170495031\n",
            "Epoch: 005, Loss: 1.5309, Train: 0.8708, Test: 0.7668\n",
            "Early stopping:  0.5023733882815371\n",
            "Epoch: 006, Loss: 1.1929, Train: 0.8875, Test: 0.7883\n",
            "Early stopping:  0.5416833889540318\n",
            "Epoch: 007, Loss: 0.9096, Train: 0.8958, Test: 0.8030\n",
            "Early stopping:  0.5356316473735622\n",
            "Epoch: 008, Loss: 0.6924, Train: 0.8958, Test: 0.8126\n",
            "Early stopping:  0.4814058071351697\n",
            "Epoch: 009, Loss: 0.5388, Train: 0.8958, Test: 0.8190\n",
            "Early stopping:  0.3971311356071842\n",
            "Epoch: 010, Loss: 0.4345, Train: 0.9021, Test: 0.8205\n",
            "Early stopping:  0.30372838496523585\n",
            "Epoch: 011, Loss: 0.3616, Train: 0.9104, Test: 0.8208\n",
            "Early stopping:  0.21882724396252823\n",
            "Epoch: 012, Loss: 0.3071, Train: 0.9146, Test: 0.8214\n",
            "Early stopping:  0.15301493328583268\n",
            "Epoch: 013, Loss: 0.2632, Train: 0.9292, Test: 0.8224\n",
            "Early stopping:  0.10895757581263195\n",
            "Epoch: 014, Loss: 0.2255, Train: 0.9437, Test: 0.8229\n",
            "Early stopping:  0.08240316646016378\n",
            "Epoch: 015, Loss: 0.1933, Train: 0.9521, Test: 0.8227\n",
            "Early stopping:  0.06649354314553801\n",
            "Epoch: 016, Loss: 0.1665, Train: 0.9646, Test: 0.8221\n",
            "Early stopping:  0.05576224400744104\n",
            "Epoch: 017, Loss: 0.1439, Train: 0.9750, Test: 0.8213\n",
            "Early stopping:  0.04729682934138164\n",
            "Epoch: 018, Loss: 0.1239, Train: 0.9750, Test: 0.8215\n",
            "Early stopping:  0.04014640376858696\n",
            "Epoch: 019, Loss: 0.1060, Train: 0.9812, Test: 0.8202\n",
            "Early stopping:  0.03447016417039078\n",
            "Epoch: 020, Loss: 0.0901, Train: 0.9854, Test: 0.8201\n",
            "Early stopping:  0.030247417690065784\n",
            "Epoch: 021, Loss: 0.0763, Train: 0.9896, Test: 0.8185\n",
            "Early stopping:  0.02676426339930426\n",
            "Epoch: 022, Loss: 0.0648, Train: 0.9938, Test: 0.8184\n",
            "Early stopping:  0.023460639845382384\n",
            "Epoch: 023, Loss: 0.0552, Train: 0.9958, Test: 0.8160\n",
            "Early stopping:  0.020166320029125037\n",
            "Epoch: 024, Loss: 0.0474, Train: 0.9979, Test: 0.8130\n",
            "Early stopping:  0.01695925551334905\n",
            "Epoch: 025, Loss: 0.0411, Train: 1.0000, Test: 0.8121\n",
            "Early stopping:  0.013990760968703718\n",
            "Epoch: 026, Loss: 0.0362, Train: 1.0000, Test: 0.8111\n",
            "Early stopping:  0.011340435813314145\n",
            "Epoch: 027, Loss: 0.0323, Train: 1.0000, Test: 0.8110\n",
            "Early stopping:  0.009063608576374059\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.82      0.81       372\n",
            "             ecology       0.86      0.86      0.86       859\n",
            "            economic       0.83      0.71      0.77      1379\n",
            "          geophysics       0.96      0.85      0.91      1172\n",
            "  gravitional_theory       0.75      0.92      0.83       100\n",
            "               hydro       0.67      0.89      0.76       325\n",
            "                math       0.90      0.80      0.85      1309\n",
            "              metals       0.63      0.87      0.73       171\n",
            "          networking       0.76      0.95      0.85       315\n",
            "        neuroscience       0.96      0.95      0.95       277\n",
            "        oceanography       0.89      0.86      0.88       960\n",
            "             politic       0.68      0.74      0.71       573\n",
            "           sociology       0.61      0.70      0.66       709\n",
            "software_engineering       0.87      0.85      0.86       494\n",
            "          statistics       0.70      0.83      0.76       617\n",
            "    theory_computing       0.80      0.70      0.75       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7725, Train: 0.7146, Test: 0.6339\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5091, Train: 0.8146, Test: 0.7311\n",
            "Early stopping:  0.18622214122271633\n",
            "Epoch: 003, Loss: 2.1838, Train: 0.8396, Test: 0.7355\n",
            "Early stopping:  0.2948855051562514\n",
            "Epoch: 004, Loss: 1.8165, Train: 0.8458, Test: 0.7444\n",
            "Early stopping:  0.4133381091098523\n",
            "Epoch: 005, Loss: 1.4523, Train: 0.8521, Test: 0.7628\n",
            "Early stopping:  0.5280824425737051\n",
            "Epoch: 006, Loss: 1.1258, Train: 0.8625, Test: 0.7810\n",
            "Early stopping:  0.5532426984993425\n",
            "Epoch: 007, Loss: 0.8593, Train: 0.8854, Test: 0.7966\n",
            "Early stopping:  0.5291006929926712\n",
            "Epoch: 008, Loss: 0.6619, Train: 0.8854, Test: 0.8069\n",
            "Early stopping:  0.46191073095457796\n",
            "Epoch: 009, Loss: 0.5288, Train: 0.8896, Test: 0.8106\n",
            "Early stopping:  0.37042926659384484\n",
            "Epoch: 010, Loss: 0.4414, Train: 0.8917, Test: 0.8105\n",
            "Early stopping:  0.27457508117501794\n",
            "Epoch: 011, Loss: 0.3783, Train: 0.9042, Test: 0.8124\n",
            "Early stopping:  0.19173091283939733\n",
            "Epoch: 012, Loss: 0.3279, Train: 0.9187, Test: 0.8117\n",
            "Early stopping:  0.13200363864221015\n",
            "Epoch: 013, Loss: 0.2859, Train: 0.9229, Test: 0.8109\n",
            "Early stopping:  0.09579045802069905\n",
            "Epoch: 014, Loss: 0.2495, Train: 0.9313, Test: 0.8105\n",
            "Early stopping:  0.07575037810174111\n",
            "Epoch: 015, Loss: 0.2154, Train: 0.9521, Test: 0.8096\n",
            "Early stopping:  0.06411681012781228\n",
            "Epoch: 016, Loss: 0.1837, Train: 0.9625, Test: 0.8084\n",
            "Early stopping:  0.056827508555550174\n",
            "Epoch: 017, Loss: 0.1556, Train: 0.9667, Test: 0.8100\n",
            "Early stopping:  0.05168406488753809\n",
            "Epoch: 018, Loss: 0.1315, Train: 0.9708, Test: 0.8085\n",
            "Early stopping:  0.04687379337648121\n",
            "Epoch: 019, Loss: 0.1106, Train: 0.9854, Test: 0.8074\n",
            "Early stopping:  0.04155665420737038\n",
            "Epoch: 020, Loss: 0.0927, Train: 0.9875, Test: 0.8061\n",
            "Early stopping:  0.03603437081503897\n",
            "Epoch: 021, Loss: 0.0780, Train: 0.9917, Test: 0.8045\n",
            "Early stopping:  0.030821692636506245\n",
            "Epoch: 022, Loss: 0.0661, Train: 0.9938, Test: 0.8034\n",
            "Early stopping:  0.025994672974295623\n",
            "Epoch: 023, Loss: 0.0563, Train: 0.9979, Test: 0.8038\n",
            "Early stopping:  0.02150743238643621\n",
            "Epoch: 024, Loss: 0.0484, Train: 1.0000, Test: 0.8038\n",
            "Early stopping:  0.017539476511043005\n",
            "Epoch: 025, Loss: 0.0422, Train: 1.0000, Test: 0.8050\n",
            "Early stopping:  0.014220458812486991\n",
            "Epoch: 026, Loss: 0.0373, Train: 1.0000, Test: 0.8041\n",
            "Early stopping:  0.01144006357340633\n",
            "Epoch: 027, Loss: 0.0336, Train: 1.0000, Test: 0.8037\n",
            "Early stopping:  0.0090559725083334\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.82      0.80       372\n",
            "             ecology       0.87      0.77      0.82       859\n",
            "            economic       0.83      0.71      0.76      1379\n",
            "          geophysics       0.96      0.88      0.92      1172\n",
            "  gravitional_theory       0.75      0.95      0.84       100\n",
            "               hydro       0.78      0.84      0.81       325\n",
            "                math       0.94      0.74      0.83      1309\n",
            "              metals       0.53      0.94      0.68       171\n",
            "          networking       0.82      0.90      0.86       315\n",
            "        neuroscience       0.89      0.97      0.93       277\n",
            "        oceanography       0.82      0.90      0.86       960\n",
            "             politic       0.74      0.76      0.75       573\n",
            "           sociology       0.65      0.71      0.68       709\n",
            "software_engineering       0.88      0.83      0.85       494\n",
            "          statistics       0.71      0.82      0.76       617\n",
            "    theory_computing       0.55      0.80      0.65       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.78      0.83      0.80     10044\n",
            "        weighted avg       0.82      0.80      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7776, Train: 0.8083, Test: 0.6763\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5114, Train: 0.8583, Test: 0.7609\n",
            "Early stopping:  0.18819798548090702\n",
            "Epoch: 003, Loss: 2.1807, Train: 0.8625, Test: 0.7722\n",
            "Early stopping:  0.299015322677685\n",
            "Epoch: 004, Loss: 1.8054, Train: 0.8688, Test: 0.7773\n",
            "Early stopping:  0.42041622280474866\n",
            "Epoch: 005, Loss: 1.4357, Train: 0.8750, Test: 0.7845\n",
            "Early stopping:  0.5371359804109225\n",
            "Epoch: 006, Loss: 1.1061, Train: 0.8875, Test: 0.7941\n",
            "Early stopping:  0.5623461605286679\n",
            "Epoch: 007, Loss: 0.8350, Train: 0.8938, Test: 0.8024\n",
            "Early stopping:  0.537212179098389\n",
            "Epoch: 008, Loss: 0.6299, Train: 0.9083, Test: 0.8099\n",
            "Early stopping:  0.4695867292557351\n",
            "Epoch: 009, Loss: 0.4873, Train: 0.9125, Test: 0.8148\n",
            "Early stopping:  0.37979947949476733\n",
            "Epoch: 010, Loss: 0.3914, Train: 0.9229, Test: 0.8174\n",
            "Early stopping:  0.2863762843589555\n",
            "Epoch: 011, Loss: 0.3252, Train: 0.9271, Test: 0.8198\n",
            "Early stopping:  0.20365598304031654\n",
            "Epoch: 012, Loss: 0.2760, Train: 0.9354, Test: 0.8213\n",
            "Early stopping:  0.14064682601995732\n",
            "Epoch: 013, Loss: 0.2365, Train: 0.9396, Test: 0.8230\n",
            "Early stopping:  0.0991365455837519\n",
            "Epoch: 014, Loss: 0.2037, Train: 0.9437, Test: 0.8227\n",
            "Early stopping:  0.07411525584637627\n",
            "Epoch: 015, Loss: 0.1751, Train: 0.9604, Test: 0.8240\n",
            "Early stopping:  0.05925658031265442\n",
            "Epoch: 016, Loss: 0.1498, Train: 0.9688, Test: 0.8239\n",
            "Early stopping:  0.049796511838353714\n",
            "Epoch: 017, Loss: 0.1278, Train: 0.9771, Test: 0.8253\n",
            "Early stopping:  0.043021695820848374\n",
            "Epoch: 018, Loss: 0.1084, Train: 0.9812, Test: 0.8256\n",
            "Early stopping:  0.037713578970804525\n",
            "Epoch: 019, Loss: 0.0912, Train: 0.9875, Test: 0.8257\n",
            "Early stopping:  0.03318289979158013\n",
            "Epoch: 020, Loss: 0.0764, Train: 0.9917, Test: 0.8260\n",
            "Early stopping:  0.02910124891510564\n",
            "Epoch: 021, Loss: 0.0641, Train: 0.9917, Test: 0.8251\n",
            "Early stopping:  0.025301053668228195\n",
            "Epoch: 022, Loss: 0.0540, Train: 0.9938, Test: 0.8239\n",
            "Early stopping:  0.021589193979911884\n",
            "Epoch: 023, Loss: 0.0461, Train: 0.9958, Test: 0.8229\n",
            "Early stopping:  0.017906302107237627\n",
            "Epoch: 024, Loss: 0.0400, Train: 0.9979, Test: 0.8203\n",
            "Early stopping:  0.014485600112781691\n",
            "Epoch: 025, Loss: 0.0351, Train: 0.9979, Test: 0.8201\n",
            "Early stopping:  0.011491479190604307\n",
            "Epoch: 026, Loss: 0.0313, Train: 0.9979, Test: 0.8206\n",
            "Early stopping:  0.008993905431727406\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.81      0.78      0.80       372\n",
            "             ecology       0.84      0.86      0.85       859\n",
            "            economic       0.84      0.73      0.78      1379\n",
            "          geophysics       0.97      0.86      0.92      1172\n",
            "  gravitional_theory       0.82      0.93      0.87       100\n",
            "               hydro       0.73      0.88      0.80       325\n",
            "                math       0.85      0.86      0.86      1309\n",
            "              metals       0.62      0.93      0.74       171\n",
            "          networking       0.80      0.92      0.86       315\n",
            "        neuroscience       0.90      0.98      0.94       277\n",
            "        oceanography       0.87      0.86      0.86       960\n",
            "             politic       0.69      0.80      0.74       573\n",
            "           sociology       0.69      0.64      0.67       709\n",
            "software_engineering       0.86      0.85      0.85       494\n",
            "          statistics       0.75      0.81      0.78       617\n",
            "    theory_computing       0.79      0.76      0.78       412\n",
            "\n",
            "            accuracy                           0.82     10044\n",
            "           macro avg       0.80      0.84      0.82     10044\n",
            "        weighted avg       0.83      0.82      0.82     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7779, Train: 0.6729, Test: 0.6098\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4979, Train: 0.8042, Test: 0.7474\n",
            "Early stopping:  0.19800707441593846\n",
            "Epoch: 003, Loss: 2.1566, Train: 0.8250, Test: 0.7715\n",
            "Early stopping:  0.3111633662133304\n",
            "Epoch: 004, Loss: 1.7757, Train: 0.8417, Test: 0.7833\n",
            "Early stopping:  0.43318824848553095\n",
            "Epoch: 005, Loss: 1.4082, Train: 0.8583, Test: 0.8001\n",
            "Early stopping:  0.548186590579062\n",
            "Epoch: 006, Loss: 1.0876, Train: 0.8708, Test: 0.8084\n",
            "Early stopping:  0.5645012354927579\n",
            "Epoch: 007, Loss: 0.8313, Train: 0.8812, Test: 0.8162\n",
            "Early stopping:  0.5294382842169276\n",
            "Epoch: 008, Loss: 0.6428, Train: 0.8875, Test: 0.8173\n",
            "Early stopping:  0.4530281615883869\n",
            "Epoch: 009, Loss: 0.5132, Train: 0.8833, Test: 0.8173\n",
            "Early stopping:  0.35843430761779516\n",
            "Epoch: 010, Loss: 0.4271, Train: 0.8958, Test: 0.8153\n",
            "Early stopping:  0.2646137001252507\n",
            "Epoch: 011, Loss: 0.3671, Train: 0.9083, Test: 0.8144\n",
            "Early stopping:  0.18535301815383304\n",
            "Epoch: 012, Loss: 0.3196, Train: 0.9167, Test: 0.8170\n",
            "Early stopping:  0.12795808068506176\n",
            "Epoch: 013, Loss: 0.2784, Train: 0.9229, Test: 0.8199\n",
            "Early stopping:  0.09234033447960817\n",
            "Epoch: 014, Loss: 0.2429, Train: 0.9375, Test: 0.8242\n",
            "Early stopping:  0.07267169269071061\n",
            "Epoch: 015, Loss: 0.2123, Train: 0.9521, Test: 0.8248\n",
            "Early stopping:  0.06131812910530184\n",
            "Epoch: 016, Loss: 0.1849, Train: 0.9542, Test: 0.8227\n",
            "Early stopping:  0.053212966632917656\n",
            "Epoch: 017, Loss: 0.1600, Train: 0.9646, Test: 0.8225\n",
            "Early stopping:  0.04672523207648684\n",
            "Epoch: 018, Loss: 0.1371, Train: 0.9729, Test: 0.8202\n",
            "Early stopping:  0.04178319243984016\n",
            "Epoch: 019, Loss: 0.1168, Train: 0.9792, Test: 0.8188\n",
            "Early stopping:  0.03780117063104745\n",
            "Epoch: 020, Loss: 0.0996, Train: 0.9812, Test: 0.8157\n",
            "Early stopping:  0.033904189018954485\n",
            "Epoch: 021, Loss: 0.0850, Train: 0.9917, Test: 0.8101\n",
            "Early stopping:  0.02976801736264037\n",
            "Epoch: 022, Loss: 0.0727, Train: 0.9958, Test: 0.8063\n",
            "Early stopping:  0.025540696660454408\n",
            "Epoch: 023, Loss: 0.0626, Train: 1.0000, Test: 0.8058\n",
            "Early stopping:  0.021529712900353683\n",
            "Epoch: 024, Loss: 0.0543, Train: 1.0000, Test: 0.8069\n",
            "Early stopping:  0.01796980006346526\n",
            "Epoch: 025, Loss: 0.0476, Train: 1.0000, Test: 0.8069\n",
            "Early stopping:  0.014824207670656115\n",
            "Epoch: 026, Loss: 0.0421, Train: 1.0000, Test: 0.8055\n",
            "Early stopping:  0.012107403298546858\n",
            "Epoch: 027, Loss: 0.0375, Train: 1.0000, Test: 0.8043\n",
            "Early stopping:  0.009909304389279916\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.85      0.80       372\n",
            "             ecology       0.82      0.87      0.84       859\n",
            "            economic       0.83      0.62      0.71      1379\n",
            "          geophysics       0.92      0.92      0.92      1172\n",
            "  gravitional_theory       0.86      0.89      0.88       100\n",
            "               hydro       0.75      0.88      0.81       325\n",
            "                math       0.93      0.78      0.85      1309\n",
            "              metals       0.67      0.87      0.76       171\n",
            "          networking       0.81      0.92      0.86       315\n",
            "        neuroscience       0.91      0.97      0.94       277\n",
            "        oceanography       0.92      0.80      0.86       960\n",
            "             politic       0.67      0.76      0.71       573\n",
            "           sociology       0.57      0.75      0.65       709\n",
            "software_engineering       0.86      0.81      0.83       494\n",
            "          statistics       0.71      0.87      0.78       617\n",
            "    theory_computing       0.76      0.72      0.74       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.80      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.80      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7858, Train: 0.5646, Test: 0.4832\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5496, Train: 0.7708, Test: 0.6925\n",
            "Early stopping:  0.1670304894595837\n",
            "Epoch: 003, Loss: 2.2580, Train: 0.8021, Test: 0.6985\n",
            "Early stopping:  0.26437288253126506\n",
            "Epoch: 004, Loss: 1.9114, Train: 0.8104, Test: 0.7141\n",
            "Early stopping:  0.37766028383805733\n",
            "Epoch: 005, Loss: 1.5528, Train: 0.8250, Test: 0.7344\n",
            "Early stopping:  0.49249502452974675\n",
            "Epoch: 006, Loss: 1.2194, Train: 0.8438, Test: 0.7637\n",
            "Early stopping:  0.5324357824911632\n",
            "Epoch: 007, Loss: 0.9374, Train: 0.8688, Test: 0.7888\n",
            "Early stopping:  0.5275146077419497\n",
            "Epoch: 008, Loss: 0.7219, Train: 0.8812, Test: 0.8075\n",
            "Early stopping:  0.47562113568290526\n",
            "Epoch: 009, Loss: 0.5701, Train: 0.8896, Test: 0.8150\n",
            "Early stopping:  0.3936522638404735\n",
            "Epoch: 010, Loss: 0.4662, Train: 0.8938, Test: 0.8182\n",
            "Early stopping:  0.30155323294206043\n",
            "Epoch: 011, Loss: 0.3918, Train: 0.8917, Test: 0.8172\n",
            "Early stopping:  0.2175601690542631\n",
            "Epoch: 012, Loss: 0.3339, Train: 0.9146, Test: 0.8178\n",
            "Early stopping:  0.15375750342963923\n",
            "Epoch: 013, Loss: 0.2888, Train: 0.9208, Test: 0.8163\n",
            "Early stopping:  0.11135711987275877\n",
            "Epoch: 014, Loss: 0.2527, Train: 0.9313, Test: 0.8152\n",
            "Early stopping:  0.08466769167556885\n",
            "Epoch: 015, Loss: 0.2200, Train: 0.9375, Test: 0.8163\n",
            "Early stopping:  0.06765922087865262\n",
            "Epoch: 016, Loss: 0.1900, Train: 0.9479, Test: 0.8161\n",
            "Early stopping:  0.056555501617181286\n",
            "Epoch: 017, Loss: 0.1647, Train: 0.9625, Test: 0.8154\n",
            "Early stopping:  0.049253231352464545\n",
            "Epoch: 018, Loss: 0.1428, Train: 0.9708, Test: 0.8159\n",
            "Early stopping:  0.04363470927265596\n",
            "Epoch: 019, Loss: 0.1228, Train: 0.9792, Test: 0.8151\n",
            "Early stopping:  0.03832648122987739\n",
            "Epoch: 020, Loss: 0.1052, Train: 0.9854, Test: 0.8126\n",
            "Early stopping:  0.03353615349005041\n",
            "Epoch: 021, Loss: 0.0901, Train: 0.9938, Test: 0.8116\n",
            "Early stopping:  0.02962199144414238\n",
            "Epoch: 022, Loss: 0.0771, Train: 0.9938, Test: 0.8110\n",
            "Early stopping:  0.0260322593000433\n",
            "Epoch: 023, Loss: 0.0663, Train: 0.9938, Test: 0.8099\n",
            "Early stopping:  0.02239848518726387\n",
            "Epoch: 024, Loss: 0.0578, Train: 0.9958, Test: 0.8094\n",
            "Early stopping:  0.01883760334001823\n",
            "Epoch: 025, Loss: 0.0509, Train: 1.0000, Test: 0.8090\n",
            "Early stopping:  0.01557132558631248\n",
            "Epoch: 026, Loss: 0.0449, Train: 1.0000, Test: 0.8081\n",
            "Early stopping:  0.01272558635352216\n",
            "Epoch: 027, Loss: 0.0401, Train: 1.0000, Test: 0.8073\n",
            "Early stopping:  0.010389093631311526\n",
            "Epoch: 028, Loss: 0.0367, Train: 1.0000, Test: 0.8074\n",
            "Early stopping:  0.008460674310097492\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.87      0.63      0.73       372\n",
            "             ecology       0.79      0.87      0.83       859\n",
            "            economic       0.83      0.74      0.78      1379\n",
            "          geophysics       0.94      0.87      0.90      1172\n",
            "  gravitional_theory       0.84      0.93      0.88       100\n",
            "               hydro       0.72      0.82      0.77       325\n",
            "                math       0.93      0.76      0.84      1309\n",
            "              metals       0.51      0.95      0.67       171\n",
            "          networking       0.79      0.91      0.85       315\n",
            "        neuroscience       0.94      0.94      0.94       277\n",
            "        oceanography       0.87      0.82      0.84       960\n",
            "             politic       0.72      0.76      0.74       573\n",
            "           sociology       0.67      0.72      0.69       709\n",
            "software_engineering       0.81      0.86      0.83       494\n",
            "          statistics       0.76      0.87      0.81       617\n",
            "    theory_computing       0.68      0.82      0.74       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7795, Train: 0.6354, Test: 0.5481\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5185, Train: 0.7958, Test: 0.7115\n",
            "Early stopping:  0.1845290180243367\n",
            "Epoch: 003, Loss: 2.1945, Train: 0.8146, Test: 0.7327\n",
            "Early stopping:  0.29303387723310953\n",
            "Epoch: 004, Loss: 1.8264, Train: 0.8354, Test: 0.7472\n",
            "Early stopping:  0.412116639831918\n",
            "Epoch: 005, Loss: 1.4629, Train: 0.8458, Test: 0.7705\n",
            "Early stopping:  0.5269426249250058\n",
            "Epoch: 006, Loss: 1.1349, Train: 0.8604, Test: 0.7930\n",
            "Early stopping:  0.5533832876627269\n",
            "Epoch: 007, Loss: 0.8625, Train: 0.8688, Test: 0.8046\n",
            "Early stopping:  0.5314950036399365\n",
            "Epoch: 008, Loss: 0.6577, Train: 0.8729, Test: 0.8110\n",
            "Early stopping:  0.46719726252228333\n",
            "Epoch: 009, Loss: 0.5181, Train: 0.8792, Test: 0.8125\n",
            "Early stopping:  0.3788900661087621\n",
            "Epoch: 010, Loss: 0.4266, Train: 0.8896, Test: 0.8159\n",
            "Early stopping:  0.2842289774098055\n",
            "Epoch: 011, Loss: 0.3627, Train: 0.9021, Test: 0.8180\n",
            "Early stopping:  0.19962947728238317\n",
            "Epoch: 012, Loss: 0.3133, Train: 0.9083, Test: 0.8184\n",
            "Early stopping:  0.13646966807484445\n",
            "Epoch: 013, Loss: 0.2725, Train: 0.9229, Test: 0.8182\n",
            "Early stopping:  0.09689483102903446\n",
            "Epoch: 014, Loss: 0.2372, Train: 0.9375, Test: 0.8173\n",
            "Early stopping:  0.07470006752933842\n",
            "Epoch: 015, Loss: 0.2051, Train: 0.9563, Test: 0.8179\n",
            "Early stopping:  0.062093125424094606\n",
            "Epoch: 016, Loss: 0.1759, Train: 0.9646, Test: 0.8177\n",
            "Early stopping:  0.05420751879532982\n",
            "Epoch: 017, Loss: 0.1492, Train: 0.9708, Test: 0.8177\n",
            "Early stopping:  0.048745725777838284\n",
            "Epoch: 018, Loss: 0.1248, Train: 0.9792, Test: 0.8173\n",
            "Early stopping:  0.04442404177889051\n",
            "Epoch: 019, Loss: 0.1031, Train: 0.9812, Test: 0.8167\n",
            "Early stopping:  0.040388990526707975\n",
            "Epoch: 020, Loss: 0.0850, Train: 0.9896, Test: 0.8156\n",
            "Early stopping:  0.03610602360277257\n",
            "Epoch: 021, Loss: 0.0704, Train: 0.9979, Test: 0.8149\n",
            "Early stopping:  0.03135512495976185\n",
            "Epoch: 022, Loss: 0.0586, Train: 1.0000, Test: 0.8143\n",
            "Early stopping:  0.026307516640714974\n",
            "Epoch: 023, Loss: 0.0488, Train: 1.0000, Test: 0.8141\n",
            "Early stopping:  0.02152865029529475\n",
            "Epoch: 024, Loss: 0.0410, Train: 1.0000, Test: 0.8146\n",
            "Early stopping:  0.01748159473727212\n",
            "Epoch: 025, Loss: 0.0351, Train: 1.0000, Test: 0.8135\n",
            "Early stopping:  0.014067385680323112\n",
            "Epoch: 026, Loss: 0.0309, Train: 1.0000, Test: 0.8118\n",
            "Early stopping:  0.011061702094797416\n",
            "Epoch: 027, Loss: 0.0277, Train: 1.0000, Test: 0.8099\n",
            "Early stopping:  0.008390181458735591\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.83      0.78       372\n",
            "             ecology       0.81      0.87      0.84       859\n",
            "            economic       0.81      0.73      0.77      1379\n",
            "          geophysics       0.96      0.90      0.93      1172\n",
            "  gravitional_theory       0.76      0.93      0.84       100\n",
            "               hydro       0.75      0.85      0.80       325\n",
            "                math       0.88      0.78      0.83      1309\n",
            "              metals       0.60      0.91      0.72       171\n",
            "          networking       0.76      0.90      0.83       315\n",
            "        neuroscience       0.96      0.94      0.95       277\n",
            "        oceanography       0.87      0.86      0.87       960\n",
            "             politic       0.74      0.75      0.74       573\n",
            "           sociology       0.70      0.66      0.68       709\n",
            "software_engineering       0.82      0.87      0.84       494\n",
            "          statistics       0.73      0.75      0.74       617\n",
            "    theory_computing       0.68      0.74      0.71       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7646, Train: 0.6500, Test: 0.4983\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4724, Train: 0.8313, Test: 0.6876\n",
            "Early stopping:  0.20660165976427997\n",
            "Epoch: 003, Loss: 2.1284, Train: 0.8562, Test: 0.7423\n",
            "Early stopping:  0.318449130276049\n",
            "Epoch: 004, Loss: 1.7422, Train: 0.8583, Test: 0.7700\n",
            "Early stopping:  0.4412435564515281\n",
            "Epoch: 005, Loss: 1.3656, Train: 0.8646, Test: 0.7875\n",
            "Early stopping:  0.5586849019428209\n",
            "Epoch: 006, Loss: 1.0380, Train: 0.8688, Test: 0.8016\n",
            "Early stopping:  0.5744159001121728\n",
            "Epoch: 007, Loss: 0.7788, Train: 0.8646, Test: 0.8077\n",
            "Early stopping:  0.5397169293028222\n",
            "Epoch: 008, Loss: 0.5911, Train: 0.8875, Test: 0.8096\n",
            "Early stopping:  0.4606612504927431\n",
            "Epoch: 009, Loss: 0.4650, Train: 0.8979, Test: 0.8103\n",
            "Early stopping:  0.36106319770792883\n",
            "Epoch: 010, Loss: 0.3815, Train: 0.9042, Test: 0.8124\n",
            "Early stopping:  0.26312268248539256\n",
            "Epoch: 011, Loss: 0.3227, Train: 0.9083, Test: 0.8163\n",
            "Early stopping:  0.18193818375870893\n",
            "Epoch: 012, Loss: 0.2772, Train: 0.9125, Test: 0.8196\n",
            "Early stopping:  0.12435675507854255\n",
            "Epoch: 013, Loss: 0.2398, Train: 0.9292, Test: 0.8209\n",
            "Early stopping:  0.08887957193565894\n",
            "Epoch: 014, Loss: 0.2079, Train: 0.9479, Test: 0.8193\n",
            "Early stopping:  0.06854556340922029\n",
            "Epoch: 015, Loss: 0.1799, Train: 0.9563, Test: 0.8170\n",
            "Early stopping:  0.05638318979842885\n",
            "Epoch: 016, Loss: 0.1549, Train: 0.9604, Test: 0.8170\n",
            "Early stopping:  0.048280423009537304\n",
            "Epoch: 017, Loss: 0.1324, Train: 0.9750, Test: 0.8173\n",
            "Early stopping:  0.04242139201065533\n",
            "Epoch: 018, Loss: 0.1124, Train: 0.9812, Test: 0.8163\n",
            "Early stopping:  0.03775718103030321\n",
            "Epoch: 019, Loss: 0.0955, Train: 0.9896, Test: 0.8145\n",
            "Early stopping:  0.0335081299899065\n",
            "Epoch: 020, Loss: 0.0814, Train: 0.9938, Test: 0.8128\n",
            "Early stopping:  0.029204626539626237\n",
            "Epoch: 021, Loss: 0.0697, Train: 0.9938, Test: 0.8121\n",
            "Early stopping:  0.024885980373362895\n",
            "Epoch: 022, Loss: 0.0597, Train: 0.9979, Test: 0.8107\n",
            "Early stopping:  0.020868705442110193\n",
            "Epoch: 023, Loss: 0.0516, Train: 0.9979, Test: 0.8106\n",
            "Early stopping:  0.017393529869539924\n",
            "Epoch: 024, Loss: 0.0453, Train: 0.9979, Test: 0.8105\n",
            "Early stopping:  0.014382309512809244\n",
            "Epoch: 025, Loss: 0.0402, Train: 1.0000, Test: 0.8098\n",
            "Early stopping:  0.01171222526294645\n",
            "Epoch: 026, Loss: 0.0359, Train: 1.0000, Test: 0.8090\n",
            "Early stopping:  0.009428302258129411\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.83      0.76      0.79       372\n",
            "             ecology       0.84      0.81      0.83       859\n",
            "            economic       0.86      0.65      0.74      1379\n",
            "          geophysics       0.94      0.90      0.92      1172\n",
            "  gravitional_theory       0.67      0.93      0.78       100\n",
            "               hydro       0.76      0.82      0.79       325\n",
            "                math       0.89      0.81      0.85      1309\n",
            "              metals       0.67      0.92      0.78       171\n",
            "          networking       0.77      0.92      0.84       315\n",
            "        neuroscience       0.91      0.96      0.94       277\n",
            "        oceanography       0.83      0.88      0.85       960\n",
            "             politic       0.63      0.87      0.73       573\n",
            "           sociology       0.67      0.69      0.68       709\n",
            "software_engineering       0.93      0.81      0.87       494\n",
            "          statistics       0.71      0.81      0.76       617\n",
            "    theory_computing       0.73      0.81      0.77       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7874, Train: 0.6042, Test: 0.4579\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5308, Train: 0.7896, Test: 0.6932\n",
            "Early stopping:  0.18144690328643204\n",
            "Epoch: 003, Loss: 2.2117, Train: 0.8229, Test: 0.7497\n",
            "Early stopping:  0.2884286657436697\n",
            "Epoch: 004, Loss: 1.8464, Train: 0.8396, Test: 0.7717\n",
            "Early stopping:  0.406876879716627\n",
            "Epoch: 005, Loss: 1.4824, Train: 0.8542, Test: 0.7832\n",
            "Early stopping:  0.5221531809278938\n",
            "Epoch: 006, Loss: 1.1543, Train: 0.8604, Test: 0.7940\n",
            "Early stopping:  0.550762718582208\n",
            "Epoch: 007, Loss: 0.8829, Train: 0.8750, Test: 0.8036\n",
            "Early stopping:  0.5305500365119887\n",
            "Epoch: 008, Loss: 0.6759, Train: 0.8938, Test: 0.8115\n",
            "Early stopping:  0.46759739314683635\n",
            "Epoch: 009, Loss: 0.5301, Train: 0.9062, Test: 0.8128\n",
            "Early stopping:  0.38114084590823777\n",
            "Epoch: 010, Loss: 0.4317, Train: 0.9062, Test: 0.8108\n",
            "Early stopping:  0.2894542797884415\n",
            "Epoch: 011, Loss: 0.3646, Train: 0.9104, Test: 0.8133\n",
            "Early stopping:  0.20723728768833807\n",
            "Epoch: 012, Loss: 0.3163, Train: 0.9187, Test: 0.8166\n",
            "Early stopping:  0.1431908338467165\n",
            "Epoch: 013, Loss: 0.2774, Train: 0.9333, Test: 0.8210\n",
            "Early stopping:  0.09992804274685649\n",
            "Epoch: 014, Loss: 0.2436, Train: 0.9333, Test: 0.8225\n",
            "Early stopping:  0.07399237551245337\n",
            "Epoch: 015, Loss: 0.2131, Train: 0.9417, Test: 0.8218\n",
            "Early stopping:  0.05964792473569364\n",
            "Epoch: 016, Loss: 0.1847, Train: 0.9500, Test: 0.8207\n",
            "Early stopping:  0.051892369917673584\n",
            "Epoch: 017, Loss: 0.1585, Train: 0.9646, Test: 0.8212\n",
            "Early stopping:  0.0469777782115334\n",
            "Epoch: 018, Loss: 0.1353, Train: 0.9771, Test: 0.8188\n",
            "Early stopping:  0.04291920823507486\n",
            "Epoch: 019, Loss: 0.1149, Train: 0.9833, Test: 0.8180\n",
            "Early stopping:  0.038916962376648816\n",
            "Epoch: 020, Loss: 0.0976, Train: 0.9854, Test: 0.8176\n",
            "Early stopping:  0.03453627258002987\n",
            "Epoch: 021, Loss: 0.0834, Train: 0.9875, Test: 0.8149\n",
            "Early stopping:  0.02986345902105372\n",
            "Epoch: 022, Loss: 0.0717, Train: 0.9917, Test: 0.8126\n",
            "Early stopping:  0.025254496056241547\n",
            "Epoch: 023, Loss: 0.0624, Train: 0.9958, Test: 0.8122\n",
            "Early stopping:  0.02085156085557745\n",
            "Epoch: 024, Loss: 0.0549, Train: 0.9958, Test: 0.8117\n",
            "Early stopping:  0.016951012596725167\n",
            "Epoch: 025, Loss: 0.0487, Train: 0.9958, Test: 0.8107\n",
            "Early stopping:  0.013739394332257595\n",
            "Epoch: 026, Loss: 0.0433, Train: 1.0000, Test: 0.8104\n",
            "Early stopping:  0.011212217257895047\n",
            "Epoch: 027, Loss: 0.0388, Train: 1.0000, Test: 0.8114\n",
            "Early stopping:  0.009311263019337712\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.83      0.80       372\n",
            "             ecology       0.82      0.87      0.85       859\n",
            "            economic       0.81      0.68      0.74      1379\n",
            "          geophysics       0.88      0.93      0.91      1172\n",
            "  gravitional_theory       0.81      0.88      0.84       100\n",
            "               hydro       0.81      0.78      0.79       325\n",
            "                math       0.93      0.77      0.84      1309\n",
            "              metals       0.65      0.93      0.76       171\n",
            "          networking       0.79      0.93      0.86       315\n",
            "        neuroscience       0.95      0.98      0.97       277\n",
            "        oceanography       0.89      0.83      0.86       960\n",
            "             politic       0.70      0.77      0.73       573\n",
            "           sociology       0.68      0.74      0.71       709\n",
            "software_engineering       0.88      0.83      0.86       494\n",
            "          statistics       0.74      0.77      0.76       617\n",
            "    theory_computing       0.66      0.83      0.73       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.80      0.84      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "time: 12.2 s (started: 2024-08-16 14:14:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylq7DgQpKnn3",
        "outputId": "d0307a6b-b0c4-4a1b-b7e6-543ac0daea55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 438 ms (started: 2024-08-16 14:14:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 80% ❎"
      ],
      "metadata": {
        "id": "vCZztommtuWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "tZ7_c9TatuWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN','80%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e70edd2-7c57-44da-e9b3-2b5c32e85b1e",
        "id": "nTuWqiEatuWh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4239, Train: 0.1653, Test: 0.1721\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 12.5963, Train: 0.1701, Test: 0.1716\n",
            "Early stopping:  5.0716402002967955\n",
            "Epoch: 003, Loss: 12.6517, Train: 0.2808, Test: 0.2779\n",
            "Early stopping:  4.157081515254105\n",
            "Epoch: 004, Loss: 9.7003, Train: 0.3666, Test: 0.3700\n",
            "Early stopping:  3.4043249504935438\n",
            "Epoch: 005, Loss: 6.1940, Train: 0.3171, Test: 0.3063\n",
            "Early stopping:  3.4252941513867863\n",
            "Epoch: 006, Loss: 6.8460, Train: 0.3470, Test: 0.3395\n",
            "Early stopping:  3.061305589056082\n",
            "Epoch: 007, Loss: 5.9098, Train: 0.3689, Test: 0.3489\n",
            "Early stopping:  2.8789463899110386\n",
            "Epoch: 008, Loss: 4.6041, Train: 0.4345, Test: 0.4068\n",
            "Early stopping:  1.8897503744647153\n",
            "Epoch: 009, Loss: 3.3294, Train: 0.4780, Test: 0.4658\n",
            "Early stopping:  1.4052968555629086\n",
            "Epoch: 010, Loss: 2.5441, Train: 0.4861, Test: 0.4742\n",
            "Early stopping:  1.7741348707177251\n",
            "Epoch: 011, Loss: 2.1945, Train: 0.4786, Test: 0.4742\n",
            "Early stopping:  1.5361664649930546\n",
            "Epoch: 012, Loss: 2.1153, Train: 0.4884, Test: 0.4884\n",
            "Early stopping:  1.0382682358694975\n",
            "Epoch: 013, Loss: 2.0185, Train: 0.5059, Test: 0.5058\n",
            "Early stopping:  0.5350744034233902\n",
            "Epoch: 014, Loss: 1.9285, Train: 0.5261, Test: 0.5263\n",
            "Early stopping:  0.236830303491132\n",
            "Epoch: 015, Loss: 1.8811, Train: 0.5350, Test: 0.5305\n",
            "Early stopping:  0.12932903921228825\n",
            "Epoch: 016, Loss: 1.8514, Train: 0.5363, Test: 0.5284\n",
            "Early stopping:  0.10785576630700999\n",
            "Epoch: 017, Loss: 1.8195, Train: 0.5354, Test: 0.5168\n",
            "Early stopping:  0.07755088704364024\n",
            "Epoch: 018, Loss: 1.7841, Train: 0.5366, Test: 0.5174\n",
            "Early stopping:  0.0555964886155765\n",
            "Epoch: 019, Loss: 1.7418, Train: 0.5397, Test: 0.5263\n",
            "Early stopping:  0.054821297976712625\n",
            "Epoch: 020, Loss: 1.6967, Train: 0.5466, Test: 0.5374\n",
            "Early stopping:  0.06135692955299135\n",
            "Epoch: 021, Loss: 1.6570, Train: 0.5588, Test: 0.5426\n",
            "Early stopping:  0.0652464804202298\n",
            "Epoch: 022, Loss: 1.6251, Train: 0.5614, Test: 0.5505\n",
            "Early stopping:  0.0638124747128677\n",
            "Epoch: 023, Loss: 1.6043, Train: 0.5637, Test: 0.5558\n",
            "Early stopping:  0.05532674466113909\n",
            "Epoch: 024, Loss: 1.5740, Train: 0.5722, Test: 0.5658\n",
            "Early stopping:  0.04740354960759753\n",
            "Epoch: 025, Loss: 1.5304, Train: 0.5862, Test: 0.5732\n",
            "Early stopping:  0.04847202464756509\n",
            "Epoch: 026, Loss: 1.4832, Train: 0.6005, Test: 0.5884\n",
            "Early stopping:  0.05724978422253327\n",
            "Epoch: 027, Loss: 1.4394, Train: 0.6064, Test: 0.5968\n",
            "Early stopping:  0.0666775130726855\n",
            "Epoch: 028, Loss: 1.4055, Train: 0.6084, Test: 0.6032\n",
            "Early stopping:  0.06776952358042676\n",
            "Epoch: 029, Loss: 1.3797, Train: 0.6100, Test: 0.6095\n",
            "Early stopping:  0.06036474469905804\n",
            "Epoch: 030, Loss: 1.3561, Train: 0.6184, Test: 0.6158\n",
            "Early stopping:  0.05008137574560487\n",
            "Epoch: 031, Loss: 1.3315, Train: 0.6284, Test: 0.6200\n",
            "Early stopping:  0.04206559863247094\n",
            "Epoch: 032, Loss: 1.3079, Train: 0.6358, Test: 0.6237\n",
            "Early stopping:  0.03849524940905926\n",
            "Epoch: 033, Loss: 1.2856, Train: 0.6436, Test: 0.6284\n",
            "Early stopping:  0.03736403055476659\n",
            "Epoch: 034, Loss: 1.2641, Train: 0.6467, Test: 0.6342\n",
            "Early stopping:  0.036363395513895634\n",
            "Epoch: 035, Loss: 1.2449, Train: 0.6522, Test: 0.6358\n",
            "Early stopping:  0.03432456443909158\n",
            "Epoch: 036, Loss: 1.2284, Train: 0.6571, Test: 0.6342\n",
            "Early stopping:  0.03164643925943185\n",
            "Epoch: 037, Loss: 1.2139, Train: 0.6596, Test: 0.6305\n",
            "Early stopping:  0.02842707638177374\n",
            "Epoch: 038, Loss: 1.1999, Train: 0.6641, Test: 0.6358\n",
            "Early stopping:  0.02527757007331365\n",
            "Epoch: 039, Loss: 1.1845, Train: 0.6687, Test: 0.6342\n",
            "Early stopping:  0.023613604012597846\n",
            "Epoch: 040, Loss: 1.1680, Train: 0.6729, Test: 0.6379\n",
            "Early stopping:  0.023758674791758357\n",
            "Epoch: 041, Loss: 1.1521, Train: 0.6754, Test: 0.6447\n",
            "Early stopping:  0.024603882984558716\n",
            "Epoch: 042, Loss: 1.1386, Train: 0.6787, Test: 0.6474\n",
            "Early stopping:  0.024509142899299435\n",
            "Epoch: 043, Loss: 1.1267, Train: 0.6814, Test: 0.6484\n",
            "Early stopping:  0.022974579760014876\n",
            "Epoch: 044, Loss: 1.1146, Train: 0.6839, Test: 0.6505\n",
            "Early stopping:  0.02090978643820986\n",
            "Epoch: 045, Loss: 1.1010, Train: 0.6874, Test: 0.6500\n",
            "Early stopping:  0.019941110464412406\n",
            "Epoch: 046, Loss: 1.0867, Train: 0.6922, Test: 0.6516\n",
            "Early stopping:  0.020512335716433395\n",
            "Epoch: 047, Loss: 1.0733, Train: 0.6963, Test: 0.6505\n",
            "Early stopping:  0.02132595677813182\n",
            "Epoch: 048, Loss: 1.0606, Train: 0.6976, Test: 0.6511\n",
            "Early stopping:  0.021477737856859302\n",
            "Epoch: 049, Loss: 1.0478, Train: 0.7000, Test: 0.6521\n",
            "Early stopping:  0.020945661118540838\n",
            "Epoch: 050, Loss: 1.0353, Train: 0.7049, Test: 0.6521\n",
            "Early stopping:  0.020267012409659356\n",
            "Epoch: 051, Loss: 1.0233, Train: 0.7075, Test: 0.6532\n",
            "Early stopping:  0.019798668723245347\n",
            "Epoch: 052, Loss: 1.0120, Train: 0.7096, Test: 0.6542\n",
            "Early stopping:  0.019248298592190856\n",
            "Epoch: 053, Loss: 1.0013, Train: 0.7121, Test: 0.6621\n",
            "Early stopping:  0.01841133510047832\n",
            "Epoch: 054, Loss: 0.9909, Train: 0.7146, Test: 0.6626\n",
            "Early stopping:  0.017532566699218734\n",
            "Epoch: 055, Loss: 0.9806, Train: 0.7178, Test: 0.6626\n",
            "Early stopping:  0.016844812623615354\n",
            "Epoch: 056, Loss: 0.9706, Train: 0.7203, Test: 0.6616\n",
            "Early stopping:  0.016352640094311838\n",
            "Epoch: 057, Loss: 0.9607, Train: 0.7233, Test: 0.6589\n",
            "Early stopping:  0.01603096270012212\n",
            "Epoch: 058, Loss: 0.9509, Train: 0.7242, Test: 0.6574\n",
            "Early stopping:  0.015786022167001812\n",
            "Epoch: 059, Loss: 0.9412, Train: 0.7274, Test: 0.6589\n",
            "Early stopping:  0.015572815168514064\n",
            "Epoch: 060, Loss: 0.9309, Train: 0.7312, Test: 0.6605\n",
            "Early stopping:  0.015652151253346434\n",
            "Epoch: 061, Loss: 0.9204, Train: 0.7351, Test: 0.6632\n",
            "Early stopping:  0.015926041812441668\n",
            "Epoch: 062, Loss: 0.9102, Train: 0.7382, Test: 0.6600\n",
            "Early stopping:  0.016166887271059752\n",
            "Epoch: 063, Loss: 0.9005, Train: 0.7417, Test: 0.6626\n",
            "Early stopping:  0.016156446042344622\n",
            "Epoch: 064, Loss: 0.8909, Train: 0.7439, Test: 0.6616\n",
            "Early stopping:  0.015799009773888168\n",
            "Epoch: 065, Loss: 0.8812, Train: 0.7445, Test: 0.6637\n",
            "Early stopping:  0.015466555144039251\n",
            "Epoch: 066, Loss: 0.8713, Train: 0.7486, Test: 0.6642\n",
            "Early stopping:  0.015361889479492147\n",
            "Epoch: 067, Loss: 0.8620, Train: 0.7511, Test: 0.6621\n",
            "Early stopping:  0.015270509499633149\n",
            "Epoch: 068, Loss: 0.8530, Train: 0.7526, Test: 0.6632\n",
            "Early stopping:  0.015011758069938753\n",
            "Epoch: 069, Loss: 0.8441, Train: 0.7561, Test: 0.6653\n",
            "Early stopping:  0.014612497527103446\n",
            "Epoch: 070, Loss: 0.8352, Train: 0.7591, Test: 0.6658\n",
            "Early stopping:  0.0142489003532951\n",
            "Epoch: 071, Loss: 0.8265, Train: 0.7607, Test: 0.6647\n",
            "Early stopping:  0.014046312518466036\n",
            "Epoch: 072, Loss: 0.8179, Train: 0.7634, Test: 0.6668\n",
            "Early stopping:  0.013904503686034731\n",
            "Epoch: 073, Loss: 0.8096, Train: 0.7634, Test: 0.6668\n",
            "Early stopping:  0.013633304260739054\n",
            "Epoch: 074, Loss: 0.8021, Train: 0.7692, Test: 0.6689\n",
            "Early stopping:  0.013139044506301444\n",
            "Epoch: 075, Loss: 0.7955, Train: 0.7676, Test: 0.6684\n",
            "Early stopping:  0.012316018204185115\n",
            "Epoch: 076, Loss: 0.7877, Train: 0.7746, Test: 0.6663\n",
            "Early stopping:  0.011781906658248175\n",
            "Epoch: 077, Loss: 0.7788, Train: 0.7780, Test: 0.6684\n",
            "Early stopping:  0.012036764427484126\n",
            "Epoch: 078, Loss: 0.7718, Train: 0.7774, Test: 0.6695\n",
            "Early stopping:  0.012233818650007722\n",
            "Epoch: 079, Loss: 0.7660, Train: 0.7812, Test: 0.6705\n",
            "Early stopping:  0.011885652988251167\n",
            "Epoch: 080, Loss: 0.7589, Train: 0.7826, Test: 0.6695\n",
            "Early stopping:  0.011169581185853383\n",
            "Epoch: 081, Loss: 0.7509, Train: 0.7828, Test: 0.6716\n",
            "Early stopping:  0.010894971410284865\n",
            "Epoch: 082, Loss: 0.7447, Train: 0.7849, Test: 0.6737\n",
            "Early stopping:  0.010968593175075462\n",
            "Epoch: 083, Loss: 0.7392, Train: 0.7875, Test: 0.6721\n",
            "Early stopping:  0.010753373509270275\n",
            "Epoch: 084, Loss: 0.7311, Train: 0.7895, Test: 0.6747\n",
            "Early stopping:  0.010662799593181997\n",
            "Epoch: 085, Loss: 0.7255, Train: 0.7887, Test: 0.6758\n",
            "Early stopping:  0.010174042834463666\n",
            "Epoch: 086, Loss: 0.7218, Train: 0.7911, Test: 0.6779\n",
            "Early stopping:  0.009477208247899155\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.80      0.74      0.77        96\n",
            "            CAD_and_CAM       0.70      0.79      0.74        92\n",
            "              Companies       0.47      0.47      0.47        85\n",
            "       Computer_Science       0.67      0.73      0.70        88\n",
            "            Consultants       0.58      0.69      0.63        96\n",
            "           Data_Formats       0.74      0.76      0.75       103\n",
            "    Data_Communications       0.78      0.74      0.76       110\n",
            "              Education       0.81      0.79      0.80        97\n",
            "               Graphics       0.80      0.84      0.82       109\n",
            "               Hardware       0.60      0.67      0.63        93\n",
            "               Internet       0.67      0.53      0.59       110\n",
            "       Mobile_Computing       0.78      0.70      0.74        87\n",
            "             Multimedia       0.67      0.76      0.71        98\n",
            "            Open_Source       0.70      0.60      0.64       114\n",
            "            Programming       0.53      0.53      0.53        98\n",
            "               Robotics       0.92      0.90      0.91       105\n",
            "               Security       0.77      0.86      0.81       107\n",
            "               Software       0.30      0.26      0.28       109\n",
            "                Systems       0.55      0.55      0.55       103\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.68      0.68      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.3125, Train: 0.1188, Test: 0.1121\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 7.7659, Train: 0.1670, Test: 0.1716\n",
            "Early stopping:  1.7347841780208955\n",
            "Epoch: 003, Loss: 11.6801, Train: 0.3338, Test: 0.3453\n",
            "Early stopping:  3.2115713074603778\n",
            "Epoch: 004, Loss: 9.9249, Train: 0.3617, Test: 0.3553\n",
            "Early stopping:  2.752287925847119\n",
            "Epoch: 005, Loss: 8.3579, Train: 0.3314, Test: 0.3311\n",
            "Early stopping:  2.387657144502172\n",
            "Epoch: 006, Loss: 7.6708, Train: 0.3718, Test: 0.3558\n",
            "Early stopping:  1.7108873112488998\n",
            "Epoch: 007, Loss: 6.0698, Train: 0.3980, Test: 0.3932\n",
            "Early stopping:  2.148690713528631\n",
            "Epoch: 008, Loss: 4.5321, Train: 0.4450, Test: 0.4395\n",
            "Early stopping:  2.080904812773993\n",
            "Epoch: 009, Loss: 3.1492, Train: 0.4891, Test: 0.4795\n",
            "Early stopping:  2.158001719367549\n",
            "Epoch: 010, Loss: 2.3812, Train: 0.4980, Test: 0.4932\n",
            "Early stopping:  2.15018554256216\n",
            "Epoch: 011, Loss: 2.1093, Train: 0.4595, Test: 0.4521\n",
            "Early stopping:  1.6483091469911257\n",
            "Epoch: 012, Loss: 2.0410, Train: 0.4575, Test: 0.4584\n",
            "Early stopping:  1.041831025372623\n",
            "Epoch: 013, Loss: 1.9590, Train: 0.4808, Test: 0.4932\n",
            "Early stopping:  0.48568736479251573\n",
            "Epoch: 014, Loss: 1.8446, Train: 0.5084, Test: 0.5179\n",
            "Early stopping:  0.20146573778453664\n",
            "Epoch: 015, Loss: 1.7452, Train: 0.5370, Test: 0.5284\n",
            "Early stopping:  0.1468968419030819\n",
            "Epoch: 016, Loss: 1.6882, Train: 0.5503, Test: 0.5463\n",
            "Early stopping:  0.14612060376217995\n",
            "Epoch: 017, Loss: 1.6779, Train: 0.5503, Test: 0.5437\n",
            "Early stopping:  0.11855348497974041\n",
            "Epoch: 018, Loss: 1.6739, Train: 0.5464, Test: 0.5421\n",
            "Early stopping:  0.07225536018510861\n",
            "Epoch: 019, Loss: 1.6552, Train: 0.5561, Test: 0.5484\n",
            "Early stopping:  0.03406065036290205\n",
            "Epoch: 020, Loss: 1.6215, Train: 0.5663, Test: 0.5579\n",
            "Early stopping:  0.026247387276168765\n",
            "Epoch: 021, Loss: 1.5822, Train: 0.5812, Test: 0.5658\n",
            "Early stopping:  0.04023454061575514\n",
            "Epoch: 022, Loss: 1.5407, Train: 0.5928, Test: 0.5742\n",
            "Early stopping:  0.054141811078869805\n",
            "Epoch: 023, Loss: 1.4985, Train: 0.6054, Test: 0.5889\n",
            "Early stopping:  0.06239227789874312\n",
            "Epoch: 024, Loss: 1.4592, Train: 0.6132, Test: 0.5968\n",
            "Early stopping:  0.06456989383021466\n",
            "Epoch: 025, Loss: 1.4255, Train: 0.6175, Test: 0.6000\n",
            "Early stopping:  0.06250890376596473\n",
            "Epoch: 026, Loss: 1.3973, Train: 0.6232, Test: 0.6021\n",
            "Early stopping:  0.057089902526369786\n",
            "Epoch: 027, Loss: 1.3707, Train: 0.6266, Test: 0.6032\n",
            "Early stopping:  0.05038145735111513\n",
            "Epoch: 028, Loss: 1.3448, Train: 0.6345, Test: 0.6074\n",
            "Early stopping:  0.044895283132226946\n",
            "Epoch: 029, Loss: 1.3220, Train: 0.6412, Test: 0.6126\n",
            "Early stopping:  0.041039820093397204\n",
            "Epoch: 030, Loss: 1.3011, Train: 0.6468, Test: 0.6205\n",
            "Early stopping:  0.038151994916122874\n",
            "Epoch: 031, Loss: 1.2796, Train: 0.6514, Test: 0.6268\n",
            "Early stopping:  0.0357675852787645\n",
            "Epoch: 032, Loss: 1.2582, Train: 0.6534, Test: 0.6300\n",
            "Early stopping:  0.03408853080068282\n",
            "Epoch: 033, Loss: 1.2393, Train: 0.6593, Test: 0.6274\n",
            "Early stopping:  0.03295547388472625\n",
            "Epoch: 034, Loss: 1.2225, Train: 0.6614, Test: 0.6316\n",
            "Early stopping:  0.03127014675890157\n",
            "Epoch: 035, Loss: 1.2061, Train: 0.6680, Test: 0.6332\n",
            "Early stopping:  0.02892749595030283\n",
            "Epoch: 036, Loss: 1.1891, Train: 0.6729, Test: 0.6358\n",
            "Early stopping:  0.027116892118380653\n",
            "Epoch: 037, Loss: 1.1730, Train: 0.6774, Test: 0.6395\n",
            "Early stopping:  0.02623351886614585\n",
            "Epoch: 038, Loss: 1.1582, Train: 0.6786, Test: 0.6447\n",
            "Early stopping:  0.02556129607982264\n",
            "Epoch: 039, Loss: 1.1437, Train: 0.6816, Test: 0.6474\n",
            "Early stopping:  0.024611268106999262\n",
            "Epoch: 040, Loss: 1.1293, Train: 0.6850, Test: 0.6458\n",
            "Early stopping:  0.02355623836453409\n",
            "Epoch: 041, Loss: 1.1150, Train: 0.6874, Test: 0.6500\n",
            "Early stopping:  0.022915354409022023\n",
            "Epoch: 042, Loss: 1.1009, Train: 0.6909, Test: 0.6505\n",
            "Early stopping:  0.022653821127505615\n",
            "Epoch: 043, Loss: 1.0875, Train: 0.6945, Test: 0.6511\n",
            "Early stopping:  0.02225686084047582\n",
            "Epoch: 044, Loss: 1.0752, Train: 0.6946, Test: 0.6516\n",
            "Early stopping:  0.02146067860892689\n",
            "Epoch: 045, Loss: 1.0631, Train: 0.6987, Test: 0.6521\n",
            "Early stopping:  0.02049242831112441\n",
            "Epoch: 046, Loss: 1.0510, Train: 0.7001, Test: 0.6505\n",
            "Early stopping:  0.01966331289408762\n",
            "Epoch: 047, Loss: 1.0393, Train: 0.7030, Test: 0.6505\n",
            "Early stopping:  0.01909779587269283\n",
            "Epoch: 048, Loss: 1.0282, Train: 0.7046, Test: 0.6558\n",
            "Early stopping:  0.01865653147974514\n",
            "Epoch: 049, Loss: 1.0172, Train: 0.7086, Test: 0.6605\n",
            "Early stopping:  0.018140282071794495\n",
            "Epoch: 050, Loss: 1.0062, Train: 0.7114, Test: 0.6605\n",
            "Early stopping:  0.017642398416721303\n",
            "Epoch: 051, Loss: 0.9952, Train: 0.7157, Test: 0.6642\n",
            "Early stopping:  0.017387864070060682\n",
            "Epoch: 052, Loss: 0.9846, Train: 0.7186, Test: 0.6663\n",
            "Early stopping:  0.01724652489837393\n",
            "Epoch: 053, Loss: 0.9746, Train: 0.7201, Test: 0.6674\n",
            "Early stopping:  0.016879087352495045\n",
            "Epoch: 054, Loss: 0.9653, Train: 0.7222, Test: 0.6668\n",
            "Early stopping:  0.016203654146711384\n",
            "Epoch: 055, Loss: 0.9556, Train: 0.7246, Test: 0.6705\n",
            "Early stopping:  0.015581893284925971\n",
            "Epoch: 056, Loss: 0.9457, Train: 0.7283, Test: 0.6732\n",
            "Early stopping:  0.015321390994560452\n",
            "Epoch: 057, Loss: 0.9362, Train: 0.7305, Test: 0.6742\n",
            "Early stopping:  0.015264801591080481\n",
            "Epoch: 058, Loss: 0.9274, Train: 0.7314, Test: 0.6732\n",
            "Early stopping:  0.015059269231482802\n",
            "Epoch: 059, Loss: 0.9190, Train: 0.7338, Test: 0.6737\n",
            "Early stopping:  0.014465918843380783\n",
            "Epoch: 060, Loss: 0.9107, Train: 0.7383, Test: 0.6763\n",
            "Early stopping:  0.013771431650504422\n",
            "Epoch: 061, Loss: 0.9024, Train: 0.7392, Test: 0.6774\n",
            "Early stopping:  0.013344372601041068\n",
            "Epoch: 062, Loss: 0.8942, Train: 0.7413, Test: 0.6753\n",
            "Early stopping:  0.013136676012804922\n",
            "Epoch: 063, Loss: 0.8860, Train: 0.7447, Test: 0.6768\n",
            "Early stopping:  0.013060157088032355\n",
            "Epoch: 064, Loss: 0.8780, Train: 0.7462, Test: 0.6811\n",
            "Early stopping:  0.012918500822510868\n",
            "Epoch: 065, Loss: 0.8702, Train: 0.7479, Test: 0.6847\n",
            "Early stopping:  0.012726335787471604\n",
            "Epoch: 066, Loss: 0.8626, Train: 0.7511, Test: 0.6853\n",
            "Early stopping:  0.012487325811831944\n",
            "Epoch: 067, Loss: 0.8547, Train: 0.7528, Test: 0.6863\n",
            "Early stopping:  0.012343776691030506\n",
            "Epoch: 068, Loss: 0.8470, Train: 0.7557, Test: 0.6847\n",
            "Early stopping:  0.012259123254479582\n",
            "Epoch: 069, Loss: 0.8393, Train: 0.7570, Test: 0.6874\n",
            "Early stopping:  0.012238323052719347\n",
            "Epoch: 070, Loss: 0.8319, Train: 0.7576, Test: 0.6868\n",
            "Early stopping:  0.012133800649019241\n",
            "Epoch: 071, Loss: 0.8250, Train: 0.7604, Test: 0.6879\n",
            "Early stopping:  0.011768838680821833\n",
            "Epoch: 072, Loss: 0.8184, Train: 0.7638, Test: 0.6858\n",
            "Early stopping:  0.011320448175476316\n",
            "Epoch: 073, Loss: 0.8124, Train: 0.7650, Test: 0.6879\n",
            "Early stopping:  0.010660995325172984\n",
            "Epoch: 074, Loss: 0.8046, Train: 0.7658, Test: 0.6879\n",
            "Early stopping:  0.010657704302219483\n",
            "Epoch: 075, Loss: 0.7985, Train: 0.7691, Test: 0.6858\n",
            "Early stopping:  0.010584900087438507\n",
            "Epoch: 076, Loss: 0.7931, Train: 0.7721, Test: 0.6895\n",
            "Early stopping:  0.010228561661411228\n",
            "Epoch: 077, Loss: 0.7847, Train: 0.7699, Test: 0.6884\n",
            "Early stopping:  0.010593130852659635\n",
            "Epoch: 078, Loss: 0.7811, Train: 0.7757, Test: 0.6863\n",
            "Early stopping:  0.009655050657762142\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 14, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.73      0.76      0.74       109\n",
            "            CAD_and_CAM       0.68      0.76      0.72        94\n",
            "              Companies       0.49      0.46      0.48        95\n",
            "       Computer_Science       0.76      0.73      0.75       119\n",
            "            Consultants       0.47      0.52      0.49        94\n",
            "           Data_Formats       0.82      0.71      0.76       112\n",
            "    Data_Communications       0.75      0.75      0.75       111\n",
            "              Education       0.91      0.92      0.91       106\n",
            "               Graphics       0.81      0.85      0.83       104\n",
            "               Hardware       0.63      0.68      0.66        94\n",
            "               Internet       0.56      0.65      0.60        82\n",
            "       Mobile_Computing       0.76      0.72      0.74        85\n",
            "             Multimedia       0.66      0.72      0.69        83\n",
            "            Open_Source       0.67      0.77      0.72       101\n",
            "            Programming       0.54      0.60      0.57        97\n",
            "               Robotics       0.96      0.88      0.92       108\n",
            "               Security       0.75      0.78      0.77       103\n",
            "               Software       0.28      0.22      0.25        94\n",
            "                Systems       0.66      0.48      0.55       109\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.69      0.69      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.9366, Train: 0.2466, Test: 0.2489\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 10.6697, Train: 0.2059, Test: 0.2195\n",
            "Early stopping:  4.053901367516104\n",
            "Epoch: 003, Loss: 9.7448, Train: 0.2667, Test: 0.2705\n",
            "Early stopping:  3.077939232890603\n",
            "Epoch: 004, Loss: 8.1261, Train: 0.3279, Test: 0.3337\n",
            "Early stopping:  2.5183506110143217\n",
            "Epoch: 005, Loss: 6.4897, Train: 0.3659, Test: 0.3595\n",
            "Early stopping:  2.337337448734257\n",
            "Epoch: 006, Loss: 4.6449, Train: 0.3939, Test: 0.3811\n",
            "Early stopping:  2.4343339455662947\n",
            "Epoch: 007, Loss: 3.3945, Train: 0.4395, Test: 0.4316\n",
            "Early stopping:  2.5620572216688777\n",
            "Epoch: 008, Loss: 2.5565, Train: 0.4889, Test: 0.4784\n",
            "Early stopping:  2.2725140570852798\n",
            "Epoch: 009, Loss: 2.0564, Train: 0.5049, Test: 0.4947\n",
            "Early stopping:  1.7815211354050113\n",
            "Epoch: 010, Loss: 1.8829, Train: 0.4914, Test: 0.4974\n",
            "Early stopping:  1.1350451594670299\n",
            "Epoch: 011, Loss: 1.8498, Train: 0.5088, Test: 0.5111\n",
            "Early stopping:  0.6496657059842349\n",
            "Epoch: 012, Loss: 1.7877, Train: 0.5353, Test: 0.5395\n",
            "Early stopping:  0.31252301823958917\n",
            "Epoch: 013, Loss: 1.6936, Train: 0.5550, Test: 0.5574\n",
            "Early stopping:  0.13411535737378713\n",
            "Epoch: 014, Loss: 1.6152, Train: 0.5793, Test: 0.5884\n",
            "Early stopping:  0.1108211967472182\n",
            "Epoch: 015, Loss: 1.5422, Train: 0.6043, Test: 0.6021\n",
            "Early stopping:  0.12472517357948135\n",
            "Epoch: 016, Loss: 1.4783, Train: 0.6228, Test: 0.6126\n",
            "Early stopping:  0.12211438972789689\n",
            "Epoch: 017, Loss: 1.4251, Train: 0.6270, Test: 0.6137\n",
            "Early stopping:  0.10686071359477606\n",
            "Epoch: 018, Loss: 1.3817, Train: 0.6318, Test: 0.6174\n",
            "Early stopping:  0.09285703620837076\n",
            "Epoch: 019, Loss: 1.3499, Train: 0.6339, Test: 0.6205\n",
            "Early stopping:  0.07672893800614218\n",
            "Epoch: 020, Loss: 1.3302, Train: 0.6347, Test: 0.6242\n",
            "Early stopping:  0.05962991869999984\n",
            "Epoch: 021, Loss: 1.3158, Train: 0.6359, Test: 0.6195\n",
            "Early stopping:  0.043714857305763376\n",
            "Epoch: 022, Loss: 1.2971, Train: 0.6416, Test: 0.6211\n",
            "Early stopping:  0.03253146314171232\n",
            "Epoch: 023, Loss: 1.2715, Train: 0.6489, Test: 0.6279\n",
            "Early stopping:  0.030175813489812187\n",
            "Epoch: 024, Loss: 1.2438, Train: 0.6528, Test: 0.6405\n",
            "Early stopping:  0.03461292601760443\n",
            "Epoch: 025, Loss: 1.2179, Train: 0.6638, Test: 0.6426\n",
            "Early stopping:  0.03945313165147818\n",
            "Epoch: 026, Loss: 1.1954, Train: 0.6697, Test: 0.6463\n",
            "Early stopping:  0.04063815823384275\n",
            "Epoch: 027, Loss: 1.1744, Train: 0.6749, Test: 0.6579\n",
            "Early stopping:  0.038433802757444645\n",
            "Epoch: 028, Loss: 1.1535, Train: 0.6786, Test: 0.6611\n",
            "Early stopping:  0.03547389210453638\n",
            "Epoch: 029, Loss: 1.1333, Train: 0.6837, Test: 0.6632\n",
            "Early stopping:  0.033394990743432613\n",
            "Epoch: 030, Loss: 1.1137, Train: 0.6847, Test: 0.6584\n",
            "Early stopping:  0.03233791946057076\n",
            "Epoch: 031, Loss: 1.0953, Train: 0.6862, Test: 0.6589\n",
            "Early stopping:  0.03129524569124821\n",
            "Epoch: 032, Loss: 1.0772, Train: 0.6930, Test: 0.6600\n",
            "Early stopping:  0.030131868636136733\n",
            "Epoch: 033, Loss: 1.0602, Train: 0.6978, Test: 0.6637\n",
            "Early stopping:  0.028904266782078455\n",
            "Epoch: 034, Loss: 1.0437, Train: 0.7021, Test: 0.6689\n",
            "Early stopping:  0.02772742928008084\n",
            "Epoch: 035, Loss: 1.0301, Train: 0.7061, Test: 0.6726\n",
            "Early stopping:  0.025988851935652965\n",
            "Epoch: 036, Loss: 1.0169, Train: 0.7104, Test: 0.6763\n",
            "Early stopping:  0.02387759825807913\n",
            "Epoch: 037, Loss: 1.0028, Train: 0.7158, Test: 0.6758\n",
            "Early stopping:  0.0223869045256054\n",
            "Epoch: 038, Loss: 0.9875, Train: 0.7158, Test: 0.6758\n",
            "Early stopping:  0.022098619049596716\n",
            "Epoch: 039, Loss: 0.9737, Train: 0.7192, Test: 0.6758\n",
            "Early stopping:  0.02248293740135586\n",
            "Epoch: 040, Loss: 0.9613, Train: 0.7228, Test: 0.6816\n",
            "Early stopping:  0.022195300234334917\n",
            "Epoch: 041, Loss: 0.9500, Train: 0.7259, Test: 0.6805\n",
            "Early stopping:  0.020855716711672256\n",
            "Epoch: 042, Loss: 0.9382, Train: 0.7274, Test: 0.6847\n",
            "Early stopping:  0.019346936102995316\n",
            "Epoch: 043, Loss: 0.9255, Train: 0.7303, Test: 0.6863\n",
            "Early stopping:  0.01890402207264053\n",
            "Epoch: 044, Loss: 0.9121, Train: 0.7355, Test: 0.6874\n",
            "Early stopping:  0.019465573764105425\n",
            "Epoch: 045, Loss: 0.8995, Train: 0.7387, Test: 0.6874\n",
            "Early stopping:  0.020117633303636008\n",
            "Epoch: 046, Loss: 0.8878, Train: 0.7420, Test: 0.6916\n",
            "Early stopping:  0.02006544724132574\n",
            "Epoch: 047, Loss: 0.8767, Train: 0.7463, Test: 0.6932\n",
            "Early stopping:  0.019296944313731045\n",
            "Epoch: 048, Loss: 0.8660, Train: 0.7503, Test: 0.6932\n",
            "Early stopping:  0.01816844862499232\n",
            "Epoch: 049, Loss: 0.8550, Train: 0.7526, Test: 0.6900\n",
            "Early stopping:  0.01749023951504551\n",
            "Epoch: 050, Loss: 0.8449, Train: 0.7543, Test: 0.6889\n",
            "Early stopping:  0.0169933331361998\n",
            "Epoch: 051, Loss: 0.8354, Train: 0.7583, Test: 0.6905\n",
            "Early stopping:  0.016414413625365856\n",
            "Epoch: 052, Loss: 0.8260, Train: 0.7604, Test: 0.6889\n",
            "Early stopping:  0.01576395463129064\n",
            "Epoch: 053, Loss: 0.8164, Train: 0.7632, Test: 0.6916\n",
            "Early stopping:  0.01520693242548727\n",
            "Epoch: 054, Loss: 0.8066, Train: 0.7653, Test: 0.6926\n",
            "Early stopping:  0.015118458780804929\n",
            "Epoch: 055, Loss: 0.7971, Train: 0.7693, Test: 0.6932\n",
            "Early stopping:  0.015197590101973085\n",
            "Epoch: 056, Loss: 0.7880, Train: 0.7712, Test: 0.6926\n",
            "Early stopping:  0.015063505240130157\n",
            "Epoch: 057, Loss: 0.7792, Train: 0.7746, Test: 0.6937\n",
            "Early stopping:  0.014686242881540188\n",
            "Epoch: 058, Loss: 0.7701, Train: 0.7776, Test: 0.6921\n",
            "Early stopping:  0.014352309175839982\n",
            "Epoch: 059, Loss: 0.7612, Train: 0.7797, Test: 0.6942\n",
            "Early stopping:  0.01418319749677269\n",
            "Epoch: 060, Loss: 0.7526, Train: 0.7811, Test: 0.6937\n",
            "Early stopping:  0.01405173984585784\n",
            "Epoch: 061, Loss: 0.7444, Train: 0.7838, Test: 0.6958\n",
            "Early stopping:  0.01377614231679562\n",
            "Epoch: 062, Loss: 0.7361, Train: 0.7853, Test: 0.6968\n",
            "Early stopping:  0.013400526524005424\n",
            "Epoch: 063, Loss: 0.7278, Train: 0.7891, Test: 0.6974\n",
            "Early stopping:  0.013173302684002993\n",
            "Epoch: 064, Loss: 0.7195, Train: 0.7901, Test: 0.6984\n",
            "Early stopping:  0.013092400129810689\n",
            "Epoch: 065, Loss: 0.7115, Train: 0.7926, Test: 0.6958\n",
            "Early stopping:  0.013041411943134119\n",
            "Epoch: 066, Loss: 0.7035, Train: 0.7949, Test: 0.6963\n",
            "Early stopping:  0.012905900443030194\n",
            "Epoch: 067, Loss: 0.6955, Train: 0.7987, Test: 0.6979\n",
            "Early stopping:  0.01273785351200539\n",
            "Epoch: 068, Loss: 0.6877, Train: 0.7999, Test: 0.6979\n",
            "Early stopping:  0.01259550614052047\n",
            "Epoch: 069, Loss: 0.6800, Train: 0.8036, Test: 0.6984\n",
            "Early stopping:  0.012444947673917519\n",
            "Epoch: 070, Loss: 0.6728, Train: 0.8045, Test: 0.6968\n",
            "Early stopping:  0.012149734148701078\n",
            "Epoch: 071, Loss: 0.6666, Train: 0.8050, Test: 0.6989\n",
            "Early stopping:  0.011508340869185485\n",
            "Epoch: 072, Loss: 0.6626, Train: 0.8036, Test: 0.6921\n",
            "Early stopping:  0.01011698829401328\n",
            "Epoch: 073, Loss: 0.6564, Train: 0.8083, Test: 0.6984\n",
            "Early stopping:  0.009107562609659186\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.75      0.69      0.72        93\n",
            "            CAD_and_CAM       0.76      0.77      0.76       101\n",
            "              Companies       0.51      0.66      0.58        82\n",
            "       Computer_Science       0.72      0.75      0.74        93\n",
            "            Consultants       0.57      0.67      0.62        98\n",
            "           Data_Formats       0.78      0.83      0.81       110\n",
            "    Data_Communications       0.73      0.73      0.73        98\n",
            "              Education       0.85      0.93      0.89        94\n",
            "               Graphics       0.88      0.86      0.87       111\n",
            "               Hardware       0.69      0.65      0.67       101\n",
            "               Internet       0.67      0.59      0.63       103\n",
            "       Mobile_Computing       0.74      0.71      0.72        94\n",
            "             Multimedia       0.60      0.71      0.65        89\n",
            "            Open_Source       0.69      0.66      0.67        92\n",
            "            Programming       0.59      0.56      0.58       109\n",
            "               Robotics       0.90      0.85      0.88       116\n",
            "               Security       0.87      0.79      0.83        95\n",
            "               Software       0.31      0.29      0.30       100\n",
            "                Systems       0.65      0.56      0.60       121\n",
            "\n",
            "               accuracy                           0.70      1900\n",
            "              macro avg       0.70      0.70      0.70      1900\n",
            "           weighted avg       0.70      0.70      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4510, Train: 0.1259, Test: 0.1237\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 15.8626, Train: 0.2059, Test: 0.1937\n",
            "Early stopping:  7.362155864042935\n",
            "Epoch: 003, Loss: 12.6815, Train: 0.1483, Test: 0.1532\n",
            "Early stopping:  5.335460803753879\n",
            "Epoch: 004, Loss: 14.1048, Train: 0.2372, Test: 0.2484\n",
            "Early stopping:  4.571717810773416\n",
            "Epoch: 005, Loss: 8.7252, Train: 0.2799, Test: 0.2795\n",
            "Early stopping:  4.225296553071645\n",
            "Epoch: 006, Loss: 7.7943, Train: 0.3172, Test: 0.3158\n",
            "Early stopping:  3.4673016923918176\n",
            "Epoch: 007, Loss: 7.0640, Train: 0.3416, Test: 0.3300\n",
            "Early stopping:  3.1274246968871955\n",
            "Epoch: 008, Loss: 5.9707, Train: 0.3937, Test: 0.3900\n",
            "Early stopping:  3.1683315339159908\n",
            "Epoch: 009, Loss: 4.1858, Train: 0.4311, Test: 0.4279\n",
            "Early stopping:  1.7516229994792785\n",
            "Epoch: 010, Loss: 3.0142, Train: 0.4278, Test: 0.4205\n",
            "Early stopping:  1.9862957742025766\n",
            "Epoch: 011, Loss: 2.5420, Train: 0.4404, Test: 0.4347\n",
            "Early stopping:  1.9275522908190494\n",
            "Epoch: 012, Loss: 2.2557, Train: 0.4503, Test: 0.4395\n",
            "Early stopping:  1.5192916581349047\n",
            "Epoch: 013, Loss: 2.0680, Train: 0.4586, Test: 0.4458\n",
            "Early stopping:  0.8461926147078235\n",
            "Epoch: 014, Loss: 1.9644, Train: 0.4699, Test: 0.4553\n",
            "Early stopping:  0.42229670466773195\n",
            "Epoch: 015, Loss: 1.8767, Train: 0.5009, Test: 0.4842\n",
            "Early stopping:  0.26471912668840936\n",
            "Epoch: 016, Loss: 1.7898, Train: 0.5349, Test: 0.5068\n",
            "Early stopping:  0.1804477224684668\n",
            "Epoch: 017, Loss: 1.7271, Train: 0.5439, Test: 0.5295\n",
            "Early stopping:  0.13588242265534528\n",
            "Epoch: 018, Loss: 1.6869, Train: 0.5526, Test: 0.5268\n",
            "Early stopping:  0.11259912013271058\n",
            "Epoch: 019, Loss: 1.6486, Train: 0.5564, Test: 0.5284\n",
            "Early stopping:  0.08993322316465069\n",
            "Epoch: 020, Loss: 1.6106, Train: 0.5614, Test: 0.5395\n",
            "Early stopping:  0.0695412407356648\n",
            "Epoch: 021, Loss: 1.5754, Train: 0.5686, Test: 0.5542\n",
            "Early stopping:  0.06006423264325888\n",
            "Epoch: 022, Loss: 1.5380, Train: 0.5833, Test: 0.5647\n",
            "Early stopping:  0.05868258343879888\n",
            "Epoch: 023, Loss: 1.4970, Train: 0.5939, Test: 0.5747\n",
            "Early stopping:  0.05942426297755751\n",
            "Epoch: 024, Loss: 1.4557, Train: 0.6033, Test: 0.5895\n",
            "Early stopping:  0.061403831381676995\n",
            "Epoch: 025, Loss: 1.4166, Train: 0.6116, Test: 0.6011\n",
            "Early stopping:  0.06323326817179029\n",
            "Epoch: 026, Loss: 1.3823, Train: 0.6180, Test: 0.6100\n",
            "Early stopping:  0.06199762114561587\n",
            "Epoch: 027, Loss: 1.3550, Train: 0.6214, Test: 0.6121\n",
            "Early stopping:  0.056701440550763635\n",
            "Epoch: 028, Loss: 1.3342, Train: 0.6292, Test: 0.6089\n",
            "Early stopping:  0.04849911612679764\n",
            "Epoch: 029, Loss: 1.3164, Train: 0.6337, Test: 0.6100\n",
            "Early stopping:  0.03964305872645341\n",
            "Epoch: 030, Loss: 1.2981, Train: 0.6359, Test: 0.6147\n",
            "Early stopping:  0.032863229243603215\n",
            "Epoch: 031, Loss: 1.2782, Train: 0.6401, Test: 0.6158\n",
            "Early stopping:  0.03000269292357295\n",
            "Epoch: 032, Loss: 1.2574, Train: 0.6496, Test: 0.6258\n",
            "Early stopping:  0.03034204362125147\n",
            "Epoch: 033, Loss: 1.2373, Train: 0.6537, Test: 0.6279\n",
            "Early stopping:  0.03144805819997895\n",
            "Epoch: 034, Loss: 1.2187, Train: 0.6584, Test: 0.6321\n",
            "Early stopping:  0.03159616944898387\n",
            "Epoch: 035, Loss: 1.2018, Train: 0.6622, Test: 0.6358\n",
            "Early stopping:  0.030329828411161684\n",
            "Epoch: 036, Loss: 1.1870, Train: 0.6674, Test: 0.6300\n",
            "Early stopping:  0.02792783848128922\n",
            "Epoch: 037, Loss: 1.1741, Train: 0.6704, Test: 0.6326\n",
            "Early stopping:  0.025059060120320675\n",
            "Epoch: 038, Loss: 1.1608, Train: 0.6758, Test: 0.6332\n",
            "Early stopping:  0.022716896804065613\n",
            "Epoch: 039, Loss: 1.1464, Train: 0.6787, Test: 0.6379\n",
            "Early stopping:  0.021646668853413664\n",
            "Epoch: 040, Loss: 1.1321, Train: 0.6838, Test: 0.6405\n",
            "Early stopping:  0.021732523542412438\n",
            "Epoch: 041, Loss: 1.1178, Train: 0.6867, Test: 0.6416\n",
            "Early stopping:  0.02232559273282254\n",
            "Epoch: 042, Loss: 1.1036, Train: 0.6904, Test: 0.6432\n",
            "Early stopping:  0.02262682021187835\n",
            "Epoch: 043, Loss: 1.0899, Train: 0.6924, Test: 0.6432\n",
            "Early stopping:  0.022395966253246725\n",
            "Epoch: 044, Loss: 1.0773, Train: 0.6941, Test: 0.6416\n",
            "Early stopping:  0.021756791866345337\n",
            "Epoch: 045, Loss: 1.0651, Train: 0.6967, Test: 0.6437\n",
            "Early stopping:  0.020820689162498247\n",
            "Epoch: 046, Loss: 1.0527, Train: 0.6983, Test: 0.6474\n",
            "Early stopping:  0.01999775160208762\n",
            "Epoch: 047, Loss: 1.0408, Train: 0.7012, Test: 0.6463\n",
            "Early stopping:  0.019419750260190324\n",
            "Epoch: 048, Loss: 1.0293, Train: 0.7032, Test: 0.6484\n",
            "Early stopping:  0.019017472772138767\n",
            "Epoch: 049, Loss: 1.0179, Train: 0.7084, Test: 0.6516\n",
            "Early stopping:  0.018643950179140546\n",
            "Epoch: 050, Loss: 1.0065, Train: 0.7097, Test: 0.6558\n",
            "Early stopping:  0.018246790827081628\n",
            "Epoch: 051, Loss: 0.9959, Train: 0.7120, Test: 0.6568\n",
            "Early stopping:  0.017798323292834574\n",
            "Epoch: 052, Loss: 0.9859, Train: 0.7143, Test: 0.6589\n",
            "Early stopping:  0.017205566307307754\n",
            "Epoch: 053, Loss: 0.9760, Train: 0.7171, Test: 0.6611\n",
            "Early stopping:  0.016510919441392047\n",
            "Epoch: 054, Loss: 0.9661, Train: 0.7193, Test: 0.6600\n",
            "Early stopping:  0.01593206510870833\n",
            "Epoch: 055, Loss: 0.9563, Train: 0.7221, Test: 0.6595\n",
            "Early stopping:  0.015653320937589758\n",
            "Epoch: 056, Loss: 0.9468, Train: 0.7267, Test: 0.6600\n",
            "Early stopping:  0.015476162162859361\n",
            "Epoch: 057, Loss: 0.9379, Train: 0.7291, Test: 0.6616\n",
            "Early stopping:  0.015077639463445085\n",
            "Epoch: 058, Loss: 0.9295, Train: 0.7305, Test: 0.6616\n",
            "Early stopping:  0.014491592399714003\n",
            "Epoch: 059, Loss: 0.9207, Train: 0.7328, Test: 0.6642\n",
            "Early stopping:  0.01402319694085973\n",
            "Epoch: 060, Loss: 0.9121, Train: 0.7355, Test: 0.6621\n",
            "Early stopping:  0.013688140131856439\n",
            "Epoch: 061, Loss: 0.9040, Train: 0.7368, Test: 0.6626\n",
            "Early stopping:  0.01347236007743364\n",
            "Epoch: 062, Loss: 0.8959, Train: 0.7374, Test: 0.6663\n",
            "Early stopping:  0.013265507834003638\n",
            "Epoch: 063, Loss: 0.8879, Train: 0.7421, Test: 0.6679\n",
            "Early stopping:  0.012944230040937208\n",
            "Epoch: 064, Loss: 0.8799, Train: 0.7451, Test: 0.6689\n",
            "Early stopping:  0.012733195386370747\n",
            "Epoch: 065, Loss: 0.8722, Train: 0.7461, Test: 0.6684\n",
            "Early stopping:  0.012584843818087893\n",
            "Epoch: 066, Loss: 0.8645, Train: 0.7479, Test: 0.6705\n",
            "Early stopping:  0.012398600930653864\n",
            "Epoch: 067, Loss: 0.8570, Train: 0.7513, Test: 0.6732\n",
            "Early stopping:  0.012196060161779645\n",
            "Epoch: 068, Loss: 0.8499, Train: 0.7532, Test: 0.6732\n",
            "Early stopping:  0.011891889343966516\n",
            "Epoch: 069, Loss: 0.8427, Train: 0.7545, Test: 0.6747\n",
            "Early stopping:  0.011629005898457575\n",
            "Epoch: 070, Loss: 0.8356, Train: 0.7557, Test: 0.6753\n",
            "Early stopping:  0.011407082857055833\n",
            "Epoch: 071, Loss: 0.8286, Train: 0.7579, Test: 0.6758\n",
            "Early stopping:  0.01124045518532399\n",
            "Epoch: 072, Loss: 0.8218, Train: 0.7587, Test: 0.6747\n",
            "Early stopping:  0.011122550731838073\n",
            "Epoch: 073, Loss: 0.8150, Train: 0.7618, Test: 0.6721\n",
            "Early stopping:  0.01095696367707344\n",
            "Epoch: 074, Loss: 0.8083, Train: 0.7638, Test: 0.6732\n",
            "Early stopping:  0.01079849979315659\n",
            "Epoch: 075, Loss: 0.8018, Train: 0.7649, Test: 0.6732\n",
            "Early stopping:  0.010609958542568161\n",
            "Epoch: 076, Loss: 0.7957, Train: 0.7650, Test: 0.6732\n",
            "Early stopping:  0.010330495381316791\n",
            "Epoch: 077, Loss: 0.7897, Train: 0.7668, Test: 0.6779\n",
            "Early stopping:  0.00998943311270413\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.77      0.78       106\n",
            "            CAD_and_CAM       0.66      0.76      0.70        94\n",
            "              Companies       0.47      0.47      0.47       112\n",
            "       Computer_Science       0.70      0.69      0.70       106\n",
            "            Consultants       0.54      0.53      0.54       103\n",
            "           Data_Formats       0.79      0.79      0.79        87\n",
            "    Data_Communications       0.65      0.69      0.67        91\n",
            "              Education       0.90      0.87      0.88       100\n",
            "               Graphics       0.84      0.96      0.89        91\n",
            "               Hardware       0.61      0.62      0.62       111\n",
            "               Internet       0.70      0.63      0.66        99\n",
            "       Mobile_Computing       0.88      0.75      0.81       109\n",
            "             Multimedia       0.65      0.72      0.68        89\n",
            "            Open_Source       0.57      0.71      0.63        98\n",
            "            Programming       0.53      0.52      0.53       111\n",
            "               Robotics       0.87      0.90      0.89       103\n",
            "               Security       0.76      0.79      0.78        86\n",
            "               Software       0.43      0.25      0.32       105\n",
            "                Systems       0.55      0.57      0.56        99\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.67      0.68      0.67      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.3091, Train: 0.0517, Test: 0.0563\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 22.4621, Train: 0.1799, Test: 0.1811\n",
            "Early stopping:  11.421914969152477\n",
            "Epoch: 003, Loss: 12.4523, Train: 0.2432, Test: 0.2337\n",
            "Early stopping:  8.153280352260717\n",
            "Epoch: 004, Loss: 10.8130, Train: 0.2154, Test: 0.2084\n",
            "Early stopping:  6.816224367146422\n",
            "Epoch: 005, Loss: 11.4437, Train: 0.2558, Test: 0.2574\n",
            "Early stopping:  5.9443943299769\n",
            "Epoch: 006, Loss: 9.7402, Train: 0.3025, Test: 0.3068\n",
            "Early stopping:  5.170422872373015\n",
            "Epoch: 007, Loss: 8.4065, Train: 0.3296, Test: 0.3442\n",
            "Early stopping:  1.560038617301263\n",
            "Epoch: 008, Loss: 7.2332, Train: 0.3846, Test: 0.4026\n",
            "Early stopping:  1.7240953490409074\n",
            "Epoch: 009, Loss: 5.9888, Train: 0.4199, Test: 0.4342\n",
            "Early stopping:  2.127408296861798\n",
            "Epoch: 010, Loss: 5.0124, Train: 0.4254, Test: 0.4395\n",
            "Early stopping:  1.8796876648658158\n",
            "Epoch: 011, Loss: 4.2799, Train: 0.4149, Test: 0.4363\n",
            "Early stopping:  1.6640769501880053\n",
            "Epoch: 012, Loss: 3.6885, Train: 0.4222, Test: 0.4395\n",
            "Early stopping:  1.4066064232054778\n",
            "Epoch: 013, Loss: 3.1063, Train: 0.4401, Test: 0.4437\n",
            "Early stopping:  1.1283051662948527\n",
            "Epoch: 014, Loss: 2.5972, Train: 0.4604, Test: 0.4516\n",
            "Early stopping:  0.9513756993100394\n",
            "Epoch: 015, Loss: 2.2781, Train: 0.4741, Test: 0.4774\n",
            "Early stopping:  0.8102852012474651\n",
            "Epoch: 016, Loss: 2.1267, Train: 0.4812, Test: 0.4853\n",
            "Early stopping:  0.6406876120238897\n",
            "Epoch: 017, Loss: 2.0403, Train: 0.4937, Test: 0.4879\n",
            "Early stopping:  0.43369459859683074\n",
            "Epoch: 018, Loss: 1.9607, Train: 0.5028, Test: 0.5037\n",
            "Early stopping:  0.2510255492660473\n",
            "Epoch: 019, Loss: 1.8858, Train: 0.5158, Test: 0.5153\n",
            "Early stopping:  0.15217684835332046\n",
            "Epoch: 020, Loss: 1.8349, Train: 0.5307, Test: 0.5237\n",
            "Early stopping:  0.1171603370645654\n",
            "Epoch: 021, Loss: 1.7996, Train: 0.5409, Test: 0.5342\n",
            "Early stopping:  0.09719622240078943\n",
            "Epoch: 022, Loss: 1.7695, Train: 0.5511, Test: 0.5479\n",
            "Early stopping:  0.07547425440067734\n",
            "Epoch: 023, Loss: 1.7360, Train: 0.5574, Test: 0.5542\n",
            "Early stopping:  0.058028451104213355\n",
            "Epoch: 024, Loss: 1.6963, Train: 0.5676, Test: 0.5605\n",
            "Early stopping:  0.0539442853388239\n",
            "Epoch: 025, Loss: 1.6518, Train: 0.5809, Test: 0.5753\n",
            "Early stopping:  0.058491065212123314\n",
            "Epoch: 026, Loss: 1.6045, Train: 0.5918, Test: 0.5847\n",
            "Early stopping:  0.06563639449157797\n",
            "Epoch: 027, Loss: 1.5566, Train: 0.6011, Test: 0.5826\n",
            "Early stopping:  0.07132413001988559\n",
            "Epoch: 028, Loss: 1.5120, Train: 0.6037, Test: 0.5842\n",
            "Early stopping:  0.07335727849093426\n",
            "Epoch: 029, Loss: 1.4757, Train: 0.5986, Test: 0.5826\n",
            "Early stopping:  0.07038837847622025\n",
            "Epoch: 030, Loss: 1.4503, Train: 0.5979, Test: 0.5884\n",
            "Early stopping:  0.06195990910218164\n",
            "Epoch: 031, Loss: 1.4302, Train: 0.6011, Test: 0.5947\n",
            "Early stopping:  0.050345787318373025\n",
            "Epoch: 032, Loss: 1.4061, Train: 0.6103, Test: 0.6079\n",
            "Early stopping:  0.04093823414093384\n",
            "Epoch: 033, Loss: 1.3768, Train: 0.6212, Test: 0.6168\n",
            "Early stopping:  0.03835482495670844\n",
            "Epoch: 034, Loss: 1.3476, Train: 0.6305, Test: 0.6295\n",
            "Early stopping:  0.041063460651510106\n",
            "Epoch: 035, Loss: 1.3225, Train: 0.6380, Test: 0.6326\n",
            "Early stopping:  0.043367482558952876\n",
            "Epoch: 036, Loss: 1.3028, Train: 0.6426, Test: 0.6337\n",
            "Early stopping:  0.04137741888431322\n",
            "Epoch: 037, Loss: 1.2868, Train: 0.6461, Test: 0.6405\n",
            "Early stopping:  0.035775994281702896\n",
            "Epoch: 038, Loss: 1.2717, Train: 0.6501, Test: 0.6432\n",
            "Early stopping:  0.029800498891666943\n",
            "Epoch: 039, Loss: 1.2550, Train: 0.6532, Test: 0.6468\n",
            "Early stopping:  0.026267952502998463\n",
            "Epoch: 040, Loss: 1.2363, Train: 0.6563, Test: 0.6453\n",
            "Early stopping:  0.02608713494189914\n",
            "Epoch: 041, Loss: 1.2170, Train: 0.6570, Test: 0.6547\n",
            "Early stopping:  0.027717843827314986\n",
            "Epoch: 042, Loss: 1.1992, Train: 0.6616, Test: 0.6553\n",
            "Early stopping:  0.028962837168965305\n",
            "Epoch: 043, Loss: 1.1832, Train: 0.6661, Test: 0.6579\n",
            "Early stopping:  0.028595676022632764\n",
            "Epoch: 044, Loss: 1.1682, Train: 0.6717, Test: 0.6600\n",
            "Early stopping:  0.026903456646664884\n",
            "Epoch: 045, Loss: 1.1533, Train: 0.6762, Test: 0.6600\n",
            "Early stopping:  0.02503889973116441\n",
            "Epoch: 046, Loss: 1.1393, Train: 0.6791, Test: 0.6632\n",
            "Early stopping:  0.02365250229875618\n",
            "Epoch: 047, Loss: 1.1269, Train: 0.6832, Test: 0.6611\n",
            "Early stopping:  0.02238409074576175\n",
            "Epoch: 048, Loss: 1.1154, Train: 0.6854, Test: 0.6658\n",
            "Early stopping:  0.02090767914266828\n",
            "Epoch: 049, Loss: 1.1044, Train: 0.6888, Test: 0.6658\n",
            "Early stopping:  0.0192877497671906\n",
            "Epoch: 050, Loss: 1.0943, Train: 0.6903, Test: 0.6679\n",
            "Early stopping:  0.017823728490809248\n",
            "Epoch: 051, Loss: 1.0844, Train: 0.6918, Test: 0.6737\n",
            "Early stopping:  0.016791666218399535\n",
            "Epoch: 052, Loss: 1.0743, Train: 0.6922, Test: 0.6732\n",
            "Early stopping:  0.01616670091311962\n",
            "Epoch: 053, Loss: 1.0638, Train: 0.6950, Test: 0.6711\n",
            "Early stopping:  0.015991104349275607\n",
            "Epoch: 054, Loss: 1.0533, Train: 0.6963, Test: 0.6763\n",
            "Early stopping:  0.01620022076048947\n",
            "Epoch: 055, Loss: 1.0427, Train: 0.6983, Test: 0.6753\n",
            "Early stopping:  0.016503568118345874\n",
            "Epoch: 056, Loss: 1.0319, Train: 0.7020, Test: 0.6747\n",
            "Early stopping:  0.016735460712003613\n",
            "Epoch: 057, Loss: 1.0218, Train: 0.7034, Test: 0.6768\n",
            "Early stopping:  0.016684568548467768\n",
            "Epoch: 058, Loss: 1.0128, Train: 0.7066, Test: 0.6816\n",
            "Early stopping:  0.016138494604910916\n",
            "Epoch: 059, Loss: 1.0043, Train: 0.7078, Test: 0.6858\n",
            "Early stopping:  0.015180986684239294\n",
            "Epoch: 060, Loss: 0.9959, Train: 0.7099, Test: 0.6863\n",
            "Early stopping:  0.014177647928772345\n",
            "Epoch: 061, Loss: 0.9873, Train: 0.7116, Test: 0.6842\n",
            "Early stopping:  0.013590810204719353\n",
            "Epoch: 062, Loss: 0.9787, Train: 0.7133, Test: 0.6863\n",
            "Early stopping:  0.013454096911008141\n",
            "Epoch: 063, Loss: 0.9707, Train: 0.7149, Test: 0.6874\n",
            "Early stopping:  0.013352639962944505\n",
            "Epoch: 064, Loss: 0.9629, Train: 0.7184, Test: 0.6884\n",
            "Early stopping:  0.013041731409706457\n",
            "Epoch: 065, Loss: 0.9554, Train: 0.7205, Test: 0.6884\n",
            "Early stopping:  0.012596117103928488\n",
            "Epoch: 066, Loss: 0.9479, Train: 0.7239, Test: 0.6895\n",
            "Early stopping:  0.012172838869371745\n",
            "Epoch: 067, Loss: 0.9406, Train: 0.7266, Test: 0.6900\n",
            "Early stopping:  0.011886089207622512\n",
            "Epoch: 068, Loss: 0.9332, Train: 0.7280, Test: 0.6916\n",
            "Early stopping:  0.011727688572014313\n",
            "Epoch: 069, Loss: 0.9261, Train: 0.7305, Test: 0.6921\n",
            "Early stopping:  0.01157271485096393\n",
            "Epoch: 070, Loss: 0.9193, Train: 0.7318, Test: 0.6916\n",
            "Early stopping:  0.011326235870178272\n",
            "Epoch: 071, Loss: 0.9127, Train: 0.7342, Test: 0.6942\n",
            "Early stopping:  0.011018134803932727\n",
            "Epoch: 072, Loss: 0.9061, Train: 0.7363, Test: 0.6974\n",
            "Early stopping:  0.010695030229004842\n",
            "Epoch: 073, Loss: 0.8993, Train: 0.7372, Test: 0.6968\n",
            "Early stopping:  0.010549306974293517\n",
            "Epoch: 074, Loss: 0.8924, Train: 0.7368, Test: 0.6947\n",
            "Early stopping:  0.010617758546224876\n",
            "Epoch: 075, Loss: 0.8858, Train: 0.7387, Test: 0.6963\n",
            "Early stopping:  0.010684625393067202\n",
            "Epoch: 076, Loss: 0.8793, Train: 0.7400, Test: 0.6968\n",
            "Early stopping:  0.010632552015208522\n",
            "Epoch: 077, Loss: 0.8729, Train: 0.7422, Test: 0.7011\n",
            "Early stopping:  0.010436205096244846\n",
            "Epoch: 078, Loss: 0.8666, Train: 0.7458, Test: 0.7037\n",
            "Early stopping:  0.010185886634733972\n",
            "Epoch: 079, Loss: 0.8605, Train: 0.7492, Test: 0.7074\n",
            "Early stopping:  0.01000047671009691\n",
            "Epoch: 080, Loss: 0.8544, Train: 0.7518, Test: 0.7063\n",
            "Early stopping:  0.009852538579288316\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.85      0.81       107\n",
            "            CAD_and_CAM       0.71      0.82      0.76       106\n",
            "              Companies       0.44      0.53      0.48        88\n",
            "       Computer_Science       0.72      0.70      0.71        91\n",
            "            Consultants       0.56      0.66      0.61        89\n",
            "           Data_Formats       0.78      0.82      0.80        98\n",
            "    Data_Communications       0.78      0.85      0.81       110\n",
            "              Education       0.86      0.89      0.88        99\n",
            "               Graphics       0.85      0.90      0.87       108\n",
            "               Hardware       0.72      0.57      0.64       101\n",
            "               Internet       0.66      0.63      0.64       100\n",
            "       Mobile_Computing       0.89      0.79      0.84       101\n",
            "             Multimedia       0.72      0.73      0.72       107\n",
            "            Open_Source       0.67      0.69      0.68        97\n",
            "            Programming       0.57      0.52      0.54       123\n",
            "               Robotics       0.94      0.90      0.92        89\n",
            "               Security       0.86      0.75      0.80        88\n",
            "               Software       0.33      0.23      0.27        96\n",
            "                Systems       0.56      0.57      0.57       102\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.70      0.71      0.70      1900\n",
            "           weighted avg       0.70      0.71      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 7.7494, Train: 0.1388, Test: 0.1353\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 11.9659, Train: 0.1195, Test: 0.1095\n",
            "Early stopping:  2.9814626688640344\n",
            "Epoch: 003, Loss: 17.9357, Train: 0.2309, Test: 0.2189\n",
            "Early stopping:  5.118225748661317\n",
            "Epoch: 004, Loss: 17.1732, Train: 0.2597, Test: 0.2558\n",
            "Early stopping:  4.7756649067412935\n",
            "Epoch: 005, Loss: 11.3107, Train: 0.3236, Test: 0.3153\n",
            "Early stopping:  4.272326593571155\n",
            "Epoch: 006, Loss: 8.6427, Train: 0.3163, Test: 0.3179\n",
            "Early stopping:  3.99572653681996\n",
            "Epoch: 007, Loss: 7.2417, Train: 0.3538, Test: 0.3368\n",
            "Early stopping:  4.88161997871805\n",
            "Epoch: 008, Loss: 5.0120, Train: 0.3992, Test: 0.3858\n",
            "Early stopping:  4.675106756754058\n",
            "Epoch: 009, Loss: 3.6361, Train: 0.4183, Test: 0.4132\n",
            "Early stopping:  3.0192964159425983\n",
            "Epoch: 010, Loss: 2.9772, Train: 0.4354, Test: 0.4268\n",
            "Early stopping:  2.3972372077637742\n",
            "Epoch: 011, Loss: 2.5224, Train: 0.4432, Test: 0.4326\n",
            "Early stopping:  1.9045150243414872\n",
            "Epoch: 012, Loss: 2.2515, Train: 0.4697, Test: 0.4626\n",
            "Early stopping:  1.1011634730988287\n",
            "Epoch: 013, Loss: 2.1086, Train: 0.4821, Test: 0.4768\n",
            "Early stopping:  0.6195798867792538\n",
            "Epoch: 014, Loss: 2.0177, Train: 0.4682, Test: 0.4584\n",
            "Early stopping:  0.38673317478832364\n",
            "Epoch: 015, Loss: 1.9698, Train: 0.4709, Test: 0.4526\n",
            "Early stopping:  0.22236023701874827\n",
            "Epoch: 016, Loss: 1.9369, Train: 0.4891, Test: 0.4732\n",
            "Early stopping:  0.12656884703907895\n",
            "Epoch: 017, Loss: 1.8861, Train: 0.5096, Test: 0.4942\n",
            "Early stopping:  0.08467227534272968\n",
            "Epoch: 018, Loss: 1.8334, Train: 0.5153, Test: 0.5053\n",
            "Early stopping:  0.07169659720835996\n",
            "Epoch: 019, Loss: 1.7963, Train: 0.5083, Test: 0.5084\n",
            "Early stopping:  0.07142857745395266\n",
            "Epoch: 020, Loss: 1.7685, Train: 0.5142, Test: 0.5147\n",
            "Early stopping:  0.06796268063365887\n",
            "Epoch: 021, Loss: 1.7373, Train: 0.5286, Test: 0.5305\n",
            "Early stopping:  0.05781757501234689\n",
            "Epoch: 022, Loss: 1.6972, Train: 0.5420, Test: 0.5479\n",
            "Early stopping:  0.052508442750495606\n",
            "Epoch: 023, Loss: 1.6505, Train: 0.5554, Test: 0.5605\n",
            "Early stopping:  0.0577228894594429\n",
            "Epoch: 024, Loss: 1.6038, Train: 0.5661, Test: 0.5642\n",
            "Early stopping:  0.06601422193280596\n",
            "Epoch: 025, Loss: 1.5613, Train: 0.5692, Test: 0.5711\n",
            "Early stopping:  0.07046013646378897\n",
            "Epoch: 026, Loss: 1.5241, Train: 0.5763, Test: 0.5732\n",
            "Early stopping:  0.06893329395581009\n",
            "Epoch: 027, Loss: 1.4898, Train: 0.5883, Test: 0.5821\n",
            "Early stopping:  0.06356120596604911\n",
            "Epoch: 028, Loss: 1.4555, Train: 0.5958, Test: 0.5879\n",
            "Early stopping:  0.05826516365088492\n",
            "Epoch: 029, Loss: 1.4202, Train: 0.6042, Test: 0.5947\n",
            "Early stopping:  0.05544766959684334\n",
            "Epoch: 030, Loss: 1.3855, Train: 0.6171, Test: 0.6111\n",
            "Early stopping:  0.05484365358797087\n",
            "Epoch: 031, Loss: 1.3542, Train: 0.6297, Test: 0.6137\n",
            "Early stopping:  0.05396057906871852\n",
            "Epoch: 032, Loss: 1.3277, Train: 0.6355, Test: 0.6258\n",
            "Early stopping:  0.05093848995031329\n",
            "Epoch: 033, Loss: 1.3073, Train: 0.6382, Test: 0.6316\n",
            "Early stopping:  0.04506571442452865\n",
            "Epoch: 034, Loss: 1.2914, Train: 0.6396, Test: 0.6279\n",
            "Early stopping:  0.03747152033949998\n",
            "Epoch: 035, Loss: 1.2762, Train: 0.6428, Test: 0.6326\n",
            "Early stopping:  0.030639189036399946\n",
            "Epoch: 036, Loss: 1.2593, Train: 0.6466, Test: 0.6305\n",
            "Early stopping:  0.02658666739114848\n",
            "Epoch: 037, Loss: 1.2412, Train: 0.6517, Test: 0.6342\n",
            "Early stopping:  0.025993929927940165\n",
            "Epoch: 038, Loss: 1.2232, Train: 0.6542, Test: 0.6389\n",
            "Early stopping:  0.027101452474503403\n",
            "Epoch: 039, Loss: 1.2062, Train: 0.6601, Test: 0.6453\n",
            "Early stopping:  0.027846810499159772\n",
            "Epoch: 040, Loss: 1.1902, Train: 0.6647, Test: 0.6468\n",
            "Early stopping:  0.027404960183275397\n",
            "Epoch: 041, Loss: 1.1756, Train: 0.6684, Test: 0.6489\n",
            "Early stopping:  0.02599499319302067\n",
            "Epoch: 042, Loss: 1.1627, Train: 0.6691, Test: 0.6547\n",
            "Early stopping:  0.02401677486782636\n",
            "Epoch: 043, Loss: 1.1510, Train: 0.6746, Test: 0.6600\n",
            "Early stopping:  0.02182748997530666\n",
            "Epoch: 044, Loss: 1.1392, Train: 0.6772, Test: 0.6679\n",
            "Early stopping:  0.020032006148255597\n",
            "Epoch: 045, Loss: 1.1263, Train: 0.6818, Test: 0.6668\n",
            "Early stopping:  0.019290956546721652\n",
            "Epoch: 046, Loss: 1.1125, Train: 0.6866, Test: 0.6668\n",
            "Early stopping:  0.019775151096078386\n",
            "Epoch: 047, Loss: 1.0992, Train: 0.6912, Test: 0.6705\n",
            "Early stopping:  0.020619580820930485\n",
            "Epoch: 048, Loss: 1.0875, Train: 0.6925, Test: 0.6742\n",
            "Early stopping:  0.020651123301418176\n",
            "Epoch: 049, Loss: 1.0772, Train: 0.6979, Test: 0.6737\n",
            "Early stopping:  0.019535829661121353\n",
            "Epoch: 050, Loss: 1.0670, Train: 0.7003, Test: 0.6753\n",
            "Early stopping:  0.017898370724291467\n",
            "Epoch: 051, Loss: 1.0570, Train: 0.7017, Test: 0.6753\n",
            "Early stopping:  0.016593956437116763\n",
            "Epoch: 052, Loss: 1.0469, Train: 0.7033, Test: 0.6784\n",
            "Early stopping:  0.01604663137685665\n",
            "Epoch: 053, Loss: 1.0368, Train: 0.7066, Test: 0.6842\n",
            "Early stopping:  0.015967562545205132\n",
            "Epoch: 054, Loss: 1.0271, Train: 0.7092, Test: 0.6837\n",
            "Early stopping:  0.015819020621027805\n",
            "Epoch: 055, Loss: 1.0182, Train: 0.7122, Test: 0.6826\n",
            "Early stopping:  0.0153984030457735\n",
            "Epoch: 056, Loss: 1.0098, Train: 0.7143, Test: 0.6842\n",
            "Early stopping:  0.014669694860784248\n",
            "Epoch: 057, Loss: 1.0016, Train: 0.7166, Test: 0.6863\n",
            "Early stopping:  0.013869226985270587\n",
            "Epoch: 058, Loss: 0.9936, Train: 0.7187, Test: 0.6858\n",
            "Early stopping:  0.013243409612450163\n",
            "Epoch: 059, Loss: 0.9854, Train: 0.7187, Test: 0.6826\n",
            "Early stopping:  0.012931042385740832\n",
            "Epoch: 060, Loss: 0.9770, Train: 0.7217, Test: 0.6826\n",
            "Early stopping:  0.012942470980155883\n",
            "Epoch: 061, Loss: 0.9685, Train: 0.7233, Test: 0.6805\n",
            "Early stopping:  0.013094352981402362\n",
            "Epoch: 062, Loss: 0.9605, Train: 0.7253, Test: 0.6805\n",
            "Early stopping:  0.013114191215946838\n",
            "Epoch: 063, Loss: 0.9531, Train: 0.7280, Test: 0.6826\n",
            "Early stopping:  0.012835462946233679\n",
            "Epoch: 064, Loss: 0.9456, Train: 0.7311, Test: 0.6847\n",
            "Early stopping:  0.01236689325653176\n",
            "Epoch: 065, Loss: 0.9381, Train: 0.7338, Test: 0.6847\n",
            "Early stopping:  0.011967052172151759\n",
            "Epoch: 066, Loss: 0.9309, Train: 0.7353, Test: 0.6868\n",
            "Early stopping:  0.011747501308288924\n",
            "Epoch: 067, Loss: 0.9239, Train: 0.7353, Test: 0.6858\n",
            "Early stopping:  0.011539714019349522\n",
            "Epoch: 068, Loss: 0.9174, Train: 0.7355, Test: 0.6853\n",
            "Early stopping:  0.011169257267787426\n",
            "Epoch: 069, Loss: 0.9110, Train: 0.7378, Test: 0.6895\n",
            "Early stopping:  0.010704225688949983\n",
            "Epoch: 070, Loss: 0.9047, Train: 0.7384, Test: 0.6884\n",
            "Early stopping:  0.010307190111575184\n",
            "Epoch: 071, Loss: 0.8985, Train: 0.7411, Test: 0.6900\n",
            "Early stopping:  0.010039867710102\n",
            "Epoch: 072, Loss: 0.8918, Train: 0.7433, Test: 0.6905\n",
            "Early stopping:  0.010053793810764728\n",
            "Epoch: 073, Loss: 0.8856, Train: 0.7447, Test: 0.6916\n",
            "Early stopping:  0.010065779023615043\n",
            "Epoch: 074, Loss: 0.8800, Train: 0.7461, Test: 0.6932\n",
            "Early stopping:  0.009858528348148222\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.78      0.78        91\n",
            "            CAD_and_CAM       0.71      0.79      0.75       106\n",
            "              Companies       0.50      0.50      0.50       105\n",
            "       Computer_Science       0.75      0.70      0.72       101\n",
            "            Consultants       0.67      0.62      0.65       112\n",
            "           Data_Formats       0.83      0.82      0.83        96\n",
            "    Data_Communications       0.67      0.81      0.73        86\n",
            "              Education       0.87      0.88      0.87       124\n",
            "               Graphics       0.85      0.88      0.86        97\n",
            "               Hardware       0.67      0.70      0.69        89\n",
            "               Internet       0.64      0.64      0.64       103\n",
            "       Mobile_Computing       0.76      0.72      0.74        93\n",
            "             Multimedia       0.66      0.65      0.66       100\n",
            "            Open_Source       0.69      0.65      0.67       100\n",
            "            Programming       0.48      0.55      0.51        89\n",
            "               Robotics       0.92      0.86      0.89       100\n",
            "               Security       0.75      0.79      0.77       104\n",
            "               Software       0.31      0.28      0.30        95\n",
            "                Systems       0.59      0.52      0.56       109\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.69      0.69      0.69      1900\n",
            "           weighted avg       0.69      0.69      0.69      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 8.0573, Train: 0.0821, Test: 0.0695\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 13.9940, Train: 0.1555, Test: 0.1500\n",
            "Early stopping:  4.197887477496478\n",
            "Epoch: 003, Loss: 11.9797, Train: 0.2092, Test: 0.2068\n",
            "Early stopping:  3.0190350450761074\n",
            "Epoch: 004, Loss: 11.3626, Train: 0.2591, Test: 0.2574\n",
            "Early stopping:  2.465050043615335\n",
            "Epoch: 005, Loss: 10.5562, Train: 0.3132, Test: 0.3063\n",
            "Early stopping:  2.1639901487116564\n",
            "Epoch: 006, Loss: 10.3435, Train: 0.3367, Test: 0.3374\n",
            "Early stopping:  1.4653319791386685\n",
            "Epoch: 007, Loss: 9.3042, Train: 0.3451, Test: 0.3563\n",
            "Early stopping:  1.0213347284839374\n",
            "Epoch: 008, Loss: 7.8471, Train: 0.3482, Test: 0.3505\n",
            "Early stopping:  1.3541053099038363\n",
            "Epoch: 009, Loss: 6.4741, Train: 0.3838, Test: 0.3916\n",
            "Early stopping:  1.7308417196845474\n",
            "Epoch: 010, Loss: 4.7220, Train: 0.4207, Test: 0.4211\n",
            "Early stopping:  2.233077039872778\n",
            "Epoch: 011, Loss: 3.5813, Train: 0.4516, Test: 0.4332\n",
            "Early stopping:  2.307224925380533\n",
            "Epoch: 012, Loss: 2.7975, Train: 0.4622, Test: 0.4547\n",
            "Early stopping:  2.0727003933175454\n",
            "Epoch: 013, Loss: 2.2960, Train: 0.4811, Test: 0.4753\n",
            "Early stopping:  1.670646295094943\n",
            "Epoch: 014, Loss: 2.0485, Train: 0.4859, Test: 0.4784\n",
            "Early stopping:  1.0846274460614025\n",
            "Epoch: 015, Loss: 1.9568, Train: 0.4828, Test: 0.4779\n",
            "Early stopping:  0.6693133070733446\n",
            "Epoch: 016, Loss: 1.9338, Train: 0.4745, Test: 0.4653\n",
            "Early stopping:  0.360221242586436\n",
            "Epoch: 017, Loss: 1.9257, Train: 0.4697, Test: 0.4674\n",
            "Early stopping:  0.1553881963387043\n",
            "Epoch: 018, Loss: 1.9203, Train: 0.4670, Test: 0.4632\n",
            "Early stopping:  0.05299849849556903\n",
            "Epoch: 019, Loss: 1.9138, Train: 0.4661, Test: 0.4700\n",
            "Early stopping:  0.01661234644282628\n",
            "Epoch: 020, Loss: 1.9011, Train: 0.4691, Test: 0.4611\n",
            "Early stopping:  0.012357750250969215\n",
            "Epoch: 021, Loss: 1.8815, Train: 0.4762, Test: 0.4647\n",
            "Early stopping:  0.01765629913222398\n",
            "Epoch: 022, Loss: 1.8536, Train: 0.4845, Test: 0.4747\n",
            "Early stopping:  0.027052634388988465\n",
            "Epoch: 023, Loss: 1.8176, Train: 0.4942, Test: 0.4863\n",
            "Early stopping:  0.038644346231603725\n",
            "Epoch: 024, Loss: 1.7749, Train: 0.5063, Test: 0.4942\n",
            "Early stopping:  0.05053165684514199\n",
            "Epoch: 025, Loss: 1.7282, Train: 0.5167, Test: 0.5053\n",
            "Early stopping:  0.06120880345838588\n",
            "Epoch: 026, Loss: 1.6785, Train: 0.5328, Test: 0.5200\n",
            "Early stopping:  0.06962423282611604\n",
            "Epoch: 027, Loss: 1.6268, Train: 0.5503, Test: 0.5384\n",
            "Early stopping:  0.0756353044005876\n",
            "Epoch: 028, Loss: 1.5765, Train: 0.5647, Test: 0.5495\n",
            "Early stopping:  0.07878138328119301\n",
            "Epoch: 029, Loss: 1.5320, Train: 0.5728, Test: 0.5521\n",
            "Early stopping:  0.07818813422235592\n",
            "Epoch: 030, Loss: 1.4959, Train: 0.5791, Test: 0.5621\n",
            "Early stopping:  0.07290570015328238\n",
            "Epoch: 031, Loss: 1.4673, Train: 0.5891, Test: 0.5674\n",
            "Early stopping:  0.06357728607442284\n",
            "Epoch: 032, Loss: 1.4425, Train: 0.5970, Test: 0.5737\n",
            "Early stopping:  0.05300967146776455\n",
            "Epoch: 033, Loss: 1.4174, Train: 0.6024, Test: 0.5884\n",
            "Early stopping:  0.04483949907754147\n",
            "Epoch: 034, Loss: 1.3920, Train: 0.6118, Test: 0.5958\n",
            "Early stopping:  0.04076667052537771\n",
            "Epoch: 035, Loss: 1.3694, Train: 0.6167, Test: 0.5995\n",
            "Early stopping:  0.03894298064916616\n",
            "Epoch: 036, Loss: 1.3505, Train: 0.6257, Test: 0.6026\n",
            "Early stopping:  0.036755290036591115\n",
            "Epoch: 037, Loss: 1.3328, Train: 0.6311, Test: 0.6121\n",
            "Early stopping:  0.033435589696682366\n",
            "Epoch: 038, Loss: 1.3146, Train: 0.6379, Test: 0.6247\n",
            "Early stopping:  0.030283227464988578\n",
            "Epoch: 039, Loss: 1.2969, Train: 0.6430, Test: 0.6211\n",
            "Early stopping:  0.02859404027829825\n",
            "Epoch: 040, Loss: 1.2804, Train: 0.6497, Test: 0.6274\n",
            "Early stopping:  0.027827687708588074\n",
            "Epoch: 041, Loss: 1.2651, Train: 0.6539, Test: 0.6279\n",
            "Early stopping:  0.02683742491079529\n",
            "Epoch: 042, Loss: 1.2499, Train: 0.6566, Test: 0.6342\n",
            "Early stopping:  0.025508294902765127\n",
            "Epoch: 043, Loss: 1.2350, Train: 0.6592, Test: 0.6332\n",
            "Early stopping:  0.02439108607560296\n",
            "Epoch: 044, Loss: 1.2205, Train: 0.6618, Test: 0.6379\n",
            "Early stopping:  0.023697509584710084\n",
            "Epoch: 045, Loss: 1.2064, Train: 0.6654, Test: 0.6395\n",
            "Early stopping:  0.023215601393898187\n",
            "Epoch: 046, Loss: 1.1925, Train: 0.6696, Test: 0.6416\n",
            "Early stopping:  0.02268626589342992\n",
            "Epoch: 047, Loss: 1.1794, Train: 0.6738, Test: 0.6468\n",
            "Early stopping:  0.022018472988661015\n",
            "Epoch: 048, Loss: 1.1671, Train: 0.6761, Test: 0.6516\n",
            "Early stopping:  0.021171436983708804\n",
            "Epoch: 049, Loss: 1.1558, Train: 0.6809, Test: 0.6526\n",
            "Early stopping:  0.02002396791921363\n",
            "Epoch: 050, Loss: 1.1453, Train: 0.6839, Test: 0.6516\n",
            "Early stopping:  0.018670845239502687\n",
            "Epoch: 051, Loss: 1.1351, Train: 0.6859, Test: 0.6589\n",
            "Early stopping:  0.017447001618539293\n",
            "Epoch: 052, Loss: 1.1252, Train: 0.6870, Test: 0.6616\n",
            "Early stopping:  0.016531371315411665\n",
            "Epoch: 053, Loss: 1.1156, Train: 0.6887, Test: 0.6637\n",
            "Early stopping:  0.015899248750865434\n",
            "Epoch: 054, Loss: 1.1065, Train: 0.6896, Test: 0.6658\n",
            "Early stopping:  0.015364207495565933\n",
            "Epoch: 055, Loss: 1.0977, Train: 0.6909, Test: 0.6605\n",
            "Early stopping:  0.014784810679624474\n",
            "Epoch: 056, Loss: 1.0893, Train: 0.6924, Test: 0.6595\n",
            "Early stopping:  0.014176184046942252\n",
            "Epoch: 057, Loss: 1.0813, Train: 0.6943, Test: 0.6642\n",
            "Early stopping:  0.013582353735173856\n",
            "Epoch: 058, Loss: 1.0732, Train: 0.6954, Test: 0.6647\n",
            "Early stopping:  0.0131172025780034\n",
            "Epoch: 059, Loss: 1.0651, Train: 0.6997, Test: 0.6668\n",
            "Early stopping:  0.012865957614901384\n",
            "Epoch: 060, Loss: 1.0568, Train: 0.7020, Test: 0.6679\n",
            "Early stopping:  0.012835280397936301\n",
            "Epoch: 061, Loss: 1.0489, Train: 0.7036, Test: 0.6684\n",
            "Early stopping:  0.012838938340074931\n",
            "Epoch: 062, Loss: 1.0413, Train: 0.7045, Test: 0.6711\n",
            "Early stopping:  0.012673112381665177\n",
            "Epoch: 063, Loss: 1.0339, Train: 0.7084, Test: 0.6726\n",
            "Early stopping:  0.012309443402679227\n",
            "Epoch: 064, Loss: 1.0268, Train: 0.7103, Test: 0.6721\n",
            "Early stopping:  0.011872724018292515\n",
            "Epoch: 065, Loss: 1.0196, Train: 0.7109, Test: 0.6747\n",
            "Early stopping:  0.011569132784316509\n",
            "Epoch: 066, Loss: 1.0124, Train: 0.7130, Test: 0.6732\n",
            "Early stopping:  0.011386706277942001\n",
            "Epoch: 067, Loss: 1.0057, Train: 0.7146, Test: 0.6737\n",
            "Early stopping:  0.011201743368103739\n",
            "Epoch: 068, Loss: 0.9992, Train: 0.7157, Test: 0.6800\n",
            "Early stopping:  0.010910569368662635\n",
            "Epoch: 069, Loss: 0.9930, Train: 0.7174, Test: 0.6779\n",
            "Early stopping:  0.01049725160846406\n",
            "Epoch: 070, Loss: 0.9867, Train: 0.7188, Test: 0.6763\n",
            "Early stopping:  0.010157167443304508\n",
            "Epoch: 071, Loss: 0.9803, Train: 0.7209, Test: 0.6768\n",
            "Early stopping:  0.010003355041092486\n",
            "Epoch: 072, Loss: 0.9741, Train: 0.7226, Test: 0.6753\n",
            "Early stopping:  0.009950230457029944\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.88      0.76      0.82       105\n",
            "            CAD_and_CAM       0.76      0.71      0.73        95\n",
            "              Companies       0.53      0.56      0.55        86\n",
            "       Computer_Science       0.71      0.78      0.74        87\n",
            "            Consultants       0.52      0.55      0.53        91\n",
            "           Data_Formats       0.80      0.77      0.79       105\n",
            "    Data_Communications       0.67      0.77      0.72       114\n",
            "              Education       0.88      0.87      0.87       106\n",
            "               Graphics       0.81      0.89      0.85       113\n",
            "               Hardware       0.55      0.67      0.60        90\n",
            "               Internet       0.59      0.54      0.56       109\n",
            "       Mobile_Computing       0.68      0.72      0.70        85\n",
            "             Multimedia       0.65      0.67      0.66        91\n",
            "            Open_Source       0.67      0.61      0.64        96\n",
            "            Programming       0.46      0.57      0.51        86\n",
            "               Robotics       0.88      0.89      0.88       102\n",
            "               Security       0.81      0.79      0.80       117\n",
            "               Software       0.34      0.23      0.27       108\n",
            "                Systems       0.53      0.45      0.48       114\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.67      0.67      0.67      1900\n",
            "           weighted avg       0.67      0.68      0.67      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.3568, Train: 0.0536, Test: 0.0584\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 16.5997, Train: 0.1987, Test: 0.2095\n",
            "Early stopping:  7.949913222665061\n",
            "Epoch: 003, Loss: 11.4921, Train: 0.2237, Test: 0.2305\n",
            "Early stopping:  5.629260314395797\n",
            "Epoch: 004, Loss: 9.3644, Train: 0.3022, Test: 0.2963\n",
            "Early stopping:  4.682133502201359\n",
            "Epoch: 005, Loss: 7.9344, Train: 0.2934, Test: 0.2826\n",
            "Early stopping:  4.2397071049867785\n",
            "Epoch: 006, Loss: 7.0493, Train: 0.3182, Test: 0.3000\n",
            "Early stopping:  3.8068680424936083\n",
            "Epoch: 007, Loss: 6.0617, Train: 0.3587, Test: 0.3442\n",
            "Early stopping:  2.121207460650554\n",
            "Epoch: 008, Loss: 4.8809, Train: 0.3553, Test: 0.3447\n",
            "Early stopping:  1.7190042112772126\n",
            "Epoch: 009, Loss: 4.0635, Train: 0.4054, Test: 0.3937\n",
            "Early stopping:  1.569148533842885\n",
            "Epoch: 010, Loss: 2.9936, Train: 0.4679, Test: 0.4600\n",
            "Early stopping:  1.600240356362029\n",
            "Epoch: 011, Loss: 2.2853, Train: 0.5159, Test: 0.5089\n",
            "Early stopping:  1.4972729744325126\n",
            "Epoch: 012, Loss: 1.9593, Train: 0.5175, Test: 0.5158\n",
            "Early stopping:  1.2229995632551562\n",
            "Epoch: 013, Loss: 1.8828, Train: 0.4896, Test: 0.4842\n",
            "Early stopping:  0.9101885202741495\n",
            "Epoch: 014, Loss: 1.8676, Train: 0.4853, Test: 0.4768\n",
            "Early stopping:  0.47594230536516774\n",
            "Epoch: 015, Loss: 1.8347, Train: 0.5064, Test: 0.5079\n",
            "Early stopping:  0.18428614997614887\n",
            "Epoch: 016, Loss: 1.7637, Train: 0.5261, Test: 0.5253\n",
            "Early stopping:  0.07132640462577092\n",
            "Epoch: 017, Loss: 1.7013, Train: 0.5454, Test: 0.5363\n",
            "Early stopping:  0.07612803496859068\n",
            "Epoch: 018, Loss: 1.6449, Train: 0.5686, Test: 0.5595\n",
            "Early stopping:  0.09197852666586594\n",
            "Epoch: 019, Loss: 1.5890, Train: 0.5837, Test: 0.5721\n",
            "Early stopping:  0.09660773502258428\n",
            "Epoch: 020, Loss: 1.5413, Train: 0.5930, Test: 0.5784\n",
            "Early stopping:  0.08818407566271456\n",
            "Epoch: 021, Loss: 1.5019, Train: 0.6004, Test: 0.5947\n",
            "Early stopping:  0.0796468219539192\n",
            "Epoch: 022, Loss: 1.4632, Train: 0.6105, Test: 0.6011\n",
            "Early stopping:  0.07146866292036326\n",
            "Epoch: 023, Loss: 1.4276, Train: 0.6205, Test: 0.6074\n",
            "Early stopping:  0.06349706616797259\n",
            "Epoch: 024, Loss: 1.3969, Train: 0.6261, Test: 0.6121\n",
            "Early stopping:  0.05747530204877146\n",
            "Epoch: 025, Loss: 1.3660, Train: 0.6332, Test: 0.6184\n",
            "Early stopping:  0.053520361311254226\n",
            "Epoch: 026, Loss: 1.3336, Train: 0.6380, Test: 0.6242\n",
            "Early stopping:  0.050716633577372695\n",
            "Epoch: 027, Loss: 1.3067, Train: 0.6380, Test: 0.6258\n",
            "Early stopping:  0.048247322246969836\n",
            "Epoch: 028, Loss: 1.2869, Train: 0.6422, Test: 0.6211\n",
            "Early stopping:  0.04434204999878476\n",
            "Epoch: 029, Loss: 1.2666, Train: 0.6457, Test: 0.6247\n",
            "Early stopping:  0.0390683924637447\n",
            "Epoch: 030, Loss: 1.2417, Train: 0.6507, Test: 0.6242\n",
            "Early stopping:  0.03544870791787958\n",
            "Epoch: 031, Loss: 1.2183, Train: 0.6564, Test: 0.6237\n",
            "Early stopping:  0.035153022560697134\n",
            "Epoch: 032, Loss: 1.2004, Train: 0.6596, Test: 0.6316\n",
            "Early stopping:  0.03503177568535156\n",
            "Epoch: 033, Loss: 1.1857, Train: 0.6650, Test: 0.6321\n",
            "Early stopping:  0.032297283677037186\n",
            "Epoch: 034, Loss: 1.1710, Train: 0.6699, Test: 0.6316\n",
            "Early stopping:  0.02765863505311376\n",
            "Epoch: 035, Loss: 1.1552, Train: 0.6750, Test: 0.6400\n",
            "Early stopping:  0.0245989851802415\n",
            "Epoch: 036, Loss: 1.1393, Train: 0.6787, Test: 0.6395\n",
            "Early stopping:  0.02417177656752452\n",
            "Epoch: 037, Loss: 1.1243, Train: 0.6816, Test: 0.6447\n",
            "Early stopping:  0.024453557966355167\n",
            "Epoch: 038, Loss: 1.1112, Train: 0.6846, Test: 0.6437\n",
            "Early stopping:  0.023830113251908636\n",
            "Epoch: 039, Loss: 1.0992, Train: 0.6886, Test: 0.6484\n",
            "Early stopping:  0.02218164808032286\n",
            "Epoch: 040, Loss: 1.0873, Train: 0.6924, Test: 0.6532\n",
            "Early stopping:  0.020405823118409307\n",
            "Epoch: 041, Loss: 1.0750, Train: 0.6946, Test: 0.6516\n",
            "Early stopping:  0.019369690531899925\n",
            "Epoch: 042, Loss: 1.0623, Train: 0.6980, Test: 0.6505\n",
            "Early stopping:  0.01930440067567588\n",
            "Epoch: 043, Loss: 1.0500, Train: 0.7017, Test: 0.6553\n",
            "Early stopping:  0.019536262692795677\n",
            "Epoch: 044, Loss: 1.0380, Train: 0.7036, Test: 0.6579\n",
            "Early stopping:  0.0195442536590369\n",
            "Epoch: 045, Loss: 1.0261, Train: 0.7068, Test: 0.6611\n",
            "Early stopping:  0.01928681083031215\n",
            "Epoch: 046, Loss: 1.0149, Train: 0.7100, Test: 0.6642\n",
            "Early stopping:  0.018754569074838245\n",
            "Epoch: 047, Loss: 1.0045, Train: 0.7121, Test: 0.6658\n",
            "Early stopping:  0.01805428395609109\n",
            "Epoch: 048, Loss: 0.9944, Train: 0.7142, Test: 0.6658\n",
            "Early stopping:  0.017229917823997937\n",
            "Epoch: 049, Loss: 0.9844, Train: 0.7172, Test: 0.6653\n",
            "Early stopping:  0.016433593889651613\n",
            "Epoch: 050, Loss: 0.9746, Train: 0.7178, Test: 0.6668\n",
            "Early stopping:  0.01591991748342506\n",
            "Epoch: 051, Loss: 0.9649, Train: 0.7203, Test: 0.6711\n",
            "Early stopping:  0.01565195318573677\n",
            "Epoch: 052, Loss: 0.9554, Train: 0.7234, Test: 0.6721\n",
            "Early stopping:  0.015420242802701137\n",
            "Epoch: 053, Loss: 0.9466, Train: 0.7267, Test: 0.6700\n",
            "Early stopping:  0.015002183432801028\n",
            "Epoch: 054, Loss: 0.9381, Train: 0.7283, Test: 0.6726\n",
            "Early stopping:  0.014439188093385276\n",
            "Epoch: 055, Loss: 0.9293, Train: 0.7293, Test: 0.6742\n",
            "Early stopping:  0.013997821847357103\n",
            "Epoch: 056, Loss: 0.9204, Train: 0.7305, Test: 0.6711\n",
            "Early stopping:  0.01379657723485016\n",
            "Epoch: 057, Loss: 0.9123, Train: 0.7313, Test: 0.6679\n",
            "Early stopping:  0.013623959102101369\n",
            "Epoch: 058, Loss: 0.9043, Train: 0.7343, Test: 0.6716\n",
            "Early stopping:  0.013382185475245563\n",
            "Epoch: 059, Loss: 0.8960, Train: 0.7367, Test: 0.6742\n",
            "Early stopping:  0.013069259840747274\n",
            "Epoch: 060, Loss: 0.8880, Train: 0.7393, Test: 0.6747\n",
            "Early stopping:  0.012824125789209887\n",
            "Epoch: 061, Loss: 0.8802, Train: 0.7408, Test: 0.6779\n",
            "Early stopping:  0.012740357205620044\n",
            "Epoch: 062, Loss: 0.8722, Train: 0.7442, Test: 0.6789\n",
            "Early stopping:  0.012642657317212987\n",
            "Epoch: 063, Loss: 0.8644, Train: 0.7475, Test: 0.6768\n",
            "Early stopping:  0.01250041570865751\n",
            "Epoch: 064, Loss: 0.8567, Train: 0.7484, Test: 0.6789\n",
            "Early stopping:  0.012405875487390203\n",
            "Epoch: 065, Loss: 0.8491, Train: 0.7500, Test: 0.6837\n",
            "Early stopping:  0.012279456869011123\n",
            "Epoch: 066, Loss: 0.8416, Train: 0.7516, Test: 0.6842\n",
            "Early stopping:  0.012091298262625914\n",
            "Epoch: 067, Loss: 0.8342, Train: 0.7549, Test: 0.6842\n",
            "Early stopping:  0.011926170579609244\n",
            "Epoch: 068, Loss: 0.8270, Train: 0.7564, Test: 0.6853\n",
            "Early stopping:  0.01175391332206198\n",
            "Epoch: 069, Loss: 0.8198, Train: 0.7582, Test: 0.6858\n",
            "Early stopping:  0.011595504936313828\n",
            "Epoch: 070, Loss: 0.8127, Train: 0.7599, Test: 0.6868\n",
            "Early stopping:  0.011425714030628429\n",
            "Epoch: 071, Loss: 0.8059, Train: 0.7624, Test: 0.6874\n",
            "Early stopping:  0.011217566808131756\n",
            "Epoch: 072, Loss: 0.7991, Train: 0.7641, Test: 0.6900\n",
            "Early stopping:  0.011014625476914461\n",
            "Epoch: 073, Loss: 0.7923, Train: 0.7668, Test: 0.6900\n",
            "Early stopping:  0.010858666330468909\n",
            "Epoch: 074, Loss: 0.7855, Train: 0.7692, Test: 0.6900\n",
            "Early stopping:  0.010770157974807121\n",
            "Epoch: 075, Loss: 0.7787, Train: 0.7724, Test: 0.6889\n",
            "Early stopping:  0.01072613762743437\n",
            "Epoch: 076, Loss: 0.7721, Train: 0.7755, Test: 0.6889\n",
            "Early stopping:  0.010667448011533704\n",
            "Epoch: 077, Loss: 0.7655, Train: 0.7771, Test: 0.6895\n",
            "Early stopping:  0.010574891762670516\n",
            "Epoch: 078, Loss: 0.7590, Train: 0.7788, Test: 0.6895\n",
            "Early stopping:  0.010480146017307936\n",
            "Epoch: 079, Loss: 0.7525, Train: 0.7795, Test: 0.6905\n",
            "Early stopping:  0.010395313284162638\n",
            "Epoch: 080, Loss: 0.7461, Train: 0.7816, Test: 0.6900\n",
            "Early stopping:  0.010303941496146498\n",
            "Epoch: 081, Loss: 0.7398, Train: 0.7814, Test: 0.6900\n",
            "Early stopping:  0.010182248337289113\n",
            "Epoch: 082, Loss: 0.7335, Train: 0.7845, Test: 0.6900\n",
            "Early stopping:  0.01007159620818189\n",
            "Epoch: 083, Loss: 0.7273, Train: 0.7864, Test: 0.6884\n",
            "Early stopping:  0.009954356928943671\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 13, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.81      0.76      0.79       106\n",
            "            CAD_and_CAM       0.70      0.73      0.71        92\n",
            "              Companies       0.60      0.51      0.55        98\n",
            "       Computer_Science       0.75      0.83      0.79        99\n",
            "            Consultants       0.60      0.64      0.62       122\n",
            "           Data_Formats       0.82      0.75      0.78       114\n",
            "    Data_Communications       0.69      0.69      0.69       112\n",
            "              Education       0.81      0.87      0.84       103\n",
            "               Graphics       0.84      0.91      0.87       109\n",
            "               Hardware       0.63      0.60      0.61        97\n",
            "               Internet       0.68      0.70      0.69        99\n",
            "       Mobile_Computing       0.81      0.76      0.79       114\n",
            "             Multimedia       0.66      0.66      0.66        95\n",
            "            Open_Source       0.60      0.69      0.64        84\n",
            "            Programming       0.48      0.50      0.49        98\n",
            "               Robotics       0.91      0.91      0.91        91\n",
            "               Security       0.69      0.80      0.74        85\n",
            "               Software       0.36      0.26      0.31       102\n",
            "                Systems       0.47      0.46      0.47        80\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.68      0.69      0.68      1900\n",
            "           weighted avg       0.68      0.69      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.1396, Train: 0.1171, Test: 0.1100\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 12.1429, Train: 0.1738, Test: 0.1632\n",
            "Early stopping:  4.245015071976995\n",
            "Epoch: 003, Loss: 11.0395, Train: 0.2659, Test: 0.2705\n",
            "Early stopping:  3.1954872528135545\n",
            "Epoch: 004, Loss: 10.0546, Train: 0.2926, Test: 0.2774\n",
            "Early stopping:  2.6128729365607066\n",
            "Epoch: 005, Loss: 8.0370, Train: 0.3086, Test: 0.2921\n",
            "Early stopping:  2.402807263099867\n",
            "Epoch: 006, Loss: 6.4811, Train: 0.3434, Test: 0.3374\n",
            "Early stopping:  2.286038874424296\n",
            "Epoch: 007, Loss: 4.9339, Train: 0.3662, Test: 0.3626\n",
            "Early stopping:  2.5054733589124636\n",
            "Epoch: 008, Loss: 3.7900, Train: 0.3737, Test: 0.3611\n",
            "Early stopping:  2.4833138354701356\n",
            "Epoch: 009, Loss: 2.9894, Train: 0.4004, Test: 0.3937\n",
            "Early stopping:  2.038673924256314\n",
            "Epoch: 010, Loss: 2.4201, Train: 0.4314, Test: 0.4226\n",
            "Early stopping:  1.6212668803187686\n",
            "Epoch: 011, Loss: 2.0942, Train: 0.4626, Test: 0.4505\n",
            "Early stopping:  1.1423157481793333\n",
            "Epoch: 012, Loss: 1.9253, Train: 0.4787, Test: 0.4600\n",
            "Early stopping:  0.7585566286267789\n",
            "Epoch: 013, Loss: 1.8873, Train: 0.4887, Test: 0.4732\n",
            "Early stopping:  0.45716432729130074\n",
            "Epoch: 014, Loss: 1.8602, Train: 0.5030, Test: 0.4905\n",
            "Early stopping:  0.2324770329298993\n",
            "Epoch: 015, Loss: 1.8245, Train: 0.5178, Test: 0.5079\n",
            "Early stopping:  0.1050327747580049\n",
            "Epoch: 016, Loss: 1.7786, Train: 0.5284, Test: 0.5232\n",
            "Early stopping:  0.05651919638664663\n",
            "Epoch: 017, Loss: 1.7291, Train: 0.5345, Test: 0.5258\n",
            "Early stopping:  0.0633715071982792\n",
            "Epoch: 018, Loss: 1.6834, Train: 0.5400, Test: 0.5316\n",
            "Early stopping:  0.07111115547361405\n",
            "Epoch: 019, Loss: 1.6445, Train: 0.5499, Test: 0.5374\n",
            "Early stopping:  0.07203473435872866\n",
            "Epoch: 020, Loss: 1.6074, Train: 0.5592, Test: 0.5484\n",
            "Early stopping:  0.06764253576409457\n",
            "Epoch: 021, Loss: 1.5666, Train: 0.5672, Test: 0.5632\n",
            "Early stopping:  0.0634251444740861\n",
            "Epoch: 022, Loss: 1.5226, Train: 0.5766, Test: 0.5747\n",
            "Early stopping:  0.06319459845681896\n",
            "Epoch: 023, Loss: 1.4796, Train: 0.5879, Test: 0.5837\n",
            "Early stopping:  0.06558244162821106\n",
            "Epoch: 024, Loss: 1.4423, Train: 0.5961, Test: 0.5905\n",
            "Early stopping:  0.06597695432112875\n",
            "Epoch: 025, Loss: 1.4112, Train: 0.6043, Test: 0.6000\n",
            "Early stopping:  0.062002231012748585\n",
            "Epoch: 026, Loss: 1.3847, Train: 0.6111, Test: 0.6089\n",
            "Early stopping:  0.05469211402808363\n",
            "Epoch: 027, Loss: 1.3614, Train: 0.6153, Test: 0.6100\n",
            "Early stopping:  0.04670723588569767\n",
            "Epoch: 028, Loss: 1.3419, Train: 0.6191, Test: 0.6068\n",
            "Early stopping:  0.039776242052622805\n",
            "Epoch: 029, Loss: 1.3267, Train: 0.6221, Test: 0.6132\n",
            "Early stopping:  0.033664125306914\n",
            "Epoch: 030, Loss: 1.3135, Train: 0.6245, Test: 0.6179\n",
            "Early stopping:  0.028191883771539722\n",
            "Epoch: 031, Loss: 1.2988, Train: 0.6291, Test: 0.6195\n",
            "Early stopping:  0.024344479684558416\n",
            "Epoch: 032, Loss: 1.2810, Train: 0.6354, Test: 0.6216\n",
            "Early stopping:  0.0237099640059135\n",
            "Epoch: 033, Loss: 1.2614, Train: 0.6420, Test: 0.6263\n",
            "Early stopping:  0.025877849817406963\n",
            "Epoch: 034, Loss: 1.2430, Train: 0.6487, Test: 0.6300\n",
            "Early stopping:  0.028249804924286113\n",
            "Epoch: 035, Loss: 1.2271, Train: 0.6530, Test: 0.6337\n",
            "Early stopping:  0.028712927588884486\n",
            "Epoch: 036, Loss: 1.2116, Train: 0.6562, Test: 0.6374\n",
            "Early stopping:  0.02738830540310995\n",
            "Epoch: 037, Loss: 1.1954, Train: 0.6616, Test: 0.6368\n",
            "Early stopping:  0.025840139175645434\n",
            "Epoch: 038, Loss: 1.1789, Train: 0.6632, Test: 0.6374\n",
            "Early stopping:  0.02527670882521492\n",
            "Epoch: 039, Loss: 1.1634, Train: 0.6649, Test: 0.6426\n",
            "Early stopping:  0.025321037260276822\n",
            "Epoch: 040, Loss: 1.1493, Train: 0.6701, Test: 0.6437\n",
            "Early stopping:  0.02479363773512144\n",
            "Epoch: 041, Loss: 1.1367, Train: 0.6733, Test: 0.6468\n",
            "Early stopping:  0.023283878230999554\n",
            "Epoch: 042, Loss: 1.1243, Train: 0.6770, Test: 0.6500\n",
            "Early stopping:  0.021523021407756714\n",
            "Epoch: 043, Loss: 1.1105, Train: 0.6809, Test: 0.6542\n",
            "Early stopping:  0.020670590853219734\n",
            "Epoch: 044, Loss: 1.0971, Train: 0.6864, Test: 0.6584\n",
            "Early stopping:  0.020638463579761582\n",
            "Epoch: 045, Loss: 1.0856, Train: 0.6887, Test: 0.6574\n",
            "Early stopping:  0.020448030376665986\n",
            "Epoch: 046, Loss: 1.0754, Train: 0.6933, Test: 0.6579\n",
            "Early stopping:  0.019413207934155463\n",
            "Epoch: 047, Loss: 1.0654, Train: 0.6958, Test: 0.6611\n",
            "Early stopping:  0.017729578979632\n",
            "Epoch: 048, Loss: 1.0554, Train: 0.6986, Test: 0.6600\n",
            "Early stopping:  0.016389554139810136\n",
            "Epoch: 049, Loss: 1.0455, Train: 0.7009, Test: 0.6642\n",
            "Early stopping:  0.01584757147140275\n",
            "Epoch: 050, Loss: 1.0356, Train: 0.7030, Test: 0.6647\n",
            "Early stopping:  0.01573965022340292\n",
            "Epoch: 051, Loss: 1.0261, Train: 0.7067, Test: 0.6653\n",
            "Early stopping:  0.01555792180568037\n",
            "Epoch: 052, Loss: 1.0176, Train: 0.7082, Test: 0.6674\n",
            "Early stopping:  0.015050120458039749\n",
            "Epoch: 053, Loss: 1.0092, Train: 0.7124, Test: 0.6679\n",
            "Early stopping:  0.014344941220954128\n",
            "Epoch: 054, Loss: 1.0003, Train: 0.7150, Test: 0.6732\n",
            "Early stopping:  0.013821354133162395\n",
            "Epoch: 055, Loss: 0.9916, Train: 0.7171, Test: 0.6726\n",
            "Early stopping:  0.013632170649891586\n",
            "Epoch: 056, Loss: 0.9837, Train: 0.7209, Test: 0.6705\n",
            "Early stopping:  0.013487072831070887\n",
            "Epoch: 057, Loss: 0.9761, Train: 0.7221, Test: 0.6716\n",
            "Early stopping:  0.01312148038618392\n",
            "Epoch: 058, Loss: 0.9680, Train: 0.7229, Test: 0.6726\n",
            "Early stopping:  0.012690770064339513\n",
            "Epoch: 059, Loss: 0.9600, Train: 0.7241, Test: 0.6716\n",
            "Early stopping:  0.012469057332922475\n",
            "Epoch: 060, Loss: 0.9526, Train: 0.7272, Test: 0.6732\n",
            "Early stopping:  0.012387124491442176\n",
            "Epoch: 061, Loss: 0.9451, Train: 0.7291, Test: 0.6726\n",
            "Early stopping:  0.012216639670723511\n",
            "Epoch: 062, Loss: 0.9377, Train: 0.7296, Test: 0.6779\n",
            "Early stopping:  0.011937323724846549\n",
            "Epoch: 063, Loss: 0.9306, Train: 0.7337, Test: 0.6800\n",
            "Early stopping:  0.011672678744010003\n",
            "Epoch: 064, Loss: 0.9238, Train: 0.7349, Test: 0.6805\n",
            "Early stopping:  0.011401516421315532\n",
            "Epoch: 065, Loss: 0.9169, Train: 0.7349, Test: 0.6800\n",
            "Early stopping:  0.011140219298453186\n",
            "Epoch: 066, Loss: 0.9101, Train: 0.7363, Test: 0.6816\n",
            "Early stopping:  0.010906605486207295\n",
            "Epoch: 067, Loss: 0.9036, Train: 0.7382, Test: 0.6800\n",
            "Early stopping:  0.010689413588352716\n",
            "Epoch: 068, Loss: 0.8974, Train: 0.7395, Test: 0.6768\n",
            "Early stopping:  0.010433503300430962\n",
            "Epoch: 069, Loss: 0.8912, Train: 0.7392, Test: 0.6795\n",
            "Early stopping:  0.010103898717217489\n",
            "Epoch: 070, Loss: 0.8849, Train: 0.7417, Test: 0.6800\n",
            "Early stopping:  0.009901243436400408\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.82      0.80      0.81        94\n",
            "            CAD_and_CAM       0.70      0.70      0.70       105\n",
            "              Companies       0.53      0.50      0.51       102\n",
            "       Computer_Science       0.76      0.71      0.73       102\n",
            "            Consultants       0.60      0.59      0.59       120\n",
            "           Data_Formats       0.79      0.77      0.78        98\n",
            "    Data_Communications       0.64      0.79      0.71        92\n",
            "              Education       0.79      0.88      0.84        92\n",
            "               Graphics       0.87      0.94      0.90       111\n",
            "               Hardware       0.66      0.66      0.66       105\n",
            "               Internet       0.62      0.64      0.63       102\n",
            "       Mobile_Computing       0.84      0.71      0.77       112\n",
            "             Multimedia       0.69      0.73      0.71       106\n",
            "            Open_Source       0.61      0.70      0.65        96\n",
            "            Programming       0.47      0.50      0.48        88\n",
            "               Robotics       0.89      0.84      0.86       104\n",
            "               Security       0.80      0.74      0.77        89\n",
            "               Software       0.27      0.21      0.24        94\n",
            "                Systems       0.48      0.49      0.48        88\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.67      0.68      0.67      1900\n",
            "           weighted avg       0.68      0.68      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.3753, Train: 0.1279, Test: 0.1295\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 11.2331, Train: 0.2043, Test: 0.2026\n",
            "Early stopping:  3.434958375569009\n",
            "Epoch: 003, Loss: 12.9520, Train: 0.2605, Test: 0.2705\n",
            "Early stopping:  3.4109124671007236\n",
            "Epoch: 004, Loss: 13.1174, Train: 0.2778, Test: 0.2847\n",
            "Early stopping:  3.1469472751097376\n",
            "Epoch: 005, Loss: 14.2314, Train: 0.2721, Test: 0.2653\n",
            "Early stopping:  3.10181655005766\n",
            "Epoch: 006, Loss: 12.3199, Train: 0.3132, Test: 0.3089\n",
            "Early stopping:  1.1016143831136973\n",
            "Epoch: 007, Loss: 10.3874, Train: 0.3478, Test: 0.3426\n",
            "Early stopping:  1.4165973154276925\n",
            "Epoch: 008, Loss: 8.3855, Train: 0.4064, Test: 0.4037\n",
            "Early stopping:  2.318903359534125\n",
            "Epoch: 009, Loss: 6.6007, Train: 0.4604, Test: 0.4658\n",
            "Early stopping:  3.0355266569954527\n",
            "Epoch: 010, Loss: 4.9854, Train: 0.4749, Test: 0.4737\n",
            "Early stopping:  2.9206453812192303\n",
            "Epoch: 011, Loss: 4.0761, Train: 0.4764, Test: 0.4726\n",
            "Early stopping:  2.554285006130948\n",
            "Epoch: 012, Loss: 3.4808, Train: 0.4838, Test: 0.4689\n",
            "Early stopping:  1.9941791749292348\n",
            "Epoch: 013, Loss: 2.9492, Train: 0.4933, Test: 0.4874\n",
            "Early stopping:  1.435158066786176\n",
            "Epoch: 014, Loss: 2.4819, Train: 0.4917, Test: 0.4837\n",
            "Early stopping:  0.9790025108020783\n",
            "Epoch: 015, Loss: 2.2035, Train: 0.4779, Test: 0.4784\n",
            "Early stopping:  0.7561972132122291\n",
            "Epoch: 016, Loss: 2.1196, Train: 0.4705, Test: 0.4726\n",
            "Early stopping:  0.5675802120543668\n",
            "Epoch: 017, Loss: 2.0484, Train: 0.4626, Test: 0.4726\n",
            "Early stopping:  0.36789844500980295\n",
            "Epoch: 018, Loss: 2.0113, Train: 0.4711, Test: 0.4832\n",
            "Early stopping:  0.18767424113751213\n",
            "Epoch: 019, Loss: 1.9599, Train: 0.4833, Test: 0.4863\n",
            "Early stopping:  0.09526785728939496\n",
            "Epoch: 020, Loss: 1.8838, Train: 0.5042, Test: 0.5111\n",
            "Early stopping:  0.08912309503687728\n",
            "Epoch: 021, Loss: 1.8042, Train: 0.5263, Test: 0.5321\n",
            "Early stopping:  0.09848049558667767\n",
            "Epoch: 022, Loss: 1.7295, Train: 0.5430, Test: 0.5542\n",
            "Early stopping:  0.11401301482892542\n",
            "Epoch: 023, Loss: 1.6678, Train: 0.5508, Test: 0.5600\n",
            "Early stopping:  0.116890378128573\n",
            "Epoch: 024, Loss: 1.6208, Train: 0.5561, Test: 0.5663\n",
            "Early stopping:  0.1052736373340699\n",
            "Epoch: 025, Loss: 1.5847, Train: 0.5617, Test: 0.5732\n",
            "Early stopping:  0.0874775385308153\n",
            "Epoch: 026, Loss: 1.5531, Train: 0.5676, Test: 0.5816\n",
            "Early stopping:  0.06958243448081823\n",
            "Epoch: 027, Loss: 1.5217, Train: 0.5814, Test: 0.5905\n",
            "Early stopping:  0.057115033006203825\n",
            "Epoch: 028, Loss: 1.4873, Train: 0.5971, Test: 0.6026\n",
            "Early stopping:  0.052209895281442764\n",
            "Epoch: 029, Loss: 1.4512, Train: 0.5992, Test: 0.6011\n",
            "Early stopping:  0.05265987660451284\n",
            "Epoch: 030, Loss: 1.4197, Train: 0.5997, Test: 0.6021\n",
            "Early stopping:  0.05334770962920019\n",
            "Epoch: 031, Loss: 1.3942, Train: 0.6007, Test: 0.6095\n",
            "Early stopping:  0.05112581906717573\n",
            "Epoch: 032, Loss: 1.3673, Train: 0.6117, Test: 0.6079\n",
            "Early stopping:  0.047068249018526916\n",
            "Epoch: 033, Loss: 1.3372, Train: 0.6246, Test: 0.6153\n",
            "Early stopping:  0.04436422284078959\n",
            "Epoch: 034, Loss: 1.3113, Train: 0.6304, Test: 0.6232\n",
            "Early stopping:  0.04332931812119079\n",
            "Epoch: 035, Loss: 1.2960, Train: 0.6324, Test: 0.6274\n",
            "Early stopping:  0.04013948125284451\n",
            "Epoch: 036, Loss: 1.2812, Train: 0.6366, Test: 0.6316\n",
            "Early stopping:  0.03422878650520206\n",
            "Epoch: 037, Loss: 1.2583, Train: 0.6459, Test: 0.6368\n",
            "Early stopping:  0.029871061332105302\n",
            "Epoch: 038, Loss: 1.2328, Train: 0.6538, Test: 0.6442\n",
            "Early stopping:  0.03100937552957437\n",
            "Epoch: 039, Loss: 1.2123, Train: 0.6596, Test: 0.6479\n",
            "Early stopping:  0.03422073837145551\n",
            "Epoch: 040, Loss: 1.1974, Train: 0.6637, Test: 0.6489\n",
            "Early stopping:  0.033889882442558264\n",
            "Epoch: 041, Loss: 1.1853, Train: 0.6654, Test: 0.6537\n",
            "Early stopping:  0.028999935584888923\n",
            "Epoch: 042, Loss: 1.1733, Train: 0.6688, Test: 0.6600\n",
            "Early stopping:  0.023250357927008806\n",
            "Epoch: 043, Loss: 1.1607, Train: 0.6724, Test: 0.6563\n",
            "Early stopping:  0.020151140791823573\n",
            "Epoch: 044, Loss: 1.1475, Train: 0.6771, Test: 0.6595\n",
            "Early stopping:  0.019677920272405204\n",
            "Epoch: 045, Loss: 1.1338, Train: 0.6812, Test: 0.6653\n",
            "Early stopping:  0.020379106007449087\n",
            "Epoch: 046, Loss: 1.1197, Train: 0.6824, Test: 0.6616\n",
            "Early stopping:  0.021214508913573054\n",
            "Epoch: 047, Loss: 1.1062, Train: 0.6837, Test: 0.6584\n",
            "Early stopping:  0.021638856837172926\n",
            "Epoch: 048, Loss: 1.0933, Train: 0.6861, Test: 0.6563\n",
            "Early stopping:  0.021522901915007792\n",
            "Epoch: 049, Loss: 1.0813, Train: 0.6892, Test: 0.6553\n",
            "Early stopping:  0.02077149969225494\n",
            "Epoch: 050, Loss: 1.0701, Train: 0.6945, Test: 0.6563\n",
            "Early stopping:  0.019630980840174016\n",
            "Epoch: 051, Loss: 1.0580, Train: 0.6983, Test: 0.6542\n",
            "Early stopping:  0.01888658639837806\n",
            "Epoch: 052, Loss: 1.0454, Train: 0.6988, Test: 0.6600\n",
            "Early stopping:  0.018829306711763152\n",
            "Epoch: 053, Loss: 1.0337, Train: 0.7018, Test: 0.6663\n",
            "Early stopping:  0.018955882329409476\n",
            "Epoch: 054, Loss: 1.0231, Train: 0.7058, Test: 0.6637\n",
            "Early stopping:  0.018705457445782784\n",
            "Epoch: 055, Loss: 1.0126, Train: 0.7088, Test: 0.6684\n",
            "Early stopping:  0.017908050725213225\n",
            "Epoch: 056, Loss: 1.0024, Train: 0.7114, Test: 0.6689\n",
            "Early stopping:  0.016932798532849083\n",
            "Epoch: 057, Loss: 0.9928, Train: 0.7143, Test: 0.6737\n",
            "Early stopping:  0.01620530276148719\n",
            "Epoch: 058, Loss: 0.9828, Train: 0.7166, Test: 0.6747\n",
            "Early stopping:  0.01588293352411218\n",
            "Epoch: 059, Loss: 0.9723, Train: 0.7201, Test: 0.6763\n",
            "Early stopping:  0.01585848479217436\n",
            "Epoch: 060, Loss: 0.9622, Train: 0.7233, Test: 0.6800\n",
            "Early stopping:  0.01598145996929016\n",
            "Epoch: 061, Loss: 0.9525, Train: 0.7261, Test: 0.6800\n",
            "Early stopping:  0.016025011608794583\n",
            "Epoch: 062, Loss: 0.9432, Train: 0.7270, Test: 0.6774\n",
            "Early stopping:  0.01566059687515869\n",
            "Epoch: 063, Loss: 0.9339, Train: 0.7297, Test: 0.6811\n",
            "Early stopping:  0.015136567852282734\n",
            "Epoch: 064, Loss: 0.9247, Train: 0.7314, Test: 0.6805\n",
            "Early stopping:  0.01478142417760752\n",
            "Epoch: 065, Loss: 0.9157, Train: 0.7359, Test: 0.6800\n",
            "Early stopping:  0.014532480658054407\n",
            "Epoch: 066, Loss: 0.9069, Train: 0.7371, Test: 0.6774\n",
            "Early stopping:  0.014351264726812932\n",
            "Epoch: 067, Loss: 0.8982, Train: 0.7393, Test: 0.6784\n",
            "Early stopping:  0.01412774627584642\n",
            "Epoch: 068, Loss: 0.8894, Train: 0.7425, Test: 0.6768\n",
            "Early stopping:  0.01395852582938559\n",
            "Epoch: 069, Loss: 0.8804, Train: 0.7454, Test: 0.6768\n",
            "Early stopping:  0.01394444520549371\n",
            "Epoch: 070, Loss: 0.8718, Train: 0.7489, Test: 0.6789\n",
            "Early stopping:  0.013913149811276584\n",
            "Epoch: 071, Loss: 0.8632, Train: 0.7495, Test: 0.6795\n",
            "Early stopping:  0.013834836261516012\n",
            "Epoch: 072, Loss: 0.8549, Train: 0.7518, Test: 0.6800\n",
            "Early stopping:  0.01361985125492737\n",
            "Epoch: 073, Loss: 0.8467, Train: 0.7536, Test: 0.6805\n",
            "Early stopping:  0.013333800065614368\n",
            "Epoch: 074, Loss: 0.8386, Train: 0.7564, Test: 0.6816\n",
            "Early stopping:  0.013091244670876473\n",
            "Epoch: 075, Loss: 0.8304, Train: 0.7584, Test: 0.6805\n",
            "Early stopping:  0.012939003657816526\n",
            "Epoch: 076, Loss: 0.8223, Train: 0.7620, Test: 0.6811\n",
            "Early stopping:  0.012869408171079314\n",
            "Epoch: 077, Loss: 0.8145, Train: 0.7633, Test: 0.6811\n",
            "Early stopping:  0.012758762467546607\n",
            "Epoch: 078, Loss: 0.8068, Train: 0.7651, Test: 0.6795\n",
            "Early stopping:  0.012589029426022634\n",
            "Epoch: 079, Loss: 0.7991, Train: 0.7674, Test: 0.6811\n",
            "Early stopping:  0.01235040445845912\n",
            "Epoch: 080, Loss: 0.7915, Train: 0.7691, Test: 0.6821\n",
            "Early stopping:  0.012164748072661116\n",
            "Epoch: 081, Loss: 0.7839, Train: 0.7724, Test: 0.6826\n",
            "Early stopping:  0.012093390316259257\n",
            "Epoch: 082, Loss: 0.7764, Train: 0.7758, Test: 0.6811\n",
            "Early stopping:  0.012024620978799206\n",
            "Epoch: 083, Loss: 0.7691, Train: 0.7768, Test: 0.6800\n",
            "Early stopping:  0.0119001265083275\n",
            "Epoch: 084, Loss: 0.7619, Train: 0.7799, Test: 0.6826\n",
            "Early stopping:  0.011722856283797066\n",
            "Epoch: 085, Loss: 0.7548, Train: 0.7818, Test: 0.6816\n",
            "Early stopping:  0.011484710914943734\n",
            "Epoch: 086, Loss: 0.7476, Train: 0.7837, Test: 0.6821\n",
            "Early stopping:  0.011373047229828178\n",
            "Epoch: 087, Loss: 0.7407, Train: 0.7858, Test: 0.6811\n",
            "Early stopping:  0.01124308489909527\n",
            "Epoch: 088, Loss: 0.7342, Train: 0.7883, Test: 0.6826\n",
            "Early stopping:  0.010985834356150694\n",
            "Epoch: 089, Loss: 0.7273, Train: 0.7916, Test: 0.6779\n",
            "Early stopping:  0.010832543335341492\n",
            "Epoch: 090, Loss: 0.7203, Train: 0.7920, Test: 0.6763\n",
            "Early stopping:  0.010744657306071855\n",
            "Epoch: 091, Loss: 0.7134, Train: 0.7951, Test: 0.6789\n",
            "Early stopping:  0.01083034634551382\n",
            "Epoch: 092, Loss: 0.7070, Train: 0.7958, Test: 0.6800\n",
            "Early stopping:  0.010806845385418718\n",
            "Epoch: 093, Loss: 0.7008, Train: 0.7982, Test: 0.6826\n",
            "Early stopping:  0.010487936156677868\n",
            "Epoch: 094, Loss: 0.6945, Train: 0.7987, Test: 0.6816\n",
            "Early stopping:  0.010157247284480954\n",
            "Epoch: 095, Loss: 0.6883, Train: 0.8026, Test: 0.6842\n",
            "Early stopping:  0.009904465191378768\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.84      0.81        85\n",
            "            CAD_and_CAM       0.71      0.70      0.70       100\n",
            "              Companies       0.50      0.49      0.49       104\n",
            "       Computer_Science       0.73      0.73      0.73        89\n",
            "            Consultants       0.61      0.59      0.60       111\n",
            "           Data_Formats       0.78      0.78      0.78       103\n",
            "    Data_Communications       0.79      0.75      0.77       112\n",
            "              Education       0.90      0.84      0.87       115\n",
            "               Graphics       0.85      0.92      0.88       103\n",
            "               Hardware       0.65      0.67      0.66       105\n",
            "               Internet       0.57      0.66      0.61        77\n",
            "       Mobile_Computing       0.78      0.71      0.75       101\n",
            "             Multimedia       0.72      0.76      0.74        92\n",
            "            Open_Source       0.61      0.64      0.62       113\n",
            "            Programming       0.49      0.45      0.47       106\n",
            "               Robotics       0.91      0.87      0.89        93\n",
            "               Security       0.76      0.77      0.76       107\n",
            "               Software       0.24      0.24      0.24        92\n",
            "                Systems       0.57      0.59      0.58        92\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.69      0.68      0.68      1900\n",
            "\n",
            "time: 44.5 s (started: 2024-08-17 14:13:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7a2eb3-ed8b-4a02-cf81-3e5a77dfc0da",
        "id": "LGNe4AS6tuWi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 396 ms (started: 2024-08-17 14:17:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "IWyiswvetuWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT','80%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899399b2-2444-456e-c562-06c4ea1436d1",
        "id": "3dULufOytuWi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9516, Train: 0.3708, Test: 0.3642\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8218, Train: 0.5242, Test: 0.5100\n",
            "Early stopping:  0.09182567602547291\n",
            "Epoch: 003, Loss: 2.6574, Train: 0.5462, Test: 0.5168\n",
            "Early stopping:  0.1474472939959434\n",
            "Epoch: 004, Loss: 2.4685, Train: 0.5671, Test: 0.5389\n",
            "Early stopping:  0.20904086312435768\n",
            "Epoch: 005, Loss: 2.2699, Train: 0.5899, Test: 0.5705\n",
            "Early stopping:  0.27232267987651354\n",
            "Epoch: 006, Loss: 2.0711, Train: 0.6104, Test: 0.5905\n",
            "Early stopping:  0.29885855196152794\n",
            "Epoch: 007, Loss: 1.8796, Train: 0.6309, Test: 0.6184\n",
            "Early stopping:  0.3088029222987719\n",
            "Epoch: 008, Loss: 1.7019, Train: 0.6404, Test: 0.6263\n",
            "Early stopping:  0.30419721714024717\n",
            "Epoch: 009, Loss: 1.5475, Train: 0.6449, Test: 0.6311\n",
            "Early stopping:  0.2871609024585015\n",
            "Epoch: 010, Loss: 1.4215, Train: 0.6512, Test: 0.6321\n",
            "Early stopping:  0.25878232986934796\n",
            "Epoch: 011, Loss: 1.3216, Train: 0.6562, Test: 0.6411\n",
            "Early stopping:  0.22216628590195142\n",
            "Epoch: 012, Loss: 1.2469, Train: 0.6624, Test: 0.6463\n",
            "Early stopping:  0.1813135164486129\n",
            "Epoch: 013, Loss: 1.1921, Train: 0.6670, Test: 0.6458\n",
            "Early stopping:  0.14178203801841696\n",
            "Epoch: 014, Loss: 1.1515, Train: 0.6709, Test: 0.6521\n",
            "Early stopping:  0.107460981114981\n",
            "Epoch: 015, Loss: 1.1214, Train: 0.6784, Test: 0.6563\n",
            "Early stopping:  0.07962488725270865\n",
            "Epoch: 016, Loss: 1.0954, Train: 0.6839, Test: 0.6616\n",
            "Early stopping:  0.0598101394881908\n",
            "Epoch: 017, Loss: 1.0721, Train: 0.6897, Test: 0.6658\n",
            "Early stopping:  0.047121103113732576\n",
            "Epoch: 018, Loss: 1.0496, Train: 0.6947, Test: 0.6726\n",
            "Early stopping:  0.04011127827823064\n",
            "Epoch: 019, Loss: 1.0275, Train: 0.7012, Test: 0.6753\n",
            "Early stopping:  0.03695207907389025\n",
            "Epoch: 020, Loss: 1.0060, Train: 0.7064, Test: 0.6816\n",
            "Early stopping:  0.0353250541711163\n",
            "Epoch: 021, Loss: 0.9847, Train: 0.7134, Test: 0.6805\n",
            "Early stopping:  0.03453561015337304\n",
            "Epoch: 022, Loss: 0.9642, Train: 0.7188, Test: 0.6800\n",
            "Early stopping:  0.033766452486883224\n",
            "Epoch: 023, Loss: 0.9447, Train: 0.7214, Test: 0.6816\n",
            "Early stopping:  0.03277652852062915\n",
            "Epoch: 024, Loss: 0.9267, Train: 0.7276, Test: 0.6805\n",
            "Early stopping:  0.031402368950015365\n",
            "Epoch: 025, Loss: 0.9097, Train: 0.7307, Test: 0.6832\n",
            "Early stopping:  0.02968016006460772\n",
            "Epoch: 026, Loss: 0.8943, Train: 0.7368, Test: 0.6847\n",
            "Early stopping:  0.02766837743899011\n",
            "Epoch: 027, Loss: 0.8800, Train: 0.7411, Test: 0.6900\n",
            "Early stopping:  0.025601973397657574\n",
            "Epoch: 028, Loss: 0.8668, Train: 0.7453, Test: 0.6905\n",
            "Early stopping:  0.023662767038170607\n",
            "Epoch: 029, Loss: 0.8543, Train: 0.7508, Test: 0.6911\n",
            "Early stopping:  0.021891941161106376\n",
            "Epoch: 030, Loss: 0.8425, Train: 0.7555, Test: 0.6947\n",
            "Early stopping:  0.020488822878023012\n",
            "Epoch: 031, Loss: 0.8313, Train: 0.7593, Test: 0.6974\n",
            "Early stopping:  0.019269841601609584\n",
            "Epoch: 032, Loss: 0.8207, Train: 0.7642, Test: 0.7021\n",
            "Early stopping:  0.018213324653789997\n",
            "Epoch: 033, Loss: 0.8106, Train: 0.7687, Test: 0.7011\n",
            "Early stopping:  0.017254925811888006\n",
            "Epoch: 034, Loss: 0.8013, Train: 0.7742, Test: 0.7037\n",
            "Early stopping:  0.0162840354238815\n",
            "Epoch: 035, Loss: 0.7927, Train: 0.7774, Test: 0.7063\n",
            "Early stopping:  0.015283274711368372\n",
            "Epoch: 036, Loss: 0.7845, Train: 0.7784, Test: 0.7079\n",
            "Early stopping:  0.014299750159083517\n",
            "Epoch: 037, Loss: 0.7766, Train: 0.7812, Test: 0.7121\n",
            "Early stopping:  0.013422193997884387\n",
            "Epoch: 038, Loss: 0.7688, Train: 0.7855, Test: 0.7153\n",
            "Early stopping:  0.012842288300946208\n",
            "Epoch: 039, Loss: 0.7609, Train: 0.7875, Test: 0.7184\n",
            "Early stopping:  0.012540174040411458\n",
            "Epoch: 040, Loss: 0.7529, Train: 0.7911, Test: 0.7211\n",
            "Early stopping:  0.01246915696772053\n",
            "Epoch: 041, Loss: 0.7450, Train: 0.7938, Test: 0.7263\n",
            "Early stopping:  0.0125087416157174\n",
            "Epoch: 042, Loss: 0.7370, Train: 0.7972, Test: 0.7253\n",
            "Early stopping:  0.012557362989496241\n",
            "Epoch: 043, Loss: 0.7290, Train: 0.7992, Test: 0.7279\n",
            "Early stopping:  0.012609435898871068\n",
            "Epoch: 044, Loss: 0.7210, Train: 0.8009, Test: 0.7274\n",
            "Early stopping:  0.012623270065262347\n",
            "Epoch: 045, Loss: 0.7130, Train: 0.8029, Test: 0.7284\n",
            "Early stopping:  0.012644651661993018\n",
            "Epoch: 046, Loss: 0.7049, Train: 0.8057, Test: 0.7268\n",
            "Early stopping:  0.012672082004577135\n",
            "Epoch: 047, Loss: 0.6969, Train: 0.8061, Test: 0.7274\n",
            "Early stopping:  0.012704561046410018\n",
            "Epoch: 048, Loss: 0.6888, Train: 0.8087, Test: 0.7279\n",
            "Early stopping:  0.012738999949210772\n",
            "Epoch: 049, Loss: 0.6808, Train: 0.8093, Test: 0.7242\n",
            "Early stopping:  0.012726131701791741\n",
            "Epoch: 050, Loss: 0.6729, Train: 0.8116, Test: 0.7216\n",
            "Early stopping:  0.012677999581430392\n",
            "Epoch: 051, Loss: 0.6649, Train: 0.8125, Test: 0.7205\n",
            "Early stopping:  0.012614547275915895\n",
            "Epoch: 052, Loss: 0.6573, Train: 0.8150, Test: 0.7195\n",
            "Early stopping:  0.012485978768550334\n",
            "Epoch: 053, Loss: 0.6499, Train: 0.8164, Test: 0.7216\n",
            "Early stopping:  0.012250148067519045\n",
            "Epoch: 054, Loss: 0.6427, Train: 0.8191, Test: 0.7232\n",
            "Early stopping:  0.01192858154572874\n",
            "Epoch: 055, Loss: 0.6348, Train: 0.8238, Test: 0.7258\n",
            "Early stopping:  0.011853593646126703\n",
            "Epoch: 056, Loss: 0.6267, Train: 0.8234, Test: 0.7274\n",
            "Early stopping:  0.012069726960726277\n",
            "Epoch: 057, Loss: 0.6197, Train: 0.8274, Test: 0.7232\n",
            "Early stopping:  0.012087991241407412\n",
            "Epoch: 058, Loss: 0.6128, Train: 0.8299, Test: 0.7226\n",
            "Early stopping:  0.011845588908391231\n",
            "Epoch: 059, Loss: 0.6049, Train: 0.8339, Test: 0.7242\n",
            "Early stopping:  0.011641239558616274\n",
            "Epoch: 060, Loss: 0.5971, Train: 0.8349, Test: 0.7226\n",
            "Early stopping:  0.011678198210566234\n",
            "Epoch: 061, Loss: 0.5906, Train: 0.8376, Test: 0.7279\n",
            "Early stopping:  0.0116815137308487\n",
            "Epoch: 062, Loss: 0.5842, Train: 0.8414, Test: 0.7242\n",
            "Early stopping:  0.011330262477110435\n",
            "Epoch: 063, Loss: 0.5768, Train: 0.8414, Test: 0.7242\n",
            "Early stopping:  0.010958339002251597\n",
            "Epoch: 064, Loss: 0.5697, Train: 0.8455, Test: 0.7284\n",
            "Early stopping:  0.010877873050434736\n",
            "Epoch: 065, Loss: 0.5631, Train: 0.8470, Test: 0.7258\n",
            "Early stopping:  0.010976641565610219\n",
            "Epoch: 066, Loss: 0.5571, Train: 0.8484, Test: 0.7289\n",
            "Early stopping:  0.010738011761412907\n",
            "Epoch: 067, Loss: 0.5508, Train: 0.8518, Test: 0.7221\n",
            "Early stopping:  0.010222878966566144\n",
            "Epoch: 068, Loss: 0.5435, Train: 0.8533, Test: 0.7247\n",
            "Early stopping:  0.01021699926934684\n",
            "Epoch: 069, Loss: 0.5370, Train: 0.8546, Test: 0.7232\n",
            "Early stopping:  0.010386546661363082\n",
            "Epoch: 070, Loss: 0.5311, Train: 0.8582, Test: 0.7237\n",
            "Early stopping:  0.010392165466425105\n",
            "Epoch: 071, Loss: 0.5238, Train: 0.8589, Test: 0.7226\n",
            "Early stopping:  0.010507558397059675\n",
            "Epoch: 072, Loss: 0.5159, Train: 0.8629, Test: 0.7232\n",
            "Early stopping:  0.010870515153598683\n",
            "Epoch: 073, Loss: 0.5095, Train: 0.8621, Test: 0.7237\n",
            "Early stopping:  0.011123233589638865\n",
            "Epoch: 074, Loss: 0.5038, Train: 0.8658, Test: 0.7205\n",
            "Early stopping:  0.010908456770365883\n",
            "Epoch: 075, Loss: 0.4968, Train: 0.8688, Test: 0.7232\n",
            "Early stopping:  0.01046106432249451\n",
            "Epoch: 076, Loss: 0.4898, Train: 0.8687, Test: 0.7200\n",
            "Early stopping:  0.010250813318318017\n",
            "Epoch: 077, Loss: 0.4838, Train: 0.8741, Test: 0.7211\n",
            "Early stopping:  0.010352729942781047\n",
            "Epoch: 078, Loss: 0.4783, Train: 0.8718, Test: 0.7179\n",
            "Early stopping:  0.010132834472605153\n",
            "Epoch: 079, Loss: 0.4729, Train: 0.8761, Test: 0.7189\n",
            "Early stopping:  0.009381889600343977\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.86      0.80      0.83        96\n",
            "            CAD_and_CAM       0.78      0.83      0.80        92\n",
            "              Companies       0.50      0.61      0.55        85\n",
            "       Computer_Science       0.74      0.76      0.75        88\n",
            "            Consultants       0.63      0.65      0.64        96\n",
            "           Data_Formats       0.78      0.83      0.81       103\n",
            "    Data_Communications       0.75      0.68      0.71       110\n",
            "              Education       0.81      0.80      0.81        97\n",
            "               Graphics       0.84      0.93      0.88       109\n",
            "               Hardware       0.69      0.69      0.69        93\n",
            "               Internet       0.65      0.61      0.63       110\n",
            "       Mobile_Computing       0.75      0.76      0.75        87\n",
            "             Multimedia       0.68      0.81      0.73        98\n",
            "            Open_Source       0.76      0.68      0.72       114\n",
            "            Programming       0.57      0.50      0.53        98\n",
            "               Robotics       0.92      0.93      0.93       105\n",
            "               Security       0.79      0.88      0.83       107\n",
            "               Software       0.38      0.31      0.34       109\n",
            "                Systems       0.69      0.61      0.65       103\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.71      0.72      0.72      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9559, Train: 0.3533, Test: 0.3579\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8366, Train: 0.5284, Test: 0.5211\n",
            "Early stopping:  0.0843715843978362\n",
            "Epoch: 003, Loss: 2.6814, Train: 0.5387, Test: 0.5332\n",
            "Early stopping:  0.1376297558091337\n",
            "Epoch: 004, Loss: 2.4977, Train: 0.5718, Test: 0.5632\n",
            "Early stopping:  0.19835482407790547\n",
            "Epoch: 005, Loss: 2.3008, Train: 0.5891, Test: 0.5774\n",
            "Early stopping:  0.26193024496645095\n",
            "Epoch: 006, Loss: 2.0982, Train: 0.6075, Test: 0.5968\n",
            "Early stopping:  0.2940683016459043\n",
            "Epoch: 007, Loss: 1.9025, Train: 0.6239, Test: 0.6200\n",
            "Early stopping:  0.3095352219163954\n",
            "Epoch: 008, Loss: 1.7232, Train: 0.6395, Test: 0.6311\n",
            "Early stopping:  0.30798411462496145\n",
            "Epoch: 009, Loss: 1.5643, Train: 0.6513, Test: 0.6395\n",
            "Early stopping:  0.2925138389192938\n",
            "Epoch: 010, Loss: 1.4336, Train: 0.6525, Test: 0.6389\n",
            "Early stopping:  0.26439084321971945\n",
            "Epoch: 011, Loss: 1.3330, Train: 0.6559, Test: 0.6421\n",
            "Early stopping:  0.22723172233419076\n",
            "Epoch: 012, Loss: 1.2551, Train: 0.6618, Test: 0.6463\n",
            "Early stopping:  0.18637108171946018\n",
            "Epoch: 013, Loss: 1.1977, Train: 0.6675, Test: 0.6532\n",
            "Early stopping:  0.14594302181618948\n",
            "Epoch: 014, Loss: 1.1540, Train: 0.6741, Test: 0.6526\n",
            "Early stopping:  0.11128064071545656\n",
            "Epoch: 015, Loss: 1.1207, Train: 0.6789, Test: 0.6600\n",
            "Early stopping:  0.08425674148333191\n",
            "Epoch: 016, Loss: 1.0938, Train: 0.6847, Test: 0.6637\n",
            "Early stopping:  0.06390270797967622\n",
            "Epoch: 017, Loss: 1.0689, Train: 0.6929, Test: 0.6679\n",
            "Early stopping:  0.05060175392306804\n",
            "Epoch: 018, Loss: 1.0460, Train: 0.6999, Test: 0.6684\n",
            "Early stopping:  0.04247405393945998\n",
            "Epoch: 019, Loss: 1.0242, Train: 0.7055, Test: 0.6679\n",
            "Early stopping:  0.038132152230265615\n",
            "Epoch: 020, Loss: 1.0022, Train: 0.7130, Test: 0.6742\n",
            "Early stopping:  0.03605967104630082\n",
            "Epoch: 021, Loss: 0.9808, Train: 0.7196, Test: 0.6779\n",
            "Early stopping:  0.03479494868918641\n",
            "Epoch: 022, Loss: 0.9607, Train: 0.7239, Test: 0.6832\n",
            "Early stopping:  0.03382772037762324\n",
            "Epoch: 023, Loss: 0.9419, Train: 0.7276, Test: 0.6837\n",
            "Early stopping:  0.03260074845439215\n",
            "Epoch: 024, Loss: 0.9238, Train: 0.7349, Test: 0.6853\n",
            "Early stopping:  0.030975303778285685\n",
            "Epoch: 025, Loss: 0.9072, Train: 0.7380, Test: 0.6874\n",
            "Early stopping:  0.029114519598581787\n",
            "Epoch: 026, Loss: 0.8921, Train: 0.7416, Test: 0.6900\n",
            "Early stopping:  0.02720532904345904\n",
            "Epoch: 027, Loss: 0.8775, Train: 0.7457, Test: 0.6926\n",
            "Early stopping:  0.025393805152014906\n",
            "Epoch: 028, Loss: 0.8636, Train: 0.7493, Test: 0.6942\n",
            "Early stopping:  0.023757343561947777\n",
            "Epoch: 029, Loss: 0.8508, Train: 0.7539, Test: 0.6953\n",
            "Early stopping:  0.022380394993317183\n",
            "Epoch: 030, Loss: 0.8388, Train: 0.7605, Test: 0.6953\n",
            "Early stopping:  0.021096419254286364\n",
            "Epoch: 031, Loss: 0.8273, Train: 0.7620, Test: 0.6963\n",
            "Early stopping:  0.019822885549266707\n",
            "Epoch: 032, Loss: 0.8166, Train: 0.7657, Test: 0.6979\n",
            "Early stopping:  0.018572332554967357\n",
            "Epoch: 033, Loss: 0.8066, Train: 0.7697, Test: 0.7016\n",
            "Early stopping:  0.017467889312391804\n",
            "Epoch: 034, Loss: 0.7971, Train: 0.7730, Test: 0.7068\n",
            "Early stopping:  0.016449875028389356\n",
            "Epoch: 035, Loss: 0.7881, Train: 0.7757, Test: 0.7079\n",
            "Early stopping:  0.015485004851095621\n",
            "Epoch: 036, Loss: 0.7796, Train: 0.7784, Test: 0.7111\n",
            "Early stopping:  0.014656049492363491\n",
            "Epoch: 037, Loss: 0.7715, Train: 0.7816, Test: 0.7116\n",
            "Early stopping:  0.013903640981679725\n",
            "Epoch: 038, Loss: 0.7637, Train: 0.7867, Test: 0.7105\n",
            "Early stopping:  0.013205860510193504\n",
            "Epoch: 039, Loss: 0.7560, Train: 0.7895, Test: 0.7084\n",
            "Early stopping:  0.012648180512063088\n",
            "Epoch: 040, Loss: 0.7484, Train: 0.7908, Test: 0.7126\n",
            "Early stopping:  0.012305316589724943\n",
            "Epoch: 041, Loss: 0.7405, Train: 0.7924, Test: 0.7137\n",
            "Early stopping:  0.012205926066380603\n",
            "Epoch: 042, Loss: 0.7326, Train: 0.7968, Test: 0.7126\n",
            "Early stopping:  0.012285997716707282\n",
            "Epoch: 043, Loss: 0.7246, Train: 0.7989, Test: 0.7137\n",
            "Early stopping:  0.012452769338202264\n",
            "Epoch: 044, Loss: 0.7165, Train: 0.8012, Test: 0.7137\n",
            "Early stopping:  0.012605251292556578\n",
            "Epoch: 045, Loss: 0.7085, Train: 0.8036, Test: 0.7132\n",
            "Early stopping:  0.012664601822771458\n",
            "Epoch: 046, Loss: 0.7006, Train: 0.8058, Test: 0.7147\n",
            "Early stopping:  0.012648678250407636\n",
            "Epoch: 047, Loss: 0.6927, Train: 0.8067, Test: 0.7126\n",
            "Early stopping:  0.012601524495315765\n",
            "Epoch: 048, Loss: 0.6847, Train: 0.8105, Test: 0.7111\n",
            "Early stopping:  0.01257031575385351\n",
            "Epoch: 049, Loss: 0.6766, Train: 0.8117, Test: 0.7126\n",
            "Early stopping:  0.012609621809589997\n",
            "Epoch: 050, Loss: 0.6685, Train: 0.8129, Test: 0.7116\n",
            "Early stopping:  0.012687638864489183\n",
            "Epoch: 051, Loss: 0.6604, Train: 0.8162, Test: 0.7105\n",
            "Early stopping:  0.01274181394514593\n",
            "Epoch: 052, Loss: 0.6524, Train: 0.8193, Test: 0.7116\n",
            "Early stopping:  0.012752377518641738\n",
            "Epoch: 053, Loss: 0.6444, Train: 0.8213, Test: 0.7153\n",
            "Early stopping:  0.012739408661448317\n",
            "Epoch: 054, Loss: 0.6363, Train: 0.8250, Test: 0.7168\n",
            "Early stopping:  0.012720481171470219\n",
            "Epoch: 055, Loss: 0.6283, Train: 0.8270, Test: 0.7153\n",
            "Early stopping:  0.012715919645192447\n",
            "Epoch: 056, Loss: 0.6203, Train: 0.8293, Test: 0.7147\n",
            "Early stopping:  0.012687180809936449\n",
            "Epoch: 057, Loss: 0.6125, Train: 0.8328, Test: 0.7158\n",
            "Early stopping:  0.012612299477906286\n",
            "Epoch: 058, Loss: 0.6046, Train: 0.8346, Test: 0.7200\n",
            "Early stopping:  0.012526841800594877\n",
            "Epoch: 059, Loss: 0.5968, Train: 0.8392, Test: 0.7158\n",
            "Early stopping:  0.01243747895502642\n",
            "Epoch: 060, Loss: 0.5893, Train: 0.8393, Test: 0.7147\n",
            "Early stopping:  0.01227743938493364\n",
            "Epoch: 061, Loss: 0.5824, Train: 0.8442, Test: 0.7153\n",
            "Early stopping:  0.011924743115224448\n",
            "Epoch: 062, Loss: 0.5765, Train: 0.8420, Test: 0.7116\n",
            "Early stopping:  0.011190508140719972\n",
            "Epoch: 063, Loss: 0.5700, Train: 0.8499, Test: 0.7142\n",
            "Early stopping:  0.01053473179709125\n",
            "Epoch: 064, Loss: 0.5613, Train: 0.8488, Test: 0.7158\n",
            "Early stopping:  0.010851622699109876\n",
            "Epoch: 065, Loss: 0.5527, Train: 0.8520, Test: 0.7147\n",
            "Early stopping:  0.011824078742901162\n",
            "Epoch: 066, Loss: 0.5470, Train: 0.8546, Test: 0.7142\n",
            "Early stopping:  0.012078544885802655\n",
            "Epoch: 067, Loss: 0.5410, Train: 0.8578, Test: 0.7132\n",
            "Early stopping:  0.011471411745145751\n",
            "Epoch: 068, Loss: 0.5337, Train: 0.8583, Test: 0.7100\n",
            "Early stopping:  0.010599546228192869\n",
            "Epoch: 069, Loss: 0.5275, Train: 0.8626, Test: 0.7095\n",
            "Early stopping:  0.010090544971973701\n",
            "Epoch: 070, Loss: 0.5200, Train: 0.8641, Test: 0.7142\n",
            "Early stopping:  0.010675372327597884\n",
            "Epoch: 071, Loss: 0.5119, Train: 0.8672, Test: 0.7074\n",
            "Early stopping:  0.011394409258811068\n",
            "Epoch: 072, Loss: 0.5071, Train: 0.8699, Test: 0.7132\n",
            "Early stopping:  0.010915250761460283\n",
            "Epoch: 073, Loss: 0.4997, Train: 0.8717, Test: 0.7132\n",
            "Early stopping:  0.010865123085151238\n",
            "Epoch: 074, Loss: 0.4918, Train: 0.8736, Test: 0.7095\n",
            "Early stopping:  0.010884810378358612\n",
            "Epoch: 075, Loss: 0.4880, Train: 0.8749, Test: 0.7142\n",
            "Early stopping:  0.010019456529430575\n",
            "Epoch: 076, Loss: 0.4819, Train: 0.8772, Test: 0.7100\n",
            "Early stopping:  0.009886717206648016\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.77      0.80      0.78       109\n",
            "            CAD_and_CAM       0.74      0.83      0.78        94\n",
            "              Companies       0.56      0.53      0.54        95\n",
            "       Computer_Science       0.84      0.79      0.81       119\n",
            "            Consultants       0.58      0.52      0.55        94\n",
            "           Data_Formats       0.76      0.76      0.76       112\n",
            "    Data_Communications       0.74      0.73      0.74       111\n",
            "              Education       0.90      0.90      0.90       106\n",
            "               Graphics       0.80      0.90      0.85       104\n",
            "               Hardware       0.64      0.73      0.69        94\n",
            "               Internet       0.58      0.65      0.61        82\n",
            "       Mobile_Computing       0.69      0.72      0.71        85\n",
            "             Multimedia       0.67      0.70      0.68        83\n",
            "            Open_Source       0.73      0.74      0.74       101\n",
            "            Programming       0.60      0.57      0.58        97\n",
            "               Robotics       0.95      0.94      0.94       108\n",
            "               Security       0.78      0.79      0.78       103\n",
            "               Software       0.34      0.28      0.31        94\n",
            "                Systems       0.59      0.52      0.55       109\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.70      0.70      0.70      1900\n",
            "           weighted avg       0.71      0.71      0.71      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9526, Train: 0.3528, Test: 0.3526\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8248, Train: 0.4964, Test: 0.4826\n",
            "Early stopping:  0.09042134303308992\n",
            "Epoch: 003, Loss: 2.6527, Train: 0.5453, Test: 0.5342\n",
            "Early stopping:  0.1504940453483352\n",
            "Epoch: 004, Loss: 2.4506, Train: 0.5637, Test: 0.5526\n",
            "Early stopping:  0.21770247345808807\n",
            "Epoch: 005, Loss: 2.2401, Train: 0.5924, Test: 0.5821\n",
            "Early stopping:  0.28573434188729124\n",
            "Epoch: 006, Loss: 2.0301, Train: 0.6166, Test: 0.6068\n",
            "Early stopping:  0.3167750275407988\n",
            "Epoch: 007, Loss: 1.8304, Train: 0.6330, Test: 0.6153\n",
            "Early stopping:  0.3265423956463792\n",
            "Epoch: 008, Loss: 1.6521, Train: 0.6388, Test: 0.6268\n",
            "Early stopping:  0.3174635917596645\n",
            "Epoch: 009, Loss: 1.5018, Train: 0.6450, Test: 0.6337\n",
            "Early stopping:  0.2938620436638376\n",
            "Epoch: 010, Loss: 1.3816, Train: 0.6517, Test: 0.6332\n",
            "Early stopping:  0.2582606117401852\n",
            "Epoch: 011, Loss: 1.2904, Train: 0.6587, Test: 0.6411\n",
            "Early stopping:  0.21528939344960304\n",
            "Epoch: 012, Loss: 1.2238, Train: 0.6632, Test: 0.6447\n",
            "Early stopping:  0.17090297332617893\n",
            "Epoch: 013, Loss: 1.1757, Train: 0.6689, Test: 0.6447\n",
            "Early stopping:  0.13003089882951008\n",
            "Epoch: 014, Loss: 1.1414, Train: 0.6751, Test: 0.6547\n",
            "Early stopping:  0.09574919041604758\n",
            "Epoch: 015, Loss: 1.1147, Train: 0.6784, Test: 0.6579\n",
            "Early stopping:  0.06973360911449963\n",
            "Epoch: 016, Loss: 1.0926, Train: 0.6833, Test: 0.6616\n",
            "Early stopping:  0.051780984664062925\n",
            "Epoch: 017, Loss: 1.0710, Train: 0.6878, Test: 0.6711\n",
            "Early stopping:  0.04104460262584819\n",
            "Epoch: 018, Loss: 1.0502, Train: 0.6947, Test: 0.6774\n",
            "Early stopping:  0.03576259908503665\n",
            "Epoch: 019, Loss: 1.0284, Train: 0.6999, Test: 0.6816\n",
            "Early stopping:  0.03398408345786859\n",
            "Epoch: 020, Loss: 1.0061, Train: 0.7043, Test: 0.6837\n",
            "Early stopping:  0.0340984373123067\n",
            "Epoch: 021, Loss: 0.9838, Train: 0.7114, Test: 0.6879\n",
            "Early stopping:  0.0345828589040015\n",
            "Epoch: 022, Loss: 0.9621, Train: 0.7164, Test: 0.6921\n",
            "Early stopping:  0.034935493181796305\n",
            "Epoch: 023, Loss: 0.9417, Train: 0.7224, Test: 0.6937\n",
            "Early stopping:  0.03436789627975015\n",
            "Epoch: 024, Loss: 0.9232, Train: 0.7286, Test: 0.6911\n",
            "Early stopping:  0.0328626801069908\n",
            "Epoch: 025, Loss: 0.9066, Train: 0.7314, Test: 0.6942\n",
            "Early stopping:  0.030605397094648004\n",
            "Epoch: 026, Loss: 0.8913, Train: 0.7361, Test: 0.6984\n",
            "Early stopping:  0.02799204744150073\n",
            "Epoch: 027, Loss: 0.8775, Train: 0.7407, Test: 0.7011\n",
            "Early stopping:  0.025406558889136\n",
            "Epoch: 028, Loss: 0.8645, Train: 0.7441, Test: 0.7068\n",
            "Early stopping:  0.023202261526150455\n",
            "Epoch: 029, Loss: 0.8524, Train: 0.7491, Test: 0.7147\n",
            "Early stopping:  0.021391058033571785\n",
            "Epoch: 030, Loss: 0.8409, Train: 0.7526, Test: 0.7142\n",
            "Early stopping:  0.019926315308468962\n",
            "Epoch: 031, Loss: 0.8301, Train: 0.7584, Test: 0.7158\n",
            "Early stopping:  0.018729053038584017\n",
            "Epoch: 032, Loss: 0.8197, Train: 0.7599, Test: 0.7179\n",
            "Early stopping:  0.017694525502959035\n",
            "Epoch: 033, Loss: 0.8099, Train: 0.7634, Test: 0.7200\n",
            "Early stopping:  0.016790689000884842\n",
            "Epoch: 034, Loss: 0.8005, Train: 0.7676, Test: 0.7216\n",
            "Early stopping:  0.015957794371135252\n",
            "Epoch: 035, Loss: 0.7915, Train: 0.7700, Test: 0.7247\n",
            "Early stopping:  0.01524984356438826\n",
            "Epoch: 036, Loss: 0.7827, Train: 0.7747, Test: 0.7253\n",
            "Early stopping:  0.014633553499658199\n",
            "Epoch: 037, Loss: 0.7739, Train: 0.7788, Test: 0.7253\n",
            "Early stopping:  0.01421291217272343\n",
            "Epoch: 038, Loss: 0.7653, Train: 0.7825, Test: 0.7242\n",
            "Early stopping:  0.013927338271459285\n",
            "Epoch: 039, Loss: 0.7568, Train: 0.7850, Test: 0.7263\n",
            "Early stopping:  0.013716554206062402\n",
            "Epoch: 040, Loss: 0.7484, Train: 0.7870, Test: 0.7279\n",
            "Early stopping:  0.013534655420886037\n",
            "Epoch: 041, Loss: 0.7401, Train: 0.7897, Test: 0.7289\n",
            "Early stopping:  0.01334604427718446\n",
            "Epoch: 042, Loss: 0.7319, Train: 0.7934, Test: 0.7295\n",
            "Early stopping:  0.013176703146766614\n",
            "Epoch: 043, Loss: 0.7238, Train: 0.7966, Test: 0.7316\n",
            "Early stopping:  0.013049407771519764\n",
            "Epoch: 044, Loss: 0.7155, Train: 0.7967, Test: 0.7347\n",
            "Early stopping:  0.012983857711357022\n",
            "Epoch: 045, Loss: 0.7074, Train: 0.7999, Test: 0.7368\n",
            "Early stopping:  0.012943928135182537\n",
            "Epoch: 046, Loss: 0.6993, Train: 0.8017, Test: 0.7358\n",
            "Early stopping:  0.012921169301663418\n",
            "Epoch: 047, Loss: 0.6911, Train: 0.8039, Test: 0.7347\n",
            "Early stopping:  0.01288887799555754\n",
            "Epoch: 048, Loss: 0.6830, Train: 0.8054, Test: 0.7337\n",
            "Early stopping:  0.012855977014989584\n",
            "Epoch: 049, Loss: 0.6749, Train: 0.8066, Test: 0.7342\n",
            "Early stopping:  0.012843138437636051\n",
            "Epoch: 050, Loss: 0.6668, Train: 0.8082, Test: 0.7347\n",
            "Early stopping:  0.012826288310643436\n",
            "Epoch: 051, Loss: 0.6589, Train: 0.8111, Test: 0.7316\n",
            "Early stopping:  0.012768130494637393\n",
            "Epoch: 052, Loss: 0.6513, Train: 0.8145, Test: 0.7374\n",
            "Early stopping:  0.012582788185821682\n",
            "Epoch: 053, Loss: 0.6436, Train: 0.8153, Test: 0.7342\n",
            "Early stopping:  0.012347192415690345\n",
            "Epoch: 054, Loss: 0.6359, Train: 0.8200, Test: 0.7395\n",
            "Early stopping:  0.012189982518961187\n",
            "Epoch: 055, Loss: 0.6274, Train: 0.8213, Test: 0.7395\n",
            "Early stopping:  0.012399168212733756\n",
            "Epoch: 056, Loss: 0.6193, Train: 0.8239, Test: 0.7363\n",
            "Early stopping:  0.012689222352887003\n",
            "Epoch: 057, Loss: 0.6119, Train: 0.8253, Test: 0.7411\n",
            "Early stopping:  0.012662425490188972\n",
            "Epoch: 058, Loss: 0.6047, Train: 0.8270, Test: 0.7374\n",
            "Early stopping:  0.012303710299257725\n",
            "Epoch: 059, Loss: 0.5973, Train: 0.8301, Test: 0.7426\n",
            "Early stopping:  0.01180345038390427\n",
            "Epoch: 060, Loss: 0.5893, Train: 0.8336, Test: 0.7411\n",
            "Early stopping:  0.011774165648958054\n",
            "Epoch: 061, Loss: 0.5816, Train: 0.8366, Test: 0.7395\n",
            "Early stopping:  0.012028089959943764\n",
            "Epoch: 062, Loss: 0.5742, Train: 0.8399, Test: 0.7405\n",
            "Early stopping:  0.012129826596681482\n",
            "Epoch: 063, Loss: 0.5673, Train: 0.8418, Test: 0.7395\n",
            "Early stopping:  0.011888884237378616\n",
            "Epoch: 064, Loss: 0.5605, Train: 0.8470, Test: 0.7405\n",
            "Early stopping:  0.011381323203331285\n",
            "Epoch: 065, Loss: 0.5538, Train: 0.8463, Test: 0.7432\n",
            "Early stopping:  0.01096470618841327\n",
            "Epoch: 066, Loss: 0.5474, Train: 0.8525, Test: 0.7389\n",
            "Early stopping:  0.010636730094729123\n",
            "Epoch: 067, Loss: 0.5408, Train: 0.8521, Test: 0.7437\n",
            "Early stopping:  0.010458122707855108\n",
            "Epoch: 068, Loss: 0.5337, Train: 0.8564, Test: 0.7389\n",
            "Early stopping:  0.010514875164944428\n",
            "Epoch: 069, Loss: 0.5258, Train: 0.8629, Test: 0.7416\n",
            "Early stopping:  0.011007531198250296\n",
            "Epoch: 070, Loss: 0.5187, Train: 0.8607, Test: 0.7395\n",
            "Early stopping:  0.011432718231150371\n",
            "Epoch: 071, Loss: 0.5128, Train: 0.8675, Test: 0.7321\n",
            "Early stopping:  0.01124798007129107\n",
            "Epoch: 072, Loss: 0.5059, Train: 0.8686, Test: 0.7400\n",
            "Early stopping:  0.010868895990012088\n",
            "Epoch: 073, Loss: 0.4982, Train: 0.8701, Test: 0.7379\n",
            "Early stopping:  0.010774524917188439\n",
            "Epoch: 074, Loss: 0.4913, Train: 0.8742, Test: 0.7379\n",
            "Early stopping:  0.010982380311275942\n",
            "Epoch: 075, Loss: 0.4860, Train: 0.8729, Test: 0.7384\n",
            "Early stopping:  0.010790216225197527\n",
            "Epoch: 076, Loss: 0.4801, Train: 0.8768, Test: 0.7363\n",
            "Early stopping:  0.010125446258894508\n",
            "Epoch: 077, Loss: 0.4729, Train: 0.8793, Test: 0.7368\n",
            "Early stopping:  0.009777109291950655\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.83      0.83      0.83        93\n",
            "            CAD_and_CAM       0.82      0.83      0.83       101\n",
            "              Companies       0.54      0.63      0.58        82\n",
            "       Computer_Science       0.79      0.77      0.78        93\n",
            "            Consultants       0.70      0.68      0.69        98\n",
            "           Data_Formats       0.81      0.88      0.84       110\n",
            "    Data_Communications       0.68      0.80      0.73        98\n",
            "              Education       0.88      0.91      0.90        94\n",
            "               Graphics       0.89      0.94      0.91       111\n",
            "               Hardware       0.69      0.72      0.71       101\n",
            "               Internet       0.69      0.65      0.67       103\n",
            "       Mobile_Computing       0.80      0.68      0.74        94\n",
            "             Multimedia       0.67      0.73      0.70        89\n",
            "            Open_Source       0.69      0.74      0.71        92\n",
            "            Programming       0.58      0.55      0.57       109\n",
            "               Robotics       0.93      0.88      0.90       116\n",
            "               Security       0.88      0.79      0.83        95\n",
            "               Software       0.40      0.40      0.40       100\n",
            "                Systems       0.72      0.57      0.64       121\n",
            "\n",
            "               accuracy                           0.74      1900\n",
            "              macro avg       0.74      0.74      0.73      1900\n",
            "           weighted avg       0.74      0.74      0.74      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9507, Train: 0.5125, Test: 0.4979\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8042, Train: 0.5209, Test: 0.5158\n",
            "Early stopping:  0.10362477055979472\n",
            "Epoch: 003, Loss: 2.6301, Train: 0.5366, Test: 0.5342\n",
            "Early stopping:  0.16050560052420584\n",
            "Epoch: 004, Loss: 2.4255, Train: 0.5630, Test: 0.5537\n",
            "Early stopping:  0.22650214937044416\n",
            "Epoch: 005, Loss: 2.2099, Train: 0.5967, Test: 0.5695\n",
            "Early stopping:  0.2950112791417871\n",
            "Epoch: 006, Loss: 1.9993, Train: 0.6236, Test: 0.6079\n",
            "Early stopping:  0.3212221017964812\n",
            "Epoch: 007, Loss: 1.8024, Train: 0.6403, Test: 0.6253\n",
            "Early stopping:  0.3291880781889459\n",
            "Epoch: 008, Loss: 1.6256, Train: 0.6474, Test: 0.6289\n",
            "Early stopping:  0.3176569824371903\n",
            "Epoch: 009, Loss: 1.4778, Train: 0.6538, Test: 0.6363\n",
            "Early stopping:  0.2912846101436007\n",
            "Epoch: 010, Loss: 1.3617, Train: 0.6592, Test: 0.6447\n",
            "Early stopping:  0.25424351820862395\n",
            "Epoch: 011, Loss: 1.2726, Train: 0.6670, Test: 0.6463\n",
            "Early stopping:  0.211074014222303\n",
            "Epoch: 012, Loss: 1.2073, Train: 0.6692, Test: 0.6526\n",
            "Early stopping:  0.16669508361690477\n",
            "Epoch: 013, Loss: 1.1609, Train: 0.6749, Test: 0.6568\n",
            "Early stopping:  0.12650717199289316\n",
            "Epoch: 014, Loss: 1.1260, Train: 0.6801, Test: 0.6611\n",
            "Early stopping:  0.09376533221386898\n",
            "Epoch: 015, Loss: 1.0991, Train: 0.6846, Test: 0.6632\n",
            "Early stopping:  0.06876112304884693\n",
            "Epoch: 016, Loss: 1.0766, Train: 0.6899, Test: 0.6653\n",
            "Early stopping:  0.051639820711987854\n",
            "Epoch: 017, Loss: 1.0547, Train: 0.6991, Test: 0.6721\n",
            "Early stopping:  0.04159879958279852\n",
            "Epoch: 018, Loss: 1.0339, Train: 0.7049, Test: 0.6784\n",
            "Early stopping:  0.036184589116150044\n",
            "Epoch: 019, Loss: 1.0123, Train: 0.7095, Test: 0.6821\n",
            "Early stopping:  0.0342175793594026\n",
            "Epoch: 020, Loss: 0.9908, Train: 0.7112, Test: 0.6847\n",
            "Early stopping:  0.03384949069208448\n",
            "Epoch: 021, Loss: 0.9700, Train: 0.7193, Test: 0.6868\n",
            "Early stopping:  0.03362449469652095\n",
            "Epoch: 022, Loss: 0.9500, Train: 0.7258, Test: 0.6895\n",
            "Early stopping:  0.0332373553195799\n",
            "Epoch: 023, Loss: 0.9314, Train: 0.7295, Test: 0.6905\n",
            "Early stopping:  0.03203431200208414\n",
            "Epoch: 024, Loss: 0.9140, Train: 0.7333, Test: 0.6947\n",
            "Early stopping:  0.030380769941945517\n",
            "Epoch: 025, Loss: 0.8982, Train: 0.7371, Test: 0.6947\n",
            "Early stopping:  0.02841701187685132\n",
            "Epoch: 026, Loss: 0.8831, Train: 0.7421, Test: 0.6937\n",
            "Early stopping:  0.026428886421516442\n",
            "Epoch: 027, Loss: 0.8693, Train: 0.7454, Test: 0.6958\n",
            "Early stopping:  0.02455083699753835\n",
            "Epoch: 028, Loss: 0.8560, Train: 0.7517, Test: 0.6974\n",
            "Early stopping:  0.022920355832103584\n",
            "Epoch: 029, Loss: 0.8435, Train: 0.7557, Test: 0.7016\n",
            "Early stopping:  0.021577637918944657\n",
            "Epoch: 030, Loss: 0.8320, Train: 0.7595, Test: 0.7021\n",
            "Early stopping:  0.020267230604387496\n",
            "Epoch: 031, Loss: 0.8210, Train: 0.7637, Test: 0.7032\n",
            "Early stopping:  0.019110745694362004\n",
            "Epoch: 032, Loss: 0.8108, Train: 0.7695, Test: 0.7026\n",
            "Early stopping:  0.017896224103631165\n",
            "Epoch: 033, Loss: 0.8011, Train: 0.7728, Test: 0.7074\n",
            "Early stopping:  0.016768801842211477\n",
            "Epoch: 034, Loss: 0.7922, Train: 0.7763, Test: 0.7079\n",
            "Early stopping:  0.015720843417809126\n",
            "Epoch: 035, Loss: 0.7836, Train: 0.7812, Test: 0.7084\n",
            "Early stopping:  0.014743840351819682\n",
            "Epoch: 036, Loss: 0.7754, Train: 0.7846, Test: 0.7111\n",
            "Early stopping:  0.013963231254866622\n",
            "Epoch: 037, Loss: 0.7673, Train: 0.7879, Test: 0.7100\n",
            "Early stopping:  0.013374894979122878\n",
            "Epoch: 038, Loss: 0.7594, Train: 0.7916, Test: 0.7116\n",
            "Early stopping:  0.01298123531487609\n",
            "Epoch: 039, Loss: 0.7515, Train: 0.7932, Test: 0.7132\n",
            "Early stopping:  0.012700667004846646\n",
            "Epoch: 040, Loss: 0.7436, Train: 0.7953, Test: 0.7079\n",
            "Early stopping:  0.012556597805327531\n",
            "Epoch: 041, Loss: 0.7357, Train: 0.7983, Test: 0.7079\n",
            "Early stopping:  0.012464321363198588\n",
            "Epoch: 042, Loss: 0.7279, Train: 0.7999, Test: 0.7100\n",
            "Early stopping:  0.012425005084003501\n",
            "Epoch: 043, Loss: 0.7201, Train: 0.8008, Test: 0.7079\n",
            "Early stopping:  0.012384950941493129\n",
            "Epoch: 044, Loss: 0.7122, Train: 0.8025, Test: 0.7063\n",
            "Early stopping:  0.012385499023406119\n",
            "Epoch: 045, Loss: 0.7042, Train: 0.8037, Test: 0.7084\n",
            "Early stopping:  0.01246649248868108\n",
            "Epoch: 046, Loss: 0.6961, Train: 0.8057, Test: 0.7079\n",
            "Early stopping:  0.01259794773133784\n",
            "Epoch: 047, Loss: 0.6879, Train: 0.8083, Test: 0.7042\n",
            "Early stopping:  0.01273319149125172\n",
            "Epoch: 048, Loss: 0.6797, Train: 0.8096, Test: 0.7063\n",
            "Early stopping:  0.012837557715948772\n",
            "Epoch: 049, Loss: 0.6716, Train: 0.8113, Test: 0.7047\n",
            "Early stopping:  0.012876775104141727\n",
            "Epoch: 050, Loss: 0.6635, Train: 0.8125, Test: 0.7058\n",
            "Early stopping:  0.012874524364585601\n",
            "Epoch: 051, Loss: 0.6555, Train: 0.8157, Test: 0.7063\n",
            "Early stopping:  0.012833281802768673\n",
            "Epoch: 052, Loss: 0.6475, Train: 0.8179, Test: 0.7079\n",
            "Early stopping:  0.012757202432414362\n",
            "Epoch: 053, Loss: 0.6396, Train: 0.8200, Test: 0.7068\n",
            "Early stopping:  0.012667422726687886\n",
            "Epoch: 054, Loss: 0.6318, Train: 0.8225, Test: 0.7089\n",
            "Early stopping:  0.012526291541664679\n",
            "Epoch: 055, Loss: 0.6242, Train: 0.8255, Test: 0.7089\n",
            "Early stopping:  0.012371815873990737\n",
            "Epoch: 056, Loss: 0.6167, Train: 0.8264, Test: 0.7074\n",
            "Early stopping:  0.012176028605185211\n",
            "Epoch: 057, Loss: 0.6094, Train: 0.8287, Test: 0.7089\n",
            "Early stopping:  0.011931183724449346\n",
            "Epoch: 058, Loss: 0.6030, Train: 0.8292, Test: 0.7063\n",
            "Early stopping:  0.011448495338572843\n",
            "Epoch: 059, Loss: 0.5976, Train: 0.8317, Test: 0.7089\n",
            "Early stopping:  0.010574647988395363\n",
            "Epoch: 060, Loss: 0.5928, Train: 0.8346, Test: 0.7084\n",
            "Early stopping:  0.009467797313381389\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.83      0.81       106\n",
            "            CAD_and_CAM       0.73      0.80      0.76        94\n",
            "              Companies       0.52      0.56      0.54       112\n",
            "       Computer_Science       0.77      0.75      0.76       106\n",
            "            Consultants       0.57      0.58      0.58       103\n",
            "           Data_Formats       0.80      0.87      0.84        87\n",
            "    Data_Communications       0.68      0.70      0.69        91\n",
            "              Education       0.91      0.88      0.89       100\n",
            "               Graphics       0.83      0.92      0.87        91\n",
            "               Hardware       0.69      0.62      0.65       111\n",
            "               Internet       0.68      0.53      0.59        99\n",
            "       Mobile_Computing       0.82      0.77      0.80       109\n",
            "             Multimedia       0.66      0.74      0.70        89\n",
            "            Open_Source       0.63      0.79      0.70        98\n",
            "            Programming       0.59      0.55      0.57       111\n",
            "               Robotics       0.90      0.93      0.91       103\n",
            "               Security       0.74      0.81      0.78        86\n",
            "               Software       0.48      0.28      0.35       105\n",
            "                Systems       0.64      0.66      0.65        99\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.71      0.71      0.71      1900\n",
            "           weighted avg       0.70      0.71      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9472, Train: 0.3855, Test: 0.3753\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8209, Train: 0.4976, Test: 0.4932\n",
            "Early stopping:  0.08932198463653176\n",
            "Epoch: 003, Loss: 2.6613, Train: 0.5437, Test: 0.5553\n",
            "Early stopping:  0.14326582218744668\n",
            "Epoch: 004, Loss: 2.4789, Train: 0.5792, Test: 0.5753\n",
            "Early stopping:  0.202595420735739\n",
            "Epoch: 005, Loss: 2.2823, Train: 0.5979, Test: 0.5984\n",
            "Early stopping:  0.26522917461216344\n",
            "Epoch: 006, Loss: 2.0800, Train: 0.6170, Test: 0.6158\n",
            "Early stopping:  0.29452316487752844\n",
            "Epoch: 007, Loss: 1.8840, Train: 0.6312, Test: 0.6384\n",
            "Early stopping:  0.3089387892771784\n",
            "Epoch: 008, Loss: 1.7058, Train: 0.6405, Test: 0.6453\n",
            "Early stopping:  0.3075408107970936\n",
            "Epoch: 009, Loss: 1.5521, Train: 0.6430, Test: 0.6526\n",
            "Early stopping:  0.29048909675959766\n",
            "Epoch: 010, Loss: 1.4272, Train: 0.6497, Test: 0.6516\n",
            "Early stopping:  0.2598541189614128\n",
            "Epoch: 011, Loss: 1.3310, Train: 0.6558, Test: 0.6611\n",
            "Early stopping:  0.22043496592779271\n",
            "Epoch: 012, Loss: 1.2576, Train: 0.6611, Test: 0.6611\n",
            "Early stopping:  0.17852402103870552\n",
            "Epoch: 013, Loss: 1.2030, Train: 0.6659, Test: 0.6684\n",
            "Early stopping:  0.1389659263617282\n",
            "Epoch: 014, Loss: 1.1620, Train: 0.6729, Test: 0.6732\n",
            "Early stopping:  0.10553637700527206\n",
            "Epoch: 015, Loss: 1.1287, Train: 0.6789, Test: 0.6784\n",
            "Early stopping:  0.08008312951095904\n",
            "Epoch: 016, Loss: 1.1011, Train: 0.6839, Test: 0.6805\n",
            "Early stopping:  0.06180093529847986\n",
            "Epoch: 017, Loss: 1.0761, Train: 0.6900, Test: 0.6853\n",
            "Early stopping:  0.0500213701418422\n",
            "Epoch: 018, Loss: 1.0524, Train: 0.6926, Test: 0.6905\n",
            "Early stopping:  0.043092516945482266\n",
            "Epoch: 019, Loss: 1.0299, Train: 0.6979, Test: 0.6921\n",
            "Early stopping:  0.03895479503179055\n",
            "Epoch: 020, Loss: 1.0078, Train: 0.7030, Test: 0.6953\n",
            "Early stopping:  0.036820300042953245\n",
            "Epoch: 021, Loss: 0.9868, Train: 0.7082, Test: 0.7021\n",
            "Early stopping:  0.0353176782626956\n",
            "Epoch: 022, Loss: 0.9665, Train: 0.7163, Test: 0.7016\n",
            "Early stopping:  0.03397999346990762\n",
            "Epoch: 023, Loss: 0.9472, Train: 0.7208, Test: 0.7074\n",
            "Early stopping:  0.0326895124677112\n",
            "Epoch: 024, Loss: 0.9292, Train: 0.7241, Test: 0.7063\n",
            "Early stopping:  0.031104727818809918\n",
            "Epoch: 025, Loss: 0.9122, Train: 0.7295, Test: 0.7100\n",
            "Early stopping:  0.029487339345826348\n",
            "Epoch: 026, Loss: 0.8965, Train: 0.7354, Test: 0.7105\n",
            "Early stopping:  0.027690829263894275\n",
            "Epoch: 027, Loss: 0.8817, Train: 0.7420, Test: 0.7137\n",
            "Early stopping:  0.025902820382761524\n",
            "Epoch: 028, Loss: 0.8677, Train: 0.7484, Test: 0.7168\n",
            "Early stopping:  0.024288704027169712\n",
            "Epoch: 029, Loss: 0.8547, Train: 0.7514, Test: 0.7153\n",
            "Early stopping:  0.022760083891308985\n",
            "Epoch: 030, Loss: 0.8423, Train: 0.7559, Test: 0.7174\n",
            "Early stopping:  0.021435855799812515\n",
            "Epoch: 031, Loss: 0.8308, Train: 0.7603, Test: 0.7163\n",
            "Early stopping:  0.02014118510594603\n",
            "Epoch: 032, Loss: 0.8199, Train: 0.7637, Test: 0.7216\n",
            "Early stopping:  0.018907687195575455\n",
            "Epoch: 033, Loss: 0.8098, Train: 0.7674, Test: 0.7237\n",
            "Early stopping:  0.017732042334596628\n",
            "Epoch: 034, Loss: 0.8001, Train: 0.7726, Test: 0.7258\n",
            "Early stopping:  0.01664904078473388\n",
            "Epoch: 035, Loss: 0.7910, Train: 0.7762, Test: 0.7232\n",
            "Early stopping:  0.015715728183901954\n",
            "Epoch: 036, Loss: 0.7822, Train: 0.7800, Test: 0.7221\n",
            "Early stopping:  0.014917419827522357\n",
            "Epoch: 037, Loss: 0.7736, Train: 0.7822, Test: 0.7263\n",
            "Early stopping:  0.014293884542682326\n",
            "Epoch: 038, Loss: 0.7653, Train: 0.7847, Test: 0.7284\n",
            "Early stopping:  0.013780792586091032\n",
            "Epoch: 039, Loss: 0.7569, Train: 0.7876, Test: 0.7295\n",
            "Early stopping:  0.013452686206424274\n",
            "Epoch: 040, Loss: 0.7488, Train: 0.7903, Test: 0.7300\n",
            "Early stopping:  0.013182831189034445\n",
            "Epoch: 041, Loss: 0.7407, Train: 0.7913, Test: 0.7284\n",
            "Early stopping:  0.013006797923829689\n",
            "Epoch: 042, Loss: 0.7327, Train: 0.7949, Test: 0.7274\n",
            "Early stopping:  0.012878327135195499\n",
            "Epoch: 043, Loss: 0.7246, Train: 0.7959, Test: 0.7311\n",
            "Early stopping:  0.012775786293840921\n",
            "Epoch: 044, Loss: 0.7169, Train: 0.7974, Test: 0.7326\n",
            "Early stopping:  0.012636287752441971\n",
            "Epoch: 045, Loss: 0.7096, Train: 0.8013, Test: 0.7321\n",
            "Early stopping:  0.012339200230192048\n",
            "Epoch: 046, Loss: 0.7007, Train: 0.8049, Test: 0.7321\n",
            "Early stopping:  0.012499067848589309\n",
            "Epoch: 047, Loss: 0.6916, Train: 0.8058, Test: 0.7300\n",
            "Early stopping:  0.013014030554432093\n",
            "Epoch: 048, Loss: 0.6838, Train: 0.8071, Test: 0.7300\n",
            "Early stopping:  0.013322123346969398\n",
            "Epoch: 049, Loss: 0.6764, Train: 0.8093, Test: 0.7342\n",
            "Early stopping:  0.013183033140394691\n",
            "Epoch: 050, Loss: 0.6683, Train: 0.8120, Test: 0.7363\n",
            "Early stopping:  0.01265673919409083\n",
            "Epoch: 051, Loss: 0.6597, Train: 0.8149, Test: 0.7389\n",
            "Early stopping:  0.01253792725487627\n",
            "Epoch: 052, Loss: 0.6521, Train: 0.8159, Test: 0.7353\n",
            "Early stopping:  0.012647289355144972\n",
            "Epoch: 053, Loss: 0.6448, Train: 0.8195, Test: 0.7389\n",
            "Early stopping:  0.012543984452297322\n",
            "Epoch: 054, Loss: 0.6367, Train: 0.8214, Test: 0.7353\n",
            "Early stopping:  0.012346339131739532\n",
            "Epoch: 055, Loss: 0.6286, Train: 0.8249, Test: 0.7342\n",
            "Early stopping:  0.0122632482150304\n",
            "Epoch: 056, Loss: 0.6213, Train: 0.8249, Test: 0.7337\n",
            "Early stopping:  0.012293444448131276\n",
            "Epoch: 057, Loss: 0.6144, Train: 0.8279, Test: 0.7321\n",
            "Early stopping:  0.0120754467286583\n",
            "Epoch: 058, Loss: 0.6078, Train: 0.8274, Test: 0.7326\n",
            "Early stopping:  0.011416738686772102\n",
            "Epoch: 059, Loss: 0.6024, Train: 0.8322, Test: 0.7316\n",
            "Early stopping:  0.010457120558010367\n",
            "Epoch: 060, Loss: 0.5975, Train: 0.8321, Test: 0.7311\n",
            "Early stopping:  0.00946391926248485\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.84      0.85      0.85       107\n",
            "            CAD_and_CAM       0.75      0.87      0.81       106\n",
            "              Companies       0.58      0.51      0.54        88\n",
            "       Computer_Science       0.70      0.74      0.72        91\n",
            "            Consultants       0.54      0.73      0.62        89\n",
            "           Data_Formats       0.77      0.87      0.81        98\n",
            "    Data_Communications       0.78      0.80      0.79       110\n",
            "              Education       0.85      0.86      0.85        99\n",
            "               Graphics       0.83      0.94      0.88       108\n",
            "               Hardware       0.73      0.59      0.66       101\n",
            "               Internet       0.67      0.68      0.68       100\n",
            "       Mobile_Computing       0.82      0.83      0.82       101\n",
            "             Multimedia       0.68      0.72      0.70       107\n",
            "            Open_Source       0.69      0.77      0.73        97\n",
            "            Programming       0.62      0.49      0.55       123\n",
            "               Robotics       0.97      0.94      0.95        89\n",
            "               Security       0.87      0.77      0.82        88\n",
            "               Software       0.43      0.29      0.35        96\n",
            "                Systems       0.68      0.64      0.66       102\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.73      0.73      0.73      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9531, Train: 0.3882, Test: 0.3826\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8282, Train: 0.5370, Test: 0.5279\n",
            "Early stopping:  0.08831113462184648\n",
            "Epoch: 003, Loss: 2.6547, Train: 0.5446, Test: 0.5484\n",
            "Early stopping:  0.1498345993932378\n",
            "Epoch: 004, Loss: 2.4527, Train: 0.5618, Test: 0.5658\n",
            "Early stopping:  0.21737257712663113\n",
            "Epoch: 005, Loss: 2.2438, Train: 0.5950, Test: 0.5858\n",
            "Early stopping:  0.2849776676685221\n",
            "Epoch: 006, Loss: 2.0318, Train: 0.6218, Test: 0.6142\n",
            "Early stopping:  0.3170503363794623\n",
            "Epoch: 007, Loss: 1.8296, Train: 0.6384, Test: 0.6274\n",
            "Early stopping:  0.3274767517098677\n",
            "Epoch: 008, Loss: 1.6482, Train: 0.6422, Test: 0.6447\n",
            "Early stopping:  0.32001084084150005\n",
            "Epoch: 009, Loss: 1.4949, Train: 0.6463, Test: 0.6400\n",
            "Early stopping:  0.2980616492425498\n",
            "Epoch: 010, Loss: 1.3750, Train: 0.6517, Test: 0.6437\n",
            "Early stopping:  0.26191470238513215\n",
            "Epoch: 011, Loss: 1.2851, Train: 0.6607, Test: 0.6479\n",
            "Early stopping:  0.21734166375428096\n",
            "Epoch: 012, Loss: 1.2197, Train: 0.6630, Test: 0.6516\n",
            "Early stopping:  0.17092097729209207\n",
            "Epoch: 013, Loss: 1.1743, Train: 0.6691, Test: 0.6516\n",
            "Early stopping:  0.1280626134914338\n",
            "Epoch: 014, Loss: 1.1407, Train: 0.6732, Test: 0.6553\n",
            "Early stopping:  0.09332423419917778\n",
            "Epoch: 015, Loss: 1.1145, Train: 0.6766, Test: 0.6632\n",
            "Early stopping:  0.06754556762670398\n",
            "Epoch: 016, Loss: 1.0916, Train: 0.6818, Test: 0.6689\n",
            "Early stopping:  0.05044806601571917\n",
            "Epoch: 017, Loss: 1.0696, Train: 0.6870, Test: 0.6737\n",
            "Early stopping:  0.04101971407539372\n",
            "Epoch: 018, Loss: 1.0484, Train: 0.6938, Test: 0.6779\n",
            "Early stopping:  0.03630606085562599\n",
            "Epoch: 019, Loss: 1.0267, Train: 0.7003, Test: 0.6842\n",
            "Early stopping:  0.03459331604394732\n",
            "Epoch: 020, Loss: 1.0051, Train: 0.7054, Test: 0.6874\n",
            "Early stopping:  0.034138575286957684\n",
            "Epoch: 021, Loss: 0.9843, Train: 0.7125, Test: 0.6895\n",
            "Early stopping:  0.03381347784045233\n",
            "Epoch: 022, Loss: 0.9642, Train: 0.7191, Test: 0.6911\n",
            "Early stopping:  0.03334833823694975\n",
            "Epoch: 023, Loss: 0.9454, Train: 0.7245, Test: 0.6947\n",
            "Early stopping:  0.03219606046570571\n",
            "Epoch: 024, Loss: 0.9279, Train: 0.7280, Test: 0.7011\n",
            "Early stopping:  0.030585283402244173\n",
            "Epoch: 025, Loss: 0.9118, Train: 0.7312, Test: 0.7026\n",
            "Early stopping:  0.02869516178911834\n",
            "Epoch: 026, Loss: 0.8970, Train: 0.7380, Test: 0.7047\n",
            "Early stopping:  0.026592245597673748\n",
            "Epoch: 027, Loss: 0.8832, Train: 0.7416, Test: 0.7068\n",
            "Early stopping:  0.024607291599012544\n",
            "Epoch: 028, Loss: 0.8703, Train: 0.7443, Test: 0.7121\n",
            "Early stopping:  0.022777093085583153\n",
            "Epoch: 029, Loss: 0.8578, Train: 0.7493, Test: 0.7121\n",
            "Early stopping:  0.021302862349748358\n",
            "Epoch: 030, Loss: 0.8460, Train: 0.7518, Test: 0.7132\n",
            "Early stopping:  0.02013131436592991\n",
            "Epoch: 031, Loss: 0.8347, Train: 0.7561, Test: 0.7163\n",
            "Early stopping:  0.019172384669583552\n",
            "Epoch: 032, Loss: 0.8239, Train: 0.7601, Test: 0.7147\n",
            "Early stopping:  0.018319087576160728\n",
            "Epoch: 033, Loss: 0.8141, Train: 0.7638, Test: 0.7163\n",
            "Early stopping:  0.017332949871308644\n",
            "Epoch: 034, Loss: 0.8049, Train: 0.7676, Test: 0.7226\n",
            "Early stopping:  0.016271900326262925\n",
            "Epoch: 035, Loss: 0.7966, Train: 0.7717, Test: 0.7258\n",
            "Early stopping:  0.015073816959378792\n",
            "Epoch: 036, Loss: 0.7886, Train: 0.7730, Test: 0.7274\n",
            "Early stopping:  0.013949130194778003\n",
            "Epoch: 037, Loss: 0.7809, Train: 0.7755, Test: 0.7289\n",
            "Early stopping:  0.013079185782476986\n",
            "Epoch: 038, Loss: 0.7731, Train: 0.7792, Test: 0.7274\n",
            "Early stopping:  0.012550344179199396\n",
            "Epoch: 039, Loss: 0.7652, Train: 0.7816, Test: 0.7295\n",
            "Early stopping:  0.012370896927088102\n",
            "Epoch: 040, Loss: 0.7573, Train: 0.7857, Test: 0.7332\n",
            "Early stopping:  0.012377115886500882\n",
            "Epoch: 041, Loss: 0.7492, Train: 0.7883, Test: 0.7337\n",
            "Early stopping:  0.012521664347710173\n",
            "Epoch: 042, Loss: 0.7410, Train: 0.7922, Test: 0.7342\n",
            "Early stopping:  0.012690765351399812\n",
            "Epoch: 043, Loss: 0.7326, Train: 0.7946, Test: 0.7326\n",
            "Early stopping:  0.012893501851009069\n",
            "Epoch: 044, Loss: 0.7243, Train: 0.7984, Test: 0.7363\n",
            "Early stopping:  0.013069664113638371\n",
            "Epoch: 045, Loss: 0.7159, Train: 0.8004, Test: 0.7358\n",
            "Early stopping:  0.0131855985405747\n",
            "Epoch: 046, Loss: 0.7075, Train: 0.8018, Test: 0.7358\n",
            "Early stopping:  0.013247960529716102\n",
            "Epoch: 047, Loss: 0.6990, Train: 0.8051, Test: 0.7342\n",
            "Early stopping:  0.0132746027094465\n",
            "Epoch: 048, Loss: 0.6905, Train: 0.8066, Test: 0.7347\n",
            "Early stopping:  0.01332175402300176\n",
            "Epoch: 049, Loss: 0.6821, Train: 0.8095, Test: 0.7342\n",
            "Early stopping:  0.013343579799069633\n",
            "Epoch: 050, Loss: 0.6739, Train: 0.8118, Test: 0.7337\n",
            "Early stopping:  0.013296422959490677\n",
            "Epoch: 051, Loss: 0.6659, Train: 0.8142, Test: 0.7332\n",
            "Early stopping:  0.013124013480346362\n",
            "Epoch: 052, Loss: 0.6583, Train: 0.8175, Test: 0.7316\n",
            "Early stopping:  0.012780764225465173\n",
            "Epoch: 053, Loss: 0.6508, Train: 0.8176, Test: 0.7363\n",
            "Early stopping:  0.012362833072776847\n",
            "Epoch: 054, Loss: 0.6430, Train: 0.8205, Test: 0.7342\n",
            "Early stopping:  0.012140824046807396\n",
            "Epoch: 055, Loss: 0.6344, Train: 0.8229, Test: 0.7358\n",
            "Early stopping:  0.012370254617089505\n",
            "Epoch: 056, Loss: 0.6264, Train: 0.8270, Test: 0.7353\n",
            "Early stopping:  0.01268043501003039\n",
            "Epoch: 057, Loss: 0.6192, Train: 0.8303, Test: 0.7342\n",
            "Early stopping:  0.012637912888098133\n",
            "Epoch: 058, Loss: 0.6118, Train: 0.8322, Test: 0.7347\n",
            "Early stopping:  0.012286813356771594\n",
            "Epoch: 059, Loss: 0.6041, Train: 0.8354, Test: 0.7326\n",
            "Early stopping:  0.011885764067243761\n",
            "Epoch: 060, Loss: 0.5968, Train: 0.8378, Test: 0.7332\n",
            "Early stopping:  0.011757458754118776\n",
            "Epoch: 061, Loss: 0.5897, Train: 0.8408, Test: 0.7353\n",
            "Early stopping:  0.011698393212677762\n",
            "Epoch: 062, Loss: 0.5827, Train: 0.8432, Test: 0.7316\n",
            "Early stopping:  0.011468752463305356\n",
            "Epoch: 063, Loss: 0.5752, Train: 0.8493, Test: 0.7337\n",
            "Early stopping:  0.011365313307289371\n",
            "Epoch: 064, Loss: 0.5677, Train: 0.8479, Test: 0.7353\n",
            "Early stopping:  0.011491790490671196\n",
            "Epoch: 065, Loss: 0.5608, Train: 0.8537, Test: 0.7316\n",
            "Early stopping:  0.011511625036450058\n",
            "Epoch: 066, Loss: 0.5546, Train: 0.8520, Test: 0.7347\n",
            "Early stopping:  0.011180773323994468\n",
            "Epoch: 067, Loss: 0.5485, Train: 0.8562, Test: 0.7263\n",
            "Early stopping:  0.010524650314172755\n",
            "Epoch: 068, Loss: 0.5428, Train: 0.8538, Test: 0.7347\n",
            "Early stopping:  0.00982246409780903\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.82      0.80        91\n",
            "            CAD_and_CAM       0.85      0.86      0.85       106\n",
            "              Companies       0.67      0.42      0.51       105\n",
            "       Computer_Science       0.77      0.78      0.77       101\n",
            "            Consultants       0.57      0.73      0.64       112\n",
            "           Data_Formats       0.90      0.82      0.86        96\n",
            "    Data_Communications       0.72      0.81      0.77        86\n",
            "              Education       0.88      0.86      0.87       124\n",
            "               Graphics       0.81      0.88      0.84        97\n",
            "               Hardware       0.77      0.67      0.72        89\n",
            "               Internet       0.70      0.68      0.69       103\n",
            "       Mobile_Computing       0.75      0.84      0.79        93\n",
            "             Multimedia       0.66      0.65      0.66       100\n",
            "            Open_Source       0.69      0.81      0.74       100\n",
            "            Programming       0.61      0.58      0.60        89\n",
            "               Robotics       0.94      0.91      0.92       100\n",
            "               Security       0.78      0.86      0.82       104\n",
            "               Software       0.36      0.33      0.34        95\n",
            "                Systems       0.71      0.61      0.66       109\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.73      0.73      0.73      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9472, Train: 0.3857, Test: 0.3674\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8102, Train: 0.4100, Test: 0.3968\n",
            "Early stopping:  0.09690909171806887\n",
            "Epoch: 003, Loss: 2.6376, Train: 0.4862, Test: 0.4758\n",
            "Early stopping:  0.1551868153779312\n",
            "Epoch: 004, Loss: 2.4369, Train: 0.5512, Test: 0.5442\n",
            "Early stopping:  0.2207235803184367\n",
            "Epoch: 005, Loss: 2.2273, Train: 0.5899, Test: 0.5758\n",
            "Early stopping:  0.2876625727444257\n",
            "Epoch: 006, Loss: 2.0207, Train: 0.6151, Test: 0.6021\n",
            "Early stopping:  0.3147435914749436\n",
            "Epoch: 007, Loss: 1.8254, Train: 0.6305, Test: 0.6226\n",
            "Early stopping:  0.3226418936111478\n",
            "Epoch: 008, Loss: 1.6503, Train: 0.6376, Test: 0.6247\n",
            "Early stopping:  0.3124749258430127\n",
            "Epoch: 009, Loss: 1.5016, Train: 0.6453, Test: 0.6337\n",
            "Early stopping:  0.28863846319145736\n",
            "Epoch: 010, Loss: 1.3827, Train: 0.6516, Test: 0.6468\n",
            "Early stopping:  0.2540832886446053\n",
            "Epoch: 011, Loss: 1.2921, Train: 0.6603, Test: 0.6579\n",
            "Early stopping:  0.2126189508951778\n",
            "Epoch: 012, Loss: 1.2260, Train: 0.6649, Test: 0.6621\n",
            "Early stopping:  0.1692600885928723\n",
            "Epoch: 013, Loss: 1.1782, Train: 0.6684, Test: 0.6595\n",
            "Early stopping:  0.12897969019154495\n",
            "Epoch: 014, Loss: 1.1436, Train: 0.6732, Test: 0.6637\n",
            "Early stopping:  0.09524844870982796\n",
            "Epoch: 015, Loss: 1.1163, Train: 0.6772, Test: 0.6705\n",
            "Early stopping:  0.06972924329199104\n",
            "Epoch: 016, Loss: 1.0927, Train: 0.6854, Test: 0.6758\n",
            "Early stopping:  0.05249441249143064\n",
            "Epoch: 017, Loss: 1.0702, Train: 0.6936, Test: 0.6763\n",
            "Early stopping:  0.04237852887176429\n",
            "Epoch: 018, Loss: 1.0480, Train: 0.6987, Test: 0.6779\n",
            "Early stopping:  0.03756333035241403\n",
            "Epoch: 019, Loss: 1.0257, Train: 0.7057, Test: 0.6826\n",
            "Early stopping:  0.03572923857403476\n",
            "Epoch: 020, Loss: 1.0037, Train: 0.7107, Test: 0.6879\n",
            "Early stopping:  0.03519982941127694\n",
            "Epoch: 021, Loss: 0.9823, Train: 0.7171, Test: 0.6921\n",
            "Early stopping:  0.03478756032166093\n",
            "Epoch: 022, Loss: 0.9622, Train: 0.7225, Test: 0.6900\n",
            "Early stopping:  0.034007238518337785\n",
            "Epoch: 023, Loss: 0.9434, Train: 0.7268, Test: 0.6911\n",
            "Early stopping:  0.03261110472652402\n",
            "Epoch: 024, Loss: 0.9258, Train: 0.7322, Test: 0.6932\n",
            "Early stopping:  0.030820868526321037\n",
            "Epoch: 025, Loss: 0.9093, Train: 0.7376, Test: 0.6968\n",
            "Early stopping:  0.028874690272654494\n",
            "Epoch: 026, Loss: 0.8936, Train: 0.7422, Test: 0.6995\n",
            "Early stopping:  0.02709792379969108\n",
            "Epoch: 027, Loss: 0.8790, Train: 0.7462, Test: 0.7005\n",
            "Early stopping:  0.025453498160812455\n",
            "Epoch: 028, Loss: 0.8650, Train: 0.7521, Test: 0.7016\n",
            "Early stopping:  0.024006684030196953\n",
            "Epoch: 029, Loss: 0.8518, Train: 0.7547, Test: 0.7026\n",
            "Early stopping:  0.022686809641386287\n",
            "Epoch: 030, Loss: 0.8394, Train: 0.7574, Test: 0.7053\n",
            "Early stopping:  0.021438255502592103\n",
            "Epoch: 031, Loss: 0.8276, Train: 0.7628, Test: 0.7079\n",
            "Early stopping:  0.020330590625647866\n",
            "Epoch: 032, Loss: 0.8165, Train: 0.7679, Test: 0.7074\n",
            "Early stopping:  0.01920239236706225\n",
            "Epoch: 033, Loss: 0.8063, Train: 0.7711, Test: 0.7068\n",
            "Early stopping:  0.018025688740113906\n",
            "Epoch: 034, Loss: 0.7968, Train: 0.7750, Test: 0.7074\n",
            "Early stopping:  0.01684628572654949\n",
            "Epoch: 035, Loss: 0.7877, Train: 0.7783, Test: 0.7095\n",
            "Early stopping:  0.015750461926691925\n",
            "Epoch: 036, Loss: 0.7788, Train: 0.7811, Test: 0.7126\n",
            "Early stopping:  0.014875038843201708\n",
            "Epoch: 037, Loss: 0.7701, Train: 0.7847, Test: 0.7121\n",
            "Early stopping:  0.014309370637617107\n",
            "Epoch: 038, Loss: 0.7615, Train: 0.7891, Test: 0.7153\n",
            "Early stopping:  0.013945176540287502\n",
            "Epoch: 039, Loss: 0.7528, Train: 0.7912, Test: 0.7168\n",
            "Early stopping:  0.013768583651881175\n",
            "Epoch: 040, Loss: 0.7440, Train: 0.7926, Test: 0.7174\n",
            "Early stopping:  0.013748708180780101\n",
            "Epoch: 041, Loss: 0.7351, Train: 0.7941, Test: 0.7168\n",
            "Early stopping:  0.013837836285243604\n",
            "Epoch: 042, Loss: 0.7260, Train: 0.7980, Test: 0.7189\n",
            "Early stopping:  0.01400315118888029\n",
            "Epoch: 043, Loss: 0.7171, Train: 0.8005, Test: 0.7216\n",
            "Early stopping:  0.014119227762509323\n",
            "Epoch: 044, Loss: 0.7082, Train: 0.8018, Test: 0.7168\n",
            "Early stopping:  0.014171447342972517\n",
            "Epoch: 045, Loss: 0.6992, Train: 0.8029, Test: 0.7205\n",
            "Early stopping:  0.014160285444174232\n",
            "Epoch: 046, Loss: 0.6906, Train: 0.8061, Test: 0.7179\n",
            "Early stopping:  0.014028044198062651\n",
            "Epoch: 047, Loss: 0.6827, Train: 0.8064, Test: 0.7237\n",
            "Early stopping:  0.013656046234349128\n",
            "Epoch: 048, Loss: 0.6751, Train: 0.8087, Test: 0.7200\n",
            "Early stopping:  0.013085896636331102\n",
            "Epoch: 049, Loss: 0.6660, Train: 0.8136, Test: 0.7211\n",
            "Early stopping:  0.012968973744076392\n",
            "Epoch: 050, Loss: 0.6551, Train: 0.8155, Test: 0.7216\n",
            "Early stopping:  0.013896176007500654\n",
            "Epoch: 051, Loss: 0.6482, Train: 0.8171, Test: 0.7237\n",
            "Early stopping:  0.014110007159802002\n",
            "Epoch: 052, Loss: 0.6412, Train: 0.8201, Test: 0.7232\n",
            "Early stopping:  0.013603818460281572\n",
            "Epoch: 053, Loss: 0.6307, Train: 0.8228, Test: 0.7232\n",
            "Early stopping:  0.013414060179717075\n",
            "Epoch: 054, Loss: 0.6236, Train: 0.8237, Test: 0.7242\n",
            "Early stopping:  0.01276462397620781\n",
            "Epoch: 055, Loss: 0.6168, Train: 0.8287, Test: 0.7258\n",
            "Early stopping:  0.012740455541123956\n",
            "Epoch: 056, Loss: 0.6075, Train: 0.8307, Test: 0.7237\n",
            "Early stopping:  0.012880621639592768\n",
            "Epoch: 057, Loss: 0.6004, Train: 0.8321, Test: 0.7226\n",
            "Early stopping:  0.012148873349657665\n",
            "Epoch: 058, Loss: 0.5934, Train: 0.8355, Test: 0.7211\n",
            "Early stopping:  0.01214933135094963\n",
            "Epoch: 059, Loss: 0.5852, Train: 0.8374, Test: 0.7216\n",
            "Early stopping:  0.01223196776106957\n",
            "Epoch: 060, Loss: 0.5780, Train: 0.8404, Test: 0.7211\n",
            "Early stopping:  0.011761213218831123\n",
            "Epoch: 061, Loss: 0.5707, Train: 0.8430, Test: 0.7258\n",
            "Early stopping:  0.01184541931243228\n",
            "Epoch: 062, Loss: 0.5634, Train: 0.8479, Test: 0.7221\n",
            "Early stopping:  0.011791979971380774\n",
            "Epoch: 063, Loss: 0.5561, Train: 0.8495, Test: 0.7216\n",
            "Early stopping:  0.011517290904650356\n",
            "Epoch: 064, Loss: 0.5489, Train: 0.8534, Test: 0.7253\n",
            "Early stopping:  0.011505616504062517\n",
            "Epoch: 065, Loss: 0.5423, Train: 0.8514, Test: 0.7200\n",
            "Early stopping:  0.011265747836079103\n",
            "Epoch: 066, Loss: 0.5352, Train: 0.8566, Test: 0.7200\n",
            "Early stopping:  0.011086752805371905\n",
            "Epoch: 067, Loss: 0.5290, Train: 0.8574, Test: 0.7216\n",
            "Early stopping:  0.010717700773862714\n",
            "Epoch: 068, Loss: 0.5242, Train: 0.8578, Test: 0.7184\n",
            "Early stopping:  0.009916765441129226\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.88      0.82      0.85       105\n",
            "            CAD_and_CAM       0.80      0.78      0.79        95\n",
            "              Companies       0.57      0.62      0.59        86\n",
            "       Computer_Science       0.81      0.78      0.80        87\n",
            "            Consultants       0.61      0.56      0.59        91\n",
            "           Data_Formats       0.81      0.82      0.82       105\n",
            "    Data_Communications       0.73      0.73      0.73       114\n",
            "              Education       0.87      0.92      0.89       106\n",
            "               Graphics       0.81      0.89      0.85       113\n",
            "               Hardware       0.65      0.71      0.68        90\n",
            "               Internet       0.66      0.61      0.63       109\n",
            "       Mobile_Computing       0.69      0.81      0.75        85\n",
            "             Multimedia       0.69      0.75      0.72        91\n",
            "            Open_Source       0.72      0.75      0.73        96\n",
            "            Programming       0.43      0.52      0.47        86\n",
            "               Robotics       0.91      0.94      0.92       102\n",
            "               Security       0.84      0.84      0.84       117\n",
            "               Software       0.40      0.32      0.36       108\n",
            "                Systems       0.65      0.46      0.54       114\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.71      0.72      0.71      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9452, Train: 0.4041, Test: 0.3837\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8044, Train: 0.4733, Test: 0.4595\n",
            "Early stopping:  0.09951697011652895\n",
            "Epoch: 003, Loss: 2.6282, Train: 0.5403, Test: 0.5284\n",
            "Early stopping:  0.15878519143719774\n",
            "Epoch: 004, Loss: 2.4264, Train: 0.5996, Test: 0.5968\n",
            "Early stopping:  0.22435099359797867\n",
            "Epoch: 005, Loss: 2.2160, Train: 0.6217, Test: 0.6168\n",
            "Early stopping:  0.2912209973541165\n",
            "Epoch: 006, Loss: 2.0099, Train: 0.6358, Test: 0.6253\n",
            "Early stopping:  0.31660450445666183\n",
            "Epoch: 007, Loss: 1.8146, Train: 0.6463, Test: 0.6395\n",
            "Early stopping:  0.3231755123069772\n",
            "Epoch: 008, Loss: 1.6377, Train: 0.6512, Test: 0.6474\n",
            "Early stopping:  0.3130569756154879\n",
            "Epoch: 009, Loss: 1.4890, Train: 0.6538, Test: 0.6453\n",
            "Early stopping:  0.2893016934135734\n",
            "Epoch: 010, Loss: 1.3704, Train: 0.6578, Test: 0.6468\n",
            "Early stopping:  0.2548774763849439\n",
            "Epoch: 011, Loss: 1.2789, Train: 0.6624, Test: 0.6537\n",
            "Early stopping:  0.21337413775776423\n",
            "Epoch: 012, Loss: 1.2133, Train: 0.6659, Test: 0.6626\n",
            "Early stopping:  0.16941569976842724\n",
            "Epoch: 013, Loss: 1.1667, Train: 0.6674, Test: 0.6600\n",
            "Early stopping:  0.12878876831286248\n",
            "Epoch: 014, Loss: 1.1335, Train: 0.6726, Test: 0.6642\n",
            "Early stopping:  0.09442012499039118\n",
            "Epoch: 015, Loss: 1.1094, Train: 0.6772, Test: 0.6637\n",
            "Early stopping:  0.0674706333938903\n",
            "Epoch: 016, Loss: 1.0890, Train: 0.6832, Test: 0.6674\n",
            "Early stopping:  0.049072803358137386\n",
            "Epoch: 017, Loss: 1.0692, Train: 0.6907, Test: 0.6689\n",
            "Early stopping:  0.03810589150844805\n",
            "Epoch: 018, Loss: 1.0493, Train: 0.6974, Test: 0.6726\n",
            "Early stopping:  0.03302005449516394\n",
            "Epoch: 019, Loss: 1.0278, Train: 0.7028, Test: 0.6805\n",
            "Early stopping:  0.03206998586626933\n",
            "Epoch: 020, Loss: 1.0059, Train: 0.7087, Test: 0.6821\n",
            "Early stopping:  0.032839875653204\n",
            "Epoch: 021, Loss: 0.9843, Train: 0.7155, Test: 0.6863\n",
            "Early stopping:  0.033704063048400026\n",
            "Epoch: 022, Loss: 0.9634, Train: 0.7205, Test: 0.6863\n",
            "Early stopping:  0.03403243050859932\n",
            "Epoch: 023, Loss: 0.9436, Train: 0.7261, Test: 0.6905\n",
            "Early stopping:  0.03337241354914112\n",
            "Epoch: 024, Loss: 0.9249, Train: 0.7307, Test: 0.6937\n",
            "Early stopping:  0.03205212853221451\n",
            "Epoch: 025, Loss: 0.9081, Train: 0.7334, Test: 0.6963\n",
            "Early stopping:  0.030207099007834855\n",
            "Epoch: 026, Loss: 0.8926, Train: 0.7396, Test: 0.6974\n",
            "Early stopping:  0.028042987929942764\n",
            "Epoch: 027, Loss: 0.8785, Train: 0.7445, Test: 0.6979\n",
            "Early stopping:  0.025732871964855686\n",
            "Epoch: 028, Loss: 0.8656, Train: 0.7470, Test: 0.7047\n",
            "Early stopping:  0.023476327226596566\n",
            "Epoch: 029, Loss: 0.8536, Train: 0.7525, Test: 0.7074\n",
            "Early stopping:  0.0215389435856333\n",
            "Epoch: 030, Loss: 0.8424, Train: 0.7597, Test: 0.7095\n",
            "Early stopping:  0.019825060369003302\n",
            "Epoch: 031, Loss: 0.8316, Train: 0.7649, Test: 0.7105\n",
            "Early stopping:  0.018511429380847554\n",
            "Epoch: 032, Loss: 0.8215, Train: 0.7689, Test: 0.7142\n",
            "Early stopping:  0.017447468350374123\n",
            "Epoch: 033, Loss: 0.8119, Train: 0.7741, Test: 0.7189\n",
            "Early stopping:  0.01650347452870618\n",
            "Epoch: 034, Loss: 0.8027, Train: 0.7784, Test: 0.7184\n",
            "Early stopping:  0.01569379689064044\n",
            "Epoch: 035, Loss: 0.7939, Train: 0.7796, Test: 0.7195\n",
            "Early stopping:  0.014896533153946031\n",
            "Epoch: 036, Loss: 0.7854, Train: 0.7828, Test: 0.7189\n",
            "Early stopping:  0.014237490842215412\n",
            "Epoch: 037, Loss: 0.7771, Train: 0.7837, Test: 0.7200\n",
            "Early stopping:  0.013738611452477852\n",
            "Epoch: 038, Loss: 0.7688, Train: 0.7864, Test: 0.7226\n",
            "Early stopping:  0.013389247056969126\n",
            "Epoch: 039, Loss: 0.7603, Train: 0.7879, Test: 0.7247\n",
            "Early stopping:  0.013254624187540212\n",
            "Epoch: 040, Loss: 0.7520, Train: 0.7912, Test: 0.7268\n",
            "Early stopping:  0.013223745882325107\n",
            "Epoch: 041, Loss: 0.7436, Train: 0.7937, Test: 0.7253\n",
            "Early stopping:  0.013234941235825505\n",
            "Epoch: 042, Loss: 0.7352, Train: 0.7943, Test: 0.7258\n",
            "Early stopping:  0.013270754241269648\n",
            "Epoch: 043, Loss: 0.7269, Train: 0.7971, Test: 0.7289\n",
            "Early stopping:  0.013241715591403587\n",
            "Epoch: 044, Loss: 0.7186, Train: 0.7992, Test: 0.7300\n",
            "Early stopping:  0.0132080397943351\n",
            "Epoch: 045, Loss: 0.7103, Train: 0.8009, Test: 0.7289\n",
            "Early stopping:  0.01314151331446169\n",
            "Epoch: 046, Loss: 0.7021, Train: 0.8036, Test: 0.7295\n",
            "Early stopping:  0.013060052124382687\n",
            "Epoch: 047, Loss: 0.6940, Train: 0.8050, Test: 0.7289\n",
            "Early stopping:  0.013003859874281086\n",
            "Epoch: 048, Loss: 0.6859, Train: 0.8084, Test: 0.7279\n",
            "Early stopping:  0.012917406219649971\n",
            "Epoch: 049, Loss: 0.6779, Train: 0.8113, Test: 0.7284\n",
            "Early stopping:  0.012824732126188253\n",
            "Epoch: 050, Loss: 0.6700, Train: 0.8137, Test: 0.7300\n",
            "Early stopping:  0.012709242089439935\n",
            "Epoch: 051, Loss: 0.6621, Train: 0.8158, Test: 0.7332\n",
            "Early stopping:  0.012586317351605229\n",
            "Epoch: 052, Loss: 0.6544, Train: 0.8180, Test: 0.7321\n",
            "Early stopping:  0.012470050056697081\n",
            "Epoch: 053, Loss: 0.6467, Train: 0.8193, Test: 0.7326\n",
            "Early stopping:  0.01232765266130841\n",
            "Epoch: 054, Loss: 0.6395, Train: 0.8195, Test: 0.7326\n",
            "Early stopping:  0.012069864820136703\n",
            "Epoch: 055, Loss: 0.6333, Train: 0.8191, Test: 0.7311\n",
            "Early stopping:  0.011479772141060055\n",
            "Epoch: 056, Loss: 0.6297, Train: 0.8234, Test: 0.7295\n",
            "Early stopping:  0.010012474234940626\n",
            "Epoch: 057, Loss: 0.6220, Train: 0.8257, Test: 0.7305\n",
            "Early stopping:  0.009438945341118334\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.92      0.82      0.87       106\n",
            "            CAD_and_CAM       0.76      0.73      0.74        92\n",
            "              Companies       0.55      0.43      0.48        98\n",
            "       Computer_Science       0.80      0.91      0.85        99\n",
            "            Consultants       0.62      0.66      0.64       122\n",
            "           Data_Formats       0.81      0.80      0.80       114\n",
            "    Data_Communications       0.78      0.78      0.78       112\n",
            "              Education       0.86      0.89      0.88       103\n",
            "               Graphics       0.83      0.93      0.87       109\n",
            "               Hardware       0.64      0.59      0.61        97\n",
            "               Internet       0.77      0.72      0.74        99\n",
            "       Mobile_Computing       0.81      0.81      0.81       114\n",
            "             Multimedia       0.72      0.75      0.73        95\n",
            "            Open_Source       0.68      0.69      0.69        84\n",
            "            Programming       0.58      0.70      0.64        98\n",
            "               Robotics       0.92      0.96      0.94        91\n",
            "               Security       0.73      0.82      0.77        85\n",
            "               Software       0.48      0.34      0.40       102\n",
            "                Systems       0.48      0.51      0.49        80\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.72      0.73      0.72      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9461, Train: 0.4150, Test: 0.4253\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8237, Train: 0.5186, Test: 0.5332\n",
            "Early stopping:  0.08658125937156534\n",
            "Epoch: 003, Loss: 2.6605, Train: 0.5237, Test: 0.5389\n",
            "Early stopping:  0.14329819125533585\n",
            "Epoch: 004, Loss: 2.4665, Train: 0.5541, Test: 0.5637\n",
            "Early stopping:  0.2078530120840532\n",
            "Epoch: 005, Loss: 2.2611, Train: 0.5938, Test: 0.6026\n",
            "Early stopping:  0.27441515764101415\n",
            "Epoch: 006, Loss: 2.0546, Train: 0.6234, Test: 0.6284\n",
            "Early stopping:  0.3066992331957564\n",
            "Epoch: 007, Loss: 1.8575, Train: 0.6354, Test: 0.6400\n",
            "Early stopping:  0.3190785136513894\n",
            "Epoch: 008, Loss: 1.6814, Train: 0.6426, Test: 0.6405\n",
            "Early stopping:  0.31224531780464293\n",
            "Epoch: 009, Loss: 1.5312, Train: 0.6463, Test: 0.6405\n",
            "Early stopping:  0.29038234612907676\n",
            "Epoch: 010, Loss: 1.4085, Train: 0.6470, Test: 0.6405\n",
            "Early stopping:  0.2569562609056735\n",
            "Epoch: 011, Loss: 1.3145, Train: 0.6537, Test: 0.6453\n",
            "Early stopping:  0.21637789602942548\n",
            "Epoch: 012, Loss: 1.2443, Train: 0.6607, Test: 0.6558\n",
            "Early stopping:  0.17428790338746528\n",
            "Epoch: 013, Loss: 1.1925, Train: 0.6651, Test: 0.6600\n",
            "Early stopping:  0.13488970071357073\n",
            "Epoch: 014, Loss: 1.1532, Train: 0.6709, Test: 0.6584\n",
            "Early stopping:  0.10150260405531636\n",
            "Epoch: 015, Loss: 1.1224, Train: 0.6770, Test: 0.6589\n",
            "Early stopping:  0.07617106056825199\n",
            "Epoch: 016, Loss: 1.0958, Train: 0.6846, Test: 0.6663\n",
            "Early stopping:  0.058618233047102675\n",
            "Epoch: 017, Loss: 1.0723, Train: 0.6912, Test: 0.6726\n",
            "Early stopping:  0.047343513724830665\n",
            "Epoch: 018, Loss: 1.0505, Train: 0.6964, Test: 0.6758\n",
            "Early stopping:  0.04049445411525504\n",
            "Epoch: 019, Loss: 1.0293, Train: 0.7033, Test: 0.6779\n",
            "Early stopping:  0.03661819810192755\n",
            "Epoch: 020, Loss: 1.0085, Train: 0.7093, Test: 0.6821\n",
            "Early stopping:  0.034396485672262736\n",
            "Epoch: 021, Loss: 0.9881, Train: 0.7130, Test: 0.6847\n",
            "Early stopping:  0.03327471826726108\n",
            "Epoch: 022, Loss: 0.9682, Train: 0.7172, Test: 0.6837\n",
            "Early stopping:  0.03253991381836485\n",
            "Epoch: 023, Loss: 0.9492, Train: 0.7232, Test: 0.6889\n",
            "Early stopping:  0.03171867882877413\n",
            "Epoch: 024, Loss: 0.9312, Train: 0.7284, Test: 0.6942\n",
            "Early stopping:  0.0306018956359531\n",
            "Epoch: 025, Loss: 0.9143, Train: 0.7325, Test: 0.6953\n",
            "Early stopping:  0.02921616396962442\n",
            "Epoch: 026, Loss: 0.8988, Train: 0.7380, Test: 0.6921\n",
            "Early stopping:  0.027494702856909352\n",
            "Epoch: 027, Loss: 0.8847, Train: 0.7428, Test: 0.6958\n",
            "Early stopping:  0.025535584380620622\n",
            "Epoch: 028, Loss: 0.8715, Train: 0.7474, Test: 0.7016\n",
            "Early stopping:  0.0235830912056937\n",
            "Epoch: 029, Loss: 0.8587, Train: 0.7524, Test: 0.7053\n",
            "Early stopping:  0.02190302250361358\n",
            "Epoch: 030, Loss: 0.8465, Train: 0.7563, Test: 0.7063\n",
            "Early stopping:  0.02065964478722831\n",
            "Epoch: 031, Loss: 0.8347, Train: 0.7617, Test: 0.7100\n",
            "Early stopping:  0.019781215661948844\n",
            "Epoch: 032, Loss: 0.8234, Train: 0.7662, Test: 0.7089\n",
            "Early stopping:  0.019006876871602497\n",
            "Epoch: 033, Loss: 0.8129, Train: 0.7683, Test: 0.7121\n",
            "Early stopping:  0.018160616197901738\n",
            "Epoch: 034, Loss: 0.8030, Train: 0.7725, Test: 0.7153\n",
            "Early stopping:  0.01722715132040638\n",
            "Epoch: 035, Loss: 0.7939, Train: 0.7747, Test: 0.7195\n",
            "Early stopping:  0.01614259962836199\n",
            "Epoch: 036, Loss: 0.7853, Train: 0.7783, Test: 0.7174\n",
            "Early stopping:  0.015053485700545412\n",
            "Epoch: 037, Loss: 0.7772, Train: 0.7800, Test: 0.7174\n",
            "Early stopping:  0.01409103685078652\n",
            "Epoch: 038, Loss: 0.7693, Train: 0.7832, Test: 0.7158\n",
            "Early stopping:  0.013321815845124674\n",
            "Epoch: 039, Loss: 0.7614, Train: 0.7853, Test: 0.7142\n",
            "Early stopping:  0.012817021948568891\n",
            "Epoch: 040, Loss: 0.7535, Train: 0.7867, Test: 0.7105\n",
            "Early stopping:  0.012560788758734323\n",
            "Epoch: 041, Loss: 0.7454, Train: 0.7897, Test: 0.7137\n",
            "Early stopping:  0.012538351457067328\n",
            "Epoch: 042, Loss: 0.7370, Train: 0.7925, Test: 0.7168\n",
            "Early stopping:  0.012719551312960282\n",
            "Epoch: 043, Loss: 0.7286, Train: 0.7961, Test: 0.7153\n",
            "Early stopping:  0.01297167546957695\n",
            "Epoch: 044, Loss: 0.7202, Train: 0.8004, Test: 0.7184\n",
            "Early stopping:  0.013191960705091248\n",
            "Epoch: 045, Loss: 0.7117, Train: 0.8039, Test: 0.7179\n",
            "Early stopping:  0.013322359420289222\n",
            "Epoch: 046, Loss: 0.7033, Train: 0.8057, Test: 0.7200\n",
            "Early stopping:  0.013348060191018656\n",
            "Epoch: 047, Loss: 0.6949, Train: 0.8089, Test: 0.7205\n",
            "Early stopping:  0.013330402457877545\n",
            "Epoch: 048, Loss: 0.6866, Train: 0.8084, Test: 0.7242\n",
            "Early stopping:  0.013275856623706115\n",
            "Epoch: 049, Loss: 0.6784, Train: 0.8100, Test: 0.7195\n",
            "Early stopping:  0.013177143610871078\n",
            "Epoch: 050, Loss: 0.6704, Train: 0.8109, Test: 0.7205\n",
            "Early stopping:  0.013003075879128188\n",
            "Epoch: 051, Loss: 0.6626, Train: 0.8139, Test: 0.7205\n",
            "Early stopping:  0.012755047095842747\n",
            "Epoch: 052, Loss: 0.6547, Train: 0.8167, Test: 0.7179\n",
            "Early stopping:  0.012565888243557696\n",
            "Epoch: 053, Loss: 0.6466, Train: 0.8192, Test: 0.7216\n",
            "Early stopping:  0.01253430893162882\n",
            "Epoch: 054, Loss: 0.6384, Train: 0.8217, Test: 0.7200\n",
            "Early stopping:  0.0126647797813801\n",
            "Epoch: 055, Loss: 0.6307, Train: 0.8224, Test: 0.7232\n",
            "Early stopping:  0.012695047747879174\n",
            "Epoch: 056, Loss: 0.6234, Train: 0.8255, Test: 0.7221\n",
            "Early stopping:  0.012420740013509627\n",
            "Epoch: 057, Loss: 0.6166, Train: 0.8270, Test: 0.7184\n",
            "Early stopping:  0.011876958721100398\n",
            "Epoch: 058, Loss: 0.6098, Train: 0.8300, Test: 0.7205\n",
            "Early stopping:  0.011285289152677394\n",
            "Epoch: 059, Loss: 0.6028, Train: 0.8309, Test: 0.7179\n",
            "Early stopping:  0.010981086457428367\n",
            "Epoch: 060, Loss: 0.5957, Train: 0.8349, Test: 0.7221\n",
            "Early stopping:  0.010942984116797838\n",
            "Epoch: 061, Loss: 0.5888, Train: 0.8367, Test: 0.7189\n",
            "Early stopping:  0.011009827083394427\n",
            "Epoch: 062, Loss: 0.5824, Train: 0.8397, Test: 0.7221\n",
            "Early stopping:  0.01087789863104184\n",
            "Epoch: 063, Loss: 0.5757, Train: 0.8438, Test: 0.7205\n",
            "Early stopping:  0.010662707763973944\n",
            "Epoch: 064, Loss: 0.5686, Train: 0.8442, Test: 0.7211\n",
            "Early stopping:  0.010625867635154931\n",
            "Epoch: 065, Loss: 0.5614, Train: 0.8483, Test: 0.7200\n",
            "Early stopping:  0.01083343264657807\n",
            "Epoch: 066, Loss: 0.5548, Train: 0.8513, Test: 0.7184\n",
            "Early stopping:  0.01100280275432812\n",
            "Epoch: 067, Loss: 0.5485, Train: 0.8533, Test: 0.7216\n",
            "Early stopping:  0.010800414799312418\n",
            "Epoch: 068, Loss: 0.5420, Train: 0.8554, Test: 0.7184\n",
            "Early stopping:  0.010461886412637462\n",
            "Epoch: 069, Loss: 0.5354, Train: 0.8593, Test: 0.7174\n",
            "Early stopping:  0.010222567980191043\n",
            "Epoch: 070, Loss: 0.5293, Train: 0.8588, Test: 0.7163\n",
            "Early stopping:  0.010122468254610729\n",
            "Epoch: 071, Loss: 0.5241, Train: 0.8626, Test: 0.7142\n",
            "Early stopping:  0.009738946065227612\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.82      0.80      0.81        94\n",
            "            CAD_and_CAM       0.78      0.73      0.75       105\n",
            "              Companies       0.65      0.49      0.56       102\n",
            "       Computer_Science       0.78      0.82      0.80       102\n",
            "            Consultants       0.65      0.64      0.65       120\n",
            "           Data_Formats       0.78      0.79      0.78        98\n",
            "    Data_Communications       0.65      0.77      0.70        92\n",
            "              Education       0.81      0.90      0.86        92\n",
            "               Graphics       0.90      0.98      0.94       111\n",
            "               Hardware       0.74      0.64      0.68       105\n",
            "               Internet       0.66      0.66      0.66       102\n",
            "       Mobile_Computing       0.88      0.77      0.82       112\n",
            "             Multimedia       0.73      0.71      0.72       106\n",
            "            Open_Source       0.69      0.82      0.75        96\n",
            "            Programming       0.55      0.48      0.51        88\n",
            "               Robotics       0.89      0.89      0.89       104\n",
            "               Security       0.79      0.78      0.78        89\n",
            "               Software       0.29      0.37      0.33        94\n",
            "                Systems       0.52      0.47      0.49        88\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.71      0.71      0.71      1900\n",
            "           weighted avg       0.72      0.71      0.71      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9497, Train: 0.3920, Test: 0.3958\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8142, Train: 0.5561, Test: 0.5553\n",
            "Early stopping:  0.095803158413143\n",
            "Epoch: 003, Loss: 2.6451, Train: 0.5791, Test: 0.5958\n",
            "Early stopping:  0.15257526519635942\n",
            "Epoch: 004, Loss: 2.4534, Train: 0.5903, Test: 0.5968\n",
            "Early stopping:  0.21465660721693972\n",
            "Epoch: 005, Loss: 2.2535, Train: 0.6075, Test: 0.6132\n",
            "Early stopping:  0.2779759846966889\n",
            "Epoch: 006, Loss: 2.0497, Train: 0.6205, Test: 0.6289\n",
            "Early stopping:  0.303866336801959\n",
            "Epoch: 007, Loss: 1.8517, Train: 0.6304, Test: 0.6342\n",
            "Early stopping:  0.31473004133375937\n",
            "Epoch: 008, Loss: 1.6716, Train: 0.6380, Test: 0.6458\n",
            "Early stopping:  0.310808814047786\n",
            "Epoch: 009, Loss: 1.5195, Train: 0.6428, Test: 0.6489\n",
            "Early stopping:  0.29234792323143466\n",
            "Epoch: 010, Loss: 1.3979, Train: 0.6507, Test: 0.6463\n",
            "Early stopping:  0.25979823044957684\n",
            "Epoch: 011, Loss: 1.3040, Train: 0.6549, Test: 0.6516\n",
            "Early stopping:  0.21817984968653834\n",
            "Epoch: 012, Loss: 1.2360, Train: 0.6613, Test: 0.6563\n",
            "Early stopping:  0.1738045814514877\n",
            "Epoch: 013, Loss: 1.1851, Train: 0.6672, Test: 0.6679\n",
            "Early stopping:  0.13323369184123923\n",
            "Epoch: 014, Loss: 1.1482, Train: 0.6761, Test: 0.6674\n",
            "Early stopping:  0.0993148246814409\n",
            "Epoch: 015, Loss: 1.1197, Train: 0.6803, Test: 0.6663\n",
            "Early stopping:  0.0732356339266611\n",
            "Epoch: 016, Loss: 1.0960, Train: 0.6845, Test: 0.6711\n",
            "Early stopping:  0.05527564393627781\n",
            "Epoch: 017, Loss: 1.0744, Train: 0.6889, Test: 0.6763\n",
            "Early stopping:  0.04353874667124478\n",
            "Epoch: 018, Loss: 1.0534, Train: 0.6938, Test: 0.6821\n",
            "Early stopping:  0.037236423386699136\n",
            "Epoch: 019, Loss: 1.0321, Train: 0.7012, Test: 0.6816\n",
            "Early stopping:  0.0344627080782938\n",
            "Epoch: 020, Loss: 1.0103, Train: 0.7068, Test: 0.6858\n",
            "Early stopping:  0.033794323265612186\n",
            "Epoch: 021, Loss: 0.9886, Train: 0.7136, Test: 0.6889\n",
            "Early stopping:  0.033959294029138394\n",
            "Epoch: 022, Loss: 0.9676, Train: 0.7179, Test: 0.6905\n",
            "Early stopping:  0.03401632611742777\n",
            "Epoch: 023, Loss: 0.9480, Train: 0.7225, Test: 0.6921\n",
            "Early stopping:  0.03334957553751022\n",
            "Epoch: 024, Loss: 0.9295, Train: 0.7278, Test: 0.6953\n",
            "Early stopping:  0.03197591520319335\n",
            "Epoch: 025, Loss: 0.9129, Train: 0.7333, Test: 0.6989\n",
            "Early stopping:  0.02998255086781049\n",
            "Epoch: 026, Loss: 0.8975, Train: 0.7380, Test: 0.7026\n",
            "Early stopping:  0.02774340281859115\n",
            "Epoch: 027, Loss: 0.8836, Train: 0.7439, Test: 0.7037\n",
            "Early stopping:  0.025467880510651842\n",
            "Epoch: 028, Loss: 0.8705, Train: 0.7500, Test: 0.7089\n",
            "Early stopping:  0.023332172771357983\n",
            "Epoch: 029, Loss: 0.8585, Train: 0.7530, Test: 0.7147\n",
            "Early stopping:  0.02151262613618836\n",
            "Epoch: 030, Loss: 0.8471, Train: 0.7578, Test: 0.7153\n",
            "Early stopping:  0.019932394041317875\n",
            "Epoch: 031, Loss: 0.8364, Train: 0.7607, Test: 0.7163\n",
            "Early stopping:  0.018636561347139147\n",
            "Epoch: 032, Loss: 0.8262, Train: 0.7651, Test: 0.7195\n",
            "Early stopping:  0.017505081433656602\n",
            "Epoch: 033, Loss: 0.8166, Train: 0.7684, Test: 0.7205\n",
            "Early stopping:  0.016555599064114468\n",
            "Epoch: 034, Loss: 0.8074, Train: 0.7701, Test: 0.7237\n",
            "Early stopping:  0.015678114772287786\n",
            "Epoch: 035, Loss: 0.7987, Train: 0.7733, Test: 0.7242\n",
            "Early stopping:  0.014883176915054622\n",
            "Epoch: 036, Loss: 0.7902, Train: 0.7772, Test: 0.7247\n",
            "Early stopping:  0.014209709999408807\n",
            "Epoch: 037, Loss: 0.7818, Train: 0.7800, Test: 0.7232\n",
            "Early stopping:  0.013740950332011772\n",
            "Epoch: 038, Loss: 0.7733, Train: 0.7822, Test: 0.7242\n",
            "Early stopping:  0.013458861700237237\n",
            "Epoch: 039, Loss: 0.7651, Train: 0.7849, Test: 0.7237\n",
            "Early stopping:  0.01330604821305812\n",
            "Epoch: 040, Loss: 0.7568, Train: 0.7892, Test: 0.7237\n",
            "Early stopping:  0.01318764585596929\n",
            "Epoch: 041, Loss: 0.7488, Train: 0.7917, Test: 0.7247\n",
            "Early stopping:  0.01302865386981179\n",
            "Epoch: 042, Loss: 0.7409, Train: 0.7946, Test: 0.7274\n",
            "Early stopping:  0.012830286153819613\n",
            "Epoch: 043, Loss: 0.7329, Train: 0.7964, Test: 0.7237\n",
            "Early stopping:  0.01268040081525156\n",
            "Epoch: 044, Loss: 0.7250, Train: 0.7986, Test: 0.7247\n",
            "Early stopping:  0.012591038391407615\n",
            "Epoch: 045, Loss: 0.7169, Train: 0.8016, Test: 0.7268\n",
            "Early stopping:  0.01261574645640622\n",
            "Epoch: 046, Loss: 0.7087, Train: 0.8014, Test: 0.7263\n",
            "Early stopping:  0.012700942657594144\n",
            "Epoch: 047, Loss: 0.7006, Train: 0.8063, Test: 0.7295\n",
            "Early stopping:  0.012781435405599046\n",
            "Epoch: 048, Loss: 0.6927, Train: 0.8034, Test: 0.7247\n",
            "Early stopping:  0.012787370795882652\n",
            "Epoch: 049, Loss: 0.6850, Train: 0.8092, Test: 0.7289\n",
            "Early stopping:  0.012640618590891795\n",
            "Epoch: 050, Loss: 0.6775, Train: 0.8103, Test: 0.7226\n",
            "Early stopping:  0.012352272259163287\n",
            "Epoch: 051, Loss: 0.6699, Train: 0.8151, Test: 0.7279\n",
            "Early stopping:  0.012105344872300335\n",
            "Epoch: 052, Loss: 0.6614, Train: 0.8158, Test: 0.7263\n",
            "Early stopping:  0.012261472987930793\n",
            "Epoch: 053, Loss: 0.6532, Train: 0.8199, Test: 0.7263\n",
            "Early stopping:  0.012609311410840702\n",
            "Epoch: 054, Loss: 0.6457, Train: 0.8226, Test: 0.7279\n",
            "Early stopping:  0.012704210399839799\n",
            "Epoch: 055, Loss: 0.6384, Train: 0.8245, Test: 0.7253\n",
            "Early stopping:  0.012442564974536361\n",
            "Epoch: 056, Loss: 0.6308, Train: 0.8262, Test: 0.7237\n",
            "Early stopping:  0.012013252517118921\n",
            "Epoch: 057, Loss: 0.6230, Train: 0.8287, Test: 0.7242\n",
            "Early stopping:  0.011901157949223235\n",
            "Epoch: 058, Loss: 0.6155, Train: 0.8307, Test: 0.7221\n",
            "Early stopping:  0.012017473726583515\n",
            "Epoch: 059, Loss: 0.6084, Train: 0.8329, Test: 0.7237\n",
            "Early stopping:  0.011914011430683576\n",
            "Epoch: 060, Loss: 0.6010, Train: 0.8354, Test: 0.7258\n",
            "Early stopping:  0.011714730787576315\n",
            "Epoch: 061, Loss: 0.5934, Train: 0.8378, Test: 0.7232\n",
            "Early stopping:  0.011639480189674416\n",
            "Epoch: 062, Loss: 0.5856, Train: 0.8416, Test: 0.7232\n",
            "Early stopping:  0.011817766262363277\n",
            "Epoch: 063, Loss: 0.5783, Train: 0.8429, Test: 0.7247\n",
            "Early stopping:  0.01196602551595796\n",
            "Epoch: 064, Loss: 0.5714, Train: 0.8464, Test: 0.7216\n",
            "Early stopping:  0.011754260233059883\n",
            "Epoch: 065, Loss: 0.5649, Train: 0.8475, Test: 0.7258\n",
            "Early stopping:  0.011272891493123632\n",
            "Epoch: 066, Loss: 0.5586, Train: 0.8508, Test: 0.7226\n",
            "Early stopping:  0.010682620909272432\n",
            "Epoch: 067, Loss: 0.5519, Train: 0.8530, Test: 0.7226\n",
            "Early stopping:  0.010397348488401981\n",
            "Epoch: 068, Loss: 0.5447, Train: 0.8541, Test: 0.7232\n",
            "Early stopping:  0.010513182308802103\n",
            "Epoch: 069, Loss: 0.5381, Train: 0.8566, Test: 0.7174\n",
            "Early stopping:  0.010660105754729643\n",
            "Epoch: 070, Loss: 0.5324, Train: 0.8553, Test: 0.7221\n",
            "Early stopping:  0.010467893801546425\n",
            "Epoch: 071, Loss: 0.5274, Train: 0.8564, Test: 0.7205\n",
            "Early stopping:  0.00971761357740619\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.83      0.88      0.86        85\n",
            "            CAD_and_CAM       0.75      0.72      0.73       100\n",
            "              Companies       0.59      0.45      0.51       104\n",
            "       Computer_Science       0.79      0.76      0.78        89\n",
            "            Consultants       0.66      0.64      0.65       111\n",
            "           Data_Formats       0.84      0.84      0.84       103\n",
            "    Data_Communications       0.77      0.73      0.75       112\n",
            "              Education       0.88      0.89      0.88       115\n",
            "               Graphics       0.84      0.90      0.87       103\n",
            "               Hardware       0.68      0.82      0.74       105\n",
            "               Internet       0.58      0.70      0.64        77\n",
            "       Mobile_Computing       0.80      0.81      0.81       101\n",
            "             Multimedia       0.71      0.79      0.75        92\n",
            "            Open_Source       0.76      0.56      0.64       113\n",
            "            Programming       0.53      0.59      0.56       106\n",
            "               Robotics       0.90      0.94      0.92        93\n",
            "               Security       0.78      0.79      0.79       107\n",
            "               Software       0.29      0.26      0.27        92\n",
            "                Systems       0.62      0.60      0.61        92\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.72      0.72      0.72      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "time: 36.3 s (started: 2024-08-17 14:17:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d673e761-f977-41d1-d978-fc62a39f847c",
        "id": "Sr9iM6wbtuWi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 515 ms (started: 2024-08-17 14:18:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "# Hetrogenous Graph for Keyphrase = 2 or 3\n",
        "--------------------------------------"
      ],
      "metadata": {
        "id": "YXXZu4w_dcAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"class\"].unique())\n",
        "class_number = len(df[\"class\"].unique())\n",
        "print(class_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbce0c1-4e0c-450f-8958-4b5df14da82f",
        "id": "_l4qyKl5dcBB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data_management' 'ecology' 'economic' 'geophysics' 'gravitional_theory'\n",
            " 'hydro' 'math' 'metals' 'networking' 'neuroscience' 'oceanography'\n",
            " 'politic' 'sociology' 'software_engineering' 'statistics'\n",
            " 'theory_computing']\n",
            "16\n",
            "time: 9 ms (started: 2024-08-16 14:14:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change here to change wich keypharse to use\n",
        "keyphrase = \"keyphrase23\"\n",
        "\n",
        "model_name = dataset_name+\"_\"+keyphrase"
      ],
      "metadata": {
        "id": "Uig08WAHdcBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f395a7f-3ca2-4ee8-c599-40565c14f059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 453 µs (started: 2024-08-16 14:14:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Graph"
      ],
      "metadata": {
        "id": "Oywnn6k1ezSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Graph Nodes and Edges 👀\n",
        "\n",
        "- `Nodes` - documents and contexts\n",
        "- `Edges`\n",
        "  - document <- has -> context\n",
        "- `Labels` - documents classes\n"
      ],
      "metadata": {
        "id": "5gvup-2jezSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Nodes and Edges 👀"
      ],
      "metadata": {
        "id": "7bwipeK4ezSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Defining Docmuent nodes, Context nodes and edges between them"
      ],
      "metadata": {
        "id": "tUbeMm8SezSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_contexts_list =[]\n",
        "edges1,edges2 = [],[]\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes = []\n",
        "context_nodes = []\n",
        "\n",
        "edges_tuple = []\n",
        "\n",
        "sentences = []\n",
        "cont_sentences = 0\n",
        "dit_sentences = {}\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding document nodes for every doc in df\n",
        "    document_nodes.append(df[\"text_embeddings\"][i])\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list: # if NOT\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes.append(df[keyphrase+\"_embeddings\"][i][j])\n",
        "            # add a new edge between doc and new context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(new_edge_cont)\n",
        "            edges_tuple.append((df[\"id\"][i],new_edge_cont))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(all_contexts_list.index(key[0]))\n",
        "            edges_tuple.append((df[\"id\"][i],all_contexts_list.index(key[0])))\n",
        "            cont+=1\n",
        "\n",
        "    # organize sentences, sentences_embeddings, and a dict with the corresponding document for each sentence\n",
        "    aux = df['sentences_embeddings'][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        sentences.append(key)\n",
        "        dit_sentences[cont_sentences] = df[\"id\"][i]\n",
        "        cont_sentences += 1\n",
        "\n",
        "\n",
        "document_nodes = np.array(document_nodes)\n",
        "context_nodes = np.array(context_nodes)"
      ],
      "metadata": {
        "id": "P5GOHYWPezSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca772fa3-c466-44be-bfd4-15584ccf6adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21.6 s (started: 2024-08-16 14:14:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of document nodes:\",len(document_nodes))\n",
        "print(\"number of context nodes:\",len(context_nodes))\n",
        "print(\"number of shared contexts:\",cont)\n",
        "print(\"number of direct edges (first dimension):\",len(edges1))\n",
        "print(\"number of direct edges (second dimension):\",len(edges2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGzYkYUTezSn",
        "outputId": "2785b1dd-0f8b-413d-d655-bda2882b84e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 10524\n",
            "number of context nodes: 44786\n",
            "number of shared contexts: 6772\n",
            "number of direct edges (first dimension): 51558\n",
            "number of direct edges (second dimension): 51558\n",
            "time: 4.75 ms (started: 2024-08-16 14:15:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=15, metric='cosine').fit(context_nodes)\n",
        "\n",
        "neighbors_list = nbrs.kneighbors(sentences, return_distance=False)\n",
        "\n",
        "# cria aresta para cada vizinho encontrado\n",
        "for i,neighbors in enumerate(neighbors_list):\n",
        "        for n in neighbors:\n",
        "            edges1.append(dit_sentences[i])\n",
        "            edges2.append(n)\n",
        "            edges_tuple.append((dit_sentences[i],n))"
      ],
      "metadata": {
        "id": "hlsR62oUezSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2106aa6e-c864-46de-9acf-d4c5b824e1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.5 s (started: 2024-08-16 14:15:02 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ajusting everything to Tensor Objects"
      ],
      "metadata": {
        "id": "WmkD7gXqezSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms egdes to tensor\n",
        "edges = np.array([edges1,edges2])\n",
        "edges = torch.tensor(edges)"
      ],
      "metadata": {
        "id": "6hAGiWQzezSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c5e6e1-d296-4b5c-cc73-eb2cbd21acf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 24.8 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms nodes to tensor\n",
        "document_nodes = np.array(document_nodes)\n",
        "document_nodes = torch.tensor(document_nodes)\n",
        "context_nodes = np.array(context_nodes)\n",
        "context_nodes = torch.tensor(context_nodes)"
      ],
      "metadata": {
        "id": "jUrNmaRLezSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770c8080-4cbe-4444-c39b-3e6401dedd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 77.1 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show documents nodes\n",
        "document_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dktQZftFezSp",
        "outputId": "298c78b4-1a92-4fd4-fbbc-d3a5e416a94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0480, -0.0557,  0.0054,  ..., -0.0087,  0.0842, -0.0266],\n",
              "        [ 0.0155, -0.0795,  0.0167,  ..., -0.0797, -0.0875,  0.0055],\n",
              "        [-0.0026, -0.0006, -0.0163,  ..., -0.0748,  0.0273, -0.0155],\n",
              "        ...,\n",
              "        [-0.0508,  0.0383, -0.0004,  ...,  0.0265, -0.0420, -0.0559],\n",
              "        [-0.0322, -0.0649,  0.0155,  ...,  0.1090,  0.0247, -0.0226],\n",
              "        [ 0.0748,  0.0328,  0.0163,  ..., -0.0506, -0.0447,  0.0613]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.23 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of document nodes\n",
        "len(document_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhiFco_zezSq",
        "outputId": "a2cd112e-e40e-4aaa-c0ca-2f707672c061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10524"
            ]
          },
          "metadata": {},
          "execution_count": 139
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.4 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show context nodes\n",
        "context_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDakiiWbezSq",
        "outputId": "3e9ce25c-b2c1-4c34-b7d2-a99611f26730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0531, -0.0962,  0.0218,  ..., -0.0255,  0.0040, -0.0138],\n",
              "        [ 0.0517, -0.0748,  0.0175,  ...,  0.0007,  0.0347, -0.0148],\n",
              "        [ 0.0352,  0.0249,  0.0602,  ...,  0.1391,  0.0714, -0.0247],\n",
              "        ...,\n",
              "        [ 0.0433,  0.0305, -0.0150,  ..., -0.0104, -0.0447,  0.0464],\n",
              "        [ 0.0851,  0.0426, -0.0437,  ..., -0.0166, -0.0241, -0.0064],\n",
              "        [ 0.0135,  0.0573,  0.0189,  ..., -0.0285,  0.0146,  0.0765]])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.64 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of context nodes\n",
        "len(context_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW_aQW0JezSq",
        "outputId": "496af58f-25aa-4d8d-f8fe-50387711d121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44786"
            ]
          },
          "metadata": {},
          "execution_count": 141
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.95 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantity of edges\n",
        "print(len(edges[0]))\n",
        "print(len(edges[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfBtcgy3ezSr",
        "outputId": "199574b4-ae72-426d-de06-95db400ac350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210813\n",
            "210813\n",
            "time: 564 µs (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing edges\n",
        "print(edges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXz-gsYvezSr",
        "outputId": "af3d99d4-d9f4-43e7-bd17-0b8db49c591e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ..., 10523, 10523, 10523],\n",
            "        [    0,     1,     2,  ..., 43530, 43845, 44327]])\n",
            "time: 2.21 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class Labels"
      ],
      "metadata": {
        "id": "fPZtNmU3ezSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All different classes\n",
        "print(df[\"class\"].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt1SF8dRezSs",
        "outputId": "9ef220d3-1206-45e4-99ca-7b789d1b5f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data_management' 'ecology' 'economic' 'geophysics' 'gravitional_theory'\n",
            " 'hydro' 'math' 'metals' 'networking' 'neuroscience' 'oceanography'\n",
            " 'politic' 'sociology' 'software_engineering' 'statistics'\n",
            " 'theory_computing']\n",
            "time: 1.37 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating labels for classification\n",
        "# the dictionary is a numeric representation for each possible \"document\" class\n",
        "\n",
        "dit = {}\n",
        "for i,classe in enumerate(df[\"class\"].unique()):\n",
        "  dit[classe] = i\n",
        "\n",
        "print(dit,'\\n')\n",
        "\n",
        "labels = df[[\"class\"]]\n",
        "for i in range(len(df[[\"class\"]])):\n",
        "    labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0TJ12U1ezSs",
        "outputId": "d948409d-1d49-4fe4-c277-d333481bf5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data_management': 0, 'ecology': 1, 'economic': 2, 'geophysics': 3, 'gravitional_theory': 4, 'hydro': 5, 'math': 6, 'metals': 7, 'networking': 8, 'neuroscience': 9, 'oceanography': 10, 'politic': 11, 'sociology': 12, 'software_engineering': 13, 'statistics': 14, 'theory_computing': 15} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-145-a018e46c97dd>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class\n",
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62121eb1-eb14-46eb-822c-8563d5891c62\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62121eb1-eb14-46eb-822c-8563d5891c62')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62121eb1-eb14-46eb-822c-8563d5891c62 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62121eb1-eb14-46eb-822c-8563d5891c62');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1df3e136-7763-4918-ba19-595aeafd1b92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1df3e136-7763-4918-ba19-595aeafd1b92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1df3e136-7763-4918-ba19-595aeafd1b92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels",
              "summary": "{\n  \"name\": \"labels\",\n  \"rows\": 10524,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 145
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 974 ms (started: 2024-08-16 14:15:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tranfors class dataframe into tensor\n",
        "y = labels[\"class\"].tolist()\n",
        "y = x_np = torch.tensor(y)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK0Fa3e8ezSt",
        "outputId": "28b3eec3-fec5-47a6-ea26-dc8b10221540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  ..., 15, 15, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.19 ms (started: 2024-08-16 14:15:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWNve9W7ezSt",
        "outputId": "aed24cc9-fafe-4ff6-9150-b3c23459b51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10524"
            ]
          },
          "metadata": {},
          "execution_count": 147
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.85 ms (started: 2024-08-16 14:15:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Graph with Networkx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OzuE-CP9ezSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining overal graph in networkx maner"
      ],
      "metadata": {
        "id": "IhPvsB_-ezSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run graph Representation for networkx\n",
        "all_contexts_list =[]\n",
        "edges_test = []\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes_test = []\n",
        "context_nodes_test = []\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding new documents for every node\n",
        "    document_nodes_test.append(\"doc_\"+str(i)) # in the actual graph nodes -> documents embeddings\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list:\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes_test.append(\"contx_\"+str(new_edge_cont)) # in the actual graph nodes -> context embeddings\n",
        "\n",
        "            # add a new edge between doc and new context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(new_edge_cont)))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(all_contexts_list.index(key[0]))))\n",
        "            cont+=1"
      ],
      "metadata": {
        "id": "W5BG-1ZWezSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e50f494-eedd-49eb-9c74-99a85cee4314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18 s (started: 2024-08-16 14:15:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_test = [(\"doc_\"+str(i[0]),\"contx_\"+str(i[1])) for i in edges_tuple]"
      ],
      "metadata": {
        "id": "WYwDFDqtezSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c533a9-f42d-460a-de51-a49277b544ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 133 ms (started: 2024-08-16 14:15:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of document nodes:\",len(document_nodes_test))\n",
        "print(\"number of context nodes:\",len(context_nodes_test))\n",
        "print(\"number of edges:\",len(edges_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsJjDZi6ezSv",
        "outputId": "4533c0b7-d356-4ce1-beb3-15c342230d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 10524\n",
            "number of context nodes: 44786\n",
            "number of edges: 210813\n",
            "time: 1.1 ms (started: 2024-08-16 14:15:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(edges_test[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trPJcUUIezSv",
        "outputId": "1d72149b-c4eb-4348-c58d-8c4d45070b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('doc_0', 'contx_0'), ('doc_0', 'contx_1'), ('doc_0', 'contx_2'), ('doc_0', 'contx_3'), ('doc_0', 'contx_4'), ('doc_1', 'contx_5'), ('doc_1', 'contx_6'), ('doc_1', 'contx_7'), ('doc_1', 'contx_8'), ('doc_1', 'contx_9'), ('doc_2', 'contx_10'), ('doc_2', 'contx_11'), ('doc_2', 'contx_12'), ('doc_2', 'contx_13'), ('doc_2', 'contx_14'), ('doc_3', 'contx_15'), ('doc_3', 'contx_16'), ('doc_3', 'contx_17'), ('doc_3', 'contx_18'), ('doc_3', 'contx_19'), ('doc_4', 'contx_20'), ('doc_4', 'contx_21'), ('doc_4', 'contx_22'), ('doc_4', 'contx_23'), ('doc_4', 'contx_24'), ('doc_5', 'contx_25'), ('doc_5', 'contx_26'), ('doc_5', 'contx_27'), ('doc_5', 'contx_28'), ('doc_5', 'contx_29'), ('doc_6', 'contx_30'), ('doc_6', 'contx_31'), ('doc_6', 'contx_32'), ('doc_6', 'contx_33'), ('doc_6', 'contx_34'), ('doc_7', 'contx_35'), ('doc_7', 'contx_36'), ('doc_7', 'contx_37'), ('doc_7', 'contx_38'), ('doc_7', 'contx_39'), ('doc_8', 'contx_40'), ('doc_8', 'contx_41'), ('doc_8', 'contx_42'), ('doc_8', 'contx_43'), ('doc_8', 'contx_44'), ('doc_9', 'contx_45'), ('doc_9', 'contx_46'), ('doc_9', 'contx_13'), ('doc_10', 'contx_47'), ('doc_10', 'contx_48'), ('doc_10', 'contx_49'), ('doc_10', 'contx_50'), ('doc_10', 'contx_51'), ('doc_11', 'contx_52'), ('doc_11', 'contx_53'), ('doc_11', 'contx_54'), ('doc_11', 'contx_55'), ('doc_11', 'contx_56'), ('doc_12', 'contx_57'), ('doc_12', 'contx_58'), ('doc_12', 'contx_59'), ('doc_12', 'contx_60'), ('doc_12', 'contx_61'), ('doc_13', 'contx_62'), ('doc_13', 'contx_63'), ('doc_13', 'contx_64'), ('doc_13', 'contx_65'), ('doc_13', 'contx_66'), ('doc_14', 'contx_67'), ('doc_14', 'contx_68'), ('doc_14', 'contx_69'), ('doc_14', 'contx_70'), ('doc_14', 'contx_71'), ('doc_15', 'contx_72'), ('doc_15', 'contx_73'), ('doc_15', 'contx_74'), ('doc_16', 'contx_75'), ('doc_16', 'contx_76'), ('doc_16', 'contx_77'), ('doc_16', 'contx_78'), ('doc_16', 'contx_79'), ('doc_17', 'contx_80'), ('doc_17', 'contx_81'), ('doc_17', 'contx_82'), ('doc_17', 'contx_83'), ('doc_17', 'contx_84'), ('doc_18', 'contx_85'), ('doc_18', 'contx_86'), ('doc_18', 'contx_87'), ('doc_18', 'contx_88'), ('doc_18', 'contx_89'), ('doc_19', 'contx_90'), ('doc_19', 'contx_91'), ('doc_19', 'contx_92'), ('doc_19', 'contx_93'), ('doc_19', 'contx_94'), ('doc_20', 'contx_95'), ('doc_20', 'contx_96'), ('doc_20', 'contx_97'), ('doc_20', 'contx_98')]\n",
            "time: 393 µs (started: 2024-08-16 14:15:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test graph Conectivity with networkx"
      ],
      "metadata": {
        "id": "Bvu3pgTlezSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Bipartide graph\n",
        "B = nx.Graph()\n",
        "B.add_nodes_from(document_nodes_test, bipartite=0)\n",
        "B.add_nodes_from(context_nodes_test, bipartite=1)\n",
        "B.add_edges_from(edges_test)"
      ],
      "metadata": {
        "id": "QQGjtv2vezSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07d9641-9736-49fc-ee9a-a37ca14b18a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 400 ms (started: 2024-08-16 14:15:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.is_connected(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGT1IC92ezSx",
        "outputId": "84802007-792a-4f30-a055-395a10a9e269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 153
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 111 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of conected elements\n",
        "# if == 1 -> all elements of the graph are conected\n",
        "print(nx.number_connected_components(B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhuaVnMTezSx",
        "outputId": "59064ac8-c322-4a33-ba7e-a0e58d1c5640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "time: 144 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size of a cluster\n",
        "print(len(nx.node_connected_component(B,'doc_0')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_xYJY5iezSx",
        "outputId": "794531ca-c8fd-455f-abdd-185eb0132cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55310\n",
            "time: 92.3 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Graph HeteroData Object"
      ],
      "metadata": {
        "id": "psEyYySbezSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining nodes, edges and class labels\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# nodes\n",
        "data['document'].x = document_nodes\n",
        "data['concept'].x = context_nodes\n",
        "\n",
        "# edges\n",
        "data['document', 'has', 'concept'].edge_index = edges\n",
        "\n",
        "#class labels\n",
        "data['document'].y = y\n"
      ],
      "metadata": {
        "id": "YdfHMacMezSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748295c0-67f1-4123-f342-6ad2c64c2cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.12 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting graph to undirected\n",
        "data = T.ToUndirected()(data)"
      ],
      "metadata": {
        "id": "Mz5-5BRBezSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f95c3c-2784-4a72-a07d-59832a9c213f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.58 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicate edges\n",
        "data = T.RemoveDuplicatedEdges()(data)"
      ],
      "metadata": {
        "id": "Kzsvd0YwezSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed82f41-32ce-4e9d-cdab-a5691e0c42ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 41.2 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date in using gpu\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "JmgI63i8ezSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5db5a19-d044-4afe-a6ba-c89f34f872aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 20.8 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8ypz15KezSz",
        "outputId": "0e885347-58b9-4d0e-a88d-e5f22f73e6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  document={\n",
              "    x=[10524, 384],\n",
              "    y=[10524],\n",
              "  },\n",
              "  concept={ x=[44786, 384] },\n",
              "  (document, has, concept)={ edge_index=[2, 165253] },\n",
              "  (concept, rev_has, document)={ edge_index=[2, 165253] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 160
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.99 ms (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "1MyJeUqJKlm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 1 ❎"
      ],
      "metadata": {
        "id": "JjRru0uaKlm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "Ix5tfCQMKlm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgnRMhMwKlm5",
        "outputId": "00a97683-f9ad-46c5-de95-3f08da4235f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7656, Train: 0.9375, Test: 0.4432\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.3733, Train: 0.9375, Test: 0.5634\n",
            "Early stopping:  1.6916215483140404\n",
            "Epoch: 003, Loss: 0.1518, Train: 1.0000, Test: 0.6053\n",
            "Early stopping:  1.4493726572604395\n",
            "Epoch: 004, Loss: 0.0026, Train: 0.9375, Test: 0.5908\n",
            "Early stopping:  1.3037733709508545\n",
            "Epoch: 005, Loss: 0.1747, Train: 1.0000, Test: 0.5805\n",
            "Early stopping:  1.1657732102720948\n",
            "Epoch: 006, Loss: 0.0017, Train: 1.0000, Test: 0.5763\n",
            "Early stopping:  0.1531017888688253\n",
            "Epoch: 007, Loss: 0.0001, Train: 1.0000, Test: 0.5694\n",
            "Early stopping:  0.08897229387521467\n",
            "Epoch: 008, Loss: 0.0002, Train: 1.0000, Test: 0.5629\n",
            "Early stopping:  0.07760282685470796\n",
            "Epoch: 009, Loss: 0.0002, Train: 1.0000, Test: 0.5602\n",
            "Early stopping:  0.07787025130847951\n",
            "Epoch: 010, Loss: 0.0002, Train: 1.0000, Test: 0.5586\n",
            "Early stopping:  0.0006975922754950241\n",
            "PREDICTIONS -> tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.18      0.96      0.31       401\n",
            "             ecology       0.59      0.30      0.40       888\n",
            "            economic       0.56      0.64      0.60      1408\n",
            "          geophysics       0.96      0.81      0.88      1201\n",
            "  gravitional_theory       0.67      0.84      0.74       129\n",
            "               hydro       0.27      0.61      0.38       354\n",
            "                math       0.75      0.75      0.75      1338\n",
            "              metals       0.52      0.94      0.67       200\n",
            "          networking       0.87      0.80      0.83       344\n",
            "        neuroscience       0.96      0.92      0.94       306\n",
            "        oceanography       0.59      0.71      0.65       989\n",
            "             politic       0.84      0.28      0.42       602\n",
            "           sociology       0.52      0.38      0.44       738\n",
            "software_engineering       0.93      0.12      0.22       523\n",
            "          statistics       0.48      0.02      0.05       646\n",
            "    theory_computing       0.98      0.10      0.18       441\n",
            "\n",
            "            accuracy                           0.56     10508\n",
            "           macro avg       0.67      0.57      0.53     10508\n",
            "        weighted avg       0.68      0.56      0.55     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0999, Train: 0.9375, Test: 0.4893\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2843, Train: 1.0000, Test: 0.5145\n",
            "Early stopping:  1.9909534608399584\n",
            "Epoch: 003, Loss: 0.0116, Train: 1.0000, Test: 0.5204\n",
            "Early stopping:  1.7097779173935599\n",
            "Epoch: 004, Loss: 0.0032, Train: 1.0000, Test: 0.5335\n",
            "Early stopping:  1.505784935914694\n",
            "Epoch: 005, Loss: 0.0010, Train: 1.0000, Test: 0.5433\n",
            "Early stopping:  1.35816936308679\n",
            "Epoch: 006, Loss: 0.0004, Train: 1.0000, Test: 0.5484\n",
            "Early stopping:  0.1254071143338663\n",
            "Epoch: 007, Loss: 0.0002, Train: 1.0000, Test: 0.5460\n",
            "Early stopping:  0.00479151815103881\n",
            "PREDICTIONS -> tensor([15, 14, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.11      0.04      0.06       401\n",
            "             ecology       0.92      0.45      0.61       888\n",
            "            economic       0.48      0.74      0.59      1408\n",
            "          geophysics       0.88      0.73      0.80      1201\n",
            "  gravitional_theory       0.25      0.95      0.40       129\n",
            "               hydro       0.38      0.66      0.48       354\n",
            "                math       0.98      0.16      0.28      1338\n",
            "              metals       0.29      0.88      0.44       200\n",
            "          networking       0.73      0.71      0.72       344\n",
            "        neuroscience       0.86      0.98      0.92       306\n",
            "        oceanography       0.69      0.91      0.78       989\n",
            "             politic       0.45      0.64      0.53       602\n",
            "           sociology       0.65      0.10      0.17       738\n",
            "software_engineering       0.83      0.49      0.61       523\n",
            "          statistics       0.22      0.17      0.19       646\n",
            "    theory_computing       0.35      0.86      0.50       441\n",
            "\n",
            "            accuracy                           0.55     10508\n",
            "           macro avg       0.57      0.59      0.50     10508\n",
            "        weighted avg       0.65      0.55      0.52     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0583, Train: 1.0000, Test: 0.4847\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2718, Train: 1.0000, Test: 0.5876\n",
            "Early stopping:  1.9702868168929142\n",
            "Epoch: 003, Loss: 0.0120, Train: 1.0000, Test: 0.5977\n",
            "Early stopping:  1.6887472914255346\n",
            "Epoch: 004, Loss: 0.0048, Train: 1.0000, Test: 0.5811\n",
            "Early stopping:  1.4862237410887138\n",
            "Epoch: 005, Loss: 0.0005, Train: 1.0000, Test: 0.5647\n",
            "Early stopping:  1.3403300234919298\n",
            "Epoch: 006, Loss: 0.0002, Train: 1.0000, Test: 0.5488\n",
            "Early stopping:  0.11970904881865525\n",
            "Epoch: 007, Loss: 0.0002, Train: 1.0000, Test: 0.5372\n",
            "Early stopping:  0.005103309245396378\n",
            "PREDICTIONS -> tensor([15, 11, 13,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.83      0.05      0.09       401\n",
            "             ecology       0.70      0.76      0.73       888\n",
            "            economic       0.63      0.53      0.58      1408\n",
            "          geophysics       0.83      0.86      0.84      1201\n",
            "  gravitional_theory       0.93      0.84      0.88       129\n",
            "               hydro       0.13      0.48      0.20       354\n",
            "                math       1.00      0.10      0.18      1338\n",
            "              metals       0.57      0.86      0.69       200\n",
            "          networking       0.87      0.86      0.86       344\n",
            "        neuroscience       0.93      0.97      0.95       306\n",
            "        oceanography       0.72      0.71      0.71       989\n",
            "             politic       0.46      0.49      0.47       602\n",
            "           sociology       0.72      0.16      0.27       738\n",
            "software_engineering       0.68      0.80      0.74       523\n",
            "          statistics       0.32      0.18      0.23       646\n",
            "    theory_computing       0.20      0.80      0.32       441\n",
            "\n",
            "            accuracy                           0.54     10508\n",
            "           macro avg       0.66      0.59      0.55     10508\n",
            "        weighted avg       0.69      0.54      0.52     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1635, Train: 0.9375, Test: 0.4943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2621, Train: 1.0000, Test: 0.4875\n",
            "Early stopping:  2.051623069554693\n",
            "Epoch: 003, Loss: 0.0051, Train: 1.0000, Test: 0.4401\n",
            "Early stopping:  1.7540408931230769\n",
            "Epoch: 004, Loss: 0.0009, Train: 1.0000, Test: 0.4013\n",
            "Early stopping:  1.5419201361595212\n",
            "Epoch: 005, Loss: 0.0007, Train: 1.0000, Test: 0.3796\n",
            "Early stopping:  1.38928139407552\n",
            "Epoch: 006, Loss: 0.0003, Train: 1.0000, Test: 0.3650\n",
            "Early stopping:  0.11643993570838994\n",
            "Epoch: 007, Loss: 0.0001, Train: 1.0000, Test: 0.3541\n",
            "Early stopping:  0.0020820170800874855\n",
            "PREDICTIONS -> tensor([15,  0, 15,  ..., 14, 15, 14], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.15      0.34      0.21       401\n",
            "             ecology       0.56      0.44      0.50       888\n",
            "            economic       0.93      0.10      0.17      1408\n",
            "          geophysics       0.92      0.57      0.70      1201\n",
            "  gravitional_theory       0.06      0.98      0.12       129\n",
            "               hydro       0.48      0.64      0.55       354\n",
            "                math       0.85      0.09      0.16      1338\n",
            "              metals       0.78      0.69      0.74       200\n",
            "          networking       0.73      0.16      0.26       344\n",
            "        neuroscience       0.93      0.97      0.95       306\n",
            "        oceanography       0.32      0.22      0.26       989\n",
            "             politic       0.24      0.02      0.04       602\n",
            "           sociology       0.24      0.89      0.37       738\n",
            "software_engineering       0.90      0.54      0.67       523\n",
            "          statistics       0.31      0.30      0.31       646\n",
            "    theory_computing       0.12      0.10      0.11       441\n",
            "\n",
            "            accuracy                           0.35     10508\n",
            "           macro avg       0.53      0.44      0.38     10508\n",
            "        weighted avg       0.60      0.35      0.35     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5850, Train: 0.9375, Test: 0.4558\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.1984, Train: 1.0000, Test: 0.5374\n",
            "Early stopping:  2.3946302860527524\n",
            "Epoch: 003, Loss: 0.0040, Train: 1.0000, Test: 0.5486\n",
            "Early stopping:  2.013681512968786\n",
            "Epoch: 004, Loss: 0.0002, Train: 1.0000, Test: 0.5417\n",
            "Early stopping:  1.76113171063472\n",
            "Epoch: 005, Loss: 0.0000, Train: 1.0000, Test: 0.5347\n",
            "Early stopping:  1.5828771330861386\n",
            "Epoch: 006, Loss: 0.0000, Train: 1.0000, Test: 0.5289\n",
            "Early stopping:  0.08827899823483785\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.5213\n",
            "Early stopping:  0.0017617100280640245\n",
            "PREDICTIONS -> tensor([0, 0, 0,  ..., 6, 0, 6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.36      0.85      0.51       401\n",
            "             ecology       0.77      0.74      0.75       888\n",
            "            economic       0.53      0.15      0.23      1408\n",
            "          geophysics       0.91      0.76      0.83      1201\n",
            "  gravitional_theory       0.94      0.83      0.88       129\n",
            "               hydro       0.65      0.07      0.12       354\n",
            "                math       0.51      0.75      0.60      1338\n",
            "              metals       0.76      0.73      0.74       200\n",
            "          networking       0.90      0.80      0.85       344\n",
            "        neuroscience       0.93      0.97      0.95       306\n",
            "        oceanography       0.57      0.85      0.68       989\n",
            "             politic       0.25      0.52      0.34       602\n",
            "           sociology       0.13      0.07      0.09       738\n",
            "software_engineering       0.84      0.18      0.29       523\n",
            "          statistics       0.44      0.30      0.36       646\n",
            "    theory_computing       0.04      0.07      0.05       441\n",
            "\n",
            "            accuracy                           0.52     10508\n",
            "           macro avg       0.60      0.54      0.52     10508\n",
            "        weighted avg       0.57      0.52      0.50     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3103, Train: 0.9375, Test: 0.4324\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5294, Train: 0.9375, Test: 0.5213\n",
            "Early stopping:  1.966366738512916\n",
            "Epoch: 003, Loss: 0.4774, Train: 1.0000, Test: 0.5863\n",
            "Early stopping:  1.6207532282077317\n",
            "Epoch: 004, Loss: 0.0114, Train: 1.0000, Test: 0.6008\n",
            "Early stopping:  1.5035876990689152\n",
            "Epoch: 005, Loss: 0.0011, Train: 1.0000, Test: 0.6058\n",
            "Early stopping:  1.3889940379651642\n",
            "Epoch: 006, Loss: 0.0270, Train: 1.0000, Test: 0.6064\n",
            "Early stopping:  0.2693159253970321\n",
            "Epoch: 007, Loss: 0.0244, Train: 1.0000, Test: 0.6042\n",
            "Early stopping:  0.2066353616901138\n",
            "Epoch: 008, Loss: 0.0015, Train: 1.0000, Test: 0.6028\n",
            "Early stopping:  0.01224398741055755\n",
            "Epoch: 009, Loss: 0.0001, Train: 1.0000, Test: 0.5994\n",
            "Early stopping:  0.013599389849951791\n",
            "Epoch: 010, Loss: 0.0000, Train: 1.0000, Test: 0.5960\n",
            "Early stopping:  0.013804900959425414\n",
            "Epoch: 011, Loss: 0.0000, Train: 1.0000, Test: 0.5942\n",
            "Early stopping:  0.010730350173897434\n",
            "Epoch: 012, Loss: 0.0000, Train: 1.0000, Test: 0.5909\n",
            "Early stopping:  0.0006773576342802018\n",
            "PREDICTIONS -> tensor([13, 13, 13,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.44      0.02      0.03       401\n",
            "             ecology       0.64      0.92      0.76       888\n",
            "            economic       0.71      0.44      0.54      1408\n",
            "          geophysics       0.88      0.82      0.85      1201\n",
            "  gravitional_theory       0.62      0.94      0.75       129\n",
            "               hydro       0.40      0.52      0.45       354\n",
            "                math       0.86      0.57      0.68      1338\n",
            "              metals       0.63      0.46      0.53       200\n",
            "          networking       0.84      0.69      0.76       344\n",
            "        neuroscience       0.95      0.90      0.92       306\n",
            "        oceanography       0.91      0.76      0.83       989\n",
            "             politic       0.34      0.24      0.28       602\n",
            "           sociology       0.24      0.33      0.28       738\n",
            "software_engineering       0.41      0.92      0.57       523\n",
            "          statistics       0.50      0.39      0.44       646\n",
            "    theory_computing       0.24      0.55      0.33       441\n",
            "\n",
            "            accuracy                           0.59     10508\n",
            "           macro avg       0.60      0.59      0.56     10508\n",
            "        weighted avg       0.65      0.59      0.59     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0947, Train: 1.0000, Test: 0.4333\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.2447, Train: 1.0000, Test: 0.5533\n",
            "Early stopping:  2.0152704117263944\n",
            "Epoch: 003, Loss: 0.0042, Train: 1.0000, Test: 0.5607\n",
            "Early stopping:  1.7190852319163878\n",
            "Epoch: 004, Loss: 0.0020, Train: 1.0000, Test: 0.5615\n",
            "Early stopping:  1.5098265770108559\n",
            "Epoch: 005, Loss: 0.0006, Train: 1.0000, Test: 0.5569\n",
            "Early stopping:  1.3599288468803883\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.5543\n",
            "Early stopping:  0.10865826179114137\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.5512\n",
            "Early stopping:  0.0017756519243776767\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.52      0.36      0.43       401\n",
            "             ecology       0.48      0.88      0.62       888\n",
            "            economic       0.56      0.79      0.65      1408\n",
            "          geophysics       0.86      0.76      0.81      1201\n",
            "  gravitional_theory       0.31      0.95      0.47       129\n",
            "               hydro       0.41      0.88      0.56       354\n",
            "                math       0.94      0.16      0.27      1338\n",
            "              metals       0.68      0.89      0.77       200\n",
            "          networking       0.94      0.33      0.49       344\n",
            "        neuroscience       0.95      0.93      0.94       306\n",
            "        oceanography       0.89      0.19      0.31       989\n",
            "             politic       0.71      0.42      0.53       602\n",
            "           sociology       0.38      0.57      0.46       738\n",
            "software_engineering       0.69      0.76      0.72       523\n",
            "          statistics       0.04      0.01      0.01       646\n",
            "    theory_computing       0.32      0.84      0.47       441\n",
            "\n",
            "            accuracy                           0.55     10508\n",
            "           macro avg       0.60      0.61      0.53     10508\n",
            "        weighted avg       0.64      0.55      0.51     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4110, Train: 0.9375, Test: 0.4992\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.1155, Train: 1.0000, Test: 0.5489\n",
            "Early stopping:  2.3302834701725637\n",
            "Epoch: 003, Loss: 0.0014, Train: 1.0000, Test: 0.5260\n",
            "Early stopping:  1.9364408953476402\n",
            "Epoch: 004, Loss: 0.0001, Train: 1.0000, Test: 0.4913\n",
            "Early stopping:  1.6868635375927268\n",
            "Epoch: 005, Loss: 0.0001, Train: 1.0000, Test: 0.4635\n",
            "Early stopping:  1.5131634399079297\n",
            "Epoch: 006, Loss: 0.0004, Train: 1.0000, Test: 0.4518\n",
            "Early stopping:  0.051414999309762836\n",
            "Epoch: 007, Loss: 0.0006, Train: 1.0000, Test: 0.4419\n",
            "Early stopping:  0.0005193889780229807\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.54      0.65       401\n",
            "             ecology       0.37      0.15      0.22       888\n",
            "            economic       0.43      0.01      0.03      1408\n",
            "          geophysics       0.95      0.22      0.36      1201\n",
            "  gravitional_theory       0.83      0.94      0.88       129\n",
            "               hydro       0.53      0.03      0.05       354\n",
            "                math       0.90      0.43      0.58      1338\n",
            "              metals       0.82      0.71      0.76       200\n",
            "          networking       0.82      0.85      0.83       344\n",
            "        neuroscience       0.97      0.92      0.94       306\n",
            "        oceanography       0.23      0.77      0.35       989\n",
            "             politic       0.86      0.47      0.61       602\n",
            "           sociology       0.42      0.76      0.54       738\n",
            "software_engineering       0.30      0.94      0.46       523\n",
            "          statistics       0.32      0.53      0.40       646\n",
            "    theory_computing       0.56      0.33      0.41       441\n",
            "\n",
            "            accuracy                           0.44     10508\n",
            "           macro avg       0.63      0.54      0.50     10508\n",
            "        weighted avg       0.60      0.44      0.41     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2879, Train: 1.0000, Test: 0.4931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.3098, Train: 1.0000, Test: 0.5296\n",
            "Early stopping:  2.105887221306166\n",
            "Epoch: 003, Loss: 0.0121, Train: 1.0000, Test: 0.5044\n",
            "Early stopping:  1.81149504704757\n",
            "Epoch: 004, Loss: 0.0012, Train: 1.0000, Test: 0.4793\n",
            "Early stopping:  1.596545689794886\n",
            "Epoch: 005, Loss: 0.0003, Train: 1.0000, Test: 0.4552\n",
            "Early stopping:  1.4403526651742646\n",
            "Epoch: 006, Loss: 0.0001, Train: 1.0000, Test: 0.4360\n",
            "Early stopping:  0.1371017008748761\n",
            "Epoch: 007, Loss: 0.0001, Train: 1.0000, Test: 0.4232\n",
            "Early stopping:  0.0052617910693435075\n",
            "PREDICTIONS -> tensor([ 0, 13, 13,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.90      0.23      0.37       401\n",
            "             ecology       0.26      0.75      0.39       888\n",
            "            economic       0.85      0.08      0.15      1408\n",
            "          geophysics       0.88      0.02      0.05      1201\n",
            "  gravitional_theory       0.73      0.95      0.82       129\n",
            "               hydro       0.11      0.10      0.11       354\n",
            "                math       0.71      0.64      0.67      1338\n",
            "              metals       0.78      0.36      0.50       200\n",
            "          networking       0.53      0.18      0.27       344\n",
            "        neuroscience       0.99      0.90      0.94       306\n",
            "        oceanography       0.35      0.92      0.51       989\n",
            "             politic       0.58      0.44      0.50       602\n",
            "           sociology       0.08      0.07      0.07       738\n",
            "software_engineering       0.50      0.89      0.64       523\n",
            "          statistics       0.57      0.14      0.23       646\n",
            "    theory_computing       0.43      0.76      0.55       441\n",
            "\n",
            "            accuracy                           0.42     10508\n",
            "           macro avg       0.58      0.47      0.42     10508\n",
            "        weighted avg       0.59      0.42      0.37     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7797, Train: 0.9375, Test: 0.4418\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.3293, Train: 1.0000, Test: 0.4950\n",
            "Early stopping:  2.439774428768923\n",
            "Epoch: 003, Loss: 0.0138, Train: 1.0000, Test: 0.4953\n",
            "Early stopping:  2.0891042784209977\n",
            "Epoch: 004, Loss: 0.0006, Train: 1.0000, Test: 0.4843\n",
            "Early stopping:  1.8388245944229293\n",
            "Epoch: 005, Loss: 0.0001, Train: 1.0000, Test: 0.4753\n",
            "Early stopping:  1.6578505775070016\n",
            "Epoch: 006, Loss: 0.0000, Train: 1.0000, Test: 0.4644\n",
            "Early stopping:  0.14576352616477656\n",
            "Epoch: 007, Loss: 0.0000, Train: 1.0000, Test: 0.4522\n",
            "Early stopping:  0.006105407653584933\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 14, 14, 14], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.55      0.59       401\n",
            "             ecology       0.28      0.46      0.35       888\n",
            "            economic       0.68      0.26      0.38      1408\n",
            "          geophysics       0.94      0.62      0.74      1201\n",
            "  gravitional_theory       0.87      0.88      0.87       129\n",
            "               hydro       0.31      0.74      0.44       354\n",
            "                math       0.95      0.29      0.45      1338\n",
            "              metals       0.36      0.89      0.52       200\n",
            "          networking       0.82      0.83      0.83       344\n",
            "        neuroscience       0.61      0.97      0.75       306\n",
            "        oceanography       0.09      0.04      0.06       989\n",
            "             politic       0.79      0.45      0.57       602\n",
            "           sociology       0.41      0.44      0.43       738\n",
            "software_engineering       0.85      0.20      0.32       523\n",
            "          statistics       0.35      0.88      0.50       646\n",
            "    theory_computing       0.13      0.38      0.20       441\n",
            "\n",
            "            accuracy                           0.45     10508\n",
            "           macro avg       0.57      0.56      0.50     10508\n",
            "        weighted avg       0.59      0.45      0.45     10508\n",
            "\n",
            "time: 3.48 s (started: 2024-08-16 14:15:33 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7HXLkDOKlm5",
        "outputId": "33e0ba08-b750-436a-bf8c-9505fadccc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 442 ms (started: 2024-08-16 14:15:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "EtKhAkE-Klm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqW--ZbcKlm5",
        "outputId": "5cc275fe-83ca-4ac7-9a47-e27b90c240f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7618, Train: 1.0000, Test: 0.4900\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3102, Train: 1.0000, Test: 0.5280\n",
            "Early stopping:  0.31939168400125373\n",
            "Epoch: 003, Loss: 1.8120, Train: 1.0000, Test: 0.5318\n",
            "Early stopping:  0.47509662924031393\n",
            "Epoch: 004, Loss: 1.2791, Train: 1.0000, Test: 0.5379\n",
            "Early stopping:  0.6390080506841475\n",
            "Epoch: 005, Loss: 0.7960, Train: 1.0000, Test: 0.5491\n",
            "Early stopping:  0.7849577588815857\n",
            "Epoch: 006, Loss: 0.4296, Train: 1.0000, Test: 0.5589\n",
            "Early stopping:  0.7568808326612992\n",
            "Epoch: 007, Loss: 0.2032, Train: 1.0000, Test: 0.5695\n",
            "Early stopping:  0.6506006240324268\n",
            "Epoch: 008, Loss: 0.0897, Train: 1.0000, Test: 0.5736\n",
            "Early stopping:  0.48432877697212345\n",
            "Epoch: 009, Loss: 0.0403, Train: 1.0000, Test: 0.5799\n",
            "Early stopping:  0.3094927819544455\n",
            "Epoch: 010, Loss: 0.0192, Train: 1.0000, Test: 0.5817\n",
            "Early stopping:  0.16850720823904503\n",
            "Epoch: 011, Loss: 0.0097, Train: 1.0000, Test: 0.5838\n",
            "Early stopping:  0.07935223933108099\n",
            "Epoch: 012, Loss: 0.0053, Train: 1.0000, Test: 0.5849\n",
            "Early stopping:  0.034520535173584624\n",
            "Epoch: 013, Loss: 0.0031, Train: 1.0000, Test: 0.5860\n",
            "Early stopping:  0.015156307657724266\n",
            "Epoch: 014, Loss: 0.0019, Train: 1.0000, Test: 0.5872\n",
            "Early stopping:  0.0069977981553530945\n",
            "PREDICTIONS -> tensor([ 0,  0, 13,  ...,  0, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.34      0.67      0.45       401\n",
            "             ecology       0.40      0.12      0.18       888\n",
            "            economic       0.67      0.43      0.52      1408\n",
            "          geophysics       0.93      0.83      0.88      1201\n",
            "  gravitional_theory       0.73      0.85      0.79       129\n",
            "               hydro       0.26      0.34      0.30       354\n",
            "                math       0.87      0.56      0.68      1338\n",
            "              metals       0.54      0.93      0.68       200\n",
            "          networking       0.73      0.91      0.81       344\n",
            "        neuroscience       0.90      0.95      0.93       306\n",
            "        oceanography       0.47      0.75      0.58       989\n",
            "             politic       0.61      0.82      0.70       602\n",
            "           sociology       0.43      0.51      0.47       738\n",
            "software_engineering       0.75      0.52      0.61       523\n",
            "          statistics       0.38      0.64      0.48       646\n",
            "    theory_computing       0.63      0.29      0.40       441\n",
            "\n",
            "            accuracy                           0.59     10508\n",
            "           macro avg       0.60      0.63      0.59     10508\n",
            "        weighted avg       0.63      0.59      0.58     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.8030, Train: 0.9375, Test: 0.1979\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3601, Train: 1.0000, Test: 0.4117\n",
            "Early stopping:  0.31316153685437337\n",
            "Epoch: 003, Loss: 1.8715, Train: 1.0000, Test: 0.5078\n",
            "Early stopping:  0.46591038639359245\n",
            "Epoch: 004, Loss: 1.3398, Train: 1.0000, Test: 0.5438\n",
            "Early stopping:  0.6302642150422822\n",
            "Epoch: 005, Loss: 0.8437, Train: 1.0000, Test: 0.5654\n",
            "Early stopping:  0.7812832785749232\n",
            "Epoch: 006, Loss: 0.4559, Train: 1.0000, Test: 0.5688\n",
            "Early stopping:  0.7657014297095042\n",
            "Epoch: 007, Loss: 0.2126, Train: 1.0000, Test: 0.5631\n",
            "Early stopping:  0.6708508783899227\n",
            "Epoch: 008, Loss: 0.0900, Train: 1.0000, Test: 0.5507\n",
            "Early stopping:  0.5091490045945518\n",
            "Epoch: 009, Loss: 0.0376, Train: 1.0000, Test: 0.5382\n",
            "Early stopping:  0.3303562342395066\n",
            "Epoch: 010, Loss: 0.0165, Train: 1.0000, Test: 0.5287\n",
            "Early stopping:  0.18083043754286515\n",
            "Epoch: 011, Loss: 0.0078, Train: 1.0000, Test: 0.5229\n",
            "Early stopping:  0.08440218351418294\n",
            "Epoch: 012, Loss: 0.0040, Train: 1.0000, Test: 0.5166\n",
            "Early stopping:  0.03539772624086779\n",
            "Epoch: 013, Loss: 0.0022, Train: 1.0000, Test: 0.5116\n",
            "Early stopping:  0.014507041753286256\n",
            "Epoch: 014, Loss: 0.0013, Train: 1.0000, Test: 0.5071\n",
            "Early stopping:  0.006195484620898671\n",
            "PREDICTIONS -> tensor([15,  8, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.32      0.16      0.22       401\n",
            "             ecology       0.83      0.66      0.74       888\n",
            "            economic       0.29      0.14      0.19      1408\n",
            "          geophysics       0.90      0.82      0.86      1201\n",
            "  gravitional_theory       0.17      0.97      0.29       129\n",
            "               hydro       0.53      0.70      0.60       354\n",
            "                math       0.68      0.14      0.24      1338\n",
            "              metals       0.27      0.89      0.42       200\n",
            "          networking       0.72      0.68      0.70       344\n",
            "        neuroscience       0.87      0.98      0.92       306\n",
            "        oceanography       0.69      0.89      0.78       989\n",
            "             politic       0.24      0.94      0.38       602\n",
            "           sociology       0.63      0.03      0.06       738\n",
            "software_engineering       0.79      0.55      0.65       523\n",
            "          statistics       0.52      0.18      0.26       646\n",
            "    theory_computing       0.45      0.79      0.57       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.56      0.59      0.49     10508\n",
            "        weighted avg       0.60      0.51      0.48     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7895, Train: 1.0000, Test: 0.3726\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3415, Train: 1.0000, Test: 0.4685\n",
            "Early stopping:  0.31682241211602963\n",
            "Epoch: 003, Loss: 1.8432, Train: 1.0000, Test: 0.5125\n",
            "Early stopping:  0.47335446303414286\n",
            "Epoch: 004, Loss: 1.3094, Train: 1.0000, Test: 0.5333\n",
            "Early stopping:  0.6380550215446502\n",
            "Epoch: 005, Loss: 0.8184, Train: 1.0000, Test: 0.5541\n",
            "Early stopping:  0.786818633591867\n",
            "Epoch: 006, Loss: 0.4410, Train: 1.0000, Test: 0.5740\n",
            "Early stopping:  0.764330514406995\n",
            "Epoch: 007, Loss: 0.2056, Train: 1.0000, Test: 0.5908\n",
            "Early stopping:  0.6622162968020273\n",
            "Epoch: 008, Loss: 0.0868, Train: 1.0000, Test: 0.5986\n",
            "Early stopping:  0.49781886474377846\n",
            "Epoch: 009, Loss: 0.0360, Train: 1.0000, Test: 0.6018\n",
            "Early stopping:  0.3206047067953399\n",
            "Epoch: 010, Loss: 0.0157, Train: 1.0000, Test: 0.6038\n",
            "Early stopping:  0.1750371179145524\n",
            "Epoch: 011, Loss: 0.0074, Train: 1.0000, Test: 0.6028\n",
            "Early stopping:  0.08167812335496454\n",
            "Epoch: 012, Loss: 0.0038, Train: 1.0000, Test: 0.6016\n",
            "Early stopping:  0.03415124318419842\n",
            "Epoch: 013, Loss: 0.0021, Train: 1.0000, Test: 0.5998\n",
            "Early stopping:  0.013879261252763899\n",
            "Epoch: 014, Loss: 0.0013, Train: 1.0000, Test: 0.5986\n",
            "Early stopping:  0.005872674379779762\n",
            "PREDICTIONS -> tensor([ 0, 11, 13,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.54      0.31      0.39       401\n",
            "             ecology       0.73      0.67      0.70       888\n",
            "            economic       0.68      0.54      0.60      1408\n",
            "          geophysics       0.84      0.83      0.84      1201\n",
            "  gravitional_theory       0.54      0.95      0.69       129\n",
            "               hydro       0.14      0.57      0.23       354\n",
            "                math       0.92      0.47      0.62      1338\n",
            "              metals       0.56      0.65      0.60       200\n",
            "          networking       0.63      0.91      0.74       344\n",
            "        neuroscience       0.96      0.92      0.93       306\n",
            "        oceanography       0.71      0.71      0.71       989\n",
            "             politic       0.44      0.75      0.56       602\n",
            "           sociology       0.72      0.21      0.33       738\n",
            "software_engineering       0.69      0.88      0.77       523\n",
            "          statistics       0.38      0.39      0.39       646\n",
            "    theory_computing       0.47      0.26      0.33       441\n",
            "\n",
            "            accuracy                           0.60     10508\n",
            "           macro avg       0.62      0.63      0.59     10508\n",
            "        weighted avg       0.68      0.60      0.61     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7645, Train: 0.9375, Test: 0.2310\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3124, Train: 1.0000, Test: 0.3308\n",
            "Early stopping:  0.3196961528348964\n",
            "Epoch: 003, Loss: 1.8171, Train: 1.0000, Test: 0.3693\n",
            "Early stopping:  0.47389649414929386\n",
            "Epoch: 004, Loss: 1.2872, Train: 1.0000, Test: 0.4013\n",
            "Early stopping:  0.6365179682512804\n",
            "Epoch: 005, Loss: 0.7992, Train: 1.0000, Test: 0.4406\n",
            "Early stopping:  0.783838170043372\n",
            "Epoch: 006, Loss: 0.4289, Train: 1.0000, Test: 0.4724\n",
            "Early stopping:  0.7579478607246771\n",
            "Epoch: 007, Loss: 0.2010, Train: 1.0000, Test: 0.4957\n",
            "Early stopping:  0.6540941319500448\n",
            "Epoch: 008, Loss: 0.0863, Train: 1.0000, Test: 0.5095\n",
            "Early stopping:  0.4890061641742045\n",
            "Epoch: 009, Loss: 0.0366, Train: 1.0000, Test: 0.5168\n",
            "Early stopping:  0.3123102169355852\n",
            "Epoch: 010, Loss: 0.0163, Train: 1.0000, Test: 0.5201\n",
            "Early stopping:  0.1696222910476472\n",
            "Epoch: 011, Loss: 0.0078, Train: 1.0000, Test: 0.5189\n",
            "Early stopping:  0.07952218483566074\n",
            "Epoch: 012, Loss: 0.0041, Train: 1.0000, Test: 0.5178\n",
            "Early stopping:  0.03376167694460277\n",
            "Epoch: 013, Loss: 0.0023, Train: 1.0000, Test: 0.5161\n",
            "Early stopping:  0.014051693441405019\n",
            "Epoch: 014, Loss: 0.0013, Train: 1.0000, Test: 0.5144\n",
            "Early stopping:  0.006103788219404617\n",
            "PREDICTIONS -> tensor([15,  0, 13,  ..., 14, 15,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.22      0.24      0.23       401\n",
            "             ecology       0.50      0.68      0.58       888\n",
            "            economic       0.80      0.64      0.71      1408\n",
            "          geophysics       0.94      0.64      0.76      1201\n",
            "  gravitional_theory       0.20      0.98      0.33       129\n",
            "               hydro       0.39      0.84      0.53       354\n",
            "                math       0.72      0.27      0.39      1338\n",
            "              metals       0.57      0.86      0.69       200\n",
            "          networking       0.81      0.21      0.33       344\n",
            "        neuroscience       0.89      0.97      0.93       306\n",
            "        oceanography       0.41      0.26      0.32       989\n",
            "             politic       0.45      0.81      0.57       602\n",
            "           sociology       0.28      0.12      0.17       738\n",
            "software_engineering       0.74      0.69      0.71       523\n",
            "          statistics       0.32      0.69      0.44       646\n",
            "    theory_computing       0.16      0.14      0.15       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.53      0.57      0.49     10508\n",
            "        weighted avg       0.58      0.51      0.51     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7728, Train: 0.9375, Test: 0.4318\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3274, Train: 1.0000, Test: 0.5128\n",
            "Early stopping:  0.314986158220107\n",
            "Epoch: 003, Loss: 1.8274, Train: 1.0000, Test: 0.5328\n",
            "Early stopping:  0.47296293127966343\n",
            "Epoch: 004, Loss: 1.2876, Train: 1.0000, Test: 0.5404\n",
            "Early stopping:  0.6403524115757554\n",
            "Epoch: 005, Loss: 0.7921, Train: 1.0000, Test: 0.5445\n",
            "Early stopping:  0.7911509745134071\n",
            "Epoch: 006, Loss: 0.4195, Train: 1.0000, Test: 0.5482\n",
            "Early stopping:  0.768473453725654\n",
            "Epoch: 007, Loss: 0.1912, Train: 1.0000, Test: 0.5483\n",
            "Early stopping:  0.6623921179365864\n",
            "Epoch: 008, Loss: 0.0787, Train: 1.0000, Test: 0.5455\n",
            "Early stopping:  0.49255371572336426\n",
            "Epoch: 009, Loss: 0.0318, Train: 1.0000, Test: 0.5404\n",
            "Early stopping:  0.3119159138684069\n",
            "Epoch: 010, Loss: 0.0134, Train: 1.0000, Test: 0.5348\n",
            "Early stopping:  0.16735139884629185\n",
            "Epoch: 011, Loss: 0.0061, Train: 1.0000, Test: 0.5266\n",
            "Early stopping:  0.07640055252060765\n",
            "Epoch: 012, Loss: 0.0031, Train: 1.0000, Test: 0.5201\n",
            "Early stopping:  0.031154348688750104\n",
            "Epoch: 013, Loss: 0.0017, Train: 1.0000, Test: 0.5148\n",
            "Early stopping:  0.012354987101583023\n",
            "Epoch: 014, Loss: 0.0010, Train: 1.0000, Test: 0.5090\n",
            "Early stopping:  0.0050874798636085135\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 13,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.50      0.68      0.57       401\n",
            "             ecology       0.81      0.67      0.73       888\n",
            "            economic       0.46      0.14      0.22      1408\n",
            "          geophysics       0.86      0.76      0.80      1201\n",
            "  gravitional_theory       0.60      0.94      0.73       129\n",
            "               hydro       0.43      0.36      0.39       354\n",
            "                math       0.70      0.33      0.45      1338\n",
            "              metals       0.35      0.90      0.50       200\n",
            "          networking       0.44      0.96      0.61       344\n",
            "        neuroscience       0.80      0.98      0.88       306\n",
            "        oceanography       0.73      0.79      0.76       989\n",
            "             politic       0.24      0.64      0.35       602\n",
            "           sociology       0.11      0.05      0.07       738\n",
            "software_engineering       0.57      0.39      0.46       523\n",
            "          statistics       0.53      0.61      0.56       646\n",
            "    theory_computing       0.09      0.17      0.11       441\n",
            "\n",
            "            accuracy                           0.51     10508\n",
            "           macro avg       0.51      0.59      0.51     10508\n",
            "        weighted avg       0.56      0.51      0.50     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7762, Train: 0.8750, Test: 0.3215\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3873, Train: 0.9375, Test: 0.5175\n",
            "Early stopping:  0.2750014430378218\n",
            "Epoch: 003, Loss: 1.9409, Train: 1.0000, Test: 0.5848\n",
            "Early stopping:  0.41795758206390604\n",
            "Epoch: 004, Loss: 1.4410, Train: 1.0000, Test: 0.6025\n",
            "Early stopping:  0.5756348414459248\n",
            "Epoch: 005, Loss: 0.9559, Train: 1.0000, Test: 0.6157\n",
            "Early stopping:  0.726070058933524\n",
            "Epoch: 006, Loss: 0.5536, Train: 1.0000, Test: 0.6189\n",
            "Early stopping:  0.7360199677657286\n",
            "Epoch: 007, Loss: 0.2768, Train: 1.0000, Test: 0.6145\n",
            "Early stopping:  0.6704989182763116\n",
            "Epoch: 008, Loss: 0.1244, Train: 1.0000, Test: 0.5998\n",
            "Early stopping:  0.5343214565253765\n",
            "Epoch: 009, Loss: 0.0544, Train: 1.0000, Test: 0.5883\n",
            "Early stopping:  0.36844449225640963\n",
            "Epoch: 010, Loss: 0.0247, Train: 1.0000, Test: 0.5787\n",
            "Early stopping:  0.21701307892683833\n",
            "Epoch: 011, Loss: 0.0119, Train: 1.0000, Test: 0.5699\n",
            "Early stopping:  0.1088001972840445\n",
            "Epoch: 012, Loss: 0.0062, Train: 1.0000, Test: 0.5640\n",
            "Early stopping:  0.048480281029479336\n",
            "Epoch: 013, Loss: 0.0034, Train: 1.0000, Test: 0.5617\n",
            "Early stopping:  0.020849824499037467\n",
            "Epoch: 014, Loss: 0.0020, Train: 1.0000, Test: 0.5578\n",
            "Early stopping:  0.00924009011554583\n",
            "PREDICTIONS -> tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.37      0.59      0.46       401\n",
            "             ecology       0.46      0.94      0.61       888\n",
            "            economic       0.60      0.30      0.40      1408\n",
            "          geophysics       0.96      0.56      0.71      1201\n",
            "  gravitional_theory       0.80      0.90      0.85       129\n",
            "               hydro       0.51      0.36      0.42       354\n",
            "                math       0.60      0.56      0.58      1338\n",
            "              metals       0.39      0.29      0.33       200\n",
            "          networking       0.74      0.72      0.73       344\n",
            "        neuroscience       0.93      0.85      0.89       306\n",
            "        oceanography       0.85      0.73      0.78       989\n",
            "             politic       0.53      0.52      0.52       602\n",
            "           sociology       0.18      0.11      0.14       738\n",
            "software_engineering       0.66      0.71      0.68       523\n",
            "          statistics       0.38      0.75      0.50       646\n",
            "    theory_computing       0.36      0.38      0.37       441\n",
            "\n",
            "            accuracy                           0.56     10508\n",
            "           macro avg       0.58      0.58      0.56     10508\n",
            "        weighted avg       0.60      0.56      0.55     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7895, Train: 0.9375, Test: 0.3535\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3448, Train: 1.0000, Test: 0.5401\n",
            "Early stopping:  0.3144193674013325\n",
            "Epoch: 003, Loss: 1.8617, Train: 1.0000, Test: 0.5491\n",
            "Early stopping:  0.4640296761574809\n",
            "Epoch: 004, Loss: 1.3303, Train: 1.0000, Test: 0.5579\n",
            "Early stopping:  0.6279936375984112\n",
            "Epoch: 005, Loss: 0.8350, Train: 1.0000, Test: 0.5738\n",
            "Early stopping:  0.7788418155118461\n",
            "Epoch: 006, Loss: 0.4548, Train: 1.0000, Test: 0.5892\n",
            "Early stopping:  0.7611454966244652\n",
            "Epoch: 007, Loss: 0.2157, Train: 1.0000, Test: 0.6003\n",
            "Early stopping:  0.6657491365372398\n",
            "Epoch: 008, Loss: 0.0924, Train: 1.0000, Test: 0.6049\n",
            "Early stopping:  0.5034890706970713\n",
            "Epoch: 009, Loss: 0.0385, Train: 1.0000, Test: 0.6068\n",
            "Early stopping:  0.3259584065960613\n",
            "Epoch: 010, Loss: 0.0168, Train: 1.0000, Test: 0.6064\n",
            "Early stopping:  0.18012774888758928\n",
            "Epoch: 011, Loss: 0.0079, Train: 1.0000, Test: 0.6058\n",
            "Early stopping:  0.08563228118456631\n",
            "Epoch: 012, Loss: 0.0040, Train: 1.0000, Test: 0.6025\n",
            "Early stopping:  0.03634666925247786\n",
            "Epoch: 013, Loss: 0.0022, Train: 1.0000, Test: 0.6028\n",
            "Early stopping:  0.014879399576129394\n",
            "Epoch: 014, Loss: 0.0013, Train: 1.0000, Test: 0.6031\n",
            "Early stopping:  0.006301215719823734\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.60      0.60      0.60       401\n",
            "             ecology       0.58      0.84      0.69       888\n",
            "            economic       0.76      0.52      0.62      1408\n",
            "          geophysics       0.92      0.44      0.59      1201\n",
            "  gravitional_theory       0.61      0.95      0.74       129\n",
            "               hydro       0.36      0.80      0.50       354\n",
            "                math       0.70      0.64      0.67      1338\n",
            "              metals       0.66      0.89      0.75       200\n",
            "          networking       0.82      0.81      0.82       344\n",
            "        neuroscience       0.92      0.92      0.92       306\n",
            "        oceanography       0.82      0.45      0.58       989\n",
            "             politic       0.40      0.87      0.55       602\n",
            "           sociology       0.56      0.43      0.48       738\n",
            "software_engineering       0.74      0.82      0.78       523\n",
            "          statistics       0.24      0.13      0.17       646\n",
            "    theory_computing       0.36      0.65      0.46       441\n",
            "\n",
            "            accuracy                           0.60     10508\n",
            "           macro avg       0.63      0.67      0.62     10508\n",
            "        weighted avg       0.66      0.60      0.60     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7860, Train: 0.9375, Test: 0.2946\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3400, Train: 1.0000, Test: 0.4051\n",
            "Early stopping:  0.3154280257798952\n",
            "Epoch: 003, Loss: 1.8392, Train: 1.0000, Test: 0.4590\n",
            "Early stopping:  0.4736789807430769\n",
            "Epoch: 004, Loss: 1.2950, Train: 1.0000, Test: 0.4783\n",
            "Early stopping:  0.6427535623027313\n",
            "Epoch: 005, Loss: 0.7934, Train: 1.0000, Test: 0.4965\n",
            "Early stopping:  0.7957692493287355\n",
            "Epoch: 006, Loss: 0.4162, Train: 1.0000, Test: 0.5134\n",
            "Early stopping:  0.7751061322013608\n",
            "Epoch: 007, Loss: 0.1888, Train: 1.0000, Test: 0.5332\n",
            "Early stopping:  0.6687775351662303\n",
            "Epoch: 008, Loss: 0.0780, Train: 1.0000, Test: 0.5388\n",
            "Early stopping:  0.4963200656679062\n",
            "Epoch: 009, Loss: 0.0318, Train: 1.0000, Test: 0.5402\n",
            "Early stopping:  0.31248306093508144\n",
            "Epoch: 010, Loss: 0.0137, Train: 1.0000, Test: 0.5401\n",
            "Early stopping:  0.1658748813573261\n",
            "Epoch: 011, Loss: 0.0064, Train: 1.0000, Test: 0.5369\n",
            "Early stopping:  0.07529030024534221\n",
            "Epoch: 012, Loss: 0.0032, Train: 1.0000, Test: 0.5343\n",
            "Early stopping:  0.030779556083399346\n",
            "Epoch: 013, Loss: 0.0018, Train: 1.0000, Test: 0.5327\n",
            "Early stopping:  0.012305050293099386\n",
            "Epoch: 014, Loss: 0.0010, Train: 1.0000, Test: 0.5299\n",
            "Early stopping:  0.005144932853825928\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.75      0.75       401\n",
            "             ecology       0.43      0.51      0.46       888\n",
            "            economic       0.32      0.04      0.08      1408\n",
            "          geophysics       0.70      0.92      0.79      1201\n",
            "  gravitional_theory       0.78      0.93      0.85       129\n",
            "               hydro       0.29      0.53      0.38       354\n",
            "                math       0.84      0.28      0.42      1338\n",
            "              metals       0.53      0.91      0.67       200\n",
            "          networking       0.77      0.87      0.82       344\n",
            "        neuroscience       0.63      0.96      0.76       306\n",
            "        oceanography       0.43      0.33      0.38       989\n",
            "             politic       0.61      0.79      0.69       602\n",
            "           sociology       0.50      0.45      0.47       738\n",
            "software_engineering       0.51      0.90      0.65       523\n",
            "          statistics       0.30      0.68      0.41       646\n",
            "    theory_computing       0.67      0.35      0.46       441\n",
            "\n",
            "            accuracy                           0.53     10508\n",
            "           macro avg       0.57      0.64      0.57     10508\n",
            "        weighted avg       0.55      0.53      0.49     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7828, Train: 1.0000, Test: 0.4135\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3831, Train: 1.0000, Test: 0.4861\n",
            "Early stopping:  0.2826362603518732\n",
            "Epoch: 003, Loss: 1.9304, Train: 1.0000, Test: 0.4917\n",
            "Early stopping:  0.4264571276438805\n",
            "Epoch: 004, Loss: 1.4244, Train: 1.0000, Test: 0.4917\n",
            "Early stopping:  0.5853325381361127\n",
            "Epoch: 005, Loss: 0.9394, Train: 1.0000, Test: 0.4971\n",
            "Early stopping:  0.7352050008331622\n",
            "Epoch: 006, Loss: 0.5445, Train: 1.0000, Test: 0.5045\n",
            "Early stopping:  0.7386590930860261\n",
            "Epoch: 007, Loss: 0.2751, Train: 1.0000, Test: 0.5111\n",
            "Early stopping:  0.667046045928305\n",
            "Epoch: 008, Loss: 0.1233, Train: 1.0000, Test: 0.5083\n",
            "Early stopping:  0.5272376718802263\n",
            "Epoch: 009, Loss: 0.0522, Train: 1.0000, Test: 0.4999\n",
            "Early stopping:  0.36197919373511667\n",
            "Epoch: 010, Loss: 0.0227, Train: 1.0000, Test: 0.4895\n",
            "Early stopping:  0.21413182681597806\n",
            "Epoch: 011, Loss: 0.0107, Train: 1.0000, Test: 0.4804\n",
            "Early stopping:  0.10883741290081371\n",
            "Epoch: 012, Loss: 0.0055, Train: 1.0000, Test: 0.4734\n",
            "Early stopping:  0.048461585847181056\n",
            "Epoch: 013, Loss: 0.0032, Train: 1.0000, Test: 0.4672\n",
            "Early stopping:  0.020121637029449205\n",
            "Epoch: 014, Loss: 0.0019, Train: 1.0000, Test: 0.4621\n",
            "Early stopping:  0.008465807859331818\n",
            "PREDICTIONS -> tensor([ 0,  8, 15,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.49      0.58       401\n",
            "             ecology       0.35      0.67      0.46       888\n",
            "            economic       0.81      0.22      0.35      1408\n",
            "          geophysics       0.88      0.60      0.71      1201\n",
            "  gravitional_theory       0.17      0.97      0.30       129\n",
            "               hydro       0.19      0.05      0.08       354\n",
            "                math       0.35      0.20      0.25      1338\n",
            "              metals       0.39      0.81      0.53       200\n",
            "          networking       0.30      0.90      0.45       344\n",
            "        neuroscience       0.82      0.98      0.89       306\n",
            "        oceanography       0.49      0.80      0.61       989\n",
            "             politic       0.64      0.72      0.68       602\n",
            "           sociology       0.13      0.11      0.12       738\n",
            "software_engineering       0.86      0.30      0.44       523\n",
            "          statistics       0.53      0.18      0.27       646\n",
            "    theory_computing       0.45      0.61      0.52       441\n",
            "\n",
            "            accuracy                           0.46     10508\n",
            "           macro avg       0.50      0.54      0.45     10508\n",
            "        weighted avg       0.55      0.46      0.44     10508\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7887, Train: 0.9375, Test: 0.2675\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3778, Train: 1.0000, Test: 0.4338\n",
            "Early stopping:  0.29054014314681825\n",
            "Epoch: 003, Loss: 1.9044, Train: 1.0000, Test: 0.4857\n",
            "Early stopping:  0.4425336506454026\n",
            "Epoch: 004, Loss: 1.3831, Train: 1.0000, Test: 0.5035\n",
            "Early stopping:  0.6063749524880632\n",
            "Epoch: 005, Loss: 0.8890, Train: 1.0000, Test: 0.5150\n",
            "Early stopping:  0.7587053825997603\n",
            "Epoch: 006, Loss: 0.4948, Train: 1.0000, Test: 0.5186\n",
            "Early stopping:  0.7567703996025548\n",
            "Epoch: 007, Loss: 0.2370, Train: 1.0000, Test: 0.5185\n",
            "Early stopping:  0.6731884664598256\n",
            "Epoch: 008, Loss: 0.1019, Train: 1.0000, Test: 0.5102\n",
            "Early stopping:  0.5209300304595219\n",
            "Epoch: 009, Loss: 0.0424, Train: 1.0000, Test: 0.5035\n",
            "Early stopping:  0.346574196817759\n",
            "Epoch: 010, Loss: 0.0183, Train: 1.0000, Test: 0.4966\n",
            "Early stopping:  0.19593306291484142\n",
            "Epoch: 011, Loss: 0.0084, Train: 1.0000, Test: 0.4888\n",
            "Early stopping:  0.09414073712685478\n",
            "Epoch: 012, Loss: 0.0043, Train: 1.0000, Test: 0.4825\n",
            "Early stopping:  0.04018383989228877\n",
            "Epoch: 013, Loss: 0.0024, Train: 1.0000, Test: 0.4773\n",
            "Early stopping:  0.016426632348484977\n",
            "Epoch: 014, Loss: 0.0014, Train: 1.0000, Test: 0.4739\n",
            "Early stopping:  0.006875889702212199\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  7, 13,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.56      0.87      0.68       401\n",
            "             ecology       0.24      0.21      0.22       888\n",
            "            economic       0.63      0.48      0.54      1408\n",
            "          geophysics       0.94      0.44      0.60      1201\n",
            "  gravitional_theory       0.87      0.84      0.85       129\n",
            "               hydro       0.21      0.76      0.33       354\n",
            "                math       0.91      0.40      0.56      1338\n",
            "              metals       0.28      0.90      0.42       200\n",
            "          networking       0.48      0.93      0.63       344\n",
            "        neuroscience       0.78      0.96      0.86       306\n",
            "        oceanography       0.37      0.43      0.40       989\n",
            "             politic       0.50      0.65      0.57       602\n",
            "           sociology       0.33      0.19      0.24       738\n",
            "software_engineering       0.58      0.30      0.39       523\n",
            "          statistics       0.48      0.55      0.51       646\n",
            "    theory_computing       0.16      0.15      0.15       441\n",
            "\n",
            "            accuracy                           0.47     10508\n",
            "           macro avg       0.52      0.57      0.50     10508\n",
            "        weighted avg       0.56      0.47      0.48     10508\n",
            "\n",
            "time: 7.03 s (started: 2024-08-16 14:15:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcbgZvJYKlm5",
        "outputId": "5dc90c27-e6ed-4f8b-81a8-f09c4a4c1c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 416 ms (started: 2024-08-16 14:15:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 5 ❎"
      ],
      "metadata": {
        "id": "2oVJvbBwKlm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "LEOVZ5dhKlm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ7ikfDfKlm5",
        "outputId": "827898e8-b438-4004-a7c8-504aab5da1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5453, Train: 0.9250, Test: 0.7193\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5474, Train: 0.9750, Test: 0.7545\n",
            "Early stopping:  2.1198567099514047\n",
            "Epoch: 003, Loss: 0.1191, Train: 1.0000, Test: 0.7479\n",
            "Early stopping:  1.8668255554496043\n",
            "Epoch: 004, Loss: 0.0285, Train: 1.0000, Test: 0.7349\n",
            "Early stopping:  1.6722035551627097\n",
            "Epoch: 005, Loss: 0.0103, Train: 1.0000, Test: 0.7225\n",
            "Early stopping:  1.522364452423201\n",
            "Epoch: 006, Loss: 0.0047, Train: 1.0000, Test: 0.7136\n",
            "Early stopping:  0.2312532348403261\n",
            "Epoch: 007, Loss: 0.0021, Train: 1.0000, Test: 0.7096\n",
            "Early stopping:  0.049224984983880185\n",
            "Epoch: 008, Loss: 0.0009, Train: 1.0000, Test: 0.7046\n",
            "Early stopping:  0.01133496485395092\n",
            "Epoch: 009, Loss: 0.0005, Train: 1.0000, Test: 0.7022\n",
            "Early stopping:  0.004041564476810928\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.65      0.81      0.72       397\n",
            "             ecology       0.90      0.68      0.77       884\n",
            "            economic       0.88      0.56      0.68      1404\n",
            "          geophysics       0.98      0.70      0.82      1197\n",
            "  gravitional_theory       0.23      0.98      0.37       125\n",
            "               hydro       0.47      0.89      0.62       350\n",
            "                math       0.70      0.42      0.52      1334\n",
            "              metals       0.56      0.94      0.70       196\n",
            "          networking       0.87      0.81      0.84       340\n",
            "        neuroscience       0.96      0.96      0.96       302\n",
            "        oceanography       0.82      0.83      0.82       985\n",
            "             politic       0.66      0.81      0.73       598\n",
            "           sociology       0.63      0.75      0.68       734\n",
            "software_engineering       0.93      0.56      0.70       519\n",
            "          statistics       0.52      0.87      0.65       642\n",
            "    theory_computing       0.57      0.82      0.67       437\n",
            "\n",
            "            accuracy                           0.70     10444\n",
            "           macro avg       0.71      0.77      0.70     10444\n",
            "        weighted avg       0.77      0.70      0.71     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1411, Train: 0.9125, Test: 0.7046\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5493, Train: 1.0000, Test: 0.7307\n",
            "Early stopping:  1.832653754269679\n",
            "Epoch: 003, Loss: 0.1086, Train: 1.0000, Test: 0.7324\n",
            "Early stopping:  1.6384721237113296\n",
            "Epoch: 004, Loss: 0.0195, Train: 1.0000, Test: 0.7275\n",
            "Early stopping:  1.475933978841593\n",
            "Epoch: 005, Loss: 0.0060, Train: 1.0000, Test: 0.7257\n",
            "Early stopping:  1.3467632634376214\n",
            "Epoch: 006, Loss: 0.0018, Train: 1.0000, Test: 0.7238\n",
            "Early stopping:  0.2345551969843798\n",
            "Epoch: 007, Loss: 0.0008, Train: 1.0000, Test: 0.7211\n",
            "Early stopping:  0.04602553744047917\n",
            "Epoch: 008, Loss: 0.0005, Train: 1.0000, Test: 0.7190\n",
            "Early stopping:  0.00799950585844894\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.62      0.79      0.70       397\n",
            "             ecology       0.79      0.51      0.62       884\n",
            "            economic       0.82      0.69      0.75      1404\n",
            "          geophysics       0.87      0.84      0.86      1197\n",
            "  gravitional_theory       0.90      0.93      0.91       125\n",
            "               hydro       0.78      0.71      0.74       350\n",
            "                math       0.90      0.69      0.78      1334\n",
            "              metals       0.52      0.90      0.66       196\n",
            "          networking       0.92      0.71      0.80       340\n",
            "        neuroscience       0.40      0.99      0.57       302\n",
            "        oceanography       0.78      0.78      0.78       985\n",
            "             politic       0.55      0.81      0.66       598\n",
            "           sociology       0.58      0.37      0.45       734\n",
            "software_engineering       0.73      0.78      0.76       519\n",
            "          statistics       0.75      0.75      0.75       642\n",
            "    theory_computing       0.53      0.83      0.65       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.72      0.76      0.71     10444\n",
            "        weighted avg       0.75      0.72      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2151, Train: 0.8250, Test: 0.6430\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.7242, Train: 0.9500, Test: 0.6926\n",
            "Early stopping:  1.7613696092714457\n",
            "Epoch: 003, Loss: 0.2765, Train: 0.9750, Test: 0.6949\n",
            "Early stopping:  1.5832882177989622\n",
            "Epoch: 004, Loss: 0.0711, Train: 1.0000, Test: 0.6983\n",
            "Early stopping:  1.4547199068839083\n",
            "Epoch: 005, Loss: 0.0126, Train: 1.0000, Test: 0.6869\n",
            "Early stopping:  1.345927529727326\n",
            "Epoch: 006, Loss: 0.0094, Train: 0.9875, Test: 0.6730\n",
            "Early stopping:  0.30286914806964\n",
            "Epoch: 007, Loss: 0.0189, Train: 1.0000, Test: 0.6621\n",
            "Early stopping:  0.11394153760242827\n",
            "Epoch: 008, Loss: 0.0099, Train: 1.0000, Test: 0.6553\n",
            "Early stopping:  0.0263815530828816\n",
            "Epoch: 009, Loss: 0.0020, Train: 1.0000, Test: 0.6532\n",
            "Early stopping:  0.006088695443740376\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.67      0.70       397\n",
            "             ecology       0.89      0.41      0.56       884\n",
            "            economic       0.83      0.58      0.68      1404\n",
            "          geophysics       0.95      0.56      0.71      1197\n",
            "  gravitional_theory       0.87      0.94      0.90       125\n",
            "               hydro       0.55      0.79      0.65       350\n",
            "                math       0.76      0.65      0.70      1334\n",
            "              metals       0.33      0.79      0.47       196\n",
            "          networking       0.50      0.96      0.66       340\n",
            "        neuroscience       0.32      1.00      0.48       302\n",
            "        oceanography       0.78      0.65      0.71       985\n",
            "             politic       0.79      0.50      0.62       598\n",
            "           sociology       0.58      0.74      0.65       734\n",
            "software_engineering       0.89      0.55      0.68       519\n",
            "          statistics       0.55      0.88      0.68       642\n",
            "    theory_computing       0.49      0.73      0.59       437\n",
            "\n",
            "            accuracy                           0.65     10444\n",
            "           macro avg       0.68      0.71      0.65     10444\n",
            "        weighted avg       0.74      0.65      0.66     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3524, Train: 0.8750, Test: 0.6567\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5697, Train: 0.9375, Test: 0.7137\n",
            "Early stopping:  1.9676115035368367\n",
            "Epoch: 003, Loss: 0.1672, Train: 1.0000, Test: 0.7197\n",
            "Early stopping:  1.7344630921954598\n",
            "Epoch: 004, Loss: 0.0420, Train: 1.0000, Test: 0.7207\n",
            "Early stopping:  1.562655302184422\n",
            "Epoch: 005, Loss: 0.0095, Train: 1.0000, Test: 0.7054\n",
            "Early stopping:  1.4285846278838108\n",
            "Epoch: 006, Loss: 0.0042, Train: 1.0000, Test: 0.6928\n",
            "Early stopping:  0.23914154921430106\n",
            "Epoch: 007, Loss: 0.0022, Train: 1.0000, Test: 0.6821\n",
            "Early stopping:  0.07017897782539577\n",
            "Epoch: 008, Loss: 0.0011, Train: 1.0000, Test: 0.6717\n",
            "Early stopping:  0.017188204119193366\n",
            "Epoch: 009, Loss: 0.0007, Train: 1.0000, Test: 0.6648\n",
            "Early stopping:  0.003599223083347356\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  2, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.64      0.72       397\n",
            "             ecology       0.74      0.66      0.70       884\n",
            "            economic       0.48      0.68      0.56      1404\n",
            "          geophysics       0.91      0.62      0.74      1197\n",
            "  gravitional_theory       0.93      0.80      0.86       125\n",
            "               hydro       0.38      0.76      0.51       350\n",
            "                math       0.91      0.63      0.74      1334\n",
            "              metals       0.44      0.94      0.60       196\n",
            "          networking       0.83      0.69      0.76       340\n",
            "        neuroscience       0.69      0.99      0.82       302\n",
            "        oceanography       0.81      0.63      0.71       985\n",
            "             politic       0.57      0.82      0.67       598\n",
            "           sociology       0.60      0.54      0.57       734\n",
            "software_engineering       0.81      0.84      0.82       519\n",
            "          statistics       0.55      0.39      0.46       642\n",
            "    theory_computing       0.73      0.65      0.69       437\n",
            "\n",
            "            accuracy                           0.66     10444\n",
            "           macro avg       0.70      0.71      0.68     10444\n",
            "        weighted avg       0.71      0.66      0.67     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3456, Train: 0.7625, Test: 0.6397\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8929, Train: 0.9375, Test: 0.6762\n",
            "Early stopping:  1.7343834457852538\n",
            "Epoch: 003, Loss: 0.2718, Train: 0.9875, Test: 0.6838\n",
            "Early stopping:  1.6253563726406368\n",
            "Epoch: 004, Loss: 0.0759, Train: 1.0000, Test: 0.6794\n",
            "Early stopping:  1.5068685391437033\n",
            "Epoch: 005, Loss: 0.0264, Train: 1.0000, Test: 0.6939\n",
            "Early stopping:  1.397825237477215\n",
            "Epoch: 006, Loss: 0.0109, Train: 1.0000, Test: 0.7093\n",
            "Early stopping:  0.3711591046561563\n",
            "Epoch: 007, Loss: 0.0047, Train: 1.0000, Test: 0.7201\n",
            "Early stopping:  0.11189812416435672\n",
            "Epoch: 008, Loss: 0.0037, Train: 1.0000, Test: 0.7229\n",
            "Early stopping:  0.03022558006717558\n",
            "Epoch: 009, Loss: 0.0018, Train: 1.0000, Test: 0.7269\n",
            "Early stopping:  0.010062990925418923\n",
            "Epoch: 010, Loss: 0.0006, Train: 1.0000, Test: 0.7271\n",
            "Early stopping:  0.004003468858358722\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.88      0.63      0.74       397\n",
            "             ecology       0.63      0.93      0.75       884\n",
            "            economic       0.73      0.56      0.63      1404\n",
            "          geophysics       0.92      0.91      0.91      1197\n",
            "  gravitional_theory       0.70      0.94      0.81       125\n",
            "               hydro       0.57      0.84      0.68       350\n",
            "                math       0.93      0.61      0.74      1334\n",
            "              metals       0.75      0.82      0.79       196\n",
            "          networking       0.90      0.74      0.81       340\n",
            "        neuroscience       0.91      0.97      0.94       302\n",
            "        oceanography       0.94      0.64      0.76       985\n",
            "             politic       0.62      0.62      0.62       598\n",
            "           sociology       0.49      0.59      0.54       734\n",
            "software_engineering       0.56      0.92      0.69       519\n",
            "          statistics       0.67      0.79      0.72       642\n",
            "    theory_computing       0.66      0.66      0.66       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.74      0.76      0.74     10444\n",
            "        weighted avg       0.76      0.73      0.73     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1575, Train: 0.8125, Test: 0.5993\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6650, Train: 0.9125, Test: 0.6970\n",
            "Early stopping:  1.7624933285464432\n",
            "Epoch: 003, Loss: 0.2392, Train: 0.9875, Test: 0.7228\n",
            "Early stopping:  1.5764043200819415\n",
            "Epoch: 004, Loss: 0.0541, Train: 0.9875, Test: 0.7162\n",
            "Early stopping:  1.441903299215031\n",
            "Epoch: 005, Loss: 0.0648, Train: 0.9875, Test: 0.7094\n",
            "Early stopping:  1.3210756856172923\n",
            "Epoch: 006, Loss: 0.0482, Train: 1.0000, Test: 0.6960\n",
            "Early stopping:  0.26426822114736154\n",
            "Epoch: 007, Loss: 0.0168, Train: 0.9875, Test: 0.6857\n",
            "Early stopping:  0.08826496439048492\n",
            "Epoch: 008, Loss: 0.0245, Train: 0.9875, Test: 0.6804\n",
            "Early stopping:  0.020275761422391056\n",
            "Epoch: 009, Loss: 0.0128, Train: 1.0000, Test: 0.6766\n",
            "Early stopping:  0.02226223830426485\n",
            "Epoch: 010, Loss: 0.0045, Train: 0.9875, Test: 0.6734\n",
            "Early stopping:  0.016640819651723444\n",
            "Epoch: 011, Loss: 0.0103, Train: 1.0000, Test: 0.6690\n",
            "Early stopping:  0.007474423042065349\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ...,  6, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.63      0.84      0.72       397\n",
            "             ecology       0.60      0.49      0.54       884\n",
            "            economic       0.68      0.61      0.65      1404\n",
            "          geophysics       0.94      0.87      0.91      1197\n",
            "  gravitional_theory       0.81      0.86      0.84       125\n",
            "               hydro       0.32      0.66      0.43       350\n",
            "                math       0.79      0.69      0.74      1334\n",
            "              metals       0.62      0.82      0.71       196\n",
            "          networking       0.79      0.83      0.81       340\n",
            "        neuroscience       0.46      1.00      0.63       302\n",
            "        oceanography       0.55      0.59      0.57       985\n",
            "             politic       0.72      0.65      0.69       598\n",
            "           sociology       0.53      0.43      0.47       734\n",
            "software_engineering       0.78      0.87      0.82       519\n",
            "          statistics       0.75      0.69      0.72       642\n",
            "    theory_computing       0.76      0.33      0.46       437\n",
            "\n",
            "            accuracy                           0.67     10444\n",
            "           macro avg       0.67      0.70      0.67     10444\n",
            "        weighted avg       0.69      0.67      0.67     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4837, Train: 0.8375, Test: 0.6655\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.7278, Train: 0.9500, Test: 0.6696\n",
            "Early stopping:  1.9486851240382317\n",
            "Epoch: 003, Loss: 0.2650, Train: 0.9750, Test: 0.7128\n",
            "Early stopping:  1.7401530912344558\n",
            "Epoch: 004, Loss: 0.1115, Train: 0.9750, Test: 0.7204\n",
            "Early stopping:  1.5796407500018597\n",
            "Epoch: 005, Loss: 0.0848, Train: 0.9875, Test: 0.7173\n",
            "Early stopping:  1.4481306163584033\n",
            "Epoch: 006, Loss: 0.0189, Train: 1.0000, Test: 0.6919\n",
            "Early stopping:  0.2863557660088019\n",
            "Epoch: 007, Loss: 0.0026, Train: 1.0000, Test: 0.6830\n",
            "Early stopping:  0.10437641928170913\n",
            "Epoch: 008, Loss: 0.0023, Train: 1.0000, Test: 0.6787\n",
            "Early stopping:  0.050776126512225\n",
            "Epoch: 009, Loss: 0.0014, Train: 1.0000, Test: 0.6758\n",
            "Early stopping:  0.0358779357301691\n",
            "Epoch: 010, Loss: 0.0008, Train: 1.0000, Test: 0.6732\n",
            "Early stopping:  0.007711743111546732\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.72      0.77       397\n",
            "             ecology       0.50      0.95      0.65       884\n",
            "            economic       0.82      0.63      0.71      1404\n",
            "          geophysics       0.95      0.85      0.90      1197\n",
            "  gravitional_theory       0.42      0.94      0.58       125\n",
            "               hydro       0.70      0.79      0.75       350\n",
            "                math       0.71      0.62      0.66      1334\n",
            "              metals       0.49      0.93      0.64       196\n",
            "          networking       0.45      0.94      0.61       340\n",
            "        neuroscience       0.94      0.94      0.94       302\n",
            "        oceanography       0.92      0.15      0.26       985\n",
            "             politic       0.63      0.76      0.69       598\n",
            "           sociology       0.64      0.49      0.56       734\n",
            "software_engineering       0.90      0.78      0.83       519\n",
            "          statistics       0.46      0.53      0.49       642\n",
            "    theory_computing       0.74      0.67      0.70       437\n",
            "\n",
            "            accuracy                           0.67     10444\n",
            "           macro avg       0.69      0.73      0.67     10444\n",
            "        weighted avg       0.73      0.67      0.66     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1605, Train: 0.8250, Test: 0.6234\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8017, Train: 0.9375, Test: 0.6866\n",
            "Early stopping:  1.6679506559317154\n",
            "Epoch: 003, Loss: 0.4405, Train: 0.9750, Test: 0.6841\n",
            "Early stopping:  1.4772290472697924\n",
            "Epoch: 004, Loss: 0.1957, Train: 1.0000, Test: 0.6939\n",
            "Early stopping:  1.3635315722465255\n",
            "Epoch: 005, Loss: 0.0451, Train: 0.9875, Test: 0.7028\n",
            "Early stopping:  1.2799985187865217\n",
            "Epoch: 006, Loss: 0.0759, Train: 0.9875, Test: 0.7015\n",
            "Early stopping:  0.31495103163821425\n",
            "Epoch: 007, Loss: 0.0292, Train: 0.9875, Test: 0.6979\n",
            "Early stopping:  0.17122024636630082\n",
            "Epoch: 008, Loss: 0.0371, Train: 0.9875, Test: 0.7014\n",
            "Early stopping:  0.06887114142137629\n",
            "Epoch: 009, Loss: 0.0212, Train: 1.0000, Test: 0.7070\n",
            "Early stopping:  0.021108933268931904\n",
            "Epoch: 010, Loss: 0.0023, Train: 1.0000, Test: 0.7099\n",
            "Early stopping:  0.027203841638262463\n",
            "Epoch: 011, Loss: 0.0027, Train: 1.0000, Test: 0.7106\n",
            "Early stopping:  0.01567302502344243\n",
            "Epoch: 012, Loss: 0.0079, Train: 1.0000, Test: 0.7113\n",
            "Early stopping:  0.014909757842337631\n",
            "Epoch: 013, Loss: 0.0057, Train: 1.0000, Test: 0.7113\n",
            "Early stopping:  0.0077359289104940296\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.62      0.72      0.66       397\n",
            "             ecology       0.79      0.63      0.70       884\n",
            "            economic       0.84      0.64      0.72      1404\n",
            "          geophysics       0.96      0.79      0.87      1197\n",
            "  gravitional_theory       0.94      0.70      0.80       125\n",
            "               hydro       0.73      0.56      0.63       350\n",
            "                math       0.92      0.62      0.74      1334\n",
            "              metals       0.39      0.96      0.55       196\n",
            "          networking       0.74      0.91      0.81       340\n",
            "        neuroscience       0.59      1.00      0.75       302\n",
            "        oceanography       0.66      0.90      0.76       985\n",
            "             politic       0.47      0.74      0.58       598\n",
            "           sociology       0.70      0.64      0.67       734\n",
            "software_engineering       0.66      0.61      0.64       519\n",
            "          statistics       0.73      0.63      0.67       642\n",
            "    theory_computing       0.55      0.69      0.61       437\n",
            "\n",
            "            accuracy                           0.71     10444\n",
            "           macro avg       0.70      0.73      0.70     10444\n",
            "        weighted avg       0.75      0.71      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9953, Train: 0.8500, Test: 0.6346\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6394, Train: 0.9625, Test: 0.6694\n",
            "Early stopping:  1.6658204699142778\n",
            "Epoch: 003, Loss: 0.1708, Train: 1.0000, Test: 0.7151\n",
            "Early stopping:  1.513681524032549\n",
            "Epoch: 004, Loss: 0.0256, Train: 1.0000, Test: 0.7378\n",
            "Early stopping:  1.383365673548445\n",
            "Epoch: 005, Loss: 0.0060, Train: 1.0000, Test: 0.7444\n",
            "Early stopping:  1.271389990345056\n",
            "Epoch: 006, Loss: 0.0026, Train: 1.0000, Test: 0.7449\n",
            "Early stopping:  0.2720877647527873\n",
            "Epoch: 007, Loss: 0.0022, Train: 1.0000, Test: 0.7395\n",
            "Early stopping:  0.07292593629321595\n",
            "Epoch: 008, Loss: 0.0027, Train: 1.0000, Test: 0.7338\n",
            "Early stopping:  0.010026269910031799\n",
            "Epoch: 009, Loss: 0.0020, Train: 1.0000, Test: 0.7310\n",
            "Early stopping:  0.0016415604472328868\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.44      0.56       397\n",
            "             ecology       0.79      0.82      0.81       884\n",
            "            economic       0.76      0.58      0.66      1404\n",
            "          geophysics       0.92      0.89      0.91      1197\n",
            "  gravitional_theory       0.83      0.94      0.88       125\n",
            "               hydro       0.95      0.61      0.74       350\n",
            "                math       0.91      0.72      0.80      1334\n",
            "              metals       0.60      0.93      0.73       196\n",
            "          networking       0.72      0.82      0.77       340\n",
            "        neuroscience       0.96      0.95      0.95       302\n",
            "        oceanography       0.83      0.80      0.82       985\n",
            "             politic       0.46      0.75      0.57       598\n",
            "           sociology       0.41      0.58      0.48       734\n",
            "software_engineering       0.95      0.53      0.68       519\n",
            "          statistics       0.65      0.82      0.73       642\n",
            "    theory_computing       0.59      0.78      0.67       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.76      0.75      0.73     10444\n",
            "        weighted avg       0.77      0.73      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1394, Train: 0.8875, Test: 0.7158\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.5013, Train: 0.9500, Test: 0.6783\n",
            "Early stopping:  1.8654255842383554\n",
            "Epoch: 003, Loss: 0.1651, Train: 1.0000, Test: 0.7094\n",
            "Early stopping:  1.628861039809942\n",
            "Epoch: 004, Loss: 0.0223, Train: 0.9875, Test: 0.7065\n",
            "Early stopping:  1.4686998997156337\n",
            "Epoch: 005, Loss: 0.0166, Train: 1.0000, Test: 0.6991\n",
            "Early stopping:  1.3396650552080431\n",
            "Epoch: 006, Loss: 0.0058, Train: 1.0000, Test: 0.6919\n",
            "Early stopping:  0.2110858979286807\n",
            "Epoch: 007, Loss: 0.0040, Train: 1.0000, Test: 0.6866\n",
            "Early stopping:  0.06880576582001273\n",
            "Epoch: 008, Loss: 0.0017, Train: 1.0000, Test: 0.6809\n",
            "Early stopping:  0.008907751540025278\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.47      0.69      0.56       397\n",
            "             ecology       0.83      0.85      0.84       884\n",
            "            economic       0.60      0.33      0.43      1404\n",
            "          geophysics       0.96      0.79      0.86      1197\n",
            "  gravitional_theory       0.84      0.95      0.89       125\n",
            "               hydro       0.63      0.78      0.70       350\n",
            "                math       0.94      0.58      0.72      1334\n",
            "              metals       0.41      0.93      0.57       196\n",
            "          networking       0.71      0.95      0.81       340\n",
            "        neuroscience       0.89      0.98      0.93       302\n",
            "        oceanography       0.74      0.87      0.80       985\n",
            "             politic       0.69      0.61      0.65       598\n",
            "           sociology       0.53      0.46      0.49       734\n",
            "software_engineering       0.71      0.84      0.77       519\n",
            "          statistics       0.39      0.84      0.53       642\n",
            "    theory_computing       0.77      0.39      0.52       437\n",
            "\n",
            "            accuracy                           0.68     10444\n",
            "           macro avg       0.69      0.74      0.69     10444\n",
            "        weighted avg       0.72      0.68      0.68     10444\n",
            "\n",
            "time: 3.95 s (started: 2024-08-16 14:15:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3KzKnExKlm6",
        "outputId": "aee0eaaa-383b-4f61-a976-b0541dc300fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 407 ms (started: 2024-08-16 14:15:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "mrkHReCGKlm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SzUAzbVKlm6",
        "outputId": "351ef7b5-440a-43a5-e43e-700b56fc6fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7588, Train: 0.9000, Test: 0.6344\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4537, Train: 0.9625, Test: 0.7415\n",
            "Early stopping:  0.2157154942862757\n",
            "Epoch: 003, Loss: 2.0785, Train: 0.9875, Test: 0.7600\n",
            "Early stopping:  0.34076605373258784\n",
            "Epoch: 004, Loss: 1.6498, Train: 0.9875, Test: 0.7639\n",
            "Early stopping:  0.4792950372445119\n",
            "Epoch: 005, Loss: 1.2206, Train: 0.9875, Test: 0.7695\n",
            "Early stopping:  0.6149638617779677\n",
            "Epoch: 006, Loss: 0.8394, Train: 1.0000, Test: 0.7781\n",
            "Early stopping:  0.6463595469435263\n",
            "Epoch: 007, Loss: 0.5370, Train: 1.0000, Test: 0.7875\n",
            "Early stopping:  0.6170511444039583\n",
            "Epoch: 008, Loss: 0.3219, Train: 1.0000, Test: 0.7938\n",
            "Early stopping:  0.5323859250279785\n",
            "Epoch: 009, Loss: 0.1854, Train: 1.0000, Test: 0.7939\n",
            "Early stopping:  0.416388166294593\n",
            "Epoch: 010, Loss: 0.1063, Train: 1.0000, Test: 0.7919\n",
            "Early stopping:  0.29588486548539866\n",
            "Epoch: 011, Loss: 0.0624, Train: 1.0000, Test: 0.7900\n",
            "Early stopping:  0.19187420591718704\n",
            "Epoch: 012, Loss: 0.0377, Train: 1.0000, Test: 0.7893\n",
            "Early stopping:  0.11480630635573899\n",
            "Epoch: 013, Loss: 0.0234, Train: 1.0000, Test: 0.7879\n",
            "Early stopping:  0.0652738390432866\n",
            "Epoch: 014, Loss: 0.0149, Train: 1.0000, Test: 0.7881\n",
            "Early stopping:  0.03676447858124984\n",
            "Epoch: 015, Loss: 0.0098, Train: 1.0000, Test: 0.7876\n",
            "Early stopping:  0.021138957506459433\n",
            "Epoch: 016, Loss: 0.0066, Train: 1.0000, Test: 0.7874\n",
            "Early stopping:  0.012496579669375816\n",
            "Epoch: 017, Loss: 0.0046, Train: 1.0000, Test: 0.7879\n",
            "Early stopping:  0.007566174710875409\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.79      0.79       397\n",
            "             ecology       0.81      0.83      0.82       884\n",
            "            economic       0.84      0.70      0.76      1404\n",
            "          geophysics       0.97      0.79      0.87      1197\n",
            "  gravitional_theory       0.43      0.95      0.59       125\n",
            "               hydro       0.76      0.81      0.78       350\n",
            "                math       0.80      0.81      0.80      1334\n",
            "              metals       0.63      0.91      0.74       196\n",
            "          networking       0.80      0.80      0.80       340\n",
            "        neuroscience       0.94      0.97      0.96       302\n",
            "        oceanography       0.85      0.82      0.83       985\n",
            "             politic       0.74      0.79      0.76       598\n",
            "           sociology       0.72      0.67      0.69       734\n",
            "software_engineering       0.85      0.81      0.83       519\n",
            "          statistics       0.68      0.79      0.73       642\n",
            "    theory_computing       0.63      0.75      0.69       437\n",
            "\n",
            "            accuracy                           0.79     10444\n",
            "           macro avg       0.76      0.81      0.78     10444\n",
            "        weighted avg       0.80      0.79      0.79     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7790, Train: 0.8750, Test: 0.5508\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4841, Train: 0.9750, Test: 0.7414\n",
            "Early stopping:  0.20856030810827825\n",
            "Epoch: 003, Loss: 2.1267, Train: 0.9875, Test: 0.7551\n",
            "Early stopping:  0.32667270780030405\n",
            "Epoch: 004, Loss: 1.7137, Train: 1.0000, Test: 0.7553\n",
            "Early stopping:  0.45999955709888185\n",
            "Epoch: 005, Loss: 1.2882, Train: 1.0000, Test: 0.7591\n",
            "Early stopping:  0.594811238632731\n",
            "Epoch: 006, Loss: 0.8969, Train: 1.0000, Test: 0.7629\n",
            "Early stopping:  0.6347172406370731\n",
            "Epoch: 007, Loss: 0.5778, Train: 1.0000, Test: 0.7653\n",
            "Early stopping:  0.6197799022640939\n",
            "Epoch: 008, Loss: 0.3479, Train: 1.0000, Test: 0.7638\n",
            "Early stopping:  0.5478127921984932\n",
            "Epoch: 009, Loss: 0.2002, Train: 1.0000, Test: 0.7622\n",
            "Early stopping:  0.4377065468539655\n",
            "Epoch: 010, Loss: 0.1130, Train: 1.0000, Test: 0.7615\n",
            "Early stopping:  0.3161827395496708\n",
            "Epoch: 011, Loss: 0.0638, Train: 1.0000, Test: 0.7629\n",
            "Early stopping:  0.2076102621207085\n",
            "Epoch: 012, Loss: 0.0364, Train: 1.0000, Test: 0.7646\n",
            "Early stopping:  0.12582772903402326\n",
            "Epoch: 013, Loss: 0.0213, Train: 1.0000, Test: 0.7646\n",
            "Early stopping:  0.07226912027353728\n",
            "Epoch: 014, Loss: 0.0130, Train: 1.0000, Test: 0.7642\n",
            "Early stopping:  0.040427447752372385\n",
            "Epoch: 015, Loss: 0.0082, Train: 1.0000, Test: 0.7643\n",
            "Early stopping:  0.02243521475124812\n",
            "Epoch: 016, Loss: 0.0055, Train: 1.0000, Test: 0.7625\n",
            "Early stopping:  0.012487887233768641\n",
            "Epoch: 017, Loss: 0.0038, Train: 1.0000, Test: 0.7618\n",
            "Early stopping:  0.007053116011796822\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.76      0.74       397\n",
            "             ecology       0.83      0.64      0.73       884\n",
            "            economic       0.84      0.63      0.72      1404\n",
            "          geophysics       0.90      0.84      0.87      1197\n",
            "  gravitional_theory       0.78      0.92      0.85       125\n",
            "               hydro       0.76      0.77      0.76       350\n",
            "                math       0.87      0.73      0.79      1334\n",
            "              metals       0.64      0.87      0.74       196\n",
            "          networking       0.85      0.86      0.85       340\n",
            "        neuroscience       0.78      1.00      0.87       302\n",
            "        oceanography       0.75      0.85      0.80       985\n",
            "             politic       0.64      0.81      0.72       598\n",
            "           sociology       0.60      0.58      0.59       734\n",
            "software_engineering       0.80      0.80      0.80       519\n",
            "          statistics       0.62      0.86      0.72       642\n",
            "    theory_computing       0.64      0.80      0.71       437\n",
            "\n",
            "            accuracy                           0.76     10444\n",
            "           macro avg       0.75      0.80      0.77     10444\n",
            "        weighted avg       0.78      0.76      0.76     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7958, Train: 0.7625, Test: 0.4758\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4886, Train: 0.9375, Test: 0.7064\n",
            "Early stopping:  0.2172196310159325\n",
            "Epoch: 003, Loss: 2.1255, Train: 0.9625, Test: 0.7105\n",
            "Early stopping:  0.3355410953320844\n",
            "Epoch: 004, Loss: 1.7031, Train: 0.9750, Test: 0.7199\n",
            "Early stopping:  0.47124270182332995\n",
            "Epoch: 005, Loss: 1.2730, Train: 0.9750, Test: 0.7287\n",
            "Early stopping:  0.6071543364665519\n",
            "Epoch: 006, Loss: 0.8873, Train: 1.0000, Test: 0.7409\n",
            "Early stopping:  0.641406164911078\n",
            "Epoch: 007, Loss: 0.5791, Train: 1.0000, Test: 0.7471\n",
            "Early stopping:  0.6192278855688508\n",
            "Epoch: 008, Loss: 0.3574, Train: 1.0000, Test: 0.7464\n",
            "Early stopping:  0.5393695730881023\n",
            "Epoch: 009, Loss: 0.2136, Train: 1.0000, Test: 0.7384\n",
            "Early stopping:  0.42567875753937173\n",
            "Epoch: 010, Loss: 0.1278, Train: 1.0000, Test: 0.7339\n",
            "Early stopping:  0.3060520475370581\n",
            "Epoch: 011, Loss: 0.0778, Train: 1.0000, Test: 0.7319\n",
            "Early stopping:  0.20216541170445612\n",
            "Epoch: 012, Loss: 0.0479, Train: 1.0000, Test: 0.7331\n",
            "Early stopping:  0.12458491502930297\n",
            "Epoch: 013, Loss: 0.0297, Train: 1.0000, Test: 0.7349\n",
            "Early stopping:  0.07387573665386274\n",
            "Epoch: 014, Loss: 0.0187, Train: 1.0000, Test: 0.7368\n",
            "Early stopping:  0.04383126884091184\n",
            "Epoch: 015, Loss: 0.0123, Train: 1.0000, Test: 0.7378\n",
            "Early stopping:  0.026369672147671937\n",
            "Epoch: 016, Loss: 0.0084, Train: 1.0000, Test: 0.7387\n",
            "Early stopping:  0.01588491467610546\n",
            "Epoch: 017, Loss: 0.0060, Train: 1.0000, Test: 0.7385\n",
            "Early stopping:  0.009508863441113536\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.75      0.73       397\n",
            "             ecology       0.83      0.74      0.78       884\n",
            "            economic       0.74      0.76      0.75      1404\n",
            "          geophysics       0.93      0.70      0.80      1197\n",
            "  gravitional_theory       0.87      0.93      0.90       125\n",
            "               hydro       0.60      0.74      0.66       350\n",
            "                math       0.88      0.72      0.79      1334\n",
            "              metals       0.51      0.86      0.64       196\n",
            "          networking       0.80      0.91      0.85       340\n",
            "        neuroscience       0.75      1.00      0.86       302\n",
            "        oceanography       0.74      0.88      0.81       985\n",
            "             politic       0.68      0.73      0.70       598\n",
            "           sociology       0.67      0.51      0.58       734\n",
            "software_engineering       0.85      0.40      0.54       519\n",
            "          statistics       0.68      0.83      0.75       642\n",
            "    theory_computing       0.44      0.76      0.56       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.73      0.76      0.73     10444\n",
            "        weighted avg       0.76      0.74      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7756, Train: 0.7125, Test: 0.3918\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4575, Train: 0.9250, Test: 0.5968\n",
            "Early stopping:  0.22492340057414775\n",
            "Epoch: 003, Loss: 2.0866, Train: 0.9500, Test: 0.6395\n",
            "Early stopping:  0.3448115777745948\n",
            "Epoch: 004, Loss: 1.6701, Train: 0.9625, Test: 0.6685\n",
            "Early stopping:  0.4768814288067211\n",
            "Epoch: 005, Loss: 1.2546, Train: 0.9625, Test: 0.6944\n",
            "Early stopping:  0.6064090723598308\n",
            "Epoch: 006, Loss: 0.8824, Train: 0.9875, Test: 0.7203\n",
            "Early stopping:  0.6298248630685455\n",
            "Epoch: 007, Loss: 0.5829, Train: 0.9875, Test: 0.7382\n",
            "Early stopping:  0.6013214444153311\n",
            "Epoch: 008, Loss: 0.3672, Train: 0.9875, Test: 0.7486\n",
            "Early stopping:  0.522077491502041\n",
            "Epoch: 009, Loss: 0.2267, Train: 1.0000, Test: 0.7521\n",
            "Early stopping:  0.41300586541223505\n",
            "Epoch: 010, Loss: 0.1401, Train: 1.0000, Test: 0.7534\n",
            "Early stopping:  0.2986719445898043\n",
            "Epoch: 011, Loss: 0.0864, Train: 1.0000, Test: 0.7492\n",
            "Early stopping:  0.19954161613874485\n",
            "Epoch: 012, Loss: 0.0527, Train: 1.0000, Test: 0.7455\n",
            "Early stopping:  0.12613494803319236\n",
            "Epoch: 013, Loss: 0.0325, Train: 1.0000, Test: 0.7401\n",
            "Early stopping:  0.07801486832236919\n",
            "Epoch: 014, Loss: 0.0209, Train: 1.0000, Test: 0.7366\n",
            "Early stopping:  0.04804746505491548\n",
            "Epoch: 015, Loss: 0.0141, Train: 1.0000, Test: 0.7341\n",
            "Early stopping:  0.02912255525332609\n",
            "Epoch: 016, Loss: 0.0098, Train: 1.0000, Test: 0.7330\n",
            "Early stopping:  0.017216881064432088\n",
            "Epoch: 017, Loss: 0.0070, Train: 1.0000, Test: 0.7323\n",
            "Early stopping:  0.010197897585744532\n",
            "Epoch: 018, Loss: 0.0051, Train: 1.0000, Test: 0.7333\n",
            "Early stopping:  0.0063081348221897555\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.74      0.75       397\n",
            "             ecology       0.84      0.65      0.73       884\n",
            "            economic       0.75      0.56      0.64      1404\n",
            "          geophysics       0.87      0.86      0.87      1197\n",
            "  gravitional_theory       0.88      0.88      0.88       125\n",
            "               hydro       0.50      0.70      0.58       350\n",
            "                math       0.89      0.69      0.78      1334\n",
            "              metals       0.46      0.95      0.62       196\n",
            "          networking       0.83      0.87      0.85       340\n",
            "        neuroscience       0.90      0.96      0.93       302\n",
            "        oceanography       0.80      0.79      0.80       985\n",
            "             politic       0.51      0.89      0.65       598\n",
            "           sociology       0.54      0.55      0.55       734\n",
            "software_engineering       0.70      0.92      0.80       519\n",
            "          statistics       0.76      0.71      0.73       642\n",
            "    theory_computing       0.74      0.64      0.69       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.73      0.77      0.74     10444\n",
            "        weighted avg       0.76      0.73      0.74     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7774, Train: 0.8500, Test: 0.5115\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4708, Train: 0.9625, Test: 0.6691\n",
            "Early stopping:  0.21680895212404233\n",
            "Epoch: 003, Loss: 2.1072, Train: 0.9750, Test: 0.7177\n",
            "Early stopping:  0.3354982535628509\n",
            "Epoch: 004, Loss: 1.6883, Train: 0.9875, Test: 0.7368\n",
            "Early stopping:  0.46984811834039086\n",
            "Epoch: 005, Loss: 1.2646, Train: 0.9875, Test: 0.7460\n",
            "Early stopping:  0.6033915185330038\n",
            "Epoch: 006, Loss: 0.8839, Train: 0.9875, Test: 0.7451\n",
            "Early stopping:  0.6352537426690779\n",
            "Epoch: 007, Loss: 0.5823, Train: 0.9875, Test: 0.7431\n",
            "Early stopping:  0.6106830993695546\n",
            "Epoch: 008, Loss: 0.3686, Train: 0.9875, Test: 0.7375\n",
            "Early stopping:  0.5294985881904709\n",
            "Epoch: 009, Loss: 0.2304, Train: 0.9875, Test: 0.7321\n",
            "Early stopping:  0.41565877916018573\n",
            "Epoch: 010, Loss: 0.1446, Train: 1.0000, Test: 0.7271\n",
            "Early stopping:  0.2972971012272806\n",
            "Epoch: 011, Loss: 0.0906, Train: 1.0000, Test: 0.7247\n",
            "Early stopping:  0.19737461065956702\n",
            "Epoch: 012, Loss: 0.0566, Train: 1.0000, Test: 0.7226\n",
            "Early stopping:  0.12505808326553783\n",
            "Epoch: 013, Loss: 0.0357, Train: 1.0000, Test: 0.7212\n",
            "Early stopping:  0.07815193965958402\n",
            "Epoch: 014, Loss: 0.0229, Train: 1.0000, Test: 0.7205\n",
            "Early stopping:  0.04892168991104796\n",
            "Epoch: 015, Loss: 0.0149, Train: 1.0000, Test: 0.7195\n",
            "Early stopping:  0.030384356164345237\n",
            "Epoch: 016, Loss: 0.0099, Train: 1.0000, Test: 0.7192\n",
            "Early stopping:  0.018710670738794944\n",
            "Epoch: 017, Loss: 0.0067, Train: 1.0000, Test: 0.7190\n",
            "Early stopping:  0.011617405003221242\n",
            "Epoch: 018, Loss: 0.0047, Train: 1.0000, Test: 0.7180\n",
            "Early stopping:  0.0072993960916064695\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.59      0.81      0.68       397\n",
            "             ecology       0.72      0.80      0.76       884\n",
            "            economic       0.83      0.50      0.63      1404\n",
            "          geophysics       0.94      0.87      0.90      1197\n",
            "  gravitional_theory       0.89      0.93      0.91       125\n",
            "               hydro       0.42      0.91      0.57       350\n",
            "                math       0.91      0.64      0.75      1334\n",
            "              metals       0.79      0.82      0.80       196\n",
            "          networking       0.88      0.83      0.85       340\n",
            "        neuroscience       0.93      0.95      0.94       302\n",
            "        oceanography       0.89      0.74      0.81       985\n",
            "             politic       0.47      0.72      0.57       598\n",
            "           sociology       0.46      0.58      0.51       734\n",
            "software_engineering       0.73      0.66      0.69       519\n",
            "          statistics       0.68      0.76      0.72       642\n",
            "    theory_computing       0.65      0.66      0.65       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.74      0.76      0.73     10444\n",
            "        weighted avg       0.76      0.72      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7865, Train: 0.8000, Test: 0.5009\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4998, Train: 0.9500, Test: 0.6398\n",
            "Early stopping:  0.2027394911541357\n",
            "Epoch: 003, Loss: 2.1553, Train: 0.9500, Test: 0.6871\n",
            "Early stopping:  0.31605034403618915\n",
            "Epoch: 004, Loss: 1.7508, Train: 0.9750, Test: 0.7088\n",
            "Early stopping:  0.4468888759445184\n",
            "Epoch: 005, Loss: 1.3371, Train: 0.9750, Test: 0.7196\n",
            "Early stopping:  0.5783537886486331\n",
            "Epoch: 006, Loss: 0.9622, Train: 1.0000, Test: 0.7285\n",
            "Early stopping:  0.6158833025533489\n",
            "Epoch: 007, Loss: 0.6576, Train: 1.0000, Test: 0.7401\n",
            "Early stopping:  0.5993103806984595\n",
            "Epoch: 008, Loss: 0.4316, Train: 1.0000, Test: 0.7449\n",
            "Early stopping:  0.5280342570596411\n",
            "Epoch: 009, Loss: 0.2760, Train: 1.0000, Test: 0.7443\n",
            "Early stopping:  0.4251073465201227\n",
            "Epoch: 010, Loss: 0.1757, Train: 1.0000, Test: 0.7403\n",
            "Early stopping:  0.3156054643842738\n",
            "Epoch: 011, Loss: 0.1139, Train: 1.0000, Test: 0.7365\n",
            "Early stopping:  0.21852467509846177\n",
            "Epoch: 012, Loss: 0.0752, Train: 1.0000, Test: 0.7349\n",
            "Early stopping:  0.14311701091975196\n",
            "Epoch: 013, Loss: 0.0502, Train: 1.0000, Test: 0.7322\n",
            "Early stopping:  0.09039812719176908\n",
            "Epoch: 014, Loss: 0.0341, Train: 1.0000, Test: 0.7334\n",
            "Early stopping:  0.056680179161860425\n",
            "Epoch: 015, Loss: 0.0238, Train: 1.0000, Test: 0.7321\n",
            "Early stopping:  0.03609693127019308\n",
            "Epoch: 016, Loss: 0.0172, Train: 1.0000, Test: 0.7314\n",
            "Early stopping:  0.02323788307841851\n",
            "Epoch: 017, Loss: 0.0128, Train: 1.0000, Test: 0.7297\n",
            "Early stopping:  0.014967629540512454\n",
            "Epoch: 018, Loss: 0.0097, Train: 1.0000, Test: 0.7287\n",
            "Early stopping:  0.009743831334887834\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 13, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.73      0.74       397\n",
            "             ecology       0.77      0.60      0.68       884\n",
            "            economic       0.74      0.75      0.75      1404\n",
            "          geophysics       0.86      0.91      0.88      1197\n",
            "  gravitional_theory       0.81      0.93      0.86       125\n",
            "               hydro       0.49      0.84      0.62       350\n",
            "                math       0.88      0.60      0.71      1334\n",
            "              metals       0.75      0.68      0.71       196\n",
            "          networking       0.75      0.92      0.82       340\n",
            "        neuroscience       0.76      0.99      0.86       302\n",
            "        oceanography       0.70      0.78      0.74       985\n",
            "             politic       0.66      0.70      0.68       598\n",
            "           sociology       0.61      0.47      0.53       734\n",
            "software_engineering       0.72      0.83      0.77       519\n",
            "          statistics       0.63      0.85      0.73       642\n",
            "    theory_computing       0.66      0.40      0.50       437\n",
            "\n",
            "            accuracy                           0.73     10444\n",
            "           macro avg       0.72      0.75      0.72     10444\n",
            "        weighted avg       0.74      0.73      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7846, Train: 0.8875, Test: 0.5571\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4434, Train: 0.9750, Test: 0.7334\n",
            "Early stopping:  0.24128194118037807\n",
            "Epoch: 003, Loss: 2.0513, Train: 0.9875, Test: 0.7565\n",
            "Early stopping:  0.3669378818020907\n",
            "Epoch: 004, Loss: 1.6129, Train: 0.9875, Test: 0.7609\n",
            "Early stopping:  0.5052072948577403\n",
            "Epoch: 005, Loss: 1.1762, Train: 0.9875, Test: 0.7597\n",
            "Early stopping:  0.6407784065624316\n",
            "Epoch: 006, Loss: 0.7916, Train: 0.9875, Test: 0.7550\n",
            "Early stopping:  0.6608928050293125\n",
            "Epoch: 007, Loss: 0.4952, Train: 1.0000, Test: 0.7498\n",
            "Early stopping:  0.6237037246413307\n",
            "Epoch: 008, Loss: 0.2933, Train: 1.0000, Test: 0.7438\n",
            "Early stopping:  0.5302597839004494\n",
            "Epoch: 009, Loss: 0.1686, Train: 1.0000, Test: 0.7395\n",
            "Early stopping:  0.40582272556485793\n",
            "Epoch: 010, Loss: 0.0954, Train: 1.0000, Test: 0.7340\n",
            "Early stopping:  0.2807579793832411\n",
            "Epoch: 011, Loss: 0.0538, Train: 1.0000, Test: 0.7319\n",
            "Early stopping:  0.17812604064167042\n",
            "Epoch: 012, Loss: 0.0310, Train: 1.0000, Test: 0.7277\n",
            "Early stopping:  0.10601448840048348\n",
            "Epoch: 013, Loss: 0.0185, Train: 1.0000, Test: 0.7250\n",
            "Early stopping:  0.06068917874711616\n",
            "Epoch: 014, Loss: 0.0116, Train: 1.0000, Test: 0.7220\n",
            "Early stopping:  0.03386248924961956\n",
            "Epoch: 015, Loss: 0.0076, Train: 1.0000, Test: 0.7196\n",
            "Early stopping:  0.018655039411975115\n",
            "Epoch: 016, Loss: 0.0052, Train: 1.0000, Test: 0.7194\n",
            "Early stopping:  0.010393892763837434\n",
            "Epoch: 017, Loss: 0.0037, Train: 1.0000, Test: 0.7173\n",
            "Early stopping:  0.0059623372256561715\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.85      0.76       397\n",
            "             ecology       0.60      0.93      0.73       884\n",
            "            economic       0.81      0.68      0.74      1404\n",
            "          geophysics       0.93      0.88      0.91      1197\n",
            "  gravitional_theory       0.24      0.97      0.38       125\n",
            "               hydro       0.75      0.86      0.80       350\n",
            "                math       0.81      0.51      0.62      1334\n",
            "              metals       0.55      0.91      0.69       196\n",
            "          networking       0.69      0.88      0.77       340\n",
            "        neuroscience       0.91      0.97      0.94       302\n",
            "        oceanography       0.91      0.47      0.62       985\n",
            "             politic       0.61      0.80      0.69       598\n",
            "           sociology       0.66      0.43      0.52       734\n",
            "software_engineering       0.87      0.84      0.85       519\n",
            "          statistics       0.62      0.71      0.66       642\n",
            "    theory_computing       0.74      0.71      0.72       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.71      0.77      0.71     10444\n",
            "        weighted avg       0.76      0.72      0.72     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7704, Train: 0.7000, Test: 0.4030\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4771, Train: 0.9875, Test: 0.6749\n",
            "Early stopping:  0.2074049786968976\n",
            "Epoch: 003, Loss: 2.1258, Train: 0.9500, Test: 0.7213\n",
            "Early stopping:  0.32271056850228164\n",
            "Epoch: 004, Loss: 1.7208, Train: 0.9375, Test: 0.7323\n",
            "Early stopping:  0.4529944093654092\n",
            "Epoch: 005, Loss: 1.3110, Train: 0.9375, Test: 0.7381\n",
            "Early stopping:  0.58239723914509\n",
            "Epoch: 006, Loss: 0.9360, Train: 0.9625, Test: 0.7466\n",
            "Early stopping:  0.6163713529635663\n",
            "Epoch: 007, Loss: 0.6266, Train: 0.9750, Test: 0.7464\n",
            "Early stopping:  0.5990501862944337\n",
            "Epoch: 008, Loss: 0.4011, Train: 0.9750, Test: 0.7430\n",
            "Early stopping:  0.5287737176627529\n",
            "Epoch: 009, Loss: 0.2541, Train: 0.9875, Test: 0.7396\n",
            "Early stopping:  0.4249623677216463\n",
            "Epoch: 010, Loss: 0.1614, Train: 1.0000, Test: 0.7404\n",
            "Early stopping:  0.31147049241321784\n",
            "Epoch: 011, Loss: 0.1017, Train: 1.0000, Test: 0.7424\n",
            "Early stopping:  0.21044258289720646\n",
            "Epoch: 012, Loss: 0.0641, Train: 1.0000, Test: 0.7447\n",
            "Early stopping:  0.1350449550909364\n",
            "Epoch: 013, Loss: 0.0421, Train: 1.0000, Test: 0.7456\n",
            "Early stopping:  0.08531660618200959\n",
            "Epoch: 014, Loss: 0.0291, Train: 1.0000, Test: 0.7462\n",
            "Early stopping:  0.05334394505151672\n",
            "Epoch: 015, Loss: 0.0204, Train: 1.0000, Test: 0.7461\n",
            "Early stopping:  0.03255145442634095\n",
            "Epoch: 016, Loss: 0.0143, Train: 1.0000, Test: 0.7460\n",
            "Early stopping:  0.019812501116169057\n",
            "Epoch: 017, Loss: 0.0102, Train: 1.0000, Test: 0.7452\n",
            "Early stopping:  0.012717542375937523\n",
            "Epoch: 018, Loss: 0.0076, Train: 1.0000, Test: 0.7443\n",
            "Early stopping:  0.008593318193375622\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.60      0.77      0.67       397\n",
            "             ecology       0.88      0.56      0.68       884\n",
            "            economic       0.82      0.69      0.75      1404\n",
            "          geophysics       0.88      0.89      0.88      1197\n",
            "  gravitional_theory       0.94      0.86      0.90       125\n",
            "               hydro       0.58      0.70      0.64       350\n",
            "                math       0.92      0.69      0.79      1334\n",
            "              metals       0.37      0.95      0.54       196\n",
            "          networking       0.72      0.92      0.81       340\n",
            "        neuroscience       0.85      0.99      0.92       302\n",
            "        oceanography       0.70      0.86      0.77       985\n",
            "             politic       0.70      0.72      0.71       598\n",
            "           sociology       0.67      0.72      0.69       734\n",
            "software_engineering       0.82      0.56      0.67       519\n",
            "          statistics       0.71      0.75      0.73       642\n",
            "    theory_computing       0.58      0.66      0.62       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.73      0.77      0.73     10444\n",
            "        weighted avg       0.77      0.74      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7877, Train: 0.6750, Test: 0.3961\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4969, Train: 0.9125, Test: 0.6170\n",
            "Early stopping:  0.20556518446570007\n",
            "Epoch: 003, Loss: 2.1432, Train: 0.9250, Test: 0.6811\n",
            "Early stopping:  0.32276497463865556\n",
            "Epoch: 004, Loss: 1.7363, Train: 0.9375, Test: 0.7005\n",
            "Early stopping:  0.454098512729151\n",
            "Epoch: 005, Loss: 1.3295, Train: 0.9500, Test: 0.7160\n",
            "Early stopping:  0.5827233538329136\n",
            "Epoch: 006, Loss: 0.9674, Train: 0.9625, Test: 0.7325\n",
            "Early stopping:  0.6125401512680935\n",
            "Epoch: 007, Loss: 0.6706, Train: 0.9875, Test: 0.7437\n",
            "Early stopping:  0.5883991598274775\n",
            "Epoch: 008, Loss: 0.4458, Train: 0.9875, Test: 0.7445\n",
            "Early stopping:  0.5155098881227778\n",
            "Epoch: 009, Loss: 0.2900, Train: 0.9875, Test: 0.7446\n",
            "Early stopping:  0.4162482157602728\n",
            "Epoch: 010, Loss: 0.1881, Train: 1.0000, Test: 0.7444\n",
            "Early stopping:  0.3126886161753739\n",
            "Epoch: 011, Loss: 0.1214, Train: 1.0000, Test: 0.7451\n",
            "Early stopping:  0.22009097120857327\n",
            "Epoch: 012, Loss: 0.0781, Train: 1.0000, Test: 0.7443\n",
            "Early stopping:  0.14716191193699835\n",
            "Epoch: 013, Loss: 0.0507, Train: 1.0000, Test: 0.7422\n",
            "Early stopping:  0.09592760875808273\n",
            "Epoch: 014, Loss: 0.0337, Train: 1.0000, Test: 0.7419\n",
            "Early stopping:  0.061996861361140264\n",
            "Epoch: 015, Loss: 0.0228, Train: 1.0000, Test: 0.7424\n",
            "Early stopping:  0.03955264345019031\n",
            "Epoch: 016, Loss: 0.0157, Train: 1.0000, Test: 0.7416\n",
            "Early stopping:  0.024965513055650652\n",
            "Epoch: 017, Loss: 0.0110, Train: 1.0000, Test: 0.7405\n",
            "Early stopping:  0.015879428595218504\n",
            "Epoch: 018, Loss: 0.0079, Train: 1.0000, Test: 0.7400\n",
            "Early stopping:  0.010294417993694316\n",
            "Epoch: 019, Loss: 0.0058, Train: 1.0000, Test: 0.7398\n",
            "Early stopping:  0.006781891948599035\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.67      0.72       397\n",
            "             ecology       0.83      0.81      0.82       884\n",
            "            economic       0.77      0.54      0.64      1404\n",
            "          geophysics       0.92      0.87      0.90      1197\n",
            "  gravitional_theory       0.90      0.90      0.90       125\n",
            "               hydro       0.76      0.84      0.79       350\n",
            "                math       0.90      0.64      0.75      1334\n",
            "              metals       0.57      0.92      0.70       196\n",
            "          networking       0.78      0.86      0.82       340\n",
            "        neuroscience       0.89      0.97      0.93       302\n",
            "        oceanography       0.87      0.80      0.83       985\n",
            "             politic       0.52      0.71      0.60       598\n",
            "           sociology       0.43      0.68      0.53       734\n",
            "software_engineering       0.91      0.74      0.81       519\n",
            "          statistics       0.72      0.75      0.74       642\n",
            "    theory_computing       0.52      0.77      0.62       437\n",
            "\n",
            "            accuracy                           0.74     10444\n",
            "           macro avg       0.75      0.78      0.76     10444\n",
            "        weighted avg       0.77      0.74      0.75     10444\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7845, Train: 0.7625, Test: 0.4523\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4920, Train: 0.9500, Test: 0.6188\n",
            "Early stopping:  0.2068466172478253\n",
            "Epoch: 003, Loss: 2.1362, Train: 0.9750, Test: 0.6489\n",
            "Early stopping:  0.3246491620642601\n",
            "Epoch: 004, Loss: 1.7254, Train: 0.9750, Test: 0.6583\n",
            "Early stopping:  0.457413249890647\n",
            "Epoch: 005, Loss: 1.3112, Train: 0.9750, Test: 0.6671\n",
            "Early stopping:  0.5885521242748895\n",
            "Epoch: 006, Loss: 0.9367, Train: 0.9750, Test: 0.6808\n",
            "Early stopping:  0.6224686966131721\n",
            "Epoch: 007, Loss: 0.6282, Train: 0.9750, Test: 0.6942\n",
            "Early stopping:  0.6025587449776916\n",
            "Epoch: 008, Loss: 0.3983, Train: 0.9875, Test: 0.7059\n",
            "Early stopping:  0.5308670290358591\n",
            "Epoch: 009, Loss: 0.2446, Train: 1.0000, Test: 0.7153\n",
            "Early stopping:  0.4281024723730922\n",
            "Epoch: 010, Loss: 0.1495, Train: 1.0000, Test: 0.7193\n",
            "Early stopping:  0.31683514661459633\n",
            "Epoch: 011, Loss: 0.0916, Train: 1.0000, Test: 0.7193\n",
            "Early stopping:  0.2159330936615179\n",
            "Epoch: 012, Loss: 0.0564, Train: 1.0000, Test: 0.7200\n",
            "Early stopping:  0.13741485649824758\n",
            "Epoch: 013, Loss: 0.0353, Train: 1.0000, Test: 0.7183\n",
            "Early stopping:  0.08415848274868919\n",
            "Epoch: 014, Loss: 0.0228, Train: 1.0000, Test: 0.7187\n",
            "Early stopping:  0.05098838309463419\n",
            "Epoch: 015, Loss: 0.0152, Train: 1.0000, Test: 0.7169\n",
            "Early stopping:  0.03072325712630197\n",
            "Epoch: 016, Loss: 0.0104, Train: 1.0000, Test: 0.7162\n",
            "Early stopping:  0.018423475749945076\n",
            "Epoch: 017, Loss: 0.0074, Train: 1.0000, Test: 0.7157\n",
            "Early stopping:  0.011173407712079928\n",
            "Epoch: 018, Loss: 0.0053, Train: 1.0000, Test: 0.7154\n",
            "Early stopping:  0.006964991534597437\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.66      0.70      0.67       397\n",
            "             ecology       0.79      0.87      0.83       884\n",
            "            economic       0.76      0.30      0.43      1404\n",
            "          geophysics       0.87      0.90      0.89      1197\n",
            "  gravitional_theory       0.91      0.90      0.90       125\n",
            "               hydro       0.65      0.84      0.73       350\n",
            "                math       0.93      0.62      0.75      1334\n",
            "              metals       0.59      0.87      0.70       196\n",
            "          networking       0.69      0.94      0.80       340\n",
            "        neuroscience       0.87      0.98      0.92       302\n",
            "        oceanography       0.84      0.77      0.81       985\n",
            "             politic       0.59      0.72      0.65       598\n",
            "           sociology       0.55      0.66      0.60       734\n",
            "software_engineering       0.85      0.74      0.79       519\n",
            "          statistics       0.43      0.81      0.57       642\n",
            "    theory_computing       0.61      0.76      0.68       437\n",
            "\n",
            "            accuracy                           0.72     10444\n",
            "           macro avg       0.73      0.77      0.73     10444\n",
            "        weighted avg       0.75      0.72      0.71     10444\n",
            "\n",
            "time: 8.73 s (started: 2024-08-16 14:15:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZavWYiuhKlm6",
        "outputId": "64acaa0b-758e-491a-a993-ee15d7c97485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 429 ms (started: 2024-08-16 14:15:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 10 ❎"
      ],
      "metadata": {
        "id": "loYWk3alKlm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "XLvcrt87Klm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btxhhiqeKlm6",
        "outputId": "fd90bca2-c453-4b44-d9c4-4ebc35ff92b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3292, Train: 0.7000, Test: 0.6100\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9907, Train: 0.8750, Test: 0.7355\n",
            "Early stopping:  1.6535387939645063\n",
            "Epoch: 003, Loss: 0.3939, Train: 0.9500, Test: 0.7701\n",
            "Early stopping:  1.5513601312953795\n",
            "Epoch: 004, Loss: 0.1679, Train: 0.9812, Test: 0.7881\n",
            "Early stopping:  1.4480469592266985\n",
            "Epoch: 005, Loss: 0.0746, Train: 1.0000, Test: 0.7830\n",
            "Early stopping:  1.3546997811835928\n",
            "Epoch: 006, Loss: 0.0355, Train: 1.0000, Test: 0.7771\n",
            "Early stopping:  0.3933338149205471\n",
            "Epoch: 007, Loss: 0.0189, Train: 1.0000, Test: 0.7750\n",
            "Early stopping:  0.1541980734510987\n",
            "Epoch: 008, Loss: 0.0111, Train: 1.0000, Test: 0.7707\n",
            "Early stopping:  0.064274221770051\n",
            "Epoch: 009, Loss: 0.0066, Train: 1.0000, Test: 0.7669\n",
            "Early stopping:  0.02759376930905674\n",
            "Epoch: 010, Loss: 0.0042, Train: 1.0000, Test: 0.7622\n",
            "Early stopping:  0.012602688417282295\n",
            "Epoch: 011, Loss: 0.0026, Train: 1.0000, Test: 0.7587\n",
            "Early stopping:  0.006518543409636213\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.83      0.77       392\n",
            "             ecology       0.89      0.72      0.79       879\n",
            "            economic       0.83      0.51      0.63      1399\n",
            "          geophysics       0.86      0.95      0.90      1192\n",
            "  gravitional_theory       0.55      0.97      0.70       120\n",
            "               hydro       0.77      0.73      0.75       345\n",
            "                math       0.89      0.71      0.79      1329\n",
            "              metals       0.72      0.89      0.79       191\n",
            "          networking       0.73      0.89      0.80       335\n",
            "        neuroscience       0.89      0.98      0.93       297\n",
            "        oceanography       0.80      0.90      0.85       980\n",
            "             politic       0.50      0.86      0.64       593\n",
            "           sociology       0.57      0.62      0.59       729\n",
            "software_engineering       0.78      0.89      0.83       514\n",
            "          statistics       0.71      0.67      0.69       637\n",
            "    theory_computing       0.73      0.61      0.66       432\n",
            "\n",
            "            accuracy                           0.76     10364\n",
            "           macro avg       0.75      0.80      0.76     10364\n",
            "        weighted avg       0.78      0.76      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3486, Train: 0.8125, Test: 0.6762\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8850, Train: 0.8562, Test: 0.7378\n",
            "Early stopping:  1.7419974847029895\n",
            "Epoch: 003, Loss: 0.3815, Train: 0.9437, Test: 0.7751\n",
            "Early stopping:  1.587779598723518\n",
            "Epoch: 004, Loss: 0.1517, Train: 0.9563, Test: 0.7675\n",
            "Early stopping:  1.470179236161575\n",
            "Epoch: 005, Loss: 0.1038, Train: 0.9812, Test: 0.7504\n",
            "Early stopping:  1.363002952888801\n",
            "Epoch: 006, Loss: 0.0600, Train: 0.9875, Test: 0.7488\n",
            "Early stopping:  0.34119450018472697\n",
            "Epoch: 007, Loss: 0.0278, Train: 0.9938, Test: 0.7462\n",
            "Early stopping:  0.14020686937450771\n",
            "Epoch: 008, Loss: 0.0144, Train: 1.0000, Test: 0.7404\n",
            "Early stopping:  0.05651067668425965\n",
            "Epoch: 009, Loss: 0.0103, Train: 1.0000, Test: 0.7362\n",
            "Early stopping:  0.03905939873764722\n",
            "Epoch: 010, Loss: 0.0101, Train: 1.0000, Test: 0.7344\n",
            "Early stopping:  0.021097933444849327\n",
            "Epoch: 011, Loss: 0.0077, Train: 1.0000, Test: 0.7338\n",
            "Early stopping:  0.00805147440178571\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.76      0.75       392\n",
            "             ecology       0.84      0.59      0.69       879\n",
            "            economic       0.74      0.62      0.67      1399\n",
            "          geophysics       0.91      0.88      0.89      1192\n",
            "  gravitional_theory       0.32      0.97      0.48       120\n",
            "               hydro       0.45      0.90      0.60       345\n",
            "                math       0.82      0.69      0.75      1329\n",
            "              metals       0.68      0.91      0.78       191\n",
            "          networking       0.94      0.68      0.79       335\n",
            "        neuroscience       0.90      0.98      0.94       297\n",
            "        oceanography       0.90      0.70      0.79       980\n",
            "             politic       0.73      0.78      0.75       593\n",
            "           sociology       0.50      0.76      0.60       729\n",
            "software_engineering       0.86      0.73      0.79       514\n",
            "          statistics       0.70      0.76      0.72       637\n",
            "    theory_computing       0.73      0.66      0.69       432\n",
            "\n",
            "            accuracy                           0.73     10364\n",
            "           macro avg       0.73      0.77      0.73     10364\n",
            "        weighted avg       0.77      0.73      0.74     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3929, Train: 0.7500, Test: 0.6347\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0123, Train: 0.8812, Test: 0.7431\n",
            "Early stopping:  1.683342769301591\n",
            "Epoch: 003, Loss: 0.4269, Train: 0.9187, Test: 0.7673\n",
            "Early stopping:  1.5709582818974475\n",
            "Epoch: 004, Loss: 0.2002, Train: 0.9625, Test: 0.7477\n",
            "Early stopping:  1.4637855689382373\n",
            "Epoch: 005, Loss: 0.1232, Train: 0.9875, Test: 0.7475\n",
            "Early stopping:  1.3654946100170808\n",
            "Epoch: 006, Loss: 0.0662, Train: 0.9938, Test: 0.7555\n",
            "Early stopping:  0.38657583880186064\n",
            "Epoch: 007, Loss: 0.0278, Train: 1.0000, Test: 0.7514\n",
            "Early stopping:  0.15817200405630016\n",
            "Epoch: 008, Loss: 0.0156, Train: 1.0000, Test: 0.7407\n",
            "Early stopping:  0.07607819219324023\n",
            "Epoch: 009, Loss: 0.0126, Train: 1.0000, Test: 0.7310\n",
            "Early stopping:  0.04661837132475707\n",
            "Epoch: 010, Loss: 0.0096, Train: 1.0000, Test: 0.7215\n",
            "Early stopping:  0.02331555633430367\n",
            "Epoch: 011, Loss: 0.0070, Train: 1.0000, Test: 0.7148\n",
            "Early stopping:  0.008107998548492463\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.62      0.70       392\n",
            "             ecology       0.81      0.50      0.62       879\n",
            "            economic       0.78      0.72      0.75      1399\n",
            "          geophysics       0.93      0.81      0.87      1192\n",
            "  gravitional_theory       0.84      0.80      0.82       120\n",
            "               hydro       0.58      0.84      0.69       345\n",
            "                math       0.85      0.55      0.67      1329\n",
            "              metals       0.33      0.93      0.48       191\n",
            "          networking       0.69      0.95      0.80       335\n",
            "        neuroscience       0.91      0.97      0.94       297\n",
            "        oceanography       0.62      0.82      0.71       980\n",
            "             politic       0.75      0.58      0.65       593\n",
            "           sociology       0.63      0.56      0.59       729\n",
            "software_engineering       0.83      0.89      0.86       514\n",
            "          statistics       0.58      0.78      0.66       637\n",
            "    theory_computing       0.58      0.80      0.67       432\n",
            "\n",
            "            accuracy                           0.71     10364\n",
            "           macro avg       0.72      0.76      0.72     10364\n",
            "        weighted avg       0.75      0.71      0.72     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2024, Train: 0.8375, Test: 0.6533\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8182, Train: 0.9187, Test: 0.7775\n",
            "Early stopping:  1.6858905462940745\n",
            "Epoch: 003, Loss: 0.3029, Train: 0.9563, Test: 0.7696\n",
            "Early stopping:  1.546873663305823\n",
            "Epoch: 004, Loss: 0.1311, Train: 0.9875, Test: 0.7481\n",
            "Early stopping:  1.4227564218147921\n",
            "Epoch: 005, Loss: 0.0583, Train: 0.9812, Test: 0.7310\n",
            "Early stopping:  1.3194437830331012\n",
            "Epoch: 006, Loss: 0.0573, Train: 1.0000, Test: 0.7540\n",
            "Early stopping:  0.3204759158614627\n",
            "Epoch: 007, Loss: 0.0154, Train: 1.0000, Test: 0.7569\n",
            "Early stopping:  0.11402374625355312\n",
            "Epoch: 008, Loss: 0.0073, Train: 1.0000, Test: 0.7604\n",
            "Early stopping:  0.049122432997413486\n",
            "Epoch: 009, Loss: 0.0062, Train: 0.9938, Test: 0.7565\n",
            "Early stopping:  0.026640508178859674\n",
            "Epoch: 010, Loss: 0.0166, Train: 1.0000, Test: 0.7544\n",
            "Early stopping:  0.021091911271602645\n",
            "Epoch: 011, Loss: 0.0034, Train: 1.0000, Test: 0.7525\n",
            "Early stopping:  0.005871518856113259\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.84      0.74       392\n",
            "             ecology       0.83      0.75      0.78       879\n",
            "            economic       0.74      0.73      0.73      1399\n",
            "          geophysics       0.98      0.79      0.87      1192\n",
            "  gravitional_theory       0.69      0.94      0.80       120\n",
            "               hydro       0.83      0.75      0.79       345\n",
            "                math       0.95      0.58      0.72      1329\n",
            "              metals       0.41      0.96      0.57       191\n",
            "          networking       0.81      0.90      0.86       335\n",
            "        neuroscience       0.81      0.98      0.89       297\n",
            "        oceanography       0.82      0.85      0.83       980\n",
            "             politic       0.78      0.69      0.74       593\n",
            "           sociology       0.62      0.73      0.67       729\n",
            "software_engineering       0.64      0.75      0.69       514\n",
            "          statistics       0.63      0.84      0.72       637\n",
            "    theory_computing       0.58      0.54      0.56       432\n",
            "\n",
            "            accuracy                           0.75     10364\n",
            "           macro avg       0.74      0.79      0.75     10364\n",
            "        weighted avg       0.78      0.75      0.76     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3156, Train: 0.7625, Test: 0.6385\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9249, Train: 0.8750, Test: 0.7140\n",
            "Early stopping:  1.6904757862372717\n",
            "Epoch: 003, Loss: 0.3913, Train: 0.9563, Test: 0.7935\n",
            "Early stopping:  1.5573322119918103\n",
            "Epoch: 004, Loss: 0.1432, Train: 0.9563, Test: 0.7779\n",
            "Early stopping:  1.4516695127952064\n",
            "Epoch: 005, Loss: 0.0946, Train: 0.9938, Test: 0.7706\n",
            "Early stopping:  1.3498542895367818\n",
            "Epoch: 006, Loss: 0.0318, Train: 0.9938, Test: 0.7611\n",
            "Early stopping:  0.3660658214470115\n",
            "Epoch: 007, Loss: 0.0233, Train: 1.0000, Test: 0.7503\n",
            "Early stopping:  0.15040556336701186\n",
            "Epoch: 008, Loss: 0.0104, Train: 1.0000, Test: 0.7441\n",
            "Early stopping:  0.05641379029451785\n",
            "Epoch: 009, Loss: 0.0085, Train: 1.0000, Test: 0.7368\n",
            "Early stopping:  0.035369647039362954\n",
            "Epoch: 010, Loss: 0.0071, Train: 1.0000, Test: 0.7358\n",
            "Early stopping:  0.010813644643170207\n",
            "Epoch: 011, Loss: 0.0042, Train: 1.0000, Test: 0.7359\n",
            "Early stopping:  0.007375825805164604\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.65      0.80      0.72       392\n",
            "             ecology       0.62      0.93      0.74       879\n",
            "            economic       0.81      0.57      0.67      1399\n",
            "          geophysics       0.95      0.88      0.91      1192\n",
            "  gravitional_theory       0.92      0.91      0.92       120\n",
            "               hydro       0.80      0.85      0.83       345\n",
            "                math       0.95      0.56      0.70      1329\n",
            "              metals       0.48      0.94      0.63       191\n",
            "          networking       0.75      0.93      0.83       335\n",
            "        neuroscience       0.93      0.98      0.95       297\n",
            "        oceanography       0.93      0.51      0.66       980\n",
            "             politic       0.66      0.73      0.69       593\n",
            "           sociology       0.53      0.77      0.63       729\n",
            "software_engineering       0.84      0.75      0.80       514\n",
            "          statistics       0.59      0.81      0.68       637\n",
            "    theory_computing       0.62      0.78      0.69       432\n",
            "\n",
            "            accuracy                           0.74     10364\n",
            "           macro avg       0.75      0.79      0.75     10364\n",
            "        weighted avg       0.78      0.74      0.74     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1317, Train: 0.8375, Test: 0.7064\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.6418, Train: 0.9563, Test: 0.7487\n",
            "Early stopping:  1.760639204386752\n",
            "Epoch: 003, Loss: 0.1785, Train: 0.9812, Test: 0.7423\n",
            "Early stopping:  1.588281139749771\n",
            "Epoch: 004, Loss: 0.0742, Train: 1.0000, Test: 0.7600\n",
            "Early stopping:  1.43808079818607\n",
            "Epoch: 005, Loss: 0.0216, Train: 1.0000, Test: 0.7658\n",
            "Early stopping:  1.3210182202900198\n",
            "Epoch: 006, Loss: 0.0103, Train: 1.0000, Test: 0.7708\n",
            "Early stopping:  0.26371879816866267\n",
            "Epoch: 007, Loss: 0.0051, Train: 1.0000, Test: 0.7735\n",
            "Early stopping:  0.07276442964015642\n",
            "Epoch: 008, Loss: 0.0033, Train: 1.0000, Test: 0.7728\n",
            "Early stopping:  0.029563241344258848\n",
            "Epoch: 009, Loss: 0.0018, Train: 1.0000, Test: 0.7689\n",
            "Early stopping:  0.008013814517263799\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.72      0.76       392\n",
            "             ecology       0.82      0.79      0.81       879\n",
            "            economic       0.84      0.69      0.76      1399\n",
            "          geophysics       0.98      0.81      0.89      1192\n",
            "  gravitional_theory       0.82      0.93      0.87       120\n",
            "               hydro       0.68      0.87      0.77       345\n",
            "                math       0.83      0.67      0.74      1329\n",
            "              metals       0.37      0.96      0.54       191\n",
            "          networking       0.91      0.72      0.81       335\n",
            "        neuroscience       0.95      0.93      0.94       297\n",
            "        oceanography       0.86      0.85      0.86       980\n",
            "             politic       0.61      0.77      0.68       593\n",
            "           sociology       0.59      0.72      0.65       729\n",
            "software_engineering       0.80      0.85      0.82       514\n",
            "          statistics       0.63      0.88      0.73       637\n",
            "    theory_computing       0.79      0.56      0.65       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.77      0.80      0.77     10364\n",
            "        weighted avg       0.80      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4426, Train: 0.7250, Test: 0.5915\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9525, Train: 0.9000, Test: 0.7578\n",
            "Early stopping:  1.7606989686179404\n",
            "Epoch: 003, Loss: 0.4491, Train: 0.9313, Test: 0.7698\n",
            "Early stopping:  1.6028130780062977\n",
            "Epoch: 004, Loss: 0.3068, Train: 0.9625, Test: 0.7789\n",
            "Early stopping:  1.4629995054099345\n",
            "Epoch: 005, Loss: 0.1607, Train: 0.9750, Test: 0.7655\n",
            "Early stopping:  1.363567343068087\n",
            "Epoch: 006, Loss: 0.1031, Train: 0.9812, Test: 0.7599\n",
            "Early stopping:  0.33973494596373727\n",
            "Epoch: 007, Loss: 0.0968, Train: 0.9812, Test: 0.7615\n",
            "Early stopping:  0.15195073999580755\n",
            "Epoch: 008, Loss: 0.0404, Train: 0.9812, Test: 0.7621\n",
            "Early stopping:  0.1017266833542811\n",
            "Epoch: 009, Loss: 0.0295, Train: 0.9875, Test: 0.7638\n",
            "Early stopping:  0.05307031611723479\n",
            "Epoch: 010, Loss: 0.0283, Train: 0.9938, Test: 0.7676\n",
            "Early stopping:  0.03716001777489246\n",
            "Epoch: 011, Loss: 0.0158, Train: 1.0000, Test: 0.7747\n",
            "Early stopping:  0.031731730958803765\n",
            "Epoch: 012, Loss: 0.0062, Train: 1.0000, Test: 0.7766\n",
            "Early stopping:  0.0132265278620538\n",
            "Epoch: 013, Loss: 0.0076, Train: 1.0000, Test: 0.7787\n",
            "Early stopping:  0.011060416895365443\n",
            "Epoch: 014, Loss: 0.0052, Train: 1.0000, Test: 0.7742\n",
            "Early stopping:  0.009740794230174347\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.77      0.78       392\n",
            "             ecology       0.91      0.66      0.76       879\n",
            "            economic       0.77      0.75      0.76      1399\n",
            "          geophysics       0.90      0.92      0.91      1192\n",
            "  gravitional_theory       0.70      0.93      0.80       120\n",
            "               hydro       0.56      0.88      0.68       345\n",
            "                math       0.91      0.68      0.78      1329\n",
            "              metals       0.72      0.78      0.75       191\n",
            "          networking       0.77      0.89      0.82       335\n",
            "        neuroscience       0.94      0.97      0.96       297\n",
            "        oceanography       0.79      0.87      0.83       980\n",
            "             politic       0.56      0.81      0.66       593\n",
            "           sociology       0.65      0.50      0.56       729\n",
            "software_engineering       0.89      0.72      0.80       514\n",
            "          statistics       0.70      0.88      0.78       637\n",
            "    theory_computing       0.69      0.74      0.71       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.77      0.80      0.77     10364\n",
            "        weighted avg       0.79      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4785, Train: 0.7125, Test: 0.6089\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9623, Train: 0.9125, Test: 0.7386\n",
            "Early stopping:  1.7792110024488348\n",
            "Epoch: 003, Loss: 0.3745, Train: 0.9375, Test: 0.7614\n",
            "Early stopping:  1.6487893942466356\n",
            "Epoch: 004, Loss: 0.2035, Train: 0.9750, Test: 0.7570\n",
            "Early stopping:  1.517723570327656\n",
            "Epoch: 005, Loss: 0.1225, Train: 0.9875, Test: 0.7662\n",
            "Early stopping:  1.4085422674916166\n",
            "Epoch: 006, Loss: 0.0497, Train: 1.0000, Test: 0.7699\n",
            "Early stopping:  0.36695142843620737\n",
            "Epoch: 007, Loss: 0.0165, Train: 1.0000, Test: 0.7718\n",
            "Early stopping:  0.14305288921981554\n",
            "Epoch: 008, Loss: 0.0110, Train: 1.0000, Test: 0.7713\n",
            "Early stopping:  0.08179542084543774\n",
            "Epoch: 009, Loss: 0.0078, Train: 1.0000, Test: 0.7714\n",
            "Early stopping:  0.048270903427203395\n",
            "Epoch: 010, Loss: 0.0071, Train: 1.0000, Test: 0.7721\n",
            "Early stopping:  0.017901675180909522\n",
            "Epoch: 011, Loss: 0.0064, Train: 1.0000, Test: 0.7708\n",
            "Early stopping:  0.004162172622972109\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 14], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.74      0.74       392\n",
            "             ecology       0.71      0.86      0.77       879\n",
            "            economic       0.81      0.67      0.74      1399\n",
            "          geophysics       0.96      0.85      0.91      1192\n",
            "  gravitional_theory       0.83      0.90      0.86       120\n",
            "               hydro       0.81      0.74      0.77       345\n",
            "                math       0.87      0.76      0.81      1329\n",
            "              metals       0.80      0.80      0.80       191\n",
            "          networking       0.78      0.93      0.85       335\n",
            "        neuroscience       0.94      0.98      0.96       297\n",
            "        oceanography       0.90      0.72      0.80       980\n",
            "             politic       0.67      0.83      0.74       593\n",
            "           sociology       0.59      0.62      0.61       729\n",
            "software_engineering       0.77      0.88      0.82       514\n",
            "          statistics       0.53      0.89      0.66       637\n",
            "    theory_computing       0.77      0.45      0.57       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.78      0.79      0.78     10364\n",
            "        weighted avg       0.79      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3561, Train: 0.8125, Test: 0.6856\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8592, Train: 0.9187, Test: 0.7842\n",
            "Early stopping:  1.7655821024864127\n",
            "Epoch: 003, Loss: 0.2738, Train: 0.9750, Test: 0.7860\n",
            "Early stopping:  1.6369453109919623\n",
            "Epoch: 004, Loss: 0.1107, Train: 0.9812, Test: 0.7803\n",
            "Early stopping:  1.5054690564570563\n",
            "Epoch: 005, Loss: 0.0529, Train: 1.0000, Test: 0.7755\n",
            "Early stopping:  1.3930337395179446\n",
            "Epoch: 006, Loss: 0.0207, Train: 1.0000, Test: 0.7721\n",
            "Early stopping:  0.3469918148906807\n",
            "Epoch: 007, Loss: 0.0078, Train: 1.0000, Test: 0.7685\n",
            "Early stopping:  0.1085277932642546\n",
            "Epoch: 008, Loss: 0.0067, Train: 1.0000, Test: 0.7664\n",
            "Early stopping:  0.04381501130123989\n",
            "Epoch: 009, Loss: 0.0068, Train: 1.0000, Test: 0.7637\n",
            "Early stopping:  0.019858041690460355\n",
            "Epoch: 010, Loss: 0.0034, Train: 1.0000, Test: 0.7651\n",
            "Early stopping:  0.0067154414132217935\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.67      0.77      0.72       392\n",
            "             ecology       0.84      0.72      0.77       879\n",
            "            economic       0.78      0.65      0.71      1399\n",
            "          geophysics       0.93      0.84      0.89      1192\n",
            "  gravitional_theory       0.93      0.85      0.89       120\n",
            "               hydro       0.40      0.93      0.56       345\n",
            "                math       0.93      0.72      0.81      1329\n",
            "              metals       0.75      0.86      0.80       191\n",
            "          networking       0.88      0.84      0.86       335\n",
            "        neuroscience       0.90      0.98      0.94       297\n",
            "        oceanography       0.88      0.80      0.84       980\n",
            "             politic       0.82      0.67      0.74       593\n",
            "           sociology       0.54      0.76      0.63       729\n",
            "software_engineering       0.71      0.90      0.79       514\n",
            "          statistics       0.73      0.72      0.72       637\n",
            "    theory_computing       0.69      0.73      0.71       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.77      0.80      0.77     10364\n",
            "        weighted avg       0.80      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1535, Train: 0.7812, Test: 0.6490\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9317, Train: 0.9250, Test: 0.7690\n",
            "Early stopping:  1.5710402576613427\n",
            "Epoch: 003, Loss: 0.2656, Train: 0.9500, Test: 0.7545\n",
            "Early stopping:  1.5121875241906215\n",
            "Epoch: 004, Loss: 0.1476, Train: 0.9750, Test: 0.7480\n",
            "Early stopping:  1.3959578727927608\n",
            "Epoch: 005, Loss: 0.0823, Train: 1.0000, Test: 0.7519\n",
            "Early stopping:  1.2956864519281128\n",
            "Epoch: 006, Loss: 0.0288, Train: 0.9938, Test: 0.7590\n",
            "Early stopping:  0.3687939862861478\n",
            "Epoch: 007, Loss: 0.0201, Train: 0.9938, Test: 0.7650\n",
            "Early stopping:  0.10131812133348135\n",
            "Epoch: 008, Loss: 0.0144, Train: 1.0000, Test: 0.7691\n",
            "Early stopping:  0.056592292300492446\n",
            "Epoch: 009, Loss: 0.0073, Train: 1.0000, Test: 0.7694\n",
            "Early stopping:  0.029964684801302936\n",
            "Epoch: 010, Loss: 0.0032, Train: 1.0000, Test: 0.7675\n",
            "Early stopping:  0.010169458698497526\n",
            "Epoch: 011, Loss: 0.0022, Train: 1.0000, Test: 0.7661\n",
            "Early stopping:  0.007640991391959653\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.72      0.68       392\n",
            "             ecology       0.89      0.69      0.78       879\n",
            "            economic       0.81      0.66      0.72      1399\n",
            "          geophysics       0.93      0.84      0.89      1192\n",
            "  gravitional_theory       0.58      0.94      0.72       120\n",
            "               hydro       0.43      0.92      0.58       345\n",
            "                math       0.90      0.76      0.82      1329\n",
            "              metals       0.62      0.93      0.74       191\n",
            "          networking       0.90      0.72      0.80       335\n",
            "        neuroscience       0.94      0.96      0.95       297\n",
            "        oceanography       0.86      0.81      0.83       980\n",
            "             politic       0.76      0.71      0.73       593\n",
            "           sociology       0.56      0.71      0.63       729\n",
            "software_engineering       0.88      0.82      0.85       514\n",
            "          statistics       0.67      0.77      0.72       637\n",
            "    theory_computing       0.68      0.78      0.72       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.75      0.80      0.76     10364\n",
            "        weighted avg       0.80      0.77      0.77     10364\n",
            "\n",
            "time: 4.46 s (started: 2024-08-16 14:15:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKmFGLlPKlm6",
        "outputId": "4ee29768-b0cb-4a15-baa3-9d14d996a7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 427 ms (started: 2024-08-16 14:16:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "692qnr_KKlm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0oIp4NXKlm6",
        "outputId": "b935118b-a70a-46ae-b3a6-86364bd000d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7781, Train: 0.7438, Test: 0.5773\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5157, Train: 0.9000, Test: 0.6702\n",
            "Early stopping:  0.1855720682312843\n",
            "Epoch: 003, Loss: 2.1878, Train: 0.9062, Test: 0.7025\n",
            "Early stopping:  0.2957661203934359\n",
            "Epoch: 004, Loss: 1.8059, Train: 0.9125, Test: 0.7299\n",
            "Early stopping:  0.4202932513092133\n",
            "Epoch: 005, Loss: 1.4198, Train: 0.9437, Test: 0.7603\n",
            "Early stopping:  0.5433501357021463\n",
            "Epoch: 006, Loss: 1.0674, Train: 0.9625, Test: 0.7851\n",
            "Early stopping:  0.579634714024966\n",
            "Epoch: 007, Loss: 0.7706, Train: 0.9688, Test: 0.7925\n",
            "Early stopping:  0.5656568226307641\n",
            "Epoch: 008, Loss: 0.5426, Train: 0.9812, Test: 0.7904\n",
            "Early stopping:  0.5046114622497601\n",
            "Epoch: 009, Loss: 0.3825, Train: 0.9875, Test: 0.7894\n",
            "Early stopping:  0.41544257416037994\n",
            "Epoch: 010, Loss: 0.2720, Train: 0.9938, Test: 0.7889\n",
            "Early stopping:  0.31838275466707744\n",
            "Epoch: 011, Loss: 0.1924, Train: 1.0000, Test: 0.7895\n",
            "Early stopping:  0.23039351980536377\n",
            "Epoch: 012, Loss: 0.1347, Train: 1.0000, Test: 0.7889\n",
            "Early stopping:  0.1622002991509231\n",
            "Epoch: 013, Loss: 0.0942, Train: 1.0000, Test: 0.7871\n",
            "Early stopping:  0.11496251096894838\n",
            "Epoch: 014, Loss: 0.0664, Train: 1.0000, Test: 0.7871\n",
            "Early stopping:  0.08216821266274707\n",
            "Epoch: 015, Loss: 0.0471, Train: 1.0000, Test: 0.7866\n",
            "Early stopping:  0.05800526397385549\n",
            "Epoch: 016, Loss: 0.0336, Train: 1.0000, Test: 0.7856\n",
            "Early stopping:  0.04028991029339469\n",
            "Epoch: 017, Loss: 0.0240, Train: 1.0000, Test: 0.7837\n",
            "Early stopping:  0.02796656959688944\n",
            "Epoch: 018, Loss: 0.0175, Train: 1.0000, Test: 0.7820\n",
            "Early stopping:  0.01954356811121162\n",
            "Epoch: 019, Loss: 0.0130, Train: 1.0000, Test: 0.7790\n",
            "Early stopping:  0.01363623687235802\n",
            "Epoch: 020, Loss: 0.0100, Train: 1.0000, Test: 0.7773\n",
            "Early stopping:  0.009397682189828\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.87      0.77       392\n",
            "             ecology       0.83      0.83      0.83       879\n",
            "            economic       0.82      0.58      0.68      1399\n",
            "          geophysics       0.88      0.93      0.91      1192\n",
            "  gravitional_theory       0.63      0.95      0.75       120\n",
            "               hydro       0.73      0.85      0.79       345\n",
            "                math       0.86      0.77      0.82      1329\n",
            "              metals       0.72      0.89      0.80       191\n",
            "          networking       0.68      0.91      0.78       335\n",
            "        neuroscience       0.91      0.98      0.94       297\n",
            "        oceanography       0.87      0.80      0.83       980\n",
            "             politic       0.61      0.83      0.70       593\n",
            "           sociology       0.58      0.60      0.59       729\n",
            "software_engineering       0.78      0.90      0.84       514\n",
            "          statistics       0.72      0.68      0.70       637\n",
            "    theory_computing       0.76      0.57      0.65       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.76      0.81      0.77     10364\n",
            "        weighted avg       0.79      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7765, Train: 0.7438, Test: 0.6269\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5064, Train: 0.9000, Test: 0.7408\n",
            "Early stopping:  0.19100631429088494\n",
            "Epoch: 003, Loss: 2.1680, Train: 0.8938, Test: 0.7421\n",
            "Early stopping:  0.30489865192387017\n",
            "Epoch: 004, Loss: 1.7746, Train: 0.8938, Test: 0.7465\n",
            "Early stopping:  0.43321377649595294\n",
            "Epoch: 005, Loss: 1.3770, Train: 0.9062, Test: 0.7584\n",
            "Early stopping:  0.5599188348866125\n",
            "Epoch: 006, Loss: 1.0181, Train: 0.9125, Test: 0.7753\n",
            "Early stopping:  0.5959421610974905\n",
            "Epoch: 007, Loss: 0.7248, Train: 0.9313, Test: 0.7866\n",
            "Early stopping:  0.576974541365538\n",
            "Epoch: 008, Loss: 0.5083, Train: 0.9375, Test: 0.7958\n",
            "Early stopping:  0.5068110065841425\n",
            "Epoch: 009, Loss: 0.3618, Train: 0.9563, Test: 0.7981\n",
            "Early stopping:  0.40718678672108216\n",
            "Epoch: 010, Loss: 0.2642, Train: 0.9563, Test: 0.7966\n",
            "Early stopping:  0.3021854365653548\n",
            "Epoch: 011, Loss: 0.1944, Train: 0.9875, Test: 0.7926\n",
            "Early stopping:  0.21146183451031225\n",
            "Epoch: 012, Loss: 0.1409, Train: 0.9938, Test: 0.7883\n",
            "Early stopping:  0.14558454695440123\n",
            "Epoch: 013, Loss: 0.1005, Train: 1.0000, Test: 0.7816\n",
            "Early stopping:  0.10363502257686946\n",
            "Epoch: 014, Loss: 0.0734, Train: 1.0000, Test: 0.7776\n",
            "Early stopping:  0.07632480919978749\n",
            "Epoch: 015, Loss: 0.0564, Train: 1.0000, Test: 0.7769\n",
            "Early stopping:  0.055510613658642474\n",
            "Epoch: 016, Loss: 0.0427, Train: 1.0000, Test: 0.7787\n",
            "Early stopping:  0.03899658301830212\n",
            "Epoch: 017, Loss: 0.0305, Train: 1.0000, Test: 0.7794\n",
            "Early stopping:  0.02738092392782564\n",
            "Epoch: 018, Loss: 0.0217, Train: 1.0000, Test: 0.7799\n",
            "Early stopping:  0.02057936276478274\n",
            "Epoch: 019, Loss: 0.0165, Train: 1.0000, Test: 0.7794\n",
            "Early stopping:  0.0161771500251715\n",
            "Epoch: 020, Loss: 0.0136, Train: 1.0000, Test: 0.7784\n",
            "Early stopping:  0.011780599017336418\n",
            "Epoch: 021, Loss: 0.0116, Train: 1.0000, Test: 0.7773\n",
            "Early stopping:  0.007576818161783245\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.73      0.74       392\n",
            "             ecology       0.78      0.82      0.80       879\n",
            "            economic       0.75      0.69      0.72      1399\n",
            "          geophysics       0.95      0.85      0.90      1192\n",
            "  gravitional_theory       0.72      0.93      0.81       120\n",
            "               hydro       0.68      0.80      0.74       345\n",
            "                math       0.84      0.82      0.83      1329\n",
            "              metals       0.65      0.93      0.77       191\n",
            "          networking       0.91      0.73      0.81       335\n",
            "        neuroscience       0.94      0.97      0.95       297\n",
            "        oceanography       0.90      0.70      0.79       980\n",
            "             politic       0.67      0.80      0.73       593\n",
            "           sociology       0.60      0.68      0.64       729\n",
            "software_engineering       0.88      0.75      0.81       514\n",
            "          statistics       0.64      0.82      0.72       637\n",
            "    theory_computing       0.69      0.71      0.70       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.80      0.78     10364\n",
            "        weighted avg       0.79      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7830, Train: 0.7812, Test: 0.5566\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5009, Train: 0.9125, Test: 0.7412\n",
            "Early stopping:  0.1995029503632871\n",
            "Epoch: 003, Loss: 2.1507, Train: 0.9062, Test: 0.7552\n",
            "Early stopping:  0.3167674007925939\n",
            "Epoch: 004, Loss: 1.7500, Train: 0.9062, Test: 0.7630\n",
            "Early stopping:  0.4466299584639679\n",
            "Epoch: 005, Loss: 1.3517, Train: 0.9125, Test: 0.7756\n",
            "Early stopping:  0.5727016646541226\n",
            "Epoch: 006, Loss: 0.9942, Train: 0.9250, Test: 0.7894\n",
            "Early stopping:  0.602955457870082\n",
            "Epoch: 007, Loss: 0.7033, Train: 0.9500, Test: 0.7965\n",
            "Early stopping:  0.578352228367574\n",
            "Epoch: 008, Loss: 0.4896, Train: 0.9500, Test: 0.7974\n",
            "Early stopping:  0.5044833561977268\n",
            "Epoch: 009, Loss: 0.3445, Train: 0.9437, Test: 0.7941\n",
            "Early stopping:  0.4039039212633764\n",
            "Epoch: 010, Loss: 0.2464, Train: 0.9688, Test: 0.7924\n",
            "Early stopping:  0.29944533074919055\n",
            "Epoch: 011, Loss: 0.1764, Train: 0.9875, Test: 0.7909\n",
            "Early stopping:  0.20997223935414727\n",
            "Epoch: 012, Loss: 0.1242, Train: 1.0000, Test: 0.7907\n",
            "Early stopping:  0.14506314785547444\n",
            "Epoch: 013, Loss: 0.0855, Train: 1.0000, Test: 0.7908\n",
            "Early stopping:  0.10287756148853293\n",
            "Epoch: 014, Loss: 0.0580, Train: 1.0000, Test: 0.7890\n",
            "Early stopping:  0.07513005078622949\n",
            "Epoch: 015, Loss: 0.0394, Train: 1.0000, Test: 0.7868\n",
            "Early stopping:  0.05480712263757431\n",
            "Epoch: 016, Loss: 0.0273, Train: 1.0000, Test: 0.7860\n",
            "Early stopping:  0.038835607556662766\n",
            "Epoch: 017, Loss: 0.0195, Train: 1.0000, Test: 0.7856\n",
            "Early stopping:  0.026454370309136756\n",
            "Epoch: 018, Loss: 0.0143, Train: 1.0000, Test: 0.7836\n",
            "Early stopping:  0.01745329668317228\n",
            "Epoch: 019, Loss: 0.0108, Train: 1.0000, Test: 0.7815\n",
            "Early stopping:  0.011407172014361951\n",
            "Epoch: 020, Loss: 0.0084, Train: 1.0000, Test: 0.7802\n",
            "Early stopping:  0.0075413893249881014\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.75      0.75       392\n",
            "             ecology       0.85      0.74      0.79       879\n",
            "            economic       0.80      0.71      0.75      1399\n",
            "          geophysics       0.87      0.93      0.90      1192\n",
            "  gravitional_theory       0.92      0.88      0.90       120\n",
            "               hydro       0.73      0.83      0.78       345\n",
            "                math       0.90      0.68      0.78      1329\n",
            "              metals       0.45      0.87      0.59       191\n",
            "          networking       0.77      0.93      0.84       335\n",
            "        neuroscience       0.93      0.95      0.94       297\n",
            "        oceanography       0.80      0.88      0.84       980\n",
            "             politic       0.74      0.68      0.71       593\n",
            "           sociology       0.60      0.64      0.62       729\n",
            "software_engineering       0.86      0.87      0.87       514\n",
            "          statistics       0.70      0.77      0.73       637\n",
            "    theory_computing       0.65      0.73      0.68       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.80      0.78     10364\n",
            "        weighted avg       0.79      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7738, Train: 0.7750, Test: 0.5485\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4886, Train: 0.9375, Test: 0.7286\n",
            "Early stopping:  0.20165952030789255\n",
            "Epoch: 003, Loss: 2.1392, Train: 0.9187, Test: 0.7457\n",
            "Early stopping:  0.3178527748114299\n",
            "Epoch: 004, Loss: 1.7375, Train: 0.9313, Test: 0.7557\n",
            "Early stopping:  0.4477508584847404\n",
            "Epoch: 005, Loss: 1.3367, Train: 0.9437, Test: 0.7750\n",
            "Early stopping:  0.5745747926783276\n",
            "Epoch: 006, Loss: 0.9759, Train: 0.9563, Test: 0.7947\n",
            "Early stopping:  0.6054473958647919\n",
            "Epoch: 007, Loss: 0.6807, Train: 0.9688, Test: 0.8003\n",
            "Early stopping:  0.5827078138576532\n",
            "Epoch: 008, Loss: 0.4658, Train: 0.9625, Test: 0.7985\n",
            "Early stopping:  0.5092722259717456\n",
            "Epoch: 009, Loss: 0.3221, Train: 0.9750, Test: 0.7945\n",
            "Early stopping:  0.4073540032945607\n",
            "Epoch: 010, Loss: 0.2263, Train: 0.9812, Test: 0.7902\n",
            "Early stopping:  0.3004379755719132\n",
            "Epoch: 011, Loss: 0.1600, Train: 0.9875, Test: 0.7854\n",
            "Early stopping:  0.2078390184027293\n",
            "Epoch: 012, Loss: 0.1139, Train: 0.9938, Test: 0.7828\n",
            "Early stopping:  0.14024047460856398\n",
            "Epoch: 013, Loss: 0.0817, Train: 0.9938, Test: 0.7803\n",
            "Early stopping:  0.09585580801162423\n",
            "Epoch: 014, Loss: 0.0584, Train: 1.0000, Test: 0.7784\n",
            "Early stopping:  0.06681901933953431\n",
            "Epoch: 015, Loss: 0.0415, Train: 1.0000, Test: 0.7764\n",
            "Early stopping:  0.04709289793230723\n",
            "Epoch: 016, Loss: 0.0302, Train: 1.0000, Test: 0.7750\n",
            "Early stopping:  0.03343936586115909\n",
            "Epoch: 017, Loss: 0.0227, Train: 1.0000, Test: 0.7731\n",
            "Early stopping:  0.02366742574117955\n",
            "Epoch: 018, Loss: 0.0173, Train: 1.0000, Test: 0.7750\n",
            "Early stopping:  0.016407155107076642\n",
            "Epoch: 019, Loss: 0.0133, Train: 1.0000, Test: 0.7747\n",
            "Early stopping:  0.01120724331723654\n",
            "Epoch: 020, Loss: 0.0105, Train: 1.0000, Test: 0.7742\n",
            "Early stopping:  0.007845019785254464\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.80      0.78       392\n",
            "             ecology       0.74      0.83      0.78       879\n",
            "            economic       0.79      0.73      0.76      1399\n",
            "          geophysics       0.97      0.84      0.90      1192\n",
            "  gravitional_theory       0.75      0.93      0.83       120\n",
            "               hydro       0.72      0.83      0.77       345\n",
            "                math       0.94      0.66      0.78      1329\n",
            "              metals       0.45      0.97      0.62       191\n",
            "          networking       0.82      0.92      0.87       335\n",
            "        neuroscience       0.84      0.98      0.90       297\n",
            "        oceanography       0.86      0.73      0.79       980\n",
            "             politic       0.72      0.76      0.74       593\n",
            "           sociology       0.66      0.64      0.65       729\n",
            "software_engineering       0.80      0.78      0.79       514\n",
            "          statistics       0.64      0.88      0.74       637\n",
            "    theory_computing       0.62      0.71      0.66       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.75      0.81      0.77     10364\n",
            "        weighted avg       0.80      0.77      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7793, Train: 0.7688, Test: 0.5741\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5005, Train: 0.9125, Test: 0.6959\n",
            "Early stopping:  0.19717998466076067\n",
            "Epoch: 003, Loss: 2.1495, Train: 0.9000, Test: 0.7121\n",
            "Early stopping:  0.31561869266300485\n",
            "Epoch: 004, Loss: 1.7489, Train: 0.9250, Test: 0.7300\n",
            "Early stopping:  0.44579192719127064\n",
            "Epoch: 005, Loss: 1.3490, Train: 0.9500, Test: 0.7515\n",
            "Early stopping:  0.5725799357291248\n",
            "Epoch: 006, Loss: 0.9903, Train: 0.9688, Test: 0.7734\n",
            "Early stopping:  0.6042986593166683\n",
            "Epoch: 007, Loss: 0.6977, Train: 0.9688, Test: 0.7855\n",
            "Early stopping:  0.5801414077163485\n",
            "Epoch: 008, Loss: 0.4812, Train: 0.9750, Test: 0.7898\n",
            "Early stopping:  0.5071946914094394\n",
            "Epoch: 009, Loss: 0.3340, Train: 0.9750, Test: 0.7899\n",
            "Early stopping:  0.4069628152263612\n",
            "Epoch: 010, Loss: 0.2355, Train: 0.9938, Test: 0.7893\n",
            "Early stopping:  0.30245813940218524\n",
            "Epoch: 011, Loss: 0.1669, Train: 0.9938, Test: 0.7858\n",
            "Early stopping:  0.21186689266493194\n",
            "Epoch: 012, Loss: 0.1187, Train: 0.9938, Test: 0.7808\n",
            "Early stopping:  0.14438511382771685\n",
            "Epoch: 013, Loss: 0.0847, Train: 1.0000, Test: 0.7785\n",
            "Early stopping:  0.09937391592373464\n",
            "Epoch: 014, Loss: 0.0605, Train: 1.0000, Test: 0.7776\n",
            "Early stopping:  0.0697384684164427\n",
            "Epoch: 015, Loss: 0.0435, Train: 1.0000, Test: 0.7768\n",
            "Early stopping:  0.04919145109114861\n",
            "Epoch: 016, Loss: 0.0317, Train: 1.0000, Test: 0.7766\n",
            "Early stopping:  0.034732491675586695\n",
            "Epoch: 017, Loss: 0.0232, Train: 1.0000, Test: 0.7764\n",
            "Early stopping:  0.02449799573346787\n",
            "Epoch: 018, Loss: 0.0172, Train: 1.0000, Test: 0.7751\n",
            "Early stopping:  0.017241710581250343\n",
            "Epoch: 019, Loss: 0.0130, Train: 1.0000, Test: 0.7737\n",
            "Early stopping:  0.012187255649333886\n",
            "Epoch: 020, Loss: 0.0101, Train: 1.0000, Test: 0.7714\n",
            "Early stopping:  0.008624977596307652\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.79      0.77       392\n",
            "             ecology       0.74      0.90      0.81       879\n",
            "            economic       0.82      0.58      0.68      1399\n",
            "          geophysics       0.94      0.91      0.92      1192\n",
            "  gravitional_theory       0.90      0.94      0.92       120\n",
            "               hydro       0.85      0.79      0.82       345\n",
            "                math       0.95      0.66      0.78      1329\n",
            "              metals       0.61      0.91      0.73       191\n",
            "          networking       0.72      0.93      0.81       335\n",
            "        neuroscience       0.93      0.96      0.95       297\n",
            "        oceanography       0.89      0.75      0.81       980\n",
            "             politic       0.58      0.86      0.69       593\n",
            "           sociology       0.57      0.66      0.62       729\n",
            "software_engineering       0.83      0.78      0.80       514\n",
            "          statistics       0.64      0.82      0.72       637\n",
            "    theory_computing       0.63      0.75      0.69       432\n",
            "\n",
            "            accuracy                           0.77     10364\n",
            "           macro avg       0.77      0.81      0.78     10364\n",
            "        weighted avg       0.80      0.77      0.77     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7790, Train: 0.8125, Test: 0.5838\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4918, Train: 0.9062, Test: 0.7328\n",
            "Early stopping:  0.20303384474413577\n",
            "Epoch: 003, Loss: 2.1439, Train: 0.9187, Test: 0.7497\n",
            "Early stopping:  0.31800420199415175\n",
            "Epoch: 004, Loss: 1.7447, Train: 0.9313, Test: 0.7624\n",
            "Early stopping:  0.4466703315668017\n",
            "Epoch: 005, Loss: 1.3457, Train: 0.9375, Test: 0.7776\n",
            "Early stopping:  0.5726366708527381\n",
            "Epoch: 006, Loss: 0.9862, Train: 0.9625, Test: 0.7887\n",
            "Early stopping:  0.6024997975712361\n",
            "Epoch: 007, Loss: 0.6905, Train: 0.9688, Test: 0.7940\n",
            "Early stopping:  0.5805497435675077\n",
            "Epoch: 008, Loss: 0.4705, Train: 0.9812, Test: 0.7930\n",
            "Early stopping:  0.5096822305544947\n",
            "Epoch: 009, Loss: 0.3211, Train: 0.9875, Test: 0.7897\n",
            "Early stopping:  0.41092955647645496\n",
            "Epoch: 010, Loss: 0.2231, Train: 1.0000, Test: 0.7833\n",
            "Early stopping:  0.3061520816058481\n",
            "Epoch: 011, Loss: 0.1564, Train: 1.0000, Test: 0.7832\n",
            "Early stopping:  0.21355153546026173\n",
            "Epoch: 012, Loss: 0.1090, Train: 1.0000, Test: 0.7852\n",
            "Early stopping:  0.14394393593098473\n",
            "Epoch: 013, Loss: 0.0755, Train: 1.0000, Test: 0.7858\n",
            "Early stopping:  0.09780945673488008\n",
            "Epoch: 014, Loss: 0.0522, Train: 1.0000, Test: 0.7861\n",
            "Early stopping:  0.06821327344801668\n",
            "Epoch: 015, Loss: 0.0362, Train: 1.0000, Test: 0.7862\n",
            "Early stopping:  0.04799999432153912\n",
            "Epoch: 016, Loss: 0.0253, Train: 1.0000, Test: 0.7857\n",
            "Early stopping:  0.033420069282018176\n",
            "Epoch: 017, Loss: 0.0180, Train: 1.0000, Test: 0.7858\n",
            "Early stopping:  0.022954204704707774\n",
            "Epoch: 018, Loss: 0.0131, Train: 1.0000, Test: 0.7858\n",
            "Early stopping:  0.015608377985244663\n",
            "Epoch: 019, Loss: 0.0098, Train: 1.0000, Test: 0.7843\n",
            "Early stopping:  0.010543007750709446\n",
            "Epoch: 020, Loss: 0.0076, Train: 1.0000, Test: 0.7839\n",
            "Early stopping:  0.007101657362864216\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.84      0.77       392\n",
            "             ecology       0.85      0.81      0.83       879\n",
            "            economic       0.83      0.69      0.75      1399\n",
            "          geophysics       0.94      0.89      0.91      1192\n",
            "  gravitional_theory       0.73      0.95      0.82       120\n",
            "               hydro       0.73      0.84      0.78       345\n",
            "                math       0.96      0.59      0.73      1329\n",
            "              metals       0.40      0.96      0.57       191\n",
            "          networking       0.92      0.75      0.83       335\n",
            "        neuroscience       0.93      0.96      0.94       297\n",
            "        oceanography       0.86      0.85      0.86       980\n",
            "             politic       0.71      0.76      0.73       593\n",
            "           sociology       0.61      0.75      0.67       729\n",
            "software_engineering       0.83      0.88      0.85       514\n",
            "          statistics       0.64      0.87      0.74       637\n",
            "    theory_computing       0.69      0.73      0.71       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.82      0.78     10364\n",
            "        weighted avg       0.81      0.78      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7649, Train: 0.8125, Test: 0.6583\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4759, Train: 0.8875, Test: 0.7534\n",
            "Early stopping:  0.20435118664119228\n",
            "Epoch: 003, Loss: 2.1330, Train: 0.8938, Test: 0.7646\n",
            "Early stopping:  0.31633194187933933\n",
            "Epoch: 004, Loss: 1.7501, Train: 0.8875, Test: 0.7720\n",
            "Early stopping:  0.43814430358644124\n",
            "Epoch: 005, Loss: 1.3698, Train: 0.8875, Test: 0.7810\n",
            "Early stopping:  0.5568237401480172\n",
            "Epoch: 006, Loss: 1.0271, Train: 0.8938, Test: 0.7837\n",
            "Early stopping:  0.5789664721539304\n",
            "Epoch: 007, Loss: 0.7475, Train: 0.9000, Test: 0.7883\n",
            "Early stopping:  0.5535043636126459\n",
            "Epoch: 008, Loss: 0.5386, Train: 0.9250, Test: 0.7887\n",
            "Early stopping:  0.4845931762069115\n",
            "Epoch: 009, Loss: 0.3911, Train: 0.9437, Test: 0.7886\n",
            "Early stopping:  0.3916139141734085\n",
            "Epoch: 010, Loss: 0.2887, Train: 0.9625, Test: 0.7845\n",
            "Early stopping:  0.29519073497873577\n",
            "Epoch: 011, Loss: 0.2154, Train: 0.9875, Test: 0.7810\n",
            "Early stopping:  0.21210236825375625\n",
            "Epoch: 012, Loss: 0.1597, Train: 0.9875, Test: 0.7809\n",
            "Early stopping:  0.15036316824037677\n",
            "Epoch: 013, Loss: 0.1161, Train: 0.9875, Test: 0.7807\n",
            "Early stopping:  0.10889595691810858\n",
            "Epoch: 014, Loss: 0.0828, Train: 0.9938, Test: 0.7806\n",
            "Early stopping:  0.08173173823579095\n",
            "Epoch: 015, Loss: 0.0585, Train: 1.0000, Test: 0.7810\n",
            "Early stopping:  0.06254449484520895\n",
            "Epoch: 016, Loss: 0.0413, Train: 1.0000, Test: 0.7810\n",
            "Early stopping:  0.04727975332330175\n",
            "Epoch: 017, Loss: 0.0295, Train: 1.0000, Test: 0.7796\n",
            "Early stopping:  0.03460665093305096\n",
            "Epoch: 018, Loss: 0.0215, Train: 1.0000, Test: 0.7807\n",
            "Early stopping:  0.024501359966693614\n",
            "Epoch: 019, Loss: 0.0162, Train: 1.0000, Test: 0.7796\n",
            "Early stopping:  0.016903529848747566\n",
            "Epoch: 020, Loss: 0.0125, Train: 1.0000, Test: 0.7788\n",
            "Early stopping:  0.01148841238215022\n",
            "Epoch: 021, Loss: 0.0100, Train: 1.0000, Test: 0.7772\n",
            "Early stopping:  0.007782022927595938\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.87      0.82       392\n",
            "             ecology       0.81      0.74      0.78       879\n",
            "            economic       0.77      0.72      0.75      1399\n",
            "          geophysics       0.92      0.87      0.90      1192\n",
            "  gravitional_theory       0.72      0.95      0.82       120\n",
            "               hydro       0.56      0.90      0.69       345\n",
            "                math       0.91      0.71      0.80      1329\n",
            "              metals       0.62      0.87      0.72       191\n",
            "          networking       0.84      0.89      0.86       335\n",
            "        neuroscience       0.91      0.97      0.94       297\n",
            "        oceanography       0.82      0.79      0.81       980\n",
            "             politic       0.64      0.76      0.69       593\n",
            "           sociology       0.64      0.59      0.61       729\n",
            "software_engineering       0.84      0.79      0.81       514\n",
            "          statistics       0.66      0.87      0.75       637\n",
            "    theory_computing       0.81      0.64      0.71       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.76      0.81      0.78     10364\n",
            "        weighted avg       0.79      0.78      0.78     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7826, Train: 0.7500, Test: 0.4696\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5146, Train: 0.8500, Test: 0.6579\n",
            "Early stopping:  0.18950993258135415\n",
            "Epoch: 003, Loss: 2.1860, Train: 0.8625, Test: 0.7077\n",
            "Early stopping:  0.29884514150529495\n",
            "Epoch: 004, Loss: 1.8034, Train: 0.8875, Test: 0.7381\n",
            "Early stopping:  0.4229852501291292\n",
            "Epoch: 005, Loss: 1.4171, Train: 0.9187, Test: 0.7647\n",
            "Early stopping:  0.5457487527872724\n",
            "Epoch: 006, Loss: 1.0655, Train: 0.9313, Test: 0.7862\n",
            "Early stopping:  0.5800326443612417\n",
            "Epoch: 007, Loss: 0.7733, Train: 0.9500, Test: 0.7941\n",
            "Early stopping:  0.5642070664843944\n",
            "Epoch: 008, Loss: 0.5521, Train: 0.9563, Test: 0.7980\n",
            "Early stopping:  0.5002158931230163\n",
            "Epoch: 009, Loss: 0.3963, Train: 0.9688, Test: 0.7962\n",
            "Early stopping:  0.408670922489517\n",
            "Epoch: 010, Loss: 0.2874, Train: 0.9812, Test: 0.7942\n",
            "Early stopping:  0.3111011573581901\n",
            "Epoch: 011, Loss: 0.2094, Train: 0.9875, Test: 0.7942\n",
            "Early stopping:  0.22471536283325977\n",
            "Epoch: 012, Loss: 0.1526, Train: 0.9938, Test: 0.7946\n",
            "Early stopping:  0.1589258850809597\n",
            "Epoch: 013, Loss: 0.1107, Train: 1.0000, Test: 0.7959\n",
            "Early stopping:  0.11357489411734319\n",
            "Epoch: 014, Loss: 0.0794, Train: 1.0000, Test: 0.7959\n",
            "Early stopping:  0.08266906167581871\n",
            "Epoch: 015, Loss: 0.0564, Train: 1.0000, Test: 0.7934\n",
            "Early stopping:  0.06086292855842948\n",
            "Epoch: 016, Loss: 0.0401, Train: 1.0000, Test: 0.7914\n",
            "Early stopping:  0.04485944866875248\n",
            "Epoch: 017, Loss: 0.0291, Train: 1.0000, Test: 0.7906\n",
            "Early stopping:  0.03264700632325067\n",
            "Epoch: 018, Loss: 0.0216, Train: 1.0000, Test: 0.7893\n",
            "Early stopping:  0.023137987859531434\n",
            "Epoch: 019, Loss: 0.0164, Train: 1.0000, Test: 0.7871\n",
            "Early stopping:  0.015948973474292253\n",
            "Epoch: 020, Loss: 0.0128, Train: 1.0000, Test: 0.7861\n",
            "Early stopping:  0.010891575617941903\n",
            "Epoch: 021, Loss: 0.0101, Train: 1.0000, Test: 0.7843\n",
            "Early stopping:  0.007547966285749457\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.76      0.78       392\n",
            "             ecology       0.74      0.84      0.79       879\n",
            "            economic       0.79      0.65      0.71      1399\n",
            "          geophysics       0.93      0.91      0.92      1192\n",
            "  gravitional_theory       0.84      0.93      0.88       120\n",
            "               hydro       0.67      0.76      0.71       345\n",
            "                math       0.89      0.76      0.82      1329\n",
            "              metals       0.73      0.86      0.79       191\n",
            "          networking       0.75      0.95      0.84       335\n",
            "        neuroscience       0.93      0.98      0.95       297\n",
            "        oceanography       0.91      0.78      0.84       980\n",
            "             politic       0.71      0.83      0.76       593\n",
            "           sociology       0.57      0.66      0.61       729\n",
            "software_engineering       0.83      0.78      0.80       514\n",
            "          statistics       0.69      0.80      0.74       637\n",
            "    theory_computing       0.68      0.71      0.69       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.78      0.81      0.79     10364\n",
            "        weighted avg       0.79      0.78      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7689, Train: 0.9375, Test: 0.6767\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4509, Train: 0.9625, Test: 0.7774\n",
            "Early stopping:  0.22488327677436537\n",
            "Epoch: 003, Loss: 2.0762, Train: 0.9563, Test: 0.7857\n",
            "Early stopping:  0.346745261202467\n",
            "Epoch: 004, Loss: 1.6590, Train: 0.9500, Test: 0.7884\n",
            "Early stopping:  0.4791036380814938\n",
            "Epoch: 005, Loss: 1.2535, Train: 0.9437, Test: 0.7928\n",
            "Early stopping:  0.6052175476799979\n",
            "Epoch: 006, Loss: 0.9009, Train: 0.9500, Test: 0.7970\n",
            "Early stopping:  0.6204638977566328\n",
            "Epoch: 007, Loss: 0.6219, Train: 0.9563, Test: 0.8026\n",
            "Early stopping:  0.5815038578977852\n",
            "Epoch: 008, Loss: 0.4193, Train: 0.9688, Test: 0.8023\n",
            "Early stopping:  0.49605256835779765\n",
            "Epoch: 009, Loss: 0.2823, Train: 0.9938, Test: 0.7985\n",
            "Early stopping:  0.38922337574648985\n",
            "Epoch: 010, Loss: 0.1923, Train: 0.9938, Test: 0.7952\n",
            "Early stopping:  0.2840498768304644\n",
            "Epoch: 011, Loss: 0.1312, Train: 0.9938, Test: 0.7955\n",
            "Early stopping:  0.1961823622614154\n",
            "Epoch: 012, Loss: 0.0883, Train: 1.0000, Test: 0.7944\n",
            "Early stopping:  0.13187650185792776\n",
            "Epoch: 013, Loss: 0.0590, Train: 1.0000, Test: 0.7927\n",
            "Early stopping:  0.08906807955432995\n",
            "Epoch: 014, Loss: 0.0399, Train: 1.0000, Test: 0.7922\n",
            "Early stopping:  0.061034476308076305\n",
            "Epoch: 015, Loss: 0.0276, Train: 1.0000, Test: 0.7929\n",
            "Early stopping:  0.0415357118808067\n",
            "Epoch: 016, Loss: 0.0195, Train: 1.0000, Test: 0.7925\n",
            "Early stopping:  0.027530578967237786\n",
            "Epoch: 017, Loss: 0.0142, Train: 1.0000, Test: 0.7925\n",
            "Early stopping:  0.017914197431114876\n",
            "Epoch: 018, Loss: 0.0105, Train: 1.0000, Test: 0.7923\n",
            "Early stopping:  0.011726930680258558\n",
            "Epoch: 019, Loss: 0.0080, Train: 1.0000, Test: 0.7909\n",
            "Early stopping:  0.007810265696099756\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.79      0.74       392\n",
            "             ecology       0.86      0.74      0.79       879\n",
            "            economic       0.73      0.75      0.74      1399\n",
            "          geophysics       0.94      0.89      0.91      1192\n",
            "  gravitional_theory       0.91      0.93      0.92       120\n",
            "               hydro       0.58      0.90      0.70       345\n",
            "                math       0.94      0.69      0.80      1329\n",
            "              metals       0.79      0.87      0.83       191\n",
            "          networking       0.86      0.86      0.86       335\n",
            "        neuroscience       0.91      0.98      0.94       297\n",
            "        oceanography       0.81      0.87      0.84       980\n",
            "             politic       0.72      0.77      0.74       593\n",
            "           sociology       0.65      0.65      0.65       729\n",
            "software_engineering       0.80      0.90      0.85       514\n",
            "          statistics       0.72      0.79      0.75       637\n",
            "    theory_computing       0.74      0.70      0.72       432\n",
            "\n",
            "            accuracy                           0.79     10364\n",
            "           macro avg       0.79      0.82      0.80     10364\n",
            "        weighted avg       0.80      0.79      0.79     10364\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7805, Train: 0.7000, Test: 0.5144\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4764, Train: 0.9062, Test: 0.7607\n",
            "Early stopping:  0.2150268147816077\n",
            "Epoch: 003, Loss: 2.1157, Train: 0.9125, Test: 0.7733\n",
            "Early stopping:  0.3327835882997175\n",
            "Epoch: 004, Loss: 1.7099, Train: 0.9313, Test: 0.7780\n",
            "Early stopping:  0.4621122424821158\n",
            "Epoch: 005, Loss: 1.3094, Train: 0.9375, Test: 0.7884\n",
            "Early stopping:  0.5873259484968337\n",
            "Epoch: 006, Loss: 0.9553, Train: 0.9437, Test: 0.7961\n",
            "Early stopping:  0.6086559206436296\n",
            "Epoch: 007, Loss: 0.6720, Train: 0.9500, Test: 0.7981\n",
            "Early stopping:  0.5772475396790717\n",
            "Epoch: 008, Loss: 0.4665, Train: 0.9625, Test: 0.7933\n",
            "Early stopping:  0.4978336366456588\n",
            "Epoch: 009, Loss: 0.3265, Train: 0.9750, Test: 0.7862\n",
            "Early stopping:  0.39395662222456984\n",
            "Epoch: 010, Loss: 0.2305, Train: 0.9812, Test: 0.7845\n",
            "Early stopping:  0.289938230546431\n",
            "Epoch: 011, Loss: 0.1626, Train: 1.0000, Test: 0.7847\n",
            "Early stopping:  0.2030400887199019\n",
            "Epoch: 012, Loss: 0.1144, Train: 1.0000, Test: 0.7873\n",
            "Early stopping:  0.1401953509879529\n",
            "Epoch: 013, Loss: 0.0808, Train: 1.0000, Test: 0.7882\n",
            "Early stopping:  0.09799564877475019\n",
            "Epoch: 014, Loss: 0.0569, Train: 1.0000, Test: 0.7896\n",
            "Early stopping:  0.06920989658363322\n",
            "Epoch: 015, Loss: 0.0398, Train: 1.0000, Test: 0.7901\n",
            "Early stopping:  0.0488852952815088\n",
            "Epoch: 016, Loss: 0.0282, Train: 1.0000, Test: 0.7894\n",
            "Early stopping:  0.03442807025603697\n",
            "Epoch: 017, Loss: 0.0204, Train: 1.0000, Test: 0.7887\n",
            "Early stopping:  0.024176453318844684\n",
            "Epoch: 018, Loss: 0.0151, Train: 1.0000, Test: 0.7866\n",
            "Early stopping:  0.01669637281869843\n",
            "Epoch: 019, Loss: 0.0115, Train: 1.0000, Test: 0.7855\n",
            "Early stopping:  0.011307200770418344\n",
            "Epoch: 020, Loss: 0.0089, Train: 1.0000, Test: 0.7849\n",
            "Early stopping:  0.007667431736366764\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.68      0.68       392\n",
            "             ecology       0.85      0.81      0.83       879\n",
            "            economic       0.83      0.61      0.70      1399\n",
            "          geophysics       0.90      0.90      0.90      1192\n",
            "  gravitional_theory       0.79      0.93      0.85       120\n",
            "               hydro       0.76      0.77      0.76       345\n",
            "                math       0.91      0.75      0.82      1329\n",
            "              metals       0.57      0.92      0.71       191\n",
            "          networking       0.82      0.82      0.82       335\n",
            "        neuroscience       0.94      0.97      0.96       297\n",
            "        oceanography       0.83      0.91      0.87       980\n",
            "             politic       0.73      0.77      0.75       593\n",
            "           sociology       0.54      0.70      0.61       729\n",
            "software_engineering       0.85      0.82      0.84       514\n",
            "          statistics       0.70      0.76      0.73       637\n",
            "    theory_computing       0.64      0.83      0.72       432\n",
            "\n",
            "            accuracy                           0.78     10364\n",
            "           macro avg       0.77      0.81      0.78     10364\n",
            "        weighted avg       0.80      0.78      0.79     10364\n",
            "\n",
            "time: 9.81 s (started: 2024-08-16 14:16:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUyJm6ZlKlm6",
        "outputId": "0fc26191-324a-45c5-db9a-09c0918c14e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 430 ms (started: 2024-08-16 14:16:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 20 ❎"
      ],
      "metadata": {
        "id": "BlYuEz3bKlm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "kd4PzheiKlm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAV3H3Z5Klm6",
        "outputId": "bd21bd3c-f6cc-4206-cbad-93ed9b737455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.7181, Train: 0.7156, Test: 0.5747\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1178, Train: 0.8469, Test: 0.7544\n",
            "Early stopping:  1.838644043701684\n",
            "Epoch: 003, Loss: 0.5799, Train: 0.8875, Test: 0.7926\n",
            "Early stopping:  1.6782114427483215\n",
            "Epoch: 004, Loss: 0.3312, Train: 0.9219, Test: 0.7905\n",
            "Early stopping:  1.5559067449060915\n",
            "Epoch: 005, Loss: 0.2099, Train: 0.9375, Test: 0.7839\n",
            "Early stopping:  1.4548770497208448\n",
            "Epoch: 006, Loss: 0.1939, Train: 0.9531, Test: 0.7855\n",
            "Early stopping:  0.3852080489587829\n",
            "Epoch: 007, Loss: 0.1375, Train: 0.9719, Test: 0.7902\n",
            "Early stopping:  0.176523814848216\n",
            "Epoch: 008, Loss: 0.0763, Train: 0.9812, Test: 0.7918\n",
            "Early stopping:  0.09488459365434715\n",
            "Epoch: 009, Loss: 0.0520, Train: 0.9875, Test: 0.7890\n",
            "Early stopping:  0.06966326106104957\n",
            "Epoch: 010, Loss: 0.0423, Train: 0.9969, Test: 0.7865\n",
            "Early stopping:  0.06406798051526205\n",
            "Epoch: 011, Loss: 0.0321, Train: 0.9969, Test: 0.7817\n",
            "Early stopping:  0.04215616896694689\n",
            "Epoch: 012, Loss: 0.0235, Train: 0.9969, Test: 0.7781\n",
            "Early stopping:  0.020405010985201297\n",
            "Epoch: 013, Loss: 0.0162, Train: 1.0000, Test: 0.7770\n",
            "Early stopping:  0.01434303686206773\n",
            "Epoch: 014, Loss: 0.0106, Train: 1.0000, Test: 0.7745\n",
            "Early stopping:  0.012623053036769489\n",
            "Epoch: 015, Loss: 0.0082, Train: 0.9969, Test: 0.7748\n",
            "Early stopping:  0.009789249293013513\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.79      0.74       382\n",
            "             ecology       0.85      0.84      0.84       869\n",
            "            economic       0.83      0.63      0.72      1389\n",
            "          geophysics       0.96      0.85      0.90      1182\n",
            "  gravitional_theory       0.68      0.95      0.80       110\n",
            "               hydro       0.68      0.81      0.74       335\n",
            "                math       0.92      0.71      0.80      1319\n",
            "              metals       0.59      0.88      0.71       181\n",
            "          networking       0.65      0.74      0.69       325\n",
            "        neuroscience       0.96      0.95      0.95       287\n",
            "        oceanography       0.86      0.83      0.85       970\n",
            "             politic       0.76      0.69      0.73       583\n",
            "           sociology       0.56      0.71      0.63       719\n",
            "software_engineering       0.83      0.79      0.81       504\n",
            "          statistics       0.62      0.87      0.72       627\n",
            "    theory_computing       0.61      0.81      0.70       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.75      0.80      0.77     10204\n",
            "        weighted avg       0.80      0.77      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5265, Train: 0.6031, Test: 0.5124\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.3707, Train: 0.8344, Test: 0.7410\n",
            "Early stopping:  1.5243593776281923\n",
            "Epoch: 003, Loss: 0.5927, Train: 0.8688, Test: 0.7620\n",
            "Early stopping:  1.519851617569639\n",
            "Epoch: 004, Loss: 0.3988, Train: 0.9219, Test: 0.7700\n",
            "Early stopping:  1.4324847251094224\n",
            "Epoch: 005, Loss: 0.2602, Train: 0.9406, Test: 0.7793\n",
            "Early stopping:  1.3538060647809804\n",
            "Epoch: 006, Loss: 0.1808, Train: 0.9500, Test: 0.7848\n",
            "Early stopping:  0.47905888504493416\n",
            "Epoch: 007, Loss: 0.1447, Train: 0.9625, Test: 0.7849\n",
            "Early stopping:  0.18314108029034087\n",
            "Epoch: 008, Loss: 0.0995, Train: 0.9875, Test: 0.7859\n",
            "Early stopping:  0.11754506097558282\n",
            "Epoch: 009, Loss: 0.0582, Train: 0.9906, Test: 0.7859\n",
            "Early stopping:  0.07760255450132715\n",
            "Epoch: 010, Loss: 0.0342, Train: 0.9938, Test: 0.7870\n",
            "Early stopping:  0.060292585848767774\n",
            "Epoch: 011, Loss: 0.0295, Train: 0.9938, Test: 0.7851\n",
            "Early stopping:  0.04863636748779579\n",
            "Epoch: 012, Loss: 0.0271, Train: 0.9938, Test: 0.7855\n",
            "Early stopping:  0.030454441009354714\n",
            "Epoch: 013, Loss: 0.0170, Train: 1.0000, Test: 0.7859\n",
            "Early stopping:  0.015312856506111208\n",
            "Epoch: 014, Loss: 0.0093, Train: 1.0000, Test: 0.7836\n",
            "Early stopping:  0.010087936200263433\n",
            "Epoch: 015, Loss: 0.0067, Train: 1.0000, Test: 0.7824\n",
            "Early stopping:  0.010254360554101123\n",
            "Epoch: 016, Loss: 0.0068, Train: 0.9969, Test: 0.7792\n",
            "Early stopping:  0.008780843921273014\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.74      0.74       382\n",
            "             ecology       0.84      0.76      0.80       869\n",
            "            economic       0.79      0.65      0.71      1389\n",
            "          geophysics       0.94      0.84      0.89      1182\n",
            "  gravitional_theory       0.69      0.95      0.80       110\n",
            "               hydro       0.57      0.87      0.69       335\n",
            "                math       0.90      0.73      0.81      1319\n",
            "              metals       0.57      0.90      0.70       181\n",
            "          networking       0.71      0.94      0.81       325\n",
            "        neuroscience       0.93      0.98      0.96       287\n",
            "        oceanography       0.82      0.88      0.85       970\n",
            "             politic       0.76      0.72      0.74       583\n",
            "           sociology       0.63      0.67      0.65       719\n",
            "software_engineering       0.82      0.88      0.85       504\n",
            "          statistics       0.67      0.84      0.75       627\n",
            "    theory_computing       0.68      0.70      0.69       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.75      0.81      0.78     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2525, Train: 0.6750, Test: 0.6055\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0647, Train: 0.8375, Test: 0.7827\n",
            "Early stopping:  1.5470186192057103\n",
            "Epoch: 003, Loss: 0.5813, Train: 0.8562, Test: 0.7805\n",
            "Early stopping:  1.4233416935924152\n",
            "Epoch: 004, Loss: 0.4308, Train: 0.9094, Test: 0.7906\n",
            "Early stopping:  1.308358962526467\n",
            "Epoch: 005, Loss: 0.2787, Train: 0.9531, Test: 0.7976\n",
            "Early stopping:  1.2271352026409608\n",
            "Epoch: 006, Loss: 0.1770, Train: 0.9594, Test: 0.7897\n",
            "Early stopping:  0.34759826541302546\n",
            "Epoch: 007, Loss: 0.1480, Train: 0.9719, Test: 0.7883\n",
            "Early stopping:  0.18181321068848244\n",
            "Epoch: 008, Loss: 0.1145, Train: 0.9812, Test: 0.7885\n",
            "Early stopping:  0.12802502111521843\n",
            "Epoch: 009, Loss: 0.0793, Train: 0.9906, Test: 0.7851\n",
            "Early stopping:  0.07602371866606171\n",
            "Epoch: 010, Loss: 0.0434, Train: 0.9938, Test: 0.7813\n",
            "Early stopping:  0.05318309585853979\n",
            "Epoch: 011, Loss: 0.0278, Train: 0.9938, Test: 0.7799\n",
            "Early stopping:  0.0496241809648834\n",
            "Epoch: 012, Loss: 0.0254, Train: 0.9969, Test: 0.7793\n",
            "Early stopping:  0.038198280919667614\n",
            "Epoch: 013, Loss: 0.0245, Train: 0.9938, Test: 0.7783\n",
            "Early stopping:  0.02321706925259167\n",
            "Epoch: 014, Loss: 0.0182, Train: 0.9969, Test: 0.7769\n",
            "Early stopping:  0.009370515056118424\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.77      0.77       382\n",
            "             ecology       0.77      0.88      0.82       869\n",
            "            economic       0.80      0.73      0.77      1389\n",
            "          geophysics       0.85      0.87      0.86      1182\n",
            "  gravitional_theory       0.67      0.95      0.79       110\n",
            "               hydro       0.60      0.79      0.68       335\n",
            "                math       0.87      0.68      0.76      1319\n",
            "              metals       0.51      0.92      0.66       181\n",
            "          networking       0.77      0.92      0.84       325\n",
            "        neuroscience       0.94      0.97      0.95       287\n",
            "        oceanography       0.89      0.78      0.83       970\n",
            "             politic       0.68      0.77      0.72       583\n",
            "           sociology       0.64      0.64      0.64       719\n",
            "software_engineering       0.86      0.85      0.85       504\n",
            "          statistics       0.72      0.76      0.74       627\n",
            "    theory_computing       0.74      0.58      0.65       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.76      0.80      0.77     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1557, Train: 0.7469, Test: 0.7285\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9582, Train: 0.8625, Test: 0.7617\n",
            "Early stopping:  1.5538705166618232\n",
            "Epoch: 003, Loss: 0.4809, Train: 0.8812, Test: 0.7447\n",
            "Early stopping:  1.4266052741541833\n",
            "Epoch: 004, Loss: 0.3536, Train: 0.9187, Test: 0.7544\n",
            "Early stopping:  1.3052690231062511\n",
            "Epoch: 005, Loss: 0.2365, Train: 0.9313, Test: 0.7522\n",
            "Early stopping:  1.2157557374165124\n",
            "Epoch: 006, Loss: 0.1765, Train: 0.9625, Test: 0.7454\n",
            "Early stopping:  0.31167392356895446\n",
            "Epoch: 007, Loss: 0.1122, Train: 0.9906, Test: 0.7284\n",
            "Early stopping:  0.14682836329158258\n",
            "Epoch: 008, Loss: 0.0644, Train: 0.9969, Test: 0.7197\n",
            "Early stopping:  0.11282339223361113\n",
            "Epoch: 009, Loss: 0.0395, Train: 0.9938, Test: 0.7181\n",
            "Early stopping:  0.08093484875259588\n",
            "Epoch: 010, Loss: 0.0305, Train: 0.9969, Test: 0.7252\n",
            "Early stopping:  0.060353615981593216\n",
            "Epoch: 011, Loss: 0.0204, Train: 0.9969, Test: 0.7313\n",
            "Early stopping:  0.03671630437238963\n",
            "Epoch: 012, Loss: 0.0136, Train: 1.0000, Test: 0.7346\n",
            "Early stopping:  0.019801855746707606\n",
            "Epoch: 013, Loss: 0.0090, Train: 1.0000, Test: 0.7378\n",
            "Early stopping:  0.012424461916302044\n",
            "Epoch: 014, Loss: 0.0066, Train: 1.0000, Test: 0.7390\n",
            "Early stopping:  0.009626455945742613\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.57      0.85      0.69       382\n",
            "             ecology       0.82      0.75      0.78       869\n",
            "            economic       0.81      0.67      0.73      1389\n",
            "          geophysics       0.93      0.86      0.89      1182\n",
            "  gravitional_theory       0.23      0.97      0.38       110\n",
            "               hydro       0.60      0.87      0.71       335\n",
            "                math       0.87      0.57      0.69      1319\n",
            "              metals       0.47      0.94      0.62       181\n",
            "          networking       0.91      0.75      0.82       325\n",
            "        neuroscience       0.80      0.99      0.88       287\n",
            "        oceanography       0.87      0.85      0.86       970\n",
            "             politic       0.65      0.59      0.62       583\n",
            "           sociology       0.57      0.66      0.61       719\n",
            "software_engineering       0.91      0.61      0.73       504\n",
            "          statistics       0.80      0.79      0.79       627\n",
            "    theory_computing       0.61      0.79      0.69       422\n",
            "\n",
            "            accuracy                           0.74     10204\n",
            "           macro avg       0.71      0.78      0.72     10204\n",
            "        weighted avg       0.78      0.74      0.75     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.2637, Train: 0.7625, Test: 0.7054\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9837, Train: 0.8688, Test: 0.7612\n",
            "Early stopping:  1.6122275488721893\n",
            "Epoch: 003, Loss: 0.4264, Train: 0.8969, Test: 0.7761\n",
            "Early stopping:  1.5032748597120917\n",
            "Epoch: 004, Loss: 0.2874, Train: 0.9406, Test: 0.7820\n",
            "Early stopping:  1.3820773715197834\n",
            "Epoch: 005, Loss: 0.1902, Train: 0.9594, Test: 0.7813\n",
            "Early stopping:  1.2857510754402957\n",
            "Epoch: 006, Loss: 0.1144, Train: 0.9750, Test: 0.7831\n",
            "Early stopping:  0.34626987770917417\n",
            "Epoch: 007, Loss: 0.0791, Train: 0.9938, Test: 0.7830\n",
            "Early stopping:  0.1405614967684409\n",
            "Epoch: 008, Loss: 0.0529, Train: 0.9969, Test: 0.7806\n",
            "Early stopping:  0.0949573268780517\n",
            "Epoch: 009, Loss: 0.0341, Train: 1.0000, Test: 0.7807\n",
            "Early stopping:  0.06158219932159869\n",
            "Epoch: 010, Loss: 0.0197, Train: 1.0000, Test: 0.7792\n",
            "Early stopping:  0.03768014183064504\n",
            "Epoch: 011, Loss: 0.0105, Train: 1.0000, Test: 0.7819\n",
            "Early stopping:  0.027453209204569392\n",
            "Epoch: 012, Loss: 0.0082, Train: 1.0000, Test: 0.7794\n",
            "Early stopping:  0.01856977644105798\n",
            "Epoch: 013, Loss: 0.0068, Train: 1.0000, Test: 0.7764\n",
            "Early stopping:  0.011358215136759645\n",
            "Epoch: 014, Loss: 0.0051, Train: 1.0000, Test: 0.7752\n",
            "Early stopping:  0.005731594895621994\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.62      0.86      0.72       382\n",
            "             ecology       0.77      0.85      0.81       869\n",
            "            economic       0.83      0.64      0.73      1389\n",
            "          geophysics       0.94      0.87      0.90      1182\n",
            "  gravitional_theory       0.68      0.95      0.79       110\n",
            "               hydro       0.62      0.80      0.70       335\n",
            "                math       0.90      0.76      0.83      1319\n",
            "              metals       0.51      0.96      0.66       181\n",
            "          networking       0.79      0.80      0.80       325\n",
            "        neuroscience       0.92      0.98      0.95       287\n",
            "        oceanography       0.87      0.78      0.82       970\n",
            "             politic       0.66      0.74      0.70       583\n",
            "           sociology       0.60      0.60      0.60       719\n",
            "software_engineering       0.85      0.84      0.85       504\n",
            "          statistics       0.74      0.78      0.76       627\n",
            "    theory_computing       0.62      0.71      0.66       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.75      0.81      0.77     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0670, Train: 0.7594, Test: 0.6389\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0163, Train: 0.8313, Test: 0.7605\n",
            "Early stopping:  1.4500893812940532\n",
            "Epoch: 003, Loss: 0.4924, Train: 0.8875, Test: 0.7561\n",
            "Early stopping:  1.3606953885415558\n",
            "Epoch: 004, Loss: 0.3422, Train: 0.9031, Test: 0.7587\n",
            "Early stopping:  1.2586468506904482\n",
            "Epoch: 005, Loss: 0.2250, Train: 0.9313, Test: 0.7698\n",
            "Early stopping:  1.1789565936190234\n",
            "Epoch: 006, Loss: 0.1560, Train: 0.9406, Test: 0.7593\n",
            "Early stopping:  0.34318029814196005\n",
            "Epoch: 007, Loss: 0.1521, Train: 0.9688, Test: 0.7687\n",
            "Early stopping:  0.14450470443268765\n",
            "Epoch: 008, Loss: 0.1028, Train: 0.9812, Test: 0.7756\n",
            "Early stopping:  0.09278100814207965\n",
            "Epoch: 009, Loss: 0.0810, Train: 0.9812, Test: 0.7759\n",
            "Early stopping:  0.05574772047596211\n",
            "Epoch: 010, Loss: 0.0565, Train: 0.9812, Test: 0.7674\n",
            "Early stopping:  0.04371701150878883\n",
            "Epoch: 011, Loss: 0.0394, Train: 0.9875, Test: 0.7609\n",
            "Early stopping:  0.04392534891088311\n",
            "Epoch: 012, Loss: 0.0314, Train: 0.9875, Test: 0.7596\n",
            "Early stopping:  0.029600088693662475\n",
            "Epoch: 013, Loss: 0.0354, Train: 1.0000, Test: 0.7573\n",
            "Early stopping:  0.020400660282511553\n",
            "Epoch: 014, Loss: 0.0126, Train: 1.0000, Test: 0.7569\n",
            "Early stopping:  0.015757520612612756\n",
            "Epoch: 015, Loss: 0.0117, Train: 0.9938, Test: 0.7562\n",
            "Early stopping:  0.013059346221876857\n",
            "Epoch: 016, Loss: 0.0167, Train: 0.9969, Test: 0.7569\n",
            "Early stopping:  0.011075487492016702\n",
            "Epoch: 017, Loss: 0.0117, Train: 1.0000, Test: 0.7559\n",
            "Early stopping:  0.010151076329523244\n",
            "Epoch: 018, Loss: 0.0068, Train: 0.9969, Test: 0.7553\n",
            "Early stopping:  0.0035305730759872234\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.77      0.82      0.80       382\n",
            "             ecology       0.89      0.69      0.78       869\n",
            "            economic       0.75      0.63      0.69      1389\n",
            "          geophysics       0.97      0.85      0.90      1182\n",
            "  gravitional_theory       0.45      0.95      0.61       110\n",
            "               hydro       0.59      0.84      0.69       335\n",
            "                math       0.80      0.69      0.74      1319\n",
            "              metals       0.58      0.91      0.71       181\n",
            "          networking       0.68      0.90      0.77       325\n",
            "        neuroscience       0.92      0.98      0.95       287\n",
            "        oceanography       0.76      0.90      0.83       970\n",
            "             politic       0.56      0.83      0.67       583\n",
            "           sociology       0.57      0.59      0.58       719\n",
            "software_engineering       0.92      0.79      0.85       504\n",
            "          statistics       0.87      0.66      0.75       627\n",
            "    theory_computing       0.72      0.68      0.70       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.74      0.79      0.75     10204\n",
            "        weighted avg       0.78      0.76      0.76     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4578, Train: 0.6781, Test: 0.5769\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1099, Train: 0.8344, Test: 0.7270\n",
            "Early stopping:  1.6602723430700008\n",
            "Epoch: 003, Loss: 0.5544, Train: 0.8406, Test: 0.7461\n",
            "Early stopping:  1.5411804545956898\n",
            "Epoch: 004, Loss: 0.4390, Train: 0.8906, Test: 0.7881\n",
            "Early stopping:  1.4091532872373818\n",
            "Epoch: 005, Loss: 0.2693, Train: 0.9187, Test: 0.7979\n",
            "Early stopping:  1.319320881327669\n",
            "Epoch: 006, Loss: 0.1978, Train: 0.9344, Test: 0.7931\n",
            "Early stopping:  0.3613034434269888\n",
            "Epoch: 007, Loss: 0.1602, Train: 0.9406, Test: 0.7882\n",
            "Early stopping:  0.1673655353573914\n",
            "Epoch: 008, Loss: 0.1162, Train: 0.9812, Test: 0.7838\n",
            "Early stopping:  0.12633625441430557\n",
            "Epoch: 009, Loss: 0.0707, Train: 0.9906, Test: 0.7848\n",
            "Early stopping:  0.07621143749219833\n",
            "Epoch: 010, Loss: 0.0465, Train: 0.9875, Test: 0.7845\n",
            "Early stopping:  0.06227504399809054\n",
            "Epoch: 011, Loss: 0.0436, Train: 0.9844, Test: 0.7829\n",
            "Early stopping:  0.05001810095651082\n",
            "Epoch: 012, Loss: 0.0382, Train: 0.9938, Test: 0.7806\n",
            "Early stopping:  0.03224536861649936\n",
            "Epoch: 013, Loss: 0.0250, Train: 1.0000, Test: 0.7785\n",
            "Early stopping:  0.01666477724294083\n",
            "Epoch: 014, Loss: 0.0164, Train: 0.9969, Test: 0.7761\n",
            "Early stopping:  0.012806190633312354\n",
            "Epoch: 015, Loss: 0.0135, Train: 1.0000, Test: 0.7764\n",
            "Early stopping:  0.013206958513708225\n",
            "Epoch: 016, Loss: 0.0114, Train: 1.0000, Test: 0.7770\n",
            "Early stopping:  0.010959075759821262\n",
            "Epoch: 017, Loss: 0.0099, Train: 1.0000, Test: 0.7766\n",
            "Early stopping:  0.005966588632468834\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.76      0.74       382\n",
            "             ecology       0.88      0.82      0.85       869\n",
            "            economic       0.78      0.62      0.69      1389\n",
            "          geophysics       0.90      0.89      0.89      1182\n",
            "  gravitional_theory       0.53      0.95      0.68       110\n",
            "               hydro       0.61      0.88      0.72       335\n",
            "                math       0.91      0.75      0.82      1319\n",
            "              metals       0.63      0.88      0.74       181\n",
            "          networking       0.79      0.90      0.84       325\n",
            "        neuroscience       0.98      0.92      0.95       287\n",
            "        oceanography       0.87      0.85      0.86       970\n",
            "             politic       0.58      0.70      0.64       583\n",
            "           sociology       0.60      0.65      0.63       719\n",
            "software_engineering       0.72      0.91      0.80       504\n",
            "          statistics       0.75      0.76      0.75       627\n",
            "    theory_computing       0.75      0.66      0.70       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.75      0.81      0.77     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3067, Train: 0.6656, Test: 0.5023\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0607, Train: 0.8594, Test: 0.6940\n",
            "Early stopping:  1.588127812306266\n",
            "Epoch: 003, Loss: 0.5247, Train: 0.9031, Test: 0.7626\n",
            "Early stopping:  1.475969980238771\n",
            "Epoch: 004, Loss: 0.3890, Train: 0.9219, Test: 0.7844\n",
            "Early stopping:  1.3556410539745594\n",
            "Epoch: 005, Loss: 0.2850, Train: 0.9625, Test: 0.7785\n",
            "Early stopping:  1.2620159990149735\n",
            "Epoch: 006, Loss: 0.1927, Train: 0.9500, Test: 0.7558\n",
            "Early stopping:  0.34189068115384075\n",
            "Epoch: 007, Loss: 0.1508, Train: 0.9531, Test: 0.7455\n",
            "Early stopping:  0.15172377424694358\n",
            "Epoch: 008, Loss: 0.1143, Train: 0.9750, Test: 0.7541\n",
            "Early stopping:  0.11101781438330703\n",
            "Epoch: 009, Loss: 0.0660, Train: 0.9906, Test: 0.7622\n",
            "Early stopping:  0.08319353694810094\n",
            "Epoch: 010, Loss: 0.0367, Train: 0.9938, Test: 0.7650\n",
            "Early stopping:  0.0628490237125322\n",
            "Epoch: 011, Loss: 0.0304, Train: 0.9938, Test: 0.7651\n",
            "Early stopping:  0.05176163499196604\n",
            "Epoch: 012, Loss: 0.0262, Train: 0.9969, Test: 0.7683\n",
            "Early stopping:  0.03677640217613778\n",
            "Epoch: 013, Loss: 0.0177, Train: 1.0000, Test: 0.7641\n",
            "Early stopping:  0.01843661766192604\n",
            "Epoch: 014, Loss: 0.0121, Train: 1.0000, Test: 0.7610\n",
            "Early stopping:  0.009822152273153839\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.78      0.75       382\n",
            "             ecology       0.83      0.73      0.78       869\n",
            "            economic       0.74      0.68      0.71      1389\n",
            "          geophysics       0.98      0.81      0.89      1182\n",
            "  gravitional_theory       0.35      0.95      0.51       110\n",
            "               hydro       0.81      0.80      0.81       335\n",
            "                math       0.84      0.69      0.76      1319\n",
            "              metals       0.54      0.94      0.69       181\n",
            "          networking       0.89      0.82      0.85       325\n",
            "        neuroscience       0.89      0.98      0.93       287\n",
            "        oceanography       0.75      0.87      0.81       970\n",
            "             politic       0.79      0.68      0.73       583\n",
            "           sociology       0.52      0.71      0.60       719\n",
            "software_engineering       0.87      0.80      0.83       504\n",
            "          statistics       0.77      0.68      0.73       627\n",
            "    theory_computing       0.68      0.81      0.74       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.75      0.80      0.76     10204\n",
            "        weighted avg       0.79      0.76      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3296, Train: 0.7594, Test: 0.6687\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9617, Train: 0.8531, Test: 0.7545\n",
            "Early stopping:  1.6743723605050778\n",
            "Epoch: 003, Loss: 0.4707, Train: 0.8875, Test: 0.7482\n",
            "Early stopping:  1.5286976270151074\n",
            "Epoch: 004, Loss: 0.3416, Train: 0.9187, Test: 0.7454\n",
            "Early stopping:  1.3949536599005643\n",
            "Epoch: 005, Loss: 0.2553, Train: 0.9406, Test: 0.7687\n",
            "Early stopping:  1.2914121151381162\n",
            "Epoch: 006, Loss: 0.1671, Train: 0.9500, Test: 0.7699\n",
            "Early stopping:  0.3127970999979772\n",
            "Epoch: 007, Loss: 0.1326, Train: 0.9719, Test: 0.7754\n",
            "Early stopping:  0.1369498870914764\n",
            "Epoch: 008, Loss: 0.0817, Train: 0.9812, Test: 0.7696\n",
            "Early stopping:  0.10325103066599384\n",
            "Epoch: 009, Loss: 0.0582, Train: 0.9906, Test: 0.7570\n",
            "Early stopping:  0.07773076049035106\n",
            "Epoch: 010, Loss: 0.0504, Train: 0.9938, Test: 0.7547\n",
            "Early stopping:  0.050227449743771614\n",
            "Epoch: 011, Loss: 0.0328, Train: 1.0000, Test: 0.7598\n",
            "Early stopping:  0.03859035870469522\n",
            "Epoch: 012, Loss: 0.0187, Train: 0.9969, Test: 0.7606\n",
            "Early stopping:  0.02414320490910195\n",
            "Epoch: 013, Loss: 0.0165, Train: 0.9969, Test: 0.7606\n",
            "Early stopping:  0.018617863246250456\n",
            "Epoch: 014, Loss: 0.0155, Train: 0.9969, Test: 0.7608\n",
            "Early stopping:  0.014930268440541884\n",
            "Epoch: 015, Loss: 0.0121, Train: 1.0000, Test: 0.7628\n",
            "Early stopping:  0.008025212484150215\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.83      0.77       382\n",
            "             ecology       0.76      0.86      0.81       869\n",
            "            economic       0.80      0.72      0.75      1389\n",
            "          geophysics       0.95      0.82      0.88      1182\n",
            "  gravitional_theory       0.30      0.98      0.46       110\n",
            "               hydro       0.70      0.81      0.75       335\n",
            "                math       0.87      0.60      0.71      1319\n",
            "              metals       0.43      0.96      0.60       181\n",
            "          networking       0.89      0.82      0.85       325\n",
            "        neuroscience       0.97      0.93      0.95       287\n",
            "        oceanography       0.87      0.79      0.83       970\n",
            "             politic       0.64      0.73      0.68       583\n",
            "           sociology       0.64      0.67      0.66       719\n",
            "software_engineering       0.81      0.88      0.85       504\n",
            "          statistics       0.74      0.75      0.74       627\n",
            "    theory_computing       0.74      0.69      0.71       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.74      0.80      0.75     10204\n",
            "        weighted avg       0.79      0.76      0.77     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0467, Train: 0.7094, Test: 0.6152\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1803, Train: 0.8656, Test: 0.7544\n",
            "Early stopping:  1.319737271044772\n",
            "Epoch: 003, Loss: 0.5366, Train: 0.8781, Test: 0.7932\n",
            "Early stopping:  1.3037204823413469\n",
            "Epoch: 004, Loss: 0.3504, Train: 0.9031, Test: 0.7911\n",
            "Early stopping:  1.231226847476745\n",
            "Epoch: 005, Loss: 0.2790, Train: 0.9437, Test: 0.7891\n",
            "Early stopping:  1.1561746620116249\n",
            "Epoch: 006, Loss: 0.1867, Train: 0.9594, Test: 0.7884\n",
            "Early stopping:  0.39788879325769544\n",
            "Epoch: 007, Loss: 0.1214, Train: 0.9812, Test: 0.7886\n",
            "Early stopping:  0.16090142174744887\n",
            "Epoch: 008, Loss: 0.0721, Train: 0.9875, Test: 0.7856\n",
            "Early stopping:  0.1135392417271691\n",
            "Epoch: 009, Loss: 0.0576, Train: 0.9875, Test: 0.7778\n",
            "Early stopping:  0.09109921295872181\n",
            "Epoch: 010, Loss: 0.0507, Train: 0.9938, Test: 0.7734\n",
            "Early stopping:  0.056920285090659065\n",
            "Epoch: 011, Loss: 0.0342, Train: 0.9969, Test: 0.7674\n",
            "Early stopping:  0.03324648848685206\n",
            "Epoch: 012, Loss: 0.0198, Train: 1.0000, Test: 0.7640\n",
            "Early stopping:  0.020366256691707274\n",
            "Epoch: 013, Loss: 0.0130, Train: 1.0000, Test: 0.7639\n",
            "Early stopping:  0.01916917056638259\n",
            "Epoch: 014, Loss: 0.0110, Train: 0.9969, Test: 0.7623\n",
            "Early stopping:  0.01664730100829593\n",
            "Epoch: 015, Loss: 0.0107, Train: 1.0000, Test: 0.7646\n",
            "Early stopping:  0.009894363991115738\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.82      0.76      0.79       382\n",
            "             ecology       0.86      0.77      0.81       869\n",
            "            economic       0.77      0.62      0.69      1389\n",
            "          geophysics       0.93      0.90      0.91      1182\n",
            "  gravitional_theory       0.58      0.93      0.72       110\n",
            "               hydro       0.62      0.86      0.72       335\n",
            "                math       0.91      0.77      0.83      1319\n",
            "              metals       0.58      0.94      0.72       181\n",
            "          networking       0.84      0.85      0.85       325\n",
            "        neuroscience       0.93      0.98      0.95       287\n",
            "        oceanography       0.84      0.82      0.83       970\n",
            "             politic       0.54      0.83      0.66       583\n",
            "           sociology       0.51      0.46      0.49       719\n",
            "software_engineering       0.84      0.67      0.75       504\n",
            "          statistics       0.76      0.81      0.78       627\n",
            "    theory_computing       0.57      0.78      0.66       422\n",
            "\n",
            "            accuracy                           0.76     10204\n",
            "           macro avg       0.74      0.80      0.76     10204\n",
            "        weighted avg       0.78      0.76      0.77     10204\n",
            "\n",
            "time: 5.96 s (started: 2024-08-16 14:16:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EJB3CpTKlm7",
        "outputId": "d0f09f8b-d533-407e-8773-2c769c60ad5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 474 ms (started: 2024-08-16 14:16:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "CCDBpwcfKlm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DI7lIFMKlm7",
        "outputId": "f7d2df72-3639-445b-9899-b6c65559fe10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7791, Train: 0.7375, Test: 0.6586\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5292, Train: 0.8156, Test: 0.7262\n",
            "Early stopping:  0.17669375029876744\n",
            "Epoch: 003, Loss: 2.2172, Train: 0.8500, Test: 0.7397\n",
            "Early stopping:  0.28149652359518845\n",
            "Epoch: 004, Loss: 1.8498, Train: 0.8656, Test: 0.7494\n",
            "Early stopping:  0.40160073777164906\n",
            "Epoch: 005, Loss: 1.4795, Train: 0.8781, Test: 0.7689\n",
            "Early stopping:  0.5199853074582157\n",
            "Epoch: 006, Loss: 1.1438, Train: 0.9031, Test: 0.7868\n",
            "Early stopping:  0.5549644831640589\n",
            "Epoch: 007, Loss: 0.8626, Train: 0.9062, Test: 0.8015\n",
            "Early stopping:  0.5407905619458054\n",
            "Epoch: 008, Loss: 0.6445, Train: 0.9250, Test: 0.8052\n",
            "Early stopping:  0.48113196278332426\n",
            "Epoch: 009, Loss: 0.4890, Train: 0.9313, Test: 0.8043\n",
            "Early stopping:  0.3962474832537484\n",
            "Epoch: 010, Loss: 0.3826, Train: 0.9437, Test: 0.8043\n",
            "Early stopping:  0.30481530234098675\n",
            "Epoch: 011, Loss: 0.3063, Train: 0.9531, Test: 0.8076\n",
            "Early stopping:  0.22188009170628176\n",
            "Epoch: 012, Loss: 0.2476, Train: 0.9594, Test: 0.8113\n",
            "Early stopping:  0.15733893564628668\n",
            "Epoch: 013, Loss: 0.2008, Train: 0.9656, Test: 0.8131\n",
            "Early stopping:  0.11398833873892412\n",
            "Epoch: 014, Loss: 0.1624, Train: 0.9750, Test: 0.8153\n",
            "Early stopping:  0.08713340772512958\n",
            "Epoch: 015, Loss: 0.1299, Train: 0.9844, Test: 0.8148\n",
            "Early stopping:  0.06971339348625807\n",
            "Epoch: 016, Loss: 0.1027, Train: 0.9844, Test: 0.8138\n",
            "Early stopping:  0.05733756582306013\n",
            "Epoch: 017, Loss: 0.0800, Train: 0.9969, Test: 0.8142\n",
            "Early stopping:  0.047886897542292486\n",
            "Epoch: 018, Loss: 0.0618, Train: 0.9969, Test: 0.8125\n",
            "Early stopping:  0.03993593030042766\n",
            "Epoch: 019, Loss: 0.0485, Train: 1.0000, Test: 0.8108\n",
            "Early stopping:  0.032506922272304055\n",
            "Epoch: 020, Loss: 0.0392, Train: 1.0000, Test: 0.8093\n",
            "Early stopping:  0.02542902704263393\n",
            "Epoch: 021, Loss: 0.0322, Train: 1.0000, Test: 0.8089\n",
            "Early stopping:  0.019032699302615416\n",
            "Epoch: 022, Loss: 0.0266, Train: 1.0000, Test: 0.8076\n",
            "Early stopping:  0.013905410864978035\n",
            "Epoch: 023, Loss: 0.0221, Train: 1.0000, Test: 0.8073\n",
            "Early stopping:  0.010417681582934367\n",
            "Epoch: 024, Loss: 0.0187, Train: 1.0000, Test: 0.8066\n",
            "Early stopping:  0.008134141699734202\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.81      0.78       382\n",
            "             ecology       0.83      0.86      0.84       869\n",
            "            economic       0.83      0.69      0.75      1389\n",
            "          geophysics       0.96      0.88      0.92      1182\n",
            "  gravitional_theory       0.72      0.94      0.81       110\n",
            "               hydro       0.80      0.83      0.82       335\n",
            "                math       0.91      0.76      0.83      1319\n",
            "              metals       0.64      0.89      0.75       181\n",
            "          networking       0.75      0.91      0.82       325\n",
            "        neuroscience       0.96      0.95      0.96       287\n",
            "        oceanography       0.87      0.84      0.86       970\n",
            "             politic       0.80      0.64      0.71       583\n",
            "           sociology       0.60      0.75      0.67       719\n",
            "software_engineering       0.87      0.84      0.86       504\n",
            "          statistics       0.68      0.90      0.77       627\n",
            "    theory_computing       0.69      0.81      0.74       422\n",
            "\n",
            "            accuracy                           0.81     10204\n",
            "           macro avg       0.79      0.83      0.81     10204\n",
            "        weighted avg       0.82      0.81      0.81     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7797, Train: 0.7688, Test: 0.6152\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5101, Train: 0.8656, Test: 0.7338\n",
            "Early stopping:  0.19061418001233957\n",
            "Epoch: 003, Loss: 2.1791, Train: 0.8719, Test: 0.7565\n",
            "Early stopping:  0.3008549374647984\n",
            "Epoch: 004, Loss: 1.7980, Train: 0.8625, Test: 0.7676\n",
            "Early stopping:  0.42418807347050363\n",
            "Epoch: 005, Loss: 1.4193, Train: 0.8875, Test: 0.7794\n",
            "Early stopping:  0.5440889750684299\n",
            "Epoch: 006, Loss: 1.0796, Train: 0.8969, Test: 0.7913\n",
            "Early stopping:  0.5726788743007262\n",
            "Epoch: 007, Loss: 0.8016, Train: 0.9000, Test: 0.8015\n",
            "Early stopping:  0.55024455497569\n",
            "Epoch: 008, Loss: 0.5949, Train: 0.9219, Test: 0.8049\n",
            "Early stopping:  0.48120908148367825\n",
            "Epoch: 009, Loss: 0.4532, Train: 0.9344, Test: 0.8050\n",
            "Early stopping:  0.38720606282905656\n",
            "Epoch: 010, Loss: 0.3567, Train: 0.9469, Test: 0.8071\n",
            "Early stopping:  0.28942622716338323\n",
            "Epoch: 011, Loss: 0.2861, Train: 0.9625, Test: 0.8092\n",
            "Early stopping:  0.20517848964497798\n",
            "Epoch: 012, Loss: 0.2317, Train: 0.9625, Test: 0.8092\n",
            "Early stopping:  0.14387724466874263\n",
            "Epoch: 013, Loss: 0.1890, Train: 0.9656, Test: 0.8085\n",
            "Early stopping:  0.10466658786523594\n",
            "Epoch: 014, Loss: 0.1551, Train: 0.9688, Test: 0.8074\n",
            "Early stopping:  0.07993389782195516\n",
            "Epoch: 015, Loss: 0.1274, Train: 0.9812, Test: 0.8067\n",
            "Early stopping:  0.06285744977269779\n",
            "Epoch: 016, Loss: 0.1043, Train: 0.9906, Test: 0.8078\n",
            "Early stopping:  0.050385363855122246\n",
            "Epoch: 017, Loss: 0.0852, Train: 0.9906, Test: 0.8095\n",
            "Early stopping:  0.041101568994114365\n",
            "Epoch: 018, Loss: 0.0699, Train: 0.9938, Test: 0.8094\n",
            "Early stopping:  0.03384303845514437\n",
            "Epoch: 019, Loss: 0.0577, Train: 0.9938, Test: 0.8098\n",
            "Early stopping:  0.0277142825398712\n",
            "Epoch: 020, Loss: 0.0480, Train: 0.9938, Test: 0.8091\n",
            "Early stopping:  0.022357951659714128\n",
            "Epoch: 021, Loss: 0.0402, Train: 0.9938, Test: 0.8085\n",
            "Early stopping:  0.017872175488598718\n",
            "Epoch: 022, Loss: 0.0337, Train: 0.9938, Test: 0.8064\n",
            "Early stopping:  0.014311530020905197\n",
            "Epoch: 023, Loss: 0.0285, Train: 1.0000, Test: 0.8047\n",
            "Early stopping:  0.01155862100426135\n",
            "Epoch: 024, Loss: 0.0243, Train: 1.0000, Test: 0.8025\n",
            "Early stopping:  0.009384642799989434\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 13, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.76      0.78       382\n",
            "             ecology       0.85      0.82      0.83       869\n",
            "            economic       0.80      0.73      0.76      1389\n",
            "          geophysics       0.93      0.90      0.91      1182\n",
            "  gravitional_theory       0.73      0.96      0.83       110\n",
            "               hydro       0.72      0.84      0.77       335\n",
            "                math       0.91      0.70      0.79      1319\n",
            "              metals       0.58      0.91      0.71       181\n",
            "          networking       0.81      0.90      0.85       325\n",
            "        neuroscience       0.89      0.99      0.94       287\n",
            "        oceanography       0.86      0.88      0.87       970\n",
            "             politic       0.73      0.78      0.75       583\n",
            "           sociology       0.71      0.63      0.67       719\n",
            "software_engineering       0.80      0.88      0.83       504\n",
            "          statistics       0.68      0.85      0.75       627\n",
            "    theory_computing       0.68      0.78      0.73       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.78      0.83      0.80     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7723, Train: 0.7344, Test: 0.6567\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4963, Train: 0.8562, Test: 0.7449\n",
            "Early stopping:  0.19515811604399608\n",
            "Epoch: 003, Loss: 2.1621, Train: 0.8781, Test: 0.7470\n",
            "Early stopping:  0.30555436673167463\n",
            "Epoch: 004, Loss: 1.7854, Train: 0.8750, Test: 0.7515\n",
            "Early stopping:  0.42638522526619105\n",
            "Epoch: 005, Loss: 1.4125, Train: 0.8844, Test: 0.7664\n",
            "Early stopping:  0.543421886678378\n",
            "Epoch: 006, Loss: 1.0800, Train: 0.8969, Test: 0.7821\n",
            "Early stopping:  0.5665626765709868\n",
            "Epoch: 007, Loss: 0.8084, Train: 0.9000, Test: 0.7948\n",
            "Early stopping:  0.5407342485768017\n",
            "Epoch: 008, Loss: 0.6065, Train: 0.9125, Test: 0.8052\n",
            "Early stopping:  0.4714170386916521\n",
            "Epoch: 009, Loss: 0.4684, Train: 0.9125, Test: 0.8118\n",
            "Early stopping:  0.3784224819401597\n",
            "Epoch: 010, Loss: 0.3761, Train: 0.9156, Test: 0.8155\n",
            "Early stopping:  0.28208230504597676\n",
            "Epoch: 011, Loss: 0.3112, Train: 0.9313, Test: 0.8153\n",
            "Early stopping:  0.19841247360416556\n",
            "Epoch: 012, Loss: 0.2610, Train: 0.9437, Test: 0.8151\n",
            "Early stopping:  0.13693468308249693\n",
            "Epoch: 013, Loss: 0.2181, Train: 0.9563, Test: 0.8144\n",
            "Early stopping:  0.0985638884368505\n",
            "Epoch: 014, Loss: 0.1790, Train: 0.9750, Test: 0.8128\n",
            "Early stopping:  0.07745519520393739\n",
            "Epoch: 015, Loss: 0.1436, Train: 0.9812, Test: 0.8108\n",
            "Early stopping:  0.06611303434552503\n",
            "Epoch: 016, Loss: 0.1135, Train: 0.9812, Test: 0.8102\n",
            "Early stopping:  0.05855780915536114\n",
            "Epoch: 017, Loss: 0.0890, Train: 0.9906, Test: 0.8104\n",
            "Early stopping:  0.05139907777478851\n",
            "Epoch: 018, Loss: 0.0692, Train: 0.9969, Test: 0.8077\n",
            "Early stopping:  0.043622194946417676\n",
            "Epoch: 019, Loss: 0.0538, Train: 0.9969, Test: 0.8055\n",
            "Early stopping:  0.035673663741395016\n",
            "Epoch: 020, Loss: 0.0429, Train: 1.0000, Test: 0.8032\n",
            "Early stopping:  0.02819778233021004\n",
            "Epoch: 021, Loss: 0.0354, Train: 1.0000, Test: 0.8020\n",
            "Early stopping:  0.021455782363701364\n",
            "Epoch: 022, Loss: 0.0299, Train: 1.0000, Test: 0.8020\n",
            "Early stopping:  0.015664478137347174\n",
            "Epoch: 023, Loss: 0.0254, Train: 1.0000, Test: 0.8013\n",
            "Early stopping:  0.01124490464761321\n",
            "Epoch: 024, Loss: 0.0218, Train: 1.0000, Test: 0.7995\n",
            "Early stopping:  0.00834035715317055\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.83      0.78       382\n",
            "             ecology       0.81      0.84      0.82       869\n",
            "            economic       0.82      0.74      0.78      1389\n",
            "          geophysics       0.91      0.87      0.89      1182\n",
            "  gravitional_theory       0.72      0.95      0.82       110\n",
            "               hydro       0.77      0.80      0.78       335\n",
            "                math       0.91      0.71      0.80      1319\n",
            "              metals       0.58      0.93      0.72       181\n",
            "          networking       0.79      0.93      0.85       325\n",
            "        neuroscience       0.91      0.98      0.94       287\n",
            "        oceanography       0.81      0.86      0.84       970\n",
            "             politic       0.73      0.77      0.75       583\n",
            "           sociology       0.69      0.67      0.68       719\n",
            "software_engineering       0.84      0.86      0.85       504\n",
            "          statistics       0.71      0.79      0.75       627\n",
            "    theory_computing       0.71      0.70      0.70       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.78      0.83      0.80     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7899, Train: 0.7250, Test: 0.5965\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5340, Train: 0.8344, Test: 0.7246\n",
            "Early stopping:  0.18099778646869993\n",
            "Epoch: 003, Loss: 2.2225, Train: 0.8500, Test: 0.7413\n",
            "Early stopping:  0.28416146367018097\n",
            "Epoch: 004, Loss: 1.8601, Train: 0.8500, Test: 0.7547\n",
            "Early stopping:  0.4015299844519611\n",
            "Epoch: 005, Loss: 1.4931, Train: 0.8719, Test: 0.7755\n",
            "Early stopping:  0.5180079256569459\n",
            "Epoch: 006, Loss: 1.1574, Train: 0.8750, Test: 0.7948\n",
            "Early stopping:  0.5508381913446977\n",
            "Epoch: 007, Loss: 0.8758, Train: 0.8844, Test: 0.8038\n",
            "Early stopping:  0.5376729189109625\n",
            "Epoch: 008, Loss: 0.6629, Train: 0.8969, Test: 0.8036\n",
            "Early stopping:  0.4786610459547549\n",
            "Epoch: 009, Loss: 0.5169, Train: 0.9031, Test: 0.7967\n",
            "Early stopping:  0.39148660806581925\n",
            "Epoch: 010, Loss: 0.4181, Train: 0.9094, Test: 0.7946\n",
            "Early stopping:  0.29623725677756396\n",
            "Epoch: 011, Loss: 0.3441, Train: 0.9219, Test: 0.7952\n",
            "Early stopping:  0.2114469630286746\n",
            "Epoch: 012, Loss: 0.2840, Train: 0.9406, Test: 0.7968\n",
            "Early stopping:  0.14956909436170468\n",
            "Epoch: 013, Loss: 0.2349, Train: 0.9531, Test: 0.7965\n",
            "Early stopping:  0.11145786389849067\n",
            "Epoch: 014, Loss: 0.1924, Train: 0.9656, Test: 0.7949\n",
            "Early stopping:  0.08922440174652733\n",
            "Epoch: 015, Loss: 0.1543, Train: 0.9812, Test: 0.7924\n",
            "Early stopping:  0.07484879802410468\n",
            "Epoch: 016, Loss: 0.1216, Train: 0.9875, Test: 0.7888\n",
            "Early stopping:  0.06430627798930037\n",
            "Epoch: 017, Loss: 0.0954, Train: 0.9938, Test: 0.7844\n",
            "Early stopping:  0.0555397573483013\n",
            "Epoch: 018, Loss: 0.0750, Train: 0.9969, Test: 0.7784\n",
            "Early stopping:  0.046762214609267966\n",
            "Epoch: 019, Loss: 0.0598, Train: 0.9969, Test: 0.7746\n",
            "Early stopping:  0.03764794555204969\n",
            "Epoch: 020, Loss: 0.0486, Train: 0.9969, Test: 0.7737\n",
            "Early stopping:  0.029098581302708766\n",
            "Epoch: 021, Loss: 0.0401, Train: 1.0000, Test: 0.7714\n",
            "Early stopping:  0.021973709630957804\n",
            "Epoch: 022, Loss: 0.0334, Train: 1.0000, Test: 0.7707\n",
            "Early stopping:  0.01647975731737953\n",
            "Epoch: 023, Loss: 0.0283, Train: 1.0000, Test: 0.7709\n",
            "Early stopping:  0.01249583641503902\n",
            "Epoch: 024, Loss: 0.0243, Train: 1.0000, Test: 0.7725\n",
            "Early stopping:  0.009650465868676095\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.68      0.87      0.76       382\n",
            "             ecology       0.83      0.76      0.80       869\n",
            "            economic       0.82      0.66      0.73      1389\n",
            "          geophysics       0.96      0.81      0.88      1182\n",
            "  gravitional_theory       0.35      0.97      0.52       110\n",
            "               hydro       0.63      0.91      0.74       335\n",
            "                math       0.86      0.73      0.79      1319\n",
            "              metals       0.46      0.94      0.62       181\n",
            "          networking       0.84      0.84      0.84       325\n",
            "        neuroscience       0.85      0.99      0.91       287\n",
            "        oceanography       0.84      0.86      0.85       970\n",
            "             politic       0.71      0.69      0.70       583\n",
            "           sociology       0.61      0.65      0.63       719\n",
            "software_engineering       0.94      0.76      0.84       504\n",
            "          statistics       0.80      0.75      0.78       627\n",
            "    theory_computing       0.66      0.82      0.73       422\n",
            "\n",
            "            accuracy                           0.77     10204\n",
            "           macro avg       0.74      0.81      0.76     10204\n",
            "        weighted avg       0.80      0.77      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7884, Train: 0.6844, Test: 0.5122\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5183, Train: 0.8313, Test: 0.7108\n",
            "Early stopping:  0.1910085059270075\n",
            "Epoch: 003, Loss: 2.1867, Train: 0.8594, Test: 0.7459\n",
            "Early stopping:  0.30138775489642944\n",
            "Epoch: 004, Loss: 1.8039, Train: 0.8875, Test: 0.7571\n",
            "Early stopping:  0.4253745808190687\n",
            "Epoch: 005, Loss: 1.4223, Train: 0.9000, Test: 0.7707\n",
            "Early stopping:  0.5463089176901735\n",
            "Epoch: 006, Loss: 1.0829, Train: 0.9156, Test: 0.7817\n",
            "Early stopping:  0.5749674363003707\n",
            "Epoch: 007, Loss: 0.8087, Train: 0.9187, Test: 0.7927\n",
            "Early stopping:  0.5509320458858098\n",
            "Epoch: 008, Loss: 0.6037, Train: 0.9250, Test: 0.7980\n",
            "Early stopping:  0.4798403064385212\n",
            "Epoch: 009, Loss: 0.4600, Train: 0.9250, Test: 0.8015\n",
            "Early stopping:  0.3850207087837209\n",
            "Epoch: 010, Loss: 0.3624, Train: 0.9344, Test: 0.8022\n",
            "Early stopping:  0.2883886804028244\n",
            "Epoch: 011, Loss: 0.2918, Train: 0.9437, Test: 0.8018\n",
            "Early stopping:  0.20604052945867965\n",
            "Epoch: 012, Loss: 0.2353, Train: 0.9594, Test: 0.8019\n",
            "Early stopping:  0.1456742977141142\n",
            "Epoch: 013, Loss: 0.1889, Train: 0.9656, Test: 0.8021\n",
            "Early stopping:  0.10699259262486206\n",
            "Epoch: 014, Loss: 0.1510, Train: 0.9812, Test: 0.7997\n",
            "Early stopping:  0.08373245973329035\n",
            "Epoch: 015, Loss: 0.1198, Train: 0.9906, Test: 0.7990\n",
            "Early stopping:  0.06818817061462777\n",
            "Epoch: 016, Loss: 0.0942, Train: 0.9938, Test: 0.7968\n",
            "Early stopping:  0.05594899084680082\n",
            "Epoch: 017, Loss: 0.0739, Train: 0.9969, Test: 0.7948\n",
            "Early stopping:  0.04569049931541096\n",
            "Epoch: 018, Loss: 0.0587, Train: 1.0000, Test: 0.7947\n",
            "Early stopping:  0.036778504532207476\n",
            "Epoch: 019, Loss: 0.0475, Train: 1.0000, Test: 0.7930\n",
            "Early stopping:  0.02883300102279122\n",
            "Epoch: 020, Loss: 0.0390, Train: 1.0000, Test: 0.7910\n",
            "Early stopping:  0.021948858867428436\n",
            "Epoch: 021, Loss: 0.0323, Train: 1.0000, Test: 0.7910\n",
            "Early stopping:  0.0164962209731865\n",
            "Epoch: 022, Loss: 0.0271, Train: 1.0000, Test: 0.7894\n",
            "Early stopping:  0.012527573650759277\n",
            "Epoch: 023, Loss: 0.0231, Train: 1.0000, Test: 0.7894\n",
            "Early stopping:  0.00969091597586434\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.71      0.82      0.76       382\n",
            "             ecology       0.80      0.82      0.81       869\n",
            "            economic       0.79      0.74      0.76      1389\n",
            "          geophysics       0.94      0.84      0.89      1182\n",
            "  gravitional_theory       0.82      0.94      0.88       110\n",
            "               hydro       0.63      0.83      0.72       335\n",
            "                math       0.90      0.78      0.84      1319\n",
            "              metals       0.53      0.96      0.68       181\n",
            "          networking       0.66      0.88      0.76       325\n",
            "        neuroscience       0.90      0.98      0.94       287\n",
            "        oceanography       0.85      0.80      0.82       970\n",
            "             politic       0.73      0.76      0.74       583\n",
            "           sociology       0.65      0.58      0.61       719\n",
            "software_engineering       0.88      0.85      0.86       504\n",
            "          statistics       0.77      0.81      0.79       627\n",
            "    theory_computing       0.70      0.71      0.70       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.82      0.78     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7771, Train: 0.7219, Test: 0.5968\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4992, Train: 0.8531, Test: 0.7301\n",
            "Early stopping:  0.19651086129380174\n",
            "Epoch: 003, Loss: 2.1560, Train: 0.8438, Test: 0.7502\n",
            "Early stopping:  0.31111880853066437\n",
            "Epoch: 004, Loss: 1.7701, Train: 0.8469, Test: 0.7608\n",
            "Early stopping:  0.4354504951098502\n",
            "Epoch: 005, Loss: 1.3917, Train: 0.8656, Test: 0.7759\n",
            "Early stopping:  0.5544775717424497\n",
            "Epoch: 006, Loss: 1.0564, Train: 0.8812, Test: 0.7931\n",
            "Early stopping:  0.5772784001007324\n",
            "Epoch: 007, Loss: 0.7895, Train: 0.8938, Test: 0.8015\n",
            "Early stopping:  0.5463515561169227\n",
            "Epoch: 008, Loss: 0.5975, Train: 0.9062, Test: 0.8025\n",
            "Early stopping:  0.469778553885643\n",
            "Epoch: 009, Loss: 0.4649, Train: 0.9062, Test: 0.8018\n",
            "Early stopping:  0.37124814711655424\n",
            "Epoch: 010, Loss: 0.3713, Train: 0.9094, Test: 0.8013\n",
            "Early stopping:  0.27347665586153996\n",
            "Epoch: 011, Loss: 0.3020, Train: 0.9313, Test: 0.7990\n",
            "Early stopping:  0.19376372961861466\n",
            "Epoch: 012, Loss: 0.2478, Train: 0.9531, Test: 0.7972\n",
            "Early stopping:  0.13849509967709173\n",
            "Epoch: 013, Loss: 0.2024, Train: 0.9719, Test: 0.7975\n",
            "Early stopping:  0.10363976376517052\n",
            "Epoch: 014, Loss: 0.1638, Train: 0.9781, Test: 0.7960\n",
            "Early stopping:  0.08192145920974006\n",
            "Epoch: 015, Loss: 0.1328, Train: 0.9844, Test: 0.7955\n",
            "Early stopping:  0.06716503380889517\n",
            "Epoch: 016, Loss: 0.1082, Train: 0.9844, Test: 0.7924\n",
            "Early stopping:  0.055547567522193836\n",
            "Epoch: 017, Loss: 0.0877, Train: 0.9875, Test: 0.7895\n",
            "Early stopping:  0.0454318607400568\n",
            "Epoch: 018, Loss: 0.0706, Train: 0.9938, Test: 0.7877\n",
            "Early stopping:  0.03686067310394315\n",
            "Epoch: 019, Loss: 0.0569, Train: 0.9938, Test: 0.7871\n",
            "Early stopping:  0.0301241917489193\n",
            "Epoch: 020, Loss: 0.0462, Train: 0.9969, Test: 0.7851\n",
            "Early stopping:  0.024648609031096298\n",
            "Epoch: 021, Loss: 0.0378, Train: 1.0000, Test: 0.7833\n",
            "Early stopping:  0.01982067424311685\n",
            "Epoch: 022, Loss: 0.0312, Train: 1.0000, Test: 0.7838\n",
            "Early stopping:  0.01563718477883025\n",
            "Epoch: 023, Loss: 0.0262, Train: 1.0000, Test: 0.7829\n",
            "Early stopping:  0.012215464489611664\n",
            "Epoch: 024, Loss: 0.0224, Train: 1.0000, Test: 0.7818\n",
            "Early stopping:  0.00949449249768391\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.85      0.78      0.81       382\n",
            "             ecology       0.88      0.73      0.80       869\n",
            "            economic       0.79      0.62      0.69      1389\n",
            "          geophysics       0.94      0.90      0.92      1182\n",
            "  gravitional_theory       0.61      0.95      0.75       110\n",
            "               hydro       0.75      0.81      0.78       335\n",
            "                math       0.84      0.71      0.77      1319\n",
            "              metals       0.53      0.92      0.67       181\n",
            "          networking       0.76      0.94      0.84       325\n",
            "        neuroscience       0.94      0.98      0.96       287\n",
            "        oceanography       0.80      0.90      0.85       970\n",
            "             politic       0.65      0.81      0.72       583\n",
            "           sociology       0.56      0.69      0.62       719\n",
            "software_engineering       0.92      0.78      0.85       504\n",
            "          statistics       0.82      0.78      0.80       627\n",
            "    theory_computing       0.64      0.77      0.70       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.77      0.82      0.78     10204\n",
            "        weighted avg       0.80      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7803, Train: 0.6312, Test: 0.5500\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5030, Train: 0.8094, Test: 0.6888\n",
            "Early stopping:  0.19603696212914523\n",
            "Epoch: 003, Loss: 2.1686, Train: 0.8625, Test: 0.7182\n",
            "Early stopping:  0.3062831625743526\n",
            "Epoch: 004, Loss: 1.7911, Train: 0.8688, Test: 0.7304\n",
            "Early stopping:  0.42724657346455924\n",
            "Epoch: 005, Loss: 1.4190, Train: 0.8688, Test: 0.7498\n",
            "Early stopping:  0.5439980259004155\n",
            "Epoch: 006, Loss: 1.0899, Train: 0.8625, Test: 0.7686\n",
            "Early stopping:  0.5655375736967987\n",
            "Epoch: 007, Loss: 0.8232, Train: 0.8719, Test: 0.7854\n",
            "Early stopping:  0.5375514760603411\n",
            "Epoch: 008, Loss: 0.6231, Train: 0.8750, Test: 0.7983\n",
            "Early stopping:  0.46674878825268507\n",
            "Epoch: 009, Loss: 0.4830, Train: 0.8844, Test: 0.8053\n",
            "Early stopping:  0.3745556525800307\n",
            "Epoch: 010, Loss: 0.3878, Train: 0.9062, Test: 0.8072\n",
            "Early stopping:  0.28106020634877765\n",
            "Epoch: 011, Loss: 0.3205, Train: 0.9313, Test: 0.8055\n",
            "Early stopping:  0.2005909169166681\n",
            "Epoch: 012, Loss: 0.2680, Train: 0.9344, Test: 0.8037\n",
            "Early stopping:  0.14071938984998672\n",
            "Epoch: 013, Loss: 0.2239, Train: 0.9469, Test: 0.8026\n",
            "Early stopping:  0.10212365123480921\n",
            "Epoch: 014, Loss: 0.1857, Train: 0.9625, Test: 0.8009\n",
            "Early stopping:  0.07969729302197444\n",
            "Epoch: 015, Loss: 0.1520, Train: 0.9656, Test: 0.8018\n",
            "Early stopping:  0.06656560463463032\n",
            "Epoch: 016, Loss: 0.1221, Train: 0.9844, Test: 0.8023\n",
            "Early stopping:  0.05768786422039977\n",
            "Epoch: 017, Loss: 0.0969, Train: 0.9906, Test: 0.8029\n",
            "Early stopping:  0.0503699092166004\n",
            "Epoch: 018, Loss: 0.0770, Train: 0.9969, Test: 0.8028\n",
            "Early stopping:  0.04329485808124049\n",
            "Epoch: 019, Loss: 0.0616, Train: 0.9969, Test: 0.8036\n",
            "Early stopping:  0.03600717277818165\n",
            "Epoch: 020, Loss: 0.0497, Train: 1.0000, Test: 0.8031\n",
            "Early stopping:  0.028792669558418832\n",
            "Epoch: 021, Loss: 0.0407, Train: 1.0000, Test: 0.8013\n",
            "Early stopping:  0.02236957002643158\n",
            "Epoch: 022, Loss: 0.0341, Train: 1.0000, Test: 0.7997\n",
            "Early stopping:  0.01709439278982245\n",
            "Epoch: 023, Loss: 0.0293, Train: 1.0000, Test: 0.7990\n",
            "Early stopping:  0.012859150198033818\n",
            "Epoch: 024, Loss: 0.0256, Train: 1.0000, Test: 0.7973\n",
            "Early stopping:  0.009541705905923402\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.72      0.79      0.75       382\n",
            "             ecology       0.86      0.82      0.84       869\n",
            "            economic       0.82      0.64      0.72      1389\n",
            "          geophysics       0.95      0.89      0.92      1182\n",
            "  gravitional_theory       0.81      0.94      0.87       110\n",
            "               hydro       0.66      0.85      0.74       335\n",
            "                math       0.92      0.79      0.85      1319\n",
            "              metals       0.59      0.93      0.72       181\n",
            "          networking       0.78      0.92      0.85       325\n",
            "        neuroscience       0.97      0.91      0.94       287\n",
            "        oceanography       0.85      0.84      0.85       970\n",
            "             politic       0.64      0.75      0.69       583\n",
            "           sociology       0.62      0.71      0.66       719\n",
            "software_engineering       0.80      0.89      0.85       504\n",
            "          statistics       0.72      0.81      0.76       627\n",
            "    theory_computing       0.74      0.71      0.72       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.78      0.82      0.80     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7802, Train: 0.6406, Test: 0.4165\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5149, Train: 0.8219, Test: 0.6452\n",
            "Early stopping:  0.18766002310651397\n",
            "Epoch: 003, Loss: 2.1827, Train: 0.8562, Test: 0.7277\n",
            "Early stopping:  0.2994034018030712\n",
            "Epoch: 004, Loss: 1.7947, Train: 0.8656, Test: 0.7557\n",
            "Early stopping:  0.4260609118423115\n",
            "Epoch: 005, Loss: 1.4026, Train: 0.8812, Test: 0.7776\n",
            "Early stopping:  0.551174412606662\n",
            "Epoch: 006, Loss: 1.0513, Train: 0.9000, Test: 0.7942\n",
            "Early stopping:  0.5863907433756057\n",
            "Epoch: 007, Loss: 0.7683, Train: 0.9094, Test: 0.8049\n",
            "Early stopping:  0.5659331217123592\n",
            "Epoch: 008, Loss: 0.5641, Train: 0.9125, Test: 0.8045\n",
            "Early stopping:  0.4930691137839589\n",
            "Epoch: 009, Loss: 0.4294, Train: 0.9281, Test: 0.8022\n",
            "Early stopping:  0.3908202944600088\n",
            "Epoch: 010, Loss: 0.3410, Train: 0.9313, Test: 0.8018\n",
            "Early stopping:  0.28492307926429306\n",
            "Epoch: 011, Loss: 0.2779, Train: 0.9406, Test: 0.8035\n",
            "Early stopping:  0.1954592432479634\n",
            "Epoch: 012, Loss: 0.2295, Train: 0.9406, Test: 0.8039\n",
            "Early stopping:  0.13253555988095603\n",
            "Epoch: 013, Loss: 0.1914, Train: 0.9437, Test: 0.8006\n",
            "Early stopping:  0.09419390754567677\n",
            "Epoch: 014, Loss: 0.1597, Train: 0.9531, Test: 0.7974\n",
            "Early stopping:  0.0716842994201497\n",
            "Epoch: 015, Loss: 0.1319, Train: 0.9625, Test: 0.7935\n",
            "Early stopping:  0.057563008727725705\n",
            "Epoch: 016, Loss: 0.1073, Train: 0.9781, Test: 0.7913\n",
            "Early stopping:  0.048213531551144044\n",
            "Epoch: 017, Loss: 0.0862, Train: 0.9844, Test: 0.7898\n",
            "Early stopping:  0.041683321584357544\n",
            "Epoch: 018, Loss: 0.0693, Train: 0.9906, Test: 0.7893\n",
            "Early stopping:  0.03597860511272594\n",
            "Epoch: 019, Loss: 0.0567, Train: 0.9969, Test: 0.7876\n",
            "Early stopping:  0.030038458842987625\n",
            "Epoch: 020, Loss: 0.0469, Train: 1.0000, Test: 0.7850\n",
            "Early stopping:  0.0240539392754842\n",
            "Epoch: 021, Loss: 0.0388, Train: 1.0000, Test: 0.7818\n",
            "Early stopping:  0.018722050474343457\n",
            "Epoch: 022, Loss: 0.0323, Train: 1.0000, Test: 0.7798\n",
            "Early stopping:  0.014639986525394744\n",
            "Epoch: 023, Loss: 0.0271, Train: 1.0000, Test: 0.7797\n",
            "Early stopping:  0.01175291060521909\n",
            "Epoch: 024, Loss: 0.0229, Train: 1.0000, Test: 0.7785\n",
            "Early stopping:  0.009516523880860724\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.74      0.74       382\n",
            "             ecology       0.83      0.82      0.82       869\n",
            "            economic       0.76      0.67      0.71      1389\n",
            "          geophysics       0.95      0.87      0.91      1182\n",
            "  gravitional_theory       0.49      0.95      0.65       110\n",
            "               hydro       0.69      0.85      0.76       335\n",
            "                math       0.83      0.73      0.78      1319\n",
            "              metals       0.64      0.91      0.75       181\n",
            "          networking       0.85      0.86      0.85       325\n",
            "        neuroscience       0.90      0.97      0.93       287\n",
            "        oceanography       0.84      0.86      0.85       970\n",
            "             politic       0.79      0.70      0.74       583\n",
            "           sociology       0.52      0.69      0.60       719\n",
            "software_engineering       0.88      0.80      0.84       504\n",
            "          statistics       0.79      0.71      0.75       627\n",
            "    theory_computing       0.67      0.79      0.72       422\n",
            "\n",
            "            accuracy                           0.78     10204\n",
            "           macro avg       0.76      0.81      0.77     10204\n",
            "        weighted avg       0.79      0.78      0.78     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7816, Train: 0.7406, Test: 0.5951\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4963, Train: 0.8594, Test: 0.7569\n",
            "Early stopping:  0.20173083277557297\n",
            "Epoch: 003, Loss: 2.1507, Train: 0.8719, Test: 0.7665\n",
            "Early stopping:  0.31589921705713486\n",
            "Epoch: 004, Loss: 1.7607, Train: 0.8875, Test: 0.7732\n",
            "Early stopping:  0.4410334933993247\n",
            "Epoch: 005, Loss: 1.3790, Train: 0.8969, Test: 0.7829\n",
            "Early stopping:  0.5608354845754142\n",
            "Epoch: 006, Loss: 1.0440, Train: 0.8969, Test: 0.7915\n",
            "Early stopping:  0.5814743933313855\n",
            "Epoch: 007, Loss: 0.7779, Train: 0.8844, Test: 0.8024\n",
            "Early stopping:  0.5489510090271003\n",
            "Epoch: 008, Loss: 0.5861, Train: 0.9000, Test: 0.8080\n",
            "Early stopping:  0.4703448090361339\n",
            "Epoch: 009, Loss: 0.4558, Train: 0.9094, Test: 0.8117\n",
            "Early stopping:  0.370045725129839\n",
            "Epoch: 010, Loss: 0.3660, Train: 0.9219, Test: 0.8118\n",
            "Early stopping:  0.27109390912667136\n",
            "Epoch: 011, Loss: 0.2991, Train: 0.9375, Test: 0.8127\n",
            "Early stopping:  0.19030338440608313\n",
            "Epoch: 012, Loss: 0.2458, Train: 0.9469, Test: 0.8118\n",
            "Early stopping:  0.13453620731231933\n",
            "Epoch: 013, Loss: 0.2026, Train: 0.9531, Test: 0.8120\n",
            "Early stopping:  0.1000806009872563\n",
            "Epoch: 014, Loss: 0.1678, Train: 0.9688, Test: 0.8115\n",
            "Early stopping:  0.07854487792447885\n",
            "Epoch: 015, Loss: 0.1384, Train: 0.9688, Test: 0.8120\n",
            "Early stopping:  0.06360637374574336\n",
            "Epoch: 016, Loss: 0.1132, Train: 0.9750, Test: 0.8102\n",
            "Early stopping:  0.052390546951309784\n",
            "Epoch: 017, Loss: 0.0924, Train: 0.9844, Test: 0.8088\n",
            "Early stopping:  0.04372101162294244\n",
            "Epoch: 018, Loss: 0.0755, Train: 0.9969, Test: 0.8082\n",
            "Early stopping:  0.03667546927859104\n",
            "Epoch: 019, Loss: 0.0616, Train: 1.0000, Test: 0.8065\n",
            "Early stopping:  0.030461327476705775\n",
            "Epoch: 020, Loss: 0.0500, Train: 1.0000, Test: 0.8042\n",
            "Early stopping:  0.025022924606130693\n",
            "Epoch: 021, Loss: 0.0408, Train: 1.0000, Test: 0.8029\n",
            "Early stopping:  0.020486470411945363\n",
            "Epoch: 022, Loss: 0.0338, Train: 1.0000, Test: 0.7998\n",
            "Early stopping:  0.016627718784169358\n",
            "Epoch: 023, Loss: 0.0285, Train: 1.0000, Test: 0.7984\n",
            "Early stopping:  0.01316706336257918\n",
            "Epoch: 024, Loss: 0.0245, Train: 1.0000, Test: 0.7975\n",
            "Early stopping:  0.01013251341275153\n",
            "Epoch: 025, Loss: 0.0215, Train: 1.0000, Test: 0.7961\n",
            "Early stopping:  0.00767945334480973\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.87      0.80       382\n",
            "             ecology       0.80      0.84      0.82       869\n",
            "            economic       0.83      0.72      0.77      1389\n",
            "          geophysics       0.93      0.86      0.90      1182\n",
            "  gravitional_theory       0.44      0.98      0.61       110\n",
            "               hydro       0.65      0.86      0.74       335\n",
            "                math       0.92      0.68      0.78      1319\n",
            "              metals       0.52      0.94      0.67       181\n",
            "          networking       0.83      0.90      0.86       325\n",
            "        neuroscience       0.94      0.95      0.95       287\n",
            "        oceanography       0.86      0.82      0.84       970\n",
            "             politic       0.75      0.80      0.77       583\n",
            "           sociology       0.66      0.69      0.67       719\n",
            "software_engineering       0.87      0.87      0.87       504\n",
            "          statistics       0.74      0.78      0.76       627\n",
            "    theory_computing       0.74      0.75      0.75       422\n",
            "\n",
            "            accuracy                           0.80     10204\n",
            "           macro avg       0.76      0.83      0.79     10204\n",
            "        weighted avg       0.81      0.80      0.80     10204\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7714, Train: 0.6562, Test: 0.4941\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4994, Train: 0.8375, Test: 0.7058\n",
            "Early stopping:  0.19234506678698496\n",
            "Epoch: 003, Loss: 2.1740, Train: 0.8531, Test: 0.7283\n",
            "Early stopping:  0.29907061988998035\n",
            "Epoch: 004, Loss: 1.8024, Train: 0.8625, Test: 0.7457\n",
            "Early stopping:  0.4182811500707177\n",
            "Epoch: 005, Loss: 1.4307, Train: 0.8812, Test: 0.7690\n",
            "Early stopping:  0.5352482109562944\n",
            "Epoch: 006, Loss: 1.0950, Train: 0.9000, Test: 0.7916\n",
            "Early stopping:  0.5617779966699799\n",
            "Epoch: 007, Loss: 0.8202, Train: 0.9094, Test: 0.8000\n",
            "Early stopping:  0.5409190899262464\n",
            "Epoch: 008, Loss: 0.6162, Train: 0.9125, Test: 0.7971\n",
            "Early stopping:  0.4746217790682753\n",
            "Epoch: 009, Loss: 0.4766, Train: 0.9250, Test: 0.7955\n",
            "Early stopping:  0.3824466811775044\n",
            "Epoch: 010, Loss: 0.3811, Train: 0.9313, Test: 0.7975\n",
            "Early stopping:  0.28577229617432076\n",
            "Epoch: 011, Loss: 0.3103, Train: 0.9437, Test: 0.8000\n",
            "Early stopping:  0.20281143970169427\n",
            "Epoch: 012, Loss: 0.2548, Train: 0.9563, Test: 0.8016\n",
            "Early stopping:  0.1430067788407312\n",
            "Epoch: 013, Loss: 0.2100, Train: 0.9656, Test: 0.8033\n",
            "Early stopping:  0.10545697052609414\n",
            "Epoch: 014, Loss: 0.1718, Train: 0.9656, Test: 0.8036\n",
            "Early stopping:  0.08265784169982054\n",
            "Epoch: 015, Loss: 0.1382, Train: 0.9750, Test: 0.8030\n",
            "Early stopping:  0.0678693316234715\n",
            "Epoch: 016, Loss: 0.1095, Train: 0.9844, Test: 0.8029\n",
            "Early stopping:  0.05752243670572036\n",
            "Epoch: 017, Loss: 0.0861, Train: 0.9969, Test: 0.8013\n",
            "Early stopping:  0.049266080215474115\n",
            "Epoch: 018, Loss: 0.0677, Train: 1.0000, Test: 0.8001\n",
            "Early stopping:  0.04143501138797467\n",
            "Epoch: 019, Loss: 0.0536, Train: 1.0000, Test: 0.7983\n",
            "Early stopping:  0.03365454702641406\n",
            "Epoch: 020, Loss: 0.0431, Train: 1.0000, Test: 0.7946\n",
            "Early stopping:  0.026429168874640356\n",
            "Epoch: 021, Loss: 0.0355, Train: 1.0000, Test: 0.7916\n",
            "Early stopping:  0.020170437697948268\n",
            "Epoch: 022, Loss: 0.0300, Train: 1.0000, Test: 0.7894\n",
            "Early stopping:  0.015047149955102831\n",
            "Epoch: 023, Loss: 0.0256, Train: 1.0000, Test: 0.7888\n",
            "Early stopping:  0.011103641392020597\n",
            "Epoch: 024, Loss: 0.0221, Train: 1.0000, Test: 0.7876\n",
            "Early stopping:  0.00829458411862562\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.81      0.78       382\n",
            "             ecology       0.90      0.74      0.81       869\n",
            "            economic       0.83      0.64      0.72      1389\n",
            "          geophysics       0.94      0.91      0.92      1182\n",
            "  gravitional_theory       0.80      0.94      0.86       110\n",
            "               hydro       0.76      0.85      0.80       335\n",
            "                math       0.91      0.79      0.85      1319\n",
            "              metals       0.62      0.93      0.75       181\n",
            "          networking       0.84      0.87      0.85       325\n",
            "        neuroscience       0.90      0.98      0.94       287\n",
            "        oceanography       0.81      0.90      0.85       970\n",
            "             politic       0.53      0.86      0.66       583\n",
            "           sociology       0.58      0.50      0.53       719\n",
            "software_engineering       0.82      0.77      0.79       504\n",
            "          statistics       0.78      0.84      0.81       627\n",
            "    theory_computing       0.64      0.77      0.70       422\n",
            "\n",
            "            accuracy                           0.79     10204\n",
            "           macro avg       0.77      0.82      0.79     10204\n",
            "        weighted avg       0.80      0.79      0.79     10204\n",
            "\n",
            "time: 11.4 s (started: 2024-08-16 14:16:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuYa4bJsKlm7",
        "outputId": "e217098c-644c-409d-fa85-a734ae97b42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 408 ms (started: 2024-08-16 14:16:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 30 ❎"
      ],
      "metadata": {
        "id": "mfmM0bw3Klm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "nOCszwuKKlm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtcU-4YWKlm7",
        "outputId": "bb650f3f-6246-4aa7-ace4-cfb51e7c39c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1956, Train: 0.7292, Test: 0.6424\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0072, Train: 0.8792, Test: 0.7892\n",
            "Early stopping:  1.5474217959585654\n",
            "Epoch: 003, Loss: 0.4179, Train: 0.8958, Test: 0.7959\n",
            "Early stopping:  1.4635604912794384\n",
            "Epoch: 004, Loss: 0.3117, Train: 0.9042, Test: 0.8056\n",
            "Early stopping:  1.3436327421728869\n",
            "Epoch: 005, Loss: 0.2364, Train: 0.9313, Test: 0.8048\n",
            "Early stopping:  1.2460640983461424\n",
            "Epoch: 006, Loss: 0.1841, Train: 0.9583, Test: 0.8109\n",
            "Early stopping:  0.33362976503157254\n",
            "Epoch: 007, Loss: 0.1232, Train: 0.9729, Test: 0.8124\n",
            "Early stopping:  0.11453256495284614\n",
            "Epoch: 008, Loss: 0.0795, Train: 0.9896, Test: 0.8074\n",
            "Early stopping:  0.09166117000713782\n",
            "Epoch: 009, Loss: 0.0556, Train: 0.9896, Test: 0.8035\n",
            "Early stopping:  0.07452725803192566\n",
            "Epoch: 010, Loss: 0.0441, Train: 0.9958, Test: 0.7993\n",
            "Early stopping:  0.057214121596334225\n",
            "Epoch: 011, Loss: 0.0310, Train: 1.0000, Test: 0.7980\n",
            "Early stopping:  0.03628687181287574\n",
            "Epoch: 012, Loss: 0.0187, Train: 1.0000, Test: 0.7961\n",
            "Early stopping:  0.02339234928278989\n",
            "Epoch: 013, Loss: 0.0151, Train: 1.0000, Test: 0.7955\n",
            "Early stopping:  0.017036087739972075\n",
            "Epoch: 014, Loss: 0.0112, Train: 1.0000, Test: 0.7938\n",
            "Early stopping:  0.01344452200570297\n",
            "Epoch: 015, Loss: 0.0076, Train: 1.0000, Test: 0.7904\n",
            "Early stopping:  0.009002529095074601\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.77      0.76       372\n",
            "             ecology       0.81      0.76      0.78       859\n",
            "            economic       0.78      0.71      0.74      1379\n",
            "          geophysics       0.89      0.92      0.91      1172\n",
            "  gravitional_theory       0.77      0.92      0.84       100\n",
            "               hydro       0.80      0.82      0.81       325\n",
            "                math       0.94      0.72      0.82      1309\n",
            "              metals       0.57      0.93      0.71       171\n",
            "          networking       0.83      0.90      0.86       315\n",
            "        neuroscience       0.94      0.88      0.91       277\n",
            "        oceanography       0.78      0.82      0.80       960\n",
            "             politic       0.66      0.78      0.71       573\n",
            "           sociology       0.65      0.66      0.66       709\n",
            "software_engineering       0.79      0.87      0.83       494\n",
            "          statistics       0.75      0.79      0.77       617\n",
            "    theory_computing       0.72      0.80      0.76       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.78      0.82      0.79     10044\n",
            "        weighted avg       0.80      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.4410, Train: 0.6417, Test: 0.5899\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1128, Train: 0.8083, Test: 0.7555\n",
            "Early stopping:  1.6463499746013972\n",
            "Epoch: 003, Loss: 0.5688, Train: 0.8375, Test: 0.7785\n",
            "Early stopping:  1.5256963628971087\n",
            "Epoch: 004, Loss: 0.4766, Train: 0.8896, Test: 0.7938\n",
            "Early stopping:  1.3894693456546738\n",
            "Epoch: 005, Loss: 0.3851, Train: 0.9000, Test: 0.7975\n",
            "Early stopping:  1.286032438644208\n",
            "Epoch: 006, Loss: 0.2882, Train: 0.9208, Test: 0.7837\n",
            "Early stopping:  0.32281147684560524\n",
            "Epoch: 007, Loss: 0.2361, Train: 0.9313, Test: 0.7749\n",
            "Early stopping:  0.1355989318057667\n",
            "Epoch: 008, Loss: 0.1959, Train: 0.9479, Test: 0.7742\n",
            "Early stopping:  0.11419690238966593\n",
            "Epoch: 009, Loss: 0.1593, Train: 0.9458, Test: 0.7746\n",
            "Early stopping:  0.08807109808392702\n",
            "Epoch: 010, Loss: 0.1351, Train: 0.9646, Test: 0.7761\n",
            "Early stopping:  0.06107674556874749\n",
            "Epoch: 011, Loss: 0.1013, Train: 0.9854, Test: 0.7743\n",
            "Early stopping:  0.05241568710684915\n",
            "Epoch: 012, Loss: 0.0675, Train: 0.9896, Test: 0.7689\n",
            "Early stopping:  0.049858431775978256\n",
            "Epoch: 013, Loss: 0.0509, Train: 0.9958, Test: 0.7714\n",
            "Early stopping:  0.04521704201783562\n",
            "Epoch: 014, Loss: 0.0368, Train: 0.9979, Test: 0.7725\n",
            "Early stopping:  0.03982876611417602\n",
            "Epoch: 015, Loss: 0.0296, Train: 1.0000, Test: 0.7692\n",
            "Early stopping:  0.028577108357814542\n",
            "Epoch: 016, Loss: 0.0249, Train: 1.0000, Test: 0.7632\n",
            "Early stopping:  0.017315160464740204\n",
            "Epoch: 017, Loss: 0.0202, Train: 1.0000, Test: 0.7586\n",
            "Early stopping:  0.011981476333920042\n",
            "Epoch: 018, Loss: 0.0169, Train: 1.0000, Test: 0.7573\n",
            "Early stopping:  0.007865428880545458\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.64      0.81      0.71       372\n",
            "             ecology       0.82      0.77      0.80       859\n",
            "            economic       0.82      0.58      0.68      1379\n",
            "          geophysics       0.94      0.82      0.87      1172\n",
            "  gravitional_theory       0.84      0.91      0.87       100\n",
            "               hydro       0.58      0.90      0.71       325\n",
            "                math       0.89      0.75      0.81      1309\n",
            "              metals       0.49      0.95      0.65       171\n",
            "          networking       0.80      0.88      0.84       315\n",
            "        neuroscience       0.89      0.98      0.93       277\n",
            "        oceanography       0.85      0.85      0.85       960\n",
            "             politic       0.54      0.65      0.59       573\n",
            "           sociology       0.61      0.58      0.60       709\n",
            "software_engineering       0.89      0.70      0.78       494\n",
            "          statistics       0.61      0.88      0.72       617\n",
            "    theory_computing       0.69      0.77      0.73       412\n",
            "\n",
            "            accuracy                           0.76     10044\n",
            "           macro avg       0.74      0.80      0.76     10044\n",
            "        weighted avg       0.78      0.76      0.76     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.8990, Train: 0.8000, Test: 0.6989\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.7459, Train: 0.8521, Test: 0.7626\n",
            "Early stopping:  1.5224781530449085\n",
            "Epoch: 003, Loss: 0.4797, Train: 0.8979, Test: 0.7993\n",
            "Early stopping:  1.3266495688314015\n",
            "Epoch: 004, Loss: 0.3046, Train: 0.9104, Test: 0.7962\n",
            "Early stopping:  1.2081763594411739\n",
            "Epoch: 005, Loss: 0.2542, Train: 0.9479, Test: 0.8003\n",
            "Early stopping:  1.1136997427457447\n",
            "Epoch: 006, Loss: 0.1691, Train: 0.9604, Test: 0.8034\n",
            "Early stopping:  0.22871178008193846\n",
            "Epoch: 007, Loss: 0.1241, Train: 0.9729, Test: 0.8031\n",
            "Early stopping:  0.13857995502607967\n",
            "Epoch: 008, Loss: 0.0832, Train: 0.9792, Test: 0.7932\n",
            "Early stopping:  0.09139493806345214\n",
            "Epoch: 009, Loss: 0.0697, Train: 0.9917, Test: 0.7882\n",
            "Early stopping:  0.07468967036503567\n",
            "Epoch: 010, Loss: 0.0478, Train: 0.9958, Test: 0.7859\n",
            "Early stopping:  0.04815717561967357\n",
            "Epoch: 011, Loss: 0.0328, Train: 0.9938, Test: 0.7843\n",
            "Early stopping:  0.03522241815561239\n",
            "Epoch: 012, Loss: 0.0221, Train: 0.9979, Test: 0.7844\n",
            "Early stopping:  0.025305809126586087\n",
            "Epoch: 013, Loss: 0.0164, Train: 0.9979, Test: 0.7803\n",
            "Early stopping:  0.021492271179063348\n",
            "Epoch: 014, Loss: 0.0144, Train: 1.0000, Test: 0.7803\n",
            "Early stopping:  0.013790569760868985\n",
            "Epoch: 015, Loss: 0.0114, Train: 1.0000, Test: 0.7799\n",
            "Early stopping:  0.008450592606591237\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.77      0.77       372\n",
            "             ecology       0.81      0.88      0.84       859\n",
            "            economic       0.81      0.61      0.70      1379\n",
            "          geophysics       0.95      0.86      0.90      1172\n",
            "  gravitional_theory       0.56      0.95      0.70       100\n",
            "               hydro       0.71      0.87      0.78       325\n",
            "                math       0.91      0.78      0.84      1309\n",
            "              metals       0.60      0.89      0.72       171\n",
            "          networking       0.63      0.96      0.76       315\n",
            "        neuroscience       0.95      0.94      0.94       277\n",
            "        oceanography       0.91      0.80      0.85       960\n",
            "             politic       0.62      0.74      0.68       573\n",
            "           sociology       0.56      0.61      0.59       709\n",
            "software_engineering       0.82      0.84      0.83       494\n",
            "          statistics       0.65      0.83      0.73       617\n",
            "    theory_computing       0.78      0.66      0.71       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.75      0.81      0.77     10044\n",
            "        weighted avg       0.80      0.78      0.78     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1922, Train: 0.6875, Test: 0.6212\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0434, Train: 0.8375, Test: 0.7486\n",
            "Early stopping:  1.5194664656908656\n",
            "Epoch: 003, Loss: 0.5414, Train: 0.8562, Test: 0.7594\n",
            "Early stopping:  1.4080907358395314\n",
            "Epoch: 004, Loss: 0.4678, Train: 0.8938, Test: 0.7873\n",
            "Early stopping:  1.2798384845420574\n",
            "Epoch: 005, Loss: 0.3581, Train: 0.9083, Test: 0.7941\n",
            "Early stopping:  1.1874997262660807\n",
            "Epoch: 006, Loss: 0.2887, Train: 0.9354, Test: 0.7974\n",
            "Early stopping:  0.2978430964013258\n",
            "Epoch: 007, Loss: 0.1824, Train: 0.9333, Test: 0.7860\n",
            "Early stopping:  0.14218446080073385\n",
            "Epoch: 008, Loss: 0.1483, Train: 0.9542, Test: 0.7721\n",
            "Early stopping:  0.13028685688868377\n",
            "Epoch: 009, Loss: 0.1254, Train: 0.9625, Test: 0.7752\n",
            "Early stopping:  0.09912842185222838\n",
            "Epoch: 010, Loss: 0.0932, Train: 0.9812, Test: 0.7810\n",
            "Early stopping:  0.07512850646793036\n",
            "Epoch: 011, Loss: 0.0611, Train: 0.9917, Test: 0.7874\n",
            "Early stopping:  0.04715024904310312\n",
            "Epoch: 012, Loss: 0.0402, Train: 0.9917, Test: 0.7853\n",
            "Early stopping:  0.044490084545527296\n",
            "Epoch: 013, Loss: 0.0388, Train: 0.9917, Test: 0.7813\n",
            "Early stopping:  0.037192058258373094\n",
            "Epoch: 014, Loss: 0.0351, Train: 0.9938, Test: 0.7798\n",
            "Early stopping:  0.024296228162019935\n",
            "Epoch: 015, Loss: 0.0256, Train: 0.9958, Test: 0.7797\n",
            "Early stopping:  0.013012029171644192\n",
            "Epoch: 016, Loss: 0.0181, Train: 0.9979, Test: 0.7786\n",
            "Early stopping:  0.009427672042518003\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.80      0.77       372\n",
            "             ecology       0.85      0.78      0.81       859\n",
            "            economic       0.85      0.66      0.74      1379\n",
            "          geophysics       0.93      0.87      0.90      1172\n",
            "  gravitional_theory       0.64      0.97      0.77       100\n",
            "               hydro       0.65      0.86      0.74       325\n",
            "                math       0.96      0.61      0.74      1309\n",
            "              metals       0.50      0.94      0.65       171\n",
            "          networking       0.82      0.81      0.81       315\n",
            "        neuroscience       0.85      0.97      0.91       277\n",
            "        oceanography       0.82      0.87      0.85       960\n",
            "             politic       0.72      0.80      0.76       573\n",
            "           sociology       0.66      0.71      0.68       709\n",
            "software_engineering       0.89      0.82      0.85       494\n",
            "          statistics       0.61      0.87      0.72       617\n",
            "    theory_computing       0.57      0.82      0.67       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.75      0.82      0.77     10044\n",
            "        weighted avg       0.81      0.78      0.78     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.5081, Train: 0.6687, Test: 0.5957\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.1180, Train: 0.8479, Test: 0.7743\n",
            "Early stopping:  1.6900404935858513\n",
            "Epoch: 003, Loss: 0.5035, Train: 0.8667, Test: 0.7728\n",
            "Early stopping:  1.5873331663935597\n",
            "Epoch: 004, Loss: 0.4010, Train: 0.9083, Test: 0.7937\n",
            "Early stopping:  1.4519090160763835\n",
            "Epoch: 005, Loss: 0.2997, Train: 0.9229, Test: 0.8083\n",
            "Early stopping:  1.3474288684329476\n",
            "Epoch: 006, Loss: 0.2168, Train: 0.9250, Test: 0.8067\n",
            "Early stopping:  0.35768921011029775\n",
            "Epoch: 007, Loss: 0.1747, Train: 0.9479, Test: 0.8057\n",
            "Early stopping:  0.13453146275748765\n",
            "Epoch: 008, Loss: 0.1433, Train: 0.9646, Test: 0.8134\n",
            "Early stopping:  0.10414803791265356\n",
            "Epoch: 009, Loss: 0.1048, Train: 0.9771, Test: 0.8158\n",
            "Early stopping:  0.07485084160953194\n",
            "Epoch: 010, Loss: 0.0738, Train: 0.9854, Test: 0.8153\n",
            "Early stopping:  0.05635048594955928\n",
            "Epoch: 011, Loss: 0.0522, Train: 0.9896, Test: 0.8096\n",
            "Early stopping:  0.04994412475819574\n",
            "Epoch: 012, Loss: 0.0414, Train: 0.9938, Test: 0.8076\n",
            "Early stopping:  0.04146987018715781\n",
            "Epoch: 013, Loss: 0.0319, Train: 1.0000, Test: 0.8076\n",
            "Early stopping:  0.02912687455106261\n",
            "Epoch: 014, Loss: 0.0234, Train: 0.9979, Test: 0.8062\n",
            "Early stopping:  0.01955723706262232\n",
            "Epoch: 015, Loss: 0.0190, Train: 0.9979, Test: 0.8039\n",
            "Early stopping:  0.013468376884389894\n",
            "Epoch: 016, Loss: 0.0151, Train: 1.0000, Test: 0.8016\n",
            "Early stopping:  0.01053885115236911\n",
            "Epoch: 017, Loss: 0.0107, Train: 1.0000, Test: 0.7982\n",
            "Early stopping:  0.008126209528014051\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.77      0.78       372\n",
            "             ecology       0.84      0.80      0.82       859\n",
            "            economic       0.81      0.70      0.75      1379\n",
            "          geophysics       0.95      0.91      0.93      1172\n",
            "  gravitional_theory       0.71      0.96      0.81       100\n",
            "               hydro       0.59      0.87      0.71       325\n",
            "                math       0.88      0.81      0.84      1309\n",
            "              metals       0.72      0.91      0.80       171\n",
            "          networking       0.73      0.93      0.82       315\n",
            "        neuroscience       0.95      0.97      0.96       277\n",
            "        oceanography       0.89      0.80      0.84       960\n",
            "             politic       0.65      0.79      0.71       573\n",
            "           sociology       0.61      0.58      0.60       709\n",
            "software_engineering       0.85      0.84      0.85       494\n",
            "          statistics       0.72      0.81      0.76       617\n",
            "    theory_computing       0.77      0.74      0.75       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.78      0.82      0.80     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.0631, Train: 0.7125, Test: 0.6378\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 1.0929, Train: 0.8271, Test: 0.7386\n",
            "Early stopping:  1.393104819058596\n",
            "Epoch: 003, Loss: 0.5310, Train: 0.8792, Test: 0.7874\n",
            "Early stopping:  1.3297194008432214\n",
            "Epoch: 004, Loss: 0.4152, Train: 0.8979, Test: 0.8096\n",
            "Early stopping:  1.2278977831357436\n",
            "Epoch: 005, Loss: 0.3343, Train: 0.9250, Test: 0.8161\n",
            "Early stopping:  1.143674046420436\n",
            "Epoch: 006, Loss: 0.2424, Train: 0.9417, Test: 0.7866\n",
            "Early stopping:  0.3357359792081406\n",
            "Epoch: 007, Loss: 0.1612, Train: 0.9479, Test: 0.7687\n",
            "Early stopping:  0.14457520652392944\n",
            "Epoch: 008, Loss: 0.1408, Train: 0.9708, Test: 0.7849\n",
            "Early stopping:  0.11607304152344068\n",
            "Epoch: 009, Loss: 0.0877, Train: 0.9729, Test: 0.7821\n",
            "Early stopping:  0.09646358714365595\n",
            "Epoch: 010, Loss: 0.0839, Train: 0.9792, Test: 0.7826\n",
            "Early stopping:  0.06472809792077143\n",
            "Epoch: 011, Loss: 0.0737, Train: 0.9833, Test: 0.7894\n",
            "Early stopping:  0.0389288415436382\n",
            "Epoch: 012, Loss: 0.0532, Train: 0.9917, Test: 0.7954\n",
            "Early stopping:  0.032466200470903234\n",
            "Epoch: 013, Loss: 0.0323, Train: 1.0000, Test: 0.7919\n",
            "Early stopping:  0.02317789227546931\n",
            "Epoch: 014, Loss: 0.0200, Train: 0.9979, Test: 0.7854\n",
            "Early stopping:  0.0269235154035411\n",
            "Epoch: 015, Loss: 0.0176, Train: 0.9979, Test: 0.7781\n",
            "Early stopping:  0.023823977826692884\n",
            "Epoch: 016, Loss: 0.0195, Train: 1.0000, Test: 0.7776\n",
            "Early stopping:  0.014979095940495287\n",
            "Epoch: 017, Loss: 0.0156, Train: 1.0000, Test: 0.7830\n",
            "Early stopping:  0.006559847525833011\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.69      0.86      0.76       372\n",
            "             ecology       0.73      0.91      0.81       859\n",
            "            economic       0.81      0.63      0.71      1379\n",
            "          geophysics       0.92      0.90      0.91      1172\n",
            "  gravitional_theory       0.89      0.89      0.89       100\n",
            "               hydro       0.78      0.73      0.75       325\n",
            "                math       0.88      0.80      0.84      1309\n",
            "              metals       0.56      0.92      0.70       171\n",
            "          networking       0.85      0.88      0.86       315\n",
            "        neuroscience       0.93      0.98      0.95       277\n",
            "        oceanography       0.94      0.70      0.80       960\n",
            "             politic       0.66      0.72      0.69       573\n",
            "           sociology       0.55      0.73      0.63       709\n",
            "software_engineering       0.88      0.73      0.80       494\n",
            "          statistics       0.72      0.80      0.76       617\n",
            "    theory_computing       0.73      0.74      0.74       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.78      0.81      0.79     10044\n",
            "        weighted avg       0.80      0.78      0.78     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.1638, Train: 0.7333, Test: 0.7286\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.8913, Train: 0.8333, Test: 0.7650\n",
            "Early stopping:  1.606870389398515\n",
            "Epoch: 003, Loss: 0.5281, Train: 0.8812, Test: 0.7921\n",
            "Early stopping:  1.4284427488475817\n",
            "Epoch: 004, Loss: 0.3624, Train: 0.8854, Test: 0.7947\n",
            "Early stopping:  1.303751769336669\n",
            "Epoch: 005, Loss: 0.2977, Train: 0.9271, Test: 0.7828\n",
            "Early stopping:  1.204599942250681\n",
            "Epoch: 006, Loss: 0.2228, Train: 0.9458, Test: 0.7924\n",
            "Early stopping:  0.2658905204081527\n",
            "Epoch: 007, Loss: 0.1559, Train: 0.9667, Test: 0.7968\n",
            "Early stopping:  0.1429828154056695\n",
            "Epoch: 008, Loss: 0.1051, Train: 0.9854, Test: 0.7992\n",
            "Early stopping:  0.10400300766294396\n",
            "Epoch: 009, Loss: 0.0765, Train: 0.9875, Test: 0.7968\n",
            "Early stopping:  0.08978006579008868\n",
            "Epoch: 010, Loss: 0.0583, Train: 0.9917, Test: 0.7895\n",
            "Early stopping:  0.06653055081494111\n",
            "Epoch: 011, Loss: 0.0406, Train: 0.9958, Test: 0.7808\n",
            "Early stopping:  0.04515209814952244\n",
            "Epoch: 012, Loss: 0.0323, Train: 0.9958, Test: 0.7773\n",
            "Early stopping:  0.02921811762815182\n",
            "Epoch: 013, Loss: 0.0265, Train: 0.9979, Test: 0.7776\n",
            "Early stopping:  0.020457127001716226\n",
            "Epoch: 014, Loss: 0.0193, Train: 1.0000, Test: 0.7792\n",
            "Early stopping:  0.01501239853508285\n",
            "Epoch: 015, Loss: 0.0142, Train: 1.0000, Test: 0.7800\n",
            "Early stopping:  0.01046291530384113\n",
            "Epoch: 016, Loss: 0.0114, Train: 1.0000, Test: 0.7809\n",
            "Early stopping:  0.008669903766063568\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.83      0.60      0.70       372\n",
            "             ecology       0.81      0.83      0.82       859\n",
            "            economic       0.82      0.74      0.78      1379\n",
            "          geophysics       0.93      0.85      0.89      1172\n",
            "  gravitional_theory       0.73      0.95      0.82       100\n",
            "               hydro       0.68      0.85      0.75       325\n",
            "                math       0.92      0.70      0.79      1309\n",
            "              metals       0.49      0.95      0.64       171\n",
            "          networking       0.77      0.89      0.83       315\n",
            "        neuroscience       0.94      0.94      0.94       277\n",
            "        oceanography       0.83      0.82      0.82       960\n",
            "             politic       0.67      0.62      0.64       573\n",
            "           sociology       0.62      0.69      0.66       709\n",
            "software_engineering       0.74      0.87      0.80       494\n",
            "          statistics       0.70      0.88      0.78       617\n",
            "    theory_computing       0.67      0.72      0.69       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.76      0.81      0.77     10044\n",
            "        weighted avg       0.80      0.78      0.78     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3405, Train: 0.7417, Test: 0.7217\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9380, Train: 0.8521, Test: 0.7805\n",
            "Early stopping:  1.698845138827171\n",
            "Epoch: 003, Loss: 0.4709, Train: 0.8708, Test: 0.7881\n",
            "Early stopping:  1.5397740958794586\n",
            "Epoch: 004, Loss: 0.3855, Train: 0.9062, Test: 0.7849\n",
            "Early stopping:  1.3925579132634847\n",
            "Epoch: 005, Loss: 0.3069, Train: 0.9208, Test: 0.7962\n",
            "Early stopping:  1.2826755645932777\n",
            "Epoch: 006, Loss: 0.2227, Train: 0.9437, Test: 0.8042\n",
            "Early stopping:  0.28009593639674824\n",
            "Epoch: 007, Loss: 0.1520, Train: 0.9604, Test: 0.7982\n",
            "Early stopping:  0.1266428081386426\n",
            "Epoch: 008, Loss: 0.1257, Train: 0.9750, Test: 0.7903\n",
            "Early stopping:  0.10810451250493935\n",
            "Epoch: 009, Loss: 0.0808, Train: 0.9812, Test: 0.7833\n",
            "Early stopping:  0.08870828967242195\n",
            "Epoch: 010, Loss: 0.0650, Train: 0.9917, Test: 0.7881\n",
            "Early stopping:  0.06273654320385247\n",
            "Epoch: 011, Loss: 0.0357, Train: 0.9979, Test: 0.7904\n",
            "Early stopping:  0.046786971294331174\n",
            "Epoch: 012, Loss: 0.0244, Train: 0.9938, Test: 0.7888\n",
            "Early stopping:  0.04008818067196423\n",
            "Epoch: 013, Loss: 0.0276, Train: 1.0000, Test: 0.7852\n",
            "Early stopping:  0.02489156992715653\n",
            "Epoch: 014, Loss: 0.0146, Train: 1.0000, Test: 0.7785\n",
            "Early stopping:  0.01916735103821827\n",
            "Epoch: 015, Loss: 0.0095, Train: 1.0000, Test: 0.7725\n",
            "Early stopping:  0.010417076341822817\n",
            "Epoch: 016, Loss: 0.0096, Train: 1.0000, Test: 0.7693\n",
            "Early stopping:  0.008421562121021712\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.73      0.81      0.76       372\n",
            "             ecology       0.78      0.84      0.81       859\n",
            "            economic       0.77      0.65      0.70      1379\n",
            "          geophysics       0.96      0.85      0.90      1172\n",
            "  gravitional_theory       0.54      0.93      0.68       100\n",
            "               hydro       0.60      0.88      0.71       325\n",
            "                math       0.92      0.65      0.76      1309\n",
            "              metals       0.53      0.91      0.67       171\n",
            "          networking       0.77      0.91      0.83       315\n",
            "        neuroscience       0.94      0.96      0.95       277\n",
            "        oceanography       0.85      0.86      0.86       960\n",
            "             politic       0.64      0.73      0.68       573\n",
            "           sociology       0.63      0.55      0.59       709\n",
            "software_engineering       0.80      0.85      0.82       494\n",
            "          statistics       0.68      0.86      0.76       617\n",
            "    theory_computing       0.70      0.71      0.70       412\n",
            "\n",
            "            accuracy                           0.77     10044\n",
            "           macro avg       0.74      0.81      0.76     10044\n",
            "        weighted avg       0.79      0.77      0.77     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.3840, Train: 0.7292, Test: 0.6492\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9399, Train: 0.8313, Test: 0.7392\n",
            "Early stopping:  1.7282292051125416\n",
            "Epoch: 003, Loss: 0.5659, Train: 0.8875, Test: 0.7908\n",
            "Early stopping:  1.5305100344675993\n",
            "Epoch: 004, Loss: 0.3889, Train: 0.9000, Test: 0.8051\n",
            "Early stopping:  1.3952435992571903\n",
            "Epoch: 005, Loss: 0.3054, Train: 0.9083, Test: 0.7961\n",
            "Early stopping:  1.2906514916454286\n",
            "Epoch: 006, Loss: 0.2320, Train: 0.9333, Test: 0.7907\n",
            "Early stopping:  0.28238227739174376\n",
            "Epoch: 007, Loss: 0.1779, Train: 0.9583, Test: 0.7918\n",
            "Early stopping:  0.15196185134521725\n",
            "Epoch: 008, Loss: 0.1155, Train: 0.9667, Test: 0.7909\n",
            "Early stopping:  0.10696682556029621\n",
            "Epoch: 009, Loss: 0.0876, Train: 0.9792, Test: 0.7887\n",
            "Early stopping:  0.08812765122035038\n",
            "Epoch: 010, Loss: 0.0714, Train: 0.9854, Test: 0.7792\n",
            "Early stopping:  0.06692384881543445\n",
            "Epoch: 011, Loss: 0.0585, Train: 0.9917, Test: 0.7764\n",
            "Early stopping:  0.0473884804565974\n",
            "Epoch: 012, Loss: 0.0419, Train: 0.9979, Test: 0.7782\n",
            "Early stopping:  0.028187557850857366\n",
            "Epoch: 013, Loss: 0.0262, Train: 0.9958, Test: 0.7833\n",
            "Early stopping:  0.02407161784943898\n",
            "Epoch: 014, Loss: 0.0219, Train: 0.9958, Test: 0.7854\n",
            "Early stopping:  0.021044661590185552\n",
            "Epoch: 015, Loss: 0.0220, Train: 0.9958, Test: 0.7869\n",
            "Early stopping:  0.01593328997993769\n",
            "Epoch: 016, Loss: 0.0167, Train: 0.9979, Test: 0.7865\n",
            "Early stopping:  0.009635664115651152\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.74      0.76       372\n",
            "             ecology       0.85      0.79      0.82       859\n",
            "            economic       0.83      0.65      0.73      1379\n",
            "          geophysics       0.96      0.86      0.91      1172\n",
            "  gravitional_theory       0.72      0.94      0.82       100\n",
            "               hydro       0.72      0.81      0.76       325\n",
            "                math       0.85      0.81      0.83      1309\n",
            "              metals       0.59      0.92      0.72       171\n",
            "          networking       0.75      0.94      0.83       315\n",
            "        neuroscience       0.95      0.94      0.94       277\n",
            "        oceanography       0.80      0.88      0.84       960\n",
            "             politic       0.59      0.83      0.69       573\n",
            "           sociology       0.68      0.66      0.67       709\n",
            "software_engineering       0.90      0.70      0.79       494\n",
            "          statistics       0.68      0.76      0.72       617\n",
            "    theory_computing       0.69      0.77      0.73       412\n",
            "\n",
            "            accuracy                           0.79     10044\n",
            "           macro avg       0.77      0.81      0.78     10044\n",
            "        weighted avg       0.80      0.79      0.79     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 3.6660, Train: 0.7333, Test: 0.6186\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 0.9900, Train: 0.8292, Test: 0.7399\n",
            "Early stopping:  1.892211592340969\n",
            "Epoch: 003, Loss: 0.5854, Train: 0.8750, Test: 0.7680\n",
            "Early stopping:  1.6740317891638516\n",
            "Epoch: 004, Loss: 0.4398, Train: 0.8958, Test: 0.7761\n",
            "Early stopping:  1.515099531095285\n",
            "Epoch: 005, Loss: 0.3594, Train: 0.9021, Test: 0.7821\n",
            "Early stopping:  1.3952602344915772\n",
            "Epoch: 006, Loss: 0.2858, Train: 0.9146, Test: 0.7817\n",
            "Early stopping:  0.27907033232444095\n",
            "Epoch: 007, Loss: 0.2322, Train: 0.9437, Test: 0.8078\n",
            "Early stopping:  0.1386869145611076\n",
            "Epoch: 008, Loss: 0.1761, Train: 0.9479, Test: 0.7981\n",
            "Early stopping:  0.103948071253451\n",
            "Epoch: 009, Loss: 0.1673, Train: 0.9708, Test: 0.8033\n",
            "Early stopping:  0.08015811116763845\n",
            "Epoch: 010, Loss: 0.0985, Train: 0.9729, Test: 0.7853\n",
            "Early stopping:  0.07075298957312119\n",
            "Epoch: 011, Loss: 0.0855, Train: 0.9625, Test: 0.7707\n",
            "Early stopping:  0.060274769730227565\n",
            "Epoch: 012, Loss: 0.0870, Train: 0.9812, Test: 0.7750\n",
            "Early stopping:  0.04496112540665754\n",
            "Epoch: 013, Loss: 0.0576, Train: 0.9875, Test: 0.7718\n",
            "Early stopping:  0.04093815459052159\n",
            "Epoch: 014, Loss: 0.0495, Train: 0.9896, Test: 0.7743\n",
            "Early stopping:  0.020965803716441984\n",
            "Epoch: 015, Loss: 0.0418, Train: 0.9896, Test: 0.7847\n",
            "Early stopping:  0.020811287829369526\n",
            "Epoch: 016, Loss: 0.0300, Train: 0.9896, Test: 0.7856\n",
            "Early stopping:  0.02146147093455906\n",
            "Epoch: 017, Loss: 0.0300, Train: 0.9938, Test: 0.7815\n",
            "Early stopping:  0.012098050321916165\n",
            "Epoch: 018, Loss: 0.0285, Train: 1.0000, Test: 0.7790\n",
            "Early stopping:  0.009274745125249208\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.74      0.81      0.77       372\n",
            "             ecology       0.78      0.88      0.83       859\n",
            "            economic       0.74      0.62      0.67      1379\n",
            "          geophysics       0.89      0.92      0.90      1172\n",
            "  gravitional_theory       0.84      0.89      0.86       100\n",
            "               hydro       0.65      0.85      0.74       325\n",
            "                math       0.92      0.71      0.80      1309\n",
            "              metals       0.66      0.90      0.76       171\n",
            "          networking       0.75      0.85      0.80       315\n",
            "        neuroscience       0.94      0.97      0.96       277\n",
            "        oceanography       0.91      0.74      0.82       960\n",
            "             politic       0.69      0.74      0.71       573\n",
            "           sociology       0.64      0.68      0.66       709\n",
            "software_engineering       0.88      0.82      0.85       494\n",
            "          statistics       0.68      0.79      0.73       617\n",
            "    theory_computing       0.59      0.85      0.70       412\n",
            "\n",
            "            accuracy                           0.78     10044\n",
            "           macro avg       0.77      0.81      0.79     10044\n",
            "        weighted avg       0.79      0.78      0.78     10044\n",
            "\n",
            "time: 6.53 s (started: 2024-08-16 14:16:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFFLwLvIKlm7",
        "outputId": "7096221a-5051-4a56-e524-4ad539001785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 436 ms (started: 2024-08-16 14:16:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "YrH10CYOKlm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDhx3S9WKlm7",
        "outputId": "04900a6b-5aa2-439f-aec2-d52cc6038fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7767, Train: 0.7458, Test: 0.6235\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4904, Train: 0.8500, Test: 0.7553\n",
            "Early stopping:  0.20247346024633492\n",
            "Epoch: 003, Loss: 2.1474, Train: 0.8583, Test: 0.7702\n",
            "Early stopping:  0.31509806393836925\n",
            "Epoch: 004, Loss: 1.7572, Train: 0.8708, Test: 0.7766\n",
            "Early stopping:  0.4401836050940379\n",
            "Epoch: 005, Loss: 1.3748, Train: 0.8812, Test: 0.7888\n",
            "Early stopping:  0.5602824452128944\n",
            "Epoch: 006, Loss: 1.0398, Train: 0.8812, Test: 0.7975\n",
            "Early stopping:  0.5810817295928685\n",
            "Epoch: 007, Loss: 0.7726, Train: 0.8875, Test: 0.8056\n",
            "Early stopping:  0.5496551012478964\n",
            "Epoch: 008, Loss: 0.5768, Train: 0.8979, Test: 0.8091\n",
            "Early stopping:  0.4721709892906704\n",
            "Epoch: 009, Loss: 0.4442, Train: 0.9083, Test: 0.8114\n",
            "Early stopping:  0.3729464937993651\n",
            "Epoch: 010, Loss: 0.3572, Train: 0.9083, Test: 0.8127\n",
            "Early stopping:  0.27371908855402627\n",
            "Epoch: 011, Loss: 0.2971, Train: 0.9208, Test: 0.8137\n",
            "Early stopping:  0.18996140358935754\n",
            "Epoch: 012, Loss: 0.2513, Train: 0.9271, Test: 0.8160\n",
            "Early stopping:  0.1291025708859054\n",
            "Epoch: 013, Loss: 0.2139, Train: 0.9458, Test: 0.8186\n",
            "Early stopping:  0.090871317010806\n",
            "Epoch: 014, Loss: 0.1822, Train: 0.9521, Test: 0.8205\n",
            "Early stopping:  0.06906923192298403\n",
            "Epoch: 015, Loss: 0.1545, Train: 0.9646, Test: 0.8227\n",
            "Early stopping:  0.05630354444862067\n",
            "Epoch: 016, Loss: 0.1303, Train: 0.9688, Test: 0.8223\n",
            "Early stopping:  0.04784747494371067\n",
            "Epoch: 017, Loss: 0.1092, Train: 0.9812, Test: 0.8231\n",
            "Early stopping:  0.041449986800660504\n",
            "Epoch: 018, Loss: 0.0912, Train: 0.9917, Test: 0.8221\n",
            "Early stopping:  0.036055142334695894\n",
            "Epoch: 019, Loss: 0.0762, Train: 0.9917, Test: 0.8201\n",
            "Early stopping:  0.031088352285478932\n",
            "Epoch: 020, Loss: 0.0638, Train: 0.9938, Test: 0.8189\n",
            "Early stopping:  0.026379603075070767\n",
            "Epoch: 021, Loss: 0.0537, Train: 0.9958, Test: 0.8187\n",
            "Early stopping:  0.022036607960681555\n",
            "Epoch: 022, Loss: 0.0454, Train: 0.9958, Test: 0.8173\n",
            "Early stopping:  0.018175563080049296\n",
            "Epoch: 023, Loss: 0.0387, Train: 1.0000, Test: 0.8157\n",
            "Early stopping:  0.014896183186478054\n",
            "Epoch: 024, Loss: 0.0332, Train: 1.0000, Test: 0.8141\n",
            "Early stopping:  0.012146338409521807\n",
            "Epoch: 025, Loss: 0.0289, Train: 1.0000, Test: 0.8132\n",
            "Early stopping:  0.009817238685556702\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.83      0.79       372\n",
            "             ecology       0.85      0.84      0.84       859\n",
            "            economic       0.81      0.71      0.76      1379\n",
            "          geophysics       0.94      0.91      0.92      1172\n",
            "  gravitional_theory       0.78      0.94      0.85       100\n",
            "               hydro       0.79      0.85      0.82       325\n",
            "                math       0.94      0.75      0.84      1309\n",
            "              metals       0.60      0.93      0.73       171\n",
            "          networking       0.81      0.89      0.85       315\n",
            "        neuroscience       0.94      0.93      0.93       277\n",
            "        oceanography       0.85      0.87      0.86       960\n",
            "             politic       0.70      0.76      0.73       573\n",
            "           sociology       0.68      0.70      0.69       709\n",
            "software_engineering       0.81      0.87      0.84       494\n",
            "          statistics       0.74      0.80      0.77       617\n",
            "    theory_computing       0.68      0.85      0.76       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.84      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7780, Train: 0.6750, Test: 0.5043\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5358, Train: 0.8063, Test: 0.7100\n",
            "Early stopping:  0.17128630963481975\n",
            "Epoch: 003, Loss: 2.2285, Train: 0.8313, Test: 0.7582\n",
            "Early stopping:  0.27542677856075676\n",
            "Epoch: 004, Loss: 1.8683, Train: 0.8479, Test: 0.7765\n",
            "Early stopping:  0.39351082539258647\n",
            "Epoch: 005, Loss: 1.5021, Train: 0.8750, Test: 0.7873\n",
            "Early stopping:  0.5106954042839238\n",
            "Epoch: 006, Loss: 1.1676, Train: 0.8833, Test: 0.7957\n",
            "Early stopping:  0.547729065599962\n",
            "Epoch: 007, Loss: 0.8894, Train: 0.8938, Test: 0.8002\n",
            "Early stopping:  0.534963166258179\n",
            "Epoch: 008, Loss: 0.6792, Train: 0.8938, Test: 0.8020\n",
            "Early stopping:  0.47547191212471374\n",
            "Epoch: 009, Loss: 0.5333, Train: 0.8938, Test: 0.8051\n",
            "Early stopping:  0.38816915166178645\n",
            "Epoch: 010, Loss: 0.4358, Train: 0.8979, Test: 0.8076\n",
            "Early stopping:  0.2932851818606365\n",
            "Epoch: 011, Loss: 0.3698, Train: 0.9042, Test: 0.8086\n",
            "Early stopping:  0.20780038384409877\n",
            "Epoch: 012, Loss: 0.3215, Train: 0.9125, Test: 0.8106\n",
            "Early stopping:  0.1423357428964844\n",
            "Epoch: 013, Loss: 0.2821, Train: 0.9229, Test: 0.8091\n",
            "Early stopping:  0.09922962489143444\n",
            "Epoch: 014, Loss: 0.2484, Train: 0.9271, Test: 0.8068\n",
            "Early stopping:  0.07383587459416116\n",
            "Epoch: 015, Loss: 0.2189, Train: 0.9354, Test: 0.8074\n",
            "Early stopping:  0.059566061362992605\n",
            "Epoch: 016, Loss: 0.1908, Train: 0.9333, Test: 0.8068\n",
            "Early stopping:  0.051448475623636546\n",
            "Epoch: 017, Loss: 0.1638, Train: 0.9458, Test: 0.8057\n",
            "Early stopping:  0.04654182892010377\n",
            "Epoch: 018, Loss: 0.1393, Train: 0.9667, Test: 0.8052\n",
            "Early stopping:  0.043219745660755614\n",
            "Epoch: 019, Loss: 0.1179, Train: 0.9833, Test: 0.8054\n",
            "Early stopping:  0.04014057548050687\n",
            "Epoch: 020, Loss: 0.0996, Train: 0.9875, Test: 0.8044\n",
            "Early stopping:  0.0362130295550717\n",
            "Epoch: 021, Loss: 0.0846, Train: 0.9917, Test: 0.8043\n",
            "Early stopping:  0.03147614546628074\n",
            "Epoch: 022, Loss: 0.0721, Train: 0.9958, Test: 0.8041\n",
            "Early stopping:  0.02666003214430744\n",
            "Epoch: 023, Loss: 0.0618, Train: 0.9958, Test: 0.8020\n",
            "Early stopping:  0.02221588137413599\n",
            "Epoch: 024, Loss: 0.0535, Train: 1.0000, Test: 0.8024\n",
            "Early stopping:  0.018295408935560226\n",
            "Epoch: 025, Loss: 0.0469, Train: 1.0000, Test: 0.8040\n",
            "Early stopping:  0.014971033166819286\n",
            "Epoch: 026, Loss: 0.0414, Train: 1.0000, Test: 0.8043\n",
            "Early stopping:  0.012176861408816802\n",
            "Epoch: 027, Loss: 0.0369, Train: 1.0000, Test: 0.8027\n",
            "Early stopping:  0.009867040438198285\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.75      0.83      0.79       372\n",
            "             ecology       0.84      0.80      0.82       859\n",
            "            economic       0.82      0.69      0.75      1379\n",
            "          geophysics       0.94      0.87      0.90      1172\n",
            "  gravitional_theory       0.86      0.91      0.88       100\n",
            "               hydro       0.71      0.91      0.80       325\n",
            "                math       0.91      0.79      0.84      1309\n",
            "              metals       0.55      0.94      0.69       171\n",
            "          networking       0.80      0.90      0.84       315\n",
            "        neuroscience       0.87      0.99      0.92       277\n",
            "        oceanography       0.85      0.89      0.87       960\n",
            "             politic       0.66      0.74      0.69       573\n",
            "           sociology       0.66      0.61      0.64       709\n",
            "software_engineering       0.86      0.82      0.84       494\n",
            "          statistics       0.75      0.83      0.79       617\n",
            "    theory_computing       0.69      0.81      0.74       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.78      0.83      0.80     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7798, Train: 0.7292, Test: 0.5534\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5241, Train: 0.8396, Test: 0.7080\n",
            "Early stopping:  0.18080306802857962\n",
            "Epoch: 003, Loss: 2.1963, Train: 0.8396, Test: 0.7215\n",
            "Early stopping:  0.2925201439978033\n",
            "Epoch: 004, Loss: 1.8175, Train: 0.8438, Test: 0.7307\n",
            "Early stopping:  0.41653967496833266\n",
            "Epoch: 005, Loss: 1.4411, Train: 0.8438, Test: 0.7425\n",
            "Early stopping:  0.5366073384815557\n",
            "Epoch: 006, Loss: 1.1067, Train: 0.8583, Test: 0.7608\n",
            "Early stopping:  0.567831472868072\n",
            "Epoch: 007, Loss: 0.8347, Train: 0.8812, Test: 0.7816\n",
            "Early stopping:  0.5441272405028179\n",
            "Epoch: 008, Loss: 0.6307, Train: 0.8958, Test: 0.7979\n",
            "Early stopping:  0.474342877452851\n",
            "Epoch: 009, Loss: 0.4903, Train: 0.8979, Test: 0.8098\n",
            "Early stopping:  0.38082788326664546\n",
            "Epoch: 010, Loss: 0.3987, Train: 0.9042, Test: 0.8156\n",
            "Early stopping:  0.2840548540396696\n",
            "Epoch: 011, Loss: 0.3372, Train: 0.9104, Test: 0.8177\n",
            "Early stopping:  0.19914154444830814\n",
            "Epoch: 012, Loss: 0.2905, Train: 0.9229, Test: 0.8185\n",
            "Early stopping:  0.13508599466578805\n",
            "Epoch: 013, Loss: 0.2500, Train: 0.9396, Test: 0.8168\n",
            "Early stopping:  0.09447949554368719\n",
            "Epoch: 014, Loss: 0.2145, Train: 0.9458, Test: 0.8157\n",
            "Early stopping:  0.07244565019845414\n",
            "Epoch: 015, Loss: 0.1848, Train: 0.9563, Test: 0.8158\n",
            "Early stopping:  0.06040316940530628\n",
            "Epoch: 016, Loss: 0.1590, Train: 0.9667, Test: 0.8152\n",
            "Early stopping:  0.05211004088927717\n",
            "Epoch: 017, Loss: 0.1348, Train: 0.9729, Test: 0.8167\n",
            "Early stopping:  0.04536297048235467\n",
            "Epoch: 018, Loss: 0.1135, Train: 0.9812, Test: 0.8182\n",
            "Early stopping:  0.03993784575792305\n",
            "Epoch: 019, Loss: 0.0959, Train: 0.9896, Test: 0.8177\n",
            "Early stopping:  0.035400819738861056\n",
            "Epoch: 020, Loss: 0.0813, Train: 0.9896, Test: 0.8163\n",
            "Early stopping:  0.030861612970349252\n",
            "Epoch: 021, Loss: 0.0687, Train: 0.9938, Test: 0.8152\n",
            "Early stopping:  0.026145940383738242\n",
            "Epoch: 022, Loss: 0.0582, Train: 0.9958, Test: 0.8115\n",
            "Early stopping:  0.021903890324416236\n",
            "Epoch: 023, Loss: 0.0500, Train: 0.9958, Test: 0.8099\n",
            "Early stopping:  0.018293459154558656\n",
            "Epoch: 024, Loss: 0.0437, Train: 0.9979, Test: 0.8087\n",
            "Early stopping:  0.014997380207267725\n",
            "Epoch: 025, Loss: 0.0385, Train: 0.9979, Test: 0.8087\n",
            "Early stopping:  0.011968009970143297\n",
            "Epoch: 026, Loss: 0.0343, Train: 1.0000, Test: 0.8086\n",
            "Early stopping:  0.009458438523382564\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.80      0.82      0.81       372\n",
            "             ecology       0.86      0.86      0.86       859\n",
            "            economic       0.83      0.71      0.77      1379\n",
            "          geophysics       0.97      0.84      0.90      1172\n",
            "  gravitional_theory       0.76      0.90      0.83       100\n",
            "               hydro       0.63      0.89      0.74       325\n",
            "                math       0.87      0.81      0.84      1309\n",
            "              metals       0.65      0.89      0.75       171\n",
            "          networking       0.74      0.95      0.83       315\n",
            "        neuroscience       0.94      0.95      0.94       277\n",
            "        oceanography       0.88      0.85      0.87       960\n",
            "             politic       0.68      0.77      0.72       573\n",
            "           sociology       0.64      0.69      0.66       709\n",
            "software_engineering       0.87      0.85      0.86       494\n",
            "          statistics       0.69      0.82      0.75       617\n",
            "    theory_computing       0.82      0.69      0.75       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7716, Train: 0.7792, Test: 0.6493\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5053, Train: 0.8250, Test: 0.7125\n",
            "Early stopping:  0.18828952843587268\n",
            "Epoch: 003, Loss: 2.1707, Train: 0.8292, Test: 0.7260\n",
            "Early stopping:  0.3010941816921762\n",
            "Epoch: 004, Loss: 1.7954, Train: 0.8417, Test: 0.7449\n",
            "Early stopping:  0.4224811645114891\n",
            "Epoch: 005, Loss: 1.4266, Train: 0.8708, Test: 0.7614\n",
            "Early stopping:  0.5387357476367571\n",
            "Epoch: 006, Loss: 1.0961, Train: 0.8792, Test: 0.7772\n",
            "Early stopping:  0.5634354092916956\n",
            "Epoch: 007, Loss: 0.8272, Train: 0.8771, Test: 0.7927\n",
            "Early stopping:  0.5365583436943335\n",
            "Epoch: 008, Loss: 0.6297, Train: 0.8896, Test: 0.8020\n",
            "Early stopping:  0.4665575716884834\n",
            "Epoch: 009, Loss: 0.4964, Train: 0.8938, Test: 0.8040\n",
            "Early stopping:  0.3731298030677725\n",
            "Epoch: 010, Loss: 0.4073, Train: 0.9021, Test: 0.8039\n",
            "Early stopping:  0.27604342289145356\n",
            "Epoch: 011, Loss: 0.3441, Train: 0.9208, Test: 0.8041\n",
            "Early stopping:  0.19260374439746664\n",
            "Epoch: 012, Loss: 0.2954, Train: 0.9313, Test: 0.8029\n",
            "Early stopping:  0.1324826450851468\n",
            "Epoch: 013, Loss: 0.2545, Train: 0.9396, Test: 0.8030\n",
            "Early stopping:  0.09538296292026711\n",
            "Epoch: 014, Loss: 0.2178, Train: 0.9500, Test: 0.8051\n",
            "Early stopping:  0.07455176201957464\n",
            "Epoch: 015, Loss: 0.1842, Train: 0.9542, Test: 0.8067\n",
            "Early stopping:  0.06300852543962415\n",
            "Epoch: 016, Loss: 0.1543, Train: 0.9625, Test: 0.8074\n",
            "Early stopping:  0.055841890810752616\n",
            "Epoch: 017, Loss: 0.1283, Train: 0.9667, Test: 0.8056\n",
            "Early stopping:  0.05006787438250094\n",
            "Epoch: 018, Loss: 0.1058, Train: 0.9812, Test: 0.8040\n",
            "Early stopping:  0.04439947320851071\n",
            "Epoch: 019, Loss: 0.0868, Train: 0.9958, Test: 0.8022\n",
            "Early stopping:  0.03860908312803227\n",
            "Epoch: 020, Loss: 0.0714, Train: 0.9979, Test: 0.7996\n",
            "Early stopping:  0.032933736054332606\n",
            "Epoch: 021, Loss: 0.0593, Train: 0.9979, Test: 0.7983\n",
            "Early stopping:  0.027466140255697186\n",
            "Epoch: 022, Loss: 0.0497, Train: 1.0000, Test: 0.7978\n",
            "Early stopping:  0.02227393230876436\n",
            "Epoch: 023, Loss: 0.0422, Train: 1.0000, Test: 0.7973\n",
            "Early stopping:  0.01769149360832824\n",
            "Epoch: 024, Loss: 0.0362, Train: 1.0000, Test: 0.7966\n",
            "Early stopping:  0.013974309832313583\n",
            "Epoch: 025, Loss: 0.0314, Train: 1.0000, Test: 0.7967\n",
            "Early stopping:  0.01105552154843555\n",
            "Epoch: 026, Loss: 0.0277, Train: 1.0000, Test: 0.7970\n",
            "Early stopping:  0.008750191963960489\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.76      0.83      0.79       372\n",
            "             ecology       0.86      0.78      0.82       859\n",
            "            economic       0.82      0.70      0.76      1379\n",
            "          geophysics       0.94      0.88      0.91      1172\n",
            "  gravitional_theory       0.71      0.97      0.82       100\n",
            "               hydro       0.73      0.85      0.78       325\n",
            "                math       0.94      0.68      0.79      1309\n",
            "              metals       0.51      0.96      0.66       171\n",
            "          networking       0.82      0.88      0.85       315\n",
            "        neuroscience       0.85      0.99      0.91       277\n",
            "        oceanography       0.85      0.87      0.86       960\n",
            "             politic       0.74      0.78      0.76       573\n",
            "           sociology       0.65      0.71      0.68       709\n",
            "software_engineering       0.88      0.85      0.86       494\n",
            "          statistics       0.68      0.84      0.75       617\n",
            "    theory_computing       0.60      0.78      0.68       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.77      0.83      0.79     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7725, Train: 0.7646, Test: 0.6296\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.4974, Train: 0.8438, Test: 0.7352\n",
            "Early stopping:  0.19454597521622385\n",
            "Epoch: 003, Loss: 2.1636, Train: 0.8812, Test: 0.7742\n",
            "Early stopping:  0.3049023612839433\n",
            "Epoch: 004, Loss: 1.7822, Train: 0.8854, Test: 0.7899\n",
            "Early stopping:  0.4277191807639588\n",
            "Epoch: 005, Loss: 1.4010, Train: 0.8917, Test: 0.7966\n",
            "Early stopping:  0.5479792139670374\n",
            "Epoch: 006, Loss: 1.0607, Train: 0.8917, Test: 0.8002\n",
            "Early stopping:  0.575092163753921\n",
            "Epoch: 007, Loss: 0.7902, Train: 0.8875, Test: 0.8054\n",
            "Early stopping:  0.549657001610384\n",
            "Epoch: 008, Loss: 0.5955, Train: 0.8979, Test: 0.8094\n",
            "Early stopping:  0.4755935691150436\n",
            "Epoch: 009, Loss: 0.4626, Train: 0.9146, Test: 0.8128\n",
            "Early stopping:  0.3760648372298187\n",
            "Epoch: 010, Loss: 0.3717, Train: 0.9271, Test: 0.8139\n",
            "Early stopping:  0.2755210034936187\n",
            "Epoch: 011, Loss: 0.3073, Train: 0.9271, Test: 0.8152\n",
            "Early stopping:  0.19246308408537996\n",
            "Epoch: 012, Loss: 0.2590, Train: 0.9354, Test: 0.8152\n",
            "Early stopping:  0.13361567048963252\n",
            "Epoch: 013, Loss: 0.2206, Train: 0.9437, Test: 0.8160\n",
            "Early stopping:  0.09574907108651046\n",
            "Epoch: 014, Loss: 0.1883, Train: 0.9479, Test: 0.8169\n",
            "Early stopping:  0.07239442746735106\n",
            "Epoch: 015, Loss: 0.1600, Train: 0.9625, Test: 0.8191\n",
            "Early stopping:  0.0580725668677498\n",
            "Epoch: 016, Loss: 0.1349, Train: 0.9750, Test: 0.8200\n",
            "Early stopping:  0.048983524830091135\n",
            "Epoch: 017, Loss: 0.1131, Train: 0.9812, Test: 0.8212\n",
            "Early stopping:  0.0425769918798971\n",
            "Epoch: 018, Loss: 0.0942, Train: 0.9875, Test: 0.8218\n",
            "Early stopping:  0.037302088945054566\n",
            "Epoch: 019, Loss: 0.0782, Train: 0.9917, Test: 0.8213\n",
            "Early stopping:  0.03241927859454684\n",
            "Epoch: 020, Loss: 0.0651, Train: 0.9958, Test: 0.8203\n",
            "Early stopping:  0.027725470315925104\n",
            "Epoch: 021, Loss: 0.0545, Train: 0.9979, Test: 0.8199\n",
            "Early stopping:  0.023267449325553267\n",
            "Epoch: 022, Loss: 0.0461, Train: 0.9979, Test: 0.8199\n",
            "Early stopping:  0.019114421506689374\n",
            "Epoch: 023, Loss: 0.0395, Train: 0.9979, Test: 0.8188\n",
            "Early stopping:  0.015388547452590525\n",
            "Epoch: 024, Loss: 0.0342, Train: 0.9979, Test: 0.8187\n",
            "Early stopping:  0.01225008947831009\n",
            "Epoch: 025, Loss: 0.0300, Train: 0.9979, Test: 0.8183\n",
            "Early stopping:  0.009705814028475842\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.83      0.80      0.81       372\n",
            "             ecology       0.84      0.86      0.85       859\n",
            "            economic       0.84      0.71      0.77      1379\n",
            "          geophysics       0.96      0.87      0.91      1172\n",
            "  gravitional_theory       0.82      0.94      0.87       100\n",
            "               hydro       0.66      0.85      0.75       325\n",
            "                math       0.86      0.86      0.86      1309\n",
            "              metals       0.60      0.93      0.73       171\n",
            "          networking       0.79      0.92      0.85       315\n",
            "        neuroscience       0.94      0.97      0.95       277\n",
            "        oceanography       0.90      0.84      0.87       960\n",
            "             politic       0.68      0.83      0.74       573\n",
            "           sociology       0.68      0.62      0.65       709\n",
            "software_engineering       0.87      0.85      0.86       494\n",
            "          statistics       0.76      0.81      0.78       617\n",
            "    theory_computing       0.76      0.79      0.77       412\n",
            "\n",
            "            accuracy                           0.82     10044\n",
            "           macro avg       0.80      0.84      0.82     10044\n",
            "        weighted avg       0.83      0.82      0.82     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7742, Train: 0.7250, Test: 0.5623\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5158, Train: 0.7771, Test: 0.6574\n",
            "Early stopping:  0.18271130874175884\n",
            "Epoch: 003, Loss: 2.1932, Train: 0.7937, Test: 0.6729\n",
            "Early stopping:  0.2910697880652868\n",
            "Epoch: 004, Loss: 1.8286, Train: 0.8187, Test: 0.7005\n",
            "Early stopping:  0.40901454799596215\n",
            "Epoch: 005, Loss: 1.4691, Train: 0.8458, Test: 0.7400\n",
            "Early stopping:  0.5224982149001652\n",
            "Epoch: 006, Loss: 1.1462, Train: 0.8583, Test: 0.7777\n",
            "Early stopping:  0.5477481003804163\n",
            "Epoch: 007, Loss: 0.8771, Train: 0.8792, Test: 0.8049\n",
            "Early stopping:  0.5250303084580739\n",
            "Epoch: 008, Loss: 0.6722, Train: 0.8833, Test: 0.8155\n",
            "Early stopping:  0.461857837760578\n",
            "Epoch: 009, Loss: 0.5321, Train: 0.8896, Test: 0.8171\n",
            "Early stopping:  0.3756621216490517\n",
            "Epoch: 010, Loss: 0.4410, Train: 0.8979, Test: 0.8182\n",
            "Early stopping:  0.2832177060020459\n",
            "Epoch: 011, Loss: 0.3752, Train: 0.9146, Test: 0.8216\n",
            "Early stopping:  0.20022510017351172\n",
            "Epoch: 012, Loss: 0.3219, Train: 0.9187, Test: 0.8223\n",
            "Early stopping:  0.1383069765062858\n",
            "Epoch: 013, Loss: 0.2782, Train: 0.9271, Test: 0.8224\n",
            "Early stopping:  0.10016531337170599\n",
            "Epoch: 014, Loss: 0.2420, Train: 0.9396, Test: 0.8234\n",
            "Early stopping:  0.07877486346084998\n",
            "Epoch: 015, Loss: 0.2104, Train: 0.9458, Test: 0.8222\n",
            "Early stopping:  0.06510436281458847\n",
            "Epoch: 016, Loss: 0.1825, Train: 0.9604, Test: 0.8222\n",
            "Early stopping:  0.05501014421087992\n",
            "Epoch: 017, Loss: 0.1581, Train: 0.9667, Test: 0.8214\n",
            "Early stopping:  0.04756272986797408\n",
            "Epoch: 018, Loss: 0.1360, Train: 0.9708, Test: 0.8210\n",
            "Early stopping:  0.04191309833680072\n",
            "Epoch: 019, Loss: 0.1163, Train: 0.9812, Test: 0.8201\n",
            "Early stopping:  0.037202214483661525\n",
            "Epoch: 020, Loss: 0.0991, Train: 0.9812, Test: 0.8187\n",
            "Early stopping:  0.033067074869614334\n",
            "Epoch: 021, Loss: 0.0843, Train: 0.9875, Test: 0.8165\n",
            "Early stopping:  0.029240804057738413\n",
            "Epoch: 022, Loss: 0.0719, Train: 0.9917, Test: 0.8140\n",
            "Early stopping:  0.02543626876639163\n",
            "Epoch: 023, Loss: 0.0617, Train: 0.9938, Test: 0.8134\n",
            "Early stopping:  0.021653584079007242\n",
            "Epoch: 024, Loss: 0.0538, Train: 0.9979, Test: 0.8096\n",
            "Early stopping:  0.01802718747220894\n",
            "Epoch: 025, Loss: 0.0473, Train: 1.0000, Test: 0.8085\n",
            "Early stopping:  0.014683166145632958\n",
            "Epoch: 026, Loss: 0.0420, Train: 1.0000, Test: 0.8070\n",
            "Early stopping:  0.011801409259770607\n",
            "Epoch: 027, Loss: 0.0377, Train: 1.0000, Test: 0.8058\n",
            "Early stopping:  0.00952799442221482\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.84      0.81       372\n",
            "             ecology       0.81      0.88      0.85       859\n",
            "            economic       0.83      0.62      0.71      1379\n",
            "          geophysics       0.93      0.91      0.92      1172\n",
            "  gravitional_theory       0.88      0.91      0.90       100\n",
            "               hydro       0.78      0.83      0.81       325\n",
            "                math       0.91      0.79      0.85      1309\n",
            "              metals       0.64      0.92      0.76       171\n",
            "          networking       0.84      0.90      0.87       315\n",
            "        neuroscience       0.92      0.98      0.95       277\n",
            "        oceanography       0.92      0.81      0.86       960\n",
            "             politic       0.67      0.76      0.71       573\n",
            "           sociology       0.56      0.76      0.65       709\n",
            "software_engineering       0.87      0.81      0.84       494\n",
            "          statistics       0.72      0.85      0.78       617\n",
            "    theory_computing       0.74      0.75      0.75       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.80      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7939, Train: 0.6646, Test: 0.6133\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5179, Train: 0.7958, Test: 0.7571\n",
            "Early stopping:  0.1951319849979193\n",
            "Epoch: 003, Loss: 2.1954, Train: 0.8167, Test: 0.7670\n",
            "Early stopping:  0.2995757034950223\n",
            "Epoch: 004, Loss: 1.8282, Train: 0.8396, Test: 0.7739\n",
            "Early stopping:  0.41647771933043165\n",
            "Epoch: 005, Loss: 1.4664, Train: 0.8604, Test: 0.7870\n",
            "Early stopping:  0.5297136260738537\n",
            "Epoch: 006, Loss: 1.1437, Train: 0.8646, Test: 0.8005\n",
            "Early stopping:  0.5499822523548747\n",
            "Epoch: 007, Loss: 0.8752, Train: 0.8750, Test: 0.8067\n",
            "Early stopping:  0.526706358285889\n",
            "Epoch: 008, Loss: 0.6683, Train: 0.8729, Test: 0.8098\n",
            "Early stopping:  0.46286584308669615\n",
            "Epoch: 009, Loss: 0.5209, Train: 0.8979, Test: 0.8147\n",
            "Early stopping:  0.37819124975905355\n",
            "Epoch: 010, Loss: 0.4192, Train: 0.9021, Test: 0.8146\n",
            "Early stopping:  0.28995111299776577\n",
            "Epoch: 011, Loss: 0.3479, Train: 0.9104, Test: 0.8159\n",
            "Early stopping:  0.2104701497122769\n",
            "Epoch: 012, Loss: 0.2973, Train: 0.9146, Test: 0.8165\n",
            "Early stopping:  0.14777557705003583\n",
            "Epoch: 013, Loss: 0.2586, Train: 0.9313, Test: 0.8178\n",
            "Early stopping:  0.1041285594090388\n",
            "Epoch: 014, Loss: 0.2242, Train: 0.9500, Test: 0.8183\n",
            "Early stopping:  0.07668886952881003\n",
            "Epoch: 015, Loss: 0.1924, Train: 0.9646, Test: 0.8159\n",
            "Early stopping:  0.06102718911975621\n",
            "Epoch: 016, Loss: 0.1651, Train: 0.9688, Test: 0.8139\n",
            "Early stopping:  0.05238681939195882\n",
            "Epoch: 017, Loss: 0.1420, Train: 0.9688, Test: 0.8134\n",
            "Early stopping:  0.04637702160108556\n",
            "Epoch: 018, Loss: 0.1207, Train: 0.9750, Test: 0.8114\n",
            "Early stopping:  0.040838569939519655\n",
            "Epoch: 019, Loss: 0.1015, Train: 0.9854, Test: 0.8109\n",
            "Early stopping:  0.03583143269801681\n",
            "Epoch: 020, Loss: 0.0858, Train: 0.9896, Test: 0.8116\n",
            "Early stopping:  0.031562414768278806\n",
            "Epoch: 021, Loss: 0.0730, Train: 0.9917, Test: 0.8106\n",
            "Early stopping:  0.02746085155548471\n",
            "Epoch: 022, Loss: 0.0620, Train: 0.9979, Test: 0.8105\n",
            "Early stopping:  0.023213937518704042\n",
            "Epoch: 023, Loss: 0.0527, Train: 1.0000, Test: 0.8098\n",
            "Early stopping:  0.019284110210200333\n",
            "Epoch: 024, Loss: 0.0455, Train: 1.0000, Test: 0.8084\n",
            "Early stopping:  0.01602498138915593\n",
            "Epoch: 025, Loss: 0.0401, Train: 1.0000, Test: 0.8073\n",
            "Early stopping:  0.013140181540099728\n",
            "Epoch: 026, Loss: 0.0358, Train: 1.0000, Test: 0.8081\n",
            "Early stopping:  0.010417468886866293\n",
            "Epoch: 027, Loss: 0.0321, Train: 1.0000, Test: 0.8073\n",
            "Early stopping:  0.008132264379813918\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.84      0.69      0.75       372\n",
            "             ecology       0.81      0.85      0.83       859\n",
            "            economic       0.82      0.75      0.78      1379\n",
            "          geophysics       0.94      0.87      0.90      1172\n",
            "  gravitional_theory       0.80      0.94      0.87       100\n",
            "               hydro       0.75      0.81      0.78       325\n",
            "                math       0.94      0.77      0.85      1309\n",
            "              metals       0.54      0.95      0.69       171\n",
            "          networking       0.77      0.91      0.84       315\n",
            "        neuroscience       0.95      0.94      0.94       277\n",
            "        oceanography       0.86      0.82      0.84       960\n",
            "             politic       0.67      0.75      0.71       573\n",
            "           sociology       0.67      0.71      0.69       709\n",
            "software_engineering       0.84      0.83      0.83       494\n",
            "          statistics       0.79      0.86      0.82       617\n",
            "    theory_computing       0.64      0.81      0.71       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7801, Train: 0.7146, Test: 0.5495\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5112, Train: 0.8313, Test: 0.7273\n",
            "Early stopping:  0.19016152285933258\n",
            "Epoch: 003, Loss: 2.1785, Train: 0.8458, Test: 0.7574\n",
            "Early stopping:  0.3013799890346608\n",
            "Epoch: 004, Loss: 1.7997, Train: 0.8604, Test: 0.7673\n",
            "Early stopping:  0.42385018507818845\n",
            "Epoch: 005, Loss: 1.4259, Train: 0.8562, Test: 0.7763\n",
            "Early stopping:  0.5419083875856591\n",
            "Epoch: 006, Loss: 1.0963, Train: 0.8688, Test: 0.7850\n",
            "Early stopping:  0.5665998747967492\n",
            "Epoch: 007, Loss: 0.8333, Train: 0.8750, Test: 0.7949\n",
            "Early stopping:  0.5379578449070066\n",
            "Epoch: 008, Loss: 0.6400, Train: 0.8771, Test: 0.8049\n",
            "Early stopping:  0.4639912785946687\n",
            "Epoch: 009, Loss: 0.5046, Train: 0.8854, Test: 0.8128\n",
            "Early stopping:  0.36862680412563775\n",
            "Epoch: 010, Loss: 0.4115, Train: 0.9021, Test: 0.8159\n",
            "Early stopping:  0.27375575791313245\n",
            "Epoch: 011, Loss: 0.3462, Train: 0.9021, Test: 0.8163\n",
            "Early stopping:  0.1943507281007529\n",
            "Epoch: 012, Loss: 0.2962, Train: 0.9208, Test: 0.8164\n",
            "Early stopping:  0.13643936662883566\n",
            "Epoch: 013, Loss: 0.2543, Train: 0.9292, Test: 0.8171\n",
            "Early stopping:  0.09869814423648703\n",
            "Epoch: 014, Loss: 0.2177, Train: 0.9458, Test: 0.8165\n",
            "Early stopping:  0.07636007564344882\n",
            "Epoch: 015, Loss: 0.1859, Train: 0.9542, Test: 0.8156\n",
            "Early stopping:  0.06336513306234244\n",
            "Epoch: 016, Loss: 0.1588, Train: 0.9646, Test: 0.8148\n",
            "Early stopping:  0.05445946725170192\n",
            "Epoch: 017, Loss: 0.1349, Train: 0.9708, Test: 0.8137\n",
            "Early stopping:  0.0472421023188555\n",
            "Epoch: 018, Loss: 0.1132, Train: 0.9792, Test: 0.8130\n",
            "Early stopping:  0.041213689568798444\n",
            "Epoch: 019, Loss: 0.0945, Train: 0.9875, Test: 0.8127\n",
            "Early stopping:  0.03618809464660566\n",
            "Epoch: 020, Loss: 0.0789, Train: 0.9938, Test: 0.8111\n",
            "Early stopping:  0.031765863907456574\n",
            "Epoch: 021, Loss: 0.0655, Train: 0.9979, Test: 0.8114\n",
            "Early stopping:  0.027490224006598476\n",
            "Epoch: 022, Loss: 0.0544, Train: 0.9979, Test: 0.8107\n",
            "Early stopping:  0.02329192777548785\n",
            "Epoch: 023, Loss: 0.0458, Train: 1.0000, Test: 0.8105\n",
            "Early stopping:  0.019377105536398643\n",
            "Epoch: 024, Loss: 0.0394, Train: 1.0000, Test: 0.8093\n",
            "Early stopping:  0.01572651147387857\n",
            "Epoch: 025, Loss: 0.0346, Train: 1.0000, Test: 0.8079\n",
            "Early stopping:  0.01230532072377033\n",
            "Epoch: 026, Loss: 0.0309, Train: 1.0000, Test: 0.8077\n",
            "Early stopping:  0.009348410278312496\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.70      0.85      0.77       372\n",
            "             ecology       0.81      0.88      0.84       859\n",
            "            economic       0.82      0.72      0.76      1379\n",
            "          geophysics       0.95      0.90      0.92      1172\n",
            "  gravitional_theory       0.79      0.92      0.85       100\n",
            "               hydro       0.77      0.86      0.81       325\n",
            "                math       0.88      0.79      0.83      1309\n",
            "              metals       0.63      0.90      0.74       171\n",
            "          networking       0.77      0.90      0.83       315\n",
            "        neuroscience       0.94      0.95      0.94       277\n",
            "        oceanography       0.88      0.86      0.87       960\n",
            "             politic       0.71      0.74      0.73       573\n",
            "           sociology       0.71      0.64      0.67       709\n",
            "software_engineering       0.83      0.80      0.81       494\n",
            "          statistics       0.70      0.81      0.75       617\n",
            "    theory_computing       0.71      0.71      0.71       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.81      0.81      0.81     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7826, Train: 0.5813, Test: 0.4276\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5018, Train: 0.8104, Test: 0.6949\n",
            "Early stopping:  0.19850794756364193\n",
            "Epoch: 003, Loss: 2.1695, Train: 0.8250, Test: 0.7407\n",
            "Early stopping:  0.3068692331106436\n",
            "Epoch: 004, Loss: 1.7911, Train: 0.8396, Test: 0.7572\n",
            "Early stopping:  0.427808664189249\n",
            "Epoch: 005, Loss: 1.4203, Train: 0.8562, Test: 0.7734\n",
            "Early stopping:  0.5440976583016571\n",
            "Epoch: 006, Loss: 1.0924, Train: 0.8646, Test: 0.7886\n",
            "Early stopping:  0.5643424970188147\n",
            "Epoch: 007, Loss: 0.8239, Train: 0.8688, Test: 0.8012\n",
            "Early stopping:  0.5372277798432681\n",
            "Epoch: 008, Loss: 0.6215, Train: 0.8750, Test: 0.8072\n",
            "Early stopping:  0.46720356196227264\n",
            "Epoch: 009, Loss: 0.4846, Train: 0.8938, Test: 0.8100\n",
            "Early stopping:  0.37515917547338423\n",
            "Epoch: 010, Loss: 0.3972, Train: 0.8958, Test: 0.8118\n",
            "Early stopping:  0.2794237199250287\n",
            "Epoch: 011, Loss: 0.3352, Train: 0.9083, Test: 0.8134\n",
            "Early stopping:  0.1951418226395064\n",
            "Epoch: 012, Loss: 0.2857, Train: 0.9250, Test: 0.8149\n",
            "Early stopping:  0.1326479774978854\n",
            "Epoch: 013, Loss: 0.2464, Train: 0.9354, Test: 0.8143\n",
            "Early stopping:  0.09411646163113363\n",
            "Epoch: 014, Loss: 0.2151, Train: 0.9458, Test: 0.8148\n",
            "Early stopping:  0.07227804870496393\n",
            "Epoch: 015, Loss: 0.1875, Train: 0.9542, Test: 0.8131\n",
            "Early stopping:  0.05830144592856837\n",
            "Epoch: 016, Loss: 0.1616, Train: 0.9667, Test: 0.8127\n",
            "Early stopping:  0.048732239990847685\n",
            "Epoch: 017, Loss: 0.1384, Train: 0.9750, Test: 0.8124\n",
            "Early stopping:  0.042673987313925\n",
            "Epoch: 018, Loss: 0.1188, Train: 0.9771, Test: 0.8120\n",
            "Early stopping:  0.03829085369882608\n",
            "Epoch: 019, Loss: 0.1018, Train: 0.9875, Test: 0.8103\n",
            "Early stopping:  0.03399497322465413\n",
            "Epoch: 020, Loss: 0.0863, Train: 0.9875, Test: 0.8094\n",
            "Early stopping:  0.029690996778764925\n",
            "Epoch: 021, Loss: 0.0728, Train: 0.9938, Test: 0.8075\n",
            "Early stopping:  0.025946665876611236\n",
            "Epoch: 022, Loss: 0.0621, Train: 0.9958, Test: 0.8068\n",
            "Early stopping:  0.022610912336758684\n",
            "Epoch: 023, Loss: 0.0540, Train: 0.9979, Test: 0.8064\n",
            "Early stopping:  0.01910535331735663\n",
            "Epoch: 024, Loss: 0.0473, Train: 0.9979, Test: 0.8059\n",
            "Early stopping:  0.015466255437424022\n",
            "Epoch: 025, Loss: 0.0416, Train: 1.0000, Test: 0.8052\n",
            "Early stopping:  0.012290294249401349\n",
            "Epoch: 026, Loss: 0.0371, Train: 1.0000, Test: 0.8037\n",
            "Early stopping:  0.009925903217475216\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.78      0.78      0.78       372\n",
            "             ecology       0.84      0.82      0.83       859\n",
            "            economic       0.84      0.65      0.74      1379\n",
            "          geophysics       0.94      0.89      0.92      1172\n",
            "  gravitional_theory       0.66      0.97      0.79       100\n",
            "               hydro       0.76      0.82      0.79       325\n",
            "                math       0.88      0.82      0.85      1309\n",
            "              metals       0.69      0.89      0.78       171\n",
            "          networking       0.77      0.93      0.85       315\n",
            "        neuroscience       0.94      0.95      0.94       277\n",
            "        oceanography       0.83      0.86      0.84       960\n",
            "             politic       0.61      0.83      0.70       573\n",
            "           sociology       0.67      0.65      0.66       709\n",
            "software_engineering       0.94      0.79      0.85       494\n",
            "          statistics       0.69      0.82      0.75       617\n",
            "    theory_computing       0.72      0.82      0.77       412\n",
            "\n",
            "            accuracy                           0.80     10044\n",
            "           macro avg       0.79      0.83      0.80     10044\n",
            "        weighted avg       0.81      0.80      0.80     10044\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.7765, Train: 0.6312, Test: 0.5344\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.5232, Train: 0.7958, Test: 0.6974\n",
            "Early stopping:  0.1791242747586937\n",
            "Epoch: 003, Loss: 2.2077, Train: 0.8042, Test: 0.7212\n",
            "Early stopping:  0.28495885015657446\n",
            "Epoch: 004, Loss: 1.8432, Train: 0.8292, Test: 0.7383\n",
            "Early stopping:  0.40347237678236675\n",
            "Epoch: 005, Loss: 1.4774, Train: 0.8417, Test: 0.7580\n",
            "Early stopping:  0.5197027251074118\n",
            "Epoch: 006, Loss: 1.1471, Train: 0.8667, Test: 0.7799\n",
            "Early stopping:  0.5507890661872907\n",
            "Epoch: 007, Loss: 0.8743, Train: 0.8833, Test: 0.8013\n",
            "Early stopping:  0.5326172500122314\n",
            "Epoch: 008, Loss: 0.6679, Train: 0.8833, Test: 0.8098\n",
            "Early stopping:  0.46976641214075426\n",
            "Epoch: 009, Loss: 0.5265, Train: 0.8958, Test: 0.8134\n",
            "Early stopping:  0.38113855870053004\n",
            "Epoch: 010, Loss: 0.4342, Train: 0.9021, Test: 0.8163\n",
            "Early stopping:  0.2861811291619582\n",
            "Epoch: 011, Loss: 0.3690, Train: 0.9083, Test: 0.8207\n",
            "Early stopping:  0.20173921303132977\n",
            "Epoch: 012, Loss: 0.3184, Train: 0.9125, Test: 0.8217\n",
            "Early stopping:  0.13835766578604503\n",
            "Epoch: 013, Loss: 0.2790, Train: 0.9208, Test: 0.8223\n",
            "Early stopping:  0.09792532946576014\n",
            "Epoch: 014, Loss: 0.2466, Train: 0.9313, Test: 0.8219\n",
            "Early stopping:  0.07424509003385771\n",
            "Epoch: 015, Loss: 0.2165, Train: 0.9354, Test: 0.8215\n",
            "Early stopping:  0.0599265712710316\n",
            "Epoch: 016, Loss: 0.1882, Train: 0.9500, Test: 0.8203\n",
            "Early stopping:  0.051175797907226137\n",
            "Epoch: 017, Loss: 0.1632, Train: 0.9646, Test: 0.8196\n",
            "Early stopping:  0.04592421829313718\n",
            "Epoch: 018, Loss: 0.1409, Train: 0.9729, Test: 0.8205\n",
            "Early stopping:  0.04195714452376033\n",
            "Epoch: 019, Loss: 0.1200, Train: 0.9792, Test: 0.8187\n",
            "Early stopping:  0.03807969592082003\n",
            "Epoch: 020, Loss: 0.1018, Train: 0.9792, Test: 0.8176\n",
            "Early stopping:  0.03421246731227654\n",
            "Epoch: 021, Loss: 0.0869, Train: 0.9854, Test: 0.8172\n",
            "Early stopping:  0.030391982849339718\n",
            "Epoch: 022, Loss: 0.0746, Train: 0.9917, Test: 0.8152\n",
            "Early stopping:  0.026342545902126167\n",
            "Epoch: 023, Loss: 0.0645, Train: 0.9958, Test: 0.8135\n",
            "Early stopping:  0.022002961179015634\n",
            "Epoch: 024, Loss: 0.0569, Train: 0.9979, Test: 0.8122\n",
            "Early stopping:  0.017886582567575075\n",
            "Epoch: 025, Loss: 0.0507, Train: 1.0000, Test: 0.8119\n",
            "Early stopping:  0.01438374659041119\n",
            "Epoch: 026, Loss: 0.0455, Train: 1.0000, Test: 0.8110\n",
            "Early stopping:  0.011457450066327394\n",
            "Epoch: 027, Loss: 0.0416, Train: 1.0000, Test: 0.8097\n",
            "Early stopping:  0.009098028737694122\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 15, 15, 15], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "     data_management       0.79      0.82      0.80       372\n",
            "             ecology       0.84      0.85      0.84       859\n",
            "            economic       0.78      0.70      0.74      1379\n",
            "          geophysics       0.90      0.91      0.91      1172\n",
            "  gravitional_theory       0.81      0.88      0.85       100\n",
            "               hydro       0.74      0.82      0.78       325\n",
            "                math       0.93      0.75      0.83      1309\n",
            "              metals       0.63      0.92      0.75       171\n",
            "          networking       0.79      0.93      0.85       315\n",
            "        neuroscience       0.96      0.97      0.96       277\n",
            "        oceanography       0.89      0.84      0.86       960\n",
            "             politic       0.70      0.76      0.73       573\n",
            "           sociology       0.69      0.74      0.71       709\n",
            "software_engineering       0.88      0.84      0.86       494\n",
            "          statistics       0.74      0.79      0.76       617\n",
            "    theory_computing       0.66      0.83      0.74       412\n",
            "\n",
            "            accuracy                           0.81     10044\n",
            "           macro avg       0.80      0.83      0.81     10044\n",
            "        weighted avg       0.82      0.81      0.81     10044\n",
            "\n",
            "time: 12.3 s (started: 2024-08-16 14:16:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H75kv_iKlm7",
        "outputId": "0499b4cf-e340-4c31-b95d-a3fc7f05a3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 471 ms (started: 2024-08-16 14:16:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training rotulated base = 80% ❎"
      ],
      "metadata": {
        "id": "t0Ou_DUXtx37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GCN Training"
      ],
      "metadata": {
        "id": "BLTNdzlxtx3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN','80%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e70edd2-7c57-44da-e9b3-2b5c32e85b1e",
        "id": "ewPcBAzBtx3_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4239, Train: 0.1653, Test: 0.1721\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 12.5963, Train: 0.1701, Test: 0.1716\n",
            "Early stopping:  5.0716402002967955\n",
            "Epoch: 003, Loss: 12.6517, Train: 0.2808, Test: 0.2779\n",
            "Early stopping:  4.157081515254105\n",
            "Epoch: 004, Loss: 9.7003, Train: 0.3666, Test: 0.3700\n",
            "Early stopping:  3.4043249504935438\n",
            "Epoch: 005, Loss: 6.1940, Train: 0.3171, Test: 0.3063\n",
            "Early stopping:  3.4252941513867863\n",
            "Epoch: 006, Loss: 6.8460, Train: 0.3470, Test: 0.3395\n",
            "Early stopping:  3.061305589056082\n",
            "Epoch: 007, Loss: 5.9098, Train: 0.3689, Test: 0.3489\n",
            "Early stopping:  2.8789463899110386\n",
            "Epoch: 008, Loss: 4.6041, Train: 0.4345, Test: 0.4068\n",
            "Early stopping:  1.8897503744647153\n",
            "Epoch: 009, Loss: 3.3294, Train: 0.4780, Test: 0.4658\n",
            "Early stopping:  1.4052968555629086\n",
            "Epoch: 010, Loss: 2.5441, Train: 0.4861, Test: 0.4742\n",
            "Early stopping:  1.7741348707177251\n",
            "Epoch: 011, Loss: 2.1945, Train: 0.4786, Test: 0.4742\n",
            "Early stopping:  1.5361664649930546\n",
            "Epoch: 012, Loss: 2.1153, Train: 0.4884, Test: 0.4884\n",
            "Early stopping:  1.0382682358694975\n",
            "Epoch: 013, Loss: 2.0185, Train: 0.5059, Test: 0.5058\n",
            "Early stopping:  0.5350744034233902\n",
            "Epoch: 014, Loss: 1.9285, Train: 0.5261, Test: 0.5263\n",
            "Early stopping:  0.236830303491132\n",
            "Epoch: 015, Loss: 1.8811, Train: 0.5350, Test: 0.5305\n",
            "Early stopping:  0.12932903921228825\n",
            "Epoch: 016, Loss: 1.8514, Train: 0.5363, Test: 0.5284\n",
            "Early stopping:  0.10785576630700999\n",
            "Epoch: 017, Loss: 1.8195, Train: 0.5354, Test: 0.5168\n",
            "Early stopping:  0.07755088704364024\n",
            "Epoch: 018, Loss: 1.7841, Train: 0.5366, Test: 0.5174\n",
            "Early stopping:  0.0555964886155765\n",
            "Epoch: 019, Loss: 1.7418, Train: 0.5397, Test: 0.5263\n",
            "Early stopping:  0.054821297976712625\n",
            "Epoch: 020, Loss: 1.6967, Train: 0.5466, Test: 0.5374\n",
            "Early stopping:  0.06135692955299135\n",
            "Epoch: 021, Loss: 1.6570, Train: 0.5588, Test: 0.5426\n",
            "Early stopping:  0.0652464804202298\n",
            "Epoch: 022, Loss: 1.6251, Train: 0.5614, Test: 0.5505\n",
            "Early stopping:  0.0638124747128677\n",
            "Epoch: 023, Loss: 1.6043, Train: 0.5637, Test: 0.5558\n",
            "Early stopping:  0.05532674466113909\n",
            "Epoch: 024, Loss: 1.5740, Train: 0.5722, Test: 0.5658\n",
            "Early stopping:  0.04740354960759753\n",
            "Epoch: 025, Loss: 1.5304, Train: 0.5862, Test: 0.5732\n",
            "Early stopping:  0.04847202464756509\n",
            "Epoch: 026, Loss: 1.4832, Train: 0.6005, Test: 0.5884\n",
            "Early stopping:  0.05724978422253327\n",
            "Epoch: 027, Loss: 1.4394, Train: 0.6064, Test: 0.5968\n",
            "Early stopping:  0.0666775130726855\n",
            "Epoch: 028, Loss: 1.4055, Train: 0.6084, Test: 0.6032\n",
            "Early stopping:  0.06776952358042676\n",
            "Epoch: 029, Loss: 1.3797, Train: 0.6100, Test: 0.6095\n",
            "Early stopping:  0.06036474469905804\n",
            "Epoch: 030, Loss: 1.3561, Train: 0.6184, Test: 0.6158\n",
            "Early stopping:  0.05008137574560487\n",
            "Epoch: 031, Loss: 1.3315, Train: 0.6284, Test: 0.6200\n",
            "Early stopping:  0.04206559863247094\n",
            "Epoch: 032, Loss: 1.3079, Train: 0.6358, Test: 0.6237\n",
            "Early stopping:  0.03849524940905926\n",
            "Epoch: 033, Loss: 1.2856, Train: 0.6436, Test: 0.6284\n",
            "Early stopping:  0.03736403055476659\n",
            "Epoch: 034, Loss: 1.2641, Train: 0.6467, Test: 0.6342\n",
            "Early stopping:  0.036363395513895634\n",
            "Epoch: 035, Loss: 1.2449, Train: 0.6522, Test: 0.6358\n",
            "Early stopping:  0.03432456443909158\n",
            "Epoch: 036, Loss: 1.2284, Train: 0.6571, Test: 0.6342\n",
            "Early stopping:  0.03164643925943185\n",
            "Epoch: 037, Loss: 1.2139, Train: 0.6596, Test: 0.6305\n",
            "Early stopping:  0.02842707638177374\n",
            "Epoch: 038, Loss: 1.1999, Train: 0.6641, Test: 0.6358\n",
            "Early stopping:  0.02527757007331365\n",
            "Epoch: 039, Loss: 1.1845, Train: 0.6687, Test: 0.6342\n",
            "Early stopping:  0.023613604012597846\n",
            "Epoch: 040, Loss: 1.1680, Train: 0.6729, Test: 0.6379\n",
            "Early stopping:  0.023758674791758357\n",
            "Epoch: 041, Loss: 1.1521, Train: 0.6754, Test: 0.6447\n",
            "Early stopping:  0.024603882984558716\n",
            "Epoch: 042, Loss: 1.1386, Train: 0.6787, Test: 0.6474\n",
            "Early stopping:  0.024509142899299435\n",
            "Epoch: 043, Loss: 1.1267, Train: 0.6814, Test: 0.6484\n",
            "Early stopping:  0.022974579760014876\n",
            "Epoch: 044, Loss: 1.1146, Train: 0.6839, Test: 0.6505\n",
            "Early stopping:  0.02090978643820986\n",
            "Epoch: 045, Loss: 1.1010, Train: 0.6874, Test: 0.6500\n",
            "Early stopping:  0.019941110464412406\n",
            "Epoch: 046, Loss: 1.0867, Train: 0.6922, Test: 0.6516\n",
            "Early stopping:  0.020512335716433395\n",
            "Epoch: 047, Loss: 1.0733, Train: 0.6963, Test: 0.6505\n",
            "Early stopping:  0.02132595677813182\n",
            "Epoch: 048, Loss: 1.0606, Train: 0.6976, Test: 0.6511\n",
            "Early stopping:  0.021477737856859302\n",
            "Epoch: 049, Loss: 1.0478, Train: 0.7000, Test: 0.6521\n",
            "Early stopping:  0.020945661118540838\n",
            "Epoch: 050, Loss: 1.0353, Train: 0.7049, Test: 0.6521\n",
            "Early stopping:  0.020267012409659356\n",
            "Epoch: 051, Loss: 1.0233, Train: 0.7075, Test: 0.6532\n",
            "Early stopping:  0.019798668723245347\n",
            "Epoch: 052, Loss: 1.0120, Train: 0.7096, Test: 0.6542\n",
            "Early stopping:  0.019248298592190856\n",
            "Epoch: 053, Loss: 1.0013, Train: 0.7121, Test: 0.6621\n",
            "Early stopping:  0.01841133510047832\n",
            "Epoch: 054, Loss: 0.9909, Train: 0.7146, Test: 0.6626\n",
            "Early stopping:  0.017532566699218734\n",
            "Epoch: 055, Loss: 0.9806, Train: 0.7178, Test: 0.6626\n",
            "Early stopping:  0.016844812623615354\n",
            "Epoch: 056, Loss: 0.9706, Train: 0.7203, Test: 0.6616\n",
            "Early stopping:  0.016352640094311838\n",
            "Epoch: 057, Loss: 0.9607, Train: 0.7233, Test: 0.6589\n",
            "Early stopping:  0.01603096270012212\n",
            "Epoch: 058, Loss: 0.9509, Train: 0.7242, Test: 0.6574\n",
            "Early stopping:  0.015786022167001812\n",
            "Epoch: 059, Loss: 0.9412, Train: 0.7274, Test: 0.6589\n",
            "Early stopping:  0.015572815168514064\n",
            "Epoch: 060, Loss: 0.9309, Train: 0.7312, Test: 0.6605\n",
            "Early stopping:  0.015652151253346434\n",
            "Epoch: 061, Loss: 0.9204, Train: 0.7351, Test: 0.6632\n",
            "Early stopping:  0.015926041812441668\n",
            "Epoch: 062, Loss: 0.9102, Train: 0.7382, Test: 0.6600\n",
            "Early stopping:  0.016166887271059752\n",
            "Epoch: 063, Loss: 0.9005, Train: 0.7417, Test: 0.6626\n",
            "Early stopping:  0.016156446042344622\n",
            "Epoch: 064, Loss: 0.8909, Train: 0.7439, Test: 0.6616\n",
            "Early stopping:  0.015799009773888168\n",
            "Epoch: 065, Loss: 0.8812, Train: 0.7445, Test: 0.6637\n",
            "Early stopping:  0.015466555144039251\n",
            "Epoch: 066, Loss: 0.8713, Train: 0.7486, Test: 0.6642\n",
            "Early stopping:  0.015361889479492147\n",
            "Epoch: 067, Loss: 0.8620, Train: 0.7511, Test: 0.6621\n",
            "Early stopping:  0.015270509499633149\n",
            "Epoch: 068, Loss: 0.8530, Train: 0.7526, Test: 0.6632\n",
            "Early stopping:  0.015011758069938753\n",
            "Epoch: 069, Loss: 0.8441, Train: 0.7561, Test: 0.6653\n",
            "Early stopping:  0.014612497527103446\n",
            "Epoch: 070, Loss: 0.8352, Train: 0.7591, Test: 0.6658\n",
            "Early stopping:  0.0142489003532951\n",
            "Epoch: 071, Loss: 0.8265, Train: 0.7607, Test: 0.6647\n",
            "Early stopping:  0.014046312518466036\n",
            "Epoch: 072, Loss: 0.8179, Train: 0.7634, Test: 0.6668\n",
            "Early stopping:  0.013904503686034731\n",
            "Epoch: 073, Loss: 0.8096, Train: 0.7634, Test: 0.6668\n",
            "Early stopping:  0.013633304260739054\n",
            "Epoch: 074, Loss: 0.8021, Train: 0.7692, Test: 0.6689\n",
            "Early stopping:  0.013139044506301444\n",
            "Epoch: 075, Loss: 0.7955, Train: 0.7676, Test: 0.6684\n",
            "Early stopping:  0.012316018204185115\n",
            "Epoch: 076, Loss: 0.7877, Train: 0.7746, Test: 0.6663\n",
            "Early stopping:  0.011781906658248175\n",
            "Epoch: 077, Loss: 0.7788, Train: 0.7780, Test: 0.6684\n",
            "Early stopping:  0.012036764427484126\n",
            "Epoch: 078, Loss: 0.7718, Train: 0.7774, Test: 0.6695\n",
            "Early stopping:  0.012233818650007722\n",
            "Epoch: 079, Loss: 0.7660, Train: 0.7812, Test: 0.6705\n",
            "Early stopping:  0.011885652988251167\n",
            "Epoch: 080, Loss: 0.7589, Train: 0.7826, Test: 0.6695\n",
            "Early stopping:  0.011169581185853383\n",
            "Epoch: 081, Loss: 0.7509, Train: 0.7828, Test: 0.6716\n",
            "Early stopping:  0.010894971410284865\n",
            "Epoch: 082, Loss: 0.7447, Train: 0.7849, Test: 0.6737\n",
            "Early stopping:  0.010968593175075462\n",
            "Epoch: 083, Loss: 0.7392, Train: 0.7875, Test: 0.6721\n",
            "Early stopping:  0.010753373509270275\n",
            "Epoch: 084, Loss: 0.7311, Train: 0.7895, Test: 0.6747\n",
            "Early stopping:  0.010662799593181997\n",
            "Epoch: 085, Loss: 0.7255, Train: 0.7887, Test: 0.6758\n",
            "Early stopping:  0.010174042834463666\n",
            "Epoch: 086, Loss: 0.7218, Train: 0.7911, Test: 0.6779\n",
            "Early stopping:  0.009477208247899155\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.80      0.74      0.77        96\n",
            "            CAD_and_CAM       0.70      0.79      0.74        92\n",
            "              Companies       0.47      0.47      0.47        85\n",
            "       Computer_Science       0.67      0.73      0.70        88\n",
            "            Consultants       0.58      0.69      0.63        96\n",
            "           Data_Formats       0.74      0.76      0.75       103\n",
            "    Data_Communications       0.78      0.74      0.76       110\n",
            "              Education       0.81      0.79      0.80        97\n",
            "               Graphics       0.80      0.84      0.82       109\n",
            "               Hardware       0.60      0.67      0.63        93\n",
            "               Internet       0.67      0.53      0.59       110\n",
            "       Mobile_Computing       0.78      0.70      0.74        87\n",
            "             Multimedia       0.67      0.76      0.71        98\n",
            "            Open_Source       0.70      0.60      0.64       114\n",
            "            Programming       0.53      0.53      0.53        98\n",
            "               Robotics       0.92      0.90      0.91       105\n",
            "               Security       0.77      0.86      0.81       107\n",
            "               Software       0.30      0.26      0.28       109\n",
            "                Systems       0.55      0.55      0.55       103\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.68      0.68      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.3125, Train: 0.1188, Test: 0.1121\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 7.7659, Train: 0.1670, Test: 0.1716\n",
            "Early stopping:  1.7347841780208955\n",
            "Epoch: 003, Loss: 11.6801, Train: 0.3338, Test: 0.3453\n",
            "Early stopping:  3.2115713074603778\n",
            "Epoch: 004, Loss: 9.9249, Train: 0.3617, Test: 0.3553\n",
            "Early stopping:  2.752287925847119\n",
            "Epoch: 005, Loss: 8.3579, Train: 0.3314, Test: 0.3311\n",
            "Early stopping:  2.387657144502172\n",
            "Epoch: 006, Loss: 7.6708, Train: 0.3718, Test: 0.3558\n",
            "Early stopping:  1.7108873112488998\n",
            "Epoch: 007, Loss: 6.0698, Train: 0.3980, Test: 0.3932\n",
            "Early stopping:  2.148690713528631\n",
            "Epoch: 008, Loss: 4.5321, Train: 0.4450, Test: 0.4395\n",
            "Early stopping:  2.080904812773993\n",
            "Epoch: 009, Loss: 3.1492, Train: 0.4891, Test: 0.4795\n",
            "Early stopping:  2.158001719367549\n",
            "Epoch: 010, Loss: 2.3812, Train: 0.4980, Test: 0.4932\n",
            "Early stopping:  2.15018554256216\n",
            "Epoch: 011, Loss: 2.1093, Train: 0.4595, Test: 0.4521\n",
            "Early stopping:  1.6483091469911257\n",
            "Epoch: 012, Loss: 2.0410, Train: 0.4575, Test: 0.4584\n",
            "Early stopping:  1.041831025372623\n",
            "Epoch: 013, Loss: 1.9590, Train: 0.4808, Test: 0.4932\n",
            "Early stopping:  0.48568736479251573\n",
            "Epoch: 014, Loss: 1.8446, Train: 0.5084, Test: 0.5179\n",
            "Early stopping:  0.20146573778453664\n",
            "Epoch: 015, Loss: 1.7452, Train: 0.5370, Test: 0.5284\n",
            "Early stopping:  0.1468968419030819\n",
            "Epoch: 016, Loss: 1.6882, Train: 0.5503, Test: 0.5463\n",
            "Early stopping:  0.14612060376217995\n",
            "Epoch: 017, Loss: 1.6779, Train: 0.5503, Test: 0.5437\n",
            "Early stopping:  0.11855348497974041\n",
            "Epoch: 018, Loss: 1.6739, Train: 0.5464, Test: 0.5421\n",
            "Early stopping:  0.07225536018510861\n",
            "Epoch: 019, Loss: 1.6552, Train: 0.5561, Test: 0.5484\n",
            "Early stopping:  0.03406065036290205\n",
            "Epoch: 020, Loss: 1.6215, Train: 0.5663, Test: 0.5579\n",
            "Early stopping:  0.026247387276168765\n",
            "Epoch: 021, Loss: 1.5822, Train: 0.5812, Test: 0.5658\n",
            "Early stopping:  0.04023454061575514\n",
            "Epoch: 022, Loss: 1.5407, Train: 0.5928, Test: 0.5742\n",
            "Early stopping:  0.054141811078869805\n",
            "Epoch: 023, Loss: 1.4985, Train: 0.6054, Test: 0.5889\n",
            "Early stopping:  0.06239227789874312\n",
            "Epoch: 024, Loss: 1.4592, Train: 0.6132, Test: 0.5968\n",
            "Early stopping:  0.06456989383021466\n",
            "Epoch: 025, Loss: 1.4255, Train: 0.6175, Test: 0.6000\n",
            "Early stopping:  0.06250890376596473\n",
            "Epoch: 026, Loss: 1.3973, Train: 0.6232, Test: 0.6021\n",
            "Early stopping:  0.057089902526369786\n",
            "Epoch: 027, Loss: 1.3707, Train: 0.6266, Test: 0.6032\n",
            "Early stopping:  0.05038145735111513\n",
            "Epoch: 028, Loss: 1.3448, Train: 0.6345, Test: 0.6074\n",
            "Early stopping:  0.044895283132226946\n",
            "Epoch: 029, Loss: 1.3220, Train: 0.6412, Test: 0.6126\n",
            "Early stopping:  0.041039820093397204\n",
            "Epoch: 030, Loss: 1.3011, Train: 0.6468, Test: 0.6205\n",
            "Early stopping:  0.038151994916122874\n",
            "Epoch: 031, Loss: 1.2796, Train: 0.6514, Test: 0.6268\n",
            "Early stopping:  0.0357675852787645\n",
            "Epoch: 032, Loss: 1.2582, Train: 0.6534, Test: 0.6300\n",
            "Early stopping:  0.03408853080068282\n",
            "Epoch: 033, Loss: 1.2393, Train: 0.6593, Test: 0.6274\n",
            "Early stopping:  0.03295547388472625\n",
            "Epoch: 034, Loss: 1.2225, Train: 0.6614, Test: 0.6316\n",
            "Early stopping:  0.03127014675890157\n",
            "Epoch: 035, Loss: 1.2061, Train: 0.6680, Test: 0.6332\n",
            "Early stopping:  0.02892749595030283\n",
            "Epoch: 036, Loss: 1.1891, Train: 0.6729, Test: 0.6358\n",
            "Early stopping:  0.027116892118380653\n",
            "Epoch: 037, Loss: 1.1730, Train: 0.6774, Test: 0.6395\n",
            "Early stopping:  0.02623351886614585\n",
            "Epoch: 038, Loss: 1.1582, Train: 0.6786, Test: 0.6447\n",
            "Early stopping:  0.02556129607982264\n",
            "Epoch: 039, Loss: 1.1437, Train: 0.6816, Test: 0.6474\n",
            "Early stopping:  0.024611268106999262\n",
            "Epoch: 040, Loss: 1.1293, Train: 0.6850, Test: 0.6458\n",
            "Early stopping:  0.02355623836453409\n",
            "Epoch: 041, Loss: 1.1150, Train: 0.6874, Test: 0.6500\n",
            "Early stopping:  0.022915354409022023\n",
            "Epoch: 042, Loss: 1.1009, Train: 0.6909, Test: 0.6505\n",
            "Early stopping:  0.022653821127505615\n",
            "Epoch: 043, Loss: 1.0875, Train: 0.6945, Test: 0.6511\n",
            "Early stopping:  0.02225686084047582\n",
            "Epoch: 044, Loss: 1.0752, Train: 0.6946, Test: 0.6516\n",
            "Early stopping:  0.02146067860892689\n",
            "Epoch: 045, Loss: 1.0631, Train: 0.6987, Test: 0.6521\n",
            "Early stopping:  0.02049242831112441\n",
            "Epoch: 046, Loss: 1.0510, Train: 0.7001, Test: 0.6505\n",
            "Early stopping:  0.01966331289408762\n",
            "Epoch: 047, Loss: 1.0393, Train: 0.7030, Test: 0.6505\n",
            "Early stopping:  0.01909779587269283\n",
            "Epoch: 048, Loss: 1.0282, Train: 0.7046, Test: 0.6558\n",
            "Early stopping:  0.01865653147974514\n",
            "Epoch: 049, Loss: 1.0172, Train: 0.7086, Test: 0.6605\n",
            "Early stopping:  0.018140282071794495\n",
            "Epoch: 050, Loss: 1.0062, Train: 0.7114, Test: 0.6605\n",
            "Early stopping:  0.017642398416721303\n",
            "Epoch: 051, Loss: 0.9952, Train: 0.7157, Test: 0.6642\n",
            "Early stopping:  0.017387864070060682\n",
            "Epoch: 052, Loss: 0.9846, Train: 0.7186, Test: 0.6663\n",
            "Early stopping:  0.01724652489837393\n",
            "Epoch: 053, Loss: 0.9746, Train: 0.7201, Test: 0.6674\n",
            "Early stopping:  0.016879087352495045\n",
            "Epoch: 054, Loss: 0.9653, Train: 0.7222, Test: 0.6668\n",
            "Early stopping:  0.016203654146711384\n",
            "Epoch: 055, Loss: 0.9556, Train: 0.7246, Test: 0.6705\n",
            "Early stopping:  0.015581893284925971\n",
            "Epoch: 056, Loss: 0.9457, Train: 0.7283, Test: 0.6732\n",
            "Early stopping:  0.015321390994560452\n",
            "Epoch: 057, Loss: 0.9362, Train: 0.7305, Test: 0.6742\n",
            "Early stopping:  0.015264801591080481\n",
            "Epoch: 058, Loss: 0.9274, Train: 0.7314, Test: 0.6732\n",
            "Early stopping:  0.015059269231482802\n",
            "Epoch: 059, Loss: 0.9190, Train: 0.7338, Test: 0.6737\n",
            "Early stopping:  0.014465918843380783\n",
            "Epoch: 060, Loss: 0.9107, Train: 0.7383, Test: 0.6763\n",
            "Early stopping:  0.013771431650504422\n",
            "Epoch: 061, Loss: 0.9024, Train: 0.7392, Test: 0.6774\n",
            "Early stopping:  0.013344372601041068\n",
            "Epoch: 062, Loss: 0.8942, Train: 0.7413, Test: 0.6753\n",
            "Early stopping:  0.013136676012804922\n",
            "Epoch: 063, Loss: 0.8860, Train: 0.7447, Test: 0.6768\n",
            "Early stopping:  0.013060157088032355\n",
            "Epoch: 064, Loss: 0.8780, Train: 0.7462, Test: 0.6811\n",
            "Early stopping:  0.012918500822510868\n",
            "Epoch: 065, Loss: 0.8702, Train: 0.7479, Test: 0.6847\n",
            "Early stopping:  0.012726335787471604\n",
            "Epoch: 066, Loss: 0.8626, Train: 0.7511, Test: 0.6853\n",
            "Early stopping:  0.012487325811831944\n",
            "Epoch: 067, Loss: 0.8547, Train: 0.7528, Test: 0.6863\n",
            "Early stopping:  0.012343776691030506\n",
            "Epoch: 068, Loss: 0.8470, Train: 0.7557, Test: 0.6847\n",
            "Early stopping:  0.012259123254479582\n",
            "Epoch: 069, Loss: 0.8393, Train: 0.7570, Test: 0.6874\n",
            "Early stopping:  0.012238323052719347\n",
            "Epoch: 070, Loss: 0.8319, Train: 0.7576, Test: 0.6868\n",
            "Early stopping:  0.012133800649019241\n",
            "Epoch: 071, Loss: 0.8250, Train: 0.7604, Test: 0.6879\n",
            "Early stopping:  0.011768838680821833\n",
            "Epoch: 072, Loss: 0.8184, Train: 0.7638, Test: 0.6858\n",
            "Early stopping:  0.011320448175476316\n",
            "Epoch: 073, Loss: 0.8124, Train: 0.7650, Test: 0.6879\n",
            "Early stopping:  0.010660995325172984\n",
            "Epoch: 074, Loss: 0.8046, Train: 0.7658, Test: 0.6879\n",
            "Early stopping:  0.010657704302219483\n",
            "Epoch: 075, Loss: 0.7985, Train: 0.7691, Test: 0.6858\n",
            "Early stopping:  0.010584900087438507\n",
            "Epoch: 076, Loss: 0.7931, Train: 0.7721, Test: 0.6895\n",
            "Early stopping:  0.010228561661411228\n",
            "Epoch: 077, Loss: 0.7847, Train: 0.7699, Test: 0.6884\n",
            "Early stopping:  0.010593130852659635\n",
            "Epoch: 078, Loss: 0.7811, Train: 0.7757, Test: 0.6863\n",
            "Early stopping:  0.009655050657762142\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 14, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.73      0.76      0.74       109\n",
            "            CAD_and_CAM       0.68      0.76      0.72        94\n",
            "              Companies       0.49      0.46      0.48        95\n",
            "       Computer_Science       0.76      0.73      0.75       119\n",
            "            Consultants       0.47      0.52      0.49        94\n",
            "           Data_Formats       0.82      0.71      0.76       112\n",
            "    Data_Communications       0.75      0.75      0.75       111\n",
            "              Education       0.91      0.92      0.91       106\n",
            "               Graphics       0.81      0.85      0.83       104\n",
            "               Hardware       0.63      0.68      0.66        94\n",
            "               Internet       0.56      0.65      0.60        82\n",
            "       Mobile_Computing       0.76      0.72      0.74        85\n",
            "             Multimedia       0.66      0.72      0.69        83\n",
            "            Open_Source       0.67      0.77      0.72       101\n",
            "            Programming       0.54      0.60      0.57        97\n",
            "               Robotics       0.96      0.88      0.92       108\n",
            "               Security       0.75      0.78      0.77       103\n",
            "               Software       0.28      0.22      0.25        94\n",
            "                Systems       0.66      0.48      0.55       109\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.69      0.69      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 4.9366, Train: 0.2466, Test: 0.2489\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 10.6697, Train: 0.2059, Test: 0.2195\n",
            "Early stopping:  4.053901367516104\n",
            "Epoch: 003, Loss: 9.7448, Train: 0.2667, Test: 0.2705\n",
            "Early stopping:  3.077939232890603\n",
            "Epoch: 004, Loss: 8.1261, Train: 0.3279, Test: 0.3337\n",
            "Early stopping:  2.5183506110143217\n",
            "Epoch: 005, Loss: 6.4897, Train: 0.3659, Test: 0.3595\n",
            "Early stopping:  2.337337448734257\n",
            "Epoch: 006, Loss: 4.6449, Train: 0.3939, Test: 0.3811\n",
            "Early stopping:  2.4343339455662947\n",
            "Epoch: 007, Loss: 3.3945, Train: 0.4395, Test: 0.4316\n",
            "Early stopping:  2.5620572216688777\n",
            "Epoch: 008, Loss: 2.5565, Train: 0.4889, Test: 0.4784\n",
            "Early stopping:  2.2725140570852798\n",
            "Epoch: 009, Loss: 2.0564, Train: 0.5049, Test: 0.4947\n",
            "Early stopping:  1.7815211354050113\n",
            "Epoch: 010, Loss: 1.8829, Train: 0.4914, Test: 0.4974\n",
            "Early stopping:  1.1350451594670299\n",
            "Epoch: 011, Loss: 1.8498, Train: 0.5088, Test: 0.5111\n",
            "Early stopping:  0.6496657059842349\n",
            "Epoch: 012, Loss: 1.7877, Train: 0.5353, Test: 0.5395\n",
            "Early stopping:  0.31252301823958917\n",
            "Epoch: 013, Loss: 1.6936, Train: 0.5550, Test: 0.5574\n",
            "Early stopping:  0.13411535737378713\n",
            "Epoch: 014, Loss: 1.6152, Train: 0.5793, Test: 0.5884\n",
            "Early stopping:  0.1108211967472182\n",
            "Epoch: 015, Loss: 1.5422, Train: 0.6043, Test: 0.6021\n",
            "Early stopping:  0.12472517357948135\n",
            "Epoch: 016, Loss: 1.4783, Train: 0.6228, Test: 0.6126\n",
            "Early stopping:  0.12211438972789689\n",
            "Epoch: 017, Loss: 1.4251, Train: 0.6270, Test: 0.6137\n",
            "Early stopping:  0.10686071359477606\n",
            "Epoch: 018, Loss: 1.3817, Train: 0.6318, Test: 0.6174\n",
            "Early stopping:  0.09285703620837076\n",
            "Epoch: 019, Loss: 1.3499, Train: 0.6339, Test: 0.6205\n",
            "Early stopping:  0.07672893800614218\n",
            "Epoch: 020, Loss: 1.3302, Train: 0.6347, Test: 0.6242\n",
            "Early stopping:  0.05962991869999984\n",
            "Epoch: 021, Loss: 1.3158, Train: 0.6359, Test: 0.6195\n",
            "Early stopping:  0.043714857305763376\n",
            "Epoch: 022, Loss: 1.2971, Train: 0.6416, Test: 0.6211\n",
            "Early stopping:  0.03253146314171232\n",
            "Epoch: 023, Loss: 1.2715, Train: 0.6489, Test: 0.6279\n",
            "Early stopping:  0.030175813489812187\n",
            "Epoch: 024, Loss: 1.2438, Train: 0.6528, Test: 0.6405\n",
            "Early stopping:  0.03461292601760443\n",
            "Epoch: 025, Loss: 1.2179, Train: 0.6638, Test: 0.6426\n",
            "Early stopping:  0.03945313165147818\n",
            "Epoch: 026, Loss: 1.1954, Train: 0.6697, Test: 0.6463\n",
            "Early stopping:  0.04063815823384275\n",
            "Epoch: 027, Loss: 1.1744, Train: 0.6749, Test: 0.6579\n",
            "Early stopping:  0.038433802757444645\n",
            "Epoch: 028, Loss: 1.1535, Train: 0.6786, Test: 0.6611\n",
            "Early stopping:  0.03547389210453638\n",
            "Epoch: 029, Loss: 1.1333, Train: 0.6837, Test: 0.6632\n",
            "Early stopping:  0.033394990743432613\n",
            "Epoch: 030, Loss: 1.1137, Train: 0.6847, Test: 0.6584\n",
            "Early stopping:  0.03233791946057076\n",
            "Epoch: 031, Loss: 1.0953, Train: 0.6862, Test: 0.6589\n",
            "Early stopping:  0.03129524569124821\n",
            "Epoch: 032, Loss: 1.0772, Train: 0.6930, Test: 0.6600\n",
            "Early stopping:  0.030131868636136733\n",
            "Epoch: 033, Loss: 1.0602, Train: 0.6978, Test: 0.6637\n",
            "Early stopping:  0.028904266782078455\n",
            "Epoch: 034, Loss: 1.0437, Train: 0.7021, Test: 0.6689\n",
            "Early stopping:  0.02772742928008084\n",
            "Epoch: 035, Loss: 1.0301, Train: 0.7061, Test: 0.6726\n",
            "Early stopping:  0.025988851935652965\n",
            "Epoch: 036, Loss: 1.0169, Train: 0.7104, Test: 0.6763\n",
            "Early stopping:  0.02387759825807913\n",
            "Epoch: 037, Loss: 1.0028, Train: 0.7158, Test: 0.6758\n",
            "Early stopping:  0.0223869045256054\n",
            "Epoch: 038, Loss: 0.9875, Train: 0.7158, Test: 0.6758\n",
            "Early stopping:  0.022098619049596716\n",
            "Epoch: 039, Loss: 0.9737, Train: 0.7192, Test: 0.6758\n",
            "Early stopping:  0.02248293740135586\n",
            "Epoch: 040, Loss: 0.9613, Train: 0.7228, Test: 0.6816\n",
            "Early stopping:  0.022195300234334917\n",
            "Epoch: 041, Loss: 0.9500, Train: 0.7259, Test: 0.6805\n",
            "Early stopping:  0.020855716711672256\n",
            "Epoch: 042, Loss: 0.9382, Train: 0.7274, Test: 0.6847\n",
            "Early stopping:  0.019346936102995316\n",
            "Epoch: 043, Loss: 0.9255, Train: 0.7303, Test: 0.6863\n",
            "Early stopping:  0.01890402207264053\n",
            "Epoch: 044, Loss: 0.9121, Train: 0.7355, Test: 0.6874\n",
            "Early stopping:  0.019465573764105425\n",
            "Epoch: 045, Loss: 0.8995, Train: 0.7387, Test: 0.6874\n",
            "Early stopping:  0.020117633303636008\n",
            "Epoch: 046, Loss: 0.8878, Train: 0.7420, Test: 0.6916\n",
            "Early stopping:  0.02006544724132574\n",
            "Epoch: 047, Loss: 0.8767, Train: 0.7463, Test: 0.6932\n",
            "Early stopping:  0.019296944313731045\n",
            "Epoch: 048, Loss: 0.8660, Train: 0.7503, Test: 0.6932\n",
            "Early stopping:  0.01816844862499232\n",
            "Epoch: 049, Loss: 0.8550, Train: 0.7526, Test: 0.6900\n",
            "Early stopping:  0.01749023951504551\n",
            "Epoch: 050, Loss: 0.8449, Train: 0.7543, Test: 0.6889\n",
            "Early stopping:  0.0169933331361998\n",
            "Epoch: 051, Loss: 0.8354, Train: 0.7583, Test: 0.6905\n",
            "Early stopping:  0.016414413625365856\n",
            "Epoch: 052, Loss: 0.8260, Train: 0.7604, Test: 0.6889\n",
            "Early stopping:  0.01576395463129064\n",
            "Epoch: 053, Loss: 0.8164, Train: 0.7632, Test: 0.6916\n",
            "Early stopping:  0.01520693242548727\n",
            "Epoch: 054, Loss: 0.8066, Train: 0.7653, Test: 0.6926\n",
            "Early stopping:  0.015118458780804929\n",
            "Epoch: 055, Loss: 0.7971, Train: 0.7693, Test: 0.6932\n",
            "Early stopping:  0.015197590101973085\n",
            "Epoch: 056, Loss: 0.7880, Train: 0.7712, Test: 0.6926\n",
            "Early stopping:  0.015063505240130157\n",
            "Epoch: 057, Loss: 0.7792, Train: 0.7746, Test: 0.6937\n",
            "Early stopping:  0.014686242881540188\n",
            "Epoch: 058, Loss: 0.7701, Train: 0.7776, Test: 0.6921\n",
            "Early stopping:  0.014352309175839982\n",
            "Epoch: 059, Loss: 0.7612, Train: 0.7797, Test: 0.6942\n",
            "Early stopping:  0.01418319749677269\n",
            "Epoch: 060, Loss: 0.7526, Train: 0.7811, Test: 0.6937\n",
            "Early stopping:  0.01405173984585784\n",
            "Epoch: 061, Loss: 0.7444, Train: 0.7838, Test: 0.6958\n",
            "Early stopping:  0.01377614231679562\n",
            "Epoch: 062, Loss: 0.7361, Train: 0.7853, Test: 0.6968\n",
            "Early stopping:  0.013400526524005424\n",
            "Epoch: 063, Loss: 0.7278, Train: 0.7891, Test: 0.6974\n",
            "Early stopping:  0.013173302684002993\n",
            "Epoch: 064, Loss: 0.7195, Train: 0.7901, Test: 0.6984\n",
            "Early stopping:  0.013092400129810689\n",
            "Epoch: 065, Loss: 0.7115, Train: 0.7926, Test: 0.6958\n",
            "Early stopping:  0.013041411943134119\n",
            "Epoch: 066, Loss: 0.7035, Train: 0.7949, Test: 0.6963\n",
            "Early stopping:  0.012905900443030194\n",
            "Epoch: 067, Loss: 0.6955, Train: 0.7987, Test: 0.6979\n",
            "Early stopping:  0.01273785351200539\n",
            "Epoch: 068, Loss: 0.6877, Train: 0.7999, Test: 0.6979\n",
            "Early stopping:  0.01259550614052047\n",
            "Epoch: 069, Loss: 0.6800, Train: 0.8036, Test: 0.6984\n",
            "Early stopping:  0.012444947673917519\n",
            "Epoch: 070, Loss: 0.6728, Train: 0.8045, Test: 0.6968\n",
            "Early stopping:  0.012149734148701078\n",
            "Epoch: 071, Loss: 0.6666, Train: 0.8050, Test: 0.6989\n",
            "Early stopping:  0.011508340869185485\n",
            "Epoch: 072, Loss: 0.6626, Train: 0.8036, Test: 0.6921\n",
            "Early stopping:  0.01011698829401328\n",
            "Epoch: 073, Loss: 0.6564, Train: 0.8083, Test: 0.6984\n",
            "Early stopping:  0.009107562609659186\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.75      0.69      0.72        93\n",
            "            CAD_and_CAM       0.76      0.77      0.76       101\n",
            "              Companies       0.51      0.66      0.58        82\n",
            "       Computer_Science       0.72      0.75      0.74        93\n",
            "            Consultants       0.57      0.67      0.62        98\n",
            "           Data_Formats       0.78      0.83      0.81       110\n",
            "    Data_Communications       0.73      0.73      0.73        98\n",
            "              Education       0.85      0.93      0.89        94\n",
            "               Graphics       0.88      0.86      0.87       111\n",
            "               Hardware       0.69      0.65      0.67       101\n",
            "               Internet       0.67      0.59      0.63       103\n",
            "       Mobile_Computing       0.74      0.71      0.72        94\n",
            "             Multimedia       0.60      0.71      0.65        89\n",
            "            Open_Source       0.69      0.66      0.67        92\n",
            "            Programming       0.59      0.56      0.58       109\n",
            "               Robotics       0.90      0.85      0.88       116\n",
            "               Security       0.87      0.79      0.83        95\n",
            "               Software       0.31      0.29      0.30       100\n",
            "                Systems       0.65      0.56      0.60       121\n",
            "\n",
            "               accuracy                           0.70      1900\n",
            "              macro avg       0.70      0.70      0.70      1900\n",
            "           weighted avg       0.70      0.70      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.4510, Train: 0.1259, Test: 0.1237\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 15.8626, Train: 0.2059, Test: 0.1937\n",
            "Early stopping:  7.362155864042935\n",
            "Epoch: 003, Loss: 12.6815, Train: 0.1483, Test: 0.1532\n",
            "Early stopping:  5.335460803753879\n",
            "Epoch: 004, Loss: 14.1048, Train: 0.2372, Test: 0.2484\n",
            "Early stopping:  4.571717810773416\n",
            "Epoch: 005, Loss: 8.7252, Train: 0.2799, Test: 0.2795\n",
            "Early stopping:  4.225296553071645\n",
            "Epoch: 006, Loss: 7.7943, Train: 0.3172, Test: 0.3158\n",
            "Early stopping:  3.4673016923918176\n",
            "Epoch: 007, Loss: 7.0640, Train: 0.3416, Test: 0.3300\n",
            "Early stopping:  3.1274246968871955\n",
            "Epoch: 008, Loss: 5.9707, Train: 0.3937, Test: 0.3900\n",
            "Early stopping:  3.1683315339159908\n",
            "Epoch: 009, Loss: 4.1858, Train: 0.4311, Test: 0.4279\n",
            "Early stopping:  1.7516229994792785\n",
            "Epoch: 010, Loss: 3.0142, Train: 0.4278, Test: 0.4205\n",
            "Early stopping:  1.9862957742025766\n",
            "Epoch: 011, Loss: 2.5420, Train: 0.4404, Test: 0.4347\n",
            "Early stopping:  1.9275522908190494\n",
            "Epoch: 012, Loss: 2.2557, Train: 0.4503, Test: 0.4395\n",
            "Early stopping:  1.5192916581349047\n",
            "Epoch: 013, Loss: 2.0680, Train: 0.4586, Test: 0.4458\n",
            "Early stopping:  0.8461926147078235\n",
            "Epoch: 014, Loss: 1.9644, Train: 0.4699, Test: 0.4553\n",
            "Early stopping:  0.42229670466773195\n",
            "Epoch: 015, Loss: 1.8767, Train: 0.5009, Test: 0.4842\n",
            "Early stopping:  0.26471912668840936\n",
            "Epoch: 016, Loss: 1.7898, Train: 0.5349, Test: 0.5068\n",
            "Early stopping:  0.1804477224684668\n",
            "Epoch: 017, Loss: 1.7271, Train: 0.5439, Test: 0.5295\n",
            "Early stopping:  0.13588242265534528\n",
            "Epoch: 018, Loss: 1.6869, Train: 0.5526, Test: 0.5268\n",
            "Early stopping:  0.11259912013271058\n",
            "Epoch: 019, Loss: 1.6486, Train: 0.5564, Test: 0.5284\n",
            "Early stopping:  0.08993322316465069\n",
            "Epoch: 020, Loss: 1.6106, Train: 0.5614, Test: 0.5395\n",
            "Early stopping:  0.0695412407356648\n",
            "Epoch: 021, Loss: 1.5754, Train: 0.5686, Test: 0.5542\n",
            "Early stopping:  0.06006423264325888\n",
            "Epoch: 022, Loss: 1.5380, Train: 0.5833, Test: 0.5647\n",
            "Early stopping:  0.05868258343879888\n",
            "Epoch: 023, Loss: 1.4970, Train: 0.5939, Test: 0.5747\n",
            "Early stopping:  0.05942426297755751\n",
            "Epoch: 024, Loss: 1.4557, Train: 0.6033, Test: 0.5895\n",
            "Early stopping:  0.061403831381676995\n",
            "Epoch: 025, Loss: 1.4166, Train: 0.6116, Test: 0.6011\n",
            "Early stopping:  0.06323326817179029\n",
            "Epoch: 026, Loss: 1.3823, Train: 0.6180, Test: 0.6100\n",
            "Early stopping:  0.06199762114561587\n",
            "Epoch: 027, Loss: 1.3550, Train: 0.6214, Test: 0.6121\n",
            "Early stopping:  0.056701440550763635\n",
            "Epoch: 028, Loss: 1.3342, Train: 0.6292, Test: 0.6089\n",
            "Early stopping:  0.04849911612679764\n",
            "Epoch: 029, Loss: 1.3164, Train: 0.6337, Test: 0.6100\n",
            "Early stopping:  0.03964305872645341\n",
            "Epoch: 030, Loss: 1.2981, Train: 0.6359, Test: 0.6147\n",
            "Early stopping:  0.032863229243603215\n",
            "Epoch: 031, Loss: 1.2782, Train: 0.6401, Test: 0.6158\n",
            "Early stopping:  0.03000269292357295\n",
            "Epoch: 032, Loss: 1.2574, Train: 0.6496, Test: 0.6258\n",
            "Early stopping:  0.03034204362125147\n",
            "Epoch: 033, Loss: 1.2373, Train: 0.6537, Test: 0.6279\n",
            "Early stopping:  0.03144805819997895\n",
            "Epoch: 034, Loss: 1.2187, Train: 0.6584, Test: 0.6321\n",
            "Early stopping:  0.03159616944898387\n",
            "Epoch: 035, Loss: 1.2018, Train: 0.6622, Test: 0.6358\n",
            "Early stopping:  0.030329828411161684\n",
            "Epoch: 036, Loss: 1.1870, Train: 0.6674, Test: 0.6300\n",
            "Early stopping:  0.02792783848128922\n",
            "Epoch: 037, Loss: 1.1741, Train: 0.6704, Test: 0.6326\n",
            "Early stopping:  0.025059060120320675\n",
            "Epoch: 038, Loss: 1.1608, Train: 0.6758, Test: 0.6332\n",
            "Early stopping:  0.022716896804065613\n",
            "Epoch: 039, Loss: 1.1464, Train: 0.6787, Test: 0.6379\n",
            "Early stopping:  0.021646668853413664\n",
            "Epoch: 040, Loss: 1.1321, Train: 0.6838, Test: 0.6405\n",
            "Early stopping:  0.021732523542412438\n",
            "Epoch: 041, Loss: 1.1178, Train: 0.6867, Test: 0.6416\n",
            "Early stopping:  0.02232559273282254\n",
            "Epoch: 042, Loss: 1.1036, Train: 0.6904, Test: 0.6432\n",
            "Early stopping:  0.02262682021187835\n",
            "Epoch: 043, Loss: 1.0899, Train: 0.6924, Test: 0.6432\n",
            "Early stopping:  0.022395966253246725\n",
            "Epoch: 044, Loss: 1.0773, Train: 0.6941, Test: 0.6416\n",
            "Early stopping:  0.021756791866345337\n",
            "Epoch: 045, Loss: 1.0651, Train: 0.6967, Test: 0.6437\n",
            "Early stopping:  0.020820689162498247\n",
            "Epoch: 046, Loss: 1.0527, Train: 0.6983, Test: 0.6474\n",
            "Early stopping:  0.01999775160208762\n",
            "Epoch: 047, Loss: 1.0408, Train: 0.7012, Test: 0.6463\n",
            "Early stopping:  0.019419750260190324\n",
            "Epoch: 048, Loss: 1.0293, Train: 0.7032, Test: 0.6484\n",
            "Early stopping:  0.019017472772138767\n",
            "Epoch: 049, Loss: 1.0179, Train: 0.7084, Test: 0.6516\n",
            "Early stopping:  0.018643950179140546\n",
            "Epoch: 050, Loss: 1.0065, Train: 0.7097, Test: 0.6558\n",
            "Early stopping:  0.018246790827081628\n",
            "Epoch: 051, Loss: 0.9959, Train: 0.7120, Test: 0.6568\n",
            "Early stopping:  0.017798323292834574\n",
            "Epoch: 052, Loss: 0.9859, Train: 0.7143, Test: 0.6589\n",
            "Early stopping:  0.017205566307307754\n",
            "Epoch: 053, Loss: 0.9760, Train: 0.7171, Test: 0.6611\n",
            "Early stopping:  0.016510919441392047\n",
            "Epoch: 054, Loss: 0.9661, Train: 0.7193, Test: 0.6600\n",
            "Early stopping:  0.01593206510870833\n",
            "Epoch: 055, Loss: 0.9563, Train: 0.7221, Test: 0.6595\n",
            "Early stopping:  0.015653320937589758\n",
            "Epoch: 056, Loss: 0.9468, Train: 0.7267, Test: 0.6600\n",
            "Early stopping:  0.015476162162859361\n",
            "Epoch: 057, Loss: 0.9379, Train: 0.7291, Test: 0.6616\n",
            "Early stopping:  0.015077639463445085\n",
            "Epoch: 058, Loss: 0.9295, Train: 0.7305, Test: 0.6616\n",
            "Early stopping:  0.014491592399714003\n",
            "Epoch: 059, Loss: 0.9207, Train: 0.7328, Test: 0.6642\n",
            "Early stopping:  0.01402319694085973\n",
            "Epoch: 060, Loss: 0.9121, Train: 0.7355, Test: 0.6621\n",
            "Early stopping:  0.013688140131856439\n",
            "Epoch: 061, Loss: 0.9040, Train: 0.7368, Test: 0.6626\n",
            "Early stopping:  0.01347236007743364\n",
            "Epoch: 062, Loss: 0.8959, Train: 0.7374, Test: 0.6663\n",
            "Early stopping:  0.013265507834003638\n",
            "Epoch: 063, Loss: 0.8879, Train: 0.7421, Test: 0.6679\n",
            "Early stopping:  0.012944230040937208\n",
            "Epoch: 064, Loss: 0.8799, Train: 0.7451, Test: 0.6689\n",
            "Early stopping:  0.012733195386370747\n",
            "Epoch: 065, Loss: 0.8722, Train: 0.7461, Test: 0.6684\n",
            "Early stopping:  0.012584843818087893\n",
            "Epoch: 066, Loss: 0.8645, Train: 0.7479, Test: 0.6705\n",
            "Early stopping:  0.012398600930653864\n",
            "Epoch: 067, Loss: 0.8570, Train: 0.7513, Test: 0.6732\n",
            "Early stopping:  0.012196060161779645\n",
            "Epoch: 068, Loss: 0.8499, Train: 0.7532, Test: 0.6732\n",
            "Early stopping:  0.011891889343966516\n",
            "Epoch: 069, Loss: 0.8427, Train: 0.7545, Test: 0.6747\n",
            "Early stopping:  0.011629005898457575\n",
            "Epoch: 070, Loss: 0.8356, Train: 0.7557, Test: 0.6753\n",
            "Early stopping:  0.011407082857055833\n",
            "Epoch: 071, Loss: 0.8286, Train: 0.7579, Test: 0.6758\n",
            "Early stopping:  0.01124045518532399\n",
            "Epoch: 072, Loss: 0.8218, Train: 0.7587, Test: 0.6747\n",
            "Early stopping:  0.011122550731838073\n",
            "Epoch: 073, Loss: 0.8150, Train: 0.7618, Test: 0.6721\n",
            "Early stopping:  0.01095696367707344\n",
            "Epoch: 074, Loss: 0.8083, Train: 0.7638, Test: 0.6732\n",
            "Early stopping:  0.01079849979315659\n",
            "Epoch: 075, Loss: 0.8018, Train: 0.7649, Test: 0.6732\n",
            "Early stopping:  0.010609958542568161\n",
            "Epoch: 076, Loss: 0.7957, Train: 0.7650, Test: 0.6732\n",
            "Early stopping:  0.010330495381316791\n",
            "Epoch: 077, Loss: 0.7897, Train: 0.7668, Test: 0.6779\n",
            "Early stopping:  0.00998943311270413\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.77      0.78       106\n",
            "            CAD_and_CAM       0.66      0.76      0.70        94\n",
            "              Companies       0.47      0.47      0.47       112\n",
            "       Computer_Science       0.70      0.69      0.70       106\n",
            "            Consultants       0.54      0.53      0.54       103\n",
            "           Data_Formats       0.79      0.79      0.79        87\n",
            "    Data_Communications       0.65      0.69      0.67        91\n",
            "              Education       0.90      0.87      0.88       100\n",
            "               Graphics       0.84      0.96      0.89        91\n",
            "               Hardware       0.61      0.62      0.62       111\n",
            "               Internet       0.70      0.63      0.66        99\n",
            "       Mobile_Computing       0.88      0.75      0.81       109\n",
            "             Multimedia       0.65      0.72      0.68        89\n",
            "            Open_Source       0.57      0.71      0.63        98\n",
            "            Programming       0.53      0.52      0.53       111\n",
            "               Robotics       0.87      0.90      0.89       103\n",
            "               Security       0.76      0.79      0.78        86\n",
            "               Software       0.43      0.25      0.32       105\n",
            "                Systems       0.55      0.57      0.56        99\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.67      0.68      0.67      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.3091, Train: 0.0517, Test: 0.0563\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 22.4621, Train: 0.1799, Test: 0.1811\n",
            "Early stopping:  11.421914969152477\n",
            "Epoch: 003, Loss: 12.4523, Train: 0.2432, Test: 0.2337\n",
            "Early stopping:  8.153280352260717\n",
            "Epoch: 004, Loss: 10.8130, Train: 0.2154, Test: 0.2084\n",
            "Early stopping:  6.816224367146422\n",
            "Epoch: 005, Loss: 11.4437, Train: 0.2558, Test: 0.2574\n",
            "Early stopping:  5.9443943299769\n",
            "Epoch: 006, Loss: 9.7402, Train: 0.3025, Test: 0.3068\n",
            "Early stopping:  5.170422872373015\n",
            "Epoch: 007, Loss: 8.4065, Train: 0.3296, Test: 0.3442\n",
            "Early stopping:  1.560038617301263\n",
            "Epoch: 008, Loss: 7.2332, Train: 0.3846, Test: 0.4026\n",
            "Early stopping:  1.7240953490409074\n",
            "Epoch: 009, Loss: 5.9888, Train: 0.4199, Test: 0.4342\n",
            "Early stopping:  2.127408296861798\n",
            "Epoch: 010, Loss: 5.0124, Train: 0.4254, Test: 0.4395\n",
            "Early stopping:  1.8796876648658158\n",
            "Epoch: 011, Loss: 4.2799, Train: 0.4149, Test: 0.4363\n",
            "Early stopping:  1.6640769501880053\n",
            "Epoch: 012, Loss: 3.6885, Train: 0.4222, Test: 0.4395\n",
            "Early stopping:  1.4066064232054778\n",
            "Epoch: 013, Loss: 3.1063, Train: 0.4401, Test: 0.4437\n",
            "Early stopping:  1.1283051662948527\n",
            "Epoch: 014, Loss: 2.5972, Train: 0.4604, Test: 0.4516\n",
            "Early stopping:  0.9513756993100394\n",
            "Epoch: 015, Loss: 2.2781, Train: 0.4741, Test: 0.4774\n",
            "Early stopping:  0.8102852012474651\n",
            "Epoch: 016, Loss: 2.1267, Train: 0.4812, Test: 0.4853\n",
            "Early stopping:  0.6406876120238897\n",
            "Epoch: 017, Loss: 2.0403, Train: 0.4937, Test: 0.4879\n",
            "Early stopping:  0.43369459859683074\n",
            "Epoch: 018, Loss: 1.9607, Train: 0.5028, Test: 0.5037\n",
            "Early stopping:  0.2510255492660473\n",
            "Epoch: 019, Loss: 1.8858, Train: 0.5158, Test: 0.5153\n",
            "Early stopping:  0.15217684835332046\n",
            "Epoch: 020, Loss: 1.8349, Train: 0.5307, Test: 0.5237\n",
            "Early stopping:  0.1171603370645654\n",
            "Epoch: 021, Loss: 1.7996, Train: 0.5409, Test: 0.5342\n",
            "Early stopping:  0.09719622240078943\n",
            "Epoch: 022, Loss: 1.7695, Train: 0.5511, Test: 0.5479\n",
            "Early stopping:  0.07547425440067734\n",
            "Epoch: 023, Loss: 1.7360, Train: 0.5574, Test: 0.5542\n",
            "Early stopping:  0.058028451104213355\n",
            "Epoch: 024, Loss: 1.6963, Train: 0.5676, Test: 0.5605\n",
            "Early stopping:  0.0539442853388239\n",
            "Epoch: 025, Loss: 1.6518, Train: 0.5809, Test: 0.5753\n",
            "Early stopping:  0.058491065212123314\n",
            "Epoch: 026, Loss: 1.6045, Train: 0.5918, Test: 0.5847\n",
            "Early stopping:  0.06563639449157797\n",
            "Epoch: 027, Loss: 1.5566, Train: 0.6011, Test: 0.5826\n",
            "Early stopping:  0.07132413001988559\n",
            "Epoch: 028, Loss: 1.5120, Train: 0.6037, Test: 0.5842\n",
            "Early stopping:  0.07335727849093426\n",
            "Epoch: 029, Loss: 1.4757, Train: 0.5986, Test: 0.5826\n",
            "Early stopping:  0.07038837847622025\n",
            "Epoch: 030, Loss: 1.4503, Train: 0.5979, Test: 0.5884\n",
            "Early stopping:  0.06195990910218164\n",
            "Epoch: 031, Loss: 1.4302, Train: 0.6011, Test: 0.5947\n",
            "Early stopping:  0.050345787318373025\n",
            "Epoch: 032, Loss: 1.4061, Train: 0.6103, Test: 0.6079\n",
            "Early stopping:  0.04093823414093384\n",
            "Epoch: 033, Loss: 1.3768, Train: 0.6212, Test: 0.6168\n",
            "Early stopping:  0.03835482495670844\n",
            "Epoch: 034, Loss: 1.3476, Train: 0.6305, Test: 0.6295\n",
            "Early stopping:  0.041063460651510106\n",
            "Epoch: 035, Loss: 1.3225, Train: 0.6380, Test: 0.6326\n",
            "Early stopping:  0.043367482558952876\n",
            "Epoch: 036, Loss: 1.3028, Train: 0.6426, Test: 0.6337\n",
            "Early stopping:  0.04137741888431322\n",
            "Epoch: 037, Loss: 1.2868, Train: 0.6461, Test: 0.6405\n",
            "Early stopping:  0.035775994281702896\n",
            "Epoch: 038, Loss: 1.2717, Train: 0.6501, Test: 0.6432\n",
            "Early stopping:  0.029800498891666943\n",
            "Epoch: 039, Loss: 1.2550, Train: 0.6532, Test: 0.6468\n",
            "Early stopping:  0.026267952502998463\n",
            "Epoch: 040, Loss: 1.2363, Train: 0.6563, Test: 0.6453\n",
            "Early stopping:  0.02608713494189914\n",
            "Epoch: 041, Loss: 1.2170, Train: 0.6570, Test: 0.6547\n",
            "Early stopping:  0.027717843827314986\n",
            "Epoch: 042, Loss: 1.1992, Train: 0.6616, Test: 0.6553\n",
            "Early stopping:  0.028962837168965305\n",
            "Epoch: 043, Loss: 1.1832, Train: 0.6661, Test: 0.6579\n",
            "Early stopping:  0.028595676022632764\n",
            "Epoch: 044, Loss: 1.1682, Train: 0.6717, Test: 0.6600\n",
            "Early stopping:  0.026903456646664884\n",
            "Epoch: 045, Loss: 1.1533, Train: 0.6762, Test: 0.6600\n",
            "Early stopping:  0.02503889973116441\n",
            "Epoch: 046, Loss: 1.1393, Train: 0.6791, Test: 0.6632\n",
            "Early stopping:  0.02365250229875618\n",
            "Epoch: 047, Loss: 1.1269, Train: 0.6832, Test: 0.6611\n",
            "Early stopping:  0.02238409074576175\n",
            "Epoch: 048, Loss: 1.1154, Train: 0.6854, Test: 0.6658\n",
            "Early stopping:  0.02090767914266828\n",
            "Epoch: 049, Loss: 1.1044, Train: 0.6888, Test: 0.6658\n",
            "Early stopping:  0.0192877497671906\n",
            "Epoch: 050, Loss: 1.0943, Train: 0.6903, Test: 0.6679\n",
            "Early stopping:  0.017823728490809248\n",
            "Epoch: 051, Loss: 1.0844, Train: 0.6918, Test: 0.6737\n",
            "Early stopping:  0.016791666218399535\n",
            "Epoch: 052, Loss: 1.0743, Train: 0.6922, Test: 0.6732\n",
            "Early stopping:  0.01616670091311962\n",
            "Epoch: 053, Loss: 1.0638, Train: 0.6950, Test: 0.6711\n",
            "Early stopping:  0.015991104349275607\n",
            "Epoch: 054, Loss: 1.0533, Train: 0.6963, Test: 0.6763\n",
            "Early stopping:  0.01620022076048947\n",
            "Epoch: 055, Loss: 1.0427, Train: 0.6983, Test: 0.6753\n",
            "Early stopping:  0.016503568118345874\n",
            "Epoch: 056, Loss: 1.0319, Train: 0.7020, Test: 0.6747\n",
            "Early stopping:  0.016735460712003613\n",
            "Epoch: 057, Loss: 1.0218, Train: 0.7034, Test: 0.6768\n",
            "Early stopping:  0.016684568548467768\n",
            "Epoch: 058, Loss: 1.0128, Train: 0.7066, Test: 0.6816\n",
            "Early stopping:  0.016138494604910916\n",
            "Epoch: 059, Loss: 1.0043, Train: 0.7078, Test: 0.6858\n",
            "Early stopping:  0.015180986684239294\n",
            "Epoch: 060, Loss: 0.9959, Train: 0.7099, Test: 0.6863\n",
            "Early stopping:  0.014177647928772345\n",
            "Epoch: 061, Loss: 0.9873, Train: 0.7116, Test: 0.6842\n",
            "Early stopping:  0.013590810204719353\n",
            "Epoch: 062, Loss: 0.9787, Train: 0.7133, Test: 0.6863\n",
            "Early stopping:  0.013454096911008141\n",
            "Epoch: 063, Loss: 0.9707, Train: 0.7149, Test: 0.6874\n",
            "Early stopping:  0.013352639962944505\n",
            "Epoch: 064, Loss: 0.9629, Train: 0.7184, Test: 0.6884\n",
            "Early stopping:  0.013041731409706457\n",
            "Epoch: 065, Loss: 0.9554, Train: 0.7205, Test: 0.6884\n",
            "Early stopping:  0.012596117103928488\n",
            "Epoch: 066, Loss: 0.9479, Train: 0.7239, Test: 0.6895\n",
            "Early stopping:  0.012172838869371745\n",
            "Epoch: 067, Loss: 0.9406, Train: 0.7266, Test: 0.6900\n",
            "Early stopping:  0.011886089207622512\n",
            "Epoch: 068, Loss: 0.9332, Train: 0.7280, Test: 0.6916\n",
            "Early stopping:  0.011727688572014313\n",
            "Epoch: 069, Loss: 0.9261, Train: 0.7305, Test: 0.6921\n",
            "Early stopping:  0.01157271485096393\n",
            "Epoch: 070, Loss: 0.9193, Train: 0.7318, Test: 0.6916\n",
            "Early stopping:  0.011326235870178272\n",
            "Epoch: 071, Loss: 0.9127, Train: 0.7342, Test: 0.6942\n",
            "Early stopping:  0.011018134803932727\n",
            "Epoch: 072, Loss: 0.9061, Train: 0.7363, Test: 0.6974\n",
            "Early stopping:  0.010695030229004842\n",
            "Epoch: 073, Loss: 0.8993, Train: 0.7372, Test: 0.6968\n",
            "Early stopping:  0.010549306974293517\n",
            "Epoch: 074, Loss: 0.8924, Train: 0.7368, Test: 0.6947\n",
            "Early stopping:  0.010617758546224876\n",
            "Epoch: 075, Loss: 0.8858, Train: 0.7387, Test: 0.6963\n",
            "Early stopping:  0.010684625393067202\n",
            "Epoch: 076, Loss: 0.8793, Train: 0.7400, Test: 0.6968\n",
            "Early stopping:  0.010632552015208522\n",
            "Epoch: 077, Loss: 0.8729, Train: 0.7422, Test: 0.7011\n",
            "Early stopping:  0.010436205096244846\n",
            "Epoch: 078, Loss: 0.8666, Train: 0.7458, Test: 0.7037\n",
            "Early stopping:  0.010185886634733972\n",
            "Epoch: 079, Loss: 0.8605, Train: 0.7492, Test: 0.7074\n",
            "Early stopping:  0.01000047671009691\n",
            "Epoch: 080, Loss: 0.8544, Train: 0.7518, Test: 0.7063\n",
            "Early stopping:  0.009852538579288316\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.85      0.81       107\n",
            "            CAD_and_CAM       0.71      0.82      0.76       106\n",
            "              Companies       0.44      0.53      0.48        88\n",
            "       Computer_Science       0.72      0.70      0.71        91\n",
            "            Consultants       0.56      0.66      0.61        89\n",
            "           Data_Formats       0.78      0.82      0.80        98\n",
            "    Data_Communications       0.78      0.85      0.81       110\n",
            "              Education       0.86      0.89      0.88        99\n",
            "               Graphics       0.85      0.90      0.87       108\n",
            "               Hardware       0.72      0.57      0.64       101\n",
            "               Internet       0.66      0.63      0.64       100\n",
            "       Mobile_Computing       0.89      0.79      0.84       101\n",
            "             Multimedia       0.72      0.73      0.72       107\n",
            "            Open_Source       0.67      0.69      0.68        97\n",
            "            Programming       0.57      0.52      0.54       123\n",
            "               Robotics       0.94      0.90      0.92        89\n",
            "               Security       0.86      0.75      0.80        88\n",
            "               Software       0.33      0.23      0.27        96\n",
            "                Systems       0.56      0.57      0.57       102\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.70      0.71      0.70      1900\n",
            "           weighted avg       0.70      0.71      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 7.7494, Train: 0.1388, Test: 0.1353\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 11.9659, Train: 0.1195, Test: 0.1095\n",
            "Early stopping:  2.9814626688640344\n",
            "Epoch: 003, Loss: 17.9357, Train: 0.2309, Test: 0.2189\n",
            "Early stopping:  5.118225748661317\n",
            "Epoch: 004, Loss: 17.1732, Train: 0.2597, Test: 0.2558\n",
            "Early stopping:  4.7756649067412935\n",
            "Epoch: 005, Loss: 11.3107, Train: 0.3236, Test: 0.3153\n",
            "Early stopping:  4.272326593571155\n",
            "Epoch: 006, Loss: 8.6427, Train: 0.3163, Test: 0.3179\n",
            "Early stopping:  3.99572653681996\n",
            "Epoch: 007, Loss: 7.2417, Train: 0.3538, Test: 0.3368\n",
            "Early stopping:  4.88161997871805\n",
            "Epoch: 008, Loss: 5.0120, Train: 0.3992, Test: 0.3858\n",
            "Early stopping:  4.675106756754058\n",
            "Epoch: 009, Loss: 3.6361, Train: 0.4183, Test: 0.4132\n",
            "Early stopping:  3.0192964159425983\n",
            "Epoch: 010, Loss: 2.9772, Train: 0.4354, Test: 0.4268\n",
            "Early stopping:  2.3972372077637742\n",
            "Epoch: 011, Loss: 2.5224, Train: 0.4432, Test: 0.4326\n",
            "Early stopping:  1.9045150243414872\n",
            "Epoch: 012, Loss: 2.2515, Train: 0.4697, Test: 0.4626\n",
            "Early stopping:  1.1011634730988287\n",
            "Epoch: 013, Loss: 2.1086, Train: 0.4821, Test: 0.4768\n",
            "Early stopping:  0.6195798867792538\n",
            "Epoch: 014, Loss: 2.0177, Train: 0.4682, Test: 0.4584\n",
            "Early stopping:  0.38673317478832364\n",
            "Epoch: 015, Loss: 1.9698, Train: 0.4709, Test: 0.4526\n",
            "Early stopping:  0.22236023701874827\n",
            "Epoch: 016, Loss: 1.9369, Train: 0.4891, Test: 0.4732\n",
            "Early stopping:  0.12656884703907895\n",
            "Epoch: 017, Loss: 1.8861, Train: 0.5096, Test: 0.4942\n",
            "Early stopping:  0.08467227534272968\n",
            "Epoch: 018, Loss: 1.8334, Train: 0.5153, Test: 0.5053\n",
            "Early stopping:  0.07169659720835996\n",
            "Epoch: 019, Loss: 1.7963, Train: 0.5083, Test: 0.5084\n",
            "Early stopping:  0.07142857745395266\n",
            "Epoch: 020, Loss: 1.7685, Train: 0.5142, Test: 0.5147\n",
            "Early stopping:  0.06796268063365887\n",
            "Epoch: 021, Loss: 1.7373, Train: 0.5286, Test: 0.5305\n",
            "Early stopping:  0.05781757501234689\n",
            "Epoch: 022, Loss: 1.6972, Train: 0.5420, Test: 0.5479\n",
            "Early stopping:  0.052508442750495606\n",
            "Epoch: 023, Loss: 1.6505, Train: 0.5554, Test: 0.5605\n",
            "Early stopping:  0.0577228894594429\n",
            "Epoch: 024, Loss: 1.6038, Train: 0.5661, Test: 0.5642\n",
            "Early stopping:  0.06601422193280596\n",
            "Epoch: 025, Loss: 1.5613, Train: 0.5692, Test: 0.5711\n",
            "Early stopping:  0.07046013646378897\n",
            "Epoch: 026, Loss: 1.5241, Train: 0.5763, Test: 0.5732\n",
            "Early stopping:  0.06893329395581009\n",
            "Epoch: 027, Loss: 1.4898, Train: 0.5883, Test: 0.5821\n",
            "Early stopping:  0.06356120596604911\n",
            "Epoch: 028, Loss: 1.4555, Train: 0.5958, Test: 0.5879\n",
            "Early stopping:  0.05826516365088492\n",
            "Epoch: 029, Loss: 1.4202, Train: 0.6042, Test: 0.5947\n",
            "Early stopping:  0.05544766959684334\n",
            "Epoch: 030, Loss: 1.3855, Train: 0.6171, Test: 0.6111\n",
            "Early stopping:  0.05484365358797087\n",
            "Epoch: 031, Loss: 1.3542, Train: 0.6297, Test: 0.6137\n",
            "Early stopping:  0.05396057906871852\n",
            "Epoch: 032, Loss: 1.3277, Train: 0.6355, Test: 0.6258\n",
            "Early stopping:  0.05093848995031329\n",
            "Epoch: 033, Loss: 1.3073, Train: 0.6382, Test: 0.6316\n",
            "Early stopping:  0.04506571442452865\n",
            "Epoch: 034, Loss: 1.2914, Train: 0.6396, Test: 0.6279\n",
            "Early stopping:  0.03747152033949998\n",
            "Epoch: 035, Loss: 1.2762, Train: 0.6428, Test: 0.6326\n",
            "Early stopping:  0.030639189036399946\n",
            "Epoch: 036, Loss: 1.2593, Train: 0.6466, Test: 0.6305\n",
            "Early stopping:  0.02658666739114848\n",
            "Epoch: 037, Loss: 1.2412, Train: 0.6517, Test: 0.6342\n",
            "Early stopping:  0.025993929927940165\n",
            "Epoch: 038, Loss: 1.2232, Train: 0.6542, Test: 0.6389\n",
            "Early stopping:  0.027101452474503403\n",
            "Epoch: 039, Loss: 1.2062, Train: 0.6601, Test: 0.6453\n",
            "Early stopping:  0.027846810499159772\n",
            "Epoch: 040, Loss: 1.1902, Train: 0.6647, Test: 0.6468\n",
            "Early stopping:  0.027404960183275397\n",
            "Epoch: 041, Loss: 1.1756, Train: 0.6684, Test: 0.6489\n",
            "Early stopping:  0.02599499319302067\n",
            "Epoch: 042, Loss: 1.1627, Train: 0.6691, Test: 0.6547\n",
            "Early stopping:  0.02401677486782636\n",
            "Epoch: 043, Loss: 1.1510, Train: 0.6746, Test: 0.6600\n",
            "Early stopping:  0.02182748997530666\n",
            "Epoch: 044, Loss: 1.1392, Train: 0.6772, Test: 0.6679\n",
            "Early stopping:  0.020032006148255597\n",
            "Epoch: 045, Loss: 1.1263, Train: 0.6818, Test: 0.6668\n",
            "Early stopping:  0.019290956546721652\n",
            "Epoch: 046, Loss: 1.1125, Train: 0.6866, Test: 0.6668\n",
            "Early stopping:  0.019775151096078386\n",
            "Epoch: 047, Loss: 1.0992, Train: 0.6912, Test: 0.6705\n",
            "Early stopping:  0.020619580820930485\n",
            "Epoch: 048, Loss: 1.0875, Train: 0.6925, Test: 0.6742\n",
            "Early stopping:  0.020651123301418176\n",
            "Epoch: 049, Loss: 1.0772, Train: 0.6979, Test: 0.6737\n",
            "Early stopping:  0.019535829661121353\n",
            "Epoch: 050, Loss: 1.0670, Train: 0.7003, Test: 0.6753\n",
            "Early stopping:  0.017898370724291467\n",
            "Epoch: 051, Loss: 1.0570, Train: 0.7017, Test: 0.6753\n",
            "Early stopping:  0.016593956437116763\n",
            "Epoch: 052, Loss: 1.0469, Train: 0.7033, Test: 0.6784\n",
            "Early stopping:  0.01604663137685665\n",
            "Epoch: 053, Loss: 1.0368, Train: 0.7066, Test: 0.6842\n",
            "Early stopping:  0.015967562545205132\n",
            "Epoch: 054, Loss: 1.0271, Train: 0.7092, Test: 0.6837\n",
            "Early stopping:  0.015819020621027805\n",
            "Epoch: 055, Loss: 1.0182, Train: 0.7122, Test: 0.6826\n",
            "Early stopping:  0.0153984030457735\n",
            "Epoch: 056, Loss: 1.0098, Train: 0.7143, Test: 0.6842\n",
            "Early stopping:  0.014669694860784248\n",
            "Epoch: 057, Loss: 1.0016, Train: 0.7166, Test: 0.6863\n",
            "Early stopping:  0.013869226985270587\n",
            "Epoch: 058, Loss: 0.9936, Train: 0.7187, Test: 0.6858\n",
            "Early stopping:  0.013243409612450163\n",
            "Epoch: 059, Loss: 0.9854, Train: 0.7187, Test: 0.6826\n",
            "Early stopping:  0.012931042385740832\n",
            "Epoch: 060, Loss: 0.9770, Train: 0.7217, Test: 0.6826\n",
            "Early stopping:  0.012942470980155883\n",
            "Epoch: 061, Loss: 0.9685, Train: 0.7233, Test: 0.6805\n",
            "Early stopping:  0.013094352981402362\n",
            "Epoch: 062, Loss: 0.9605, Train: 0.7253, Test: 0.6805\n",
            "Early stopping:  0.013114191215946838\n",
            "Epoch: 063, Loss: 0.9531, Train: 0.7280, Test: 0.6826\n",
            "Early stopping:  0.012835462946233679\n",
            "Epoch: 064, Loss: 0.9456, Train: 0.7311, Test: 0.6847\n",
            "Early stopping:  0.01236689325653176\n",
            "Epoch: 065, Loss: 0.9381, Train: 0.7338, Test: 0.6847\n",
            "Early stopping:  0.011967052172151759\n",
            "Epoch: 066, Loss: 0.9309, Train: 0.7353, Test: 0.6868\n",
            "Early stopping:  0.011747501308288924\n",
            "Epoch: 067, Loss: 0.9239, Train: 0.7353, Test: 0.6858\n",
            "Early stopping:  0.011539714019349522\n",
            "Epoch: 068, Loss: 0.9174, Train: 0.7355, Test: 0.6853\n",
            "Early stopping:  0.011169257267787426\n",
            "Epoch: 069, Loss: 0.9110, Train: 0.7378, Test: 0.6895\n",
            "Early stopping:  0.010704225688949983\n",
            "Epoch: 070, Loss: 0.9047, Train: 0.7384, Test: 0.6884\n",
            "Early stopping:  0.010307190111575184\n",
            "Epoch: 071, Loss: 0.8985, Train: 0.7411, Test: 0.6900\n",
            "Early stopping:  0.010039867710102\n",
            "Epoch: 072, Loss: 0.8918, Train: 0.7433, Test: 0.6905\n",
            "Early stopping:  0.010053793810764728\n",
            "Epoch: 073, Loss: 0.8856, Train: 0.7447, Test: 0.6916\n",
            "Early stopping:  0.010065779023615043\n",
            "Epoch: 074, Loss: 0.8800, Train: 0.7461, Test: 0.6932\n",
            "Early stopping:  0.009858528348148222\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.78      0.78        91\n",
            "            CAD_and_CAM       0.71      0.79      0.75       106\n",
            "              Companies       0.50      0.50      0.50       105\n",
            "       Computer_Science       0.75      0.70      0.72       101\n",
            "            Consultants       0.67      0.62      0.65       112\n",
            "           Data_Formats       0.83      0.82      0.83        96\n",
            "    Data_Communications       0.67      0.81      0.73        86\n",
            "              Education       0.87      0.88      0.87       124\n",
            "               Graphics       0.85      0.88      0.86        97\n",
            "               Hardware       0.67      0.70      0.69        89\n",
            "               Internet       0.64      0.64      0.64       103\n",
            "       Mobile_Computing       0.76      0.72      0.74        93\n",
            "             Multimedia       0.66      0.65      0.66       100\n",
            "            Open_Source       0.69      0.65      0.67       100\n",
            "            Programming       0.48      0.55      0.51        89\n",
            "               Robotics       0.92      0.86      0.89       100\n",
            "               Security       0.75      0.79      0.77       104\n",
            "               Software       0.31      0.28      0.30        95\n",
            "                Systems       0.59      0.52      0.56       109\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.69      0.69      0.69      1900\n",
            "           weighted avg       0.69      0.69      0.69      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 8.0573, Train: 0.0821, Test: 0.0695\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 13.9940, Train: 0.1555, Test: 0.1500\n",
            "Early stopping:  4.197887477496478\n",
            "Epoch: 003, Loss: 11.9797, Train: 0.2092, Test: 0.2068\n",
            "Early stopping:  3.0190350450761074\n",
            "Epoch: 004, Loss: 11.3626, Train: 0.2591, Test: 0.2574\n",
            "Early stopping:  2.465050043615335\n",
            "Epoch: 005, Loss: 10.5562, Train: 0.3132, Test: 0.3063\n",
            "Early stopping:  2.1639901487116564\n",
            "Epoch: 006, Loss: 10.3435, Train: 0.3367, Test: 0.3374\n",
            "Early stopping:  1.4653319791386685\n",
            "Epoch: 007, Loss: 9.3042, Train: 0.3451, Test: 0.3563\n",
            "Early stopping:  1.0213347284839374\n",
            "Epoch: 008, Loss: 7.8471, Train: 0.3482, Test: 0.3505\n",
            "Early stopping:  1.3541053099038363\n",
            "Epoch: 009, Loss: 6.4741, Train: 0.3838, Test: 0.3916\n",
            "Early stopping:  1.7308417196845474\n",
            "Epoch: 010, Loss: 4.7220, Train: 0.4207, Test: 0.4211\n",
            "Early stopping:  2.233077039872778\n",
            "Epoch: 011, Loss: 3.5813, Train: 0.4516, Test: 0.4332\n",
            "Early stopping:  2.307224925380533\n",
            "Epoch: 012, Loss: 2.7975, Train: 0.4622, Test: 0.4547\n",
            "Early stopping:  2.0727003933175454\n",
            "Epoch: 013, Loss: 2.2960, Train: 0.4811, Test: 0.4753\n",
            "Early stopping:  1.670646295094943\n",
            "Epoch: 014, Loss: 2.0485, Train: 0.4859, Test: 0.4784\n",
            "Early stopping:  1.0846274460614025\n",
            "Epoch: 015, Loss: 1.9568, Train: 0.4828, Test: 0.4779\n",
            "Early stopping:  0.6693133070733446\n",
            "Epoch: 016, Loss: 1.9338, Train: 0.4745, Test: 0.4653\n",
            "Early stopping:  0.360221242586436\n",
            "Epoch: 017, Loss: 1.9257, Train: 0.4697, Test: 0.4674\n",
            "Early stopping:  0.1553881963387043\n",
            "Epoch: 018, Loss: 1.9203, Train: 0.4670, Test: 0.4632\n",
            "Early stopping:  0.05299849849556903\n",
            "Epoch: 019, Loss: 1.9138, Train: 0.4661, Test: 0.4700\n",
            "Early stopping:  0.01661234644282628\n",
            "Epoch: 020, Loss: 1.9011, Train: 0.4691, Test: 0.4611\n",
            "Early stopping:  0.012357750250969215\n",
            "Epoch: 021, Loss: 1.8815, Train: 0.4762, Test: 0.4647\n",
            "Early stopping:  0.01765629913222398\n",
            "Epoch: 022, Loss: 1.8536, Train: 0.4845, Test: 0.4747\n",
            "Early stopping:  0.027052634388988465\n",
            "Epoch: 023, Loss: 1.8176, Train: 0.4942, Test: 0.4863\n",
            "Early stopping:  0.038644346231603725\n",
            "Epoch: 024, Loss: 1.7749, Train: 0.5063, Test: 0.4942\n",
            "Early stopping:  0.05053165684514199\n",
            "Epoch: 025, Loss: 1.7282, Train: 0.5167, Test: 0.5053\n",
            "Early stopping:  0.06120880345838588\n",
            "Epoch: 026, Loss: 1.6785, Train: 0.5328, Test: 0.5200\n",
            "Early stopping:  0.06962423282611604\n",
            "Epoch: 027, Loss: 1.6268, Train: 0.5503, Test: 0.5384\n",
            "Early stopping:  0.0756353044005876\n",
            "Epoch: 028, Loss: 1.5765, Train: 0.5647, Test: 0.5495\n",
            "Early stopping:  0.07878138328119301\n",
            "Epoch: 029, Loss: 1.5320, Train: 0.5728, Test: 0.5521\n",
            "Early stopping:  0.07818813422235592\n",
            "Epoch: 030, Loss: 1.4959, Train: 0.5791, Test: 0.5621\n",
            "Early stopping:  0.07290570015328238\n",
            "Epoch: 031, Loss: 1.4673, Train: 0.5891, Test: 0.5674\n",
            "Early stopping:  0.06357728607442284\n",
            "Epoch: 032, Loss: 1.4425, Train: 0.5970, Test: 0.5737\n",
            "Early stopping:  0.05300967146776455\n",
            "Epoch: 033, Loss: 1.4174, Train: 0.6024, Test: 0.5884\n",
            "Early stopping:  0.04483949907754147\n",
            "Epoch: 034, Loss: 1.3920, Train: 0.6118, Test: 0.5958\n",
            "Early stopping:  0.04076667052537771\n",
            "Epoch: 035, Loss: 1.3694, Train: 0.6167, Test: 0.5995\n",
            "Early stopping:  0.03894298064916616\n",
            "Epoch: 036, Loss: 1.3505, Train: 0.6257, Test: 0.6026\n",
            "Early stopping:  0.036755290036591115\n",
            "Epoch: 037, Loss: 1.3328, Train: 0.6311, Test: 0.6121\n",
            "Early stopping:  0.033435589696682366\n",
            "Epoch: 038, Loss: 1.3146, Train: 0.6379, Test: 0.6247\n",
            "Early stopping:  0.030283227464988578\n",
            "Epoch: 039, Loss: 1.2969, Train: 0.6430, Test: 0.6211\n",
            "Early stopping:  0.02859404027829825\n",
            "Epoch: 040, Loss: 1.2804, Train: 0.6497, Test: 0.6274\n",
            "Early stopping:  0.027827687708588074\n",
            "Epoch: 041, Loss: 1.2651, Train: 0.6539, Test: 0.6279\n",
            "Early stopping:  0.02683742491079529\n",
            "Epoch: 042, Loss: 1.2499, Train: 0.6566, Test: 0.6342\n",
            "Early stopping:  0.025508294902765127\n",
            "Epoch: 043, Loss: 1.2350, Train: 0.6592, Test: 0.6332\n",
            "Early stopping:  0.02439108607560296\n",
            "Epoch: 044, Loss: 1.2205, Train: 0.6618, Test: 0.6379\n",
            "Early stopping:  0.023697509584710084\n",
            "Epoch: 045, Loss: 1.2064, Train: 0.6654, Test: 0.6395\n",
            "Early stopping:  0.023215601393898187\n",
            "Epoch: 046, Loss: 1.1925, Train: 0.6696, Test: 0.6416\n",
            "Early stopping:  0.02268626589342992\n",
            "Epoch: 047, Loss: 1.1794, Train: 0.6738, Test: 0.6468\n",
            "Early stopping:  0.022018472988661015\n",
            "Epoch: 048, Loss: 1.1671, Train: 0.6761, Test: 0.6516\n",
            "Early stopping:  0.021171436983708804\n",
            "Epoch: 049, Loss: 1.1558, Train: 0.6809, Test: 0.6526\n",
            "Early stopping:  0.02002396791921363\n",
            "Epoch: 050, Loss: 1.1453, Train: 0.6839, Test: 0.6516\n",
            "Early stopping:  0.018670845239502687\n",
            "Epoch: 051, Loss: 1.1351, Train: 0.6859, Test: 0.6589\n",
            "Early stopping:  0.017447001618539293\n",
            "Epoch: 052, Loss: 1.1252, Train: 0.6870, Test: 0.6616\n",
            "Early stopping:  0.016531371315411665\n",
            "Epoch: 053, Loss: 1.1156, Train: 0.6887, Test: 0.6637\n",
            "Early stopping:  0.015899248750865434\n",
            "Epoch: 054, Loss: 1.1065, Train: 0.6896, Test: 0.6658\n",
            "Early stopping:  0.015364207495565933\n",
            "Epoch: 055, Loss: 1.0977, Train: 0.6909, Test: 0.6605\n",
            "Early stopping:  0.014784810679624474\n",
            "Epoch: 056, Loss: 1.0893, Train: 0.6924, Test: 0.6595\n",
            "Early stopping:  0.014176184046942252\n",
            "Epoch: 057, Loss: 1.0813, Train: 0.6943, Test: 0.6642\n",
            "Early stopping:  0.013582353735173856\n",
            "Epoch: 058, Loss: 1.0732, Train: 0.6954, Test: 0.6647\n",
            "Early stopping:  0.0131172025780034\n",
            "Epoch: 059, Loss: 1.0651, Train: 0.6997, Test: 0.6668\n",
            "Early stopping:  0.012865957614901384\n",
            "Epoch: 060, Loss: 1.0568, Train: 0.7020, Test: 0.6679\n",
            "Early stopping:  0.012835280397936301\n",
            "Epoch: 061, Loss: 1.0489, Train: 0.7036, Test: 0.6684\n",
            "Early stopping:  0.012838938340074931\n",
            "Epoch: 062, Loss: 1.0413, Train: 0.7045, Test: 0.6711\n",
            "Early stopping:  0.012673112381665177\n",
            "Epoch: 063, Loss: 1.0339, Train: 0.7084, Test: 0.6726\n",
            "Early stopping:  0.012309443402679227\n",
            "Epoch: 064, Loss: 1.0268, Train: 0.7103, Test: 0.6721\n",
            "Early stopping:  0.011872724018292515\n",
            "Epoch: 065, Loss: 1.0196, Train: 0.7109, Test: 0.6747\n",
            "Early stopping:  0.011569132784316509\n",
            "Epoch: 066, Loss: 1.0124, Train: 0.7130, Test: 0.6732\n",
            "Early stopping:  0.011386706277942001\n",
            "Epoch: 067, Loss: 1.0057, Train: 0.7146, Test: 0.6737\n",
            "Early stopping:  0.011201743368103739\n",
            "Epoch: 068, Loss: 0.9992, Train: 0.7157, Test: 0.6800\n",
            "Early stopping:  0.010910569368662635\n",
            "Epoch: 069, Loss: 0.9930, Train: 0.7174, Test: 0.6779\n",
            "Early stopping:  0.01049725160846406\n",
            "Epoch: 070, Loss: 0.9867, Train: 0.7188, Test: 0.6763\n",
            "Early stopping:  0.010157167443304508\n",
            "Epoch: 071, Loss: 0.9803, Train: 0.7209, Test: 0.6768\n",
            "Early stopping:  0.010003355041092486\n",
            "Epoch: 072, Loss: 0.9741, Train: 0.7226, Test: 0.6753\n",
            "Early stopping:  0.009950230457029944\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.88      0.76      0.82       105\n",
            "            CAD_and_CAM       0.76      0.71      0.73        95\n",
            "              Companies       0.53      0.56      0.55        86\n",
            "       Computer_Science       0.71      0.78      0.74        87\n",
            "            Consultants       0.52      0.55      0.53        91\n",
            "           Data_Formats       0.80      0.77      0.79       105\n",
            "    Data_Communications       0.67      0.77      0.72       114\n",
            "              Education       0.88      0.87      0.87       106\n",
            "               Graphics       0.81      0.89      0.85       113\n",
            "               Hardware       0.55      0.67      0.60        90\n",
            "               Internet       0.59      0.54      0.56       109\n",
            "       Mobile_Computing       0.68      0.72      0.70        85\n",
            "             Multimedia       0.65      0.67      0.66        91\n",
            "            Open_Source       0.67      0.61      0.64        96\n",
            "            Programming       0.46      0.57      0.51        86\n",
            "               Robotics       0.88      0.89      0.88       102\n",
            "               Security       0.81      0.79      0.80       117\n",
            "               Software       0.34      0.23      0.27       108\n",
            "                Systems       0.53      0.45      0.48       114\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.67      0.67      0.67      1900\n",
            "           weighted avg       0.67      0.68      0.67      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 5.3568, Train: 0.0536, Test: 0.0584\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 16.5997, Train: 0.1987, Test: 0.2095\n",
            "Early stopping:  7.949913222665061\n",
            "Epoch: 003, Loss: 11.4921, Train: 0.2237, Test: 0.2305\n",
            "Early stopping:  5.629260314395797\n",
            "Epoch: 004, Loss: 9.3644, Train: 0.3022, Test: 0.2963\n",
            "Early stopping:  4.682133502201359\n",
            "Epoch: 005, Loss: 7.9344, Train: 0.2934, Test: 0.2826\n",
            "Early stopping:  4.2397071049867785\n",
            "Epoch: 006, Loss: 7.0493, Train: 0.3182, Test: 0.3000\n",
            "Early stopping:  3.8068680424936083\n",
            "Epoch: 007, Loss: 6.0617, Train: 0.3587, Test: 0.3442\n",
            "Early stopping:  2.121207460650554\n",
            "Epoch: 008, Loss: 4.8809, Train: 0.3553, Test: 0.3447\n",
            "Early stopping:  1.7190042112772126\n",
            "Epoch: 009, Loss: 4.0635, Train: 0.4054, Test: 0.3937\n",
            "Early stopping:  1.569148533842885\n",
            "Epoch: 010, Loss: 2.9936, Train: 0.4679, Test: 0.4600\n",
            "Early stopping:  1.600240356362029\n",
            "Epoch: 011, Loss: 2.2853, Train: 0.5159, Test: 0.5089\n",
            "Early stopping:  1.4972729744325126\n",
            "Epoch: 012, Loss: 1.9593, Train: 0.5175, Test: 0.5158\n",
            "Early stopping:  1.2229995632551562\n",
            "Epoch: 013, Loss: 1.8828, Train: 0.4896, Test: 0.4842\n",
            "Early stopping:  0.9101885202741495\n",
            "Epoch: 014, Loss: 1.8676, Train: 0.4853, Test: 0.4768\n",
            "Early stopping:  0.47594230536516774\n",
            "Epoch: 015, Loss: 1.8347, Train: 0.5064, Test: 0.5079\n",
            "Early stopping:  0.18428614997614887\n",
            "Epoch: 016, Loss: 1.7637, Train: 0.5261, Test: 0.5253\n",
            "Early stopping:  0.07132640462577092\n",
            "Epoch: 017, Loss: 1.7013, Train: 0.5454, Test: 0.5363\n",
            "Early stopping:  0.07612803496859068\n",
            "Epoch: 018, Loss: 1.6449, Train: 0.5686, Test: 0.5595\n",
            "Early stopping:  0.09197852666586594\n",
            "Epoch: 019, Loss: 1.5890, Train: 0.5837, Test: 0.5721\n",
            "Early stopping:  0.09660773502258428\n",
            "Epoch: 020, Loss: 1.5413, Train: 0.5930, Test: 0.5784\n",
            "Early stopping:  0.08818407566271456\n",
            "Epoch: 021, Loss: 1.5019, Train: 0.6004, Test: 0.5947\n",
            "Early stopping:  0.0796468219539192\n",
            "Epoch: 022, Loss: 1.4632, Train: 0.6105, Test: 0.6011\n",
            "Early stopping:  0.07146866292036326\n",
            "Epoch: 023, Loss: 1.4276, Train: 0.6205, Test: 0.6074\n",
            "Early stopping:  0.06349706616797259\n",
            "Epoch: 024, Loss: 1.3969, Train: 0.6261, Test: 0.6121\n",
            "Early stopping:  0.05747530204877146\n",
            "Epoch: 025, Loss: 1.3660, Train: 0.6332, Test: 0.6184\n",
            "Early stopping:  0.053520361311254226\n",
            "Epoch: 026, Loss: 1.3336, Train: 0.6380, Test: 0.6242\n",
            "Early stopping:  0.050716633577372695\n",
            "Epoch: 027, Loss: 1.3067, Train: 0.6380, Test: 0.6258\n",
            "Early stopping:  0.048247322246969836\n",
            "Epoch: 028, Loss: 1.2869, Train: 0.6422, Test: 0.6211\n",
            "Early stopping:  0.04434204999878476\n",
            "Epoch: 029, Loss: 1.2666, Train: 0.6457, Test: 0.6247\n",
            "Early stopping:  0.0390683924637447\n",
            "Epoch: 030, Loss: 1.2417, Train: 0.6507, Test: 0.6242\n",
            "Early stopping:  0.03544870791787958\n",
            "Epoch: 031, Loss: 1.2183, Train: 0.6564, Test: 0.6237\n",
            "Early stopping:  0.035153022560697134\n",
            "Epoch: 032, Loss: 1.2004, Train: 0.6596, Test: 0.6316\n",
            "Early stopping:  0.03503177568535156\n",
            "Epoch: 033, Loss: 1.1857, Train: 0.6650, Test: 0.6321\n",
            "Early stopping:  0.032297283677037186\n",
            "Epoch: 034, Loss: 1.1710, Train: 0.6699, Test: 0.6316\n",
            "Early stopping:  0.02765863505311376\n",
            "Epoch: 035, Loss: 1.1552, Train: 0.6750, Test: 0.6400\n",
            "Early stopping:  0.0245989851802415\n",
            "Epoch: 036, Loss: 1.1393, Train: 0.6787, Test: 0.6395\n",
            "Early stopping:  0.02417177656752452\n",
            "Epoch: 037, Loss: 1.1243, Train: 0.6816, Test: 0.6447\n",
            "Early stopping:  0.024453557966355167\n",
            "Epoch: 038, Loss: 1.1112, Train: 0.6846, Test: 0.6437\n",
            "Early stopping:  0.023830113251908636\n",
            "Epoch: 039, Loss: 1.0992, Train: 0.6886, Test: 0.6484\n",
            "Early stopping:  0.02218164808032286\n",
            "Epoch: 040, Loss: 1.0873, Train: 0.6924, Test: 0.6532\n",
            "Early stopping:  0.020405823118409307\n",
            "Epoch: 041, Loss: 1.0750, Train: 0.6946, Test: 0.6516\n",
            "Early stopping:  0.019369690531899925\n",
            "Epoch: 042, Loss: 1.0623, Train: 0.6980, Test: 0.6505\n",
            "Early stopping:  0.01930440067567588\n",
            "Epoch: 043, Loss: 1.0500, Train: 0.7017, Test: 0.6553\n",
            "Early stopping:  0.019536262692795677\n",
            "Epoch: 044, Loss: 1.0380, Train: 0.7036, Test: 0.6579\n",
            "Early stopping:  0.0195442536590369\n",
            "Epoch: 045, Loss: 1.0261, Train: 0.7068, Test: 0.6611\n",
            "Early stopping:  0.01928681083031215\n",
            "Epoch: 046, Loss: 1.0149, Train: 0.7100, Test: 0.6642\n",
            "Early stopping:  0.018754569074838245\n",
            "Epoch: 047, Loss: 1.0045, Train: 0.7121, Test: 0.6658\n",
            "Early stopping:  0.01805428395609109\n",
            "Epoch: 048, Loss: 0.9944, Train: 0.7142, Test: 0.6658\n",
            "Early stopping:  0.017229917823997937\n",
            "Epoch: 049, Loss: 0.9844, Train: 0.7172, Test: 0.6653\n",
            "Early stopping:  0.016433593889651613\n",
            "Epoch: 050, Loss: 0.9746, Train: 0.7178, Test: 0.6668\n",
            "Early stopping:  0.01591991748342506\n",
            "Epoch: 051, Loss: 0.9649, Train: 0.7203, Test: 0.6711\n",
            "Early stopping:  0.01565195318573677\n",
            "Epoch: 052, Loss: 0.9554, Train: 0.7234, Test: 0.6721\n",
            "Early stopping:  0.015420242802701137\n",
            "Epoch: 053, Loss: 0.9466, Train: 0.7267, Test: 0.6700\n",
            "Early stopping:  0.015002183432801028\n",
            "Epoch: 054, Loss: 0.9381, Train: 0.7283, Test: 0.6726\n",
            "Early stopping:  0.014439188093385276\n",
            "Epoch: 055, Loss: 0.9293, Train: 0.7293, Test: 0.6742\n",
            "Early stopping:  0.013997821847357103\n",
            "Epoch: 056, Loss: 0.9204, Train: 0.7305, Test: 0.6711\n",
            "Early stopping:  0.01379657723485016\n",
            "Epoch: 057, Loss: 0.9123, Train: 0.7313, Test: 0.6679\n",
            "Early stopping:  0.013623959102101369\n",
            "Epoch: 058, Loss: 0.9043, Train: 0.7343, Test: 0.6716\n",
            "Early stopping:  0.013382185475245563\n",
            "Epoch: 059, Loss: 0.8960, Train: 0.7367, Test: 0.6742\n",
            "Early stopping:  0.013069259840747274\n",
            "Epoch: 060, Loss: 0.8880, Train: 0.7393, Test: 0.6747\n",
            "Early stopping:  0.012824125789209887\n",
            "Epoch: 061, Loss: 0.8802, Train: 0.7408, Test: 0.6779\n",
            "Early stopping:  0.012740357205620044\n",
            "Epoch: 062, Loss: 0.8722, Train: 0.7442, Test: 0.6789\n",
            "Early stopping:  0.012642657317212987\n",
            "Epoch: 063, Loss: 0.8644, Train: 0.7475, Test: 0.6768\n",
            "Early stopping:  0.01250041570865751\n",
            "Epoch: 064, Loss: 0.8567, Train: 0.7484, Test: 0.6789\n",
            "Early stopping:  0.012405875487390203\n",
            "Epoch: 065, Loss: 0.8491, Train: 0.7500, Test: 0.6837\n",
            "Early stopping:  0.012279456869011123\n",
            "Epoch: 066, Loss: 0.8416, Train: 0.7516, Test: 0.6842\n",
            "Early stopping:  0.012091298262625914\n",
            "Epoch: 067, Loss: 0.8342, Train: 0.7549, Test: 0.6842\n",
            "Early stopping:  0.011926170579609244\n",
            "Epoch: 068, Loss: 0.8270, Train: 0.7564, Test: 0.6853\n",
            "Early stopping:  0.01175391332206198\n",
            "Epoch: 069, Loss: 0.8198, Train: 0.7582, Test: 0.6858\n",
            "Early stopping:  0.011595504936313828\n",
            "Epoch: 070, Loss: 0.8127, Train: 0.7599, Test: 0.6868\n",
            "Early stopping:  0.011425714030628429\n",
            "Epoch: 071, Loss: 0.8059, Train: 0.7624, Test: 0.6874\n",
            "Early stopping:  0.011217566808131756\n",
            "Epoch: 072, Loss: 0.7991, Train: 0.7641, Test: 0.6900\n",
            "Early stopping:  0.011014625476914461\n",
            "Epoch: 073, Loss: 0.7923, Train: 0.7668, Test: 0.6900\n",
            "Early stopping:  0.010858666330468909\n",
            "Epoch: 074, Loss: 0.7855, Train: 0.7692, Test: 0.6900\n",
            "Early stopping:  0.010770157974807121\n",
            "Epoch: 075, Loss: 0.7787, Train: 0.7724, Test: 0.6889\n",
            "Early stopping:  0.01072613762743437\n",
            "Epoch: 076, Loss: 0.7721, Train: 0.7755, Test: 0.6889\n",
            "Early stopping:  0.010667448011533704\n",
            "Epoch: 077, Loss: 0.7655, Train: 0.7771, Test: 0.6895\n",
            "Early stopping:  0.010574891762670516\n",
            "Epoch: 078, Loss: 0.7590, Train: 0.7788, Test: 0.6895\n",
            "Early stopping:  0.010480146017307936\n",
            "Epoch: 079, Loss: 0.7525, Train: 0.7795, Test: 0.6905\n",
            "Early stopping:  0.010395313284162638\n",
            "Epoch: 080, Loss: 0.7461, Train: 0.7816, Test: 0.6900\n",
            "Early stopping:  0.010303941496146498\n",
            "Epoch: 081, Loss: 0.7398, Train: 0.7814, Test: 0.6900\n",
            "Early stopping:  0.010182248337289113\n",
            "Epoch: 082, Loss: 0.7335, Train: 0.7845, Test: 0.6900\n",
            "Early stopping:  0.01007159620818189\n",
            "Epoch: 083, Loss: 0.7273, Train: 0.7864, Test: 0.6884\n",
            "Early stopping:  0.009954356928943671\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 13, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.81      0.76      0.79       106\n",
            "            CAD_and_CAM       0.70      0.73      0.71        92\n",
            "              Companies       0.60      0.51      0.55        98\n",
            "       Computer_Science       0.75      0.83      0.79        99\n",
            "            Consultants       0.60      0.64      0.62       122\n",
            "           Data_Formats       0.82      0.75      0.78       114\n",
            "    Data_Communications       0.69      0.69      0.69       112\n",
            "              Education       0.81      0.87      0.84       103\n",
            "               Graphics       0.84      0.91      0.87       109\n",
            "               Hardware       0.63      0.60      0.61        97\n",
            "               Internet       0.68      0.70      0.69        99\n",
            "       Mobile_Computing       0.81      0.76      0.79       114\n",
            "             Multimedia       0.66      0.66      0.66        95\n",
            "            Open_Source       0.60      0.69      0.64        84\n",
            "            Programming       0.48      0.50      0.49        98\n",
            "               Robotics       0.91      0.91      0.91        91\n",
            "               Security       0.69      0.80      0.74        85\n",
            "               Software       0.36      0.26      0.31       102\n",
            "                Systems       0.47      0.46      0.47        80\n",
            "\n",
            "               accuracy                           0.69      1900\n",
            "              macro avg       0.68      0.69      0.68      1900\n",
            "           weighted avg       0.68      0.69      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.1396, Train: 0.1171, Test: 0.1100\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 12.1429, Train: 0.1738, Test: 0.1632\n",
            "Early stopping:  4.245015071976995\n",
            "Epoch: 003, Loss: 11.0395, Train: 0.2659, Test: 0.2705\n",
            "Early stopping:  3.1954872528135545\n",
            "Epoch: 004, Loss: 10.0546, Train: 0.2926, Test: 0.2774\n",
            "Early stopping:  2.6128729365607066\n",
            "Epoch: 005, Loss: 8.0370, Train: 0.3086, Test: 0.2921\n",
            "Early stopping:  2.402807263099867\n",
            "Epoch: 006, Loss: 6.4811, Train: 0.3434, Test: 0.3374\n",
            "Early stopping:  2.286038874424296\n",
            "Epoch: 007, Loss: 4.9339, Train: 0.3662, Test: 0.3626\n",
            "Early stopping:  2.5054733589124636\n",
            "Epoch: 008, Loss: 3.7900, Train: 0.3737, Test: 0.3611\n",
            "Early stopping:  2.4833138354701356\n",
            "Epoch: 009, Loss: 2.9894, Train: 0.4004, Test: 0.3937\n",
            "Early stopping:  2.038673924256314\n",
            "Epoch: 010, Loss: 2.4201, Train: 0.4314, Test: 0.4226\n",
            "Early stopping:  1.6212668803187686\n",
            "Epoch: 011, Loss: 2.0942, Train: 0.4626, Test: 0.4505\n",
            "Early stopping:  1.1423157481793333\n",
            "Epoch: 012, Loss: 1.9253, Train: 0.4787, Test: 0.4600\n",
            "Early stopping:  0.7585566286267789\n",
            "Epoch: 013, Loss: 1.8873, Train: 0.4887, Test: 0.4732\n",
            "Early stopping:  0.45716432729130074\n",
            "Epoch: 014, Loss: 1.8602, Train: 0.5030, Test: 0.4905\n",
            "Early stopping:  0.2324770329298993\n",
            "Epoch: 015, Loss: 1.8245, Train: 0.5178, Test: 0.5079\n",
            "Early stopping:  0.1050327747580049\n",
            "Epoch: 016, Loss: 1.7786, Train: 0.5284, Test: 0.5232\n",
            "Early stopping:  0.05651919638664663\n",
            "Epoch: 017, Loss: 1.7291, Train: 0.5345, Test: 0.5258\n",
            "Early stopping:  0.0633715071982792\n",
            "Epoch: 018, Loss: 1.6834, Train: 0.5400, Test: 0.5316\n",
            "Early stopping:  0.07111115547361405\n",
            "Epoch: 019, Loss: 1.6445, Train: 0.5499, Test: 0.5374\n",
            "Early stopping:  0.07203473435872866\n",
            "Epoch: 020, Loss: 1.6074, Train: 0.5592, Test: 0.5484\n",
            "Early stopping:  0.06764253576409457\n",
            "Epoch: 021, Loss: 1.5666, Train: 0.5672, Test: 0.5632\n",
            "Early stopping:  0.0634251444740861\n",
            "Epoch: 022, Loss: 1.5226, Train: 0.5766, Test: 0.5747\n",
            "Early stopping:  0.06319459845681896\n",
            "Epoch: 023, Loss: 1.4796, Train: 0.5879, Test: 0.5837\n",
            "Early stopping:  0.06558244162821106\n",
            "Epoch: 024, Loss: 1.4423, Train: 0.5961, Test: 0.5905\n",
            "Early stopping:  0.06597695432112875\n",
            "Epoch: 025, Loss: 1.4112, Train: 0.6043, Test: 0.6000\n",
            "Early stopping:  0.062002231012748585\n",
            "Epoch: 026, Loss: 1.3847, Train: 0.6111, Test: 0.6089\n",
            "Early stopping:  0.05469211402808363\n",
            "Epoch: 027, Loss: 1.3614, Train: 0.6153, Test: 0.6100\n",
            "Early stopping:  0.04670723588569767\n",
            "Epoch: 028, Loss: 1.3419, Train: 0.6191, Test: 0.6068\n",
            "Early stopping:  0.039776242052622805\n",
            "Epoch: 029, Loss: 1.3267, Train: 0.6221, Test: 0.6132\n",
            "Early stopping:  0.033664125306914\n",
            "Epoch: 030, Loss: 1.3135, Train: 0.6245, Test: 0.6179\n",
            "Early stopping:  0.028191883771539722\n",
            "Epoch: 031, Loss: 1.2988, Train: 0.6291, Test: 0.6195\n",
            "Early stopping:  0.024344479684558416\n",
            "Epoch: 032, Loss: 1.2810, Train: 0.6354, Test: 0.6216\n",
            "Early stopping:  0.0237099640059135\n",
            "Epoch: 033, Loss: 1.2614, Train: 0.6420, Test: 0.6263\n",
            "Early stopping:  0.025877849817406963\n",
            "Epoch: 034, Loss: 1.2430, Train: 0.6487, Test: 0.6300\n",
            "Early stopping:  0.028249804924286113\n",
            "Epoch: 035, Loss: 1.2271, Train: 0.6530, Test: 0.6337\n",
            "Early stopping:  0.028712927588884486\n",
            "Epoch: 036, Loss: 1.2116, Train: 0.6562, Test: 0.6374\n",
            "Early stopping:  0.02738830540310995\n",
            "Epoch: 037, Loss: 1.1954, Train: 0.6616, Test: 0.6368\n",
            "Early stopping:  0.025840139175645434\n",
            "Epoch: 038, Loss: 1.1789, Train: 0.6632, Test: 0.6374\n",
            "Early stopping:  0.02527670882521492\n",
            "Epoch: 039, Loss: 1.1634, Train: 0.6649, Test: 0.6426\n",
            "Early stopping:  0.025321037260276822\n",
            "Epoch: 040, Loss: 1.1493, Train: 0.6701, Test: 0.6437\n",
            "Early stopping:  0.02479363773512144\n",
            "Epoch: 041, Loss: 1.1367, Train: 0.6733, Test: 0.6468\n",
            "Early stopping:  0.023283878230999554\n",
            "Epoch: 042, Loss: 1.1243, Train: 0.6770, Test: 0.6500\n",
            "Early stopping:  0.021523021407756714\n",
            "Epoch: 043, Loss: 1.1105, Train: 0.6809, Test: 0.6542\n",
            "Early stopping:  0.020670590853219734\n",
            "Epoch: 044, Loss: 1.0971, Train: 0.6864, Test: 0.6584\n",
            "Early stopping:  0.020638463579761582\n",
            "Epoch: 045, Loss: 1.0856, Train: 0.6887, Test: 0.6574\n",
            "Early stopping:  0.020448030376665986\n",
            "Epoch: 046, Loss: 1.0754, Train: 0.6933, Test: 0.6579\n",
            "Early stopping:  0.019413207934155463\n",
            "Epoch: 047, Loss: 1.0654, Train: 0.6958, Test: 0.6611\n",
            "Early stopping:  0.017729578979632\n",
            "Epoch: 048, Loss: 1.0554, Train: 0.6986, Test: 0.6600\n",
            "Early stopping:  0.016389554139810136\n",
            "Epoch: 049, Loss: 1.0455, Train: 0.7009, Test: 0.6642\n",
            "Early stopping:  0.01584757147140275\n",
            "Epoch: 050, Loss: 1.0356, Train: 0.7030, Test: 0.6647\n",
            "Early stopping:  0.01573965022340292\n",
            "Epoch: 051, Loss: 1.0261, Train: 0.7067, Test: 0.6653\n",
            "Early stopping:  0.01555792180568037\n",
            "Epoch: 052, Loss: 1.0176, Train: 0.7082, Test: 0.6674\n",
            "Early stopping:  0.015050120458039749\n",
            "Epoch: 053, Loss: 1.0092, Train: 0.7124, Test: 0.6679\n",
            "Early stopping:  0.014344941220954128\n",
            "Epoch: 054, Loss: 1.0003, Train: 0.7150, Test: 0.6732\n",
            "Early stopping:  0.013821354133162395\n",
            "Epoch: 055, Loss: 0.9916, Train: 0.7171, Test: 0.6726\n",
            "Early stopping:  0.013632170649891586\n",
            "Epoch: 056, Loss: 0.9837, Train: 0.7209, Test: 0.6705\n",
            "Early stopping:  0.013487072831070887\n",
            "Epoch: 057, Loss: 0.9761, Train: 0.7221, Test: 0.6716\n",
            "Early stopping:  0.01312148038618392\n",
            "Epoch: 058, Loss: 0.9680, Train: 0.7229, Test: 0.6726\n",
            "Early stopping:  0.012690770064339513\n",
            "Epoch: 059, Loss: 0.9600, Train: 0.7241, Test: 0.6716\n",
            "Early stopping:  0.012469057332922475\n",
            "Epoch: 060, Loss: 0.9526, Train: 0.7272, Test: 0.6732\n",
            "Early stopping:  0.012387124491442176\n",
            "Epoch: 061, Loss: 0.9451, Train: 0.7291, Test: 0.6726\n",
            "Early stopping:  0.012216639670723511\n",
            "Epoch: 062, Loss: 0.9377, Train: 0.7296, Test: 0.6779\n",
            "Early stopping:  0.011937323724846549\n",
            "Epoch: 063, Loss: 0.9306, Train: 0.7337, Test: 0.6800\n",
            "Early stopping:  0.011672678744010003\n",
            "Epoch: 064, Loss: 0.9238, Train: 0.7349, Test: 0.6805\n",
            "Early stopping:  0.011401516421315532\n",
            "Epoch: 065, Loss: 0.9169, Train: 0.7349, Test: 0.6800\n",
            "Early stopping:  0.011140219298453186\n",
            "Epoch: 066, Loss: 0.9101, Train: 0.7363, Test: 0.6816\n",
            "Early stopping:  0.010906605486207295\n",
            "Epoch: 067, Loss: 0.9036, Train: 0.7382, Test: 0.6800\n",
            "Early stopping:  0.010689413588352716\n",
            "Epoch: 068, Loss: 0.8974, Train: 0.7395, Test: 0.6768\n",
            "Early stopping:  0.010433503300430962\n",
            "Epoch: 069, Loss: 0.8912, Train: 0.7392, Test: 0.6795\n",
            "Early stopping:  0.010103898717217489\n",
            "Epoch: 070, Loss: 0.8849, Train: 0.7417, Test: 0.6800\n",
            "Early stopping:  0.009901243436400408\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 17, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.82      0.80      0.81        94\n",
            "            CAD_and_CAM       0.70      0.70      0.70       105\n",
            "              Companies       0.53      0.50      0.51       102\n",
            "       Computer_Science       0.76      0.71      0.73       102\n",
            "            Consultants       0.60      0.59      0.59       120\n",
            "           Data_Formats       0.79      0.77      0.78        98\n",
            "    Data_Communications       0.64      0.79      0.71        92\n",
            "              Education       0.79      0.88      0.84        92\n",
            "               Graphics       0.87      0.94      0.90       111\n",
            "               Hardware       0.66      0.66      0.66       105\n",
            "               Internet       0.62      0.64      0.63       102\n",
            "       Mobile_Computing       0.84      0.71      0.77       112\n",
            "             Multimedia       0.69      0.73      0.71       106\n",
            "            Open_Source       0.61      0.70      0.65        96\n",
            "            Programming       0.47      0.50      0.48        88\n",
            "               Robotics       0.89      0.84      0.86       104\n",
            "               Security       0.80      0.74      0.77        89\n",
            "               Software       0.27      0.21      0.24        94\n",
            "                Systems       0.48      0.49      0.48        88\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.67      0.68      0.67      1900\n",
            "           weighted avg       0.68      0.68      0.68      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 6.3753, Train: 0.1279, Test: 0.1295\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 11.2331, Train: 0.2043, Test: 0.2026\n",
            "Early stopping:  3.434958375569009\n",
            "Epoch: 003, Loss: 12.9520, Train: 0.2605, Test: 0.2705\n",
            "Early stopping:  3.4109124671007236\n",
            "Epoch: 004, Loss: 13.1174, Train: 0.2778, Test: 0.2847\n",
            "Early stopping:  3.1469472751097376\n",
            "Epoch: 005, Loss: 14.2314, Train: 0.2721, Test: 0.2653\n",
            "Early stopping:  3.10181655005766\n",
            "Epoch: 006, Loss: 12.3199, Train: 0.3132, Test: 0.3089\n",
            "Early stopping:  1.1016143831136973\n",
            "Epoch: 007, Loss: 10.3874, Train: 0.3478, Test: 0.3426\n",
            "Early stopping:  1.4165973154276925\n",
            "Epoch: 008, Loss: 8.3855, Train: 0.4064, Test: 0.4037\n",
            "Early stopping:  2.318903359534125\n",
            "Epoch: 009, Loss: 6.6007, Train: 0.4604, Test: 0.4658\n",
            "Early stopping:  3.0355266569954527\n",
            "Epoch: 010, Loss: 4.9854, Train: 0.4749, Test: 0.4737\n",
            "Early stopping:  2.9206453812192303\n",
            "Epoch: 011, Loss: 4.0761, Train: 0.4764, Test: 0.4726\n",
            "Early stopping:  2.554285006130948\n",
            "Epoch: 012, Loss: 3.4808, Train: 0.4838, Test: 0.4689\n",
            "Early stopping:  1.9941791749292348\n",
            "Epoch: 013, Loss: 2.9492, Train: 0.4933, Test: 0.4874\n",
            "Early stopping:  1.435158066786176\n",
            "Epoch: 014, Loss: 2.4819, Train: 0.4917, Test: 0.4837\n",
            "Early stopping:  0.9790025108020783\n",
            "Epoch: 015, Loss: 2.2035, Train: 0.4779, Test: 0.4784\n",
            "Early stopping:  0.7561972132122291\n",
            "Epoch: 016, Loss: 2.1196, Train: 0.4705, Test: 0.4726\n",
            "Early stopping:  0.5675802120543668\n",
            "Epoch: 017, Loss: 2.0484, Train: 0.4626, Test: 0.4726\n",
            "Early stopping:  0.36789844500980295\n",
            "Epoch: 018, Loss: 2.0113, Train: 0.4711, Test: 0.4832\n",
            "Early stopping:  0.18767424113751213\n",
            "Epoch: 019, Loss: 1.9599, Train: 0.4833, Test: 0.4863\n",
            "Early stopping:  0.09526785728939496\n",
            "Epoch: 020, Loss: 1.8838, Train: 0.5042, Test: 0.5111\n",
            "Early stopping:  0.08912309503687728\n",
            "Epoch: 021, Loss: 1.8042, Train: 0.5263, Test: 0.5321\n",
            "Early stopping:  0.09848049558667767\n",
            "Epoch: 022, Loss: 1.7295, Train: 0.5430, Test: 0.5542\n",
            "Early stopping:  0.11401301482892542\n",
            "Epoch: 023, Loss: 1.6678, Train: 0.5508, Test: 0.5600\n",
            "Early stopping:  0.116890378128573\n",
            "Epoch: 024, Loss: 1.6208, Train: 0.5561, Test: 0.5663\n",
            "Early stopping:  0.1052736373340699\n",
            "Epoch: 025, Loss: 1.5847, Train: 0.5617, Test: 0.5732\n",
            "Early stopping:  0.0874775385308153\n",
            "Epoch: 026, Loss: 1.5531, Train: 0.5676, Test: 0.5816\n",
            "Early stopping:  0.06958243448081823\n",
            "Epoch: 027, Loss: 1.5217, Train: 0.5814, Test: 0.5905\n",
            "Early stopping:  0.057115033006203825\n",
            "Epoch: 028, Loss: 1.4873, Train: 0.5971, Test: 0.6026\n",
            "Early stopping:  0.052209895281442764\n",
            "Epoch: 029, Loss: 1.4512, Train: 0.5992, Test: 0.6011\n",
            "Early stopping:  0.05265987660451284\n",
            "Epoch: 030, Loss: 1.4197, Train: 0.5997, Test: 0.6021\n",
            "Early stopping:  0.05334770962920019\n",
            "Epoch: 031, Loss: 1.3942, Train: 0.6007, Test: 0.6095\n",
            "Early stopping:  0.05112581906717573\n",
            "Epoch: 032, Loss: 1.3673, Train: 0.6117, Test: 0.6079\n",
            "Early stopping:  0.047068249018526916\n",
            "Epoch: 033, Loss: 1.3372, Train: 0.6246, Test: 0.6153\n",
            "Early stopping:  0.04436422284078959\n",
            "Epoch: 034, Loss: 1.3113, Train: 0.6304, Test: 0.6232\n",
            "Early stopping:  0.04332931812119079\n",
            "Epoch: 035, Loss: 1.2960, Train: 0.6324, Test: 0.6274\n",
            "Early stopping:  0.04013948125284451\n",
            "Epoch: 036, Loss: 1.2812, Train: 0.6366, Test: 0.6316\n",
            "Early stopping:  0.03422878650520206\n",
            "Epoch: 037, Loss: 1.2583, Train: 0.6459, Test: 0.6368\n",
            "Early stopping:  0.029871061332105302\n",
            "Epoch: 038, Loss: 1.2328, Train: 0.6538, Test: 0.6442\n",
            "Early stopping:  0.03100937552957437\n",
            "Epoch: 039, Loss: 1.2123, Train: 0.6596, Test: 0.6479\n",
            "Early stopping:  0.03422073837145551\n",
            "Epoch: 040, Loss: 1.1974, Train: 0.6637, Test: 0.6489\n",
            "Early stopping:  0.033889882442558264\n",
            "Epoch: 041, Loss: 1.1853, Train: 0.6654, Test: 0.6537\n",
            "Early stopping:  0.028999935584888923\n",
            "Epoch: 042, Loss: 1.1733, Train: 0.6688, Test: 0.6600\n",
            "Early stopping:  0.023250357927008806\n",
            "Epoch: 043, Loss: 1.1607, Train: 0.6724, Test: 0.6563\n",
            "Early stopping:  0.020151140791823573\n",
            "Epoch: 044, Loss: 1.1475, Train: 0.6771, Test: 0.6595\n",
            "Early stopping:  0.019677920272405204\n",
            "Epoch: 045, Loss: 1.1338, Train: 0.6812, Test: 0.6653\n",
            "Early stopping:  0.020379106007449087\n",
            "Epoch: 046, Loss: 1.1197, Train: 0.6824, Test: 0.6616\n",
            "Early stopping:  0.021214508913573054\n",
            "Epoch: 047, Loss: 1.1062, Train: 0.6837, Test: 0.6584\n",
            "Early stopping:  0.021638856837172926\n",
            "Epoch: 048, Loss: 1.0933, Train: 0.6861, Test: 0.6563\n",
            "Early stopping:  0.021522901915007792\n",
            "Epoch: 049, Loss: 1.0813, Train: 0.6892, Test: 0.6553\n",
            "Early stopping:  0.02077149969225494\n",
            "Epoch: 050, Loss: 1.0701, Train: 0.6945, Test: 0.6563\n",
            "Early stopping:  0.019630980840174016\n",
            "Epoch: 051, Loss: 1.0580, Train: 0.6983, Test: 0.6542\n",
            "Early stopping:  0.01888658639837806\n",
            "Epoch: 052, Loss: 1.0454, Train: 0.6988, Test: 0.6600\n",
            "Early stopping:  0.018829306711763152\n",
            "Epoch: 053, Loss: 1.0337, Train: 0.7018, Test: 0.6663\n",
            "Early stopping:  0.018955882329409476\n",
            "Epoch: 054, Loss: 1.0231, Train: 0.7058, Test: 0.6637\n",
            "Early stopping:  0.018705457445782784\n",
            "Epoch: 055, Loss: 1.0126, Train: 0.7088, Test: 0.6684\n",
            "Early stopping:  0.017908050725213225\n",
            "Epoch: 056, Loss: 1.0024, Train: 0.7114, Test: 0.6689\n",
            "Early stopping:  0.016932798532849083\n",
            "Epoch: 057, Loss: 0.9928, Train: 0.7143, Test: 0.6737\n",
            "Early stopping:  0.01620530276148719\n",
            "Epoch: 058, Loss: 0.9828, Train: 0.7166, Test: 0.6747\n",
            "Early stopping:  0.01588293352411218\n",
            "Epoch: 059, Loss: 0.9723, Train: 0.7201, Test: 0.6763\n",
            "Early stopping:  0.01585848479217436\n",
            "Epoch: 060, Loss: 0.9622, Train: 0.7233, Test: 0.6800\n",
            "Early stopping:  0.01598145996929016\n",
            "Epoch: 061, Loss: 0.9525, Train: 0.7261, Test: 0.6800\n",
            "Early stopping:  0.016025011608794583\n",
            "Epoch: 062, Loss: 0.9432, Train: 0.7270, Test: 0.6774\n",
            "Early stopping:  0.01566059687515869\n",
            "Epoch: 063, Loss: 0.9339, Train: 0.7297, Test: 0.6811\n",
            "Early stopping:  0.015136567852282734\n",
            "Epoch: 064, Loss: 0.9247, Train: 0.7314, Test: 0.6805\n",
            "Early stopping:  0.01478142417760752\n",
            "Epoch: 065, Loss: 0.9157, Train: 0.7359, Test: 0.6800\n",
            "Early stopping:  0.014532480658054407\n",
            "Epoch: 066, Loss: 0.9069, Train: 0.7371, Test: 0.6774\n",
            "Early stopping:  0.014351264726812932\n",
            "Epoch: 067, Loss: 0.8982, Train: 0.7393, Test: 0.6784\n",
            "Early stopping:  0.01412774627584642\n",
            "Epoch: 068, Loss: 0.8894, Train: 0.7425, Test: 0.6768\n",
            "Early stopping:  0.01395852582938559\n",
            "Epoch: 069, Loss: 0.8804, Train: 0.7454, Test: 0.6768\n",
            "Early stopping:  0.01394444520549371\n",
            "Epoch: 070, Loss: 0.8718, Train: 0.7489, Test: 0.6789\n",
            "Early stopping:  0.013913149811276584\n",
            "Epoch: 071, Loss: 0.8632, Train: 0.7495, Test: 0.6795\n",
            "Early stopping:  0.013834836261516012\n",
            "Epoch: 072, Loss: 0.8549, Train: 0.7518, Test: 0.6800\n",
            "Early stopping:  0.01361985125492737\n",
            "Epoch: 073, Loss: 0.8467, Train: 0.7536, Test: 0.6805\n",
            "Early stopping:  0.013333800065614368\n",
            "Epoch: 074, Loss: 0.8386, Train: 0.7564, Test: 0.6816\n",
            "Early stopping:  0.013091244670876473\n",
            "Epoch: 075, Loss: 0.8304, Train: 0.7584, Test: 0.6805\n",
            "Early stopping:  0.012939003657816526\n",
            "Epoch: 076, Loss: 0.8223, Train: 0.7620, Test: 0.6811\n",
            "Early stopping:  0.012869408171079314\n",
            "Epoch: 077, Loss: 0.8145, Train: 0.7633, Test: 0.6811\n",
            "Early stopping:  0.012758762467546607\n",
            "Epoch: 078, Loss: 0.8068, Train: 0.7651, Test: 0.6795\n",
            "Early stopping:  0.012589029426022634\n",
            "Epoch: 079, Loss: 0.7991, Train: 0.7674, Test: 0.6811\n",
            "Early stopping:  0.01235040445845912\n",
            "Epoch: 080, Loss: 0.7915, Train: 0.7691, Test: 0.6821\n",
            "Early stopping:  0.012164748072661116\n",
            "Epoch: 081, Loss: 0.7839, Train: 0.7724, Test: 0.6826\n",
            "Early stopping:  0.012093390316259257\n",
            "Epoch: 082, Loss: 0.7764, Train: 0.7758, Test: 0.6811\n",
            "Early stopping:  0.012024620978799206\n",
            "Epoch: 083, Loss: 0.7691, Train: 0.7768, Test: 0.6800\n",
            "Early stopping:  0.0119001265083275\n",
            "Epoch: 084, Loss: 0.7619, Train: 0.7799, Test: 0.6826\n",
            "Early stopping:  0.011722856283797066\n",
            "Epoch: 085, Loss: 0.7548, Train: 0.7818, Test: 0.6816\n",
            "Early stopping:  0.011484710914943734\n",
            "Epoch: 086, Loss: 0.7476, Train: 0.7837, Test: 0.6821\n",
            "Early stopping:  0.011373047229828178\n",
            "Epoch: 087, Loss: 0.7407, Train: 0.7858, Test: 0.6811\n",
            "Early stopping:  0.01124308489909527\n",
            "Epoch: 088, Loss: 0.7342, Train: 0.7883, Test: 0.6826\n",
            "Early stopping:  0.010985834356150694\n",
            "Epoch: 089, Loss: 0.7273, Train: 0.7916, Test: 0.6779\n",
            "Early stopping:  0.010832543335341492\n",
            "Epoch: 090, Loss: 0.7203, Train: 0.7920, Test: 0.6763\n",
            "Early stopping:  0.010744657306071855\n",
            "Epoch: 091, Loss: 0.7134, Train: 0.7951, Test: 0.6789\n",
            "Early stopping:  0.01083034634551382\n",
            "Epoch: 092, Loss: 0.7070, Train: 0.7958, Test: 0.6800\n",
            "Early stopping:  0.010806845385418718\n",
            "Epoch: 093, Loss: 0.7008, Train: 0.7982, Test: 0.6826\n",
            "Early stopping:  0.010487936156677868\n",
            "Epoch: 094, Loss: 0.6945, Train: 0.7987, Test: 0.6816\n",
            "Early stopping:  0.010157247284480954\n",
            "Epoch: 095, Loss: 0.6883, Train: 0.8026, Test: 0.6842\n",
            "Early stopping:  0.009904465191378768\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.84      0.81        85\n",
            "            CAD_and_CAM       0.71      0.70      0.70       100\n",
            "              Companies       0.50      0.49      0.49       104\n",
            "       Computer_Science       0.73      0.73      0.73        89\n",
            "            Consultants       0.61      0.59      0.60       111\n",
            "           Data_Formats       0.78      0.78      0.78       103\n",
            "    Data_Communications       0.79      0.75      0.77       112\n",
            "              Education       0.90      0.84      0.87       115\n",
            "               Graphics       0.85      0.92      0.88       103\n",
            "               Hardware       0.65      0.67      0.66       105\n",
            "               Internet       0.57      0.66      0.61        77\n",
            "       Mobile_Computing       0.78      0.71      0.75       101\n",
            "             Multimedia       0.72      0.76      0.74        92\n",
            "            Open_Source       0.61      0.64      0.62       113\n",
            "            Programming       0.49      0.45      0.47       106\n",
            "               Robotics       0.91      0.87      0.89        93\n",
            "               Security       0.76      0.77      0.76       107\n",
            "               Software       0.24      0.24      0.24        92\n",
            "                Systems       0.57      0.59      0.58        92\n",
            "\n",
            "               accuracy                           0.68      1900\n",
            "              macro avg       0.68      0.68      0.68      1900\n",
            "           weighted avg       0.69      0.68      0.68      1900\n",
            "\n",
            "time: 44.5 s (started: 2024-08-17 14:13:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7a2eb3-ed8b-4a02-cf81-3e5a77dfc0da",
        "id": "udpNg_J-tx3_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 396 ms (started: 2024-08-17 14:17:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Network - GAT"
      ],
      "metadata": {
        "id": "xT09VPN8tx3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT','80%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899399b2-2444-456e-c562-06c4ea1436d1",
        "id": "SIF3GJ_-tx3_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9516, Train: 0.3708, Test: 0.3642\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8218, Train: 0.5242, Test: 0.5100\n",
            "Early stopping:  0.09182567602547291\n",
            "Epoch: 003, Loss: 2.6574, Train: 0.5462, Test: 0.5168\n",
            "Early stopping:  0.1474472939959434\n",
            "Epoch: 004, Loss: 2.4685, Train: 0.5671, Test: 0.5389\n",
            "Early stopping:  0.20904086312435768\n",
            "Epoch: 005, Loss: 2.2699, Train: 0.5899, Test: 0.5705\n",
            "Early stopping:  0.27232267987651354\n",
            "Epoch: 006, Loss: 2.0711, Train: 0.6104, Test: 0.5905\n",
            "Early stopping:  0.29885855196152794\n",
            "Epoch: 007, Loss: 1.8796, Train: 0.6309, Test: 0.6184\n",
            "Early stopping:  0.3088029222987719\n",
            "Epoch: 008, Loss: 1.7019, Train: 0.6404, Test: 0.6263\n",
            "Early stopping:  0.30419721714024717\n",
            "Epoch: 009, Loss: 1.5475, Train: 0.6449, Test: 0.6311\n",
            "Early stopping:  0.2871609024585015\n",
            "Epoch: 010, Loss: 1.4215, Train: 0.6512, Test: 0.6321\n",
            "Early stopping:  0.25878232986934796\n",
            "Epoch: 011, Loss: 1.3216, Train: 0.6562, Test: 0.6411\n",
            "Early stopping:  0.22216628590195142\n",
            "Epoch: 012, Loss: 1.2469, Train: 0.6624, Test: 0.6463\n",
            "Early stopping:  0.1813135164486129\n",
            "Epoch: 013, Loss: 1.1921, Train: 0.6670, Test: 0.6458\n",
            "Early stopping:  0.14178203801841696\n",
            "Epoch: 014, Loss: 1.1515, Train: 0.6709, Test: 0.6521\n",
            "Early stopping:  0.107460981114981\n",
            "Epoch: 015, Loss: 1.1214, Train: 0.6784, Test: 0.6563\n",
            "Early stopping:  0.07962488725270865\n",
            "Epoch: 016, Loss: 1.0954, Train: 0.6839, Test: 0.6616\n",
            "Early stopping:  0.0598101394881908\n",
            "Epoch: 017, Loss: 1.0721, Train: 0.6897, Test: 0.6658\n",
            "Early stopping:  0.047121103113732576\n",
            "Epoch: 018, Loss: 1.0496, Train: 0.6947, Test: 0.6726\n",
            "Early stopping:  0.04011127827823064\n",
            "Epoch: 019, Loss: 1.0275, Train: 0.7012, Test: 0.6753\n",
            "Early stopping:  0.03695207907389025\n",
            "Epoch: 020, Loss: 1.0060, Train: 0.7064, Test: 0.6816\n",
            "Early stopping:  0.0353250541711163\n",
            "Epoch: 021, Loss: 0.9847, Train: 0.7134, Test: 0.6805\n",
            "Early stopping:  0.03453561015337304\n",
            "Epoch: 022, Loss: 0.9642, Train: 0.7188, Test: 0.6800\n",
            "Early stopping:  0.033766452486883224\n",
            "Epoch: 023, Loss: 0.9447, Train: 0.7214, Test: 0.6816\n",
            "Early stopping:  0.03277652852062915\n",
            "Epoch: 024, Loss: 0.9267, Train: 0.7276, Test: 0.6805\n",
            "Early stopping:  0.031402368950015365\n",
            "Epoch: 025, Loss: 0.9097, Train: 0.7307, Test: 0.6832\n",
            "Early stopping:  0.02968016006460772\n",
            "Epoch: 026, Loss: 0.8943, Train: 0.7368, Test: 0.6847\n",
            "Early stopping:  0.02766837743899011\n",
            "Epoch: 027, Loss: 0.8800, Train: 0.7411, Test: 0.6900\n",
            "Early stopping:  0.025601973397657574\n",
            "Epoch: 028, Loss: 0.8668, Train: 0.7453, Test: 0.6905\n",
            "Early stopping:  0.023662767038170607\n",
            "Epoch: 029, Loss: 0.8543, Train: 0.7508, Test: 0.6911\n",
            "Early stopping:  0.021891941161106376\n",
            "Epoch: 030, Loss: 0.8425, Train: 0.7555, Test: 0.6947\n",
            "Early stopping:  0.020488822878023012\n",
            "Epoch: 031, Loss: 0.8313, Train: 0.7593, Test: 0.6974\n",
            "Early stopping:  0.019269841601609584\n",
            "Epoch: 032, Loss: 0.8207, Train: 0.7642, Test: 0.7021\n",
            "Early stopping:  0.018213324653789997\n",
            "Epoch: 033, Loss: 0.8106, Train: 0.7687, Test: 0.7011\n",
            "Early stopping:  0.017254925811888006\n",
            "Epoch: 034, Loss: 0.8013, Train: 0.7742, Test: 0.7037\n",
            "Early stopping:  0.0162840354238815\n",
            "Epoch: 035, Loss: 0.7927, Train: 0.7774, Test: 0.7063\n",
            "Early stopping:  0.015283274711368372\n",
            "Epoch: 036, Loss: 0.7845, Train: 0.7784, Test: 0.7079\n",
            "Early stopping:  0.014299750159083517\n",
            "Epoch: 037, Loss: 0.7766, Train: 0.7812, Test: 0.7121\n",
            "Early stopping:  0.013422193997884387\n",
            "Epoch: 038, Loss: 0.7688, Train: 0.7855, Test: 0.7153\n",
            "Early stopping:  0.012842288300946208\n",
            "Epoch: 039, Loss: 0.7609, Train: 0.7875, Test: 0.7184\n",
            "Early stopping:  0.012540174040411458\n",
            "Epoch: 040, Loss: 0.7529, Train: 0.7911, Test: 0.7211\n",
            "Early stopping:  0.01246915696772053\n",
            "Epoch: 041, Loss: 0.7450, Train: 0.7938, Test: 0.7263\n",
            "Early stopping:  0.0125087416157174\n",
            "Epoch: 042, Loss: 0.7370, Train: 0.7972, Test: 0.7253\n",
            "Early stopping:  0.012557362989496241\n",
            "Epoch: 043, Loss: 0.7290, Train: 0.7992, Test: 0.7279\n",
            "Early stopping:  0.012609435898871068\n",
            "Epoch: 044, Loss: 0.7210, Train: 0.8009, Test: 0.7274\n",
            "Early stopping:  0.012623270065262347\n",
            "Epoch: 045, Loss: 0.7130, Train: 0.8029, Test: 0.7284\n",
            "Early stopping:  0.012644651661993018\n",
            "Epoch: 046, Loss: 0.7049, Train: 0.8057, Test: 0.7268\n",
            "Early stopping:  0.012672082004577135\n",
            "Epoch: 047, Loss: 0.6969, Train: 0.8061, Test: 0.7274\n",
            "Early stopping:  0.012704561046410018\n",
            "Epoch: 048, Loss: 0.6888, Train: 0.8087, Test: 0.7279\n",
            "Early stopping:  0.012738999949210772\n",
            "Epoch: 049, Loss: 0.6808, Train: 0.8093, Test: 0.7242\n",
            "Early stopping:  0.012726131701791741\n",
            "Epoch: 050, Loss: 0.6729, Train: 0.8116, Test: 0.7216\n",
            "Early stopping:  0.012677999581430392\n",
            "Epoch: 051, Loss: 0.6649, Train: 0.8125, Test: 0.7205\n",
            "Early stopping:  0.012614547275915895\n",
            "Epoch: 052, Loss: 0.6573, Train: 0.8150, Test: 0.7195\n",
            "Early stopping:  0.012485978768550334\n",
            "Epoch: 053, Loss: 0.6499, Train: 0.8164, Test: 0.7216\n",
            "Early stopping:  0.012250148067519045\n",
            "Epoch: 054, Loss: 0.6427, Train: 0.8191, Test: 0.7232\n",
            "Early stopping:  0.01192858154572874\n",
            "Epoch: 055, Loss: 0.6348, Train: 0.8238, Test: 0.7258\n",
            "Early stopping:  0.011853593646126703\n",
            "Epoch: 056, Loss: 0.6267, Train: 0.8234, Test: 0.7274\n",
            "Early stopping:  0.012069726960726277\n",
            "Epoch: 057, Loss: 0.6197, Train: 0.8274, Test: 0.7232\n",
            "Early stopping:  0.012087991241407412\n",
            "Epoch: 058, Loss: 0.6128, Train: 0.8299, Test: 0.7226\n",
            "Early stopping:  0.011845588908391231\n",
            "Epoch: 059, Loss: 0.6049, Train: 0.8339, Test: 0.7242\n",
            "Early stopping:  0.011641239558616274\n",
            "Epoch: 060, Loss: 0.5971, Train: 0.8349, Test: 0.7226\n",
            "Early stopping:  0.011678198210566234\n",
            "Epoch: 061, Loss: 0.5906, Train: 0.8376, Test: 0.7279\n",
            "Early stopping:  0.0116815137308487\n",
            "Epoch: 062, Loss: 0.5842, Train: 0.8414, Test: 0.7242\n",
            "Early stopping:  0.011330262477110435\n",
            "Epoch: 063, Loss: 0.5768, Train: 0.8414, Test: 0.7242\n",
            "Early stopping:  0.010958339002251597\n",
            "Epoch: 064, Loss: 0.5697, Train: 0.8455, Test: 0.7284\n",
            "Early stopping:  0.010877873050434736\n",
            "Epoch: 065, Loss: 0.5631, Train: 0.8470, Test: 0.7258\n",
            "Early stopping:  0.010976641565610219\n",
            "Epoch: 066, Loss: 0.5571, Train: 0.8484, Test: 0.7289\n",
            "Early stopping:  0.010738011761412907\n",
            "Epoch: 067, Loss: 0.5508, Train: 0.8518, Test: 0.7221\n",
            "Early stopping:  0.010222878966566144\n",
            "Epoch: 068, Loss: 0.5435, Train: 0.8533, Test: 0.7247\n",
            "Early stopping:  0.01021699926934684\n",
            "Epoch: 069, Loss: 0.5370, Train: 0.8546, Test: 0.7232\n",
            "Early stopping:  0.010386546661363082\n",
            "Epoch: 070, Loss: 0.5311, Train: 0.8582, Test: 0.7237\n",
            "Early stopping:  0.010392165466425105\n",
            "Epoch: 071, Loss: 0.5238, Train: 0.8589, Test: 0.7226\n",
            "Early stopping:  0.010507558397059675\n",
            "Epoch: 072, Loss: 0.5159, Train: 0.8629, Test: 0.7232\n",
            "Early stopping:  0.010870515153598683\n",
            "Epoch: 073, Loss: 0.5095, Train: 0.8621, Test: 0.7237\n",
            "Early stopping:  0.011123233589638865\n",
            "Epoch: 074, Loss: 0.5038, Train: 0.8658, Test: 0.7205\n",
            "Early stopping:  0.010908456770365883\n",
            "Epoch: 075, Loss: 0.4968, Train: 0.8688, Test: 0.7232\n",
            "Early stopping:  0.01046106432249451\n",
            "Epoch: 076, Loss: 0.4898, Train: 0.8687, Test: 0.7200\n",
            "Early stopping:  0.010250813318318017\n",
            "Epoch: 077, Loss: 0.4838, Train: 0.8741, Test: 0.7211\n",
            "Early stopping:  0.010352729942781047\n",
            "Epoch: 078, Loss: 0.4783, Train: 0.8718, Test: 0.7179\n",
            "Early stopping:  0.010132834472605153\n",
            "Epoch: 079, Loss: 0.4729, Train: 0.8761, Test: 0.7189\n",
            "Early stopping:  0.009381889600343977\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.86      0.80      0.83        96\n",
            "            CAD_and_CAM       0.78      0.83      0.80        92\n",
            "              Companies       0.50      0.61      0.55        85\n",
            "       Computer_Science       0.74      0.76      0.75        88\n",
            "            Consultants       0.63      0.65      0.64        96\n",
            "           Data_Formats       0.78      0.83      0.81       103\n",
            "    Data_Communications       0.75      0.68      0.71       110\n",
            "              Education       0.81      0.80      0.81        97\n",
            "               Graphics       0.84      0.93      0.88       109\n",
            "               Hardware       0.69      0.69      0.69        93\n",
            "               Internet       0.65      0.61      0.63       110\n",
            "       Mobile_Computing       0.75      0.76      0.75        87\n",
            "             Multimedia       0.68      0.81      0.73        98\n",
            "            Open_Source       0.76      0.68      0.72       114\n",
            "            Programming       0.57      0.50      0.53        98\n",
            "               Robotics       0.92      0.93      0.93       105\n",
            "               Security       0.79      0.88      0.83       107\n",
            "               Software       0.38      0.31      0.34       109\n",
            "                Systems       0.69      0.61      0.65       103\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.71      0.72      0.72      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9559, Train: 0.3533, Test: 0.3579\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8366, Train: 0.5284, Test: 0.5211\n",
            "Early stopping:  0.0843715843978362\n",
            "Epoch: 003, Loss: 2.6814, Train: 0.5387, Test: 0.5332\n",
            "Early stopping:  0.1376297558091337\n",
            "Epoch: 004, Loss: 2.4977, Train: 0.5718, Test: 0.5632\n",
            "Early stopping:  0.19835482407790547\n",
            "Epoch: 005, Loss: 2.3008, Train: 0.5891, Test: 0.5774\n",
            "Early stopping:  0.26193024496645095\n",
            "Epoch: 006, Loss: 2.0982, Train: 0.6075, Test: 0.5968\n",
            "Early stopping:  0.2940683016459043\n",
            "Epoch: 007, Loss: 1.9025, Train: 0.6239, Test: 0.6200\n",
            "Early stopping:  0.3095352219163954\n",
            "Epoch: 008, Loss: 1.7232, Train: 0.6395, Test: 0.6311\n",
            "Early stopping:  0.30798411462496145\n",
            "Epoch: 009, Loss: 1.5643, Train: 0.6513, Test: 0.6395\n",
            "Early stopping:  0.2925138389192938\n",
            "Epoch: 010, Loss: 1.4336, Train: 0.6525, Test: 0.6389\n",
            "Early stopping:  0.26439084321971945\n",
            "Epoch: 011, Loss: 1.3330, Train: 0.6559, Test: 0.6421\n",
            "Early stopping:  0.22723172233419076\n",
            "Epoch: 012, Loss: 1.2551, Train: 0.6618, Test: 0.6463\n",
            "Early stopping:  0.18637108171946018\n",
            "Epoch: 013, Loss: 1.1977, Train: 0.6675, Test: 0.6532\n",
            "Early stopping:  0.14594302181618948\n",
            "Epoch: 014, Loss: 1.1540, Train: 0.6741, Test: 0.6526\n",
            "Early stopping:  0.11128064071545656\n",
            "Epoch: 015, Loss: 1.1207, Train: 0.6789, Test: 0.6600\n",
            "Early stopping:  0.08425674148333191\n",
            "Epoch: 016, Loss: 1.0938, Train: 0.6847, Test: 0.6637\n",
            "Early stopping:  0.06390270797967622\n",
            "Epoch: 017, Loss: 1.0689, Train: 0.6929, Test: 0.6679\n",
            "Early stopping:  0.05060175392306804\n",
            "Epoch: 018, Loss: 1.0460, Train: 0.6999, Test: 0.6684\n",
            "Early stopping:  0.04247405393945998\n",
            "Epoch: 019, Loss: 1.0242, Train: 0.7055, Test: 0.6679\n",
            "Early stopping:  0.038132152230265615\n",
            "Epoch: 020, Loss: 1.0022, Train: 0.7130, Test: 0.6742\n",
            "Early stopping:  0.03605967104630082\n",
            "Epoch: 021, Loss: 0.9808, Train: 0.7196, Test: 0.6779\n",
            "Early stopping:  0.03479494868918641\n",
            "Epoch: 022, Loss: 0.9607, Train: 0.7239, Test: 0.6832\n",
            "Early stopping:  0.03382772037762324\n",
            "Epoch: 023, Loss: 0.9419, Train: 0.7276, Test: 0.6837\n",
            "Early stopping:  0.03260074845439215\n",
            "Epoch: 024, Loss: 0.9238, Train: 0.7349, Test: 0.6853\n",
            "Early stopping:  0.030975303778285685\n",
            "Epoch: 025, Loss: 0.9072, Train: 0.7380, Test: 0.6874\n",
            "Early stopping:  0.029114519598581787\n",
            "Epoch: 026, Loss: 0.8921, Train: 0.7416, Test: 0.6900\n",
            "Early stopping:  0.02720532904345904\n",
            "Epoch: 027, Loss: 0.8775, Train: 0.7457, Test: 0.6926\n",
            "Early stopping:  0.025393805152014906\n",
            "Epoch: 028, Loss: 0.8636, Train: 0.7493, Test: 0.6942\n",
            "Early stopping:  0.023757343561947777\n",
            "Epoch: 029, Loss: 0.8508, Train: 0.7539, Test: 0.6953\n",
            "Early stopping:  0.022380394993317183\n",
            "Epoch: 030, Loss: 0.8388, Train: 0.7605, Test: 0.6953\n",
            "Early stopping:  0.021096419254286364\n",
            "Epoch: 031, Loss: 0.8273, Train: 0.7620, Test: 0.6963\n",
            "Early stopping:  0.019822885549266707\n",
            "Epoch: 032, Loss: 0.8166, Train: 0.7657, Test: 0.6979\n",
            "Early stopping:  0.018572332554967357\n",
            "Epoch: 033, Loss: 0.8066, Train: 0.7697, Test: 0.7016\n",
            "Early stopping:  0.017467889312391804\n",
            "Epoch: 034, Loss: 0.7971, Train: 0.7730, Test: 0.7068\n",
            "Early stopping:  0.016449875028389356\n",
            "Epoch: 035, Loss: 0.7881, Train: 0.7757, Test: 0.7079\n",
            "Early stopping:  0.015485004851095621\n",
            "Epoch: 036, Loss: 0.7796, Train: 0.7784, Test: 0.7111\n",
            "Early stopping:  0.014656049492363491\n",
            "Epoch: 037, Loss: 0.7715, Train: 0.7816, Test: 0.7116\n",
            "Early stopping:  0.013903640981679725\n",
            "Epoch: 038, Loss: 0.7637, Train: 0.7867, Test: 0.7105\n",
            "Early stopping:  0.013205860510193504\n",
            "Epoch: 039, Loss: 0.7560, Train: 0.7895, Test: 0.7084\n",
            "Early stopping:  0.012648180512063088\n",
            "Epoch: 040, Loss: 0.7484, Train: 0.7908, Test: 0.7126\n",
            "Early stopping:  0.012305316589724943\n",
            "Epoch: 041, Loss: 0.7405, Train: 0.7924, Test: 0.7137\n",
            "Early stopping:  0.012205926066380603\n",
            "Epoch: 042, Loss: 0.7326, Train: 0.7968, Test: 0.7126\n",
            "Early stopping:  0.012285997716707282\n",
            "Epoch: 043, Loss: 0.7246, Train: 0.7989, Test: 0.7137\n",
            "Early stopping:  0.012452769338202264\n",
            "Epoch: 044, Loss: 0.7165, Train: 0.8012, Test: 0.7137\n",
            "Early stopping:  0.012605251292556578\n",
            "Epoch: 045, Loss: 0.7085, Train: 0.8036, Test: 0.7132\n",
            "Early stopping:  0.012664601822771458\n",
            "Epoch: 046, Loss: 0.7006, Train: 0.8058, Test: 0.7147\n",
            "Early stopping:  0.012648678250407636\n",
            "Epoch: 047, Loss: 0.6927, Train: 0.8067, Test: 0.7126\n",
            "Early stopping:  0.012601524495315765\n",
            "Epoch: 048, Loss: 0.6847, Train: 0.8105, Test: 0.7111\n",
            "Early stopping:  0.01257031575385351\n",
            "Epoch: 049, Loss: 0.6766, Train: 0.8117, Test: 0.7126\n",
            "Early stopping:  0.012609621809589997\n",
            "Epoch: 050, Loss: 0.6685, Train: 0.8129, Test: 0.7116\n",
            "Early stopping:  0.012687638864489183\n",
            "Epoch: 051, Loss: 0.6604, Train: 0.8162, Test: 0.7105\n",
            "Early stopping:  0.01274181394514593\n",
            "Epoch: 052, Loss: 0.6524, Train: 0.8193, Test: 0.7116\n",
            "Early stopping:  0.012752377518641738\n",
            "Epoch: 053, Loss: 0.6444, Train: 0.8213, Test: 0.7153\n",
            "Early stopping:  0.012739408661448317\n",
            "Epoch: 054, Loss: 0.6363, Train: 0.8250, Test: 0.7168\n",
            "Early stopping:  0.012720481171470219\n",
            "Epoch: 055, Loss: 0.6283, Train: 0.8270, Test: 0.7153\n",
            "Early stopping:  0.012715919645192447\n",
            "Epoch: 056, Loss: 0.6203, Train: 0.8293, Test: 0.7147\n",
            "Early stopping:  0.012687180809936449\n",
            "Epoch: 057, Loss: 0.6125, Train: 0.8328, Test: 0.7158\n",
            "Early stopping:  0.012612299477906286\n",
            "Epoch: 058, Loss: 0.6046, Train: 0.8346, Test: 0.7200\n",
            "Early stopping:  0.012526841800594877\n",
            "Epoch: 059, Loss: 0.5968, Train: 0.8392, Test: 0.7158\n",
            "Early stopping:  0.01243747895502642\n",
            "Epoch: 060, Loss: 0.5893, Train: 0.8393, Test: 0.7147\n",
            "Early stopping:  0.01227743938493364\n",
            "Epoch: 061, Loss: 0.5824, Train: 0.8442, Test: 0.7153\n",
            "Early stopping:  0.011924743115224448\n",
            "Epoch: 062, Loss: 0.5765, Train: 0.8420, Test: 0.7116\n",
            "Early stopping:  0.011190508140719972\n",
            "Epoch: 063, Loss: 0.5700, Train: 0.8499, Test: 0.7142\n",
            "Early stopping:  0.01053473179709125\n",
            "Epoch: 064, Loss: 0.5613, Train: 0.8488, Test: 0.7158\n",
            "Early stopping:  0.010851622699109876\n",
            "Epoch: 065, Loss: 0.5527, Train: 0.8520, Test: 0.7147\n",
            "Early stopping:  0.011824078742901162\n",
            "Epoch: 066, Loss: 0.5470, Train: 0.8546, Test: 0.7142\n",
            "Early stopping:  0.012078544885802655\n",
            "Epoch: 067, Loss: 0.5410, Train: 0.8578, Test: 0.7132\n",
            "Early stopping:  0.011471411745145751\n",
            "Epoch: 068, Loss: 0.5337, Train: 0.8583, Test: 0.7100\n",
            "Early stopping:  0.010599546228192869\n",
            "Epoch: 069, Loss: 0.5275, Train: 0.8626, Test: 0.7095\n",
            "Early stopping:  0.010090544971973701\n",
            "Epoch: 070, Loss: 0.5200, Train: 0.8641, Test: 0.7142\n",
            "Early stopping:  0.010675372327597884\n",
            "Epoch: 071, Loss: 0.5119, Train: 0.8672, Test: 0.7074\n",
            "Early stopping:  0.011394409258811068\n",
            "Epoch: 072, Loss: 0.5071, Train: 0.8699, Test: 0.7132\n",
            "Early stopping:  0.010915250761460283\n",
            "Epoch: 073, Loss: 0.4997, Train: 0.8717, Test: 0.7132\n",
            "Early stopping:  0.010865123085151238\n",
            "Epoch: 074, Loss: 0.4918, Train: 0.8736, Test: 0.7095\n",
            "Early stopping:  0.010884810378358612\n",
            "Epoch: 075, Loss: 0.4880, Train: 0.8749, Test: 0.7142\n",
            "Early stopping:  0.010019456529430575\n",
            "Epoch: 076, Loss: 0.4819, Train: 0.8772, Test: 0.7100\n",
            "Early stopping:  0.009886717206648016\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.77      0.80      0.78       109\n",
            "            CAD_and_CAM       0.74      0.83      0.78        94\n",
            "              Companies       0.56      0.53      0.54        95\n",
            "       Computer_Science       0.84      0.79      0.81       119\n",
            "            Consultants       0.58      0.52      0.55        94\n",
            "           Data_Formats       0.76      0.76      0.76       112\n",
            "    Data_Communications       0.74      0.73      0.74       111\n",
            "              Education       0.90      0.90      0.90       106\n",
            "               Graphics       0.80      0.90      0.85       104\n",
            "               Hardware       0.64      0.73      0.69        94\n",
            "               Internet       0.58      0.65      0.61        82\n",
            "       Mobile_Computing       0.69      0.72      0.71        85\n",
            "             Multimedia       0.67      0.70      0.68        83\n",
            "            Open_Source       0.73      0.74      0.74       101\n",
            "            Programming       0.60      0.57      0.58        97\n",
            "               Robotics       0.95      0.94      0.94       108\n",
            "               Security       0.78      0.79      0.78       103\n",
            "               Software       0.34      0.28      0.31        94\n",
            "                Systems       0.59      0.52      0.55       109\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.70      0.70      0.70      1900\n",
            "           weighted avg       0.71      0.71      0.71      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9526, Train: 0.3528, Test: 0.3526\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8248, Train: 0.4964, Test: 0.4826\n",
            "Early stopping:  0.09042134303308992\n",
            "Epoch: 003, Loss: 2.6527, Train: 0.5453, Test: 0.5342\n",
            "Early stopping:  0.1504940453483352\n",
            "Epoch: 004, Loss: 2.4506, Train: 0.5637, Test: 0.5526\n",
            "Early stopping:  0.21770247345808807\n",
            "Epoch: 005, Loss: 2.2401, Train: 0.5924, Test: 0.5821\n",
            "Early stopping:  0.28573434188729124\n",
            "Epoch: 006, Loss: 2.0301, Train: 0.6166, Test: 0.6068\n",
            "Early stopping:  0.3167750275407988\n",
            "Epoch: 007, Loss: 1.8304, Train: 0.6330, Test: 0.6153\n",
            "Early stopping:  0.3265423956463792\n",
            "Epoch: 008, Loss: 1.6521, Train: 0.6388, Test: 0.6268\n",
            "Early stopping:  0.3174635917596645\n",
            "Epoch: 009, Loss: 1.5018, Train: 0.6450, Test: 0.6337\n",
            "Early stopping:  0.2938620436638376\n",
            "Epoch: 010, Loss: 1.3816, Train: 0.6517, Test: 0.6332\n",
            "Early stopping:  0.2582606117401852\n",
            "Epoch: 011, Loss: 1.2904, Train: 0.6587, Test: 0.6411\n",
            "Early stopping:  0.21528939344960304\n",
            "Epoch: 012, Loss: 1.2238, Train: 0.6632, Test: 0.6447\n",
            "Early stopping:  0.17090297332617893\n",
            "Epoch: 013, Loss: 1.1757, Train: 0.6689, Test: 0.6447\n",
            "Early stopping:  0.13003089882951008\n",
            "Epoch: 014, Loss: 1.1414, Train: 0.6751, Test: 0.6547\n",
            "Early stopping:  0.09574919041604758\n",
            "Epoch: 015, Loss: 1.1147, Train: 0.6784, Test: 0.6579\n",
            "Early stopping:  0.06973360911449963\n",
            "Epoch: 016, Loss: 1.0926, Train: 0.6833, Test: 0.6616\n",
            "Early stopping:  0.051780984664062925\n",
            "Epoch: 017, Loss: 1.0710, Train: 0.6878, Test: 0.6711\n",
            "Early stopping:  0.04104460262584819\n",
            "Epoch: 018, Loss: 1.0502, Train: 0.6947, Test: 0.6774\n",
            "Early stopping:  0.03576259908503665\n",
            "Epoch: 019, Loss: 1.0284, Train: 0.6999, Test: 0.6816\n",
            "Early stopping:  0.03398408345786859\n",
            "Epoch: 020, Loss: 1.0061, Train: 0.7043, Test: 0.6837\n",
            "Early stopping:  0.0340984373123067\n",
            "Epoch: 021, Loss: 0.9838, Train: 0.7114, Test: 0.6879\n",
            "Early stopping:  0.0345828589040015\n",
            "Epoch: 022, Loss: 0.9621, Train: 0.7164, Test: 0.6921\n",
            "Early stopping:  0.034935493181796305\n",
            "Epoch: 023, Loss: 0.9417, Train: 0.7224, Test: 0.6937\n",
            "Early stopping:  0.03436789627975015\n",
            "Epoch: 024, Loss: 0.9232, Train: 0.7286, Test: 0.6911\n",
            "Early stopping:  0.0328626801069908\n",
            "Epoch: 025, Loss: 0.9066, Train: 0.7314, Test: 0.6942\n",
            "Early stopping:  0.030605397094648004\n",
            "Epoch: 026, Loss: 0.8913, Train: 0.7361, Test: 0.6984\n",
            "Early stopping:  0.02799204744150073\n",
            "Epoch: 027, Loss: 0.8775, Train: 0.7407, Test: 0.7011\n",
            "Early stopping:  0.025406558889136\n",
            "Epoch: 028, Loss: 0.8645, Train: 0.7441, Test: 0.7068\n",
            "Early stopping:  0.023202261526150455\n",
            "Epoch: 029, Loss: 0.8524, Train: 0.7491, Test: 0.7147\n",
            "Early stopping:  0.021391058033571785\n",
            "Epoch: 030, Loss: 0.8409, Train: 0.7526, Test: 0.7142\n",
            "Early stopping:  0.019926315308468962\n",
            "Epoch: 031, Loss: 0.8301, Train: 0.7584, Test: 0.7158\n",
            "Early stopping:  0.018729053038584017\n",
            "Epoch: 032, Loss: 0.8197, Train: 0.7599, Test: 0.7179\n",
            "Early stopping:  0.017694525502959035\n",
            "Epoch: 033, Loss: 0.8099, Train: 0.7634, Test: 0.7200\n",
            "Early stopping:  0.016790689000884842\n",
            "Epoch: 034, Loss: 0.8005, Train: 0.7676, Test: 0.7216\n",
            "Early stopping:  0.015957794371135252\n",
            "Epoch: 035, Loss: 0.7915, Train: 0.7700, Test: 0.7247\n",
            "Early stopping:  0.01524984356438826\n",
            "Epoch: 036, Loss: 0.7827, Train: 0.7747, Test: 0.7253\n",
            "Early stopping:  0.014633553499658199\n",
            "Epoch: 037, Loss: 0.7739, Train: 0.7788, Test: 0.7253\n",
            "Early stopping:  0.01421291217272343\n",
            "Epoch: 038, Loss: 0.7653, Train: 0.7825, Test: 0.7242\n",
            "Early stopping:  0.013927338271459285\n",
            "Epoch: 039, Loss: 0.7568, Train: 0.7850, Test: 0.7263\n",
            "Early stopping:  0.013716554206062402\n",
            "Epoch: 040, Loss: 0.7484, Train: 0.7870, Test: 0.7279\n",
            "Early stopping:  0.013534655420886037\n",
            "Epoch: 041, Loss: 0.7401, Train: 0.7897, Test: 0.7289\n",
            "Early stopping:  0.01334604427718446\n",
            "Epoch: 042, Loss: 0.7319, Train: 0.7934, Test: 0.7295\n",
            "Early stopping:  0.013176703146766614\n",
            "Epoch: 043, Loss: 0.7238, Train: 0.7966, Test: 0.7316\n",
            "Early stopping:  0.013049407771519764\n",
            "Epoch: 044, Loss: 0.7155, Train: 0.7967, Test: 0.7347\n",
            "Early stopping:  0.012983857711357022\n",
            "Epoch: 045, Loss: 0.7074, Train: 0.7999, Test: 0.7368\n",
            "Early stopping:  0.012943928135182537\n",
            "Epoch: 046, Loss: 0.6993, Train: 0.8017, Test: 0.7358\n",
            "Early stopping:  0.012921169301663418\n",
            "Epoch: 047, Loss: 0.6911, Train: 0.8039, Test: 0.7347\n",
            "Early stopping:  0.01288887799555754\n",
            "Epoch: 048, Loss: 0.6830, Train: 0.8054, Test: 0.7337\n",
            "Early stopping:  0.012855977014989584\n",
            "Epoch: 049, Loss: 0.6749, Train: 0.8066, Test: 0.7342\n",
            "Early stopping:  0.012843138437636051\n",
            "Epoch: 050, Loss: 0.6668, Train: 0.8082, Test: 0.7347\n",
            "Early stopping:  0.012826288310643436\n",
            "Epoch: 051, Loss: 0.6589, Train: 0.8111, Test: 0.7316\n",
            "Early stopping:  0.012768130494637393\n",
            "Epoch: 052, Loss: 0.6513, Train: 0.8145, Test: 0.7374\n",
            "Early stopping:  0.012582788185821682\n",
            "Epoch: 053, Loss: 0.6436, Train: 0.8153, Test: 0.7342\n",
            "Early stopping:  0.012347192415690345\n",
            "Epoch: 054, Loss: 0.6359, Train: 0.8200, Test: 0.7395\n",
            "Early stopping:  0.012189982518961187\n",
            "Epoch: 055, Loss: 0.6274, Train: 0.8213, Test: 0.7395\n",
            "Early stopping:  0.012399168212733756\n",
            "Epoch: 056, Loss: 0.6193, Train: 0.8239, Test: 0.7363\n",
            "Early stopping:  0.012689222352887003\n",
            "Epoch: 057, Loss: 0.6119, Train: 0.8253, Test: 0.7411\n",
            "Early stopping:  0.012662425490188972\n",
            "Epoch: 058, Loss: 0.6047, Train: 0.8270, Test: 0.7374\n",
            "Early stopping:  0.012303710299257725\n",
            "Epoch: 059, Loss: 0.5973, Train: 0.8301, Test: 0.7426\n",
            "Early stopping:  0.01180345038390427\n",
            "Epoch: 060, Loss: 0.5893, Train: 0.8336, Test: 0.7411\n",
            "Early stopping:  0.011774165648958054\n",
            "Epoch: 061, Loss: 0.5816, Train: 0.8366, Test: 0.7395\n",
            "Early stopping:  0.012028089959943764\n",
            "Epoch: 062, Loss: 0.5742, Train: 0.8399, Test: 0.7405\n",
            "Early stopping:  0.012129826596681482\n",
            "Epoch: 063, Loss: 0.5673, Train: 0.8418, Test: 0.7395\n",
            "Early stopping:  0.011888884237378616\n",
            "Epoch: 064, Loss: 0.5605, Train: 0.8470, Test: 0.7405\n",
            "Early stopping:  0.011381323203331285\n",
            "Epoch: 065, Loss: 0.5538, Train: 0.8463, Test: 0.7432\n",
            "Early stopping:  0.01096470618841327\n",
            "Epoch: 066, Loss: 0.5474, Train: 0.8525, Test: 0.7389\n",
            "Early stopping:  0.010636730094729123\n",
            "Epoch: 067, Loss: 0.5408, Train: 0.8521, Test: 0.7437\n",
            "Early stopping:  0.010458122707855108\n",
            "Epoch: 068, Loss: 0.5337, Train: 0.8564, Test: 0.7389\n",
            "Early stopping:  0.010514875164944428\n",
            "Epoch: 069, Loss: 0.5258, Train: 0.8629, Test: 0.7416\n",
            "Early stopping:  0.011007531198250296\n",
            "Epoch: 070, Loss: 0.5187, Train: 0.8607, Test: 0.7395\n",
            "Early stopping:  0.011432718231150371\n",
            "Epoch: 071, Loss: 0.5128, Train: 0.8675, Test: 0.7321\n",
            "Early stopping:  0.01124798007129107\n",
            "Epoch: 072, Loss: 0.5059, Train: 0.8686, Test: 0.7400\n",
            "Early stopping:  0.010868895990012088\n",
            "Epoch: 073, Loss: 0.4982, Train: 0.8701, Test: 0.7379\n",
            "Early stopping:  0.010774524917188439\n",
            "Epoch: 074, Loss: 0.4913, Train: 0.8742, Test: 0.7379\n",
            "Early stopping:  0.010982380311275942\n",
            "Epoch: 075, Loss: 0.4860, Train: 0.8729, Test: 0.7384\n",
            "Early stopping:  0.010790216225197527\n",
            "Epoch: 076, Loss: 0.4801, Train: 0.8768, Test: 0.7363\n",
            "Early stopping:  0.010125446258894508\n",
            "Epoch: 077, Loss: 0.4729, Train: 0.8793, Test: 0.7368\n",
            "Early stopping:  0.009777109291950655\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.83      0.83      0.83        93\n",
            "            CAD_and_CAM       0.82      0.83      0.83       101\n",
            "              Companies       0.54      0.63      0.58        82\n",
            "       Computer_Science       0.79      0.77      0.78        93\n",
            "            Consultants       0.70      0.68      0.69        98\n",
            "           Data_Formats       0.81      0.88      0.84       110\n",
            "    Data_Communications       0.68      0.80      0.73        98\n",
            "              Education       0.88      0.91      0.90        94\n",
            "               Graphics       0.89      0.94      0.91       111\n",
            "               Hardware       0.69      0.72      0.71       101\n",
            "               Internet       0.69      0.65      0.67       103\n",
            "       Mobile_Computing       0.80      0.68      0.74        94\n",
            "             Multimedia       0.67      0.73      0.70        89\n",
            "            Open_Source       0.69      0.74      0.71        92\n",
            "            Programming       0.58      0.55      0.57       109\n",
            "               Robotics       0.93      0.88      0.90       116\n",
            "               Security       0.88      0.79      0.83        95\n",
            "               Software       0.40      0.40      0.40       100\n",
            "                Systems       0.72      0.57      0.64       121\n",
            "\n",
            "               accuracy                           0.74      1900\n",
            "              macro avg       0.74      0.74      0.73      1900\n",
            "           weighted avg       0.74      0.74      0.74      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9507, Train: 0.5125, Test: 0.4979\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8042, Train: 0.5209, Test: 0.5158\n",
            "Early stopping:  0.10362477055979472\n",
            "Epoch: 003, Loss: 2.6301, Train: 0.5366, Test: 0.5342\n",
            "Early stopping:  0.16050560052420584\n",
            "Epoch: 004, Loss: 2.4255, Train: 0.5630, Test: 0.5537\n",
            "Early stopping:  0.22650214937044416\n",
            "Epoch: 005, Loss: 2.2099, Train: 0.5967, Test: 0.5695\n",
            "Early stopping:  0.2950112791417871\n",
            "Epoch: 006, Loss: 1.9993, Train: 0.6236, Test: 0.6079\n",
            "Early stopping:  0.3212221017964812\n",
            "Epoch: 007, Loss: 1.8024, Train: 0.6403, Test: 0.6253\n",
            "Early stopping:  0.3291880781889459\n",
            "Epoch: 008, Loss: 1.6256, Train: 0.6474, Test: 0.6289\n",
            "Early stopping:  0.3176569824371903\n",
            "Epoch: 009, Loss: 1.4778, Train: 0.6538, Test: 0.6363\n",
            "Early stopping:  0.2912846101436007\n",
            "Epoch: 010, Loss: 1.3617, Train: 0.6592, Test: 0.6447\n",
            "Early stopping:  0.25424351820862395\n",
            "Epoch: 011, Loss: 1.2726, Train: 0.6670, Test: 0.6463\n",
            "Early stopping:  0.211074014222303\n",
            "Epoch: 012, Loss: 1.2073, Train: 0.6692, Test: 0.6526\n",
            "Early stopping:  0.16669508361690477\n",
            "Epoch: 013, Loss: 1.1609, Train: 0.6749, Test: 0.6568\n",
            "Early stopping:  0.12650717199289316\n",
            "Epoch: 014, Loss: 1.1260, Train: 0.6801, Test: 0.6611\n",
            "Early stopping:  0.09376533221386898\n",
            "Epoch: 015, Loss: 1.0991, Train: 0.6846, Test: 0.6632\n",
            "Early stopping:  0.06876112304884693\n",
            "Epoch: 016, Loss: 1.0766, Train: 0.6899, Test: 0.6653\n",
            "Early stopping:  0.051639820711987854\n",
            "Epoch: 017, Loss: 1.0547, Train: 0.6991, Test: 0.6721\n",
            "Early stopping:  0.04159879958279852\n",
            "Epoch: 018, Loss: 1.0339, Train: 0.7049, Test: 0.6784\n",
            "Early stopping:  0.036184589116150044\n",
            "Epoch: 019, Loss: 1.0123, Train: 0.7095, Test: 0.6821\n",
            "Early stopping:  0.0342175793594026\n",
            "Epoch: 020, Loss: 0.9908, Train: 0.7112, Test: 0.6847\n",
            "Early stopping:  0.03384949069208448\n",
            "Epoch: 021, Loss: 0.9700, Train: 0.7193, Test: 0.6868\n",
            "Early stopping:  0.03362449469652095\n",
            "Epoch: 022, Loss: 0.9500, Train: 0.7258, Test: 0.6895\n",
            "Early stopping:  0.0332373553195799\n",
            "Epoch: 023, Loss: 0.9314, Train: 0.7295, Test: 0.6905\n",
            "Early stopping:  0.03203431200208414\n",
            "Epoch: 024, Loss: 0.9140, Train: 0.7333, Test: 0.6947\n",
            "Early stopping:  0.030380769941945517\n",
            "Epoch: 025, Loss: 0.8982, Train: 0.7371, Test: 0.6947\n",
            "Early stopping:  0.02841701187685132\n",
            "Epoch: 026, Loss: 0.8831, Train: 0.7421, Test: 0.6937\n",
            "Early stopping:  0.026428886421516442\n",
            "Epoch: 027, Loss: 0.8693, Train: 0.7454, Test: 0.6958\n",
            "Early stopping:  0.02455083699753835\n",
            "Epoch: 028, Loss: 0.8560, Train: 0.7517, Test: 0.6974\n",
            "Early stopping:  0.022920355832103584\n",
            "Epoch: 029, Loss: 0.8435, Train: 0.7557, Test: 0.7016\n",
            "Early stopping:  0.021577637918944657\n",
            "Epoch: 030, Loss: 0.8320, Train: 0.7595, Test: 0.7021\n",
            "Early stopping:  0.020267230604387496\n",
            "Epoch: 031, Loss: 0.8210, Train: 0.7637, Test: 0.7032\n",
            "Early stopping:  0.019110745694362004\n",
            "Epoch: 032, Loss: 0.8108, Train: 0.7695, Test: 0.7026\n",
            "Early stopping:  0.017896224103631165\n",
            "Epoch: 033, Loss: 0.8011, Train: 0.7728, Test: 0.7074\n",
            "Early stopping:  0.016768801842211477\n",
            "Epoch: 034, Loss: 0.7922, Train: 0.7763, Test: 0.7079\n",
            "Early stopping:  0.015720843417809126\n",
            "Epoch: 035, Loss: 0.7836, Train: 0.7812, Test: 0.7084\n",
            "Early stopping:  0.014743840351819682\n",
            "Epoch: 036, Loss: 0.7754, Train: 0.7846, Test: 0.7111\n",
            "Early stopping:  0.013963231254866622\n",
            "Epoch: 037, Loss: 0.7673, Train: 0.7879, Test: 0.7100\n",
            "Early stopping:  0.013374894979122878\n",
            "Epoch: 038, Loss: 0.7594, Train: 0.7916, Test: 0.7116\n",
            "Early stopping:  0.01298123531487609\n",
            "Epoch: 039, Loss: 0.7515, Train: 0.7932, Test: 0.7132\n",
            "Early stopping:  0.012700667004846646\n",
            "Epoch: 040, Loss: 0.7436, Train: 0.7953, Test: 0.7079\n",
            "Early stopping:  0.012556597805327531\n",
            "Epoch: 041, Loss: 0.7357, Train: 0.7983, Test: 0.7079\n",
            "Early stopping:  0.012464321363198588\n",
            "Epoch: 042, Loss: 0.7279, Train: 0.7999, Test: 0.7100\n",
            "Early stopping:  0.012425005084003501\n",
            "Epoch: 043, Loss: 0.7201, Train: 0.8008, Test: 0.7079\n",
            "Early stopping:  0.012384950941493129\n",
            "Epoch: 044, Loss: 0.7122, Train: 0.8025, Test: 0.7063\n",
            "Early stopping:  0.012385499023406119\n",
            "Epoch: 045, Loss: 0.7042, Train: 0.8037, Test: 0.7084\n",
            "Early stopping:  0.01246649248868108\n",
            "Epoch: 046, Loss: 0.6961, Train: 0.8057, Test: 0.7079\n",
            "Early stopping:  0.01259794773133784\n",
            "Epoch: 047, Loss: 0.6879, Train: 0.8083, Test: 0.7042\n",
            "Early stopping:  0.01273319149125172\n",
            "Epoch: 048, Loss: 0.6797, Train: 0.8096, Test: 0.7063\n",
            "Early stopping:  0.012837557715948772\n",
            "Epoch: 049, Loss: 0.6716, Train: 0.8113, Test: 0.7047\n",
            "Early stopping:  0.012876775104141727\n",
            "Epoch: 050, Loss: 0.6635, Train: 0.8125, Test: 0.7058\n",
            "Early stopping:  0.012874524364585601\n",
            "Epoch: 051, Loss: 0.6555, Train: 0.8157, Test: 0.7063\n",
            "Early stopping:  0.012833281802768673\n",
            "Epoch: 052, Loss: 0.6475, Train: 0.8179, Test: 0.7079\n",
            "Early stopping:  0.012757202432414362\n",
            "Epoch: 053, Loss: 0.6396, Train: 0.8200, Test: 0.7068\n",
            "Early stopping:  0.012667422726687886\n",
            "Epoch: 054, Loss: 0.6318, Train: 0.8225, Test: 0.7089\n",
            "Early stopping:  0.012526291541664679\n",
            "Epoch: 055, Loss: 0.6242, Train: 0.8255, Test: 0.7089\n",
            "Early stopping:  0.012371815873990737\n",
            "Epoch: 056, Loss: 0.6167, Train: 0.8264, Test: 0.7074\n",
            "Early stopping:  0.012176028605185211\n",
            "Epoch: 057, Loss: 0.6094, Train: 0.8287, Test: 0.7089\n",
            "Early stopping:  0.011931183724449346\n",
            "Epoch: 058, Loss: 0.6030, Train: 0.8292, Test: 0.7063\n",
            "Early stopping:  0.011448495338572843\n",
            "Epoch: 059, Loss: 0.5976, Train: 0.8317, Test: 0.7089\n",
            "Early stopping:  0.010574647988395363\n",
            "Epoch: 060, Loss: 0.5928, Train: 0.8346, Test: 0.7084\n",
            "Early stopping:  0.009467797313381389\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.79      0.83      0.81       106\n",
            "            CAD_and_CAM       0.73      0.80      0.76        94\n",
            "              Companies       0.52      0.56      0.54       112\n",
            "       Computer_Science       0.77      0.75      0.76       106\n",
            "            Consultants       0.57      0.58      0.58       103\n",
            "           Data_Formats       0.80      0.87      0.84        87\n",
            "    Data_Communications       0.68      0.70      0.69        91\n",
            "              Education       0.91      0.88      0.89       100\n",
            "               Graphics       0.83      0.92      0.87        91\n",
            "               Hardware       0.69      0.62      0.65       111\n",
            "               Internet       0.68      0.53      0.59        99\n",
            "       Mobile_Computing       0.82      0.77      0.80       109\n",
            "             Multimedia       0.66      0.74      0.70        89\n",
            "            Open_Source       0.63      0.79      0.70        98\n",
            "            Programming       0.59      0.55      0.57       111\n",
            "               Robotics       0.90      0.93      0.91       103\n",
            "               Security       0.74      0.81      0.78        86\n",
            "               Software       0.48      0.28      0.35       105\n",
            "                Systems       0.64      0.66      0.65        99\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.71      0.71      0.71      1900\n",
            "           weighted avg       0.70      0.71      0.70      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9472, Train: 0.3855, Test: 0.3753\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8209, Train: 0.4976, Test: 0.4932\n",
            "Early stopping:  0.08932198463653176\n",
            "Epoch: 003, Loss: 2.6613, Train: 0.5437, Test: 0.5553\n",
            "Early stopping:  0.14326582218744668\n",
            "Epoch: 004, Loss: 2.4789, Train: 0.5792, Test: 0.5753\n",
            "Early stopping:  0.202595420735739\n",
            "Epoch: 005, Loss: 2.2823, Train: 0.5979, Test: 0.5984\n",
            "Early stopping:  0.26522917461216344\n",
            "Epoch: 006, Loss: 2.0800, Train: 0.6170, Test: 0.6158\n",
            "Early stopping:  0.29452316487752844\n",
            "Epoch: 007, Loss: 1.8840, Train: 0.6312, Test: 0.6384\n",
            "Early stopping:  0.3089387892771784\n",
            "Epoch: 008, Loss: 1.7058, Train: 0.6405, Test: 0.6453\n",
            "Early stopping:  0.3075408107970936\n",
            "Epoch: 009, Loss: 1.5521, Train: 0.6430, Test: 0.6526\n",
            "Early stopping:  0.29048909675959766\n",
            "Epoch: 010, Loss: 1.4272, Train: 0.6497, Test: 0.6516\n",
            "Early stopping:  0.2598541189614128\n",
            "Epoch: 011, Loss: 1.3310, Train: 0.6558, Test: 0.6611\n",
            "Early stopping:  0.22043496592779271\n",
            "Epoch: 012, Loss: 1.2576, Train: 0.6611, Test: 0.6611\n",
            "Early stopping:  0.17852402103870552\n",
            "Epoch: 013, Loss: 1.2030, Train: 0.6659, Test: 0.6684\n",
            "Early stopping:  0.1389659263617282\n",
            "Epoch: 014, Loss: 1.1620, Train: 0.6729, Test: 0.6732\n",
            "Early stopping:  0.10553637700527206\n",
            "Epoch: 015, Loss: 1.1287, Train: 0.6789, Test: 0.6784\n",
            "Early stopping:  0.08008312951095904\n",
            "Epoch: 016, Loss: 1.1011, Train: 0.6839, Test: 0.6805\n",
            "Early stopping:  0.06180093529847986\n",
            "Epoch: 017, Loss: 1.0761, Train: 0.6900, Test: 0.6853\n",
            "Early stopping:  0.0500213701418422\n",
            "Epoch: 018, Loss: 1.0524, Train: 0.6926, Test: 0.6905\n",
            "Early stopping:  0.043092516945482266\n",
            "Epoch: 019, Loss: 1.0299, Train: 0.6979, Test: 0.6921\n",
            "Early stopping:  0.03895479503179055\n",
            "Epoch: 020, Loss: 1.0078, Train: 0.7030, Test: 0.6953\n",
            "Early stopping:  0.036820300042953245\n",
            "Epoch: 021, Loss: 0.9868, Train: 0.7082, Test: 0.7021\n",
            "Early stopping:  0.0353176782626956\n",
            "Epoch: 022, Loss: 0.9665, Train: 0.7163, Test: 0.7016\n",
            "Early stopping:  0.03397999346990762\n",
            "Epoch: 023, Loss: 0.9472, Train: 0.7208, Test: 0.7074\n",
            "Early stopping:  0.0326895124677112\n",
            "Epoch: 024, Loss: 0.9292, Train: 0.7241, Test: 0.7063\n",
            "Early stopping:  0.031104727818809918\n",
            "Epoch: 025, Loss: 0.9122, Train: 0.7295, Test: 0.7100\n",
            "Early stopping:  0.029487339345826348\n",
            "Epoch: 026, Loss: 0.8965, Train: 0.7354, Test: 0.7105\n",
            "Early stopping:  0.027690829263894275\n",
            "Epoch: 027, Loss: 0.8817, Train: 0.7420, Test: 0.7137\n",
            "Early stopping:  0.025902820382761524\n",
            "Epoch: 028, Loss: 0.8677, Train: 0.7484, Test: 0.7168\n",
            "Early stopping:  0.024288704027169712\n",
            "Epoch: 029, Loss: 0.8547, Train: 0.7514, Test: 0.7153\n",
            "Early stopping:  0.022760083891308985\n",
            "Epoch: 030, Loss: 0.8423, Train: 0.7559, Test: 0.7174\n",
            "Early stopping:  0.021435855799812515\n",
            "Epoch: 031, Loss: 0.8308, Train: 0.7603, Test: 0.7163\n",
            "Early stopping:  0.02014118510594603\n",
            "Epoch: 032, Loss: 0.8199, Train: 0.7637, Test: 0.7216\n",
            "Early stopping:  0.018907687195575455\n",
            "Epoch: 033, Loss: 0.8098, Train: 0.7674, Test: 0.7237\n",
            "Early stopping:  0.017732042334596628\n",
            "Epoch: 034, Loss: 0.8001, Train: 0.7726, Test: 0.7258\n",
            "Early stopping:  0.01664904078473388\n",
            "Epoch: 035, Loss: 0.7910, Train: 0.7762, Test: 0.7232\n",
            "Early stopping:  0.015715728183901954\n",
            "Epoch: 036, Loss: 0.7822, Train: 0.7800, Test: 0.7221\n",
            "Early stopping:  0.014917419827522357\n",
            "Epoch: 037, Loss: 0.7736, Train: 0.7822, Test: 0.7263\n",
            "Early stopping:  0.014293884542682326\n",
            "Epoch: 038, Loss: 0.7653, Train: 0.7847, Test: 0.7284\n",
            "Early stopping:  0.013780792586091032\n",
            "Epoch: 039, Loss: 0.7569, Train: 0.7876, Test: 0.7295\n",
            "Early stopping:  0.013452686206424274\n",
            "Epoch: 040, Loss: 0.7488, Train: 0.7903, Test: 0.7300\n",
            "Early stopping:  0.013182831189034445\n",
            "Epoch: 041, Loss: 0.7407, Train: 0.7913, Test: 0.7284\n",
            "Early stopping:  0.013006797923829689\n",
            "Epoch: 042, Loss: 0.7327, Train: 0.7949, Test: 0.7274\n",
            "Early stopping:  0.012878327135195499\n",
            "Epoch: 043, Loss: 0.7246, Train: 0.7959, Test: 0.7311\n",
            "Early stopping:  0.012775786293840921\n",
            "Epoch: 044, Loss: 0.7169, Train: 0.7974, Test: 0.7326\n",
            "Early stopping:  0.012636287752441971\n",
            "Epoch: 045, Loss: 0.7096, Train: 0.8013, Test: 0.7321\n",
            "Early stopping:  0.012339200230192048\n",
            "Epoch: 046, Loss: 0.7007, Train: 0.8049, Test: 0.7321\n",
            "Early stopping:  0.012499067848589309\n",
            "Epoch: 047, Loss: 0.6916, Train: 0.8058, Test: 0.7300\n",
            "Early stopping:  0.013014030554432093\n",
            "Epoch: 048, Loss: 0.6838, Train: 0.8071, Test: 0.7300\n",
            "Early stopping:  0.013322123346969398\n",
            "Epoch: 049, Loss: 0.6764, Train: 0.8093, Test: 0.7342\n",
            "Early stopping:  0.013183033140394691\n",
            "Epoch: 050, Loss: 0.6683, Train: 0.8120, Test: 0.7363\n",
            "Early stopping:  0.01265673919409083\n",
            "Epoch: 051, Loss: 0.6597, Train: 0.8149, Test: 0.7389\n",
            "Early stopping:  0.01253792725487627\n",
            "Epoch: 052, Loss: 0.6521, Train: 0.8159, Test: 0.7353\n",
            "Early stopping:  0.012647289355144972\n",
            "Epoch: 053, Loss: 0.6448, Train: 0.8195, Test: 0.7389\n",
            "Early stopping:  0.012543984452297322\n",
            "Epoch: 054, Loss: 0.6367, Train: 0.8214, Test: 0.7353\n",
            "Early stopping:  0.012346339131739532\n",
            "Epoch: 055, Loss: 0.6286, Train: 0.8249, Test: 0.7342\n",
            "Early stopping:  0.0122632482150304\n",
            "Epoch: 056, Loss: 0.6213, Train: 0.8249, Test: 0.7337\n",
            "Early stopping:  0.012293444448131276\n",
            "Epoch: 057, Loss: 0.6144, Train: 0.8279, Test: 0.7321\n",
            "Early stopping:  0.0120754467286583\n",
            "Epoch: 058, Loss: 0.6078, Train: 0.8274, Test: 0.7326\n",
            "Early stopping:  0.011416738686772102\n",
            "Epoch: 059, Loss: 0.6024, Train: 0.8322, Test: 0.7316\n",
            "Early stopping:  0.010457120558010367\n",
            "Epoch: 060, Loss: 0.5975, Train: 0.8321, Test: 0.7311\n",
            "Early stopping:  0.00946391926248485\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.84      0.85      0.85       107\n",
            "            CAD_and_CAM       0.75      0.87      0.81       106\n",
            "              Companies       0.58      0.51      0.54        88\n",
            "       Computer_Science       0.70      0.74      0.72        91\n",
            "            Consultants       0.54      0.73      0.62        89\n",
            "           Data_Formats       0.77      0.87      0.81        98\n",
            "    Data_Communications       0.78      0.80      0.79       110\n",
            "              Education       0.85      0.86      0.85        99\n",
            "               Graphics       0.83      0.94      0.88       108\n",
            "               Hardware       0.73      0.59      0.66       101\n",
            "               Internet       0.67      0.68      0.68       100\n",
            "       Mobile_Computing       0.82      0.83      0.82       101\n",
            "             Multimedia       0.68      0.72      0.70       107\n",
            "            Open_Source       0.69      0.77      0.73        97\n",
            "            Programming       0.62      0.49      0.55       123\n",
            "               Robotics       0.97      0.94      0.95        89\n",
            "               Security       0.87      0.77      0.82        88\n",
            "               Software       0.43      0.29      0.35        96\n",
            "                Systems       0.68      0.64      0.66       102\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.73      0.73      0.73      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9531, Train: 0.3882, Test: 0.3826\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8282, Train: 0.5370, Test: 0.5279\n",
            "Early stopping:  0.08831113462184648\n",
            "Epoch: 003, Loss: 2.6547, Train: 0.5446, Test: 0.5484\n",
            "Early stopping:  0.1498345993932378\n",
            "Epoch: 004, Loss: 2.4527, Train: 0.5618, Test: 0.5658\n",
            "Early stopping:  0.21737257712663113\n",
            "Epoch: 005, Loss: 2.2438, Train: 0.5950, Test: 0.5858\n",
            "Early stopping:  0.2849776676685221\n",
            "Epoch: 006, Loss: 2.0318, Train: 0.6218, Test: 0.6142\n",
            "Early stopping:  0.3170503363794623\n",
            "Epoch: 007, Loss: 1.8296, Train: 0.6384, Test: 0.6274\n",
            "Early stopping:  0.3274767517098677\n",
            "Epoch: 008, Loss: 1.6482, Train: 0.6422, Test: 0.6447\n",
            "Early stopping:  0.32001084084150005\n",
            "Epoch: 009, Loss: 1.4949, Train: 0.6463, Test: 0.6400\n",
            "Early stopping:  0.2980616492425498\n",
            "Epoch: 010, Loss: 1.3750, Train: 0.6517, Test: 0.6437\n",
            "Early stopping:  0.26191470238513215\n",
            "Epoch: 011, Loss: 1.2851, Train: 0.6607, Test: 0.6479\n",
            "Early stopping:  0.21734166375428096\n",
            "Epoch: 012, Loss: 1.2197, Train: 0.6630, Test: 0.6516\n",
            "Early stopping:  0.17092097729209207\n",
            "Epoch: 013, Loss: 1.1743, Train: 0.6691, Test: 0.6516\n",
            "Early stopping:  0.1280626134914338\n",
            "Epoch: 014, Loss: 1.1407, Train: 0.6732, Test: 0.6553\n",
            "Early stopping:  0.09332423419917778\n",
            "Epoch: 015, Loss: 1.1145, Train: 0.6766, Test: 0.6632\n",
            "Early stopping:  0.06754556762670398\n",
            "Epoch: 016, Loss: 1.0916, Train: 0.6818, Test: 0.6689\n",
            "Early stopping:  0.05044806601571917\n",
            "Epoch: 017, Loss: 1.0696, Train: 0.6870, Test: 0.6737\n",
            "Early stopping:  0.04101971407539372\n",
            "Epoch: 018, Loss: 1.0484, Train: 0.6938, Test: 0.6779\n",
            "Early stopping:  0.03630606085562599\n",
            "Epoch: 019, Loss: 1.0267, Train: 0.7003, Test: 0.6842\n",
            "Early stopping:  0.03459331604394732\n",
            "Epoch: 020, Loss: 1.0051, Train: 0.7054, Test: 0.6874\n",
            "Early stopping:  0.034138575286957684\n",
            "Epoch: 021, Loss: 0.9843, Train: 0.7125, Test: 0.6895\n",
            "Early stopping:  0.03381347784045233\n",
            "Epoch: 022, Loss: 0.9642, Train: 0.7191, Test: 0.6911\n",
            "Early stopping:  0.03334833823694975\n",
            "Epoch: 023, Loss: 0.9454, Train: 0.7245, Test: 0.6947\n",
            "Early stopping:  0.03219606046570571\n",
            "Epoch: 024, Loss: 0.9279, Train: 0.7280, Test: 0.7011\n",
            "Early stopping:  0.030585283402244173\n",
            "Epoch: 025, Loss: 0.9118, Train: 0.7312, Test: 0.7026\n",
            "Early stopping:  0.02869516178911834\n",
            "Epoch: 026, Loss: 0.8970, Train: 0.7380, Test: 0.7047\n",
            "Early stopping:  0.026592245597673748\n",
            "Epoch: 027, Loss: 0.8832, Train: 0.7416, Test: 0.7068\n",
            "Early stopping:  0.024607291599012544\n",
            "Epoch: 028, Loss: 0.8703, Train: 0.7443, Test: 0.7121\n",
            "Early stopping:  0.022777093085583153\n",
            "Epoch: 029, Loss: 0.8578, Train: 0.7493, Test: 0.7121\n",
            "Early stopping:  0.021302862349748358\n",
            "Epoch: 030, Loss: 0.8460, Train: 0.7518, Test: 0.7132\n",
            "Early stopping:  0.02013131436592991\n",
            "Epoch: 031, Loss: 0.8347, Train: 0.7561, Test: 0.7163\n",
            "Early stopping:  0.019172384669583552\n",
            "Epoch: 032, Loss: 0.8239, Train: 0.7601, Test: 0.7147\n",
            "Early stopping:  0.018319087576160728\n",
            "Epoch: 033, Loss: 0.8141, Train: 0.7638, Test: 0.7163\n",
            "Early stopping:  0.017332949871308644\n",
            "Epoch: 034, Loss: 0.8049, Train: 0.7676, Test: 0.7226\n",
            "Early stopping:  0.016271900326262925\n",
            "Epoch: 035, Loss: 0.7966, Train: 0.7717, Test: 0.7258\n",
            "Early stopping:  0.015073816959378792\n",
            "Epoch: 036, Loss: 0.7886, Train: 0.7730, Test: 0.7274\n",
            "Early stopping:  0.013949130194778003\n",
            "Epoch: 037, Loss: 0.7809, Train: 0.7755, Test: 0.7289\n",
            "Early stopping:  0.013079185782476986\n",
            "Epoch: 038, Loss: 0.7731, Train: 0.7792, Test: 0.7274\n",
            "Early stopping:  0.012550344179199396\n",
            "Epoch: 039, Loss: 0.7652, Train: 0.7816, Test: 0.7295\n",
            "Early stopping:  0.012370896927088102\n",
            "Epoch: 040, Loss: 0.7573, Train: 0.7857, Test: 0.7332\n",
            "Early stopping:  0.012377115886500882\n",
            "Epoch: 041, Loss: 0.7492, Train: 0.7883, Test: 0.7337\n",
            "Early stopping:  0.012521664347710173\n",
            "Epoch: 042, Loss: 0.7410, Train: 0.7922, Test: 0.7342\n",
            "Early stopping:  0.012690765351399812\n",
            "Epoch: 043, Loss: 0.7326, Train: 0.7946, Test: 0.7326\n",
            "Early stopping:  0.012893501851009069\n",
            "Epoch: 044, Loss: 0.7243, Train: 0.7984, Test: 0.7363\n",
            "Early stopping:  0.013069664113638371\n",
            "Epoch: 045, Loss: 0.7159, Train: 0.8004, Test: 0.7358\n",
            "Early stopping:  0.0131855985405747\n",
            "Epoch: 046, Loss: 0.7075, Train: 0.8018, Test: 0.7358\n",
            "Early stopping:  0.013247960529716102\n",
            "Epoch: 047, Loss: 0.6990, Train: 0.8051, Test: 0.7342\n",
            "Early stopping:  0.0132746027094465\n",
            "Epoch: 048, Loss: 0.6905, Train: 0.8066, Test: 0.7347\n",
            "Early stopping:  0.01332175402300176\n",
            "Epoch: 049, Loss: 0.6821, Train: 0.8095, Test: 0.7342\n",
            "Early stopping:  0.013343579799069633\n",
            "Epoch: 050, Loss: 0.6739, Train: 0.8118, Test: 0.7337\n",
            "Early stopping:  0.013296422959490677\n",
            "Epoch: 051, Loss: 0.6659, Train: 0.8142, Test: 0.7332\n",
            "Early stopping:  0.013124013480346362\n",
            "Epoch: 052, Loss: 0.6583, Train: 0.8175, Test: 0.7316\n",
            "Early stopping:  0.012780764225465173\n",
            "Epoch: 053, Loss: 0.6508, Train: 0.8176, Test: 0.7363\n",
            "Early stopping:  0.012362833072776847\n",
            "Epoch: 054, Loss: 0.6430, Train: 0.8205, Test: 0.7342\n",
            "Early stopping:  0.012140824046807396\n",
            "Epoch: 055, Loss: 0.6344, Train: 0.8229, Test: 0.7358\n",
            "Early stopping:  0.012370254617089505\n",
            "Epoch: 056, Loss: 0.6264, Train: 0.8270, Test: 0.7353\n",
            "Early stopping:  0.01268043501003039\n",
            "Epoch: 057, Loss: 0.6192, Train: 0.8303, Test: 0.7342\n",
            "Early stopping:  0.012637912888098133\n",
            "Epoch: 058, Loss: 0.6118, Train: 0.8322, Test: 0.7347\n",
            "Early stopping:  0.012286813356771594\n",
            "Epoch: 059, Loss: 0.6041, Train: 0.8354, Test: 0.7326\n",
            "Early stopping:  0.011885764067243761\n",
            "Epoch: 060, Loss: 0.5968, Train: 0.8378, Test: 0.7332\n",
            "Early stopping:  0.011757458754118776\n",
            "Epoch: 061, Loss: 0.5897, Train: 0.8408, Test: 0.7353\n",
            "Early stopping:  0.011698393212677762\n",
            "Epoch: 062, Loss: 0.5827, Train: 0.8432, Test: 0.7316\n",
            "Early stopping:  0.011468752463305356\n",
            "Epoch: 063, Loss: 0.5752, Train: 0.8493, Test: 0.7337\n",
            "Early stopping:  0.011365313307289371\n",
            "Epoch: 064, Loss: 0.5677, Train: 0.8479, Test: 0.7353\n",
            "Early stopping:  0.011491790490671196\n",
            "Epoch: 065, Loss: 0.5608, Train: 0.8537, Test: 0.7316\n",
            "Early stopping:  0.011511625036450058\n",
            "Epoch: 066, Loss: 0.5546, Train: 0.8520, Test: 0.7347\n",
            "Early stopping:  0.011180773323994468\n",
            "Epoch: 067, Loss: 0.5485, Train: 0.8562, Test: 0.7263\n",
            "Early stopping:  0.010524650314172755\n",
            "Epoch: 068, Loss: 0.5428, Train: 0.8538, Test: 0.7347\n",
            "Early stopping:  0.00982246409780903\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.78      0.82      0.80        91\n",
            "            CAD_and_CAM       0.85      0.86      0.85       106\n",
            "              Companies       0.67      0.42      0.51       105\n",
            "       Computer_Science       0.77      0.78      0.77       101\n",
            "            Consultants       0.57      0.73      0.64       112\n",
            "           Data_Formats       0.90      0.82      0.86        96\n",
            "    Data_Communications       0.72      0.81      0.77        86\n",
            "              Education       0.88      0.86      0.87       124\n",
            "               Graphics       0.81      0.88      0.84        97\n",
            "               Hardware       0.77      0.67      0.72        89\n",
            "               Internet       0.70      0.68      0.69       103\n",
            "       Mobile_Computing       0.75      0.84      0.79        93\n",
            "             Multimedia       0.66      0.65      0.66       100\n",
            "            Open_Source       0.69      0.81      0.74       100\n",
            "            Programming       0.61      0.58      0.60        89\n",
            "               Robotics       0.94      0.91      0.92       100\n",
            "               Security       0.78      0.86      0.82       104\n",
            "               Software       0.36      0.33      0.34        95\n",
            "                Systems       0.71      0.61      0.66       109\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.73      0.73      0.73      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9472, Train: 0.3857, Test: 0.3674\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8102, Train: 0.4100, Test: 0.3968\n",
            "Early stopping:  0.09690909171806887\n",
            "Epoch: 003, Loss: 2.6376, Train: 0.4862, Test: 0.4758\n",
            "Early stopping:  0.1551868153779312\n",
            "Epoch: 004, Loss: 2.4369, Train: 0.5512, Test: 0.5442\n",
            "Early stopping:  0.2207235803184367\n",
            "Epoch: 005, Loss: 2.2273, Train: 0.5899, Test: 0.5758\n",
            "Early stopping:  0.2876625727444257\n",
            "Epoch: 006, Loss: 2.0207, Train: 0.6151, Test: 0.6021\n",
            "Early stopping:  0.3147435914749436\n",
            "Epoch: 007, Loss: 1.8254, Train: 0.6305, Test: 0.6226\n",
            "Early stopping:  0.3226418936111478\n",
            "Epoch: 008, Loss: 1.6503, Train: 0.6376, Test: 0.6247\n",
            "Early stopping:  0.3124749258430127\n",
            "Epoch: 009, Loss: 1.5016, Train: 0.6453, Test: 0.6337\n",
            "Early stopping:  0.28863846319145736\n",
            "Epoch: 010, Loss: 1.3827, Train: 0.6516, Test: 0.6468\n",
            "Early stopping:  0.2540832886446053\n",
            "Epoch: 011, Loss: 1.2921, Train: 0.6603, Test: 0.6579\n",
            "Early stopping:  0.2126189508951778\n",
            "Epoch: 012, Loss: 1.2260, Train: 0.6649, Test: 0.6621\n",
            "Early stopping:  0.1692600885928723\n",
            "Epoch: 013, Loss: 1.1782, Train: 0.6684, Test: 0.6595\n",
            "Early stopping:  0.12897969019154495\n",
            "Epoch: 014, Loss: 1.1436, Train: 0.6732, Test: 0.6637\n",
            "Early stopping:  0.09524844870982796\n",
            "Epoch: 015, Loss: 1.1163, Train: 0.6772, Test: 0.6705\n",
            "Early stopping:  0.06972924329199104\n",
            "Epoch: 016, Loss: 1.0927, Train: 0.6854, Test: 0.6758\n",
            "Early stopping:  0.05249441249143064\n",
            "Epoch: 017, Loss: 1.0702, Train: 0.6936, Test: 0.6763\n",
            "Early stopping:  0.04237852887176429\n",
            "Epoch: 018, Loss: 1.0480, Train: 0.6987, Test: 0.6779\n",
            "Early stopping:  0.03756333035241403\n",
            "Epoch: 019, Loss: 1.0257, Train: 0.7057, Test: 0.6826\n",
            "Early stopping:  0.03572923857403476\n",
            "Epoch: 020, Loss: 1.0037, Train: 0.7107, Test: 0.6879\n",
            "Early stopping:  0.03519982941127694\n",
            "Epoch: 021, Loss: 0.9823, Train: 0.7171, Test: 0.6921\n",
            "Early stopping:  0.03478756032166093\n",
            "Epoch: 022, Loss: 0.9622, Train: 0.7225, Test: 0.6900\n",
            "Early stopping:  0.034007238518337785\n",
            "Epoch: 023, Loss: 0.9434, Train: 0.7268, Test: 0.6911\n",
            "Early stopping:  0.03261110472652402\n",
            "Epoch: 024, Loss: 0.9258, Train: 0.7322, Test: 0.6932\n",
            "Early stopping:  0.030820868526321037\n",
            "Epoch: 025, Loss: 0.9093, Train: 0.7376, Test: 0.6968\n",
            "Early stopping:  0.028874690272654494\n",
            "Epoch: 026, Loss: 0.8936, Train: 0.7422, Test: 0.6995\n",
            "Early stopping:  0.02709792379969108\n",
            "Epoch: 027, Loss: 0.8790, Train: 0.7462, Test: 0.7005\n",
            "Early stopping:  0.025453498160812455\n",
            "Epoch: 028, Loss: 0.8650, Train: 0.7521, Test: 0.7016\n",
            "Early stopping:  0.024006684030196953\n",
            "Epoch: 029, Loss: 0.8518, Train: 0.7547, Test: 0.7026\n",
            "Early stopping:  0.022686809641386287\n",
            "Epoch: 030, Loss: 0.8394, Train: 0.7574, Test: 0.7053\n",
            "Early stopping:  0.021438255502592103\n",
            "Epoch: 031, Loss: 0.8276, Train: 0.7628, Test: 0.7079\n",
            "Early stopping:  0.020330590625647866\n",
            "Epoch: 032, Loss: 0.8165, Train: 0.7679, Test: 0.7074\n",
            "Early stopping:  0.01920239236706225\n",
            "Epoch: 033, Loss: 0.8063, Train: 0.7711, Test: 0.7068\n",
            "Early stopping:  0.018025688740113906\n",
            "Epoch: 034, Loss: 0.7968, Train: 0.7750, Test: 0.7074\n",
            "Early stopping:  0.01684628572654949\n",
            "Epoch: 035, Loss: 0.7877, Train: 0.7783, Test: 0.7095\n",
            "Early stopping:  0.015750461926691925\n",
            "Epoch: 036, Loss: 0.7788, Train: 0.7811, Test: 0.7126\n",
            "Early stopping:  0.014875038843201708\n",
            "Epoch: 037, Loss: 0.7701, Train: 0.7847, Test: 0.7121\n",
            "Early stopping:  0.014309370637617107\n",
            "Epoch: 038, Loss: 0.7615, Train: 0.7891, Test: 0.7153\n",
            "Early stopping:  0.013945176540287502\n",
            "Epoch: 039, Loss: 0.7528, Train: 0.7912, Test: 0.7168\n",
            "Early stopping:  0.013768583651881175\n",
            "Epoch: 040, Loss: 0.7440, Train: 0.7926, Test: 0.7174\n",
            "Early stopping:  0.013748708180780101\n",
            "Epoch: 041, Loss: 0.7351, Train: 0.7941, Test: 0.7168\n",
            "Early stopping:  0.013837836285243604\n",
            "Epoch: 042, Loss: 0.7260, Train: 0.7980, Test: 0.7189\n",
            "Early stopping:  0.01400315118888029\n",
            "Epoch: 043, Loss: 0.7171, Train: 0.8005, Test: 0.7216\n",
            "Early stopping:  0.014119227762509323\n",
            "Epoch: 044, Loss: 0.7082, Train: 0.8018, Test: 0.7168\n",
            "Early stopping:  0.014171447342972517\n",
            "Epoch: 045, Loss: 0.6992, Train: 0.8029, Test: 0.7205\n",
            "Early stopping:  0.014160285444174232\n",
            "Epoch: 046, Loss: 0.6906, Train: 0.8061, Test: 0.7179\n",
            "Early stopping:  0.014028044198062651\n",
            "Epoch: 047, Loss: 0.6827, Train: 0.8064, Test: 0.7237\n",
            "Early stopping:  0.013656046234349128\n",
            "Epoch: 048, Loss: 0.6751, Train: 0.8087, Test: 0.7200\n",
            "Early stopping:  0.013085896636331102\n",
            "Epoch: 049, Loss: 0.6660, Train: 0.8136, Test: 0.7211\n",
            "Early stopping:  0.012968973744076392\n",
            "Epoch: 050, Loss: 0.6551, Train: 0.8155, Test: 0.7216\n",
            "Early stopping:  0.013896176007500654\n",
            "Epoch: 051, Loss: 0.6482, Train: 0.8171, Test: 0.7237\n",
            "Early stopping:  0.014110007159802002\n",
            "Epoch: 052, Loss: 0.6412, Train: 0.8201, Test: 0.7232\n",
            "Early stopping:  0.013603818460281572\n",
            "Epoch: 053, Loss: 0.6307, Train: 0.8228, Test: 0.7232\n",
            "Early stopping:  0.013414060179717075\n",
            "Epoch: 054, Loss: 0.6236, Train: 0.8237, Test: 0.7242\n",
            "Early stopping:  0.01276462397620781\n",
            "Epoch: 055, Loss: 0.6168, Train: 0.8287, Test: 0.7258\n",
            "Early stopping:  0.012740455541123956\n",
            "Epoch: 056, Loss: 0.6075, Train: 0.8307, Test: 0.7237\n",
            "Early stopping:  0.012880621639592768\n",
            "Epoch: 057, Loss: 0.6004, Train: 0.8321, Test: 0.7226\n",
            "Early stopping:  0.012148873349657665\n",
            "Epoch: 058, Loss: 0.5934, Train: 0.8355, Test: 0.7211\n",
            "Early stopping:  0.01214933135094963\n",
            "Epoch: 059, Loss: 0.5852, Train: 0.8374, Test: 0.7216\n",
            "Early stopping:  0.01223196776106957\n",
            "Epoch: 060, Loss: 0.5780, Train: 0.8404, Test: 0.7211\n",
            "Early stopping:  0.011761213218831123\n",
            "Epoch: 061, Loss: 0.5707, Train: 0.8430, Test: 0.7258\n",
            "Early stopping:  0.01184541931243228\n",
            "Epoch: 062, Loss: 0.5634, Train: 0.8479, Test: 0.7221\n",
            "Early stopping:  0.011791979971380774\n",
            "Epoch: 063, Loss: 0.5561, Train: 0.8495, Test: 0.7216\n",
            "Early stopping:  0.011517290904650356\n",
            "Epoch: 064, Loss: 0.5489, Train: 0.8534, Test: 0.7253\n",
            "Early stopping:  0.011505616504062517\n",
            "Epoch: 065, Loss: 0.5423, Train: 0.8514, Test: 0.7200\n",
            "Early stopping:  0.011265747836079103\n",
            "Epoch: 066, Loss: 0.5352, Train: 0.8566, Test: 0.7200\n",
            "Early stopping:  0.011086752805371905\n",
            "Epoch: 067, Loss: 0.5290, Train: 0.8574, Test: 0.7216\n",
            "Early stopping:  0.010717700773862714\n",
            "Epoch: 068, Loss: 0.5242, Train: 0.8578, Test: 0.7184\n",
            "Early stopping:  0.009916765441129226\n",
            "PREDICTIONS -> tensor([14,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.88      0.82      0.85       105\n",
            "            CAD_and_CAM       0.80      0.78      0.79        95\n",
            "              Companies       0.57      0.62      0.59        86\n",
            "       Computer_Science       0.81      0.78      0.80        87\n",
            "            Consultants       0.61      0.56      0.59        91\n",
            "           Data_Formats       0.81      0.82      0.82       105\n",
            "    Data_Communications       0.73      0.73      0.73       114\n",
            "              Education       0.87      0.92      0.89       106\n",
            "               Graphics       0.81      0.89      0.85       113\n",
            "               Hardware       0.65      0.71      0.68        90\n",
            "               Internet       0.66      0.61      0.63       109\n",
            "       Mobile_Computing       0.69      0.81      0.75        85\n",
            "             Multimedia       0.69      0.75      0.72        91\n",
            "            Open_Source       0.72      0.75      0.73        96\n",
            "            Programming       0.43      0.52      0.47        86\n",
            "               Robotics       0.91      0.94      0.92       102\n",
            "               Security       0.84      0.84      0.84       117\n",
            "               Software       0.40      0.32      0.36       108\n",
            "                Systems       0.65      0.46      0.54       114\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.71      0.72      0.71      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9452, Train: 0.4041, Test: 0.3837\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8044, Train: 0.4733, Test: 0.4595\n",
            "Early stopping:  0.09951697011652895\n",
            "Epoch: 003, Loss: 2.6282, Train: 0.5403, Test: 0.5284\n",
            "Early stopping:  0.15878519143719774\n",
            "Epoch: 004, Loss: 2.4264, Train: 0.5996, Test: 0.5968\n",
            "Early stopping:  0.22435099359797867\n",
            "Epoch: 005, Loss: 2.2160, Train: 0.6217, Test: 0.6168\n",
            "Early stopping:  0.2912209973541165\n",
            "Epoch: 006, Loss: 2.0099, Train: 0.6358, Test: 0.6253\n",
            "Early stopping:  0.31660450445666183\n",
            "Epoch: 007, Loss: 1.8146, Train: 0.6463, Test: 0.6395\n",
            "Early stopping:  0.3231755123069772\n",
            "Epoch: 008, Loss: 1.6377, Train: 0.6512, Test: 0.6474\n",
            "Early stopping:  0.3130569756154879\n",
            "Epoch: 009, Loss: 1.4890, Train: 0.6538, Test: 0.6453\n",
            "Early stopping:  0.2893016934135734\n",
            "Epoch: 010, Loss: 1.3704, Train: 0.6578, Test: 0.6468\n",
            "Early stopping:  0.2548774763849439\n",
            "Epoch: 011, Loss: 1.2789, Train: 0.6624, Test: 0.6537\n",
            "Early stopping:  0.21337413775776423\n",
            "Epoch: 012, Loss: 1.2133, Train: 0.6659, Test: 0.6626\n",
            "Early stopping:  0.16941569976842724\n",
            "Epoch: 013, Loss: 1.1667, Train: 0.6674, Test: 0.6600\n",
            "Early stopping:  0.12878876831286248\n",
            "Epoch: 014, Loss: 1.1335, Train: 0.6726, Test: 0.6642\n",
            "Early stopping:  0.09442012499039118\n",
            "Epoch: 015, Loss: 1.1094, Train: 0.6772, Test: 0.6637\n",
            "Early stopping:  0.0674706333938903\n",
            "Epoch: 016, Loss: 1.0890, Train: 0.6832, Test: 0.6674\n",
            "Early stopping:  0.049072803358137386\n",
            "Epoch: 017, Loss: 1.0692, Train: 0.6907, Test: 0.6689\n",
            "Early stopping:  0.03810589150844805\n",
            "Epoch: 018, Loss: 1.0493, Train: 0.6974, Test: 0.6726\n",
            "Early stopping:  0.03302005449516394\n",
            "Epoch: 019, Loss: 1.0278, Train: 0.7028, Test: 0.6805\n",
            "Early stopping:  0.03206998586626933\n",
            "Epoch: 020, Loss: 1.0059, Train: 0.7087, Test: 0.6821\n",
            "Early stopping:  0.032839875653204\n",
            "Epoch: 021, Loss: 0.9843, Train: 0.7155, Test: 0.6863\n",
            "Early stopping:  0.033704063048400026\n",
            "Epoch: 022, Loss: 0.9634, Train: 0.7205, Test: 0.6863\n",
            "Early stopping:  0.03403243050859932\n",
            "Epoch: 023, Loss: 0.9436, Train: 0.7261, Test: 0.6905\n",
            "Early stopping:  0.03337241354914112\n",
            "Epoch: 024, Loss: 0.9249, Train: 0.7307, Test: 0.6937\n",
            "Early stopping:  0.03205212853221451\n",
            "Epoch: 025, Loss: 0.9081, Train: 0.7334, Test: 0.6963\n",
            "Early stopping:  0.030207099007834855\n",
            "Epoch: 026, Loss: 0.8926, Train: 0.7396, Test: 0.6974\n",
            "Early stopping:  0.028042987929942764\n",
            "Epoch: 027, Loss: 0.8785, Train: 0.7445, Test: 0.6979\n",
            "Early stopping:  0.025732871964855686\n",
            "Epoch: 028, Loss: 0.8656, Train: 0.7470, Test: 0.7047\n",
            "Early stopping:  0.023476327226596566\n",
            "Epoch: 029, Loss: 0.8536, Train: 0.7525, Test: 0.7074\n",
            "Early stopping:  0.0215389435856333\n",
            "Epoch: 030, Loss: 0.8424, Train: 0.7597, Test: 0.7095\n",
            "Early stopping:  0.019825060369003302\n",
            "Epoch: 031, Loss: 0.8316, Train: 0.7649, Test: 0.7105\n",
            "Early stopping:  0.018511429380847554\n",
            "Epoch: 032, Loss: 0.8215, Train: 0.7689, Test: 0.7142\n",
            "Early stopping:  0.017447468350374123\n",
            "Epoch: 033, Loss: 0.8119, Train: 0.7741, Test: 0.7189\n",
            "Early stopping:  0.01650347452870618\n",
            "Epoch: 034, Loss: 0.8027, Train: 0.7784, Test: 0.7184\n",
            "Early stopping:  0.01569379689064044\n",
            "Epoch: 035, Loss: 0.7939, Train: 0.7796, Test: 0.7195\n",
            "Early stopping:  0.014896533153946031\n",
            "Epoch: 036, Loss: 0.7854, Train: 0.7828, Test: 0.7189\n",
            "Early stopping:  0.014237490842215412\n",
            "Epoch: 037, Loss: 0.7771, Train: 0.7837, Test: 0.7200\n",
            "Early stopping:  0.013738611452477852\n",
            "Epoch: 038, Loss: 0.7688, Train: 0.7864, Test: 0.7226\n",
            "Early stopping:  0.013389247056969126\n",
            "Epoch: 039, Loss: 0.7603, Train: 0.7879, Test: 0.7247\n",
            "Early stopping:  0.013254624187540212\n",
            "Epoch: 040, Loss: 0.7520, Train: 0.7912, Test: 0.7268\n",
            "Early stopping:  0.013223745882325107\n",
            "Epoch: 041, Loss: 0.7436, Train: 0.7937, Test: 0.7253\n",
            "Early stopping:  0.013234941235825505\n",
            "Epoch: 042, Loss: 0.7352, Train: 0.7943, Test: 0.7258\n",
            "Early stopping:  0.013270754241269648\n",
            "Epoch: 043, Loss: 0.7269, Train: 0.7971, Test: 0.7289\n",
            "Early stopping:  0.013241715591403587\n",
            "Epoch: 044, Loss: 0.7186, Train: 0.7992, Test: 0.7300\n",
            "Early stopping:  0.0132080397943351\n",
            "Epoch: 045, Loss: 0.7103, Train: 0.8009, Test: 0.7289\n",
            "Early stopping:  0.01314151331446169\n",
            "Epoch: 046, Loss: 0.7021, Train: 0.8036, Test: 0.7295\n",
            "Early stopping:  0.013060052124382687\n",
            "Epoch: 047, Loss: 0.6940, Train: 0.8050, Test: 0.7289\n",
            "Early stopping:  0.013003859874281086\n",
            "Epoch: 048, Loss: 0.6859, Train: 0.8084, Test: 0.7279\n",
            "Early stopping:  0.012917406219649971\n",
            "Epoch: 049, Loss: 0.6779, Train: 0.8113, Test: 0.7284\n",
            "Early stopping:  0.012824732126188253\n",
            "Epoch: 050, Loss: 0.6700, Train: 0.8137, Test: 0.7300\n",
            "Early stopping:  0.012709242089439935\n",
            "Epoch: 051, Loss: 0.6621, Train: 0.8158, Test: 0.7332\n",
            "Early stopping:  0.012586317351605229\n",
            "Epoch: 052, Loss: 0.6544, Train: 0.8180, Test: 0.7321\n",
            "Early stopping:  0.012470050056697081\n",
            "Epoch: 053, Loss: 0.6467, Train: 0.8193, Test: 0.7326\n",
            "Early stopping:  0.01232765266130841\n",
            "Epoch: 054, Loss: 0.6395, Train: 0.8195, Test: 0.7326\n",
            "Early stopping:  0.012069864820136703\n",
            "Epoch: 055, Loss: 0.6333, Train: 0.8191, Test: 0.7311\n",
            "Early stopping:  0.011479772141060055\n",
            "Epoch: 056, Loss: 0.6297, Train: 0.8234, Test: 0.7295\n",
            "Early stopping:  0.010012474234940626\n",
            "Epoch: 057, Loss: 0.6220, Train: 0.8257, Test: 0.7305\n",
            "Early stopping:  0.009438945341118334\n",
            "PREDICTIONS -> tensor([ 1,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.92      0.82      0.87       106\n",
            "            CAD_and_CAM       0.76      0.73      0.74        92\n",
            "              Companies       0.55      0.43      0.48        98\n",
            "       Computer_Science       0.80      0.91      0.85        99\n",
            "            Consultants       0.62      0.66      0.64       122\n",
            "           Data_Formats       0.81      0.80      0.80       114\n",
            "    Data_Communications       0.78      0.78      0.78       112\n",
            "              Education       0.86      0.89      0.88       103\n",
            "               Graphics       0.83      0.93      0.87       109\n",
            "               Hardware       0.64      0.59      0.61        97\n",
            "               Internet       0.77      0.72      0.74        99\n",
            "       Mobile_Computing       0.81      0.81      0.81       114\n",
            "             Multimedia       0.72      0.75      0.73        95\n",
            "            Open_Source       0.68      0.69      0.69        84\n",
            "            Programming       0.58      0.70      0.64        98\n",
            "               Robotics       0.92      0.96      0.94        91\n",
            "               Security       0.73      0.82      0.77        85\n",
            "               Software       0.48      0.34      0.40       102\n",
            "                Systems       0.48      0.51      0.49        80\n",
            "\n",
            "               accuracy                           0.73      1900\n",
            "              macro avg       0.72      0.73      0.72      1900\n",
            "           weighted avg       0.73      0.73      0.73      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9461, Train: 0.4150, Test: 0.4253\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8237, Train: 0.5186, Test: 0.5332\n",
            "Early stopping:  0.08658125937156534\n",
            "Epoch: 003, Loss: 2.6605, Train: 0.5237, Test: 0.5389\n",
            "Early stopping:  0.14329819125533585\n",
            "Epoch: 004, Loss: 2.4665, Train: 0.5541, Test: 0.5637\n",
            "Early stopping:  0.2078530120840532\n",
            "Epoch: 005, Loss: 2.2611, Train: 0.5938, Test: 0.6026\n",
            "Early stopping:  0.27441515764101415\n",
            "Epoch: 006, Loss: 2.0546, Train: 0.6234, Test: 0.6284\n",
            "Early stopping:  0.3066992331957564\n",
            "Epoch: 007, Loss: 1.8575, Train: 0.6354, Test: 0.6400\n",
            "Early stopping:  0.3190785136513894\n",
            "Epoch: 008, Loss: 1.6814, Train: 0.6426, Test: 0.6405\n",
            "Early stopping:  0.31224531780464293\n",
            "Epoch: 009, Loss: 1.5312, Train: 0.6463, Test: 0.6405\n",
            "Early stopping:  0.29038234612907676\n",
            "Epoch: 010, Loss: 1.4085, Train: 0.6470, Test: 0.6405\n",
            "Early stopping:  0.2569562609056735\n",
            "Epoch: 011, Loss: 1.3145, Train: 0.6537, Test: 0.6453\n",
            "Early stopping:  0.21637789602942548\n",
            "Epoch: 012, Loss: 1.2443, Train: 0.6607, Test: 0.6558\n",
            "Early stopping:  0.17428790338746528\n",
            "Epoch: 013, Loss: 1.1925, Train: 0.6651, Test: 0.6600\n",
            "Early stopping:  0.13488970071357073\n",
            "Epoch: 014, Loss: 1.1532, Train: 0.6709, Test: 0.6584\n",
            "Early stopping:  0.10150260405531636\n",
            "Epoch: 015, Loss: 1.1224, Train: 0.6770, Test: 0.6589\n",
            "Early stopping:  0.07617106056825199\n",
            "Epoch: 016, Loss: 1.0958, Train: 0.6846, Test: 0.6663\n",
            "Early stopping:  0.058618233047102675\n",
            "Epoch: 017, Loss: 1.0723, Train: 0.6912, Test: 0.6726\n",
            "Early stopping:  0.047343513724830665\n",
            "Epoch: 018, Loss: 1.0505, Train: 0.6964, Test: 0.6758\n",
            "Early stopping:  0.04049445411525504\n",
            "Epoch: 019, Loss: 1.0293, Train: 0.7033, Test: 0.6779\n",
            "Early stopping:  0.03661819810192755\n",
            "Epoch: 020, Loss: 1.0085, Train: 0.7093, Test: 0.6821\n",
            "Early stopping:  0.034396485672262736\n",
            "Epoch: 021, Loss: 0.9881, Train: 0.7130, Test: 0.6847\n",
            "Early stopping:  0.03327471826726108\n",
            "Epoch: 022, Loss: 0.9682, Train: 0.7172, Test: 0.6837\n",
            "Early stopping:  0.03253991381836485\n",
            "Epoch: 023, Loss: 0.9492, Train: 0.7232, Test: 0.6889\n",
            "Early stopping:  0.03171867882877413\n",
            "Epoch: 024, Loss: 0.9312, Train: 0.7284, Test: 0.6942\n",
            "Early stopping:  0.0306018956359531\n",
            "Epoch: 025, Loss: 0.9143, Train: 0.7325, Test: 0.6953\n",
            "Early stopping:  0.02921616396962442\n",
            "Epoch: 026, Loss: 0.8988, Train: 0.7380, Test: 0.6921\n",
            "Early stopping:  0.027494702856909352\n",
            "Epoch: 027, Loss: 0.8847, Train: 0.7428, Test: 0.6958\n",
            "Early stopping:  0.025535584380620622\n",
            "Epoch: 028, Loss: 0.8715, Train: 0.7474, Test: 0.7016\n",
            "Early stopping:  0.0235830912056937\n",
            "Epoch: 029, Loss: 0.8587, Train: 0.7524, Test: 0.7053\n",
            "Early stopping:  0.02190302250361358\n",
            "Epoch: 030, Loss: 0.8465, Train: 0.7563, Test: 0.7063\n",
            "Early stopping:  0.02065964478722831\n",
            "Epoch: 031, Loss: 0.8347, Train: 0.7617, Test: 0.7100\n",
            "Early stopping:  0.019781215661948844\n",
            "Epoch: 032, Loss: 0.8234, Train: 0.7662, Test: 0.7089\n",
            "Early stopping:  0.019006876871602497\n",
            "Epoch: 033, Loss: 0.8129, Train: 0.7683, Test: 0.7121\n",
            "Early stopping:  0.018160616197901738\n",
            "Epoch: 034, Loss: 0.8030, Train: 0.7725, Test: 0.7153\n",
            "Early stopping:  0.01722715132040638\n",
            "Epoch: 035, Loss: 0.7939, Train: 0.7747, Test: 0.7195\n",
            "Early stopping:  0.01614259962836199\n",
            "Epoch: 036, Loss: 0.7853, Train: 0.7783, Test: 0.7174\n",
            "Early stopping:  0.015053485700545412\n",
            "Epoch: 037, Loss: 0.7772, Train: 0.7800, Test: 0.7174\n",
            "Early stopping:  0.01409103685078652\n",
            "Epoch: 038, Loss: 0.7693, Train: 0.7832, Test: 0.7158\n",
            "Early stopping:  0.013321815845124674\n",
            "Epoch: 039, Loss: 0.7614, Train: 0.7853, Test: 0.7142\n",
            "Early stopping:  0.012817021948568891\n",
            "Epoch: 040, Loss: 0.7535, Train: 0.7867, Test: 0.7105\n",
            "Early stopping:  0.012560788758734323\n",
            "Epoch: 041, Loss: 0.7454, Train: 0.7897, Test: 0.7137\n",
            "Early stopping:  0.012538351457067328\n",
            "Epoch: 042, Loss: 0.7370, Train: 0.7925, Test: 0.7168\n",
            "Early stopping:  0.012719551312960282\n",
            "Epoch: 043, Loss: 0.7286, Train: 0.7961, Test: 0.7153\n",
            "Early stopping:  0.01297167546957695\n",
            "Epoch: 044, Loss: 0.7202, Train: 0.8004, Test: 0.7184\n",
            "Early stopping:  0.013191960705091248\n",
            "Epoch: 045, Loss: 0.7117, Train: 0.8039, Test: 0.7179\n",
            "Early stopping:  0.013322359420289222\n",
            "Epoch: 046, Loss: 0.7033, Train: 0.8057, Test: 0.7200\n",
            "Early stopping:  0.013348060191018656\n",
            "Epoch: 047, Loss: 0.6949, Train: 0.8089, Test: 0.7205\n",
            "Early stopping:  0.013330402457877545\n",
            "Epoch: 048, Loss: 0.6866, Train: 0.8084, Test: 0.7242\n",
            "Early stopping:  0.013275856623706115\n",
            "Epoch: 049, Loss: 0.6784, Train: 0.8100, Test: 0.7195\n",
            "Early stopping:  0.013177143610871078\n",
            "Epoch: 050, Loss: 0.6704, Train: 0.8109, Test: 0.7205\n",
            "Early stopping:  0.013003075879128188\n",
            "Epoch: 051, Loss: 0.6626, Train: 0.8139, Test: 0.7205\n",
            "Early stopping:  0.012755047095842747\n",
            "Epoch: 052, Loss: 0.6547, Train: 0.8167, Test: 0.7179\n",
            "Early stopping:  0.012565888243557696\n",
            "Epoch: 053, Loss: 0.6466, Train: 0.8192, Test: 0.7216\n",
            "Early stopping:  0.01253430893162882\n",
            "Epoch: 054, Loss: 0.6384, Train: 0.8217, Test: 0.7200\n",
            "Early stopping:  0.0126647797813801\n",
            "Epoch: 055, Loss: 0.6307, Train: 0.8224, Test: 0.7232\n",
            "Early stopping:  0.012695047747879174\n",
            "Epoch: 056, Loss: 0.6234, Train: 0.8255, Test: 0.7221\n",
            "Early stopping:  0.012420740013509627\n",
            "Epoch: 057, Loss: 0.6166, Train: 0.8270, Test: 0.7184\n",
            "Early stopping:  0.011876958721100398\n",
            "Epoch: 058, Loss: 0.6098, Train: 0.8300, Test: 0.7205\n",
            "Early stopping:  0.011285289152677394\n",
            "Epoch: 059, Loss: 0.6028, Train: 0.8309, Test: 0.7179\n",
            "Early stopping:  0.010981086457428367\n",
            "Epoch: 060, Loss: 0.5957, Train: 0.8349, Test: 0.7221\n",
            "Early stopping:  0.010942984116797838\n",
            "Epoch: 061, Loss: 0.5888, Train: 0.8367, Test: 0.7189\n",
            "Early stopping:  0.011009827083394427\n",
            "Epoch: 062, Loss: 0.5824, Train: 0.8397, Test: 0.7221\n",
            "Early stopping:  0.01087789863104184\n",
            "Epoch: 063, Loss: 0.5757, Train: 0.8438, Test: 0.7205\n",
            "Early stopping:  0.010662707763973944\n",
            "Epoch: 064, Loss: 0.5686, Train: 0.8442, Test: 0.7211\n",
            "Early stopping:  0.010625867635154931\n",
            "Epoch: 065, Loss: 0.5614, Train: 0.8483, Test: 0.7200\n",
            "Early stopping:  0.01083343264657807\n",
            "Epoch: 066, Loss: 0.5548, Train: 0.8513, Test: 0.7184\n",
            "Early stopping:  0.01100280275432812\n",
            "Epoch: 067, Loss: 0.5485, Train: 0.8533, Test: 0.7216\n",
            "Early stopping:  0.010800414799312418\n",
            "Epoch: 068, Loss: 0.5420, Train: 0.8554, Test: 0.7184\n",
            "Early stopping:  0.010461886412637462\n",
            "Epoch: 069, Loss: 0.5354, Train: 0.8593, Test: 0.7174\n",
            "Early stopping:  0.010222567980191043\n",
            "Epoch: 070, Loss: 0.5293, Train: 0.8588, Test: 0.7163\n",
            "Early stopping:  0.010122468254610729\n",
            "Epoch: 071, Loss: 0.5241, Train: 0.8626, Test: 0.7142\n",
            "Early stopping:  0.009738946065227612\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.82      0.80      0.81        94\n",
            "            CAD_and_CAM       0.78      0.73      0.75       105\n",
            "              Companies       0.65      0.49      0.56       102\n",
            "       Computer_Science       0.78      0.82      0.80       102\n",
            "            Consultants       0.65      0.64      0.65       120\n",
            "           Data_Formats       0.78      0.79      0.78        98\n",
            "    Data_Communications       0.65      0.77      0.70        92\n",
            "              Education       0.81      0.90      0.86        92\n",
            "               Graphics       0.90      0.98      0.94       111\n",
            "               Hardware       0.74      0.64      0.68       105\n",
            "               Internet       0.66      0.66      0.66       102\n",
            "       Mobile_Computing       0.88      0.77      0.82       112\n",
            "             Multimedia       0.73      0.71      0.72       106\n",
            "            Open_Source       0.69      0.82      0.75        96\n",
            "            Programming       0.55      0.48      0.51        88\n",
            "               Robotics       0.89      0.89      0.89       104\n",
            "               Security       0.79      0.78      0.78        89\n",
            "               Software       0.29      0.37      0.33        94\n",
            "                Systems       0.52      0.47      0.49        88\n",
            "\n",
            "               accuracy                           0.71      1900\n",
            "              macro avg       0.71      0.71      0.71      1900\n",
            "           weighted avg       0.72      0.71      0.71      1900\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.9497, Train: 0.3920, Test: 0.3958\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.8142, Train: 0.5561, Test: 0.5553\n",
            "Early stopping:  0.095803158413143\n",
            "Epoch: 003, Loss: 2.6451, Train: 0.5791, Test: 0.5958\n",
            "Early stopping:  0.15257526519635942\n",
            "Epoch: 004, Loss: 2.4534, Train: 0.5903, Test: 0.5968\n",
            "Early stopping:  0.21465660721693972\n",
            "Epoch: 005, Loss: 2.2535, Train: 0.6075, Test: 0.6132\n",
            "Early stopping:  0.2779759846966889\n",
            "Epoch: 006, Loss: 2.0497, Train: 0.6205, Test: 0.6289\n",
            "Early stopping:  0.303866336801959\n",
            "Epoch: 007, Loss: 1.8517, Train: 0.6304, Test: 0.6342\n",
            "Early stopping:  0.31473004133375937\n",
            "Epoch: 008, Loss: 1.6716, Train: 0.6380, Test: 0.6458\n",
            "Early stopping:  0.310808814047786\n",
            "Epoch: 009, Loss: 1.5195, Train: 0.6428, Test: 0.6489\n",
            "Early stopping:  0.29234792323143466\n",
            "Epoch: 010, Loss: 1.3979, Train: 0.6507, Test: 0.6463\n",
            "Early stopping:  0.25979823044957684\n",
            "Epoch: 011, Loss: 1.3040, Train: 0.6549, Test: 0.6516\n",
            "Early stopping:  0.21817984968653834\n",
            "Epoch: 012, Loss: 1.2360, Train: 0.6613, Test: 0.6563\n",
            "Early stopping:  0.1738045814514877\n",
            "Epoch: 013, Loss: 1.1851, Train: 0.6672, Test: 0.6679\n",
            "Early stopping:  0.13323369184123923\n",
            "Epoch: 014, Loss: 1.1482, Train: 0.6761, Test: 0.6674\n",
            "Early stopping:  0.0993148246814409\n",
            "Epoch: 015, Loss: 1.1197, Train: 0.6803, Test: 0.6663\n",
            "Early stopping:  0.0732356339266611\n",
            "Epoch: 016, Loss: 1.0960, Train: 0.6845, Test: 0.6711\n",
            "Early stopping:  0.05527564393627781\n",
            "Epoch: 017, Loss: 1.0744, Train: 0.6889, Test: 0.6763\n",
            "Early stopping:  0.04353874667124478\n",
            "Epoch: 018, Loss: 1.0534, Train: 0.6938, Test: 0.6821\n",
            "Early stopping:  0.037236423386699136\n",
            "Epoch: 019, Loss: 1.0321, Train: 0.7012, Test: 0.6816\n",
            "Early stopping:  0.0344627080782938\n",
            "Epoch: 020, Loss: 1.0103, Train: 0.7068, Test: 0.6858\n",
            "Early stopping:  0.033794323265612186\n",
            "Epoch: 021, Loss: 0.9886, Train: 0.7136, Test: 0.6889\n",
            "Early stopping:  0.033959294029138394\n",
            "Epoch: 022, Loss: 0.9676, Train: 0.7179, Test: 0.6905\n",
            "Early stopping:  0.03401632611742777\n",
            "Epoch: 023, Loss: 0.9480, Train: 0.7225, Test: 0.6921\n",
            "Early stopping:  0.03334957553751022\n",
            "Epoch: 024, Loss: 0.9295, Train: 0.7278, Test: 0.6953\n",
            "Early stopping:  0.03197591520319335\n",
            "Epoch: 025, Loss: 0.9129, Train: 0.7333, Test: 0.6989\n",
            "Early stopping:  0.02998255086781049\n",
            "Epoch: 026, Loss: 0.8975, Train: 0.7380, Test: 0.7026\n",
            "Early stopping:  0.02774340281859115\n",
            "Epoch: 027, Loss: 0.8836, Train: 0.7439, Test: 0.7037\n",
            "Early stopping:  0.025467880510651842\n",
            "Epoch: 028, Loss: 0.8705, Train: 0.7500, Test: 0.7089\n",
            "Early stopping:  0.023332172771357983\n",
            "Epoch: 029, Loss: 0.8585, Train: 0.7530, Test: 0.7147\n",
            "Early stopping:  0.02151262613618836\n",
            "Epoch: 030, Loss: 0.8471, Train: 0.7578, Test: 0.7153\n",
            "Early stopping:  0.019932394041317875\n",
            "Epoch: 031, Loss: 0.8364, Train: 0.7607, Test: 0.7163\n",
            "Early stopping:  0.018636561347139147\n",
            "Epoch: 032, Loss: 0.8262, Train: 0.7651, Test: 0.7195\n",
            "Early stopping:  0.017505081433656602\n",
            "Epoch: 033, Loss: 0.8166, Train: 0.7684, Test: 0.7205\n",
            "Early stopping:  0.016555599064114468\n",
            "Epoch: 034, Loss: 0.8074, Train: 0.7701, Test: 0.7237\n",
            "Early stopping:  0.015678114772287786\n",
            "Epoch: 035, Loss: 0.7987, Train: 0.7733, Test: 0.7242\n",
            "Early stopping:  0.014883176915054622\n",
            "Epoch: 036, Loss: 0.7902, Train: 0.7772, Test: 0.7247\n",
            "Early stopping:  0.014209709999408807\n",
            "Epoch: 037, Loss: 0.7818, Train: 0.7800, Test: 0.7232\n",
            "Early stopping:  0.013740950332011772\n",
            "Epoch: 038, Loss: 0.7733, Train: 0.7822, Test: 0.7242\n",
            "Early stopping:  0.013458861700237237\n",
            "Epoch: 039, Loss: 0.7651, Train: 0.7849, Test: 0.7237\n",
            "Early stopping:  0.01330604821305812\n",
            "Epoch: 040, Loss: 0.7568, Train: 0.7892, Test: 0.7237\n",
            "Early stopping:  0.01318764585596929\n",
            "Epoch: 041, Loss: 0.7488, Train: 0.7917, Test: 0.7247\n",
            "Early stopping:  0.01302865386981179\n",
            "Epoch: 042, Loss: 0.7409, Train: 0.7946, Test: 0.7274\n",
            "Early stopping:  0.012830286153819613\n",
            "Epoch: 043, Loss: 0.7329, Train: 0.7964, Test: 0.7237\n",
            "Early stopping:  0.01268040081525156\n",
            "Epoch: 044, Loss: 0.7250, Train: 0.7986, Test: 0.7247\n",
            "Early stopping:  0.012591038391407615\n",
            "Epoch: 045, Loss: 0.7169, Train: 0.8016, Test: 0.7268\n",
            "Early stopping:  0.01261574645640622\n",
            "Epoch: 046, Loss: 0.7087, Train: 0.8014, Test: 0.7263\n",
            "Early stopping:  0.012700942657594144\n",
            "Epoch: 047, Loss: 0.7006, Train: 0.8063, Test: 0.7295\n",
            "Early stopping:  0.012781435405599046\n",
            "Epoch: 048, Loss: 0.6927, Train: 0.8034, Test: 0.7247\n",
            "Early stopping:  0.012787370795882652\n",
            "Epoch: 049, Loss: 0.6850, Train: 0.8092, Test: 0.7289\n",
            "Early stopping:  0.012640618590891795\n",
            "Epoch: 050, Loss: 0.6775, Train: 0.8103, Test: 0.7226\n",
            "Early stopping:  0.012352272259163287\n",
            "Epoch: 051, Loss: 0.6699, Train: 0.8151, Test: 0.7279\n",
            "Early stopping:  0.012105344872300335\n",
            "Epoch: 052, Loss: 0.6614, Train: 0.8158, Test: 0.7263\n",
            "Early stopping:  0.012261472987930793\n",
            "Epoch: 053, Loss: 0.6532, Train: 0.8199, Test: 0.7263\n",
            "Early stopping:  0.012609311410840702\n",
            "Epoch: 054, Loss: 0.6457, Train: 0.8226, Test: 0.7279\n",
            "Early stopping:  0.012704210399839799\n",
            "Epoch: 055, Loss: 0.6384, Train: 0.8245, Test: 0.7253\n",
            "Early stopping:  0.012442564974536361\n",
            "Epoch: 056, Loss: 0.6308, Train: 0.8262, Test: 0.7237\n",
            "Early stopping:  0.012013252517118921\n",
            "Epoch: 057, Loss: 0.6230, Train: 0.8287, Test: 0.7242\n",
            "Early stopping:  0.011901157949223235\n",
            "Epoch: 058, Loss: 0.6155, Train: 0.8307, Test: 0.7221\n",
            "Early stopping:  0.012017473726583515\n",
            "Epoch: 059, Loss: 0.6084, Train: 0.8329, Test: 0.7237\n",
            "Early stopping:  0.011914011430683576\n",
            "Epoch: 060, Loss: 0.6010, Train: 0.8354, Test: 0.7258\n",
            "Early stopping:  0.011714730787576315\n",
            "Epoch: 061, Loss: 0.5934, Train: 0.8378, Test: 0.7232\n",
            "Early stopping:  0.011639480189674416\n",
            "Epoch: 062, Loss: 0.5856, Train: 0.8416, Test: 0.7232\n",
            "Early stopping:  0.011817766262363277\n",
            "Epoch: 063, Loss: 0.5783, Train: 0.8429, Test: 0.7247\n",
            "Early stopping:  0.01196602551595796\n",
            "Epoch: 064, Loss: 0.5714, Train: 0.8464, Test: 0.7216\n",
            "Early stopping:  0.011754260233059883\n",
            "Epoch: 065, Loss: 0.5649, Train: 0.8475, Test: 0.7258\n",
            "Early stopping:  0.011272891493123632\n",
            "Epoch: 066, Loss: 0.5586, Train: 0.8508, Test: 0.7226\n",
            "Early stopping:  0.010682620909272432\n",
            "Epoch: 067, Loss: 0.5519, Train: 0.8530, Test: 0.7226\n",
            "Early stopping:  0.010397348488401981\n",
            "Epoch: 068, Loss: 0.5447, Train: 0.8541, Test: 0.7232\n",
            "Early stopping:  0.010513182308802103\n",
            "Epoch: 069, Loss: 0.5381, Train: 0.8566, Test: 0.7174\n",
            "Early stopping:  0.010660105754729643\n",
            "Epoch: 070, Loss: 0.5324, Train: 0.8553, Test: 0.7221\n",
            "Early stopping:  0.010467893801546425\n",
            "Epoch: 071, Loss: 0.5274, Train: 0.8564, Test: 0.7205\n",
            "Early stopping:  0.00971761357740619\n",
            "PREDICTIONS -> tensor([17,  0,  0,  ..., 18, 18, 18], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Artificial_Intelligence       0.83      0.88      0.86        85\n",
            "            CAD_and_CAM       0.75      0.72      0.73       100\n",
            "              Companies       0.59      0.45      0.51       104\n",
            "       Computer_Science       0.79      0.76      0.78        89\n",
            "            Consultants       0.66      0.64      0.65       111\n",
            "           Data_Formats       0.84      0.84      0.84       103\n",
            "    Data_Communications       0.77      0.73      0.75       112\n",
            "              Education       0.88      0.89      0.88       115\n",
            "               Graphics       0.84      0.90      0.87       103\n",
            "               Hardware       0.68      0.82      0.74       105\n",
            "               Internet       0.58      0.70      0.64        77\n",
            "       Mobile_Computing       0.80      0.81      0.81       101\n",
            "             Multimedia       0.71      0.79      0.75        92\n",
            "            Open_Source       0.76      0.56      0.64       113\n",
            "            Programming       0.53      0.59      0.56       106\n",
            "               Robotics       0.90      0.94      0.92        93\n",
            "               Security       0.78      0.79      0.79       107\n",
            "               Software       0.29      0.26      0.27        92\n",
            "                Systems       0.62      0.60      0.61        92\n",
            "\n",
            "               accuracy                           0.72      1900\n",
            "              macro avg       0.72      0.72      0.72      1900\n",
            "           weighted avg       0.72      0.72      0.72      1900\n",
            "\n",
            "time: 36.3 s (started: 2024-08-17 14:17:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframes\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d673e761-f977-41d1-d978-fc62a39f847c",
        "id": "j9DMrzV1tx3_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 515 ms (started: 2024-08-17 14:18:22 +00:00)\n"
          ]
        }
      ]
    }
  ]
}