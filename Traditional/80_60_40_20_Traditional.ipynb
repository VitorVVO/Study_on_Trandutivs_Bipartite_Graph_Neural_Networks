{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDgn0_O42cvh",
        "outputId": "4be4ac2e-afa7-4461-85ef-6003f8e235e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 565 µs (started: 2024-10-16 20:50:09 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# \"Dmoz_Computers\",\"Dmoz_Health\",\"Dmoz_Science\",\"Dmoz_Sports\"\n",
        "# \"classic4\",\"NSF\",\"re8\",\"review_polarity\", \"Industry_Sector\"\n",
        "# \"SyskillWebert\",\"webkb_parsed\", \"CSTR\"\n",
        "dataset_name = \"Industry_Sector\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtpe_CYHnTqU"
      },
      "source": [
        "# Imports, load data , training functions and database creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9sKW8iq7Tny"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39ccaX7dnIjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5095eba7-fdda-433f-8581-33087af2b12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 3.12 s (started: 2024-10-16 20:50:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RqdT47FJJn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607e2d39-7db3-4099-996a-72e65c8ed9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 2.32 s (started: 2024-10-16 20:50:14 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ut43TXtnIjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e53784-6a1c-46a9-bae5-cafdf8b2437a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 687 µs (started: 2024-10-16 20:50:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import os, errno\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "import json\n",
        "import yaml\n",
        "import ast\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOVwijdHyIMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f099064c-c2af-4350-c4c0-97c9d1d3d7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 199 µs (started: 2024-10-16 20:50:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall torch -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmHlqv06yLJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4dbfb4-d82c-4625-95bf-512944f7c63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 257 µs (started: 2024-10-16 20:50:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#!pip install torch==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifuE7A2bn9vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febb1106-8f36-4f1d-8eeb-50e323ac466f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "time: 26.5 s (started: 2024-10-16 20:50:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "441Hk5OlRRIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215f8d8e-0eba-468f-cb9f-392460f449ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 686 µs (started: 2024-10-16 20:50:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "from torch_geometric.nn import SAGEConv, GATConv, Linear, to_hetero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D53m3GFKNaBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37736d6e-8ca7-4ea0-d3ba-88936637e3c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 193
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.02 ms (started: 2024-10-16 20:50:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-0aDh52nO-S"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mntPGsInN7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67d7dee-370c-4932-b674-e60ed4b12405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.4 s (started: 2024-10-16 20:50:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/\"+dataset_name+\"/\"+dataset_name+\".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9OxRWYh64rn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "52507939-66f5-4f3f-c0d3-f8b62366b659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                          file_name  \\\n",
              "0   0        basic_materials_http:^^www.littonacdipe.com   \n",
              "1   1  basic_materials_http:^^www.weirton.com^contact...   \n",
              "2   2  basic_materials_http:^^home.earthlink.net^~nuc...   \n",
              "3   3    basic_materials_http:^^www.useg.com^invest.html   \n",
              "4   4  basic_materials_http:^^www.terraindustries.com...   \n",
              "\n",
              "                                                text            class  \\\n",
              "0     Litton            Litton: Interconnect Prod...  basic_materials   \n",
              "1  Allow: GET, HEAD URI:       Contact Weirton St...  basic_materials   \n",
              "2      Systems     Which system would you choose?...  basic_materials   \n",
              "3         US Energy Corporation Investor News    ...  basic_materials   \n",
              "4        Terra Industries, Inc., Company Informat...  basic_materials   \n",
              "\n",
              "                                          keyphrase2  \\\n",
              "0  [(packaging esp, 0.5861), (litton interconnect...   \n",
              "1  [(contact firm, 0.562), (contact weirton, 0.50...   \n",
              "2  [(pressure cylinders, 0.6841), (pressure bulk,...   \n",
              "3  [(images investon, 0.5688), (gif investon, 0.5...   \n",
              "4  [(terra nitrogen, 0.6228), (nitrogen fertilize...   \n",
              "\n",
              "                                          keyphrase3  \\\n",
              "0  [(electronic systems packaging, 0.6613), (syst...   \n",
              "1  [(contact weirton steel, 0.5781), (come contac...   \n",
              "2  [(high pressure cylinders, 0.7351), (choose lo...   \n",
              "3  [(images investon gif, 0.6559), (images invest...   \n",
              "4  [(terra produces nitrogen, 0.7287), (terra nit...   \n",
              "\n",
              "                                         keyphrase23  \\\n",
              "0  [(electronic systems packaging, 0.6613), (syst...   \n",
              "1  [(contact weirton steel, 0.5781), (contact fir...   \n",
              "2  [(high pressure cylinders, 0.7351), (pressure ...   \n",
              "3  [(images investon gif, 0.6559), (images invest...   \n",
              "4  [(terra produces nitrogen, 0.7287), (terra nit...   \n",
              "\n",
              "                                     text_embeddings  \\\n",
              "0  [-0.05860152, 0.0145570105, 0.017551647, -0.02...   \n",
              "1  [-0.124327414, -0.012992704, 0.014001042, 0.03...   \n",
              "2  [-0.085865036, 0.043530438, -0.071711995, -0.0...   \n",
              "3  [0.008732203, -0.013307741, -0.036661223, 0.02...   \n",
              "4  [-0.049107973, -0.043558124, -0.020951217, -0....   \n",
              "\n",
              "                               keyphrase2_embeddings  \\\n",
              "0  [[-0.07081439, 0.053046886, 0.038261086, -0.04...   \n",
              "1  [[-0.08809786, -0.019295853, 0.00693113, -0.01...   \n",
              "2  [[-0.097670145, 0.07834313, -0.067915626, -0.0...   \n",
              "3  [[-0.009558729, 0.017956274, -0.059580665, -0....   \n",
              "4  [[-0.07773064, 0.033809956, 0.025796082, -0.01...   \n",
              "\n",
              "                               keyphrase3_embeddings  \\\n",
              "0  [[-0.07142605, 0.085836, -0.04343102, -0.05472...   \n",
              "1  [[-0.10939312, -0.018659644, 0.051947754, 0.07...   \n",
              "2  [[-0.093530454, 0.07404681, -0.049339183, -0.0...   \n",
              "3  [[0.00017329463, 0.0013028142, -0.06374502, -0...   \n",
              "4  [[-0.045650914, 0.045816682, 0.023411866, -0.0...   \n",
              "\n",
              "                              keyphrase23_embeddings  \\\n",
              "0  [[-0.07142605, 0.085836, -0.04343102, -0.05472...   \n",
              "1  [[-0.10939312, -0.018659644, 0.051947754, 0.07...   \n",
              "2  [[-0.093530454, 0.07404681, -0.049339183, -0.0...   \n",
              "3  [[0.00017329463, 0.0013028142, -0.06374502, -0...   \n",
              "4  [[-0.045650914, 0.045816682, 0.023411866, -0.0...   \n",
              "\n",
              "                                sentences_embeddings  \n",
              "0  [[-0.0956507, -0.0512441, -0.009371761, 0.0161...  \n",
              "1  [[-0.09154996, 0.004012895, 0.07021415, -0.001...  \n",
              "2  [[-0.03351223, -0.006459103, -0.07732453, -0.0...  \n",
              "3  [[0.008732203, -0.013307741, -0.036661223, 0.0...  \n",
              "4  [[-0.034066442, 0.033516087, 0.018464932, -0.0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6078630e-89bf-431c-aea9-5f27076e9edc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "      <th>keyphrase2</th>\n",
              "      <th>keyphrase3</th>\n",
              "      <th>keyphrase23</th>\n",
              "      <th>text_embeddings</th>\n",
              "      <th>keyphrase2_embeddings</th>\n",
              "      <th>keyphrase3_embeddings</th>\n",
              "      <th>keyphrase23_embeddings</th>\n",
              "      <th>sentences_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>basic_materials_http:^^www.littonacdipe.com</td>\n",
              "      <td>Litton            Litton: Interconnect Prod...</td>\n",
              "      <td>basic_materials</td>\n",
              "      <td>[(packaging esp, 0.5861), (litton interconnect...</td>\n",
              "      <td>[(electronic systems packaging, 0.6613), (syst...</td>\n",
              "      <td>[(electronic systems packaging, 0.6613), (syst...</td>\n",
              "      <td>[-0.05860152, 0.0145570105, 0.017551647, -0.02...</td>\n",
              "      <td>[[-0.07081439, 0.053046886, 0.038261086, -0.04...</td>\n",
              "      <td>[[-0.07142605, 0.085836, -0.04343102, -0.05472...</td>\n",
              "      <td>[[-0.07142605, 0.085836, -0.04343102, -0.05472...</td>\n",
              "      <td>[[-0.0956507, -0.0512441, -0.009371761, 0.0161...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>basic_materials_http:^^www.weirton.com^contact...</td>\n",
              "      <td>Allow: GET, HEAD URI:       Contact Weirton St...</td>\n",
              "      <td>basic_materials</td>\n",
              "      <td>[(contact firm, 0.562), (contact weirton, 0.50...</td>\n",
              "      <td>[(contact weirton steel, 0.5781), (come contac...</td>\n",
              "      <td>[(contact weirton steel, 0.5781), (contact fir...</td>\n",
              "      <td>[-0.124327414, -0.012992704, 0.014001042, 0.03...</td>\n",
              "      <td>[[-0.08809786, -0.019295853, 0.00693113, -0.01...</td>\n",
              "      <td>[[-0.10939312, -0.018659644, 0.051947754, 0.07...</td>\n",
              "      <td>[[-0.10939312, -0.018659644, 0.051947754, 0.07...</td>\n",
              "      <td>[[-0.09154996, 0.004012895, 0.07021415, -0.001...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>basic_materials_http:^^home.earthlink.net^~nuc...</td>\n",
              "      <td>Systems     Which system would you choose?...</td>\n",
              "      <td>basic_materials</td>\n",
              "      <td>[(pressure cylinders, 0.6841), (pressure bulk,...</td>\n",
              "      <td>[(high pressure cylinders, 0.7351), (choose lo...</td>\n",
              "      <td>[(high pressure cylinders, 0.7351), (pressure ...</td>\n",
              "      <td>[-0.085865036, 0.043530438, -0.071711995, -0.0...</td>\n",
              "      <td>[[-0.097670145, 0.07834313, -0.067915626, -0.0...</td>\n",
              "      <td>[[-0.093530454, 0.07404681, -0.049339183, -0.0...</td>\n",
              "      <td>[[-0.093530454, 0.07404681, -0.049339183, -0.0...</td>\n",
              "      <td>[[-0.03351223, -0.006459103, -0.07732453, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>basic_materials_http:^^www.useg.com^invest.html</td>\n",
              "      <td>US Energy Corporation Investor News    ...</td>\n",
              "      <td>basic_materials</td>\n",
              "      <td>[(images investon, 0.5688), (gif investon, 0.5...</td>\n",
              "      <td>[(images investon gif, 0.6559), (images invest...</td>\n",
              "      <td>[(images investon gif, 0.6559), (images invest...</td>\n",
              "      <td>[0.008732203, -0.013307741, -0.036661223, 0.02...</td>\n",
              "      <td>[[-0.009558729, 0.017956274, -0.059580665, -0....</td>\n",
              "      <td>[[0.00017329463, 0.0013028142, -0.06374502, -0...</td>\n",
              "      <td>[[0.00017329463, 0.0013028142, -0.06374502, -0...</td>\n",
              "      <td>[[0.008732203, -0.013307741, -0.036661223, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>basic_materials_http:^^www.terraindustries.com...</td>\n",
              "      <td>Terra Industries, Inc., Company Informat...</td>\n",
              "      <td>basic_materials</td>\n",
              "      <td>[(terra nitrogen, 0.6228), (nitrogen fertilize...</td>\n",
              "      <td>[(terra produces nitrogen, 0.7287), (terra nit...</td>\n",
              "      <td>[(terra produces nitrogen, 0.7287), (terra nit...</td>\n",
              "      <td>[-0.049107973, -0.043558124, -0.020951217, -0....</td>\n",
              "      <td>[[-0.07773064, 0.033809956, 0.025796082, -0.01...</td>\n",
              "      <td>[[-0.045650914, 0.045816682, 0.023411866, -0.0...</td>\n",
              "      <td>[[-0.045650914, 0.045816682, 0.023411866, -0.0...</td>\n",
              "      <td>[[-0.034066442, 0.033516087, 0.018464932, -0.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6078630e-89bf-431c-aea9-5f27076e9edc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6078630e-89bf-431c-aea9-5f27076e9edc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6078630e-89bf-431c-aea9-5f27076e9edc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8c3a88d7-d348-4824-ae31-1563a2f4c952\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c3a88d7-d348-4824-ae31-1563a2f4c952')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8c3a88d7-d348-4824-ae31-1563a2f4c952 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8817,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2545,\n        \"min\": 0,\n        \"max\": 8816,\n        \"num_unique_values\": 8817,\n        \"samples\": [\n          3027,\n          6076,\n          3666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8817,\n        \"samples\": [\n          \"consumer_non-cyclical_http:^^www.tyson.com^Corporate^AllAboutTyson^NewsReleases^SOUTHA~1.HTM\",\n          \"services_http:^^www.clearchannel.com^Architext^inside.htm\",\n          \"financial_http:^^www.crestar.com^sitemap^\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8726,\n        \"samples\": [\n          \"      WELCOME To: PRIMEX Technologies, Inc.                                 &nbsp; PRIMEX Technologies, Inc. . . Out of the Olin Corporation, a new independent and innovative industrial corporation has been created. This new enterprise will capitalize on the more than 100 years of proven success of the former Olin Ordnance and Olin Aerospace companies and synergy of their technical expertise in delivering ordnance, electronics and aerospace products to the defense electronics, aerospace and commercial industries.        &nbsp;            [ Corporate Profile | News Releases | Financial Information | Aerospace Division | Ordnance Division ]       Copyright&copy; 1997 PRIMEX Technologies, Inc. All rights reserved.&nbsp;  &nbsp;  Best viewed with Netscape Navigator 3.0 (or later) and Microsoft Internet Explorer 3.0 (or later)     \",\n          \"   Eagle USA Online Tracking - Eagle TRAK   1) {  top.location=\\\"/\\\";  } var version = 0;      if (navigator.userAgent.indexOf(\\\"Mozilla/3.0\\\") != -1) version = 3;      else if (navigator.userAgent.indexOf(\\\"Mozilla/4.0\\\") != -1) version = 3;      else if (navigator.userAgent.indexOf(\\\"MSIE\\\") != -1) version = 2;      else if (navigator.userAgent.indexOf(\\\"Mozilla/2.0\\\") != -1) version = 1;      else version = 0; // preload universal images: if (version == 3){ a1 = new Image(); a1.src = \\\"images/blue-1.gif\\\" a2 = new Image(); a2.src = \\\"images/blue-2.gif\\\" x1 = new Image(); x1.src = \\\"images/box1.gif\\\" x2 = new Image(); x2.src = \\\"images/box2.gif\\\" x3 = new Image(); x3.src = \\\"images/box3.gif\\\" x4 = new Image(); x4.src = \\\"images/box4.gif\\\" box1 = new Image(); box1.src = \\\"images/javabox.gif\\\" wt1 = new Image(); wt1.src = \\\"images/news-wht.gif\\\" wt2 = new Image(); wt2.src = \\\"images/emp-wht.gif\\\" wt3 = new Image(); wt3.src = \\\"images/search-wht.gif\\\" wt4 = new Image(); wt4.src = \\\"images/index-wht.gif\\\" wt5 = new Image(); wt5.src = \\\"images/home-wht.gif\\\" wt6 = new Image(); wt6.src = \\\"images/releases-wht.gif\\\" wt7 = new Image(); wt7.src = \\\"images/guestbook-wht.gif\\\" gr1 = new Image(); gr1.src = \\\"images/news-gr.gif\\\" gr2 = new Image(); gr2.src = \\\"images/emp-gr.gif\\\" gr3 = new Image(); gr3.src = \\\"images/search-gr.gif\\\" gr4 = new Image(); gr4.src = \\\"images/index-gr.gif\\\" gr5 = new Image(); gr5.src = \\\"images/home-gr.gif\\\" gr6 = new Image(); gr6.src = \\\"images/releases-gr.gif\\\" gr7 = new Image(); gr7.src = \\\"images/guestbook-gr.gif\\\"  }  function hiLite(imgDocID,imgObjName) { // manages mouseOver animations //  imgDocID - the name or number of the document image to be replaced //  imgObjName - the name of the image object to be swapped in if (version == 3){ document.images[imgDocID].src = eval(imgObjName + \\\".src\\\") }} // -- End of JavaScript code -------------- -->                                                                            Enter House Airway Bill Number:  #        Enter Reference Number:  #       NOTE: Tracking information is limited to the last 90 days from today's date.                                    To use this online interface with the Eagle USA Eagle TRAK system, simply enter your package's House Airway Bill Number in the top area,  or enter a Reference Number in the second area. Either number may be found on your shipping receipt, and also on the white label attached to the package itself.        EAGLE USA AIRFREIGHT   Intercontinental Airport 3214 Lodestar Houston, Tx 77032  www.eagleusa.com   &copy; 1997 Eagle USA     \",\n          \"   Conseco Guest Book       Please sign our guest book below to let us know what you think of our service and how we can better serve your needs.      Your Name     Your Company   Address               City        State/Province  Zip        Country      Phone       FAX        E-Mail Address       Please send the following information:   Investor Package (includes latest annual report, recent earnings release,  most recent investor guide and other current information) Most Recent Annual Report (our 1995 Annual Report will be available  in late April) Most Recent Form 10-K Most Recent Form 10-Q  Most Recent Proxy Statement Most Recent Press Release Most Recent Investor Guide Other (please specify document)         How did you hear about the Conseco web site?  What type of World Wide Web browser are you using?  What type of computer/operating system are you using?  On a scale of 1 to 10 (10=best), how would you rate our web site for: Company Information   Product Information  Interest        Please let us know any other comments you have. Thanks for your interest.        If you have any additional comments, please e-mail the Investor Relations         Conseco Home Page   \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"transportation\",\n          \"technology\",\n          \"basic_materials\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase2\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase3\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase23\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase2_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase3_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrase23_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 195
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.02 s (started: 2024-10-16 20:50:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUqyo3PodvRh"
      },
      "source": [
        "## Training functions and Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XYyQOzSLExL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4751a727-1ab0-426e-b7ca-85ffee304122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.08 ms (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def creat_equal_masks(test_perc,y):\n",
        "    n_samples = len(y)\n",
        "    print('all:',np.bincount(y))\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_perc, random_state=0)\n",
        "    x = np.zeros(n_samples)\n",
        "    for train_idx, test_idx in splitter.split(x, y):\n",
        "        train_mask = np.full(n_samples, False, dtype=bool)\n",
        "        val_mask = np.full(n_samples, False, dtype=bool)\n",
        "        test_mask = np.full(n_samples, False, dtype=bool)\n",
        "\n",
        "        train_mask[train_idx] = True\n",
        "        test_mask[test_idx] = True\n",
        "\n",
        "        print('train:',np.bincount(y[train_idx]))\n",
        "        print('test:',np.bincount(y[test_mask]))\n",
        "\n",
        "        print('train vs test:',np.bincount(train_mask))\n",
        "        # print(np.bincount(val_mask))\n",
        "        # print(np.bincount(test_mask))\n",
        "\n",
        "        train_mask = torch.tensor(train_mask)\n",
        "        val_mask = torch.tensor(val_mask)\n",
        "        test_mask = torch.tensor(test_mask)\n",
        "\n",
        "        return train_mask, val_mask, test_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1LBCeLIWlFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16552489-6e6b-45b8-8f8d-ca0073c58691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 674 µs (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def train(model,optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    mask = data['document'].train_mask\n",
        "    loss = F.cross_entropy(out['document'][mask], data['document'].y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgvEx3HBXCc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b72612-cd6e-4867-933f-b6dd2c549636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 841 µs (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def test(model):\n",
        "    model.eval()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = out['document'].argmax(dim=1)\n",
        "    # print(\"PREDICTIONS ->\", pred)\n",
        "    accs = []\n",
        "    for mask in [data['document'].train_mask, data['document'].test_mask]:\n",
        "        accs.append(int((pred[mask] == data['document'].y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7a82kzNCt90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e298cb7e-40d8-456a-ffde-465ce99802e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 620 µs (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def get_y(model):\n",
        "    model.eval()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = out['document'].argmax(dim=1)\n",
        "    print(\"PREDICTIONS ->\", pred)\n",
        "\n",
        "    # y_pred, y_true\n",
        "    return pred[data['document'].test_mask],data['document'].y[data['document'].test_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtTe3rAQrhG-"
      },
      "source": [
        "### GNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUVqN5z0q80O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8fcf9d2-f608-45e5-ce6a-4f7aa82d22a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 727 µs (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# First Network\n",
        "from torch_geometric.nn import GraphConv\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GraphConv(-1, hidden_channels)\n",
        "        self.conv2 = GraphConv(-1, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pws3dFR9rkOI"
      },
      "source": [
        "### GAT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAYkObIpq80Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252a4fac-06f1-4169-dd42-064ac455c7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 903 µs (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Second Network\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATv2Conv((-1, -1), hidden_channels, add_self_loops=False)\n",
        "        self.lin1 = Linear(-1, hidden_channels)\n",
        "        self.conv2 = GATv2Conv((-1, -1), out_channels, add_self_loops=False)\n",
        "        self.lin2 = Linear(-1, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHo1UacopxwC"
      },
      "source": [
        "### iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia8rnSQFmssN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f14986-eb95-4ab6-c5cc-d8f19666fa68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.99 ms (started: 2024-10-16 20:50:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def iteration(i,GCN_or_GAT,rotulated):\n",
        "    # for i in range(10):\n",
        "    print(f\"\\n===============================================\")\n",
        "    print(f\"=================== MODEL {i} ===================\")\n",
        "    print(f\"===============================================\\n\")\n",
        "\n",
        "    # Defining masks\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks_big/mask_rot'+str(rotulated)+'_'+str(i)+'.pkl', 'rb') as f:\n",
        "        aux = pickle.load(f)\n",
        "        train_mask, val_mask, test_mask = aux[0],aux[1],aux[2]\n",
        "\n",
        "    data['document'].train_mask = train_mask\n",
        "    data['document'].val_mask = val_mask\n",
        "    data['document'].test_mask = test_mask\n",
        "\n",
        "    # Generate MODEL and Optmizer\n",
        "    if GCN_or_GAT == \"GCN\":\n",
        "        model = GNN(hidden_channels=64, out_channels=class_number)\n",
        "    elif GCN_or_GAT ==\"GAT\":\n",
        "        model = GAT(hidden_channels=64, out_channels=class_number)\n",
        "\n",
        "    model = to_hetero(model, data.metadata(), aggr='sum')\n",
        "    model.to(device) # GPU\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "    #contagem de tempo\n",
        "    start = time.time()\n",
        "\n",
        "    # Training\n",
        "    L_loss = []\n",
        "    for epoch in range(1, 1000):\n",
        "        loss = train(model,optimizer)\n",
        "        L_loss.append(loss)\n",
        "        train_acc, test_acc = test(model)\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Test: {test_acc:.4f}')\n",
        "        df_temp = pd.DataFrame(L_loss)\n",
        "        if len(df_temp >=5):\n",
        "          loss_es =  pd.DataFrame(L_loss).tail(5)[0].std()\n",
        "          print('Early stopping: ',loss_es)\n",
        "          if loss_es <= 0.01:\n",
        "            break\n",
        "\n",
        "    #contagem de tempo\n",
        "    end = time.time()\n",
        "\n",
        "    # Creating Classification Report\n",
        "    y_pred,y_true = get_y(model)\n",
        "    y_pred = y_pred.cpu()\n",
        "    y_true = y_true.cpu()\n",
        "    target_names = df[\"class\"].unique()\n",
        "\n",
        "    class_dit = classification_report(y_true, y_pred, target_names=target_names, output_dict = True)\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "\n",
        "    #L_results.append([model_name,'GCN',rotulated,class_dit['macro avg']['f1-score']])\n",
        "    # Updating Dataframes\n",
        "    df_list[i][0].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['accuracy']               # acuracia\n",
        "    df_list[i][1].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['precision'] # precision\n",
        "    df_list[i][2].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['recall']    # recall\n",
        "    df_list[i][3].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['f1-score']  # f1-score\n",
        "    df_list[i][4].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = class_dit['macro avg']['support']   # support\n",
        "    df_list[i][5].at[model_name+\"_\"+GCN_or_GAT, str(rotulated)] = (end - start)                       # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQWCTrxz7atg"
      },
      "source": [
        "## Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99RJ3y7cm9AG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda23e28-a9c7-41e8-d22b-2ce440a347fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 480 µs (started: 2024-10-16 21:55:36 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# # Only execude if dafaframes do not exist YET\n",
        "\n",
        "# df_list = []\n",
        "\n",
        "# for i in range(10):\n",
        "#     aux_list = []\n",
        "\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # acuracia\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # precision\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # recall\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # f1-score\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # support\n",
        "#     aux_list.append(pd.DataFrame(columns = ['1','5','10','20','30','80%'])) # time\n",
        "\n",
        "#     df_list.append(aux_list)\n",
        "\n",
        "#     try:\n",
        "#         os.makedirs(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i))\n",
        "#     except OSError as e:\n",
        "#         if e.errno != errno.EEXIST:\n",
        "#             raise\n",
        "\n",
        "#     df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "#     df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "#     df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "#     df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "#     df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "#     df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnYLm6iznJq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e16cca-b12f-4fd9-d6c9-4721719ca42a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 223 ms (started: 2024-10-16 21:55:37 +00:00)\n"
          ]
        }
      ],
      "source": [
        "df_list = []\n",
        "for i in range(10):\n",
        "    aux_list = []\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\")) # acuracia\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\")) # precision\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\")) # recall\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\"))  # f1-score\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\")) # support\n",
        "    aux_list.append(pd.read_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\")) # time\n",
        "    df_list.append(aux_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxz4VW7-iy3C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dc97d14-ff6d-40b1-c318-abc0d876e691"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        1         5        10        20  \\\n",
              "Dmoz_Computers_keyphrase2_GCN    0.229301  0.442531  0.514715  0.564583   \n",
              "Dmoz_Computers_keyphrase2_GAT    0.307984  0.469644  0.540172  0.598684   \n",
              "Dmoz_Computers_keyphrase3_GCN    0.279823   0.45008  0.534264     0.575   \n",
              "Dmoz_Computers_keyphrase3_GAT    0.299652  0.489208  0.551128  0.601645   \n",
              "Dmoz_Computers_keyphrase23_GCN   0.185951  0.417438  0.510311   0.57193   \n",
              "Dmoz_Computers_keyphrase23_GAT   0.261154  0.480489  0.553383  0.592325   \n",
              "Dmoz_Health_keyphrase2_GCN       0.262371  0.693862  0.710832  0.747596   \n",
              "Dmoz_Health_keyphrase2_GAT       0.421921  0.714841  0.754631  0.789744   \n",
              "Dmoz_Health_keyphrase3_GCN        0.38292  0.681119  0.700157  0.764263   \n",
              "Dmoz_Health_keyphrase3_GAT       0.418067   0.72805  0.756515  0.797596   \n",
              "Dmoz_Health_keyphrase23_GCN      0.292585  0.667288  0.719937  0.755128   \n",
              "Dmoz_Health_keyphrase23_GAT      0.403422   0.72028  0.755102  0.797596   \n",
              "Dmoz_Science_keyphrase2_GCN      0.359385  0.547811  0.603571  0.672569   \n",
              "Dmoz_Science_keyphrase2_GAT      0.342685  0.578114  0.672449  0.682292   \n",
              "Dmoz_Science_keyphrase3_GCN      0.312291  0.518687  0.646259  0.664236   \n",
              "Dmoz_Science_keyphrase3_GAT      0.406981  0.560606  0.671259  0.686979   \n",
              "Dmoz_Science_keyphrase23_GCN     0.341349  0.543434  0.633673  0.673785   \n",
              "Dmoz_Science_keyphrase23_GAT     0.351369  0.556902   0.67585  0.683333   \n",
              "Dmoz_Sports_keyphrase2_GCN       0.351741  0.655668  0.692668  0.740432   \n",
              "Dmoz_Sports_keyphrase2_GAT       0.496846  0.725851  0.782011  0.799306   \n",
              "Dmoz_Sports_keyphrase3_GCN       0.334818  0.652974   0.72706  0.756096   \n",
              "Dmoz_Sports_keyphrase3_GAT       0.482966  0.719716  0.777324  0.817284   \n",
              "Dmoz_Sports_keyphrase23_GCN      0.273362  0.629031  0.707785  0.757253   \n",
              "Dmoz_Sports_keyphrase23_GAT      0.454613  0.723606  0.770144  0.806096   \n",
              "Industry_Sector_keyphrase2_GCN    0.10017  0.157017  0.302863   0.14877   \n",
              "Industry_Sector_keyphrase2_GAT   0.103464  0.226447  0.381281  0.466946   \n",
              "Industry_Sector_keyphrase3_GCN   0.113799  0.170949  0.281246  0.366212   \n",
              "Industry_Sector_keyphrase3_GAT   0.128223  0.262533  0.383925   0.47231   \n",
              "Industry_Sector_keyphrase23_GCN  0.077229  0.192874  0.274577  0.316078   \n",
              "Industry_Sector_keyphrase23_GAT  0.086201  0.258308  0.385765  0.471494   \n",
              "NSF_keyphrase2_GCN               0.471545  0.734201  0.744693    0.7696   \n",
              "NSF_keyphrase2_GAT               0.594499  0.768575  0.770745  0.803606   \n",
              "\n",
              "                                       30       80%       20%       40%  \\\n",
              "Dmoz_Computers_keyphrase2_GCN    0.579395  0.693158  0.620658  0.645965   \n",
              "Dmoz_Computers_keyphrase2_GAT    0.610414  0.725789  0.669605  0.681930   \n",
              "Dmoz_Computers_keyphrase3_GCN    0.603471  0.720000  0.648026  0.672632   \n",
              "Dmoz_Computers_keyphrase3_GAT    0.615901  0.724211  0.661053  0.683860   \n",
              "Dmoz_Computers_keyphrase23_GCN    0.58533  0.698421  0.644605  0.671930   \n",
              "Dmoz_Computers_keyphrase23_GAT   0.610862  0.721053  0.666579  0.681053   \n",
              "Dmoz_Health_keyphrase2_GCN        0.76743  0.823846  0.816731  0.826154   \n",
              "Dmoz_Health_keyphrase2_GAT        0.81162  0.861538  0.851154  0.853077   \n",
              "Dmoz_Health_keyphrase3_GCN       0.787398  0.838462  0.830192  0.835641   \n",
              "Dmoz_Health_keyphrase3_GAT       0.809493  0.855385  0.850962  0.862564   \n",
              "Dmoz_Health_keyphrase23_GCN      0.797872  0.841538  0.825385  0.825128   \n",
              "Dmoz_Health_keyphrase23_GAT      0.807529  0.862308  0.850000  0.861795   \n",
              "Dmoz_Science_keyphrase2_GCN      0.713298  0.799167  0.752500  0.766944   \n",
              "Dmoz_Science_keyphrase2_GAT      0.737943  0.810833  0.777917  0.788889   \n",
              "Dmoz_Science_keyphrase3_GCN      0.714894  0.800833  0.766250  0.771389   \n",
              "Dmoz_Science_keyphrase3_GAT      0.738298  0.821667  0.775417  0.800000   \n",
              "Dmoz_Science_keyphrase23_GCN     0.722872  0.806667  0.752708  0.773889   \n",
              "Dmoz_Science_keyphrase23_GAT     0.729433  0.811667  0.774792  0.792500   \n",
              "Dmoz_Sports_keyphrase2_GCN        0.75524  0.815556  0.801296  0.802716   \n",
              "Dmoz_Sports_keyphrase2_GAT       0.819385  0.877778  0.860833  0.873951   \n",
              "Dmoz_Sports_keyphrase3_GCN       0.781009  0.829630  0.827778  0.844568   \n",
              "Dmoz_Sports_keyphrase3_GAT       0.824113  0.882963  0.863796  0.873210   \n",
              "Dmoz_Sports_keyphrase23_GCN      0.769504  0.838148  0.821296  0.829012   \n",
              "Dmoz_Sports_keyphrase23_GAT      0.820725  0.875556  0.858704  0.868889   \n",
              "Industry_Sector_keyphrase2_GCN    0.31607  0.119615  0.101644  0.485730   \n",
              "Industry_Sector_keyphrase2_GAT   0.453943  0.763039  0.662461  0.705538   \n",
              "Industry_Sector_keyphrase3_GCN   0.356273  0.543651  0.525943  0.492723   \n",
              "Industry_Sector_keyphrase3_GAT   0.473927  0.760771  0.657641  0.709885   \n",
              "Industry_Sector_keyphrase23_GCN  0.377439  0.562925  0.545223  0.146097   \n",
              "Industry_Sector_keyphrase23_GAT  0.463994  0.767007  0.667706  0.721603   \n",
              "NSF_keyphrase2_GCN               0.784946  0.842280  0.814371  0.837688   \n",
              "NSF_keyphrase2_GAT               0.812923  0.855582  0.849406  0.850673   \n",
              "\n",
              "                                      60%  \n",
              "Dmoz_Computers_keyphrase2_GCN    0.685000  \n",
              "Dmoz_Computers_keyphrase2_GAT    0.701579  \n",
              "Dmoz_Computers_keyphrase3_GCN    0.695000  \n",
              "Dmoz_Computers_keyphrase3_GAT    0.704211  \n",
              "Dmoz_Computers_keyphrase23_GCN   0.681842  \n",
              "Dmoz_Computers_keyphrase23_GAT   0.698947  \n",
              "Dmoz_Health_keyphrase2_GCN       0.835000  \n",
              "Dmoz_Health_keyphrase2_GAT       0.860000  \n",
              "Dmoz_Health_keyphrase3_GCN       0.835385  \n",
              "Dmoz_Health_keyphrase3_GAT       0.863077  \n",
              "Dmoz_Health_keyphrase23_GCN      0.829615  \n",
              "Dmoz_Health_keyphrase23_GAT      0.866923  \n",
              "Dmoz_Science_keyphrase2_GCN      0.778333  \n",
              "Dmoz_Science_keyphrase2_GAT      0.800000  \n",
              "Dmoz_Science_keyphrase3_GCN      0.778333  \n",
              "Dmoz_Science_keyphrase3_GAT      0.799167  \n",
              "Dmoz_Science_keyphrase23_GCN     0.772500  \n",
              "Dmoz_Science_keyphrase23_GAT     0.802917  \n",
              "Dmoz_Sports_keyphrase2_GCN       0.815000  \n",
              "Dmoz_Sports_keyphrase2_GAT       0.874444  \n",
              "Dmoz_Sports_keyphrase3_GCN       0.830556  \n",
              "Dmoz_Sports_keyphrase3_GAT       0.878148  \n",
              "Dmoz_Sports_keyphrase23_GCN      0.832037  \n",
              "Dmoz_Sports_keyphrase23_GAT      0.873519  \n",
              "Industry_Sector_keyphrase2_GCN   0.506379  \n",
              "Industry_Sector_keyphrase2_GAT   0.743124  \n",
              "Industry_Sector_keyphrase3_GCN   0.492487  \n",
              "Industry_Sector_keyphrase3_GAT   0.756450  \n",
              "Industry_Sector_keyphrase23_GCN  0.531897  \n",
              "Industry_Sector_keyphrase23_GAT  0.752481  \n",
              "NSF_keyphrase2_GCN               0.825416  \n",
              "NSF_keyphrase2_GAT               0.854869  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb9a9ebc-166e-44d2-853b-04d20ab968b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>5</th>\n",
              "      <th>10</th>\n",
              "      <th>20</th>\n",
              "      <th>30</th>\n",
              "      <th>80%</th>\n",
              "      <th>20%</th>\n",
              "      <th>40%</th>\n",
              "      <th>60%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase2_GCN</th>\n",
              "      <td>0.229301</td>\n",
              "      <td>0.442531</td>\n",
              "      <td>0.514715</td>\n",
              "      <td>0.564583</td>\n",
              "      <td>0.579395</td>\n",
              "      <td>0.693158</td>\n",
              "      <td>0.620658</td>\n",
              "      <td>0.645965</td>\n",
              "      <td>0.685000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase2_GAT</th>\n",
              "      <td>0.307984</td>\n",
              "      <td>0.469644</td>\n",
              "      <td>0.540172</td>\n",
              "      <td>0.598684</td>\n",
              "      <td>0.610414</td>\n",
              "      <td>0.725789</td>\n",
              "      <td>0.669605</td>\n",
              "      <td>0.681930</td>\n",
              "      <td>0.701579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase3_GCN</th>\n",
              "      <td>0.279823</td>\n",
              "      <td>0.45008</td>\n",
              "      <td>0.534264</td>\n",
              "      <td>0.575</td>\n",
              "      <td>0.603471</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.648026</td>\n",
              "      <td>0.672632</td>\n",
              "      <td>0.695000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase3_GAT</th>\n",
              "      <td>0.299652</td>\n",
              "      <td>0.489208</td>\n",
              "      <td>0.551128</td>\n",
              "      <td>0.601645</td>\n",
              "      <td>0.615901</td>\n",
              "      <td>0.724211</td>\n",
              "      <td>0.661053</td>\n",
              "      <td>0.683860</td>\n",
              "      <td>0.704211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase23_GCN</th>\n",
              "      <td>0.185951</td>\n",
              "      <td>0.417438</td>\n",
              "      <td>0.510311</td>\n",
              "      <td>0.57193</td>\n",
              "      <td>0.58533</td>\n",
              "      <td>0.698421</td>\n",
              "      <td>0.644605</td>\n",
              "      <td>0.671930</td>\n",
              "      <td>0.681842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Computers_keyphrase23_GAT</th>\n",
              "      <td>0.261154</td>\n",
              "      <td>0.480489</td>\n",
              "      <td>0.553383</td>\n",
              "      <td>0.592325</td>\n",
              "      <td>0.610862</td>\n",
              "      <td>0.721053</td>\n",
              "      <td>0.666579</td>\n",
              "      <td>0.681053</td>\n",
              "      <td>0.698947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Health_keyphrase2_GCN</th>\n",
              "      <td>0.262371</td>\n",
              "      <td>0.693862</td>\n",
              "      <td>0.710832</td>\n",
              "      <td>0.747596</td>\n",
              "      <td>0.76743</td>\n",
              "      <td>0.823846</td>\n",
              "      <td>0.816731</td>\n",
              "      <td>0.826154</td>\n",
              "      <td>0.835000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Health_keyphrase2_GAT</th>\n",
              "      <td>0.421921</td>\n",
              "      <td>0.714841</td>\n",
              "      <td>0.754631</td>\n",
              "      <td>0.789744</td>\n",
              "      <td>0.81162</td>\n",
              "      <td>0.861538</td>\n",
              "      <td>0.851154</td>\n",
              "      <td>0.853077</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Health_keyphrase3_GCN</th>\n",
              "      <td>0.38292</td>\n",
              "      <td>0.681119</td>\n",
              "      <td>0.700157</td>\n",
              "      <td>0.764263</td>\n",
              "      <td>0.787398</td>\n",
              "      <td>0.838462</td>\n",
              "      <td>0.830192</td>\n",
              "      <td>0.835641</td>\n",
              "      <td>0.835385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Health_keyphrase3_GAT</th>\n",
              "      <td>0.418067</td>\n",
              "      <td>0.72805</td>\n",
              "      <td>0.756515</td>\n",
              "      <td>0.797596</td>\n",
              "      <td>0.809493</td>\n",
              "      <td>0.855385</td>\n",
              "      <td>0.850962</td>\n",
              "      <td>0.862564</td>\n",
              "      <td>0.863077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Health_keyphrase23_GCN</th>\n",
              "      <td>0.292585</td>\n",
              "      <td>0.667288</td>\n",
              "      <td>0.719937</td>\n",
              "      <td>0.755128</td>\n",
              "      <td>0.797872</td>\n",
              "      <td>0.841538</td>\n",
              "      <td>0.825385</td>\n",
              "      <td>0.825128</td>\n",
              "      <td>0.829615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Health_keyphrase23_GAT</th>\n",
              "      <td>0.403422</td>\n",
              "      <td>0.72028</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.797596</td>\n",
              "      <td>0.807529</td>\n",
              "      <td>0.862308</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.861795</td>\n",
              "      <td>0.866923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Science_keyphrase2_GCN</th>\n",
              "      <td>0.359385</td>\n",
              "      <td>0.547811</td>\n",
              "      <td>0.603571</td>\n",
              "      <td>0.672569</td>\n",
              "      <td>0.713298</td>\n",
              "      <td>0.799167</td>\n",
              "      <td>0.752500</td>\n",
              "      <td>0.766944</td>\n",
              "      <td>0.778333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Science_keyphrase2_GAT</th>\n",
              "      <td>0.342685</td>\n",
              "      <td>0.578114</td>\n",
              "      <td>0.672449</td>\n",
              "      <td>0.682292</td>\n",
              "      <td>0.737943</td>\n",
              "      <td>0.810833</td>\n",
              "      <td>0.777917</td>\n",
              "      <td>0.788889</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Science_keyphrase3_GCN</th>\n",
              "      <td>0.312291</td>\n",
              "      <td>0.518687</td>\n",
              "      <td>0.646259</td>\n",
              "      <td>0.664236</td>\n",
              "      <td>0.714894</td>\n",
              "      <td>0.800833</td>\n",
              "      <td>0.766250</td>\n",
              "      <td>0.771389</td>\n",
              "      <td>0.778333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Science_keyphrase3_GAT</th>\n",
              "      <td>0.406981</td>\n",
              "      <td>0.560606</td>\n",
              "      <td>0.671259</td>\n",
              "      <td>0.686979</td>\n",
              "      <td>0.738298</td>\n",
              "      <td>0.821667</td>\n",
              "      <td>0.775417</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.799167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Science_keyphrase23_GCN</th>\n",
              "      <td>0.341349</td>\n",
              "      <td>0.543434</td>\n",
              "      <td>0.633673</td>\n",
              "      <td>0.673785</td>\n",
              "      <td>0.722872</td>\n",
              "      <td>0.806667</td>\n",
              "      <td>0.752708</td>\n",
              "      <td>0.773889</td>\n",
              "      <td>0.772500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Science_keyphrase23_GAT</th>\n",
              "      <td>0.351369</td>\n",
              "      <td>0.556902</td>\n",
              "      <td>0.67585</td>\n",
              "      <td>0.683333</td>\n",
              "      <td>0.729433</td>\n",
              "      <td>0.811667</td>\n",
              "      <td>0.774792</td>\n",
              "      <td>0.792500</td>\n",
              "      <td>0.802917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Sports_keyphrase2_GCN</th>\n",
              "      <td>0.351741</td>\n",
              "      <td>0.655668</td>\n",
              "      <td>0.692668</td>\n",
              "      <td>0.740432</td>\n",
              "      <td>0.75524</td>\n",
              "      <td>0.815556</td>\n",
              "      <td>0.801296</td>\n",
              "      <td>0.802716</td>\n",
              "      <td>0.815000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Sports_keyphrase2_GAT</th>\n",
              "      <td>0.496846</td>\n",
              "      <td>0.725851</td>\n",
              "      <td>0.782011</td>\n",
              "      <td>0.799306</td>\n",
              "      <td>0.819385</td>\n",
              "      <td>0.877778</td>\n",
              "      <td>0.860833</td>\n",
              "      <td>0.873951</td>\n",
              "      <td>0.874444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Sports_keyphrase3_GCN</th>\n",
              "      <td>0.334818</td>\n",
              "      <td>0.652974</td>\n",
              "      <td>0.72706</td>\n",
              "      <td>0.756096</td>\n",
              "      <td>0.781009</td>\n",
              "      <td>0.829630</td>\n",
              "      <td>0.827778</td>\n",
              "      <td>0.844568</td>\n",
              "      <td>0.830556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Sports_keyphrase3_GAT</th>\n",
              "      <td>0.482966</td>\n",
              "      <td>0.719716</td>\n",
              "      <td>0.777324</td>\n",
              "      <td>0.817284</td>\n",
              "      <td>0.824113</td>\n",
              "      <td>0.882963</td>\n",
              "      <td>0.863796</td>\n",
              "      <td>0.873210</td>\n",
              "      <td>0.878148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Sports_keyphrase23_GCN</th>\n",
              "      <td>0.273362</td>\n",
              "      <td>0.629031</td>\n",
              "      <td>0.707785</td>\n",
              "      <td>0.757253</td>\n",
              "      <td>0.769504</td>\n",
              "      <td>0.838148</td>\n",
              "      <td>0.821296</td>\n",
              "      <td>0.829012</td>\n",
              "      <td>0.832037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dmoz_Sports_keyphrase23_GAT</th>\n",
              "      <td>0.454613</td>\n",
              "      <td>0.723606</td>\n",
              "      <td>0.770144</td>\n",
              "      <td>0.806096</td>\n",
              "      <td>0.820725</td>\n",
              "      <td>0.875556</td>\n",
              "      <td>0.858704</td>\n",
              "      <td>0.868889</td>\n",
              "      <td>0.873519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase2_GCN</th>\n",
              "      <td>0.10017</td>\n",
              "      <td>0.157017</td>\n",
              "      <td>0.302863</td>\n",
              "      <td>0.14877</td>\n",
              "      <td>0.31607</td>\n",
              "      <td>0.119615</td>\n",
              "      <td>0.101644</td>\n",
              "      <td>0.485730</td>\n",
              "      <td>0.506379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase2_GAT</th>\n",
              "      <td>0.103464</td>\n",
              "      <td>0.226447</td>\n",
              "      <td>0.381281</td>\n",
              "      <td>0.466946</td>\n",
              "      <td>0.453943</td>\n",
              "      <td>0.763039</td>\n",
              "      <td>0.662461</td>\n",
              "      <td>0.705538</td>\n",
              "      <td>0.743124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase3_GCN</th>\n",
              "      <td>0.113799</td>\n",
              "      <td>0.170949</td>\n",
              "      <td>0.281246</td>\n",
              "      <td>0.366212</td>\n",
              "      <td>0.356273</td>\n",
              "      <td>0.543651</td>\n",
              "      <td>0.525943</td>\n",
              "      <td>0.492723</td>\n",
              "      <td>0.492487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase3_GAT</th>\n",
              "      <td>0.128223</td>\n",
              "      <td>0.262533</td>\n",
              "      <td>0.383925</td>\n",
              "      <td>0.47231</td>\n",
              "      <td>0.473927</td>\n",
              "      <td>0.760771</td>\n",
              "      <td>0.657641</td>\n",
              "      <td>0.709885</td>\n",
              "      <td>0.756450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase23_GCN</th>\n",
              "      <td>0.077229</td>\n",
              "      <td>0.192874</td>\n",
              "      <td>0.274577</td>\n",
              "      <td>0.316078</td>\n",
              "      <td>0.377439</td>\n",
              "      <td>0.562925</td>\n",
              "      <td>0.545223</td>\n",
              "      <td>0.146097</td>\n",
              "      <td>0.531897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry_Sector_keyphrase23_GAT</th>\n",
              "      <td>0.086201</td>\n",
              "      <td>0.258308</td>\n",
              "      <td>0.385765</td>\n",
              "      <td>0.471494</td>\n",
              "      <td>0.463994</td>\n",
              "      <td>0.767007</td>\n",
              "      <td>0.667706</td>\n",
              "      <td>0.721603</td>\n",
              "      <td>0.752481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NSF_keyphrase2_GCN</th>\n",
              "      <td>0.471545</td>\n",
              "      <td>0.734201</td>\n",
              "      <td>0.744693</td>\n",
              "      <td>0.7696</td>\n",
              "      <td>0.784946</td>\n",
              "      <td>0.842280</td>\n",
              "      <td>0.814371</td>\n",
              "      <td>0.837688</td>\n",
              "      <td>0.825416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NSF_keyphrase2_GAT</th>\n",
              "      <td>0.594499</td>\n",
              "      <td>0.768575</td>\n",
              "      <td>0.770745</td>\n",
              "      <td>0.803606</td>\n",
              "      <td>0.812923</td>\n",
              "      <td>0.855582</td>\n",
              "      <td>0.849406</td>\n",
              "      <td>0.850673</td>\n",
              "      <td>0.854869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb9a9ebc-166e-44d2-853b-04d20ab968b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb9a9ebc-166e-44d2-853b-04d20ab968b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb9a9ebc-166e-44d2-853b-04d20ab968b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a74981a8-1b1f-4b12-a548-100d6eb8a9ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a74981a8-1b1f-4b12-a548-100d6eb8a9ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a74981a8-1b1f-4b12-a548-100d6eb8a9ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_list[0][0][0:32]\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.07722884724588303,\n        \"max\": 0.5944994290064712,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.08620102214650767,\n          0.4069806279225117,\n          0.10017035775127768\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.1570172433481786,\n        \"max\": 0.7685752585216392,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.2583076396026036,\n          0.5606060606060606,\n          0.1570172433481786\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.274577440496723,\n        \"max\": 0.782010582010582,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.3857652063930091,\n          0.6712585034013605,\n          0.3028630562262849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"20\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.14876996618864405,\n        \"max\": 0.817283950617284,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          0.3160778827095721,\n          0.6737847222222222,\n          0.14876996618864405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"30\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.31606952820148987,\n        \"max\": 0.8241134751773049,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.46399432422844983,\n          0.7382978723404255,\n          0.31606952820148987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"80%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14384600054681793,\n        \"min\": 0.11961451247165533,\n        \"max\": 0.882962962962963,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.7670068027210885,\n          0.8216666666666667,\n          0.11961451247165533\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"20%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15001684409958965,\n        \"min\": 0.10164445704564785,\n        \"max\": 0.8637962962962963,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.6677062659483981,\n          0.7754166666666666,\n          0.10164445704564785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"40%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1484475110324976,\n        \"min\": 0.1460971460971461,\n        \"max\": 0.8739506172839506,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.7216027216027217,\n          0.8,\n          0.4857304857304857\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"60%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10367344522420503,\n        \"min\": 0.4924865324638503,\n        \"max\": 0.8781481481481481,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          0.5318967961440317,\n          0.7725,\n          0.5063793592288064\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 369
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 26.2 ms (started: 2024-10-16 21:55:37 +00:00)\n"
          ]
        }
      ],
      "source": [
        "df_list[0][0][0:32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_list[0][0][32:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L6jfe10lFe-9",
        "outputId": "d5a93a90-dfea-418a-b0ab-f6ef1243039a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        1         5        10        20  \\\n",
              "NSF_keyphrase3_GCN               0.565188  0.722233   0.76235  0.779008   \n",
              "NSF_keyphrase3_GAT               0.583746  0.776235  0.770648  0.800666   \n",
              "NSF_keyphrase23_GCN              0.558622  0.702221  0.758684  0.774794   \n",
              "NSF_keyphrase23_GAT              0.587172  0.787917  0.777306  0.806644   \n",
              "classic4_keyphrase2_GCN          0.440558  0.851873  0.913253  0.948396   \n",
              "classic4_keyphrase2_GAT          0.805951  0.908693  0.894401  0.951675   \n",
              "classic4_keyphrase3_GCN          0.604851  0.818233  0.873848  0.942124   \n",
              "classic4_keyphrase3_GAT          0.800028  0.885936  0.871439  0.949679   \n",
              "classic4_keyphrase23_GCN         0.636582  0.777951  0.808079  0.945403   \n",
              "classic4_keyphrase23_GAT         0.806656  0.890601  0.898086  0.935139   \n",
              "CSTR_keyphrase2_GCN              0.267797  0.738351  0.818533  0.762557   \n",
              "CSTR_keyphrase2_GAT               0.39661  0.799283  0.833977  0.808219   \n",
              "CSTR_keyphrase3_GCN              0.532203   0.78853  0.787645  0.771689   \n",
              "CSTR_keyphrase3_GAT              0.447458  0.734767  0.822394  0.789954   \n",
              "CSTR_keyphrase23_GCN             0.491525  0.810036  0.803089  0.808219   \n",
              "CSTR_keyphrase23_GAT             0.477966  0.741935  0.826255  0.812785   \n",
              "re8_keyphrase2_GCN               0.549309  0.764999  0.839215  0.713335   \n",
              "re8_keyphrase2_GAT               0.686408  0.836128  0.881485  0.921879   \n",
              "re8_keyphrase3_GCN               0.765719  0.789625  0.831314  0.586638   \n",
              "re8_keyphrase3_GAT               0.581659  0.820671  0.891098  0.927336   \n",
              "re8_keyphrase23_GCN              0.603966  0.707624  0.852515  0.881288   \n",
              "re8_keyphrase23_GAT              0.387947  0.837307  0.887279  0.918552   \n",
              "review_polarity_keyphrase2_GCN   0.558559       0.5  0.534343  0.571429   \n",
              "review_polarity_keyphrase2_GAT   0.513514       0.5  0.523232   0.57602   \n",
              "review_polarity_keyphrase3_GCN   0.508509  0.550251       0.5  0.542347   \n",
              "review_polarity_keyphrase3_GAT   0.502503   0.50402  0.523232  0.585714   \n",
              "review_polarity_keyphrase23_GCN  0.527528  0.545226  0.541919       0.5   \n",
              "review_polarity_keyphrase23_GAT  0.527528  0.515578  0.535354  0.579592   \n",
              "SyskillWebert_keyphrase2_GCN     0.744681  0.779553  0.822526  0.869565   \n",
              "SyskillWebert_keyphrase2_GAT     0.617021  0.881789  0.897611  0.932806   \n",
              "SyskillWebert_keyphrase3_GCN     0.449848  0.875399  0.863481  0.893281   \n",
              "SyskillWebert_keyphrase3_GAT     0.732523   0.84345  0.897611  0.920949   \n",
              "SyskillWebert_keyphrase23_GCN    0.744681  0.760383  0.829352  0.897233   \n",
              "SyskillWebert_keyphrase23_GAT    0.613982  0.891374   0.87372  0.920949   \n",
              "webkb_parsed_keyphrase2_GCN      0.230453  0.343398  0.299927  0.180054   \n",
              "webkb_parsed_keyphrase2_GAT      0.166163  0.323754  0.441062  0.443012   \n",
              "webkb_parsed_keyphrase3_GCN      0.160363  0.349703  0.338529  0.414026   \n",
              "webkb_parsed_keyphrase3_GAT      0.155166  0.329089  0.446055  0.439573   \n",
              "webkb_parsed_keyphrase23_GCN         0.16   0.28568  0.358378  0.263203   \n",
              "webkb_parsed_keyphrase23_GAT     0.129668  0.335516  0.452265  0.431712   \n",
              "\n",
              "                                       30       80%       20%       40%  \\\n",
              "NSF_keyphrase3_GCN               0.795798  0.858432  0.841686  0.849248   \n",
              "NSF_keyphrase3_GAT               0.817403  0.852257  0.851544  0.857641   \n",
              "NSF_keyphrase23_GCN              0.790422  0.843705  0.835986  0.850515   \n",
              "NSF_keyphrase23_GAT              0.813222  0.866983  0.852969  0.859382   \n",
              "classic4_keyphrase2_GCN          0.942652  0.958421  0.952431  0.953723   \n",
              "classic4_keyphrase2_GAT          0.942509  0.965469  0.959126  0.956542   \n",
              "classic4_keyphrase3_GCN           0.94509  0.955603  0.952607  0.959126   \n",
              "classic4_keyphrase3_GAT          0.935054  0.964059  0.960183  0.957012   \n",
              "classic4_keyphrase23_GCN         0.933763  0.967583  0.949260  0.959126   \n",
              "classic4_keyphrase23_GAT         0.941219  0.963354  0.956483  0.955837   \n",
              "CSTR_keyphrase2_GCN                   NaN  0.816667  0.841667  0.844444   \n",
              "CSTR_keyphrase2_GAT                   NaN  0.800000  0.854167  0.838889   \n",
              "CSTR_keyphrase3_GCN                   NaN  0.783333  0.858333  0.861111   \n",
              "CSTR_keyphrase3_GAT                   NaN  0.816667  0.858333  0.850000   \n",
              "CSTR_keyphrase23_GCN                  NaN  0.800000  0.816667  0.827778   \n",
              "CSTR_keyphrase23_GAT                  NaN  0.816667  0.879167  0.861111   \n",
              "re8_keyphrase2_GCN               0.881894  0.935505  0.916775  0.934419   \n",
              "re8_keyphrase2_GAT               0.904089  0.951792  0.947883  0.946580   \n",
              "re8_keyphrase3_GCN               0.874765  0.941368  0.928176  0.938111   \n",
              "re8_keyphrase3_GAT               0.901399  0.955049  0.945928  0.950271   \n",
              "re8_keyphrase23_GCN              0.892252  0.946580  0.934365  0.939848   \n",
              "re8_keyphrase23_GAT              0.894135  0.953094  0.950000  0.948317   \n",
              "review_polarity_keyphrase2_GCN   0.495876  0.500000  0.532500  0.500000   \n",
              "review_polarity_keyphrase2_GAT   0.571134  0.500000  0.683125  0.644167   \n",
              "review_polarity_keyphrase3_GCN        0.5  0.500000  0.556250  0.500000   \n",
              "review_polarity_keyphrase3_GAT   0.578351  0.500000  0.675625  0.648333   \n",
              "review_polarity_keyphrase23_GCN       0.5  0.500000  0.500000  0.500000   \n",
              "review_polarity_keyphrase23_GAT  0.566495  0.500000  0.500000  0.500000   \n",
              "SyskillWebert_keyphrase2_GCN     0.877934  0.925373  0.910112  0.875000   \n",
              "SyskillWebert_keyphrase2_GAT     0.934272  0.955224  0.906367  0.950000   \n",
              "SyskillWebert_keyphrase3_GCN     0.901408  0.895522  0.831461  0.895000   \n",
              "SyskillWebert_keyphrase3_GAT     0.934272  0.925373  0.906367  0.925000   \n",
              "SyskillWebert_keyphrase23_GCN    0.910798  0.940299  0.872659  0.900000   \n",
              "SyskillWebert_keyphrase23_GAT    0.929577  0.940299  0.883895  0.935000   \n",
              "webkb_parsed_keyphrase2_GCN      0.422324  0.436934  0.416390  0.597787   \n",
              "webkb_parsed_keyphrase2_GAT      0.514618  0.745323  0.716269  0.738229   \n",
              "webkb_parsed_keyphrase3_GCN      0.328419  0.519614  0.606852  0.485312   \n",
              "webkb_parsed_keyphrase3_GAT      0.512265  0.756789  0.707365  0.739638   \n",
              "webkb_parsed_keyphrase23_GCN     0.412537  0.366928  0.454573  0.429980   \n",
              "webkb_parsed_keyphrase23_GAT     0.518583  0.743512  0.715062  0.734004   \n",
              "\n",
              "                                      60%  \n",
              "NSF_keyphrase3_GCN               0.858670  \n",
              "NSF_keyphrase3_GAT               0.858907  \n",
              "NSF_keyphrase23_GCN              0.856532  \n",
              "NSF_keyphrase23_GAT              0.859857  \n",
              "classic4_keyphrase2_GCN          0.959126  \n",
              "classic4_keyphrase2_GAT          0.957012  \n",
              "classic4_keyphrase3_GCN          0.961240  \n",
              "classic4_keyphrase3_GAT          0.961240  \n",
              "classic4_keyphrase23_GCN         0.955603  \n",
              "classic4_keyphrase23_GAT         0.960183  \n",
              "CSTR_keyphrase2_GCN              0.833333  \n",
              "CSTR_keyphrase2_GAT              0.858333  \n",
              "CSTR_keyphrase3_GCN              0.875000  \n",
              "CSTR_keyphrase3_GAT              0.858333  \n",
              "CSTR_keyphrase23_GCN             0.850000  \n",
              "CSTR_keyphrase23_GAT             0.866667  \n",
              "re8_keyphrase2_GCN               0.925407  \n",
              "re8_keyphrase2_GAT               0.951792  \n",
              "re8_keyphrase3_GCN               0.930619  \n",
              "re8_keyphrase3_GAT               0.946580  \n",
              "re8_keyphrase23_GCN              0.934853  \n",
              "re8_keyphrase23_GAT              0.950814  \n",
              "review_polarity_keyphrase2_GCN   0.526250  \n",
              "review_polarity_keyphrase2_GAT   0.512500  \n",
              "review_polarity_keyphrase3_GCN   0.500000  \n",
              "review_polarity_keyphrase3_GAT   0.500000  \n",
              "review_polarity_keyphrase23_GCN  0.500000  \n",
              "review_polarity_keyphrase23_GAT  0.500000  \n",
              "SyskillWebert_keyphrase2_GCN     0.925373  \n",
              "SyskillWebert_keyphrase2_GAT     0.955224  \n",
              "SyskillWebert_keyphrase3_GCN     0.888060  \n",
              "SyskillWebert_keyphrase3_GAT     0.947761  \n",
              "SyskillWebert_keyphrase23_GCN    0.902985  \n",
              "SyskillWebert_keyphrase23_GAT    0.947761  \n",
              "webkb_parsed_keyphrase2_GCN      0.310595  \n",
              "webkb_parsed_keyphrase2_GAT      0.741624  \n",
              "webkb_parsed_keyphrase3_GCN      0.526411  \n",
              "webkb_parsed_keyphrase3_GAT      0.736794  \n",
              "webkb_parsed_keyphrase23_GCN     0.498340  \n",
              "webkb_parsed_keyphrase23_GAT     0.742831  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5981ee4a-b049-4808-95b0-4b9132defd4a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>5</th>\n",
              "      <th>10</th>\n",
              "      <th>20</th>\n",
              "      <th>30</th>\n",
              "      <th>80%</th>\n",
              "      <th>20%</th>\n",
              "      <th>40%</th>\n",
              "      <th>60%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NSF_keyphrase3_GCN</th>\n",
              "      <td>0.565188</td>\n",
              "      <td>0.722233</td>\n",
              "      <td>0.76235</td>\n",
              "      <td>0.779008</td>\n",
              "      <td>0.795798</td>\n",
              "      <td>0.858432</td>\n",
              "      <td>0.841686</td>\n",
              "      <td>0.849248</td>\n",
              "      <td>0.858670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NSF_keyphrase3_GAT</th>\n",
              "      <td>0.583746</td>\n",
              "      <td>0.776235</td>\n",
              "      <td>0.770648</td>\n",
              "      <td>0.800666</td>\n",
              "      <td>0.817403</td>\n",
              "      <td>0.852257</td>\n",
              "      <td>0.851544</td>\n",
              "      <td>0.857641</td>\n",
              "      <td>0.858907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NSF_keyphrase23_GCN</th>\n",
              "      <td>0.558622</td>\n",
              "      <td>0.702221</td>\n",
              "      <td>0.758684</td>\n",
              "      <td>0.774794</td>\n",
              "      <td>0.790422</td>\n",
              "      <td>0.843705</td>\n",
              "      <td>0.835986</td>\n",
              "      <td>0.850515</td>\n",
              "      <td>0.856532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NSF_keyphrase23_GAT</th>\n",
              "      <td>0.587172</td>\n",
              "      <td>0.787917</td>\n",
              "      <td>0.777306</td>\n",
              "      <td>0.806644</td>\n",
              "      <td>0.813222</td>\n",
              "      <td>0.866983</td>\n",
              "      <td>0.852969</td>\n",
              "      <td>0.859382</td>\n",
              "      <td>0.859857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classic4_keyphrase2_GCN</th>\n",
              "      <td>0.440558</td>\n",
              "      <td>0.851873</td>\n",
              "      <td>0.913253</td>\n",
              "      <td>0.948396</td>\n",
              "      <td>0.942652</td>\n",
              "      <td>0.958421</td>\n",
              "      <td>0.952431</td>\n",
              "      <td>0.953723</td>\n",
              "      <td>0.959126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classic4_keyphrase2_GAT</th>\n",
              "      <td>0.805951</td>\n",
              "      <td>0.908693</td>\n",
              "      <td>0.894401</td>\n",
              "      <td>0.951675</td>\n",
              "      <td>0.942509</td>\n",
              "      <td>0.965469</td>\n",
              "      <td>0.959126</td>\n",
              "      <td>0.956542</td>\n",
              "      <td>0.957012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classic4_keyphrase3_GCN</th>\n",
              "      <td>0.604851</td>\n",
              "      <td>0.818233</td>\n",
              "      <td>0.873848</td>\n",
              "      <td>0.942124</td>\n",
              "      <td>0.94509</td>\n",
              "      <td>0.955603</td>\n",
              "      <td>0.952607</td>\n",
              "      <td>0.959126</td>\n",
              "      <td>0.961240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classic4_keyphrase3_GAT</th>\n",
              "      <td>0.800028</td>\n",
              "      <td>0.885936</td>\n",
              "      <td>0.871439</td>\n",
              "      <td>0.949679</td>\n",
              "      <td>0.935054</td>\n",
              "      <td>0.964059</td>\n",
              "      <td>0.960183</td>\n",
              "      <td>0.957012</td>\n",
              "      <td>0.961240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classic4_keyphrase23_GCN</th>\n",
              "      <td>0.636582</td>\n",
              "      <td>0.777951</td>\n",
              "      <td>0.808079</td>\n",
              "      <td>0.945403</td>\n",
              "      <td>0.933763</td>\n",
              "      <td>0.967583</td>\n",
              "      <td>0.949260</td>\n",
              "      <td>0.959126</td>\n",
              "      <td>0.955603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classic4_keyphrase23_GAT</th>\n",
              "      <td>0.806656</td>\n",
              "      <td>0.890601</td>\n",
              "      <td>0.898086</td>\n",
              "      <td>0.935139</td>\n",
              "      <td>0.941219</td>\n",
              "      <td>0.963354</td>\n",
              "      <td>0.956483</td>\n",
              "      <td>0.955837</td>\n",
              "      <td>0.960183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSTR_keyphrase2_GCN</th>\n",
              "      <td>0.267797</td>\n",
              "      <td>0.738351</td>\n",
              "      <td>0.818533</td>\n",
              "      <td>0.762557</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.841667</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSTR_keyphrase2_GAT</th>\n",
              "      <td>0.39661</td>\n",
              "      <td>0.799283</td>\n",
              "      <td>0.833977</td>\n",
              "      <td>0.808219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.854167</td>\n",
              "      <td>0.838889</td>\n",
              "      <td>0.858333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSTR_keyphrase3_GCN</th>\n",
              "      <td>0.532203</td>\n",
              "      <td>0.78853</td>\n",
              "      <td>0.787645</td>\n",
              "      <td>0.771689</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.783333</td>\n",
              "      <td>0.858333</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSTR_keyphrase3_GAT</th>\n",
              "      <td>0.447458</td>\n",
              "      <td>0.734767</td>\n",
              "      <td>0.822394</td>\n",
              "      <td>0.789954</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.858333</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.858333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSTR_keyphrase23_GCN</th>\n",
              "      <td>0.491525</td>\n",
              "      <td>0.810036</td>\n",
              "      <td>0.803089</td>\n",
              "      <td>0.808219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.827778</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSTR_keyphrase23_GAT</th>\n",
              "      <td>0.477966</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>0.826255</td>\n",
              "      <td>0.812785</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.816667</td>\n",
              "      <td>0.879167</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>re8_keyphrase2_GCN</th>\n",
              "      <td>0.549309</td>\n",
              "      <td>0.764999</td>\n",
              "      <td>0.839215</td>\n",
              "      <td>0.713335</td>\n",
              "      <td>0.881894</td>\n",
              "      <td>0.935505</td>\n",
              "      <td>0.916775</td>\n",
              "      <td>0.934419</td>\n",
              "      <td>0.925407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>re8_keyphrase2_GAT</th>\n",
              "      <td>0.686408</td>\n",
              "      <td>0.836128</td>\n",
              "      <td>0.881485</td>\n",
              "      <td>0.921879</td>\n",
              "      <td>0.904089</td>\n",
              "      <td>0.951792</td>\n",
              "      <td>0.947883</td>\n",
              "      <td>0.946580</td>\n",
              "      <td>0.951792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>re8_keyphrase3_GCN</th>\n",
              "      <td>0.765719</td>\n",
              "      <td>0.789625</td>\n",
              "      <td>0.831314</td>\n",
              "      <td>0.586638</td>\n",
              "      <td>0.874765</td>\n",
              "      <td>0.941368</td>\n",
              "      <td>0.928176</td>\n",
              "      <td>0.938111</td>\n",
              "      <td>0.930619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>re8_keyphrase3_GAT</th>\n",
              "      <td>0.581659</td>\n",
              "      <td>0.820671</td>\n",
              "      <td>0.891098</td>\n",
              "      <td>0.927336</td>\n",
              "      <td>0.901399</td>\n",
              "      <td>0.955049</td>\n",
              "      <td>0.945928</td>\n",
              "      <td>0.950271</td>\n",
              "      <td>0.946580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>re8_keyphrase23_GCN</th>\n",
              "      <td>0.603966</td>\n",
              "      <td>0.707624</td>\n",
              "      <td>0.852515</td>\n",
              "      <td>0.881288</td>\n",
              "      <td>0.892252</td>\n",
              "      <td>0.946580</td>\n",
              "      <td>0.934365</td>\n",
              "      <td>0.939848</td>\n",
              "      <td>0.934853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>re8_keyphrase23_GAT</th>\n",
              "      <td>0.387947</td>\n",
              "      <td>0.837307</td>\n",
              "      <td>0.887279</td>\n",
              "      <td>0.918552</td>\n",
              "      <td>0.894135</td>\n",
              "      <td>0.953094</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.948317</td>\n",
              "      <td>0.950814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_polarity_keyphrase2_GCN</th>\n",
              "      <td>0.558559</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.534343</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.495876</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.532500</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.526250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_polarity_keyphrase2_GAT</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.523232</td>\n",
              "      <td>0.57602</td>\n",
              "      <td>0.571134</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.683125</td>\n",
              "      <td>0.644167</td>\n",
              "      <td>0.512500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_polarity_keyphrase3_GCN</th>\n",
              "      <td>0.508509</td>\n",
              "      <td>0.550251</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.542347</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.556250</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_polarity_keyphrase3_GAT</th>\n",
              "      <td>0.502503</td>\n",
              "      <td>0.50402</td>\n",
              "      <td>0.523232</td>\n",
              "      <td>0.585714</td>\n",
              "      <td>0.578351</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.675625</td>\n",
              "      <td>0.648333</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_polarity_keyphrase23_GCN</th>\n",
              "      <td>0.527528</td>\n",
              "      <td>0.545226</td>\n",
              "      <td>0.541919</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review_polarity_keyphrase23_GAT</th>\n",
              "      <td>0.527528</td>\n",
              "      <td>0.515578</td>\n",
              "      <td>0.535354</td>\n",
              "      <td>0.579592</td>\n",
              "      <td>0.566495</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyskillWebert_keyphrase2_GCN</th>\n",
              "      <td>0.744681</td>\n",
              "      <td>0.779553</td>\n",
              "      <td>0.822526</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.877934</td>\n",
              "      <td>0.925373</td>\n",
              "      <td>0.910112</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.925373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyskillWebert_keyphrase2_GAT</th>\n",
              "      <td>0.617021</td>\n",
              "      <td>0.881789</td>\n",
              "      <td>0.897611</td>\n",
              "      <td>0.932806</td>\n",
              "      <td>0.934272</td>\n",
              "      <td>0.955224</td>\n",
              "      <td>0.906367</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.955224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyskillWebert_keyphrase3_GCN</th>\n",
              "      <td>0.449848</td>\n",
              "      <td>0.875399</td>\n",
              "      <td>0.863481</td>\n",
              "      <td>0.893281</td>\n",
              "      <td>0.901408</td>\n",
              "      <td>0.895522</td>\n",
              "      <td>0.831461</td>\n",
              "      <td>0.895000</td>\n",
              "      <td>0.888060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyskillWebert_keyphrase3_GAT</th>\n",
              "      <td>0.732523</td>\n",
              "      <td>0.84345</td>\n",
              "      <td>0.897611</td>\n",
              "      <td>0.920949</td>\n",
              "      <td>0.934272</td>\n",
              "      <td>0.925373</td>\n",
              "      <td>0.906367</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.947761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyskillWebert_keyphrase23_GCN</th>\n",
              "      <td>0.744681</td>\n",
              "      <td>0.760383</td>\n",
              "      <td>0.829352</td>\n",
              "      <td>0.897233</td>\n",
              "      <td>0.910798</td>\n",
              "      <td>0.940299</td>\n",
              "      <td>0.872659</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.902985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SyskillWebert_keyphrase23_GAT</th>\n",
              "      <td>0.613982</td>\n",
              "      <td>0.891374</td>\n",
              "      <td>0.87372</td>\n",
              "      <td>0.920949</td>\n",
              "      <td>0.929577</td>\n",
              "      <td>0.940299</td>\n",
              "      <td>0.883895</td>\n",
              "      <td>0.935000</td>\n",
              "      <td>0.947761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webkb_parsed_keyphrase2_GCN</th>\n",
              "      <td>0.230453</td>\n",
              "      <td>0.343398</td>\n",
              "      <td>0.299927</td>\n",
              "      <td>0.180054</td>\n",
              "      <td>0.422324</td>\n",
              "      <td>0.436934</td>\n",
              "      <td>0.416390</td>\n",
              "      <td>0.597787</td>\n",
              "      <td>0.310595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webkb_parsed_keyphrase2_GAT</th>\n",
              "      <td>0.166163</td>\n",
              "      <td>0.323754</td>\n",
              "      <td>0.441062</td>\n",
              "      <td>0.443012</td>\n",
              "      <td>0.514618</td>\n",
              "      <td>0.745323</td>\n",
              "      <td>0.716269</td>\n",
              "      <td>0.738229</td>\n",
              "      <td>0.741624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webkb_parsed_keyphrase3_GCN</th>\n",
              "      <td>0.160363</td>\n",
              "      <td>0.349703</td>\n",
              "      <td>0.338529</td>\n",
              "      <td>0.414026</td>\n",
              "      <td>0.328419</td>\n",
              "      <td>0.519614</td>\n",
              "      <td>0.606852</td>\n",
              "      <td>0.485312</td>\n",
              "      <td>0.526411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webkb_parsed_keyphrase3_GAT</th>\n",
              "      <td>0.155166</td>\n",
              "      <td>0.329089</td>\n",
              "      <td>0.446055</td>\n",
              "      <td>0.439573</td>\n",
              "      <td>0.512265</td>\n",
              "      <td>0.756789</td>\n",
              "      <td>0.707365</td>\n",
              "      <td>0.739638</td>\n",
              "      <td>0.736794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webkb_parsed_keyphrase23_GCN</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.28568</td>\n",
              "      <td>0.358378</td>\n",
              "      <td>0.263203</td>\n",
              "      <td>0.412537</td>\n",
              "      <td>0.366928</td>\n",
              "      <td>0.454573</td>\n",
              "      <td>0.429980</td>\n",
              "      <td>0.498340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>webkb_parsed_keyphrase23_GAT</th>\n",
              "      <td>0.129668</td>\n",
              "      <td>0.335516</td>\n",
              "      <td>0.452265</td>\n",
              "      <td>0.431712</td>\n",
              "      <td>0.518583</td>\n",
              "      <td>0.743512</td>\n",
              "      <td>0.715062</td>\n",
              "      <td>0.734004</td>\n",
              "      <td>0.742831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5981ee4a-b049-4808-95b0-4b9132defd4a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5981ee4a-b049-4808-95b0-4b9132defd4a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5981ee4a-b049-4808-95b0-4b9132defd4a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-37ee2893-32dd-4180-a855-f66a63d64674\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37ee2893-32dd-4180-a855-f66a63d64674')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-37ee2893-32dd-4180-a855-f66a63d64674 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_list[0][0][32:]\",\n  \"rows\": 40,\n  \"fields\": [\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.1296676737160121,\n        \"max\": 0.8066563249189113,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          0.1661631419939577,\n          0.16,\n          0.44055845437879004\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.28567964108160543,\n        \"max\": 0.9086925795053004,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          0.3433975991269553,\n          0.3290893658299988,\n          0.8518727915194346\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.2999269361909401,\n        \"max\": 0.9132530120481928,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          0.44106186069167075,\n          0.35837798343886995,\n          0.9132530120481928\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"20\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.18005404077622206,\n        \"max\": 0.9516749821810406,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          0.4430115450749202,\n          0.2632031441906166,\n          0.9483962936564505\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"30\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.32841922695738357,\n        \"max\": 0.9450896057347671,\n        \"num_unique_values\": 32,\n        \"samples\": [\n          0.5122646184340932,\n          0.8941350551520043,\n          0.9107981220657277\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"80%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18618771455641772,\n        \"min\": 0.36692818346409173,\n        \"max\": 0.9675828047921071,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          0.7567893783946892,\n          0.9413680781758957,\n          0.9402985074626866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"20%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1599778512298811,\n        \"min\": 0.4163899788711138,\n        \"max\": 0.9601832276250881,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          0.9281758957654723,\n          0.8166666666666667,\n          0.952431289640592\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"40%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.165602372328553,\n        \"min\": 0.4299798792756539,\n        \"max\": 0.9591261451726568,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          0.925,\n          0.8277777777777777,\n          0.95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"60%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18531315859065198,\n        \"min\": 0.31059462722607906,\n        \"max\": 0.9612403100775194,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          0.9517915309446254,\n          0.950814332247557,\n          0.9029850746268657\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 370
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 30.5 ms (started: 2024-10-16 21:55:37 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F42AmIbTm_Uk"
      },
      "source": [
        "--------------------------------------\n",
        "# Hetrogenous Graph for Keyphrase = 2\n",
        "--------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN5oB59YhW7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38df4516-a833-4b9e-c9fe-d4b1e8101b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basic_materials' 'capital_goods' 'conglomerates_industry'\n",
            " 'consumer_cyclical' 'consumer_non-cyclical' 'energy' 'financial'\n",
            " 'healthcare' 'services' 'technology' 'transportation' 'utilities']\n",
            "12\n",
            "time: 3.89 ms (started: 2024-10-16 20:50:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(df[\"class\"].unique())\n",
        "class_number = len(df[\"class\"].unique())\n",
        "print(class_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JLOrZczNBsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8992262c-ea76-4201-9d12-fba4873ab295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 444 µs (started: 2024-10-16 20:50:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Change here to change wich keypharse to use\n",
        "keyphrase = \"keyphrase2\"\n",
        "\n",
        "model_name = dataset_name+\"_\"+keyphrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9tMAdQPDowd"
      },
      "source": [
        "## Creating the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrRkgxKu2Wiw"
      },
      "source": [
        "### Defining Graph Nodes and Edges 👀\n",
        "\n",
        "- `Nodes` - documents and contexts\n",
        "- `Edges`\n",
        "  - document <- has -> context\n",
        "- `Labels` - documents classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_kBUMGf_lWu"
      },
      "source": [
        "#### Nodes and Edges 👀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVfGvWUSs0z6"
      },
      "source": [
        "##### Defining Docmuent nodes, Context nodes and edges between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY-ZzJGr96m9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1b9a51-b579-4536-f854-7eef701a8156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 35.2 s (started: 2024-10-16 20:50:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "all_contexts_list =[]\n",
        "edges1,edges2 = [],[]\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes = []\n",
        "context_nodes = []\n",
        "\n",
        "edges_tuple = []\n",
        "\n",
        "sentences = []\n",
        "cont_sentences = 0\n",
        "dit_sentences = {}\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding document nodes for every doc in df\n",
        "    document_nodes.append(df[\"text_embeddings\"][i])\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list: # if NOT\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes.append(df[keyphrase+\"_embeddings\"][i][j])\n",
        "            # add a new edge between doc and new context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(new_edge_cont)\n",
        "            edges_tuple.append((df[\"id\"][i],new_edge_cont))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(all_contexts_list.index(key[0]))\n",
        "            edges_tuple.append((df[\"id\"][i],all_contexts_list.index(key[0])))\n",
        "            cont+=1\n",
        "\n",
        "    # organize sentences, sentences_embeddings, and a dict with the corresponding document for each sentence\n",
        "    aux = df['sentences_embeddings'][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        sentences.append(key)\n",
        "        dit_sentences[cont_sentences] = df[\"id\"][i]\n",
        "        cont_sentences += 1\n",
        "\n",
        "\n",
        "document_nodes = np.array(document_nodes)\n",
        "context_nodes = np.array(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXOWqRQplTad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273c956b-1702-4625-839f-8d7bcb5984ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 8817\n",
            "number of context nodes: 31332\n",
            "number of shared contexts: 11831\n",
            "number of direct edges (first dimension): 43163\n",
            "number of direct edges (second dimension): 43163\n",
            "time: 938 µs (started: 2024-10-16 20:51:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"number of document nodes:\",len(document_nodes))\n",
        "print(\"number of context nodes:\",len(context_nodes))\n",
        "print(\"number of shared contexts:\",cont)\n",
        "print(\"number of direct edges (first dimension):\",len(edges1))\n",
        "print(\"number of direct edges (second dimension):\",len(edges2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaRvK2v-9KCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbb9bb9-0c73-48ce-a168-9f7cd68f19ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1min 55s (started: 2024-10-16 20:51:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=15, metric='cosine').fit(context_nodes)\n",
        "\n",
        "neighbors_list = nbrs.kneighbors(sentences, return_distance=False)\n",
        "\n",
        "# cria aresta para cada vizinho encontrado\n",
        "for i,neighbors in enumerate(neighbors_list):\n",
        "        for n in neighbors:\n",
        "            edges1.append(dit_sentences[i])\n",
        "            edges2.append(n)\n",
        "            edges_tuple.append((dit_sentences[i],n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp2TAceotXLx"
      },
      "source": [
        "##### Ajusting everything to Tensor Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDIlnLOPxgNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34094658-e4cb-4dbe-e4fd-d55285e7fa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 384 ms (started: 2024-10-16 20:53:27 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# transforms egdes to tensor\n",
        "edges = np.array([edges1,edges2])\n",
        "edges = torch.tensor(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugk20HV6lZk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28aaf3db-59c9-4f53-f26f-ee46e1eb902f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 64.7 ms (started: 2024-10-16 20:53:27 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# transforms nodes to tensor\n",
        "document_nodes = np.array(document_nodes)\n",
        "document_nodes = torch.tensor(document_nodes)\n",
        "context_nodes = np.array(context_nodes)\n",
        "context_nodes = torch.tensor(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p3qraqHR3hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822a5a20-61e3-49da-8255-159f3f9c167d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.8602e-02,  1.4557e-02,  1.7552e-02,  ..., -1.8911e-02,\n",
              "         -1.8681e-02, -2.2412e-02],\n",
              "        [-1.2433e-01, -1.2993e-02,  1.4001e-02,  ..., -1.7944e-02,\n",
              "          1.1914e-02, -8.1062e-03],\n",
              "        [-8.5865e-02,  4.3530e-02, -7.1712e-02,  ...,  5.6326e-02,\n",
              "         -4.5610e-02,  3.1375e-02],\n",
              "        ...,\n",
              "        [-1.6584e-02, -3.9770e-02, -2.2835e-02,  ..., -4.2101e-05,\n",
              "          1.2257e-02,  2.9048e-02],\n",
              "        [ 3.1864e-02, -1.0648e-02, -3.8576e-02,  ..., -4.5610e-03,\n",
              "         -1.7202e-02,  4.2433e-02],\n",
              "        [ 4.3771e-02,  3.8128e-02,  2.0526e-02,  ..., -1.0110e-02,\n",
              "          3.8847e-02,  4.4935e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.2 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show documents nodes\n",
        "document_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVG81JF6RTf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d2b8f0-c707-4630-93ef-5265fc07a7be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 215
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.65 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of document nodes\n",
        "len(document_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku5cQDCur-y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292edef2-5ad3-4497-afc2-64dc91044be9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0708,  0.0530,  0.0383,  ...,  0.0177,  0.0535, -0.0077],\n",
              "        [-0.0939, -0.0394, -0.0014,  ..., -0.0023,  0.0032, -0.0307],\n",
              "        [-0.1086,  0.0885, -0.0217,  ...,  0.0218,  0.0936,  0.0187],\n",
              "        ...,\n",
              "        [ 0.0020, -0.0129, -0.0552,  ..., -0.0435,  0.0482,  0.0015],\n",
              "        [-0.0278,  0.0173, -0.0167,  ..., -0.0360,  0.0564,  0.0076],\n",
              "        [-0.0547,  0.0289,  0.0072,  ...,  0.0046, -0.0190,  0.0114]])"
            ]
          },
          "metadata": {},
          "execution_count": 216
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.61 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show context nodes\n",
        "context_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWiCUWg5r-y4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bf6bcd-abfc-46f7-933e-48ba4fa43528"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31332"
            ]
          },
          "metadata": {},
          "execution_count": 217
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.44 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of context nodes\n",
        "len(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4EeQVr62eVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b56b3a-5293-4990-d330-e5880d0821fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1946768\n",
            "1946768\n",
            "time: 846 µs (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of edges\n",
        "print(len(edges[0]))\n",
        "print(len(edges[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FOMuN9XC-0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d75f8b-bdd4-42f4-e00a-232f02089744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ...,  8816,  8816,  8816],\n",
            "        [    0,     1,     2,  ..., 23841, 17346, 27373]])\n",
            "time: 3.11 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# showing edges\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXGrNtvp_pvm"
      },
      "source": [
        "#### Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRZiISzyRMcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21d25e8-905c-4bdb-c972-cf20c37a27ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basic_materials' 'capital_goods' 'conglomerates_industry'\n",
            " 'consumer_cyclical' 'consumer_non-cyclical' 'energy' 'financial'\n",
            " 'healthcare' 'services' 'technology' 'transportation' 'utilities']\n",
            "time: 1.51 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# All different classes\n",
        "print(df[\"class\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjvgRobhDwU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac050cc6-fcc0-45b1-e324-f422b6f2a07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'basic_materials': 0, 'capital_goods': 1, 'conglomerates_industry': 2, 'consumer_cyclical': 3, 'consumer_non-cyclical': 4, 'energy': 5, 'financial': 6, 'healthcare': 7, 'services': 8, 'technology': 9, 'transportation': 10, 'utilities': 11} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-221-a018e46c97dd>:12: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
            "<ipython-input-221-a018e46c97dd>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class\n",
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-840a62c0-073a-479f-8615-22ad6f0b1169\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-840a62c0-073a-479f-8615-22ad6f0b1169')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-840a62c0-073a-479f-8615-22ad6f0b1169 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-840a62c0-073a-479f-8615-22ad6f0b1169');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-359fa67b-1c86-44cd-b975-5920c0de63a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-359fa67b-1c86-44cd-b975-5920c0de63a4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-359fa67b-1c86-44cd-b975-5920c0de63a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels",
              "summary": "{\n  \"name\": \"labels\",\n  \"rows\": 8817,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10,\n          9,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 221
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 874 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Creating labels for classification\n",
        "# the dictionary is a numeric representation for each possible \"document\" class\n",
        "\n",
        "dit = {}\n",
        "for i,classe in enumerate(df[\"class\"].unique()):\n",
        "  dit[classe] = i\n",
        "\n",
        "print(dit,'\\n')\n",
        "\n",
        "labels = df[[\"class\"]]\n",
        "for i in range(len(df[[\"class\"]])):\n",
        "    labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3wqIjcRyV8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa3fd61-3f92-496f-dc22-b0264ca2edbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  ..., 11, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.26 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# tranfors class dataframe into tensor\n",
        "y = labels[\"class\"].tolist()\n",
        "y = x_np = torch.tensor(y)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofxnswq6RlNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea63222b-2d07-472c-8512-444d0172c260"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 223
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.17 ms (started: 2024-10-16 20:53:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olhv0tQ8Skgj"
      },
      "source": [
        "### Testing Graph with Networkx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oj2BlyrsCdP"
      },
      "source": [
        "#### Defining overal graph in networkx maner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6cZnmOq-UcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e53fe1-4001-4242-b7d9-bc6a6e8a9a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.2 s (started: 2024-10-16 20:53:29 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Run graph Representation for networkx\n",
        "all_contexts_list =[]\n",
        "edges_test = []\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes_test = []\n",
        "context_nodes_test = []\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding new documents for every node\n",
        "    document_nodes_test.append(\"doc_\"+str(i)) # in the actual graph nodes -> documents embeddings\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list:\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes_test.append(\"contx_\"+str(new_edge_cont)) # in the actual graph nodes -> context embeddings\n",
        "\n",
        "            # add a new edge between doc and new context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(new_edge_cont)))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(all_contexts_list.index(key[0]))))\n",
        "            cont+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p15FEl45BtKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100e203a-bd0b-4c9a-d354-621de06b8b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.25 s (started: 2024-10-16 20:53:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "edges_test = [(\"doc_\"+str(i[0]),\"contx_\"+str(i[1])) for i in edges_tuple]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfCPWtQ3fk23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71882ac-a536-4b83-d00f-d42a6c80c550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 8817\n",
            "number of context nodes: 31332\n",
            "number of edges: 1946768\n",
            "time: 1.1 ms (started: 2024-10-16 20:53:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"number of document nodes:\",len(document_nodes_test))\n",
        "print(\"number of context nodes:\",len(context_nodes_test))\n",
        "print(\"number of edges:\",len(edges_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1OSvVPAij1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b378b8dd-72d2-4149-a571-2e434875fe4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('doc_0', 'contx_0'), ('doc_0', 'contx_1'), ('doc_0', 'contx_2'), ('doc_0', 'contx_3'), ('doc_0', 'contx_4'), ('doc_1', 'contx_5'), ('doc_1', 'contx_6'), ('doc_1', 'contx_7'), ('doc_1', 'contx_8'), ('doc_1', 'contx_9'), ('doc_2', 'contx_10'), ('doc_2', 'contx_11'), ('doc_2', 'contx_12'), ('doc_2', 'contx_13'), ('doc_2', 'contx_14'), ('doc_3', 'contx_15'), ('doc_3', 'contx_16'), ('doc_3', 'contx_17'), ('doc_3', 'contx_18'), ('doc_3', 'contx_19'), ('doc_4', 'contx_20'), ('doc_4', 'contx_21'), ('doc_4', 'contx_22'), ('doc_4', 'contx_23'), ('doc_4', 'contx_24'), ('doc_5', 'contx_25'), ('doc_5', 'contx_26'), ('doc_5', 'contx_27'), ('doc_5', 'contx_28'), ('doc_5', 'contx_29'), ('doc_6', 'contx_30'), ('doc_6', 'contx_31'), ('doc_6', 'contx_32'), ('doc_6', 'contx_33'), ('doc_6', 'contx_34'), ('doc_7', 'contx_35'), ('doc_7', 'contx_36'), ('doc_7', 'contx_37'), ('doc_7', 'contx_38'), ('doc_7', 'contx_39'), ('doc_8', 'contx_40'), ('doc_8', 'contx_41'), ('doc_8', 'contx_42'), ('doc_8', 'contx_43'), ('doc_8', 'contx_44'), ('doc_9', 'contx_45'), ('doc_9', 'contx_46'), ('doc_9', 'contx_47'), ('doc_9', 'contx_48'), ('doc_9', 'contx_49'), ('doc_10', 'contx_50'), ('doc_10', 'contx_51'), ('doc_10', 'contx_52'), ('doc_10', 'contx_53'), ('doc_10', 'contx_54'), ('doc_11', 'contx_55'), ('doc_11', 'contx_56'), ('doc_11', 'contx_57'), ('doc_11', 'contx_58'), ('doc_11', 'contx_59'), ('doc_12', 'contx_60'), ('doc_12', 'contx_61'), ('doc_12', 'contx_62'), ('doc_12', 'contx_63'), ('doc_12', 'contx_64'), ('doc_13', 'contx_65'), ('doc_13', 'contx_66'), ('doc_13', 'contx_67'), ('doc_13', 'contx_68'), ('doc_13', 'contx_69'), ('doc_14', 'contx_70'), ('doc_14', 'contx_71'), ('doc_14', 'contx_72'), ('doc_14', 'contx_73'), ('doc_14', 'contx_74'), ('doc_15', 'contx_75'), ('doc_15', 'contx_76'), ('doc_15', 'contx_77'), ('doc_15', 'contx_78'), ('doc_15', 'contx_79'), ('doc_16', 'contx_80'), ('doc_16', 'contx_81'), ('doc_16', 'contx_82'), ('doc_16', 'contx_83'), ('doc_16', 'contx_84'), ('doc_17', 'contx_85'), ('doc_17', 'contx_86'), ('doc_17', 'contx_87'), ('doc_17', 'contx_88'), ('doc_17', 'contx_89'), ('doc_18', 'contx_90'), ('doc_18', 'contx_91'), ('doc_18', 'contx_92'), ('doc_18', 'contx_93'), ('doc_18', 'contx_94'), ('doc_19', 'contx_95'), ('doc_19', 'contx_96'), ('doc_19', 'contx_97'), ('doc_19', 'contx_98'), ('doc_19', 'contx_99')]\n",
            "time: 606 µs (started: 2024-10-16 20:53:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(edges_test[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZQnu2Drr55D"
      },
      "source": [
        "#### Test graph Conectivity with networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRZ8fI49WiNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693200fd-6338-4c35-d04e-bb7c1673cec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.94 s (started: 2024-10-16 20:53:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define Bipartide graph\n",
        "B = nx.Graph()\n",
        "B.add_nodes_from(document_nodes_test, bipartite=0)\n",
        "B.add_nodes_from(context_nodes_test, bipartite=1)\n",
        "B.add_edges_from(edges_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdCzkLKRjtWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c05c8bb6-2d73-4a30-ba2d-cc9b9a9f7b3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 229
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 482 ms (started: 2024-10-16 20:53:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nx.is_connected(B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW3DrKQTd0lR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1445d7-1ca5-454b-f7b1-7f4ac9dbd00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "time: 458 ms (started: 2024-10-16 20:53:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Number of conected elements\n",
        "# if == 1 -> all elements of the graph are conected\n",
        "print(nx.number_connected_components(B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yywkADdJ002y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f449e84b-77c6-4768-9024-0038c63286c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40140\n",
            "time: 444 ms (started: 2024-10-16 20:53:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# size of a cluster\n",
        "print(len(nx.node_connected_component(B,'doc_0')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pincp8wI-yV"
      },
      "source": [
        "### Creating Graph HeteroData Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKzQhBQYLPvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5bbc5f-93ef-4c4e-d60a-e57116bd8f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.13 ms (started: 2024-10-16 20:53:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Defining nodes, edges and class labels\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# nodes\n",
        "data['document'].x = document_nodes\n",
        "data['concept'].x = context_nodes\n",
        "\n",
        "# edges\n",
        "data['document', 'has', 'concept'].edge_index = edges\n",
        "\n",
        "#class labels\n",
        "data['document'].y = y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IplyJlwsUvDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0500670a-3035-4775-abdb-76a3b1ee6de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18.8 ms (started: 2024-10-16 20:53:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Setting graph to undirected\n",
        "data = T.ToUndirected()(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KZTlEWJAP-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926a21ed-6095-4b1b-d407-193aeb372d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 501 ms (started: 2024-10-16 20:53:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Remove duplicate edges\n",
        "data = T.RemoveDuplicatedEdges()(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn0v0rKXRMWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394e3326-ee7d-48c6-cb49-d269ae2b2751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 26.1 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Ensure date in using gpu\n",
        "data = data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAOpJJKEFYt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab3b8c9-f190-4d6c-b704-265479a4da8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  document={\n",
              "    x=[8817, 384],\n",
              "    y=[8817],\n",
              "  },\n",
              "  concept={ x=[31332, 384] },\n",
              "  (document, has, concept)={ edge_index=[2, 1098776] },\n",
              "  (concept, rev_has, document)={ edge_index=[2, 1098776] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 236
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.84 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsFgg19dw8eu"
      },
      "source": [
        "## Creating all masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CwD2x2qLSdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf4ef42-dff7-4499-f379-2f5c3931dfce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.15 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    os.makedirs('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks_big/')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIe8Dr1oxMFW"
      },
      "source": [
        "### Rotulated = 20%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ap8LV_yxPlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac38f25-e7f7-471f-8044-fda3771e6696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8\n",
            "time: 493 µs (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 20\n",
        "test_perc = (100 - rotulated_perc)/100\n",
        "print(test_perc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCJy0tZfUP_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b4ad79-1d93-4e6e-ced8-a481484a62aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 199 µs (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# rotulated_perc = 80\n",
        "# test_perc = (100 - rotulated_perc)/100\n",
        "# print(test_perc)\n",
        "\n",
        "# n_samples = len(y)\n",
        "# print('all:',np.bincount(y))\n",
        "# splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_perc, random_state=0)\n",
        "# x = np.zeros(n_samples)\n",
        "# for train_idx, test_idx in splitter.split(x, y):\n",
        "#     train_mask = np.full(n_samples, False, dtype=bool)\n",
        "#     val_mask = np.full(n_samples, False, dtype=bool)\n",
        "#     test_mask = np.full(n_samples, False, dtype=bool)\n",
        "\n",
        "#     train_mask[train_idx] = True\n",
        "#     test_mask[test_idx] = True\n",
        "\n",
        "#     print('train:',np.bincount(y[train_idx]))\n",
        "#     print('test:',np.bincount(y[test_mask]))\n",
        "\n",
        "#     print('train vs test:',np.bincount(train_mask))\n",
        "#     print(np.bincount(val_mask))\n",
        "#     print(np.bincount(test_mask))\n",
        "\n",
        "#     train_mask = torch.tensor(train_mask)\n",
        "#     val_mask = torch.tensor(val_mask)\n",
        "#     test_mask = torch.tensor(test_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn4axcr5xPl7"
      },
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89RxBBLwLKdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea4d892-3d31-4234-8d49-4b1ef5da87a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [189 127  20 198 111  71 192  79 519  99 101  57]\n",
            "test: [ 758  508   80  793  446  283  767  318 2076  396  404  225]\n",
            "train vs test: [7054 1763]\n",
            "time: 136 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "\n",
        "    train_mask, val_mask, test_mask = creat_equal_masks(test_perc,y)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks_big/mask_rot'+str(rotulated_perc)+'%_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([train_mask, val_mask, test_mask], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on0xv-jO79gT"
      },
      "source": [
        "### Rotulated = 40%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oDjKH_h79gY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60572c18-7bb4-4600-be8c-6de8b494b39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6\n",
            "time: 1.5 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 40\n",
        "test_perc = (100 - rotulated_perc)/100\n",
        "print(test_perc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5dxHzRB79gY"
      },
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jrtzAC6NsF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefa2e8b-4f06-4595-edbe-2819927c3fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 379  254   40  396  223  141  383  159 1038  198  202  113]\n",
            "test: [ 568  381   60  595  334  213  576  238 1557  297  303  169]\n",
            "train vs test: [5291 3526]\n",
            "time: 157 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "\n",
        "    train_mask, val_mask, test_mask = creat_equal_masks(test_perc,y)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks_big/mask_rot'+str(rotulated_perc)+'%_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([train_mask, val_mask, test_mask], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVahHlnK79q4"
      },
      "source": [
        "### Rotulated = 60%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8gARaZ079q4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fee737-db5d-4502-9e34-59be5eaac16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4\n",
            "time: 517 µs (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 60\n",
        "test_perc = (100 - rotulated_perc)/100\n",
        "print(test_perc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdnUTbAr79q4"
      },
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAqgkGiA79q4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a40aba6-5c79-48f1-9970-0ef77045cf20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 568  381   60  595  334  213  575  238 1557  297  303  169]\n",
            "test: [ 379  254   40  396  223  141  384  159 1038  198  202  113]\n",
            "train vs test: [3527 5290]\n",
            "time: 170 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "\n",
        "    train_mask, val_mask, test_mask = creat_equal_masks(test_perc,y)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks_big/mask_rot'+str(rotulated_perc)+'%_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([train_mask, val_mask, test_mask], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ5heZBaNunS"
      },
      "source": [
        "### Rotulated = 80%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjKsOLSQNunS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469fa77e-2be3-4086-91f9-19c926f6950b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2\n",
            "time: 1.67 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 80\n",
        "test_perc = (100 - rotulated_perc)/100\n",
        "print(test_perc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XZr1amWNunS"
      },
      "source": [
        "#### Creating mask Split using RandomNodeSplit (if split not done before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsXKdLakNunT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b49656b-4e2a-4265-bb12-bbcc3ae2ddb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "all: [ 947  635  100  991  557  354  959  397 2595  495  505  282]\n",
            "train: [ 757  508   80  793  445  283  767  318 2076  396  404  226]\n",
            "test: [190 127  20 198 112  71 192  79 519  99 101  56]\n",
            "train vs test: [1764 7053]\n",
            "time: 189 ms (started: 2024-10-16 20:53:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "\n",
        "    train_mask, val_mask, test_mask = creat_equal_masks(test_perc,y)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/'+dataset_name+'/masks_big/mask_rot'+str(rotulated_perc)+'%_'+str(i)+'.pkl', 'wb') as f:\n",
        "        pickle.dump([train_mask, val_mask, test_mask], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sApynSrs_hd"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnIjviAr5gWi"
      },
      "source": [
        "### Training rotulated base = 20% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbMLG6eEU6HM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f4ef80-9aa7-4071-8654-387e7371adbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 311 µs (started: 2024-10-16 20:53:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyOMuSd28UWx"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZLP8R5Mq1TN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e574432-3413-4138-8508-b06bbf6501ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 80.4366, Train: 0.0369, Test: 0.0380\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 266.7786, Train: 0.3279, Test: 0.3262\n",
            "Early stopping:  131.76372082229022\n",
            "Epoch: 003, Loss: 397.7393, Train: 0.1441, Test: 0.1452\n",
            "Early stopping:  159.45484454691706\n",
            "Epoch: 004, Loss: 346.3765, Train: 0.2762, Test: 0.2771\n",
            "Early stopping:  139.12017091653956\n",
            "Epoch: 005, Loss: 311.2749, Train: 0.0777, Test: 0.0764\n",
            "Early stopping:  121.70199988461948\n",
            "Epoch: 006, Loss: 485.4179, Train: 0.1214, Test: 0.1202\n",
            "Early stopping:  84.25016260643493\n",
            "Epoch: 007, Loss: 504.2057, Train: 0.0516, Test: 0.0466\n",
            "Early stopping:  84.41346014455154\n",
            "Epoch: 008, Loss: 585.7357, Train: 0.2036, Test: 0.2023\n",
            "Early stopping:  114.6097868179186\n",
            "Epoch: 009, Loss: 440.5719, Train: 0.3029, Test: 0.2981\n",
            "Early stopping:  100.94334158627193\n",
            "Epoch: 010, Loss: 459.1966, Train: 0.3006, Test: 0.2927\n",
            "Early stopping:  56.24458594790368\n",
            "Epoch: 011, Loss: 385.2913, Train: 0.1242, Test: 0.1052\n",
            "Early stopping:  75.16324974200724\n",
            "Epoch: 012, Loss: 302.5596, Train: 0.2189, Test: 0.2102\n",
            "Early stopping:  104.10167901554932\n",
            "Epoch: 013, Loss: 202.8849, Train: 0.1685, Test: 0.1601\n",
            "Early stopping:  105.99290439947127\n",
            "Epoch: 014, Loss: 177.3524, Train: 0.1889, Test: 0.1832\n",
            "Early stopping:  119.3205824913304\n",
            "Epoch: 015, Loss: 124.3680, Train: 0.2717, Test: 0.2699\n",
            "Early stopping:  104.50463827410735\n",
            "Epoch: 016, Loss: 79.6891, Train: 0.2864, Test: 0.2868\n",
            "Early stopping:  84.66730896597113\n",
            "Epoch: 017, Loss: 66.0541, Train: 0.2297, Test: 0.2216\n",
            "Early stopping:  59.595326422561556\n",
            "Epoch: 018, Loss: 51.5745, Train: 0.2592, Test: 0.2540\n",
            "Early stopping:  51.202760775297385\n",
            "Epoch: 019, Loss: 27.0426, Train: 0.3556, Test: 0.3323\n",
            "Early stopping:  36.22790370910692\n",
            "Epoch: 020, Loss: 6.4431, Train: 0.2513, Test: 0.2426\n",
            "Early stopping:  29.547425133146497\n",
            "Epoch: 021, Loss: 3.1355, Train: 0.1174, Test: 0.1100\n",
            "Early stopping:  27.60038269413146\n",
            "Epoch: 022, Loss: 2.9403, Train: 0.0874, Test: 0.0824\n",
            "Early stopping:  21.15461961571561\n",
            "Epoch: 023, Loss: 2.9098, Train: 0.0908, Test: 0.0855\n",
            "Early stopping:  10.476138194184077\n",
            "Epoch: 024, Loss: 2.7544, Train: 0.0947, Test: 0.0896\n",
            "Early stopping:  1.5747106270591298\n",
            "Epoch: 025, Loss: 2.6304, Train: 0.0953, Test: 0.0906\n",
            "Early stopping:  0.19215303360940234\n",
            "Epoch: 026, Loss: 2.5558, Train: 0.0976, Test: 0.0897\n",
            "Early stopping:  0.16841014537699786\n",
            "Epoch: 027, Loss: 2.4978, Train: 0.0987, Test: 0.0903\n",
            "Early stopping:  0.16499969992758642\n",
            "Epoch: 028, Loss: 2.4555, Train: 0.1214, Test: 0.1025\n",
            "Early stopping:  0.11812788790370894\n",
            "Epoch: 029, Loss: 2.4254, Train: 0.1191, Test: 0.1045\n",
            "Early stopping:  0.08189531465490357\n",
            "Epoch: 030, Loss: 2.4056, Train: 0.1185, Test: 0.1035\n",
            "Early stopping:  0.06013167366541094\n",
            "Epoch: 031, Loss: 2.3969, Train: 0.1163, Test: 0.1042\n",
            "Early stopping:  0.041115945495707004\n",
            "Epoch: 032, Loss: 2.3955, Train: 0.1151, Test: 0.1029\n",
            "Early stopping:  0.02520067117046799\n",
            "Epoch: 033, Loss: 2.3959, Train: 0.1134, Test: 0.1042\n",
            "Early stopping:  0.01273704674154062\n",
            "Epoch: 034, Loss: 2.3949, Train: 0.1089, Test: 0.1016\n",
            "Early stopping:  0.004443391184157507\n",
            "PREDICTIONS -> tensor([9, 9, 9,  ..., 1, 1, 1], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       758\n",
            "         capital_goods       0.07      0.56      0.12       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.16      0.11      0.13       793\n",
            " consumer_non-cyclical       0.38      0.23      0.29       446\n",
            "                energy       0.00      0.00      0.00       283\n",
            "             financial       0.00      0.00      0.00       767\n",
            "            healthcare       0.37      0.19      0.25       318\n",
            "              services       0.82      0.03      0.05      2076\n",
            "            technology       0.08      0.26      0.13       396\n",
            "        transportation       0.02      0.04      0.03       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.10      7054\n",
            "             macro avg       0.16      0.12      0.08      7054\n",
            "          weighted avg       0.31      0.10      0.08      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 82.1807, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 508.6463, Train: 0.0556, Test: 0.0564\n",
            "Early stopping:  301.55670624855753\n",
            "Epoch: 003, Loss: 597.6083, Train: 0.1072, Test: 0.1075\n",
            "Early stopping:  275.51548524318895\n",
            "Epoch: 004, Loss: 427.7386, Train: 0.1151, Test: 0.1128\n",
            "Early stopping:  225.5114025421205\n",
            "Epoch: 005, Loss: 345.9026, Train: 0.0482, Test: 0.0491\n",
            "Early stopping:  197.02187147100375\n",
            "Epoch: 006, Loss: 301.2816, Train: 0.1282, Test: 0.1147\n",
            "Early stopping:  120.134596794617\n",
            "Epoch: 007, Loss: 235.1357, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  139.61149358935188\n",
            "Epoch: 008, Loss: 258.5432, Train: 0.2989, Test: 0.2976\n",
            "Early stopping:  76.50270284422035\n",
            "Epoch: 009, Loss: 255.6884, Train: 0.2745, Test: 0.2786\n",
            "Early stopping:  44.322495811203275\n",
            "Epoch: 010, Loss: 226.2138, Train: 0.3052, Test: 0.3027\n",
            "Early stopping:  29.058392130917976\n",
            "Epoch: 011, Loss: 180.3377, Train: 0.3012, Test: 0.2875\n",
            "Early stopping:  31.522413239893464\n",
            "Epoch: 012, Loss: 139.6268, Train: 0.1429, Test: 0.1489\n",
            "Early stopping:  51.276999112754275\n",
            "Epoch: 013, Loss: 119.4356, Train: 0.1044, Test: 0.1059\n",
            "Early stopping:  57.166077074467644\n",
            "Epoch: 014, Loss: 91.7636, Train: 0.1997, Test: 0.1891\n",
            "Early stopping:  52.79985027700177\n",
            "Epoch: 015, Loss: 55.6271, Train: 0.1571, Test: 0.1473\n",
            "Early stopping:  47.255776562160456\n",
            "Epoch: 016, Loss: 37.6790, Train: 0.1350, Test: 0.1347\n",
            "Early stopping:  42.55724142857531\n",
            "Epoch: 017, Loss: 16.8914, Train: 0.1185, Test: 0.1171\n",
            "Early stopping:  41.30874952947149\n",
            "Epoch: 018, Loss: 6.4553, Train: 0.1185, Test: 0.1102\n",
            "Early stopping:  33.81624210384306\n",
            "Epoch: 019, Loss: 4.2214, Train: 0.1259, Test: 0.1246\n",
            "Early stopping:  22.00443117083745\n",
            "Epoch: 020, Loss: 3.6335, Train: 0.1526, Test: 0.1490\n",
            "Early stopping:  14.394547264499977\n",
            "Epoch: 021, Loss: 3.2730, Train: 0.1634, Test: 0.1630\n",
            "Early stopping:  5.723323490265777\n",
            "Epoch: 022, Loss: 2.9377, Train: 0.1634, Test: 0.1676\n",
            "Early stopping:  1.39769475607849\n",
            "Epoch: 023, Loss: 2.6517, Train: 0.1707, Test: 0.1730\n",
            "Early stopping:  0.613012951733466\n",
            "Epoch: 024, Loss: 2.4553, Train: 0.1724, Test: 0.1724\n",
            "Early stopping:  0.47363878311137325\n",
            "Epoch: 025, Loss: 2.3643, Train: 0.1696, Test: 0.1646\n",
            "Early stopping:  0.3718806746407467\n",
            "Epoch: 026, Loss: 2.3488, Train: 0.1571, Test: 0.1561\n",
            "Early stopping:  0.24729261227918456\n",
            "Epoch: 027, Loss: 2.3528, Train: 0.1617, Test: 0.1578\n",
            "Early stopping:  0.1289792005663176\n",
            "Epoch: 028, Loss: 2.3499, Train: 0.1679, Test: 0.1606\n",
            "Early stopping:  0.04575010871195241\n",
            "Epoch: 029, Loss: 2.3464, Train: 0.1826, Test: 0.1751\n",
            "Early stopping:  0.007015220157274778\n",
            "PREDICTIONS -> tensor([4, 4, 1,  ..., 7, 6, 6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.49      0.07      0.12       758\n",
            "         capital_goods       0.22      0.16      0.18       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.06      0.01      0.01       793\n",
            " consumer_non-cyclical       0.11      0.22      0.15       446\n",
            "                energy       0.00      0.00      0.00       283\n",
            "             financial       0.18      0.53      0.27       767\n",
            "            healthcare       0.04      0.23      0.06       318\n",
            "              services       0.41      0.25      0.31      2076\n",
            "            technology       0.11      0.02      0.04       396\n",
            "        transportation       0.00      0.00      0.00       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.18      7054\n",
            "             macro avg       0.14      0.12      0.09      7054\n",
            "          weighted avg       0.23      0.18      0.16      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 74.1266, Train: 0.1299, Test: 0.1277\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 411.8520, Train: 0.1463, Test: 0.1453\n",
            "Early stopping:  238.807960285064\n",
            "Epoch: 003, Loss: 421.8684, Train: 0.2938, Test: 0.2932\n",
            "Early stopping:  197.94073271654287\n",
            "Epoch: 004, Loss: 486.2402, Train: 0.0805, Test: 0.0751\n",
            "Early stopping:  185.87588835404284\n",
            "Epoch: 005, Loss: 382.6359, Train: 0.0925, Test: 0.0910\n",
            "Early stopping:  161.69458498224856\n",
            "Epoch: 006, Loss: 419.5711, Train: 0.1214, Test: 0.1219\n",
            "Early stopping:  37.932905150866205\n",
            "Epoch: 007, Loss: 460.1505, Train: 0.0635, Test: 0.0634\n",
            "Early stopping:  40.02025196889204\n",
            "Epoch: 008, Loss: 462.5825, Train: 0.2235, Test: 0.2219\n",
            "Early stopping:  41.040065788606405\n",
            "Epoch: 009, Loss: 296.1383, Train: 0.2961, Test: 0.2959\n",
            "Early stopping:  68.77061625868883\n",
            "Epoch: 010, Loss: 289.9023, Train: 0.0998, Test: 0.1019\n",
            "Early stopping:  86.31273359449601\n",
            "Epoch: 011, Loss: 250.9393, Train: 0.1083, Test: 0.1107\n",
            "Early stopping:  101.38409331079794\n",
            "Epoch: 012, Loss: 204.6193, Train: 0.1758, Test: 0.1803\n",
            "Early stopping:  97.5331782415285\n",
            "Epoch: 013, Loss: 151.8330, Train: 0.1770, Test: 0.1792\n",
            "Early stopping:  60.782399051722784\n",
            "Epoch: 014, Loss: 126.3532, Train: 0.2286, Test: 0.2261\n",
            "Early stopping:  67.69534029952288\n",
            "Epoch: 015, Loss: 94.4184, Train: 0.2138, Test: 0.2134\n",
            "Early stopping:  62.45389209345169\n",
            "Epoch: 016, Loss: 66.6163, Train: 0.2830, Test: 0.2725\n",
            "Early stopping:  53.23055793165293\n",
            "Epoch: 017, Loss: 38.1204, Train: 0.2870, Test: 0.2786\n",
            "Early stopping:  45.42349232387756\n",
            "Epoch: 018, Loss: 17.9224, Train: 0.2955, Test: 0.2950\n",
            "Early stopping:  43.31033639345077\n",
            "Epoch: 019, Loss: 6.0329, Train: 0.3409, Test: 0.3228\n",
            "Early stopping:  36.0821355116864\n",
            "Epoch: 020, Loss: 3.6610, Train: 0.3352, Test: 0.3193\n",
            "Early stopping:  26.260437524685162\n",
            "Epoch: 021, Loss: 3.2503, Train: 0.3409, Test: 0.3204\n",
            "Early stopping:  14.856802461670052\n",
            "Epoch: 022, Loss: 2.9265, Train: 0.3409, Test: 0.3207\n",
            "Early stopping:  6.358958281312255\n",
            "Epoch: 023, Loss: 2.6731, Train: 0.3188, Test: 0.3113\n",
            "Early stopping:  1.3508002124457181\n",
            "Epoch: 024, Loss: 2.4653, Train: 0.3040, Test: 0.2954\n",
            "Early stopping:  0.47373633403177345\n",
            "Epoch: 025, Loss: 2.3110, Train: 0.2830, Test: 0.2798\n",
            "Early stopping:  0.3735567932620329\n",
            "Epoch: 026, Loss: 2.2167, Train: 0.2587, Test: 0.2644\n",
            "Early stopping:  0.2861041749978182\n",
            "Epoch: 027, Loss: 2.1683, Train: 0.2569, Test: 0.2557\n",
            "Early stopping:  0.20529046338066323\n",
            "Epoch: 028, Loss: 2.1314, Train: 0.2751, Test: 0.2681\n",
            "Early stopping:  0.13373745701054035\n",
            "Epoch: 029, Loss: 2.1068, Train: 0.2796, Test: 0.2702\n",
            "Early stopping:  0.08080613452549402\n",
            "Epoch: 030, Loss: 2.0876, Train: 0.2927, Test: 0.2752\n",
            "Early stopping:  0.05142637191352838\n",
            "Epoch: 031, Loss: 2.0705, Train: 0.3188, Test: 0.3014\n",
            "Early stopping:  0.03836501974614177\n",
            "Epoch: 032, Loss: 2.0541, Train: 0.3284, Test: 0.3164\n",
            "Early stopping:  0.030284336359331945\n",
            "Epoch: 033, Loss: 2.0392, Train: 0.3398, Test: 0.3316\n",
            "Early stopping:  0.0266822424908244\n",
            "Epoch: 034, Loss: 2.0290, Train: 0.3437, Test: 0.3322\n",
            "Early stopping:  0.02357487858795343\n",
            "Epoch: 035, Loss: 2.0255, Train: 0.3511, Test: 0.3384\n",
            "Early stopping:  0.018679133407021948\n",
            "Epoch: 036, Loss: 2.0191, Train: 0.3511, Test: 0.3419\n",
            "Early stopping:  0.013695713398922815\n",
            "Epoch: 037, Loss: 2.0082, Train: 0.3551, Test: 0.3431\n",
            "Early stopping:  0.011528544976699188\n",
            "Epoch: 038, Loss: 1.9955, Train: 0.3528, Test: 0.3438\n",
            "Early stopping:  0.01366673428085345\n",
            "Epoch: 039, Loss: 1.9820, Train: 0.3568, Test: 0.3466\n",
            "Early stopping:  0.01763701282014063\n",
            "Epoch: 040, Loss: 1.9667, Train: 0.3625, Test: 0.3459\n",
            "Early stopping:  0.020759722325502\n",
            "Epoch: 041, Loss: 1.9422, Train: 0.3732, Test: 0.3504\n",
            "Early stopping:  0.025702849548396488\n",
            "Epoch: 042, Loss: 1.9155, Train: 0.3778, Test: 0.3565\n",
            "Early stopping:  0.031949168694833184\n",
            "Epoch: 043, Loss: 1.8848, Train: 0.3761, Test: 0.3451\n",
            "Early stopping:  0.03908211018167099\n",
            "Epoch: 044, Loss: 1.8584, Train: 0.3914, Test: 0.3611\n",
            "Early stopping:  0.04335588397584878\n",
            "Epoch: 045, Loss: 1.8285, Train: 0.4039, Test: 0.3693\n",
            "Early stopping:  0.04499429946765916\n",
            "Epoch: 046, Loss: 1.7968, Train: 0.4101, Test: 0.3745\n",
            "Early stopping:  0.04647086304268653\n",
            "Epoch: 047, Loss: 1.7659, Train: 0.4265, Test: 0.3859\n",
            "Early stopping:  0.04738148337174312\n",
            "Epoch: 048, Loss: 1.7319, Train: 0.4390, Test: 0.3952\n",
            "Early stopping:  0.04991771818603308\n",
            "Epoch: 049, Loss: 1.7024, Train: 0.4453, Test: 0.4079\n",
            "Early stopping:  0.0501379262287403\n",
            "Epoch: 050, Loss: 1.6696, Train: 0.4487, Test: 0.4137\n",
            "Early stopping:  0.05023976612838897\n",
            "Epoch: 051, Loss: 1.6413, Train: 0.4668, Test: 0.4236\n",
            "Early stopping:  0.049249008690120215\n",
            "Epoch: 052, Loss: 1.6122, Train: 0.4697, Test: 0.4287\n",
            "Early stopping:  0.04753558907683768\n",
            "Epoch: 053, Loss: 1.5898, Train: 0.4799, Test: 0.4348\n",
            "Early stopping:  0.04479800714219948\n",
            "Epoch: 054, Loss: 1.5667, Train: 0.4804, Test: 0.4375\n",
            "Early stopping:  0.04077314194718082\n",
            "Epoch: 055, Loss: 1.5508, Train: 0.4918, Test: 0.4413\n",
            "Early stopping:  0.035993964701334806\n",
            "Epoch: 056, Loss: 1.5305, Train: 0.5020, Test: 0.4434\n",
            "Early stopping:  0.03205430437517372\n",
            "Epoch: 057, Loss: 1.5146, Train: 0.5065, Test: 0.4440\n",
            "Early stopping:  0.029561297208885336\n",
            "Epoch: 058, Loss: 1.4944, Train: 0.5122, Test: 0.4466\n",
            "Early stopping:  0.028606543884136183\n",
            "Epoch: 059, Loss: 1.4713, Train: 0.5207, Test: 0.4498\n",
            "Early stopping:  0.030894446166118635\n",
            "Epoch: 060, Loss: 1.4503, Train: 0.5252, Test: 0.4519\n",
            "Early stopping:  0.03227821242092298\n",
            "Epoch: 061, Loss: 1.4319, Train: 0.5275, Test: 0.4531\n",
            "Early stopping:  0.033146854001253195\n",
            "Epoch: 062, Loss: 1.4141, Train: 0.5298, Test: 0.4562\n",
            "Early stopping:  0.031674865454810665\n",
            "Epoch: 063, Loss: 1.3971, Train: 0.5406, Test: 0.4609\n",
            "Early stopping:  0.0292170502161353\n",
            "Epoch: 064, Loss: 1.3808, Train: 0.5491, Test: 0.4616\n",
            "Early stopping:  0.02748975690769416\n",
            "Epoch: 065, Loss: 1.3621, Train: 0.5508, Test: 0.4629\n",
            "Early stopping:  0.027351292120322404\n",
            "Epoch: 066, Loss: 1.3499, Train: 0.5519, Test: 0.4688\n",
            "Early stopping:  0.0258734296029828\n",
            "Epoch: 067, Loss: 1.3392, Train: 0.5683, Test: 0.4701\n",
            "Early stopping:  0.023305753102085637\n",
            "Epoch: 068, Loss: 1.3153, Train: 0.5752, Test: 0.4677\n",
            "Early stopping:  0.024544850399812034\n",
            "Epoch: 069, Loss: 1.3138, Train: 0.5729, Test: 0.4758\n",
            "Early stopping:  0.021246508170296104\n",
            "Epoch: 070, Loss: 1.2944, Train: 0.5848, Test: 0.4803\n",
            "Early stopping:  0.022065875446610414\n",
            "Epoch: 071, Loss: 1.2830, Train: 0.5944, Test: 0.4827\n",
            "Early stopping:  0.021594781826575794\n",
            "Epoch: 072, Loss: 1.2665, Train: 0.5916, Test: 0.4810\n",
            "Early stopping:  0.02074594756986622\n",
            "Epoch: 073, Loss: 1.2612, Train: 0.5893, Test: 0.4872\n",
            "Early stopping:  0.021353010397696756\n",
            "Epoch: 074, Loss: 1.2440, Train: 0.5933, Test: 0.4895\n",
            "Early stopping:  0.019575975384279073\n",
            "Epoch: 075, Loss: 1.2343, Train: 0.5961, Test: 0.4894\n",
            "Early stopping:  0.019132758810786108\n",
            "Epoch: 076, Loss: 1.2139, Train: 0.6035, Test: 0.4891\n",
            "Early stopping:  0.021230922669938153\n",
            "Epoch: 077, Loss: 1.2121, Train: 0.6064, Test: 0.4928\n",
            "Early stopping:  0.020738449974523086\n",
            "Epoch: 078, Loss: 1.1956, Train: 0.6075, Test: 0.4953\n",
            "Early stopping:  0.019209921577673643\n",
            "Epoch: 079, Loss: 1.1871, Train: 0.6183, Test: 0.4955\n",
            "Early stopping:  0.018250197079362224\n",
            "Epoch: 080, Loss: 1.1694, Train: 0.6188, Test: 0.4926\n",
            "Early stopping:  0.018449446974283516\n",
            "Epoch: 081, Loss: 1.1625, Train: 0.6262, Test: 0.4979\n",
            "Early stopping:  0.019988114845046694\n",
            "Epoch: 082, Loss: 1.1490, Train: 0.6268, Test: 0.4973\n",
            "Early stopping:  0.018758274024758055\n",
            "Epoch: 083, Loss: 1.1412, Train: 0.6313, Test: 0.5000\n",
            "Early stopping:  0.01792799564581497\n",
            "Epoch: 084, Loss: 1.1311, Train: 0.6324, Test: 0.4997\n",
            "Early stopping:  0.015542323665684716\n",
            "Epoch: 085, Loss: 1.1192, Train: 0.6387, Test: 0.5013\n",
            "Early stopping:  0.016587239548572358\n",
            "Epoch: 086, Loss: 1.1075, Train: 0.6438, Test: 0.5033\n",
            "Early stopping:  0.01667799914142716\n",
            "Epoch: 087, Loss: 1.0997, Train: 0.6432, Test: 0.5021\n",
            "Early stopping:  0.016893315379700225\n",
            "Epoch: 088, Loss: 1.0976, Train: 0.6410, Test: 0.5027\n",
            "Early stopping:  0.014058051268854238\n",
            "Epoch: 089, Loss: 1.0906, Train: 0.6506, Test: 0.5035\n",
            "Early stopping:  0.010897817840775894\n",
            "Epoch: 090, Loss: 1.0724, Train: 0.6461, Test: 0.5021\n",
            "Early stopping:  0.013266879433085663\n",
            "Epoch: 091, Loss: 1.0755, Train: 0.6546, Test: 0.5037\n",
            "Early stopping:  0.012572429982822916\n",
            "Epoch: 092, Loss: 1.0604, Train: 0.6568, Test: 0.5079\n",
            "Early stopping:  0.014857249441304773\n",
            "Epoch: 093, Loss: 1.0507, Train: 0.6551, Test: 0.5038\n",
            "Early stopping:  0.015211746774286047\n",
            "Epoch: 094, Loss: 1.0447, Train: 0.6631, Test: 0.5052\n",
            "Early stopping:  0.013337973787473995\n",
            "Epoch: 095, Loss: 1.0278, Train: 0.6602, Test: 0.5086\n",
            "Early stopping:  0.017750368316123387\n",
            "Epoch: 096, Loss: 1.0248, Train: 0.6710, Test: 0.5061\n",
            "Early stopping:  0.015131289854418781\n",
            "Epoch: 097, Loss: 1.0114, Train: 0.6699, Test: 0.5055\n",
            "Early stopping:  0.015831860308073816\n",
            "Epoch: 098, Loss: 1.0041, Train: 0.6608, Test: 0.5060\n",
            "Early stopping:  0.015722460856496256\n",
            "Epoch: 099, Loss: 1.0009, Train: 0.6721, Test: 0.5068\n",
            "Early stopping:  0.012077444874039778\n",
            "Epoch: 100, Loss: 0.9853, Train: 0.6739, Test: 0.5055\n",
            "Early stopping:  0.014476789064032079\n",
            "Epoch: 101, Loss: 0.9851, Train: 0.6744, Test: 0.5058\n",
            "Early stopping:  0.01172789626123414\n",
            "Epoch: 102, Loss: 0.9801, Train: 0.6682, Test: 0.5099\n",
            "Early stopping:  0.010657553109785086\n",
            "Epoch: 103, Loss: 0.9779, Train: 0.6773, Test: 0.5061\n",
            "Early stopping:  0.009012409631561255\n",
            "PREDICTIONS -> tensor([ 0,  6,  1,  ..., 11,  8,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.53      0.56      0.54       758\n",
            "         capital_goods       0.31      0.28      0.29       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.42      0.50      0.46       793\n",
            " consumer_non-cyclical       0.58      0.37      0.45       446\n",
            "                energy       0.59      0.40      0.47       283\n",
            "             financial       0.53      0.56      0.54       767\n",
            "            healthcare       0.63      0.44      0.51       318\n",
            "              services       0.53      0.67      0.60      2076\n",
            "            technology       0.27      0.11      0.15       396\n",
            "        transportation       0.56      0.57      0.56       404\n",
            "             utilities       0.59      0.44      0.50       225\n",
            "\n",
            "              accuracy                           0.51      7054\n",
            "             macro avg       0.46      0.41      0.42      7054\n",
            "          weighted avg       0.50      0.51      0.49      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 74.3203, Train: 0.0726, Test: 0.0747\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 263.3960, Train: 0.2745, Test: 0.2762\n",
            "Early stopping:  133.69668184223283\n",
            "Epoch: 003, Loss: 339.9444, Train: 0.1594, Test: 0.1596\n",
            "Early stopping:  136.7268853164968\n",
            "Epoch: 004, Loss: 301.1281, Train: 0.0885, Test: 0.0797\n",
            "Early stopping:  117.80551194730452\n",
            "Epoch: 005, Loss: 386.1308, Train: 0.2802, Test: 0.2793\n",
            "Early stopping:  120.03871967779\n",
            "Epoch: 006, Loss: 425.3679, Train: 0.2950, Test: 0.2946\n",
            "Early stopping:  64.69918169439602\n",
            "Epoch: 007, Loss: 527.7114, Train: 0.0261, Test: 0.0220\n",
            "Early stopping:  87.25055688144086\n",
            "Epoch: 008, Loss: 593.3896, Train: 0.0562, Test: 0.0537\n",
            "Early stopping:  115.53735563132776\n",
            "Epoch: 009, Loss: 580.6088, Train: 0.1849, Test: 0.1887\n",
            "Early stopping:  92.85549182636652\n",
            "Epoch: 010, Loss: 393.4874, Train: 0.3001, Test: 0.2944\n",
            "Early stopping:  90.57816691681434\n",
            "Epoch: 011, Loss: 355.3848, Train: 0.1293, Test: 0.1263\n",
            "Early stopping:  109.2672958736574\n",
            "Epoch: 012, Loss: 332.5047, Train: 0.1639, Test: 0.1601\n",
            "Early stopping:  126.05981339772795\n",
            "Epoch: 013, Loss: 266.5472, Train: 0.3432, Test: 0.3312\n",
            "Early stopping:  118.31758350657098\n",
            "Epoch: 014, Loss: 204.5673, Train: 0.3375, Test: 0.3303\n",
            "Early stopping:  75.06552150149544\n",
            "Epoch: 015, Loss: 177.6811, Train: 0.3324, Test: 0.3287\n",
            "Early stopping:  77.42428304785841\n",
            "Epoch: 016, Loss: 143.0655, Train: 0.3029, Test: 0.2947\n",
            "Early stopping:  75.25487014338825\n",
            "Epoch: 017, Loss: 110.6812, Train: 0.2002, Test: 0.1876\n",
            "Early stopping:  59.722936859303445\n",
            "Epoch: 018, Loss: 102.8999, Train: 0.2802, Test: 0.2747\n",
            "Early stopping:  43.39252636022721\n",
            "Epoch: 019, Loss: 66.2383, Train: 0.3313, Test: 0.3241\n",
            "Early stopping:  42.215636716785575\n",
            "Epoch: 020, Loss: 49.8613, Train: 0.3114, Test: 0.3100\n",
            "Early stopping:  37.021347759400356\n",
            "Epoch: 021, Loss: 34.6900, Train: 0.2796, Test: 0.2834\n",
            "Early stopping:  33.02488649713562\n",
            "Epoch: 022, Loss: 23.3952, Train: 0.3035, Test: 0.3062\n",
            "Early stopping:  31.05478337267352\n",
            "Epoch: 023, Loss: 12.8900, Train: 0.2819, Test: 0.2763\n",
            "Early stopping:  21.16098423927362\n",
            "Epoch: 024, Loss: 8.1143, Train: 0.1645, Test: 0.1612\n",
            "Early stopping:  16.905738738496055\n",
            "Epoch: 025, Loss: 5.7386, Train: 0.1208, Test: 0.1204\n",
            "Early stopping:  12.005165591089062\n",
            "Epoch: 026, Loss: 4.3783, Train: 0.1327, Test: 0.1297\n",
            "Early stopping:  7.697760614397828\n",
            "Epoch: 027, Loss: 3.2901, Train: 0.1702, Test: 0.1773\n",
            "Early stopping:  3.810912352019144\n",
            "Epoch: 028, Loss: 2.7401, Train: 0.2099, Test: 0.2063\n",
            "Early stopping:  2.1536600516072104\n",
            "Epoch: 029, Loss: 2.4955, Train: 0.2235, Test: 0.2197\n",
            "Early stopping:  1.3372023796162944\n",
            "Epoch: 030, Loss: 2.3893, Train: 0.2416, Test: 0.2278\n",
            "Early stopping:  0.8157289889193119\n",
            "Epoch: 031, Loss: 2.3571, Train: 0.2360, Test: 0.2156\n",
            "Early stopping:  0.3857898370585323\n",
            "Epoch: 032, Loss: 2.3505, Train: 0.2195, Test: 0.2101\n",
            "Early stopping:  0.16358738165080203\n",
            "Epoch: 033, Loss: 2.3501, Train: 0.2189, Test: 0.2006\n",
            "Early stopping:  0.06193415421674036\n",
            "Epoch: 034, Loss: 2.3517, Train: 0.2104, Test: 0.1944\n",
            "Early stopping:  0.016781026548161763\n",
            "Epoch: 035, Loss: 2.3548, Train: 0.1991, Test: 0.1890\n",
            "Early stopping:  0.003004304131525768\n",
            "PREDICTIONS -> tensor([ 3, 10,  9,  ..., 10, 10,  3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       758\n",
            "         capital_goods       0.00      0.00      0.00       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.22      0.61      0.32       793\n",
            " consumer_non-cyclical       0.45      0.20      0.28       446\n",
            "                energy       0.06      0.02      0.03       283\n",
            "             financial       0.66      0.36      0.46       767\n",
            "            healthcare       0.89      0.10      0.18       318\n",
            "              services       0.49      0.02      0.04      2076\n",
            "            technology       0.16      0.21      0.18       396\n",
            "        transportation       0.10      0.80      0.17       404\n",
            "             utilities       0.04      0.02      0.02       225\n",
            "\n",
            "              accuracy                           0.19      7054\n",
            "             macro avg       0.26      0.19      0.14      7054\n",
            "          weighted avg       0.33      0.19      0.15      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 55.9984, Train: 0.1872, Test: 0.1955\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 264.4469, Train: 0.1356, Test: 0.1331\n",
            "Early stopping:  147.39529774971396\n",
            "Epoch: 003, Loss: 461.1643, Train: 0.2915, Test: 0.2903\n",
            "Early stopping:  202.61123593878392\n",
            "Epoch: 004, Loss: 511.9359, Train: 0.2830, Test: 0.2817\n",
            "Early stopping:  207.76899956760082\n",
            "Epoch: 005, Loss: 390.6187, Train: 0.0664, Test: 0.0654\n",
            "Early stopping:  182.4280793459909\n",
            "Epoch: 006, Loss: 305.3127, Train: 0.0607, Test: 0.0590\n",
            "Early stopping:  103.45778599557617\n",
            "Epoch: 007, Loss: 342.4734, Train: 0.0715, Test: 0.0682\n",
            "Early stopping:  84.56839783487928\n",
            "Epoch: 008, Loss: 297.6904, Train: 0.1191, Test: 0.1204\n",
            "Early stopping:  87.66556509313466\n",
            "Epoch: 009, Loss: 249.8901, Train: 0.1015, Test: 0.1048\n",
            "Early stopping:  52.63459112767793\n",
            "Epoch: 010, Loss: 213.2302, Train: 0.0528, Test: 0.0515\n",
            "Early stopping:  50.51395729077636\n",
            "Epoch: 011, Loss: 142.8514, Train: 0.0289, Test: 0.0312\n",
            "Early stopping:  76.90184522716275\n",
            "Epoch: 012, Loss: 108.5802, Train: 0.0947, Test: 0.0893\n",
            "Early stopping:  77.13792905538617\n",
            "Epoch: 013, Loss: 59.9543, Train: 0.3012, Test: 0.2922\n",
            "Early stopping:  77.02421155652034\n",
            "Epoch: 014, Loss: 34.7799, Train: 0.3222, Test: 0.3064\n",
            "Early stopping:  70.50298068223846\n",
            "Epoch: 015, Loss: 22.3847, Train: 0.3052, Test: 0.2964\n",
            "Early stopping:  50.83188200478405\n",
            "Epoch: 016, Loss: 14.7166, Train: 0.2309, Test: 0.2231\n",
            "Early stopping:  37.91941889812903\n",
            "Epoch: 017, Loss: 12.7943, Train: 0.2405, Test: 0.2338\n",
            "Early stopping:  19.377493560381577\n",
            "Epoch: 018, Loss: 8.4247, Train: 0.2944, Test: 0.2875\n",
            "Early stopping:  10.349345177246784\n",
            "Epoch: 019, Loss: 4.3353, Train: 0.3375, Test: 0.3273\n",
            "Early stopping:  6.81858744063983\n",
            "Epoch: 020, Loss: 2.4636, Train: 0.3301, Test: 0.3256\n",
            "Early stopping:  5.264613374713244\n",
            "Epoch: 021, Loss: 2.2976, Train: 0.3284, Test: 0.3151\n",
            "Early stopping:  4.500185561191066\n",
            "Epoch: 022, Loss: 2.2977, Train: 0.3097, Test: 0.3012\n",
            "Early stopping:  2.638182995549632\n",
            "Epoch: 023, Loss: 2.3001, Train: 0.3057, Test: 0.2940\n",
            "Early stopping:  0.8953138656775796\n",
            "Epoch: 024, Loss: 2.2921, Train: 0.3080, Test: 0.2926\n",
            "Early stopping:  0.07463311452118615\n",
            "Epoch: 025, Loss: 2.2716, Train: 0.3137, Test: 0.2947\n",
            "Early stopping:  0.011692958702128005\n",
            "Epoch: 026, Loss: 2.2413, Train: 0.3210, Test: 0.3012\n",
            "Early stopping:  0.02463325072337144\n",
            "Epoch: 027, Loss: 2.2075, Train: 0.3233, Test: 0.3073\n",
            "Early stopping:  0.038224692015900175\n",
            "Epoch: 028, Loss: 2.1751, Train: 0.3227, Test: 0.3058\n",
            "Early stopping:  0.047326010604766364\n",
            "Epoch: 029, Loss: 2.1463, Train: 0.3279, Test: 0.3076\n",
            "Early stopping:  0.05012176884249865\n",
            "Epoch: 030, Loss: 2.1213, Train: 0.3284, Test: 0.3134\n",
            "Early stopping:  0.04774375295633645\n",
            "Epoch: 031, Loss: 2.1033, Train: 0.3279, Test: 0.3136\n",
            "Early stopping:  0.041696255819353836\n",
            "Epoch: 032, Loss: 2.0890, Train: 0.3307, Test: 0.3141\n",
            "Early stopping:  0.03433807330986146\n",
            "Epoch: 033, Loss: 2.0824, Train: 0.3335, Test: 0.3181\n",
            "Early stopping:  0.025867424726946964\n",
            "Epoch: 034, Loss: 2.0748, Train: 0.3437, Test: 0.3222\n",
            "Early stopping:  0.01841265600290771\n",
            "Epoch: 035, Loss: 2.0630, Train: 0.3358, Test: 0.3227\n",
            "Early stopping:  0.015109449685320714\n",
            "Epoch: 036, Loss: 2.0555, Train: 0.3369, Test: 0.3279\n",
            "Early stopping:  0.01372881981268258\n",
            "Epoch: 037, Loss: 2.0377, Train: 0.3347, Test: 0.3273\n",
            "Early stopping:  0.017413616280307766\n",
            "Epoch: 038, Loss: 2.0231, Train: 0.3454, Test: 0.3289\n",
            "Early stopping:  0.020541573631261675\n",
            "Epoch: 039, Loss: 2.0044, Train: 0.3545, Test: 0.3361\n",
            "Early stopping:  0.02385913214886617\n",
            "Epoch: 040, Loss: 1.9951, Train: 0.3607, Test: 0.3395\n",
            "Early stopping:  0.024480902886651057\n",
            "Epoch: 041, Loss: 1.9769, Train: 0.3630, Test: 0.3419\n",
            "Early stopping:  0.023747138258704677\n",
            "Epoch: 042, Loss: 1.9704, Train: 0.3590, Test: 0.3405\n",
            "Early stopping:  0.02124538349553128\n",
            "Epoch: 043, Loss: 1.9593, Train: 0.3556, Test: 0.3405\n",
            "Early stopping:  0.018326853579238342\n",
            "Epoch: 044, Loss: 1.9526, Train: 0.3528, Test: 0.3399\n",
            "Early stopping:  0.016524960242890754\n",
            "Epoch: 045, Loss: 1.9444, Train: 0.3562, Test: 0.3436\n",
            "Early stopping:  0.01314917134361975\n",
            "Epoch: 046, Loss: 1.9309, Train: 0.3625, Test: 0.3455\n",
            "Early stopping:  0.014963067993724212\n",
            "Epoch: 047, Loss: 1.9199, Train: 0.3642, Test: 0.3475\n",
            "Early stopping:  0.016017290042239864\n",
            "Epoch: 048, Loss: 1.9053, Train: 0.3664, Test: 0.3493\n",
            "Early stopping:  0.018888028518878348\n",
            "Epoch: 049, Loss: 1.8932, Train: 0.3670, Test: 0.3490\n",
            "Early stopping:  0.020254181404526343\n",
            "Epoch: 050, Loss: 1.8820, Train: 0.3687, Test: 0.3516\n",
            "Early stopping:  0.019705573808819467\n",
            "Epoch: 051, Loss: 1.8727, Train: 0.3721, Test: 0.3541\n",
            "Early stopping:  0.018662055722912112\n",
            "Epoch: 052, Loss: 1.8638, Train: 0.3772, Test: 0.3554\n",
            "Early stopping:  0.016401085308228574\n",
            "Epoch: 053, Loss: 1.8542, Train: 0.3834, Test: 0.3588\n",
            "Early stopping:  0.015227594763310485\n",
            "Epoch: 054, Loss: 1.8444, Train: 0.3953, Test: 0.3639\n",
            "Early stopping:  0.014848544456649616\n",
            "Epoch: 055, Loss: 1.8333, Train: 0.3971, Test: 0.3645\n",
            "Early stopping:  0.015543547070893133\n",
            "Epoch: 056, Loss: 1.8232, Train: 0.3953, Test: 0.3632\n",
            "Early stopping:  0.01612445559542988\n",
            "Epoch: 057, Loss: 1.8140, Train: 0.3988, Test: 0.3646\n",
            "Early stopping:  0.016066868510319604\n",
            "Epoch: 058, Loss: 1.8062, Train: 0.3982, Test: 0.3643\n",
            "Early stopping:  0.015159517798777566\n",
            "Epoch: 059, Loss: 1.7989, Train: 0.3942, Test: 0.3655\n",
            "Early stopping:  0.013598617148171963\n",
            "Epoch: 060, Loss: 1.7901, Train: 0.4005, Test: 0.3682\n",
            "Early stopping:  0.01286755099475282\n",
            "Epoch: 061, Loss: 1.7811, Train: 0.4027, Test: 0.3694\n",
            "Early stopping:  0.012940103298561269\n",
            "Epoch: 062, Loss: 1.7715, Train: 0.4056, Test: 0.3709\n",
            "Early stopping:  0.013803994294062975\n",
            "Epoch: 063, Loss: 1.7628, Train: 0.4078, Test: 0.3730\n",
            "Early stopping:  0.014378959181849804\n",
            "Epoch: 064, Loss: 1.7542, Train: 0.4118, Test: 0.3733\n",
            "Early stopping:  0.01426352620723752\n",
            "Epoch: 065, Loss: 1.7462, Train: 0.4152, Test: 0.3762\n",
            "Early stopping:  0.013794725385547914\n",
            "Epoch: 066, Loss: 1.7382, Train: 0.4163, Test: 0.3760\n",
            "Early stopping:  0.013167314651216021\n",
            "Epoch: 067, Loss: 1.7300, Train: 0.4175, Test: 0.3764\n",
            "Early stopping:  0.012915570271116908\n",
            "Epoch: 068, Loss: 1.7220, Train: 0.4180, Test: 0.3777\n",
            "Early stopping:  0.012760229696618998\n",
            "Epoch: 069, Loss: 1.7148, Train: 0.4192, Test: 0.3787\n",
            "Early stopping:  0.01248759508870835\n",
            "Epoch: 070, Loss: 1.7072, Train: 0.4385, Test: 0.3924\n",
            "Early stopping:  0.012196351219799112\n",
            "Epoch: 071, Loss: 1.6993, Train: 0.4356, Test: 0.3940\n",
            "Early stopping:  0.012043071864794759\n",
            "Epoch: 072, Loss: 1.6916, Train: 0.4402, Test: 0.3944\n",
            "Early stopping:  0.012073952259963194\n",
            "Epoch: 073, Loss: 1.6833, Train: 0.4407, Test: 0.3965\n",
            "Early stopping:  0.012445411967134523\n",
            "Epoch: 074, Loss: 1.6747, Train: 0.4396, Test: 0.3967\n",
            "Early stopping:  0.012808122312304823\n",
            "Epoch: 075, Loss: 1.6688, Train: 0.4413, Test: 0.3969\n",
            "Early stopping:  0.012323235626784107\n",
            "Epoch: 076, Loss: 1.6643, Train: 0.4407, Test: 0.3993\n",
            "Early stopping:  0.010995720781973342\n",
            "Epoch: 077, Loss: 1.6524, Train: 0.4396, Test: 0.3995\n",
            "Early stopping:  0.011552831874850268\n",
            "Epoch: 078, Loss: 1.6582, Train: 0.4481, Test: 0.4030\n",
            "Early stopping:  0.008739037927023565\n",
            "PREDICTIONS -> tensor([9, 6, 9,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.50      0.00      0.00       758\n",
            "         capital_goods       0.18      0.19      0.19       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.27      0.32      0.30       793\n",
            " consumer_non-cyclical       0.71      0.29      0.41       446\n",
            "                energy       0.83      0.02      0.03       283\n",
            "             financial       0.49      0.60      0.54       767\n",
            "            healthcare       0.71      0.40      0.51       318\n",
            "              services       0.42      0.79      0.55      2076\n",
            "            technology       0.34      0.31      0.32       396\n",
            "        transportation       0.00      0.00      0.00       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.40      7054\n",
            "             macro avg       0.37      0.24      0.24      7054\n",
            "          weighted avg       0.40      0.40      0.34      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 93.3205, Train: 0.1095, Test: 0.1099\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 372.4016, Train: 0.1174, Test: 0.1089\n",
            "Early stopping:  197.3400685066487\n",
            "Epoch: 003, Loss: 384.4827, Train: 0.2042, Test: 0.2114\n",
            "Early stopping:  164.72581032355075\n",
            "Epoch: 004, Loss: 399.5191, Train: 0.1458, Test: 0.1467\n",
            "Early stopping:  146.4941881295966\n",
            "Epoch: 005, Loss: 416.3529, Train: 0.2864, Test: 0.2852\n",
            "Early stopping:  135.11240734379194\n",
            "Epoch: 006, Loss: 398.6057, Train: 0.1503, Test: 0.1466\n",
            "Early stopping:  16.643502492444554\n",
            "Epoch: 007, Loss: 347.0032, Train: 0.0652, Test: 0.0607\n",
            "Early stopping:  26.148845463198565\n",
            "Epoch: 008, Loss: 365.2864, Train: 0.0851, Test: 0.0713\n",
            "Early stopping:  28.331414275834245\n",
            "Epoch: 009, Loss: 308.1473, Train: 0.3057, Test: 0.2943\n",
            "Early stopping:  42.723067896967464\n",
            "Epoch: 010, Loss: 276.1336, Train: 0.3114, Test: 0.2988\n",
            "Early stopping:  47.98938590277448\n",
            "Epoch: 011, Loss: 248.1556, Train: 0.2314, Test: 0.2270\n",
            "Early stopping:  48.46483514149308\n",
            "Epoch: 012, Loss: 198.9435, Train: 0.1588, Test: 0.1506\n",
            "Early stopping:  62.57593213855596\n",
            "Epoch: 013, Loss: 147.1491, Train: 0.2399, Test: 0.2309\n",
            "Early stopping:  63.700323665205374\n",
            "Epoch: 014, Loss: 84.8933, Train: 0.3029, Test: 0.2861\n",
            "Early stopping:  77.06994544398628\n",
            "Epoch: 015, Loss: 43.5241, Train: 0.1566, Test: 0.1521\n",
            "Early stopping:  82.86233097626616\n",
            "Epoch: 016, Loss: 31.6444, Train: 0.1980, Test: 0.1817\n",
            "Early stopping:  70.87931850305097\n",
            "Epoch: 017, Loss: 12.4244, Train: 0.2326, Test: 0.2109\n",
            "Early stopping:  53.56413623018692\n",
            "Epoch: 018, Loss: 5.9232, Train: 0.2575, Test: 0.2379\n",
            "Early stopping:  31.32912828352114\n",
            "Epoch: 019, Loss: 4.7965, Train: 0.2978, Test: 0.2726\n",
            "Early stopping:  17.139852581396262\n",
            "Epoch: 020, Loss: 3.9802, Train: 0.3040, Test: 0.2949\n",
            "Early stopping:  11.607243133184808\n",
            "Epoch: 021, Loss: 3.4260, Train: 0.3137, Test: 0.2953\n",
            "Early stopping:  3.6528157372815104\n",
            "Epoch: 022, Loss: 3.1004, Train: 0.3176, Test: 0.2981\n",
            "Early stopping:  1.1370337803504003\n",
            "Epoch: 023, Loss: 2.8549, Train: 0.3267, Test: 0.3098\n",
            "Early stopping:  0.775618260540507\n",
            "Epoch: 024, Loss: 2.6612, Train: 0.3159, Test: 0.3061\n",
            "Early stopping:  0.5193735528944142\n",
            "Epoch: 025, Loss: 2.5377, Train: 0.3108, Test: 0.3041\n",
            "Early stopping:  0.35561746937201166\n",
            "Epoch: 026, Loss: 2.4309, Train: 0.3103, Test: 0.2998\n",
            "Early stopping:  0.2660390076546643\n",
            "Epoch: 027, Loss: 2.3170, Train: 0.3120, Test: 0.3028\n",
            "Early stopping:  0.20821213774909844\n",
            "Epoch: 028, Loss: 2.2121, Train: 0.3261, Test: 0.3122\n",
            "Early stopping:  0.1770020245685387\n",
            "Epoch: 029, Loss: 2.1619, Train: 0.3330, Test: 0.3200\n",
            "Early stopping:  0.1546432933284258\n",
            "Epoch: 030, Loss: 2.1245, Train: 0.3443, Test: 0.3286\n",
            "Early stopping:  0.12467382750129084\n",
            "Epoch: 031, Loss: 2.0863, Train: 0.3466, Test: 0.3344\n",
            "Early stopping:  0.08940270460353225\n",
            "Epoch: 032, Loss: 2.0581, Train: 0.3596, Test: 0.3339\n",
            "Early stopping:  0.06092999796926148\n",
            "Epoch: 033, Loss: 2.0339, Train: 0.3596, Test: 0.3401\n",
            "Early stopping:  0.0512100863019668\n",
            "Epoch: 034, Loss: 2.0207, Train: 0.3761, Test: 0.3547\n",
            "Early stopping:  0.041746878584264346\n",
            "Epoch: 035, Loss: 2.0188, Train: 0.3851, Test: 0.3531\n",
            "Early stopping:  0.028597838080397342\n",
            "Epoch: 036, Loss: 2.0068, Train: 0.3846, Test: 0.3534\n",
            "Early stopping:  0.019544906574478448\n",
            "Epoch: 037, Loss: 1.9760, Train: 0.3857, Test: 0.3543\n",
            "Early stopping:  0.02191176485815877\n",
            "Epoch: 038, Loss: 1.9473, Train: 0.3755, Test: 0.3496\n",
            "Early stopping:  0.031592411787713985\n",
            "Epoch: 039, Loss: 1.9341, Train: 0.3772, Test: 0.3472\n",
            "Early stopping:  0.036590463730754835\n",
            "Epoch: 040, Loss: 1.9224, Train: 0.3846, Test: 0.3521\n",
            "Early stopping:  0.03412638847944522\n",
            "Epoch: 041, Loss: 1.9026, Train: 0.3965, Test: 0.3623\n",
            "Early stopping:  0.027545535407616135\n",
            "Epoch: 042, Loss: 1.8807, Train: 0.4039, Test: 0.3694\n",
            "Early stopping:  0.026296635351128996\n",
            "Epoch: 043, Loss: 1.8556, Train: 0.3993, Test: 0.3745\n",
            "Early stopping:  0.03164690181198649\n",
            "Epoch: 044, Loss: 1.8344, Train: 0.4056, Test: 0.3792\n",
            "Early stopping:  0.035283255653117324\n",
            "Epoch: 045, Loss: 1.8162, Train: 0.4107, Test: 0.3838\n",
            "Early stopping:  0.03468446844437348\n",
            "Epoch: 046, Loss: 1.8001, Train: 0.4203, Test: 0.3900\n",
            "Early stopping:  0.03184522926752715\n",
            "Epoch: 047, Loss: 1.7845, Train: 0.4282, Test: 0.3944\n",
            "Early stopping:  0.027990665689615928\n",
            "Epoch: 048, Loss: 1.7666, Train: 0.4328, Test: 0.3951\n",
            "Early stopping:  0.026469870842318076\n",
            "Epoch: 049, Loss: 1.7504, Train: 0.4385, Test: 0.4009\n",
            "Early stopping:  0.026127791906703942\n",
            "Epoch: 050, Loss: 1.7338, Train: 0.4470, Test: 0.4039\n",
            "Early stopping:  0.0263652696594298\n",
            "Epoch: 051, Loss: 1.7161, Train: 0.4549, Test: 0.4049\n",
            "Early stopping:  0.026813326124299503\n",
            "Epoch: 052, Loss: 1.6985, Train: 0.4538, Test: 0.4134\n",
            "Early stopping:  0.026955069846201836\n",
            "Epoch: 053, Loss: 1.6812, Train: 0.4606, Test: 0.4178\n",
            "Early stopping:  0.02744915226580199\n",
            "Epoch: 054, Loss: 1.6661, Train: 0.4634, Test: 0.4175\n",
            "Early stopping:  0.02693341906734891\n",
            "Epoch: 055, Loss: 1.6501, Train: 0.4640, Test: 0.4222\n",
            "Early stopping:  0.026010927600074854\n",
            "Epoch: 056, Loss: 1.6336, Train: 0.4685, Test: 0.4264\n",
            "Early stopping:  0.02546596192871168\n",
            "Epoch: 057, Loss: 1.6186, Train: 0.4719, Test: 0.4310\n",
            "Early stopping:  0.02493671162741808\n",
            "Epoch: 058, Loss: 1.6033, Train: 0.4736, Test: 0.4328\n",
            "Early stopping:  0.024815039427469366\n",
            "Epoch: 059, Loss: 1.5897, Train: 0.4850, Test: 0.4380\n",
            "Early stopping:  0.02386825018638173\n",
            "Epoch: 060, Loss: 1.5786, Train: 0.4884, Test: 0.4380\n",
            "Early stopping:  0.02200351774526449\n",
            "Epoch: 061, Loss: 1.5665, Train: 0.4861, Test: 0.4412\n",
            "Early stopping:  0.02045397889026018\n",
            "Epoch: 062, Loss: 1.5561, Train: 0.4918, Test: 0.4458\n",
            "Early stopping:  0.018645453612904563\n",
            "Epoch: 063, Loss: 1.5466, Train: 0.4929, Test: 0.4443\n",
            "Early stopping:  0.01719846080240054\n",
            "Epoch: 064, Loss: 1.5330, Train: 0.4963, Test: 0.4471\n",
            "Early stopping:  0.01756661742978892\n",
            "Epoch: 065, Loss: 1.5192, Train: 0.4986, Test: 0.4524\n",
            "Early stopping:  0.01864313097909693\n",
            "Epoch: 066, Loss: 1.5084, Train: 0.4991, Test: 0.4509\n",
            "Early stopping:  0.019431341066016062\n",
            "Epoch: 067, Loss: 1.4984, Train: 0.5048, Test: 0.4552\n",
            "Early stopping:  0.019181830552718406\n",
            "Epoch: 068, Loss: 1.4879, Train: 0.5065, Test: 0.4561\n",
            "Early stopping:  0.01757469268814117\n",
            "Epoch: 069, Loss: 1.4765, Train: 0.5048, Test: 0.4541\n",
            "Early stopping:  0.016749048009828522\n",
            "Epoch: 070, Loss: 1.4699, Train: 0.5156, Test: 0.4587\n",
            "Early stopping:  0.015679227745280497\n",
            "Epoch: 071, Loss: 1.4647, Train: 0.5156, Test: 0.4586\n",
            "Early stopping:  0.01369145967790351\n",
            "Epoch: 072, Loss: 1.4466, Train: 0.5111, Test: 0.4552\n",
            "Early stopping:  0.015294922210722206\n",
            "Epoch: 073, Loss: 1.4569, Train: 0.5173, Test: 0.4621\n",
            "Early stopping:  0.011608965854543468\n",
            "Epoch: 074, Loss: 1.4455, Train: 0.5150, Test: 0.4606\n",
            "Early stopping:  0.010794838206837425\n",
            "Epoch: 075, Loss: 1.4438, Train: 0.5224, Test: 0.4633\n",
            "Early stopping:  0.008974639701939421\n",
            "PREDICTIONS -> tensor([ 9,  8,  7,  ..., 10,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.35      0.61      0.44       758\n",
            "         capital_goods       0.34      0.07      0.12       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.45      0.42      0.44       793\n",
            " consumer_non-cyclical       0.72      0.29      0.41       446\n",
            "                energy       0.42      0.04      0.07       283\n",
            "             financial       0.51      0.50      0.50       767\n",
            "            healthcare       0.67      0.23      0.34       318\n",
            "              services       0.48      0.76      0.59      2076\n",
            "            technology       0.44      0.15      0.22       396\n",
            "        transportation       0.54      0.52      0.53       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.46      7054\n",
            "             macro avg       0.41      0.30      0.30      7054\n",
            "          weighted avg       0.46      0.46      0.42      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 33.5050, Train: 0.0635, Test: 0.0637\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 448.4086, Train: 0.3074, Test: 0.3089\n",
            "Early stopping:  293.3811212658762\n",
            "Epoch: 003, Loss: 322.1792, Train: 0.1418, Test: 0.1428\n",
            "Early stopping:  212.68583907917548\n",
            "Epoch: 004, Loss: 322.9138, Train: 0.1129, Test: 0.1015\n",
            "Early stopping:  175.81204644753842\n",
            "Epoch: 005, Loss: 382.2705, Train: 0.0726, Test: 0.0693\n",
            "Early stopping:  158.7551996709263\n",
            "Epoch: 006, Loss: 357.5671, Train: 0.0539, Test: 0.0519\n",
            "Early stopping:  52.20505989718954\n",
            "Epoch: 007, Loss: 317.6519, Train: 0.1061, Test: 0.1018\n",
            "Early stopping:  28.297609250770293\n",
            "Epoch: 008, Loss: 191.0018, Train: 0.1185, Test: 0.1117\n",
            "Early stopping:  73.79010679023892\n",
            "Epoch: 009, Loss: 178.8009, Train: 0.3012, Test: 0.3027\n",
            "Early stopping:  94.74512606934461\n",
            "Epoch: 010, Loss: 162.6431, Train: 0.2995, Test: 0.3038\n",
            "Early stopping:  89.4012920441052\n",
            "Epoch: 011, Loss: 154.0543, Train: 0.2915, Test: 0.2984\n",
            "Early stopping:  66.84956315098329\n",
            "Epoch: 012, Loss: 132.1725, Train: 0.2819, Test: 0.2874\n",
            "Early stopping:  22.70217197447258\n",
            "Epoch: 013, Loss: 105.6008, Train: 0.2836, Test: 0.2811\n",
            "Early stopping:  28.45572507700718\n",
            "Epoch: 014, Loss: 75.6859, Train: 0.2229, Test: 0.2217\n",
            "Early stopping:  35.76132931088865\n",
            "Epoch: 015, Loss: 45.6158, Train: 0.2467, Test: 0.2282\n",
            "Early stopping:  43.30891235991697\n",
            "Epoch: 016, Loss: 21.0417, Train: 0.1741, Test: 0.1783\n",
            "Early stopping:  44.65219858476644\n",
            "Epoch: 017, Loss: 12.8020, Train: 0.1310, Test: 0.1280\n",
            "Early stopping:  38.631632925946164\n",
            "Epoch: 018, Loss: 10.9976, Train: 0.1293, Test: 0.1223\n",
            "Early stopping:  27.459272134118407\n",
            "Epoch: 019, Loss: 9.1179, Train: 0.1333, Test: 0.1211\n",
            "Early stopping:  15.070609202018957\n",
            "Epoch: 020, Loss: 7.7494, Train: 0.1299, Test: 0.1204\n",
            "Early stopping:  5.224695663521446\n",
            "Epoch: 021, Loss: 6.5466, Train: 0.1322, Test: 0.1276\n",
            "Early stopping:  2.5031237977487812\n",
            "Epoch: 022, Loss: 5.3752, Train: 0.1424, Test: 0.1413\n",
            "Early stopping:  2.196068276269143\n",
            "Epoch: 023, Loss: 4.3498, Train: 0.1639, Test: 0.1585\n",
            "Early stopping:  1.8857060584045464\n",
            "Epoch: 024, Loss: 3.5596, Train: 0.1764, Test: 0.1688\n",
            "Early stopping:  1.6776128378381419\n",
            "Epoch: 025, Loss: 3.0852, Train: 0.1900, Test: 0.1768\n",
            "Early stopping:  1.3989501352404015\n",
            "Epoch: 026, Loss: 2.8825, Train: 0.1980, Test: 0.1885\n",
            "Early stopping:  1.0224018043049523\n",
            "Epoch: 027, Loss: 2.7856, Train: 0.2008, Test: 0.1972\n",
            "Early stopping:  0.6421391783818545\n",
            "Epoch: 028, Loss: 2.7291, Train: 0.2008, Test: 0.1958\n",
            "Early stopping:  0.33663747756381457\n",
            "Epoch: 029, Loss: 2.6525, Train: 0.1997, Test: 0.1968\n",
            "Early stopping:  0.1669746111850025\n",
            "Epoch: 030, Loss: 2.5229, Train: 0.1991, Test: 0.1979\n",
            "Early stopping:  0.13606497114233346\n",
            "Epoch: 031, Loss: 2.3924, Train: 0.2082, Test: 0.2007\n",
            "Early stopping:  0.1593135019692571\n",
            "Epoch: 032, Loss: 2.2890, Train: 0.2195, Test: 0.2070\n",
            "Early stopping:  0.18087804734133933\n",
            "Epoch: 033, Loss: 2.2134, Train: 0.2144, Test: 0.2013\n",
            "Early stopping:  0.1768095670488298\n",
            "Epoch: 034, Loss: 2.2184, Train: 0.2314, Test: 0.2117\n",
            "Early stopping:  0.13112186170073611\n",
            "Epoch: 035, Loss: 2.1674, Train: 0.2206, Test: 0.2068\n",
            "Early stopping:  0.08770951929953513\n",
            "Epoch: 036, Loss: 2.1664, Train: 0.2201, Test: 0.2037\n",
            "Early stopping:  0.05007233987108099\n",
            "Epoch: 037, Loss: 2.1644, Train: 0.2246, Test: 0.2024\n",
            "Early stopping:  0.02736860502589692\n",
            "Epoch: 038, Loss: 2.1551, Train: 0.2212, Test: 0.2030\n",
            "Early stopping:  0.02510985903804982\n",
            "Epoch: 039, Loss: 2.1394, Train: 0.2377, Test: 0.2095\n",
            "Early stopping:  0.01175993840400978\n",
            "Epoch: 040, Loss: 2.1202, Train: 0.2513, Test: 0.2258\n",
            "Early stopping:  0.019356941547052253\n",
            "Epoch: 041, Loss: 2.1081, Train: 0.2615, Test: 0.2401\n",
            "Early stopping:  0.02345987883783938\n",
            "Epoch: 042, Loss: 2.0813, Train: 0.2643, Test: 0.2479\n",
            "Early stopping:  0.02847983381568373\n",
            "Epoch: 043, Loss: 2.0549, Train: 0.2609, Test: 0.2528\n",
            "Early stopping:  0.0332173462244952\n",
            "Epoch: 044, Loss: 2.0354, Train: 0.2825, Test: 0.2664\n",
            "Early stopping:  0.035473803661756696\n",
            "Epoch: 045, Loss: 2.0145, Train: 0.2955, Test: 0.2855\n",
            "Early stopping:  0.03695164874149648\n",
            "Epoch: 046, Loss: 1.9951, Train: 0.3063, Test: 0.3056\n",
            "Early stopping:  0.03371026978430763\n",
            "Epoch: 047, Loss: 1.9736, Train: 0.3210, Test: 0.3141\n",
            "Early stopping:  0.03205960873117144\n",
            "Epoch: 048, Loss: 1.9489, Train: 0.3358, Test: 0.3157\n",
            "Early stopping:  0.03385980354844566\n",
            "Epoch: 049, Loss: 1.9278, Train: 0.3562, Test: 0.3270\n",
            "Early stopping:  0.034772039322453235\n",
            "Epoch: 050, Loss: 1.9108, Train: 0.3534, Test: 0.3285\n",
            "Early stopping:  0.033980678792366\n",
            "Epoch: 051, Loss: 1.8938, Train: 0.3630, Test: 0.3313\n",
            "Early stopping:  0.03138258784981496\n",
            "Epoch: 052, Loss: 1.8764, Train: 0.3619, Test: 0.3339\n",
            "Early stopping:  0.02829793520374561\n",
            "Epoch: 053, Loss: 1.8637, Train: 0.3829, Test: 0.3513\n",
            "Early stopping:  0.025733314858813392\n",
            "Epoch: 054, Loss: 1.8482, Train: 0.3863, Test: 0.3595\n",
            "Early stopping:  0.024574511909778284\n",
            "Epoch: 055, Loss: 1.8304, Train: 0.3925, Test: 0.3697\n",
            "Early stopping:  0.024562027688382254\n",
            "Epoch: 056, Loss: 1.8177, Train: 0.4010, Test: 0.3767\n",
            "Early stopping:  0.02389319181870956\n",
            "Epoch: 057, Loss: 1.8018, Train: 0.4101, Test: 0.3792\n",
            "Early stopping:  0.024414064612705167\n",
            "Epoch: 058, Loss: 1.7827, Train: 0.4146, Test: 0.3809\n",
            "Early stopping:  0.02528253848286424\n",
            "Epoch: 059, Loss: 1.7654, Train: 0.4226, Test: 0.3840\n",
            "Early stopping:  0.02613399099328301\n",
            "Epoch: 060, Loss: 1.7462, Train: 0.4277, Test: 0.3870\n",
            "Early stopping:  0.02838526542579292\n",
            "Epoch: 061, Loss: 1.7257, Train: 0.4265, Test: 0.3903\n",
            "Early stopping:  0.02987290485970591\n",
            "Epoch: 062, Loss: 1.7075, Train: 0.4311, Test: 0.3903\n",
            "Early stopping:  0.030089418449843388\n",
            "Epoch: 063, Loss: 1.6914, Train: 0.4368, Test: 0.3985\n",
            "Early stopping:  0.02954124942048461\n",
            "Epoch: 064, Loss: 1.6702, Train: 0.4368, Test: 0.3996\n",
            "Early stopping:  0.029454395288579925\n",
            "Epoch: 065, Loss: 1.6564, Train: 0.4385, Test: 0.4046\n",
            "Early stopping:  0.02783358303120055\n",
            "Epoch: 066, Loss: 1.6358, Train: 0.4526, Test: 0.4121\n",
            "Early stopping:  0.028243950497063706\n",
            "Epoch: 067, Loss: 1.6206, Train: 0.4555, Test: 0.4144\n",
            "Early stopping:  0.027896630100624344\n",
            "Epoch: 068, Loss: 1.6009, Train: 0.4617, Test: 0.4169\n",
            "Early stopping:  0.027644857800480628\n",
            "Epoch: 069, Loss: 1.5871, Train: 0.4634, Test: 0.4227\n",
            "Early stopping:  0.02749839199093727\n",
            "Epoch: 070, Loss: 1.5720, Train: 0.4668, Test: 0.4254\n",
            "Early stopping:  0.025508290739251996\n",
            "Epoch: 071, Loss: 1.5572, Train: 0.4685, Test: 0.4271\n",
            "Early stopping:  0.02464541434840423\n",
            "Epoch: 072, Loss: 1.5447, Train: 0.4742, Test: 0.4322\n",
            "Early stopping:  0.02250310026460161\n",
            "Epoch: 073, Loss: 1.5303, Train: 0.4793, Test: 0.4351\n",
            "Early stopping:  0.022289393986380836\n",
            "Epoch: 074, Loss: 1.5193, Train: 0.4816, Test: 0.4373\n",
            "Early stopping:  0.020935319436707327\n",
            "Epoch: 075, Loss: 1.5048, Train: 0.4878, Test: 0.4400\n",
            "Early stopping:  0.02059127915863304\n",
            "Epoch: 076, Loss: 1.4940, Train: 0.4895, Test: 0.4415\n",
            "Early stopping:  0.02006781722826453\n",
            "Epoch: 077, Loss: 1.4792, Train: 0.4980, Test: 0.4427\n",
            "Early stopping:  0.020207130174801916\n",
            "Epoch: 078, Loss: 1.4691, Train: 0.4997, Test: 0.4451\n",
            "Early stopping:  0.019978954005162346\n",
            "Epoch: 079, Loss: 1.4573, Train: 0.5020, Test: 0.4475\n",
            "Early stopping:  0.01898310484424753\n",
            "Epoch: 080, Loss: 1.4453, Train: 0.5026, Test: 0.4498\n",
            "Early stopping:  0.018899023997582625\n",
            "Epoch: 081, Loss: 1.4340, Train: 0.5060, Test: 0.4517\n",
            "Early stopping:  0.018052675022167577\n",
            "Epoch: 082, Loss: 1.4219, Train: 0.5099, Test: 0.4529\n",
            "Early stopping:  0.018626720310636488\n",
            "Epoch: 083, Loss: 1.4096, Train: 0.5150, Test: 0.4541\n",
            "Early stopping:  0.018805109193881377\n",
            "Epoch: 084, Loss: 1.3985, Train: 0.5190, Test: 0.4568\n",
            "Early stopping:  0.01866222000993874\n",
            "Epoch: 085, Loss: 1.3870, Train: 0.5235, Test: 0.4579\n",
            "Early stopping:  0.018560595549426906\n",
            "Epoch: 086, Loss: 1.3765, Train: 0.5241, Test: 0.4593\n",
            "Early stopping:  0.01790500873441867\n",
            "Epoch: 087, Loss: 1.3648, Train: 0.5247, Test: 0.4602\n",
            "Early stopping:  0.01761844022183987\n",
            "Epoch: 088, Loss: 1.3544, Train: 0.5281, Test: 0.4627\n",
            "Early stopping:  0.017443360653179012\n",
            "Epoch: 089, Loss: 1.3431, Train: 0.5337, Test: 0.4654\n",
            "Early stopping:  0.017399502831237413\n",
            "Epoch: 090, Loss: 1.3333, Train: 0.5366, Test: 0.4671\n",
            "Early stopping:  0.017125083647986917\n",
            "Epoch: 091, Loss: 1.3230, Train: 0.5389, Test: 0.4687\n",
            "Early stopping:  0.016592253137293723\n",
            "Epoch: 092, Loss: 1.3130, Train: 0.5411, Test: 0.4704\n",
            "Early stopping:  0.016283327828887837\n",
            "Epoch: 093, Loss: 1.3036, Train: 0.5451, Test: 0.4699\n",
            "Early stopping:  0.01569530262833191\n",
            "Epoch: 094, Loss: 1.2930, Train: 0.5496, Test: 0.4711\n",
            "Early stopping:  0.015789024246389803\n",
            "Epoch: 095, Loss: 1.2834, Train: 0.5559, Test: 0.4709\n",
            "Early stopping:  0.015657187604272066\n",
            "Epoch: 096, Loss: 1.2738, Train: 0.5598, Test: 0.4742\n",
            "Early stopping:  0.015587530707512464\n",
            "Epoch: 097, Loss: 1.2641, Train: 0.5644, Test: 0.4749\n",
            "Early stopping:  0.01552328070004158\n",
            "Epoch: 098, Loss: 1.2543, Train: 0.5683, Test: 0.4738\n",
            "Early stopping:  0.015305580970438024\n",
            "Epoch: 099, Loss: 1.2449, Train: 0.5723, Test: 0.4758\n",
            "Early stopping:  0.015258370177984612\n",
            "Epoch: 100, Loss: 1.2364, Train: 0.5701, Test: 0.4763\n",
            "Early stopping:  0.014874867396879439\n",
            "Epoch: 101, Loss: 1.2267, Train: 0.5729, Test: 0.4768\n",
            "Early stopping:  0.01466820459316155\n",
            "Epoch: 102, Loss: 1.2164, Train: 0.5769, Test: 0.4790\n",
            "Early stopping:  0.014861145520428908\n",
            "Epoch: 103, Loss: 1.2062, Train: 0.5797, Test: 0.4780\n",
            "Early stopping:  0.01542093791171675\n",
            "Epoch: 104, Loss: 1.1991, Train: 0.5831, Test: 0.4777\n",
            "Early stopping:  0.015070732664534852\n",
            "Epoch: 105, Loss: 1.1918, Train: 0.5888, Test: 0.4783\n",
            "Early stopping:  0.013840880166294986\n",
            "Epoch: 106, Loss: 1.1848, Train: 0.5956, Test: 0.4824\n",
            "Early stopping:  0.012306639548490178\n",
            "Epoch: 107, Loss: 1.1721, Train: 0.5978, Test: 0.4855\n",
            "Early stopping:  0.013153587585083636\n",
            "Epoch: 108, Loss: 1.1619, Train: 0.6001, Test: 0.4875\n",
            "Early stopping:  0.014989513878549464\n",
            "Epoch: 109, Loss: 1.1591, Train: 0.6081, Test: 0.4875\n",
            "Early stopping:  0.014194107781649626\n",
            "Epoch: 110, Loss: 1.1480, Train: 0.6081, Test: 0.4898\n",
            "Early stopping:  0.01392967623854669\n",
            "Epoch: 111, Loss: 1.1401, Train: 0.6109, Test: 0.4891\n",
            "Early stopping:  0.012434131249181664\n",
            "Epoch: 112, Loss: 1.1409, Train: 0.6137, Test: 0.4894\n",
            "Early stopping:  0.010107316787383112\n",
            "Epoch: 113, Loss: 1.1286, Train: 0.6109, Test: 0.4905\n",
            "Early stopping:  0.011215899627053778\n",
            "Epoch: 114, Loss: 1.1301, Train: 0.6075, Test: 0.4894\n",
            "Early stopping:  0.008099926189803175\n",
            "PREDICTIONS -> tensor([1, 1, 1,  ..., 6, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.46      0.58      0.52       758\n",
            "         capital_goods       0.28      0.18      0.22       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.44      0.50      0.47       793\n",
            " consumer_non-cyclical       0.53      0.45      0.49       446\n",
            "                energy       0.58      0.21      0.31       283\n",
            "             financial       0.43      0.61      0.51       767\n",
            "            healthcare       0.65      0.36      0.46       318\n",
            "              services       0.53      0.70      0.61      2076\n",
            "            technology       0.34      0.10      0.16       396\n",
            "        transportation       0.65      0.44      0.53       404\n",
            "             utilities       0.44      0.08      0.14       225\n",
            "\n",
            "              accuracy                           0.49      7054\n",
            "             macro avg       0.45      0.35      0.37      7054\n",
            "          weighted avg       0.48      0.49      0.46      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 64.4920, Train: 0.0437, Test: 0.0431\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 612.4892, Train: 0.0635, Test: 0.0600\n",
            "Early stopping:  387.49255238426224\n",
            "Epoch: 003, Loss: 565.2216, Train: 0.0641, Test: 0.0642\n",
            "Early stopping:  303.6624654132809\n",
            "Epoch: 004, Loss: 580.4896, Train: 0.1123, Test: 0.1103\n",
            "Early stopping:  261.5301156983599\n",
            "Epoch: 005, Loss: 413.3059, Train: 0.0947, Test: 0.0926\n",
            "Early stopping:  227.28285774493077\n",
            "Epoch: 006, Loss: 293.8163, Train: 0.0408, Test: 0.0398\n",
            "Early stopping:  135.2528534477288\n",
            "Epoch: 007, Loss: 231.8994, Train: 0.0868, Test: 0.0858\n",
            "Early stopping:  156.64338176137\n",
            "Epoch: 008, Loss: 152.5228, Train: 0.1707, Test: 0.1715\n",
            "Early stopping:  167.3461668553049\n",
            "Epoch: 009, Loss: 104.6596, Train: 0.1435, Test: 0.1442\n",
            "Early stopping:  121.41163542461813\n",
            "Epoch: 010, Loss: 67.4856, Train: 0.1617, Test: 0.1574\n",
            "Early stopping:  92.52837665582594\n",
            "Epoch: 011, Loss: 24.2216, Train: 0.1900, Test: 0.1851\n",
            "Early stopping:  80.11238865067625\n",
            "Epoch: 012, Loss: 7.3911, Train: 0.1628, Test: 0.1714\n",
            "Early stopping:  59.21416258954835\n",
            "Epoch: 013, Loss: 5.5548, Train: 0.1662, Test: 0.1724\n",
            "Early stopping:  43.06316057084921\n",
            "Epoch: 014, Loss: 4.4577, Train: 0.1826, Test: 0.1877\n",
            "Early stopping:  26.7643213083597\n",
            "Epoch: 015, Loss: 3.5592, Train: 0.2121, Test: 0.2107\n",
            "Early stopping:  8.60789523959327\n",
            "Epoch: 016, Loss: 2.9160, Train: 0.2592, Test: 0.2526\n",
            "Early stopping:  1.7668288591270869\n",
            "Epoch: 017, Loss: 2.6409, Train: 0.2450, Test: 0.2367\n",
            "Early stopping:  1.192836659948628\n",
            "Epoch: 018, Loss: 2.5061, Train: 0.2076, Test: 0.2053\n",
            "Early stopping:  0.803780108310664\n",
            "Epoch: 019, Loss: 2.4212, Train: 0.3199, Test: 0.3156\n",
            "Early stopping:  0.45955483993381996\n",
            "Epoch: 020, Loss: 2.3786, Train: 0.3369, Test: 0.3256\n",
            "Early stopping:  0.21656795354145078\n",
            "Epoch: 021, Loss: 2.3480, Train: 0.3307, Test: 0.3341\n",
            "Early stopping:  0.11780715900133502\n",
            "Epoch: 022, Loss: 2.3078, Train: 0.3205, Test: 0.3214\n",
            "Early stopping:  0.07592030398606922\n",
            "Epoch: 023, Loss: 2.2991, Train: 0.3250, Test: 0.3272\n",
            "Early stopping:  0.050635878318694046\n",
            "Epoch: 024, Loss: 2.3134, Train: 0.3318, Test: 0.3300\n",
            "Early stopping:  0.0332134034908614\n",
            "Epoch: 025, Loss: 2.2489, Train: 0.3358, Test: 0.3320\n",
            "Early stopping:  0.03571923037089036\n",
            "Epoch: 026, Loss: 2.2262, Train: 0.3335, Test: 0.3343\n",
            "Early stopping:  0.03909364582918129\n",
            "Epoch: 027, Loss: 2.2153, Train: 0.3403, Test: 0.3330\n",
            "Early stopping:  0.04369484527396672\n",
            "Epoch: 028, Loss: 2.1990, Train: 0.3239, Test: 0.3259\n",
            "Early stopping:  0.04456565267832807\n",
            "Epoch: 029, Loss: 2.1846, Train: 0.3403, Test: 0.3317\n",
            "Early stopping:  0.024764688139739505\n",
            "Epoch: 030, Loss: 2.1596, Train: 0.3528, Test: 0.3385\n",
            "Early stopping:  0.026186325858835062\n",
            "Epoch: 031, Loss: 2.1342, Train: 0.3500, Test: 0.3429\n",
            "Early stopping:  0.03213606711226943\n",
            "Epoch: 032, Loss: 2.1029, Train: 0.3539, Test: 0.3480\n",
            "Early stopping:  0.03865502337759675\n",
            "Epoch: 033, Loss: 2.0777, Train: 0.3630, Test: 0.3558\n",
            "Early stopping:  0.04282349794829898\n",
            "Epoch: 034, Loss: 2.0549, Train: 0.3636, Test: 0.3618\n",
            "Early stopping:  0.04211777556843058\n",
            "Epoch: 035, Loss: 2.0334, Train: 0.3670, Test: 0.3625\n",
            "Early stopping:  0.039585712200323635\n",
            "Epoch: 036, Loss: 2.0054, Train: 0.3727, Test: 0.3667\n",
            "Early stopping:  0.037843109561315244\n",
            "Epoch: 037, Loss: 1.9746, Train: 0.3812, Test: 0.3701\n",
            "Early stopping:  0.040537070141740385\n",
            "Epoch: 038, Loss: 1.9424, Train: 0.3851, Test: 0.3733\n",
            "Early stopping:  0.0449780853137402\n",
            "Epoch: 039, Loss: 1.9157, Train: 0.3919, Test: 0.3787\n",
            "Early stopping:  0.04721240583678671\n",
            "Epoch: 040, Loss: 1.8915, Train: 0.4050, Test: 0.3914\n",
            "Early stopping:  0.04543504905248447\n",
            "Epoch: 041, Loss: 1.8676, Train: 0.4169, Test: 0.3969\n",
            "Early stopping:  0.04197563133115416\n",
            "Epoch: 042, Loss: 1.8452, Train: 0.4203, Test: 0.3995\n",
            "Early stopping:  0.038373600324385734\n",
            "Epoch: 043, Loss: 1.8253, Train: 0.4226, Test: 0.4002\n",
            "Early stopping:  0.035931297959411974\n",
            "Epoch: 044, Loss: 1.8108, Train: 0.4231, Test: 0.4008\n",
            "Early stopping:  0.03234211330481038\n",
            "Epoch: 045, Loss: 1.7907, Train: 0.4214, Test: 0.4013\n",
            "Early stopping:  0.029830554646529648\n",
            "Epoch: 046, Loss: 1.7684, Train: 0.4243, Test: 0.4003\n",
            "Early stopping:  0.0298170069219583\n",
            "Epoch: 047, Loss: 1.7529, Train: 0.4317, Test: 0.4018\n",
            "Early stopping:  0.029669703967758834\n",
            "Epoch: 048, Loss: 1.7353, Train: 0.4317, Test: 0.4016\n",
            "Early stopping:  0.029901199785375615\n",
            "Epoch: 049, Loss: 1.7175, Train: 0.4385, Test: 0.4019\n",
            "Early stopping:  0.028429795057271293\n",
            "Epoch: 050, Loss: 1.6999, Train: 0.4402, Test: 0.4042\n",
            "Early stopping:  0.027254905142801097\n",
            "Epoch: 051, Loss: 1.6820, Train: 0.4436, Test: 0.4073\n",
            "Early stopping:  0.028005186909875156\n",
            "Epoch: 052, Loss: 1.6657, Train: 0.4441, Test: 0.4103\n",
            "Early stopping:  0.027632372269971693\n",
            "Epoch: 053, Loss: 1.6478, Train: 0.4475, Test: 0.4128\n",
            "Early stopping:  0.02744921958578208\n",
            "Epoch: 054, Loss: 1.6319, Train: 0.4453, Test: 0.4108\n",
            "Early stopping:  0.026918098925951747\n",
            "Epoch: 055, Loss: 1.6163, Train: 0.4526, Test: 0.4141\n",
            "Early stopping:  0.026120040293349765\n",
            "Epoch: 056, Loss: 1.6017, Train: 0.4543, Test: 0.4186\n",
            "Early stopping:  0.025242578895564947\n",
            "Epoch: 057, Loss: 1.5885, Train: 0.4566, Test: 0.4195\n",
            "Early stopping:  0.02353475787969496\n",
            "Epoch: 058, Loss: 1.5736, Train: 0.4606, Test: 0.4223\n",
            "Early stopping:  0.022850204972965586\n",
            "Epoch: 059, Loss: 1.5647, Train: 0.4668, Test: 0.4273\n",
            "Early stopping:  0.020840366859181978\n",
            "Epoch: 060, Loss: 1.5481, Train: 0.4697, Test: 0.4283\n",
            "Early stopping:  0.020758767574822132\n",
            "Epoch: 061, Loss: 1.5365, Train: 0.4736, Test: 0.4308\n",
            "Early stopping:  0.02054951602487284\n",
            "Epoch: 062, Loss: 1.5244, Train: 0.4770, Test: 0.4342\n",
            "Early stopping:  0.0200668117666359\n",
            "Epoch: 063, Loss: 1.5101, Train: 0.4821, Test: 0.4355\n",
            "Early stopping:  0.02103950012385025\n",
            "Epoch: 064, Loss: 1.5007, Train: 0.4855, Test: 0.4379\n",
            "Early stopping:  0.019206305303836782\n",
            "Epoch: 065, Loss: 1.4828, Train: 0.4867, Test: 0.4389\n",
            "Early stopping:  0.020798339899820942\n",
            "Epoch: 066, Loss: 1.4706, Train: 0.4957, Test: 0.4416\n",
            "Early stopping:  0.02139722803240492\n",
            "Epoch: 067, Loss: 1.4573, Train: 0.4957, Test: 0.4440\n",
            "Early stopping:  0.021523654438607254\n",
            "Epoch: 068, Loss: 1.4431, Train: 0.5003, Test: 0.4443\n",
            "Early stopping:  0.022282832888847377\n",
            "Epoch: 069, Loss: 1.4294, Train: 0.5043, Test: 0.4446\n",
            "Early stopping:  0.021251430096376263\n",
            "Epoch: 070, Loss: 1.4178, Train: 0.5054, Test: 0.4424\n",
            "Early stopping:  0.021113594628559555\n",
            "Epoch: 071, Loss: 1.4040, Train: 0.5105, Test: 0.4429\n",
            "Early stopping:  0.020847054958723885\n",
            "Epoch: 072, Loss: 1.3932, Train: 0.5150, Test: 0.4456\n",
            "Early stopping:  0.019795399395620278\n",
            "Epoch: 073, Loss: 1.3795, Train: 0.5105, Test: 0.4458\n",
            "Early stopping:  0.01969551001923122\n",
            "Epoch: 074, Loss: 1.3674, Train: 0.5173, Test: 0.4430\n",
            "Early stopping:  0.01986136537511379\n",
            "Epoch: 075, Loss: 1.3536, Train: 0.5201, Test: 0.4446\n",
            "Early stopping:  0.02006981084246825\n",
            "Epoch: 076, Loss: 1.3402, Train: 0.5218, Test: 0.4433\n",
            "Early stopping:  0.020880675793028113\n",
            "Epoch: 077, Loss: 1.3294, Train: 0.5286, Test: 0.4440\n",
            "Early stopping:  0.020133655420763893\n",
            "Epoch: 078, Loss: 1.3157, Train: 0.5298, Test: 0.4458\n",
            "Early stopping:  0.020178869136748545\n",
            "Epoch: 079, Loss: 1.3038, Train: 0.5320, Test: 0.4447\n",
            "Early stopping:  0.019613370710134997\n",
            "Epoch: 080, Loss: 1.2932, Train: 0.5440, Test: 0.4473\n",
            "Early stopping:  0.018910427311824445\n",
            "Epoch: 081, Loss: 1.2805, Train: 0.5440, Test: 0.4488\n",
            "Early stopping:  0.01903252407220493\n",
            "Epoch: 082, Loss: 1.2684, Train: 0.5445, Test: 0.4495\n",
            "Early stopping:  0.018629450383815475\n",
            "Epoch: 083, Loss: 1.2565, Train: 0.5604, Test: 0.4566\n",
            "Early stopping:  0.01889839627997489\n",
            "Epoch: 084, Loss: 1.2437, Train: 0.5627, Test: 0.4587\n",
            "Early stopping:  0.01946727458753088\n",
            "Epoch: 085, Loss: 1.2338, Train: 0.5615, Test: 0.4616\n",
            "Early stopping:  0.018696985046594485\n",
            "Epoch: 086, Loss: 1.2220, Train: 0.5649, Test: 0.4604\n",
            "Early stopping:  0.01828905280652047\n",
            "Epoch: 087, Loss: 1.2152, Train: 0.5638, Test: 0.4621\n",
            "Early stopping:  0.016569580539501926\n",
            "Epoch: 088, Loss: 1.2047, Train: 0.5689, Test: 0.4673\n",
            "Early stopping:  0.015313812342755712\n",
            "Epoch: 089, Loss: 1.1933, Train: 0.5689, Test: 0.4695\n",
            "Early stopping:  0.01559286726003776\n",
            "Epoch: 090, Loss: 1.1809, Train: 0.5746, Test: 0.4653\n",
            "Early stopping:  0.0165456561269407\n",
            "Epoch: 091, Loss: 1.1802, Train: 0.5683, Test: 0.4715\n",
            "Early stopping:  0.015179298285549467\n",
            "Epoch: 092, Loss: 1.1782, Train: 0.5735, Test: 0.4691\n",
            "Early stopping:  0.011317845876561882\n",
            "Epoch: 093, Loss: 1.1645, Train: 0.5859, Test: 0.4639\n",
            "Early stopping:  0.010251026790295605\n",
            "Epoch: 094, Loss: 1.1458, Train: 0.5837, Test: 0.4630\n",
            "Early stopping:  0.01507166149331504\n",
            "Epoch: 095, Loss: 1.1364, Train: 0.5797, Test: 0.4711\n",
            "Early stopping:  0.019461418914663058\n",
            "Epoch: 096, Loss: 1.1600, Train: 0.5950, Test: 0.4660\n",
            "Early stopping:  0.016312344845070974\n",
            "Epoch: 097, Loss: 1.1103, Train: 0.5939, Test: 0.4633\n",
            "Early stopping:  0.021612101021367458\n",
            "Epoch: 098, Loss: 1.1162, Train: 0.5927, Test: 0.4711\n",
            "Early stopping:  0.020596575775858172\n",
            "Epoch: 099, Loss: 1.0948, Train: 0.6012, Test: 0.4681\n",
            "Early stopping:  0.025236582887572627\n",
            "Epoch: 100, Loss: 1.0937, Train: 0.6081, Test: 0.4667\n",
            "Early stopping:  0.0269626244893288\n",
            "Epoch: 101, Loss: 1.0774, Train: 0.6081, Test: 0.4719\n",
            "Early stopping:  0.015272658519697693\n",
            "Epoch: 102, Loss: 1.0728, Train: 0.6132, Test: 0.4688\n",
            "Early stopping:  0.017112938262527324\n",
            "Epoch: 103, Loss: 1.0671, Train: 0.6154, Test: 0.4670\n",
            "Early stopping:  0.012500716486065383\n",
            "Epoch: 104, Loss: 1.0574, Train: 0.6166, Test: 0.4736\n",
            "Early stopping:  0.013445222875920348\n",
            "Epoch: 105, Loss: 1.0431, Train: 0.6194, Test: 0.4776\n",
            "Early stopping:  0.01365381744169632\n",
            "Epoch: 106, Loss: 1.0349, Train: 0.6279, Test: 0.4736\n",
            "Early stopping:  0.01591473013242216\n",
            "Epoch: 107, Loss: 1.0206, Train: 0.6313, Test: 0.4762\n",
            "Early stopping:  0.01830668255802685\n",
            "Epoch: 108, Loss: 1.0145, Train: 0.6313, Test: 0.4810\n",
            "Early stopping:  0.017263257798255025\n",
            "Epoch: 109, Loss: 1.0052, Train: 0.6427, Test: 0.4820\n",
            "Early stopping:  0.015339077093294311\n",
            "Epoch: 110, Loss: 0.9951, Train: 0.6347, Test: 0.4827\n",
            "Early stopping:  0.01514402341223312\n",
            "Epoch: 111, Loss: 0.9972, Train: 0.6347, Test: 0.4804\n",
            "Early stopping:  0.010982341470695177\n",
            "Epoch: 112, Loss: 0.9939, Train: 0.6506, Test: 0.4843\n",
            "Early stopping:  0.008646203931528089\n",
            "PREDICTIONS -> tensor([ 3, 11, 11,  ..., 11,  5,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.62      0.49       758\n",
            "         capital_goods       0.24      0.14      0.17       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.46      0.36      0.41       793\n",
            " consumer_non-cyclical       0.58      0.42      0.49       446\n",
            "                energy       0.40      0.27      0.32       283\n",
            "             financial       0.64      0.56      0.60       767\n",
            "            healthcare       0.64      0.48      0.55       318\n",
            "              services       0.49      0.71      0.58      2076\n",
            "            technology       0.42      0.27      0.33       396\n",
            "        transportation       0.41      0.15      0.22       404\n",
            "             utilities       0.55      0.49      0.52       225\n",
            "\n",
            "              accuracy                           0.48      7054\n",
            "             macro avg       0.44      0.37      0.39      7054\n",
            "          weighted avg       0.47      0.48      0.46      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 91.9239, Train: 0.0908, Test: 0.0910\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 367.1638, Train: 0.2927, Test: 0.2892\n",
            "Early stopping:  194.62404736405585\n",
            "Epoch: 003, Loss: 439.1105, Train: 0.2791, Test: 0.2756\n",
            "Early stopping:  183.2447986245262\n",
            "Epoch: 004, Loss: 262.3686, Train: 0.0533, Test: 0.0512\n",
            "Early stopping:  150.76004581789775\n",
            "Epoch: 005, Loss: 346.0779, Train: 0.1514, Test: 0.1464\n",
            "Early stopping:  132.9368893879375\n",
            "Epoch: 006, Loss: 371.0906, Train: 0.0942, Test: 0.0868\n",
            "Early stopping:  63.47884115622842\n",
            "Epoch: 007, Loss: 421.9782, Train: 0.1140, Test: 0.1177\n",
            "Early stopping:  70.03285937913292\n",
            "Epoch: 008, Loss: 407.5714, Train: 0.1055, Test: 0.1094\n",
            "Early stopping:  63.120027349350856\n",
            "Epoch: 009, Loss: 351.6394, Train: 0.1821, Test: 0.1870\n",
            "Early stopping:  33.74989671989098\n",
            "Epoch: 010, Loss: 312.5235, Train: 0.1696, Test: 0.1663\n",
            "Early stopping:  43.90720542730857\n",
            "Epoch: 011, Loss: 243.2550, Train: 0.1497, Test: 0.1572\n",
            "Early stopping:  72.90926720638436\n",
            "Epoch: 012, Loss: 182.0949, Train: 0.2955, Test: 0.2946\n",
            "Early stopping:  88.7623008080525\n",
            "Epoch: 013, Loss: 205.6450, Train: 0.2967, Test: 0.2959\n",
            "Early stopping:  71.51202492197875\n",
            "Epoch: 014, Loss: 199.9623, Train: 0.2944, Test: 0.2942\n",
            "Early stopping:  51.88364767613418\n",
            "Epoch: 015, Loss: 167.4782, Train: 0.1917, Test: 0.1849\n",
            "Early stopping:  28.638391373298095\n",
            "Epoch: 016, Loss: 149.1037, Train: 0.1792, Test: 0.1759\n",
            "Early stopping:  23.282262110391997\n",
            "Epoch: 017, Loss: 120.5733, Train: 0.2683, Test: 0.2532\n",
            "Early stopping:  35.51000601535656\n",
            "Epoch: 018, Loss: 79.4654, Train: 0.2938, Test: 0.2986\n",
            "Early stopping:  45.88094412108783\n",
            "Epoch: 019, Loss: 53.8249, Train: 0.1560, Test: 0.1569\n",
            "Early stopping:  47.29968206202962\n",
            "Epoch: 020, Loss: 49.2524, Train: 0.1946, Test: 0.1922\n",
            "Early stopping:  43.32137843859578\n",
            "Epoch: 021, Loss: 26.1927, Train: 0.1412, Test: 0.1489\n",
            "Early stopping:  35.96109512488937\n",
            "Epoch: 022, Loss: 18.4400, Train: 0.1259, Test: 0.1318\n",
            "Early stopping:  24.19723713588572\n",
            "Epoch: 023, Loss: 18.4232, Train: 0.1407, Test: 0.1406\n",
            "Early stopping:  17.090748828560564\n",
            "Epoch: 024, Loss: 11.5511, Train: 0.1792, Test: 0.1730\n",
            "Early stopping:  14.633040580772422\n",
            "Epoch: 025, Loss: 6.3834, Train: 0.1934, Test: 0.1871\n",
            "Early stopping:  7.5463805412057425\n",
            "Epoch: 026, Loss: 4.2800, Train: 0.2104, Test: 0.1914\n",
            "Early stopping:  6.593688336794461\n",
            "Epoch: 027, Loss: 3.3540, Train: 0.2172, Test: 0.1980\n",
            "Early stopping:  6.247073147811263\n",
            "Epoch: 028, Loss: 2.9471, Train: 0.2138, Test: 0.1979\n",
            "Early stopping:  3.5284715387478847\n",
            "Epoch: 029, Loss: 2.6608, Train: 0.2042, Test: 0.1951\n",
            "Early stopping:  1.5041887256515447\n",
            "Epoch: 030, Loss: 2.4825, Train: 0.1974, Test: 0.1792\n",
            "Early stopping:  0.7149008100799861\n",
            "Epoch: 031, Loss: 2.4071, Train: 0.1656, Test: 0.1612\n",
            "Early stopping:  0.38687525663926753\n",
            "Epoch: 032, Loss: 2.3831, Train: 0.1475, Test: 0.1435\n",
            "Early stopping:  0.23419126097500748\n",
            "Epoch: 033, Loss: 2.3883, Train: 0.1339, Test: 0.1331\n",
            "Early stopping:  0.11681944085141699\n",
            "Epoch: 034, Loss: 2.3789, Train: 0.1237, Test: 0.1260\n",
            "Early stopping:  0.04303336323752334\n",
            "Epoch: 035, Loss: 2.3586, Train: 0.1265, Test: 0.1287\n",
            "Early stopping:  0.01748909035163202\n",
            "Epoch: 036, Loss: 2.3350, Train: 0.1361, Test: 0.1361\n",
            "Early stopping:  0.02201270382787024\n",
            "Epoch: 037, Loss: 2.3113, Train: 0.1424, Test: 0.1440\n",
            "Early stopping:  0.03163981775817178\n",
            "Epoch: 038, Loss: 2.2905, Train: 0.1548, Test: 0.1525\n",
            "Early stopping:  0.035446045552968174\n",
            "Epoch: 039, Loss: 2.2733, Train: 0.1645, Test: 0.1646\n",
            "Early stopping:  0.03405350420129903\n",
            "Epoch: 040, Loss: 2.2572, Train: 0.1843, Test: 0.1854\n",
            "Early stopping:  0.03070301591302366\n",
            "Epoch: 041, Loss: 2.2397, Train: 0.2133, Test: 0.2158\n",
            "Early stopping:  0.02793504049512609\n",
            "Epoch: 042, Loss: 2.2190, Train: 0.2439, Test: 0.2470\n",
            "Early stopping:  0.02796362661392657\n",
            "Epoch: 043, Loss: 2.1940, Train: 0.2728, Test: 0.2686\n",
            "Early stopping:  0.0312534541704758\n",
            "Epoch: 044, Loss: 2.1666, Train: 0.2950, Test: 0.2864\n",
            "Early stopping:  0.03600130540197592\n",
            "Epoch: 045, Loss: 2.1383, Train: 0.3086, Test: 0.2998\n",
            "Early stopping:  0.040425233675075584\n",
            "Epoch: 046, Loss: 2.1119, Train: 0.3279, Test: 0.3123\n",
            "Early stopping:  0.04269448936591315\n",
            "Epoch: 047, Loss: 2.0944, Train: 0.3449, Test: 0.3244\n",
            "Early stopping:  0.040283607891843855\n",
            "Epoch: 048, Loss: 2.0755, Train: 0.3545, Test: 0.3361\n",
            "Early stopping:  0.03596835533400142\n",
            "Epoch: 049, Loss: 2.0545, Train: 0.3715, Test: 0.3482\n",
            "Early stopping:  0.03231890894820613\n",
            "Epoch: 050, Loss: 2.0421, Train: 0.3880, Test: 0.3625\n",
            "Early stopping:  0.028442835387902408\n",
            "Epoch: 051, Loss: 2.0345, Train: 0.3931, Test: 0.3733\n",
            "Early stopping:  0.024636612218194794\n",
            "Epoch: 052, Loss: 2.0253, Train: 0.3988, Test: 0.3813\n",
            "Early stopping:  0.019460461396069852\n",
            "Epoch: 053, Loss: 2.0130, Train: 0.4078, Test: 0.3870\n",
            "Early stopping:  0.015815376941497976\n",
            "Epoch: 054, Loss: 1.9969, Train: 0.4124, Test: 0.3920\n",
            "Early stopping:  0.01788658260120857\n",
            "Epoch: 055, Loss: 1.9785, Train: 0.4135, Test: 0.3978\n",
            "Early stopping:  0.022401063299497974\n",
            "Epoch: 056, Loss: 1.9612, Train: 0.4231, Test: 0.4006\n",
            "Early stopping:  0.0258306742529366\n",
            "Epoch: 057, Loss: 1.9455, Train: 0.4226, Test: 0.4002\n",
            "Early stopping:  0.02702123224010139\n",
            "Epoch: 058, Loss: 1.9309, Train: 0.4220, Test: 0.4009\n",
            "Early stopping:  0.026107599222507327\n",
            "Epoch: 059, Loss: 1.9154, Train: 0.4209, Test: 0.4006\n",
            "Early stopping:  0.024742262791167983\n",
            "Epoch: 060, Loss: 1.8987, Train: 0.4260, Test: 0.4029\n",
            "Early stopping:  0.024519030610816627\n",
            "Epoch: 061, Loss: 1.8816, Train: 0.4294, Test: 0.4045\n",
            "Early stopping:  0.025315360765032872\n",
            "Epoch: 062, Loss: 1.8667, Train: 0.4339, Test: 0.4076\n",
            "Early stopping:  0.025651457687992586\n",
            "Epoch: 063, Loss: 1.8544, Train: 0.4339, Test: 0.4059\n",
            "Early stopping:  0.024385924450575573\n",
            "Epoch: 064, Loss: 1.8442, Train: 0.4362, Test: 0.4071\n",
            "Early stopping:  0.021656635955340554\n",
            "Epoch: 065, Loss: 1.8328, Train: 0.4385, Test: 0.4083\n",
            "Early stopping:  0.019048859795011513\n",
            "Epoch: 066, Loss: 1.8197, Train: 0.4396, Test: 0.4121\n",
            "Early stopping:  0.018296162779773716\n",
            "Epoch: 067, Loss: 1.8055, Train: 0.4447, Test: 0.4134\n",
            "Early stopping:  0.01937271214798051\n",
            "Epoch: 068, Loss: 1.7930, Train: 0.4458, Test: 0.4132\n",
            "Early stopping:  0.02051582049867047\n",
            "Epoch: 069, Loss: 1.7790, Train: 0.4498, Test: 0.4145\n",
            "Early stopping:  0.021239370676950694\n",
            "Epoch: 070, Loss: 1.7656, Train: 0.4509, Test: 0.4158\n",
            "Early stopping:  0.021291138591920753\n",
            "Epoch: 071, Loss: 1.7511, Train: 0.4521, Test: 0.4191\n",
            "Early stopping:  0.021543676684897907\n",
            "Epoch: 072, Loss: 1.7379, Train: 0.4560, Test: 0.4217\n",
            "Early stopping:  0.021851044882624213\n",
            "Epoch: 073, Loss: 1.7244, Train: 0.4549, Test: 0.4254\n",
            "Early stopping:  0.021657022868806424\n",
            "Epoch: 074, Loss: 1.7108, Train: 0.4555, Test: 0.4286\n",
            "Early stopping:  0.021566893259495738\n",
            "Epoch: 075, Loss: 1.6974, Train: 0.4572, Test: 0.4312\n",
            "Early stopping:  0.021258214735924763\n",
            "Epoch: 076, Loss: 1.6840, Train: 0.4628, Test: 0.4334\n",
            "Early stopping:  0.02130877862554104\n",
            "Epoch: 077, Loss: 1.6702, Train: 0.4759, Test: 0.4405\n",
            "Early stopping:  0.021395068270422077\n",
            "Epoch: 078, Loss: 1.6555, Train: 0.4770, Test: 0.4444\n",
            "Early stopping:  0.021784389370040513\n",
            "Epoch: 079, Loss: 1.6417, Train: 0.4787, Test: 0.4474\n",
            "Early stopping:  0.022111015695000034\n",
            "Epoch: 080, Loss: 1.6296, Train: 0.4804, Test: 0.4474\n",
            "Early stopping:  0.021714387874610788\n",
            "Epoch: 081, Loss: 1.6183, Train: 0.4861, Test: 0.4501\n",
            "Early stopping:  0.020547762151084843\n",
            "Epoch: 082, Loss: 1.6070, Train: 0.4912, Test: 0.4508\n",
            "Early stopping:  0.019059669293154405\n",
            "Epoch: 083, Loss: 1.5955, Train: 0.4918, Test: 0.4522\n",
            "Early stopping:  0.018170065579285733\n",
            "Epoch: 084, Loss: 1.5845, Train: 0.4940, Test: 0.4527\n",
            "Early stopping:  0.017857802398203584\n",
            "Epoch: 085, Loss: 1.5739, Train: 0.4963, Test: 0.4570\n",
            "Early stopping:  0.01759462572930747\n",
            "Epoch: 086, Loss: 1.5633, Train: 0.4969, Test: 0.4595\n",
            "Early stopping:  0.017239656516222276\n",
            "Epoch: 087, Loss: 1.5521, Train: 0.5060, Test: 0.4623\n",
            "Early stopping:  0.0170618837963725\n",
            "Epoch: 088, Loss: 1.5405, Train: 0.5116, Test: 0.4646\n",
            "Early stopping:  0.017346805674435226\n",
            "Epoch: 089, Loss: 1.5292, Train: 0.5145, Test: 0.4673\n",
            "Early stopping:  0.017765205132276578\n",
            "Epoch: 090, Loss: 1.5180, Train: 0.5179, Test: 0.4691\n",
            "Early stopping:  0.01797294742662495\n",
            "Epoch: 091, Loss: 1.5073, Train: 0.5207, Test: 0.4697\n",
            "Early stopping:  0.017736660834060565\n",
            "Epoch: 092, Loss: 1.4967, Train: 0.5218, Test: 0.4725\n",
            "Early stopping:  0.017294311523338896\n",
            "Epoch: 093, Loss: 1.4857, Train: 0.5258, Test: 0.4752\n",
            "Early stopping:  0.01709986237264078\n",
            "Epoch: 094, Loss: 1.4747, Train: 0.5298, Test: 0.4768\n",
            "Early stopping:  0.01710259428651865\n",
            "Epoch: 095, Loss: 1.4640, Train: 0.5286, Test: 0.4780\n",
            "Early stopping:  0.017173156031239367\n",
            "Epoch: 096, Loss: 1.4532, Train: 0.5309, Test: 0.4789\n",
            "Early stopping:  0.01721073326151318\n",
            "Epoch: 097, Loss: 1.4424, Train: 0.5303, Test: 0.4799\n",
            "Early stopping:  0.017110079742071977\n",
            "Epoch: 098, Loss: 1.4319, Train: 0.5332, Test: 0.4792\n",
            "Early stopping:  0.01695630249937105\n",
            "Epoch: 099, Loss: 1.4212, Train: 0.5337, Test: 0.4790\n",
            "Early stopping:  0.01688511655661064\n",
            "Epoch: 100, Loss: 1.4099, Train: 0.5423, Test: 0.4790\n",
            "Early stopping:  0.01701853888500108\n",
            "Epoch: 101, Loss: 1.4004, Train: 0.5417, Test: 0.4804\n",
            "Early stopping:  0.016758570446509517\n",
            "Epoch: 102, Loss: 1.3912, Train: 0.5457, Test: 0.4786\n",
            "Early stopping:  0.016192890349990552\n",
            "Epoch: 103, Loss: 1.3805, Train: 0.5479, Test: 0.4793\n",
            "Early stopping:  0.015854475668134638\n",
            "Epoch: 104, Loss: 1.3698, Train: 0.5491, Test: 0.4814\n",
            "Early stopping:  0.015836755009360774\n",
            "Epoch: 105, Loss: 1.3611, Train: 0.5525, Test: 0.4841\n",
            "Early stopping:  0.015805359360777056\n",
            "Epoch: 106, Loss: 1.3524, Train: 0.5570, Test: 0.4865\n",
            "Early stopping:  0.015336457471443344\n",
            "Epoch: 107, Loss: 1.3397, Train: 0.5604, Test: 0.4880\n",
            "Early stopping:  0.01568295232112927\n",
            "Epoch: 108, Loss: 1.3306, Train: 0.5627, Test: 0.4884\n",
            "Early stopping:  0.015829367187074905\n",
            "Epoch: 109, Loss: 1.3227, Train: 0.5672, Test: 0.4887\n",
            "Early stopping:  0.01565051604730584\n",
            "Epoch: 110, Loss: 1.3114, Train: 0.5683, Test: 0.4904\n",
            "Early stopping:  0.01572757700009437\n",
            "Epoch: 111, Loss: 1.3043, Train: 0.5701, Test: 0.4912\n",
            "Early stopping:  0.014259274478922366\n",
            "Epoch: 112, Loss: 1.2958, Train: 0.5723, Test: 0.4914\n",
            "Early stopping:  0.013952484865421385\n",
            "Epoch: 113, Loss: 1.2854, Train: 0.5752, Test: 0.4943\n",
            "Early stopping:  0.014307455481959644\n",
            "Epoch: 114, Loss: 1.2777, Train: 0.5808, Test: 0.4948\n",
            "Early stopping:  0.013664781701458345\n",
            "Epoch: 115, Loss: 1.2680, Train: 0.5786, Test: 0.4925\n",
            "Early stopping:  0.01435486463137633\n",
            "Epoch: 116, Loss: 1.2616, Train: 0.5837, Test: 0.4950\n",
            "Early stopping:  0.013598986681652415\n",
            "Epoch: 117, Loss: 1.2523, Train: 0.5888, Test: 0.4946\n",
            "Early stopping:  0.01302273987104604\n",
            "Epoch: 118, Loss: 1.2447, Train: 0.5899, Test: 0.4938\n",
            "Early stopping:  0.012917913422968382\n",
            "Epoch: 119, Loss: 1.2366, Train: 0.5927, Test: 0.4942\n",
            "Early stopping:  0.012613584126481614\n",
            "Epoch: 120, Loss: 1.2289, Train: 0.5973, Test: 0.4966\n",
            "Early stopping:  0.012825217540020278\n",
            "Epoch: 121, Loss: 1.2206, Train: 0.6001, Test: 0.4962\n",
            "Early stopping:  0.012536695769796423\n",
            "Epoch: 122, Loss: 1.2122, Train: 0.6012, Test: 0.4945\n",
            "Early stopping:  0.012812582197803592\n",
            "Epoch: 123, Loss: 1.2054, Train: 0.6018, Test: 0.4949\n",
            "Early stopping:  0.012510258034040869\n",
            "Epoch: 124, Loss: 1.1986, Train: 0.6081, Test: 0.4973\n",
            "Early stopping:  0.012022363440968293\n",
            "Epoch: 125, Loss: 1.1910, Train: 0.6086, Test: 0.4942\n",
            "Early stopping:  0.01152863436589795\n",
            "Epoch: 126, Loss: 1.1823, Train: 0.6098, Test: 0.4939\n",
            "Early stopping:  0.011739815802321392\n",
            "Epoch: 127, Loss: 1.1771, Train: 0.6166, Test: 0.4956\n",
            "Early stopping:  0.0115380086332487\n",
            "Epoch: 128, Loss: 1.1704, Train: 0.6126, Test: 0.4976\n",
            "Early stopping:  0.011136049660631439\n",
            "Epoch: 129, Loss: 1.1663, Train: 0.6171, Test: 0.4950\n",
            "Early stopping:  0.009759940150002321\n",
            "PREDICTIONS -> tensor([3, 6, 1,  ..., 4, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.47      0.49      0.48       758\n",
            "         capital_goods       0.27      0.27      0.27       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.36      0.44      0.40       793\n",
            " consumer_non-cyclical       0.63      0.38      0.47       446\n",
            "                energy       0.54      0.49      0.51       283\n",
            "             financial       0.59      0.54      0.56       767\n",
            "            healthcare       0.67      0.39      0.49       318\n",
            "              services       0.53      0.74      0.62      2076\n",
            "            technology       0.41      0.09      0.15       396\n",
            "        transportation       0.61      0.53      0.56       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.50      7054\n",
            "             macro avg       0.42      0.36      0.38      7054\n",
            "          weighted avg       0.48      0.50      0.47      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 62.7986, Train: 0.2303, Test: 0.2413\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 161.7175, Train: 0.2938, Test: 0.2947\n",
            "Early stopping:  69.94626511048492\n",
            "Epoch: 003, Loss: 337.2598, Train: 0.0113, Test: 0.0118\n",
            "Early stopping:  139.0018049741378\n",
            "Epoch: 004, Loss: 594.2366, Train: 0.1202, Test: 0.1259\n",
            "Early stopping:  232.99948931733732\n",
            "Epoch: 005, Loss: 485.2892, Train: 0.1254, Test: 0.1225\n",
            "Early stopping:  220.0504840643963\n",
            "Epoch: 006, Loss: 426.2434, Train: 0.0630, Test: 0.0604\n",
            "Early stopping:  163.09596271689153\n",
            "Epoch: 007, Loss: 378.4779, Train: 0.0732, Test: 0.0716\n",
            "Early stopping:  100.34600859501246\n",
            "Epoch: 008, Loss: 290.9341, Train: 0.2734, Test: 0.2716\n",
            "Early stopping:  113.92070377179814\n",
            "Epoch: 009, Loss: 194.2830, Train: 0.2751, Test: 0.2641\n",
            "Early stopping:  114.60421845416448\n",
            "Epoch: 010, Loss: 165.4197, Train: 0.1537, Test: 0.1520\n",
            "Early stopping:  112.99086421008762\n",
            "Epoch: 011, Loss: 139.7855, Train: 0.1730, Test: 0.1667\n",
            "Early stopping:  99.08603457202418\n",
            "Epoch: 012, Loss: 93.8090, Train: 0.1980, Test: 0.1965\n",
            "Early stopping:  73.68736697189861\n",
            "Epoch: 013, Loss: 51.5124, Train: 0.1764, Test: 0.1754\n",
            "Early stopping:  56.89314496374585\n",
            "Epoch: 014, Loss: 21.9338, Train: 0.0936, Test: 0.0931\n",
            "Early stopping:  59.570636064064665\n",
            "Epoch: 015, Loss: 8.1304, Train: 0.1378, Test: 0.1368\n",
            "Early stopping:  54.02377948795846\n",
            "Epoch: 016, Loss: 4.4465, Train: 0.2059, Test: 0.1997\n",
            "Early stopping:  37.25824248246164\n",
            "Epoch: 017, Loss: 3.4986, Train: 0.2677, Test: 0.2570\n",
            "Early stopping:  20.186043800686104\n",
            "Epoch: 018, Loss: 3.0441, Train: 0.2933, Test: 0.2909\n",
            "Early stopping:  7.927768761234997\n",
            "Epoch: 019, Loss: 2.8778, Train: 0.2825, Test: 0.2851\n",
            "Early stopping:  2.1729033404913793\n",
            "Epoch: 020, Loss: 2.7658, Train: 0.3074, Test: 0.2984\n",
            "Early stopping:  0.68546146430996\n",
            "Epoch: 021, Loss: 2.5565, Train: 0.3381, Test: 0.3347\n",
            "Early stopping:  0.3548899674933754\n",
            "Epoch: 022, Loss: 2.3357, Train: 0.3812, Test: 0.3657\n",
            "Early stopping:  0.2767585543730755\n",
            "Epoch: 023, Loss: 2.1956, Train: 0.3664, Test: 0.3475\n",
            "Early stopping:  0.2852730300666173\n",
            "Epoch: 024, Loss: 2.1440, Train: 0.3528, Test: 0.3373\n",
            "Early stopping:  0.2596985817603961\n",
            "Epoch: 025, Loss: 2.1437, Train: 0.3460, Test: 0.3320\n",
            "Early stopping:  0.1758191922715613\n",
            "Epoch: 026, Loss: 2.1532, Train: 0.3364, Test: 0.3255\n",
            "Early stopping:  0.081823109361678\n",
            "Epoch: 027, Loss: 2.1694, Train: 0.3341, Test: 0.3242\n",
            "Early stopping:  0.021917351192576297\n",
            "Epoch: 028, Loss: 2.1752, Train: 0.3296, Test: 0.3279\n",
            "Early stopping:  0.014572289706259938\n",
            "Epoch: 029, Loss: 2.1729, Train: 0.3307, Test: 0.3300\n",
            "Early stopping:  0.01376952079788167\n",
            "Epoch: 030, Loss: 2.1692, Train: 0.3301, Test: 0.3306\n",
            "Early stopping:  0.008656247863773538\n",
            "PREDICTIONS -> tensor([ 8,  8,  8,  ..., 11,  8,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.14      0.23       758\n",
            "         capital_goods       0.07      0.01      0.01       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.29      0.06      0.09       793\n",
            " consumer_non-cyclical       0.00      0.00      0.00       446\n",
            "                energy       0.66      0.11      0.19       283\n",
            "             financial       0.00      0.00      0.00       767\n",
            "            healthcare       0.22      0.16      0.19       318\n",
            "              services       0.32      0.96      0.48      2076\n",
            "            technology       0.00      0.00      0.00       396\n",
            "        transportation       0.00      0.00      0.00       404\n",
            "             utilities       0.50      0.45      0.47       225\n",
            "\n",
            "              accuracy                           0.33      7054\n",
            "             macro avg       0.22      0.16      0.14      7054\n",
            "          weighted avg       0.25      0.33      0.21      7054\n",
            "\n",
            "time: 2min 1s (started: 2024-10-16 20:53:49 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFtYdJpWa09V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd198a39-cc03-4c2b-852c-46aa3ef11949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 436 ms (started: 2024-10-16 20:55:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkaPhP3p8NBt"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLzI_TAc8i6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ddda6e8-2d15-4f54-f422-1550b0863483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4706, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2602, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14879472822067816\n",
            "Epoch: 003, Loss: 2.1703, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15414674724688404\n",
            "Epoch: 004, Loss: 2.1238, Train: 0.3006, Test: 0.2994\n",
            "Early stopping:  0.1537369243800661\n",
            "Epoch: 005, Loss: 2.0568, Train: 0.3579, Test: 0.3459\n",
            "Early stopping:  0.16023597728178407\n",
            "Epoch: 006, Loss: 2.0030, Train: 0.3897, Test: 0.3740\n",
            "Early stopping:  0.09979407067052186\n",
            "Epoch: 007, Loss: 1.9466, Train: 0.4056, Test: 0.3857\n",
            "Early stopping:  0.08991586113184191\n",
            "Epoch: 008, Loss: 1.8789, Train: 0.4078, Test: 0.3889\n",
            "Early stopping:  0.09492743508595801\n",
            "Epoch: 009, Loss: 1.8109, Train: 0.4180, Test: 0.3981\n",
            "Early stopping:  0.0975186068299187\n",
            "Epoch: 010, Loss: 1.7480, Train: 0.4413, Test: 0.4256\n",
            "Early stopping:  0.10213186894071245\n",
            "Epoch: 011, Loss: 1.6834, Train: 0.4651, Test: 0.4426\n",
            "Early stopping:  0.103957852684818\n",
            "Epoch: 012, Loss: 1.6180, Train: 0.4991, Test: 0.4654\n",
            "Early stopping:  0.10269645890803908\n",
            "Epoch: 013, Loss: 1.5560, Train: 0.5247, Test: 0.4827\n",
            "Early stopping:  0.10117019248902426\n",
            "Epoch: 014, Loss: 1.4957, Train: 0.5389, Test: 0.5021\n",
            "Early stopping:  0.09992516629491652\n",
            "Epoch: 015, Loss: 1.4356, Train: 0.5632, Test: 0.5190\n",
            "Early stopping:  0.09770461195712626\n",
            "Epoch: 016, Loss: 1.3768, Train: 0.5712, Test: 0.5315\n",
            "Early stopping:  0.09532772199763667\n",
            "Epoch: 017, Loss: 1.3223, Train: 0.5876, Test: 0.5369\n",
            "Early stopping:  0.09273351211901826\n",
            "Epoch: 018, Loss: 1.2729, Train: 0.6069, Test: 0.5525\n",
            "Early stopping:  0.08843322022046414\n",
            "Epoch: 019, Loss: 1.2245, Train: 0.6262, Test: 0.5634\n",
            "Early stopping:  0.08324382455281579\n",
            "Epoch: 020, Loss: 1.1788, Train: 0.6330, Test: 0.5747\n",
            "Early stopping:  0.07810930232637914\n",
            "Epoch: 021, Loss: 1.1376, Train: 0.6478, Test: 0.5832\n",
            "Early stopping:  0.07336500579023376\n",
            "Epoch: 022, Loss: 1.0980, Train: 0.6517, Test: 0.5878\n",
            "Early stopping:  0.06914604023157024\n",
            "Epoch: 023, Loss: 1.0595, Train: 0.6597, Test: 0.5943\n",
            "Early stopping:  0.06499573252986623\n",
            "Epoch: 024, Loss: 1.0221, Train: 0.6739, Test: 0.6016\n",
            "Early stopping:  0.06190179109009542\n",
            "Epoch: 025, Loss: 0.9861, Train: 0.6852, Test: 0.6063\n",
            "Early stopping:  0.05990017880957407\n",
            "Epoch: 026, Loss: 0.9514, Train: 0.6926, Test: 0.6084\n",
            "Early stopping:  0.057965205802817374\n",
            "Epoch: 027, Loss: 0.9173, Train: 0.7045, Test: 0.6120\n",
            "Early stopping:  0.05615059881504838\n",
            "Epoch: 028, Loss: 0.8846, Train: 0.7215, Test: 0.6165\n",
            "Early stopping:  0.05437281145218582\n",
            "Epoch: 029, Loss: 0.8521, Train: 0.7396, Test: 0.6232\n",
            "Early stopping:  0.05295952607937045\n",
            "Epoch: 030, Loss: 0.8200, Train: 0.7567, Test: 0.6290\n",
            "Early stopping:  0.051875956075148305\n",
            "Epoch: 031, Loss: 0.7901, Train: 0.7669, Test: 0.6333\n",
            "Early stopping:  0.050458428798119195\n",
            "Epoch: 032, Loss: 0.7606, Train: 0.7754, Test: 0.6341\n",
            "Early stopping:  0.04902206406502155\n",
            "Epoch: 033, Loss: 0.7315, Train: 0.7856, Test: 0.6385\n",
            "Early stopping:  0.04752655436493561\n",
            "Epoch: 034, Loss: 0.7034, Train: 0.7981, Test: 0.6418\n",
            "Early stopping:  0.046151184846544395\n",
            "Epoch: 035, Loss: 0.6765, Train: 0.8094, Test: 0.6439\n",
            "Early stopping:  0.04495945284756397\n",
            "Epoch: 036, Loss: 0.6498, Train: 0.8151, Test: 0.6463\n",
            "Early stopping:  0.04374312050326888\n",
            "Epoch: 037, Loss: 0.6238, Train: 0.8270, Test: 0.6484\n",
            "Early stopping:  0.04254196455729707\n",
            "Epoch: 038, Loss: 0.5987, Train: 0.8395, Test: 0.6540\n",
            "Early stopping:  0.041443900520515685\n",
            "Epoch: 039, Loss: 0.5738, Train: 0.8508, Test: 0.6549\n",
            "Early stopping:  0.0405825569847703\n",
            "Epoch: 040, Loss: 0.5494, Train: 0.8599, Test: 0.6566\n",
            "Early stopping:  0.03965225549911429\n",
            "Epoch: 041, Loss: 0.5257, Train: 0.8718, Test: 0.6610\n",
            "Early stopping:  0.03881443656907357\n",
            "Epoch: 042, Loss: 0.5024, Train: 0.8786, Test: 0.6613\n",
            "Early stopping:  0.03804676884418546\n",
            "Epoch: 043, Loss: 0.4796, Train: 0.8866, Test: 0.6626\n",
            "Early stopping:  0.03720779373382997\n",
            "Epoch: 044, Loss: 0.4571, Train: 0.8911, Test: 0.6656\n",
            "Early stopping:  0.03648895323905723\n",
            "Epoch: 045, Loss: 0.4352, Train: 0.9030, Test: 0.6661\n",
            "Early stopping:  0.03579183503814003\n",
            "Epoch: 046, Loss: 0.4142, Train: 0.9013, Test: 0.6664\n",
            "Early stopping:  0.03493439141525412\n",
            "Epoch: 047, Loss: 0.3944, Train: 0.9206, Test: 0.6635\n",
            "Early stopping:  0.03373262971587378\n",
            "Epoch: 048, Loss: 0.3769, Train: 0.9149, Test: 0.6650\n",
            "Early stopping:  0.03182751195489453\n",
            "Epoch: 049, Loss: 0.3619, Train: 0.9319, Test: 0.6626\n",
            "Early stopping:  0.029142691055978606\n",
            "Epoch: 050, Loss: 0.3449, Train: 0.9387, Test: 0.6688\n",
            "Early stopping:  0.02708910074192255\n",
            "Epoch: 051, Loss: 0.3230, Train: 0.9433, Test: 0.6676\n",
            "Early stopping:  0.027699536707267457\n",
            "Epoch: 052, Loss: 0.3064, Train: 0.9552, Test: 0.6653\n",
            "Early stopping:  0.028498941131891293\n",
            "Epoch: 053, Loss: 0.2954, Train: 0.9524, Test: 0.6714\n",
            "Early stopping:  0.027259315930347163\n",
            "Epoch: 054, Loss: 0.2802, Train: 0.9660, Test: 0.6695\n",
            "Early stopping:  0.0249950817193714\n",
            "Epoch: 055, Loss: 0.2633, Train: 0.9699, Test: 0.6688\n",
            "Early stopping:  0.02306820601692697\n",
            "Epoch: 056, Loss: 0.2519, Train: 0.9705, Test: 0.6715\n",
            "Early stopping:  0.02236459077152181\n",
            "Epoch: 057, Loss: 0.2419, Train: 0.9784, Test: 0.6691\n",
            "Early stopping:  0.02151589114992191\n",
            "Epoch: 058, Loss: 0.2284, Train: 0.9807, Test: 0.6697\n",
            "Early stopping:  0.01986359459409585\n",
            "Epoch: 059, Loss: 0.2161, Train: 0.9796, Test: 0.6710\n",
            "Early stopping:  0.01866679834066514\n",
            "Epoch: 060, Loss: 0.2077, Train: 0.9875, Test: 0.6681\n",
            "Early stopping:  0.018103794534455892\n",
            "Epoch: 061, Loss: 0.1990, Train: 0.9875, Test: 0.6686\n",
            "Early stopping:  0.01695700897811026\n",
            "Epoch: 062, Loss: 0.1887, Train: 0.9909, Test: 0.6673\n",
            "Early stopping:  0.015294763403044137\n",
            "Epoch: 063, Loss: 0.1800, Train: 0.9921, Test: 0.6670\n",
            "Early stopping:  0.01442293683217648\n",
            "Epoch: 064, Loss: 0.1739, Train: 0.9909, Test: 0.6680\n",
            "Early stopping:  0.013715289785733975\n",
            "Epoch: 065, Loss: 0.1677, Train: 0.9938, Test: 0.6660\n",
            "Early stopping:  0.012299225910086715\n",
            "Epoch: 066, Loss: 0.1601, Train: 0.9943, Test: 0.6669\n",
            "Early stopping:  0.010987345342343918\n",
            "Epoch: 067, Loss: 0.1537, Train: 0.9943, Test: 0.6676\n",
            "Early stopping:  0.010516135910488653\n",
            "Epoch: 068, Loss: 0.1496, Train: 0.9943, Test: 0.6625\n",
            "Early stopping:  0.009955756007334633\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.61      0.71      0.66       758\n",
            "         capital_goods       0.54      0.55      0.54       508\n",
            "conglomerates_industry       0.85      0.21      0.34        80\n",
            "     consumer_cyclical       0.60      0.64      0.62       793\n",
            " consumer_non-cyclical       0.72      0.57      0.64       446\n",
            "                energy       0.78      0.71      0.75       283\n",
            "             financial       0.72      0.66      0.69       767\n",
            "            healthcare       0.76      0.77      0.77       318\n",
            "              services       0.67      0.71      0.69      2076\n",
            "            technology       0.57      0.49      0.53       396\n",
            "        transportation       0.74      0.77      0.76       404\n",
            "             utilities       0.81      0.63      0.71       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.70      0.62      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5078, Train: 0.2984, Test: 0.2970\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2820, Train: 0.2944, Test: 0.2944\n",
            "Early stopping:  0.15970384990184391\n",
            "Epoch: 003, Loss: 2.1509, Train: 0.2944, Test: 0.2944\n",
            "Early stopping:  0.1805260339373821\n",
            "Epoch: 004, Loss: 2.1319, Train: 0.3018, Test: 0.3025\n",
            "Early stopping:  0.17315047139026676\n",
            "Epoch: 005, Loss: 2.0634, Train: 0.3642, Test: 0.3564\n",
            "Early stopping:  0.1756997720883992\n",
            "Epoch: 006, Loss: 1.9979, Train: 0.4067, Test: 0.3921\n",
            "Early stopping:  0.10642001289155696\n",
            "Epoch: 007, Loss: 1.9353, Train: 0.4118, Test: 0.3975\n",
            "Early stopping:  0.09049636735922144\n",
            "Epoch: 008, Loss: 1.8752, Train: 0.4095, Test: 0.3891\n",
            "Early stopping:  0.1014459013790759\n",
            "Epoch: 009, Loss: 1.8221, Train: 0.4073, Test: 0.3928\n",
            "Early stopping:  0.0957866993482936\n",
            "Epoch: 010, Loss: 1.7684, Train: 0.4254, Test: 0.4093\n",
            "Early stopping:  0.09056214930178647\n",
            "Epoch: 011, Loss: 1.7053, Train: 0.4674, Test: 0.4334\n",
            "Early stopping:  0.08968474319592042\n",
            "Epoch: 012, Loss: 1.6382, Train: 0.4816, Test: 0.4460\n",
            "Early stopping:  0.09354972295021932\n",
            "Epoch: 013, Loss: 1.5786, Train: 0.4986, Test: 0.4616\n",
            "Early stopping:  0.09761799093805465\n",
            "Epoch: 014, Loss: 1.5249, Train: 0.5133, Test: 0.4766\n",
            "Early stopping:  0.09709494413491417\n",
            "Epoch: 015, Loss: 1.4692, Train: 0.5275, Test: 0.4923\n",
            "Early stopping:  0.0926665969745703\n",
            "Epoch: 016, Loss: 1.4127, Train: 0.5451, Test: 0.5044\n",
            "Early stopping:  0.08862107625664693\n",
            "Epoch: 017, Loss: 1.3605, Train: 0.5706, Test: 0.5187\n",
            "Early stopping:  0.08672087186438986\n",
            "Epoch: 018, Loss: 1.3117, Train: 0.5956, Test: 0.5367\n",
            "Early stopping:  0.08463980188093574\n",
            "Epoch: 019, Loss: 1.2627, Train: 0.6177, Test: 0.5583\n",
            "Early stopping:  0.08130368695390201\n",
            "Epoch: 020, Loss: 1.2156, Train: 0.6307, Test: 0.5697\n",
            "Early stopping:  0.07781122826561374\n",
            "Epoch: 021, Loss: 1.1740, Train: 0.6427, Test: 0.5785\n",
            "Early stopping:  0.07420191617412672\n",
            "Epoch: 022, Loss: 1.1337, Train: 0.6517, Test: 0.5861\n",
            "Early stopping:  0.07037110859325786\n",
            "Epoch: 023, Loss: 1.0939, Train: 0.6563, Test: 0.5910\n",
            "Early stopping:  0.06636390292819147\n",
            "Epoch: 024, Loss: 1.0564, Train: 0.6670, Test: 0.5960\n",
            "Early stopping:  0.06303147310953101\n",
            "Epoch: 025, Loss: 1.0194, Train: 0.6761, Test: 0.6011\n",
            "Early stopping:  0.06114086905484014\n",
            "Epoch: 026, Loss: 0.9861, Train: 0.6869, Test: 0.6062\n",
            "Early stopping:  0.05849170869991134\n",
            "Epoch: 027, Loss: 0.9548, Train: 0.6954, Test: 0.6097\n",
            "Early stopping:  0.05513582030987563\n",
            "Epoch: 028, Loss: 0.9222, Train: 0.7011, Test: 0.6130\n",
            "Early stopping:  0.05264564770553106\n",
            "Epoch: 029, Loss: 0.8911, Train: 0.7147, Test: 0.6182\n",
            "Early stopping:  0.05066699435934346\n",
            "Epoch: 030, Loss: 0.8616, Train: 0.7345, Test: 0.6238\n",
            "Early stopping:  0.04944114550141815\n",
            "Epoch: 031, Loss: 0.8324, Train: 0.7453, Test: 0.6269\n",
            "Early stopping:  0.04830289983973654\n",
            "Epoch: 032, Loss: 0.8042, Train: 0.7550, Test: 0.6296\n",
            "Early stopping:  0.04660577025582141\n",
            "Epoch: 033, Loss: 0.7759, Train: 0.7697, Test: 0.6313\n",
            "Early stopping:  0.045510133044157595\n",
            "Epoch: 034, Loss: 0.7494, Train: 0.7788, Test: 0.6345\n",
            "Early stopping:  0.04443215747101555\n",
            "Epoch: 035, Loss: 0.7230, Train: 0.7884, Test: 0.6399\n",
            "Early stopping:  0.04326363492935237\n",
            "Epoch: 036, Loss: 0.6974, Train: 0.8020, Test: 0.6432\n",
            "Early stopping:  0.042159980430926676\n",
            "Epoch: 037, Loss: 0.6715, Train: 0.8083, Test: 0.6426\n",
            "Early stopping:  0.04123575337459678\n",
            "Epoch: 038, Loss: 0.6468, Train: 0.8208, Test: 0.6447\n",
            "Early stopping:  0.04060701297317226\n",
            "Epoch: 039, Loss: 0.6231, Train: 0.8332, Test: 0.6501\n",
            "Early stopping:  0.03961381755849266\n",
            "Epoch: 040, Loss: 0.5996, Train: 0.8417, Test: 0.6500\n",
            "Early stopping:  0.038583761857491176\n",
            "Epoch: 041, Loss: 0.5765, Train: 0.8508, Test: 0.6508\n",
            "Early stopping:  0.03750528575984704\n",
            "Epoch: 042, Loss: 0.5542, Train: 0.8548, Test: 0.6508\n",
            "Early stopping:  0.03664433470603314\n",
            "Epoch: 043, Loss: 0.5324, Train: 0.8650, Test: 0.6548\n",
            "Early stopping:  0.03588011773621857\n",
            "Epoch: 044, Loss: 0.5105, Train: 0.8763, Test: 0.6571\n",
            "Early stopping:  0.03516539856025635\n",
            "Epoch: 045, Loss: 0.4890, Train: 0.8871, Test: 0.6586\n",
            "Early stopping:  0.034576077871949655\n",
            "Epoch: 046, Loss: 0.4681, Train: 0.8945, Test: 0.6593\n",
            "Early stopping:  0.03405308474598668\n",
            "Epoch: 047, Loss: 0.4476, Train: 0.9013, Test: 0.6627\n",
            "Early stopping:  0.033511360318372097\n",
            "Epoch: 048, Loss: 0.4274, Train: 0.9126, Test: 0.6643\n",
            "Early stopping:  0.0328231155658304\n",
            "Epoch: 049, Loss: 0.4076, Train: 0.9178, Test: 0.6656\n",
            "Early stopping:  0.03217952747174427\n",
            "Epoch: 050, Loss: 0.3888, Train: 0.9229, Test: 0.6653\n",
            "Early stopping:  0.031402994520605\n",
            "Epoch: 051, Loss: 0.3704, Train: 0.9274, Test: 0.6669\n",
            "Early stopping:  0.030526959176414868\n",
            "Epoch: 052, Loss: 0.3527, Train: 0.9365, Test: 0.6654\n",
            "Early stopping:  0.029545906884075657\n",
            "Epoch: 053, Loss: 0.3370, Train: 0.9450, Test: 0.6705\n",
            "Early stopping:  0.028085076444460406\n",
            "Epoch: 054, Loss: 0.3245, Train: 0.9450, Test: 0.6593\n",
            "Early stopping:  0.025703785539919883\n",
            "Epoch: 055, Loss: 0.3134, Train: 0.9535, Test: 0.6695\n",
            "Early stopping:  0.022592138779908576\n",
            "Epoch: 056, Loss: 0.2952, Train: 0.9609, Test: 0.6704\n",
            "Early stopping:  0.021976467903428096\n",
            "Epoch: 057, Loss: 0.2754, Train: 0.9614, Test: 0.6630\n",
            "Early stopping:  0.024274198530234423\n",
            "Epoch: 058, Loss: 0.2671, Train: 0.9643, Test: 0.6700\n",
            "Early stopping:  0.02431195582165477\n",
            "Epoch: 059, Loss: 0.2559, Train: 0.9694, Test: 0.6669\n",
            "Early stopping:  0.022914497324713765\n",
            "Epoch: 060, Loss: 0.2401, Train: 0.9716, Test: 0.6632\n",
            "Early stopping:  0.020702848652515756\n",
            "Epoch: 061, Loss: 0.2329, Train: 0.9767, Test: 0.6678\n",
            "Early stopping:  0.017857243860834048\n",
            "Epoch: 062, Loss: 0.2225, Train: 0.9790, Test: 0.6664\n",
            "Early stopping:  0.01785998063205814\n",
            "Epoch: 063, Loss: 0.2103, Train: 0.9796, Test: 0.6646\n",
            "Early stopping:  0.017310306623618155\n",
            "Epoch: 064, Loss: 0.2049, Train: 0.9836, Test: 0.6673\n",
            "Early stopping:  0.01477475729266174\n",
            "Epoch: 065, Loss: 0.1951, Train: 0.9858, Test: 0.6657\n",
            "Early stopping:  0.014837020353609497\n",
            "Epoch: 066, Loss: 0.1864, Train: 0.9864, Test: 0.6637\n",
            "Early stopping:  0.01391589116970307\n",
            "Epoch: 067, Loss: 0.1820, Train: 0.9881, Test: 0.6656\n",
            "Early stopping:  0.011972722374074709\n",
            "Epoch: 068, Loss: 0.1731, Train: 0.9887, Test: 0.6660\n",
            "Early stopping:  0.012234742796642974\n",
            "Epoch: 069, Loss: 0.1676, Train: 0.9887, Test: 0.6602\n",
            "Early stopping:  0.010842089247604781\n",
            "Epoch: 070, Loss: 0.1632, Train: 0.9875, Test: 0.6663\n",
            "Early stopping:  0.00966876791467409\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.68      0.65       758\n",
            "         capital_goods       0.54      0.54      0.54       508\n",
            "conglomerates_industry       1.00      0.33      0.49        80\n",
            "     consumer_cyclical       0.62      0.64      0.63       793\n",
            " consumer_non-cyclical       0.71      0.58      0.64       446\n",
            "                energy       0.81      0.71      0.75       283\n",
            "             financial       0.73      0.69      0.71       767\n",
            "            healthcare       0.74      0.75      0.74       318\n",
            "              services       0.66      0.74      0.70      2076\n",
            "            technology       0.53      0.43      0.48       396\n",
            "        transportation       0.79      0.73      0.76       404\n",
            "             utilities       0.80      0.64      0.71       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.71      0.62      0.65      7054\n",
            "          weighted avg       0.67      0.67      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4857, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2636, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  0.15705534194141935\n",
            "Epoch: 003, Loss: 2.1831, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  0.15671146904126462\n",
            "Epoch: 004, Loss: 2.1357, Train: 0.2972, Test: 0.2981\n",
            "Early stopping:  0.15504693789246554\n",
            "Epoch: 005, Loss: 2.0507, Train: 0.3488, Test: 0.3422\n",
            "Early stopping:  0.16549494773728338\n",
            "Epoch: 006, Loss: 1.9925, Train: 0.3885, Test: 0.3778\n",
            "Early stopping:  0.10696782095126833\n",
            "Epoch: 007, Loss: 1.9456, Train: 0.3988, Test: 0.3838\n",
            "Early stopping:  0.09823647311549151\n",
            "Epoch: 008, Loss: 1.8859, Train: 0.3959, Test: 0.3823\n",
            "Early stopping:  0.09615035642149738\n",
            "Epoch: 009, Loss: 1.8180, Train: 0.4039, Test: 0.3877\n",
            "Early stopping:  0.09061349094119377\n",
            "Epoch: 010, Loss: 1.7534, Train: 0.4317, Test: 0.4062\n",
            "Early stopping:  0.0960113670863801\n",
            "Epoch: 011, Loss: 1.6922, Train: 0.4589, Test: 0.4317\n",
            "Early stopping:  0.10112112895774857\n",
            "Epoch: 012, Loss: 1.6320, Train: 0.4844, Test: 0.4509\n",
            "Early stopping:  0.10021061974191534\n",
            "Epoch: 013, Loss: 1.5727, Train: 0.5060, Test: 0.4688\n",
            "Early stopping:  0.09675584931180883\n",
            "Epoch: 014, Loss: 1.5112, Train: 0.5286, Test: 0.4864\n",
            "Early stopping:  0.09546702083305159\n",
            "Epoch: 015, Loss: 1.4496, Train: 0.5496, Test: 0.5082\n",
            "Early stopping:  0.09579893053924861\n",
            "Epoch: 016, Loss: 1.3943, Train: 0.5712, Test: 0.5278\n",
            "Early stopping:  0.09465555068814922\n",
            "Epoch: 017, Loss: 1.3437, Train: 0.5905, Test: 0.5448\n",
            "Early stopping:  0.09098806391573226\n",
            "Epoch: 018, Loss: 1.2929, Train: 0.6064, Test: 0.5547\n",
            "Early stopping:  0.08587402405860702\n",
            "Epoch: 019, Loss: 1.2440, Train: 0.6092, Test: 0.5578\n",
            "Early stopping:  0.08109422883213951\n",
            "Epoch: 020, Loss: 1.2012, Train: 0.6177, Test: 0.5638\n",
            "Early stopping:  0.07686737029050114\n",
            "Epoch: 021, Loss: 1.1628, Train: 0.6296, Test: 0.5706\n",
            "Early stopping:  0.07180595810096121\n",
            "Epoch: 022, Loss: 1.1251, Train: 0.6495, Test: 0.5842\n",
            "Early stopping:  0.06599405396721462\n",
            "Epoch: 023, Loss: 1.0879, Train: 0.6699, Test: 0.5968\n",
            "Early stopping:  0.061422420808422105\n",
            "Epoch: 024, Loss: 1.0542, Train: 0.6841, Test: 0.6009\n",
            "Early stopping:  0.05835440943553444\n",
            "Epoch: 025, Loss: 1.0226, Train: 0.6909, Test: 0.6053\n",
            "Early stopping:  0.05559369530103193\n",
            "Epoch: 026, Loss: 0.9887, Train: 0.6971, Test: 0.6050\n",
            "Early stopping:  0.053476171051892674\n",
            "Epoch: 027, Loss: 0.9557, Train: 0.7050, Test: 0.6086\n",
            "Early stopping:  0.05215782072152864\n",
            "Epoch: 028, Loss: 0.9251, Train: 0.7153, Test: 0.6153\n",
            "Early stopping:  0.05139705943405349\n",
            "Epoch: 029, Loss: 0.8946, Train: 0.7283, Test: 0.6199\n",
            "Early stopping:  0.05052240094265692\n",
            "Epoch: 030, Loss: 0.8653, Train: 0.7396, Test: 0.6225\n",
            "Early stopping:  0.04869825313327285\n",
            "Epoch: 031, Loss: 0.8379, Train: 0.7482, Test: 0.6233\n",
            "Early stopping:  0.046724903923263304\n",
            "Epoch: 032, Loss: 0.8108, Train: 0.7584, Test: 0.6265\n",
            "Early stopping:  0.04514588173479998\n",
            "Epoch: 033, Loss: 0.7841, Train: 0.7731, Test: 0.6277\n",
            "Early stopping:  0.04357883200548192\n",
            "Epoch: 034, Loss: 0.7586, Train: 0.7811, Test: 0.6296\n",
            "Early stopping:  0.04225531141407823\n",
            "Epoch: 035, Loss: 0.7332, Train: 0.7958, Test: 0.6361\n",
            "Early stopping:  0.04137894912074313\n",
            "Epoch: 036, Loss: 0.7075, Train: 0.8043, Test: 0.6420\n",
            "Early stopping:  0.0407210426242584\n",
            "Epoch: 037, Loss: 0.6830, Train: 0.8151, Test: 0.6426\n",
            "Early stopping:  0.04004303928451542\n",
            "Epoch: 038, Loss: 0.6587, Train: 0.8191, Test: 0.6470\n",
            "Early stopping:  0.03952515369636598\n",
            "Epoch: 039, Loss: 0.6353, Train: 0.8276, Test: 0.6483\n",
            "Early stopping:  0.03868787087441819\n",
            "Epoch: 040, Loss: 0.6121, Train: 0.8406, Test: 0.6530\n",
            "Early stopping:  0.03771528875606696\n",
            "Epoch: 041, Loss: 0.5893, Train: 0.8514, Test: 0.6518\n",
            "Early stopping:  0.03702430059214766\n",
            "Epoch: 042, Loss: 0.5670, Train: 0.8599, Test: 0.6561\n",
            "Early stopping:  0.03627834781445405\n",
            "Epoch: 043, Loss: 0.5452, Train: 0.8695, Test: 0.6555\n",
            "Early stopping:  0.03562293784937521\n",
            "Epoch: 044, Loss: 0.5235, Train: 0.8775, Test: 0.6574\n",
            "Early stopping:  0.034977377245533764\n",
            "Epoch: 045, Loss: 0.5022, Train: 0.8866, Test: 0.6564\n",
            "Early stopping:  0.03439894701986219\n",
            "Epoch: 046, Loss: 0.4812, Train: 0.8956, Test: 0.6583\n",
            "Early stopping:  0.03392626314056109\n",
            "Epoch: 047, Loss: 0.4604, Train: 0.9030, Test: 0.6602\n",
            "Early stopping:  0.03350826295310754\n",
            "Epoch: 048, Loss: 0.4402, Train: 0.9126, Test: 0.6612\n",
            "Early stopping:  0.03296804183621093\n",
            "Epoch: 049, Loss: 0.4202, Train: 0.9189, Test: 0.6626\n",
            "Early stopping:  0.03242324040268155\n",
            "Epoch: 050, Loss: 0.4010, Train: 0.9268, Test: 0.6619\n",
            "Early stopping:  0.0317228804672824\n",
            "Epoch: 051, Loss: 0.3820, Train: 0.9336, Test: 0.6630\n",
            "Early stopping:  0.031006148755324602\n",
            "Epoch: 052, Loss: 0.3636, Train: 0.9404, Test: 0.6643\n",
            "Early stopping:  0.030261946120928692\n",
            "Epoch: 053, Loss: 0.3457, Train: 0.9438, Test: 0.6629\n",
            "Early stopping:  0.029466266719740477\n",
            "Epoch: 054, Loss: 0.3288, Train: 0.9524, Test: 0.6630\n",
            "Early stopping:  0.02856025930870127\n",
            "Epoch: 055, Loss: 0.3125, Train: 0.9501, Test: 0.6657\n",
            "Early stopping:  0.027467639993259824\n",
            "Epoch: 056, Loss: 0.2981, Train: 0.9654, Test: 0.6599\n",
            "Early stopping:  0.025991200241286472\n",
            "Epoch: 057, Loss: 0.2854, Train: 0.9592, Test: 0.6653\n",
            "Early stopping:  0.023983848970280285\n",
            "Epoch: 058, Loss: 0.2735, Train: 0.9716, Test: 0.6592\n",
            "Early stopping:  0.021836875822142093\n",
            "Epoch: 059, Loss: 0.2580, Train: 0.9733, Test: 0.6639\n",
            "Early stopping:  0.021158266239159427\n",
            "Epoch: 060, Loss: 0.2427, Train: 0.9762, Test: 0.6654\n",
            "Early stopping:  0.021890185998022015\n",
            "Epoch: 061, Loss: 0.2328, Train: 0.9818, Test: 0.6578\n",
            "Early stopping:  0.021550233579070807\n",
            "Epoch: 062, Loss: 0.2243, Train: 0.9801, Test: 0.6652\n",
            "Early stopping:  0.019702399725141057\n",
            "Epoch: 063, Loss: 0.2122, Train: 0.9870, Test: 0.6623\n",
            "Early stopping:  0.01744711031588256\n",
            "Epoch: 064, Loss: 0.2009, Train: 0.9921, Test: 0.6583\n",
            "Early stopping:  0.01650328706446723\n",
            "Epoch: 065, Loss: 0.1941, Train: 0.9870, Test: 0.6610\n",
            "Early stopping:  0.016015411996152352\n",
            "Epoch: 066, Loss: 0.1874, Train: 0.9938, Test: 0.6572\n",
            "Early stopping:  0.014690940615596147\n",
            "Epoch: 067, Loss: 0.1783, Train: 0.9938, Test: 0.6589\n",
            "Early stopping:  0.012913652061488683\n",
            "Epoch: 068, Loss: 0.1709, Train: 0.9932, Test: 0.6622\n",
            "Early stopping:  0.011988602744447829\n",
            "Epoch: 069, Loss: 0.1663, Train: 0.9943, Test: 0.6562\n",
            "Early stopping:  0.011470720884141585\n",
            "Epoch: 070, Loss: 0.1610, Train: 0.9949, Test: 0.6595\n",
            "Early stopping:  0.0103865214109808\n",
            "Epoch: 071, Loss: 0.1544, Train: 0.9955, Test: 0.6602\n",
            "Early stopping:  0.00915337267731342\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  1], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.70      0.68       758\n",
            "         capital_goods       0.54      0.54      0.54       508\n",
            "conglomerates_industry       1.00      0.17      0.30        80\n",
            "     consumer_cyclical       0.58      0.61      0.59       793\n",
            " consumer_non-cyclical       0.67      0.55      0.61       446\n",
            "                energy       0.80      0.69      0.74       283\n",
            "             financial       0.73      0.67      0.70       767\n",
            "            healthcare       0.75      0.76      0.76       318\n",
            "              services       0.65      0.74      0.69      2076\n",
            "            technology       0.51      0.46      0.48       396\n",
            "        transportation       0.80      0.75      0.78       404\n",
            "             utilities       0.79      0.64      0.70       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.61      0.63      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5061, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2674, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16881279538941238\n",
            "Epoch: 003, Loss: 2.1751, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17083695144278144\n",
            "Epoch: 004, Loss: 2.1326, Train: 0.3012, Test: 0.3004\n",
            "Early stopping:  0.16699968187912537\n",
            "Epoch: 005, Loss: 2.0687, Train: 0.3676, Test: 0.3550\n",
            "Early stopping:  0.1704233591019679\n",
            "Epoch: 006, Loss: 2.0175, Train: 0.3919, Test: 0.3821\n",
            "Early stopping:  0.09658477289504175\n",
            "Epoch: 007, Loss: 1.9575, Train: 0.3965, Test: 0.3821\n",
            "Early stopping:  0.08713288163006973\n",
            "Epoch: 008, Loss: 1.8894, Train: 0.3846, Test: 0.3726\n",
            "Early stopping:  0.09457531706115843\n",
            "Epoch: 009, Loss: 1.8257, Train: 0.3880, Test: 0.3731\n",
            "Early stopping:  0.09721294876089612\n",
            "Epoch: 010, Loss: 1.7687, Train: 0.4203, Test: 0.3984\n",
            "Early stopping:  0.09952997267406119\n",
            "Epoch: 011, Loss: 1.7041, Train: 0.4594, Test: 0.4312\n",
            "Early stopping:  0.09924228308719553\n",
            "Epoch: 012, Loss: 1.6344, Train: 0.4878, Test: 0.4518\n",
            "Early stopping:  0.09991126009214048\n",
            "Epoch: 013, Loss: 1.5728, Train: 0.5099, Test: 0.4681\n",
            "Early stopping:  0.10127430582108922\n",
            "Epoch: 014, Loss: 1.5160, Train: 0.5218, Test: 0.4782\n",
            "Early stopping:  0.1007471218094331\n",
            "Epoch: 015, Loss: 1.4549, Train: 0.5269, Test: 0.4850\n",
            "Early stopping:  0.09759704385234083\n",
            "Epoch: 016, Loss: 1.3961, Train: 0.5462, Test: 0.5037\n",
            "Early stopping:  0.09401937597042606\n",
            "Epoch: 017, Loss: 1.3435, Train: 0.5678, Test: 0.5279\n",
            "Early stopping:  0.09151256330922365\n",
            "Epoch: 018, Loss: 1.2912, Train: 0.6109, Test: 0.5455\n",
            "Early stopping:  0.08877719795107177\n",
            "Epoch: 019, Loss: 1.2439, Train: 0.6290, Test: 0.5617\n",
            "Early stopping:  0.08336102498607946\n",
            "Epoch: 020, Loss: 1.2004, Train: 0.6404, Test: 0.5697\n",
            "Early stopping:  0.07766825474237103\n",
            "Epoch: 021, Loss: 1.1553, Train: 0.6432, Test: 0.5743\n",
            "Early stopping:  0.07391458780763989\n",
            "Epoch: 022, Loss: 1.1143, Train: 0.6529, Test: 0.5785\n",
            "Early stopping:  0.06996766111721857\n",
            "Epoch: 023, Loss: 1.0772, Train: 0.6682, Test: 0.5862\n",
            "Early stopping:  0.0663886258691882\n",
            "Epoch: 024, Loss: 1.0388, Train: 0.6835, Test: 0.5957\n",
            "Early stopping:  0.06349920385982707\n",
            "Epoch: 025, Loss: 1.0026, Train: 0.6943, Test: 0.6019\n",
            "Early stopping:  0.06022199232962904\n",
            "Epoch: 026, Loss: 0.9669, Train: 0.7005, Test: 0.6063\n",
            "Early stopping:  0.05839897466732244\n",
            "Epoch: 027, Loss: 0.9317, Train: 0.7073, Test: 0.6086\n",
            "Early stopping:  0.05737611855596103\n",
            "Epoch: 028, Loss: 0.8981, Train: 0.7187, Test: 0.6121\n",
            "Early stopping:  0.05571910170658058\n",
            "Epoch: 029, Loss: 0.8649, Train: 0.7374, Test: 0.6187\n",
            "Early stopping:  0.054448317173042556\n",
            "Epoch: 030, Loss: 0.8323, Train: 0.7516, Test: 0.6243\n",
            "Early stopping:  0.05314750786194754\n",
            "Epoch: 031, Loss: 0.8020, Train: 0.7589, Test: 0.6256\n",
            "Early stopping:  0.051435584131129676\n",
            "Epoch: 032, Loss: 0.7721, Train: 0.7703, Test: 0.6266\n",
            "Early stopping:  0.04981342356543583\n",
            "Epoch: 033, Loss: 0.7436, Train: 0.7862, Test: 0.6284\n",
            "Early stopping:  0.047897739574955966\n",
            "Epoch: 034, Loss: 0.7159, Train: 0.7975, Test: 0.6362\n",
            "Early stopping:  0.0460538002813121\n",
            "Epoch: 035, Loss: 0.6887, Train: 0.8111, Test: 0.6389\n",
            "Early stopping:  0.044715231245371204\n",
            "Epoch: 036, Loss: 0.6628, Train: 0.8219, Test: 0.6399\n",
            "Early stopping:  0.043254092956962024\n",
            "Epoch: 037, Loss: 0.6371, Train: 0.8344, Test: 0.6412\n",
            "Early stopping:  0.04206872030007711\n",
            "Epoch: 038, Loss: 0.6120, Train: 0.8412, Test: 0.6446\n",
            "Early stopping:  0.04101645675187737\n",
            "Epoch: 039, Loss: 0.5867, Train: 0.8514, Test: 0.6469\n",
            "Early stopping:  0.04027393842391531\n",
            "Epoch: 040, Loss: 0.5625, Train: 0.8593, Test: 0.6491\n",
            "Early stopping:  0.03968293209148907\n",
            "Epoch: 041, Loss: 0.5385, Train: 0.8678, Test: 0.6506\n",
            "Early stopping:  0.03903106516890576\n",
            "Epoch: 042, Loss: 0.5150, Train: 0.8763, Test: 0.6527\n",
            "Early stopping:  0.03829304416013432\n",
            "Epoch: 043, Loss: 0.4923, Train: 0.8922, Test: 0.6541\n",
            "Early stopping:  0.03737657658685095\n",
            "Epoch: 044, Loss: 0.4699, Train: 0.8985, Test: 0.6540\n",
            "Early stopping:  0.03658492435714916\n",
            "Epoch: 045, Loss: 0.4481, Train: 0.9098, Test: 0.6540\n",
            "Early stopping:  0.03572046267888969\n",
            "Epoch: 046, Loss: 0.4269, Train: 0.9178, Test: 0.6561\n",
            "Early stopping:  0.03485091294080009\n",
            "Epoch: 047, Loss: 0.4061, Train: 0.9257, Test: 0.6581\n",
            "Early stopping:  0.034042664497807434\n",
            "Epoch: 048, Loss: 0.3861, Train: 0.9325, Test: 0.6593\n",
            "Early stopping:  0.033161398313857914\n",
            "Epoch: 049, Loss: 0.3665, Train: 0.9370, Test: 0.6592\n",
            "Early stopping:  0.03226929279399389\n",
            "Epoch: 050, Loss: 0.3479, Train: 0.9416, Test: 0.6596\n",
            "Early stopping:  0.031246492327104924\n",
            "Epoch: 051, Loss: 0.3301, Train: 0.9518, Test: 0.6593\n",
            "Early stopping:  0.03007093810841681\n",
            "Epoch: 052, Loss: 0.3130, Train: 0.9546, Test: 0.6618\n",
            "Early stopping:  0.028871712689420116\n",
            "Epoch: 053, Loss: 0.2969, Train: 0.9637, Test: 0.6616\n",
            "Early stopping:  0.027547415968686453\n",
            "Epoch: 054, Loss: 0.2821, Train: 0.9637, Test: 0.6605\n",
            "Early stopping:  0.02608994434751691\n",
            "Epoch: 055, Loss: 0.2701, Train: 0.9750, Test: 0.6562\n",
            "Early stopping:  0.02389968267581081\n",
            "Epoch: 056, Loss: 0.2591, Train: 0.9711, Test: 0.6589\n",
            "Early stopping:  0.021348515330090616\n",
            "Epoch: 057, Loss: 0.2472, Train: 0.9790, Test: 0.6595\n",
            "Early stopping:  0.019388441405676426\n",
            "Epoch: 058, Loss: 0.2308, Train: 0.9801, Test: 0.6592\n",
            "Early stopping:  0.0199128637305683\n",
            "Epoch: 059, Loss: 0.2198, Train: 0.9830, Test: 0.6586\n",
            "Early stopping:  0.020437151048270104\n",
            "Epoch: 060, Loss: 0.2138, Train: 0.9841, Test: 0.6596\n",
            "Early stopping:  0.018868838382066308\n",
            "Epoch: 061, Loss: 0.2032, Train: 0.9875, Test: 0.6588\n",
            "Early stopping:  0.016828916386025614\n",
            "Epoch: 062, Loss: 0.1921, Train: 0.9870, Test: 0.6602\n",
            "Early stopping:  0.014929692494567734\n",
            "Epoch: 063, Loss: 0.1861, Train: 0.9892, Test: 0.6589\n",
            "Early stopping:  0.014163606071949181\n",
            "Epoch: 064, Loss: 0.1804, Train: 0.9892, Test: 0.6595\n",
            "Early stopping:  0.013423852219399908\n",
            "Epoch: 065, Loss: 0.1719, Train: 0.9909, Test: 0.6591\n",
            "Early stopping:  0.01183098815005011\n",
            "Epoch: 066, Loss: 0.1650, Train: 0.9909, Test: 0.6564\n",
            "Early stopping:  0.010833245992297976\n",
            "Epoch: 067, Loss: 0.1612, Train: 0.9915, Test: 0.6595\n",
            "Early stopping:  0.010380636380196833\n",
            "Epoch: 068, Loss: 0.1569, Train: 0.9915, Test: 0.6565\n",
            "Early stopping:  0.009272803802488617\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.60      0.69      0.64       758\n",
            "         capital_goods       0.53      0.52      0.53       508\n",
            "conglomerates_industry       0.88      0.29      0.43        80\n",
            "     consumer_cyclical       0.58      0.64      0.61       793\n",
            " consumer_non-cyclical       0.68      0.59      0.63       446\n",
            "                energy       0.82      0.69      0.75       283\n",
            "             financial       0.69      0.66      0.68       767\n",
            "            healthcare       0.75      0.77      0.76       318\n",
            "              services       0.66      0.72      0.69      2076\n",
            "            technology       0.57      0.46      0.51       396\n",
            "        transportation       0.81      0.71      0.76       404\n",
            "             utilities       0.76      0.63      0.69       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.70      0.61      0.64      7054\n",
            "          weighted avg       0.66      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4901, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2748, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1522195811306734\n",
            "Epoch: 003, Loss: 2.1774, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16001378969004737\n",
            "Epoch: 004, Loss: 2.1338, Train: 0.2955, Test: 0.2952\n",
            "Early stopping:  0.1587232287181267\n",
            "Epoch: 005, Loss: 2.0798, Train: 0.3205, Test: 0.3180\n",
            "Early stopping:  0.1614141296745412\n",
            "Epoch: 006, Loss: 2.0284, Train: 0.3636, Test: 0.3509\n",
            "Early stopping:  0.0944517640431118\n",
            "Epoch: 007, Loss: 1.9699, Train: 0.3766, Test: 0.3636\n",
            "Early stopping:  0.08237556352808094\n",
            "Epoch: 008, Loss: 1.9062, Train: 0.3795, Test: 0.3710\n",
            "Early stopping:  0.08943384927178892\n",
            "Epoch: 009, Loss: 1.8448, Train: 0.3874, Test: 0.3774\n",
            "Early stopping:  0.09369737568942019\n",
            "Epoch: 010, Loss: 1.7878, Train: 0.4203, Test: 0.3958\n",
            "Early stopping:  0.09585205093498056\n",
            "Epoch: 011, Loss: 1.7280, Train: 0.4481, Test: 0.4267\n",
            "Early stopping:  0.09522211026678462\n",
            "Epoch: 012, Loss: 1.6646, Train: 0.4776, Test: 0.4410\n",
            "Early stopping:  0.09488325820286549\n",
            "Epoch: 013, Loss: 1.6066, Train: 0.4946, Test: 0.4562\n",
            "Early stopping:  0.0948145060846087\n",
            "Epoch: 014, Loss: 1.5514, Train: 0.5020, Test: 0.4640\n",
            "Early stopping:  0.09398688426386795\n",
            "Epoch: 015, Loss: 1.4918, Train: 0.5150, Test: 0.4750\n",
            "Early stopping:  0.09262348579714419\n",
            "Epoch: 016, Loss: 1.4358, Train: 0.5332, Test: 0.4892\n",
            "Early stopping:  0.09051944277686938\n",
            "Epoch: 017, Loss: 1.3835, Train: 0.5621, Test: 0.5133\n",
            "Early stopping:  0.08885406604073498\n",
            "Epoch: 018, Loss: 1.3306, Train: 0.5927, Test: 0.5305\n",
            "Early stopping:  0.0869890968169788\n",
            "Epoch: 019, Loss: 1.2817, Train: 0.6081, Test: 0.5420\n",
            "Early stopping:  0.08308815851591193\n",
            "Epoch: 020, Loss: 1.2334, Train: 0.6217, Test: 0.5523\n",
            "Early stopping:  0.08012032427053764\n",
            "Epoch: 021, Loss: 1.1853, Train: 0.6279, Test: 0.5607\n",
            "Early stopping:  0.07806834333498056\n",
            "Epoch: 022, Loss: 1.1422, Train: 0.6489, Test: 0.5727\n",
            "Early stopping:  0.07482232897040161\n",
            "Epoch: 023, Loss: 1.1003, Train: 0.6659, Test: 0.5804\n",
            "Early stopping:  0.07182773876888132\n",
            "Epoch: 024, Loss: 1.0617, Train: 0.6812, Test: 0.5892\n",
            "Early stopping:  0.06778161433562797\n",
            "Epoch: 025, Loss: 1.0235, Train: 0.6886, Test: 0.5946\n",
            "Early stopping:  0.06391162065791015\n",
            "Epoch: 026, Loss: 0.9875, Train: 0.6988, Test: 0.5999\n",
            "Early stopping:  0.06110515359234243\n",
            "Epoch: 027, Loss: 0.9529, Train: 0.7073, Test: 0.6039\n",
            "Early stopping:  0.05836034242747788\n",
            "Epoch: 028, Loss: 0.9183, Train: 0.7153, Test: 0.6086\n",
            "Early stopping:  0.05651945240273948\n",
            "Epoch: 029, Loss: 0.8849, Train: 0.7266, Test: 0.6143\n",
            "Early stopping:  0.054775775075763054\n",
            "Epoch: 030, Loss: 0.8522, Train: 0.7357, Test: 0.6194\n",
            "Early stopping:  0.053530791380568286\n",
            "Epoch: 031, Loss: 0.8209, Train: 0.7499, Test: 0.6218\n",
            "Early stopping:  0.05220591588386671\n",
            "Epoch: 032, Loss: 0.7896, Train: 0.7595, Test: 0.6270\n",
            "Early stopping:  0.05084190328159423\n",
            "Epoch: 033, Loss: 0.7593, Train: 0.7737, Test: 0.6325\n",
            "Early stopping:  0.04961528400141434\n",
            "Epoch: 034, Loss: 0.7305, Train: 0.7839, Test: 0.6351\n",
            "Early stopping:  0.048237075347758764\n",
            "Epoch: 035, Loss: 0.7022, Train: 0.7998, Test: 0.6323\n",
            "Early stopping:  0.046897841893282284\n",
            "Epoch: 036, Loss: 0.6750, Train: 0.8083, Test: 0.6361\n",
            "Early stopping:  0.045269382489840214\n",
            "Epoch: 037, Loss: 0.6482, Train: 0.8219, Test: 0.6389\n",
            "Early stopping:  0.04391587056353031\n",
            "Epoch: 038, Loss: 0.6226, Train: 0.8338, Test: 0.6420\n",
            "Early stopping:  0.042665398663118345\n",
            "Epoch: 039, Loss: 0.5966, Train: 0.8480, Test: 0.6423\n",
            "Early stopping:  0.04168992481823819\n",
            "Epoch: 040, Loss: 0.5716, Train: 0.8503, Test: 0.6459\n",
            "Early stopping:  0.04088597037972259\n",
            "Epoch: 041, Loss: 0.5467, Train: 0.8633, Test: 0.6469\n",
            "Early stopping:  0.040172437000282626\n",
            "Epoch: 042, Loss: 0.5222, Train: 0.8780, Test: 0.6486\n",
            "Early stopping:  0.039620791148780345\n",
            "Epoch: 043, Loss: 0.4989, Train: 0.8883, Test: 0.6528\n",
            "Early stopping:  0.03869756215064407\n",
            "Epoch: 044, Loss: 0.4758, Train: 0.8996, Test: 0.6534\n",
            "Early stopping:  0.03784116949244433\n",
            "Epoch: 045, Loss: 0.4534, Train: 0.9081, Test: 0.6537\n",
            "Early stopping:  0.03685907090180125\n",
            "Epoch: 046, Loss: 0.4318, Train: 0.9144, Test: 0.6534\n",
            "Early stopping:  0.035804551856347504\n",
            "Epoch: 047, Loss: 0.4106, Train: 0.9223, Test: 0.6545\n",
            "Early stopping:  0.034892356581611064\n",
            "Epoch: 048, Loss: 0.3899, Train: 0.9297, Test: 0.6549\n",
            "Early stopping:  0.03394358212886224\n",
            "Epoch: 049, Loss: 0.3702, Train: 0.9359, Test: 0.6544\n",
            "Early stopping:  0.03293402248114754\n",
            "Epoch: 050, Loss: 0.3512, Train: 0.9433, Test: 0.6545\n",
            "Early stopping:  0.031882110076323404\n",
            "Epoch: 051, Loss: 0.3330, Train: 0.9518, Test: 0.6551\n",
            "Early stopping:  0.03065288713677696\n",
            "Epoch: 052, Loss: 0.3158, Train: 0.9529, Test: 0.6561\n",
            "Early stopping:  0.029316110276719338\n",
            "Epoch: 053, Loss: 0.3000, Train: 0.9626, Test: 0.6574\n",
            "Early stopping:  0.027810876985856454\n",
            "Epoch: 054, Loss: 0.2857, Train: 0.9569, Test: 0.6548\n",
            "Early stopping:  0.025947393090190046\n",
            "Epoch: 055, Loss: 0.2740, Train: 0.9711, Test: 0.6583\n",
            "Early stopping:  0.023482683199136073\n",
            "Epoch: 056, Loss: 0.2625, Train: 0.9654, Test: 0.6575\n",
            "Early stopping:  0.021024379971466594\n",
            "Epoch: 057, Loss: 0.2476, Train: 0.9762, Test: 0.6596\n",
            "Early stopping:  0.020261282813399045\n",
            "Epoch: 058, Loss: 0.2326, Train: 0.9813, Test: 0.6582\n",
            "Early stopping:  0.021006430010324914\n",
            "Epoch: 059, Loss: 0.2235, Train: 0.9807, Test: 0.6583\n",
            "Early stopping:  0.02073934801187969\n",
            "Epoch: 060, Loss: 0.2157, Train: 0.9858, Test: 0.6598\n",
            "Early stopping:  0.01879816145169429\n",
            "Epoch: 061, Loss: 0.2042, Train: 0.9892, Test: 0.6593\n",
            "Early stopping:  0.016498413869954435\n",
            "Epoch: 062, Loss: 0.1943, Train: 0.9881, Test: 0.6598\n",
            "Early stopping:  0.015195092682734756\n",
            "Epoch: 063, Loss: 0.1885, Train: 0.9915, Test: 0.6578\n",
            "Early stopping:  0.01453283648249468\n",
            "Epoch: 064, Loss: 0.1818, Train: 0.9921, Test: 0.6596\n",
            "Early stopping:  0.013361092677414333\n",
            "Epoch: 065, Loss: 0.1737, Train: 0.9921, Test: 0.6618\n",
            "Early stopping:  0.011686927675559887\n",
            "Epoch: 066, Loss: 0.1673, Train: 0.9943, Test: 0.6583\n",
            "Early stopping:  0.010902301178248502\n",
            "Epoch: 067, Loss: 0.1627, Train: 0.9926, Test: 0.6609\n",
            "Early stopping:  0.010493986125833568\n",
            "Epoch: 068, Loss: 0.1578, Train: 0.9960, Test: 0.6619\n",
            "Early stopping:  0.009393599029121727\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.61      0.70      0.65       758\n",
            "         capital_goods       0.53      0.56      0.54       508\n",
            "conglomerates_industry       1.00      0.29      0.45        80\n",
            "     consumer_cyclical       0.64      0.62      0.63       793\n",
            " consumer_non-cyclical       0.72      0.56      0.63       446\n",
            "                energy       0.81      0.70      0.75       283\n",
            "             financial       0.68      0.68      0.68       767\n",
            "            healthcare       0.76      0.73      0.74       318\n",
            "              services       0.66      0.74      0.70      2076\n",
            "            technology       0.53      0.48      0.50       396\n",
            "        transportation       0.79      0.71      0.75       404\n",
            "             utilities       0.76      0.65      0.70       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.62      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5209, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2848, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1669537821952939\n",
            "Epoch: 003, Loss: 2.1758, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1763814326823435\n",
            "Epoch: 004, Loss: 2.1396, Train: 0.2950, Test: 0.2947\n",
            "Early stopping:  0.17185597850841114\n",
            "Epoch: 005, Loss: 2.0771, Train: 0.3284, Test: 0.3215\n",
            "Early stopping:  0.17438176938726294\n",
            "Epoch: 006, Loss: 2.0177, Train: 0.3772, Test: 0.3655\n",
            "Early stopping:  0.10144461103730798\n",
            "Epoch: 007, Loss: 1.9634, Train: 0.3965, Test: 0.3788\n",
            "Early stopping:  0.08671615657936083\n",
            "Epoch: 008, Loss: 1.9063, Train: 0.3908, Test: 0.3772\n",
            "Early stopping:  0.09177095161864053\n",
            "Epoch: 009, Loss: 1.8441, Train: 0.3936, Test: 0.3832\n",
            "Early stopping:  0.09129813188289429\n",
            "Epoch: 010, Loss: 1.7802, Train: 0.4186, Test: 0.4028\n",
            "Early stopping:  0.09402723286342872\n",
            "Epoch: 011, Loss: 1.7144, Train: 0.4526, Test: 0.4283\n",
            "Early stopping:  0.09872301269126649\n",
            "Epoch: 012, Loss: 1.6500, Train: 0.4821, Test: 0.4429\n",
            "Early stopping:  0.10154691344900528\n",
            "Epoch: 013, Loss: 1.5927, Train: 0.5026, Test: 0.4569\n",
            "Early stopping:  0.100093372749479\n",
            "Epoch: 014, Loss: 1.5359, Train: 0.5235, Test: 0.4758\n",
            "Early stopping:  0.09655254879652649\n",
            "Epoch: 015, Loss: 1.4733, Train: 0.5372, Test: 0.4936\n",
            "Early stopping:  0.0943318287627745\n",
            "Epoch: 016, Loss: 1.4128, Train: 0.5542, Test: 0.5111\n",
            "Early stopping:  0.0939435959026584\n",
            "Epoch: 017, Loss: 1.3599, Train: 0.5803, Test: 0.5315\n",
            "Early stopping:  0.0931207353417281\n",
            "Epoch: 018, Loss: 1.3102, Train: 0.5939, Test: 0.5466\n",
            "Early stopping:  0.08940555276918623\n",
            "Epoch: 019, Loss: 1.2598, Train: 0.6194, Test: 0.5611\n",
            "Early stopping:  0.08378634664797562\n",
            "Epoch: 020, Loss: 1.2090, Train: 0.6341, Test: 0.5717\n",
            "Early stopping:  0.08027072946836328\n",
            "Epoch: 021, Loss: 1.1618, Train: 0.6449, Test: 0.5826\n",
            "Early stopping:  0.07866945560664386\n",
            "Epoch: 022, Loss: 1.1198, Train: 0.6495, Test: 0.5876\n",
            "Early stopping:  0.07577352082945805\n",
            "Epoch: 023, Loss: 1.0790, Train: 0.6631, Test: 0.5909\n",
            "Early stopping:  0.07138505356364429\n",
            "Epoch: 024, Loss: 1.0381, Train: 0.6846, Test: 0.6005\n",
            "Early stopping:  0.06715063026527869\n",
            "Epoch: 025, Loss: 0.9998, Train: 0.7016, Test: 0.6077\n",
            "Early stopping:  0.064130114526013\n",
            "Epoch: 026, Loss: 0.9659, Train: 0.7090, Test: 0.6109\n",
            "Early stopping:  0.061219933719774675\n",
            "Epoch: 027, Loss: 0.9323, Train: 0.7153, Test: 0.6154\n",
            "Early stopping:  0.057868326069981386\n",
            "Epoch: 028, Loss: 0.8979, Train: 0.7221, Test: 0.6195\n",
            "Early stopping:  0.055048299180725485\n",
            "Epoch: 029, Loss: 0.8660, Train: 0.7340, Test: 0.6231\n",
            "Early stopping:  0.05308076497592466\n",
            "Epoch: 030, Loss: 0.8348, Train: 0.7453, Test: 0.6267\n",
            "Early stopping:  0.05195747725536481\n",
            "Epoch: 031, Loss: 0.8044, Train: 0.7618, Test: 0.6318\n",
            "Early stopping:  0.05043494742711769\n",
            "Epoch: 032, Loss: 0.7733, Train: 0.7742, Test: 0.6308\n",
            "Early stopping:  0.049142701019506595\n",
            "Epoch: 033, Loss: 0.7437, Train: 0.7890, Test: 0.6327\n",
            "Early stopping:  0.04839105090034483\n",
            "Epoch: 034, Loss: 0.7158, Train: 0.7992, Test: 0.6399\n",
            "Early stopping:  0.04722910675403716\n",
            "Epoch: 035, Loss: 0.6875, Train: 0.8123, Test: 0.6433\n",
            "Early stopping:  0.04607297058429601\n",
            "Epoch: 036, Loss: 0.6603, Train: 0.8208, Test: 0.6474\n",
            "Early stopping:  0.04462954092324017\n",
            "Epoch: 037, Loss: 0.6345, Train: 0.8315, Test: 0.6474\n",
            "Early stopping:  0.04329750739468181\n",
            "Epoch: 038, Loss: 0.6094, Train: 0.8395, Test: 0.6481\n",
            "Early stopping:  0.04201713385810159\n",
            "Epoch: 039, Loss: 0.5845, Train: 0.8469, Test: 0.6514\n",
            "Early stopping:  0.040614664221395626\n",
            "Epoch: 040, Loss: 0.5611, Train: 0.8559, Test: 0.6513\n",
            "Early stopping:  0.03929273932376557\n",
            "Epoch: 041, Loss: 0.5380, Train: 0.8650, Test: 0.6540\n",
            "Early stopping:  0.03818042632539517\n",
            "Epoch: 042, Loss: 0.5153, Train: 0.8769, Test: 0.6565\n",
            "Early stopping:  0.03712478269225264\n",
            "Epoch: 043, Loss: 0.4938, Train: 0.8860, Test: 0.6603\n",
            "Early stopping:  0.03592269677520791\n",
            "Epoch: 044, Loss: 0.4725, Train: 0.8962, Test: 0.6605\n",
            "Early stopping:  0.03501335143398734\n",
            "Epoch: 045, Loss: 0.4515, Train: 0.9019, Test: 0.6603\n",
            "Early stopping:  0.034145736493595036\n",
            "Epoch: 046, Loss: 0.4315, Train: 0.9058, Test: 0.6622\n",
            "Early stopping:  0.03319084219873738\n",
            "Epoch: 047, Loss: 0.4120, Train: 0.9132, Test: 0.6622\n",
            "Early stopping:  0.03235764439060712\n",
            "Epoch: 048, Loss: 0.3929, Train: 0.9206, Test: 0.6636\n",
            "Early stopping:  0.031420346787314715\n",
            "Epoch: 049, Loss: 0.3746, Train: 0.9285, Test: 0.6626\n",
            "Early stopping:  0.030432993898872534\n",
            "Epoch: 050, Loss: 0.3568, Train: 0.9370, Test: 0.6613\n",
            "Early stopping:  0.02954609163814781\n",
            "Epoch: 051, Loss: 0.3394, Train: 0.9427, Test: 0.6612\n",
            "Early stopping:  0.028648870580615313\n",
            "Epoch: 052, Loss: 0.3230, Train: 0.9541, Test: 0.6609\n",
            "Early stopping:  0.027657223812868652\n",
            "Epoch: 053, Loss: 0.3071, Train: 0.9569, Test: 0.6601\n",
            "Early stopping:  0.026681813341111717\n",
            "Epoch: 054, Loss: 0.2920, Train: 0.9643, Test: 0.6596\n",
            "Early stopping:  0.02560562641106042\n",
            "Epoch: 055, Loss: 0.2776, Train: 0.9682, Test: 0.6605\n",
            "Early stopping:  0.0244577662917485\n",
            "Epoch: 056, Loss: 0.2639, Train: 0.9767, Test: 0.6599\n",
            "Early stopping:  0.023347305541385043\n",
            "Epoch: 057, Loss: 0.2512, Train: 0.9756, Test: 0.6625\n",
            "Early stopping:  0.022139780448921363\n",
            "Epoch: 058, Loss: 0.2394, Train: 0.9824, Test: 0.6610\n",
            "Early stopping:  0.020847882743685733\n",
            "Epoch: 059, Loss: 0.2287, Train: 0.9813, Test: 0.6619\n",
            "Early stopping:  0.019370409947030577\n",
            "Epoch: 060, Loss: 0.2198, Train: 0.9892, Test: 0.6588\n",
            "Early stopping:  0.017554012140941967\n",
            "Epoch: 061, Loss: 0.2111, Train: 0.9864, Test: 0.6619\n",
            "Early stopping:  0.01580756837872357\n",
            "Epoch: 062, Loss: 0.2002, Train: 0.9915, Test: 0.6632\n",
            "Early stopping:  0.01517790837720333\n",
            "Epoch: 063, Loss: 0.1891, Train: 0.9955, Test: 0.6619\n",
            "Early stopping:  0.015657697997093316\n",
            "Epoch: 064, Loss: 0.1818, Train: 0.9915, Test: 0.6644\n",
            "Early stopping:  0.015528797947199344\n",
            "Epoch: 065, Loss: 0.1764, Train: 0.9966, Test: 0.6608\n",
            "Early stopping:  0.014028691480359508\n",
            "Epoch: 066, Loss: 0.1691, Train: 0.9966, Test: 0.6618\n",
            "Early stopping:  0.01194514013313245\n",
            "Epoch: 067, Loss: 0.1612, Train: 0.9966, Test: 0.6629\n",
            "Early stopping:  0.010847828388524283\n",
            "Epoch: 068, Loss: 0.1561, Train: 0.9972, Test: 0.6589\n",
            "Early stopping:  0.010555975882790824\n",
            "Epoch: 069, Loss: 0.1523, Train: 0.9966, Test: 0.6626\n",
            "Early stopping:  0.009785426251486361\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.69      0.68       758\n",
            "         capital_goods       0.53      0.50      0.51       508\n",
            "conglomerates_industry       0.88      0.28      0.42        80\n",
            "     consumer_cyclical       0.61      0.62      0.62       793\n",
            " consumer_non-cyclical       0.66      0.55      0.60       446\n",
            "                energy       0.81      0.67      0.74       283\n",
            "             financial       0.73      0.67      0.69       767\n",
            "            healthcare       0.76      0.70      0.73       318\n",
            "              services       0.64      0.77      0.70      2076\n",
            "            technology       0.59      0.48      0.53       396\n",
            "        transportation       0.81      0.73      0.76       404\n",
            "             utilities       0.79      0.63      0.70       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4797, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2439, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16673158400994445\n",
            "Epoch: 003, Loss: 2.1822, Train: 0.2961, Test: 0.2959\n",
            "Early stopping:  0.1570168931493956\n",
            "Epoch: 004, Loss: 2.1114, Train: 0.3301, Test: 0.3248\n",
            "Early stopping:  0.159716872085878\n",
            "Epoch: 005, Loss: 2.0387, Train: 0.3732, Test: 0.3638\n",
            "Early stopping:  0.1685976856348576\n",
            "Epoch: 006, Loss: 1.9753, Train: 0.3823, Test: 0.3723\n",
            "Early stopping:  0.1076412942515246\n",
            "Epoch: 007, Loss: 1.9122, Train: 0.3880, Test: 0.3733\n",
            "Early stopping:  0.10694941277245097\n",
            "Epoch: 008, Loss: 1.8499, Train: 0.3999, Test: 0.3815\n",
            "Early stopping:  0.10276225312940457\n",
            "Epoch: 009, Loss: 1.7873, Train: 0.4135, Test: 0.4036\n",
            "Early stopping:  0.09934452353354291\n",
            "Epoch: 010, Loss: 1.7202, Train: 0.4402, Test: 0.4239\n",
            "Early stopping:  0.10043701989481439\n",
            "Epoch: 011, Loss: 1.6533, Train: 0.4702, Test: 0.4427\n",
            "Early stopping:  0.10237328153293107\n",
            "Epoch: 012, Loss: 1.5915, Train: 0.4923, Test: 0.4619\n",
            "Early stopping:  0.10290390124386668\n",
            "Epoch: 013, Loss: 1.5288, Train: 0.5116, Test: 0.4736\n",
            "Early stopping:  0.10213383238309773\n",
            "Epoch: 014, Loss: 1.4651, Train: 0.5491, Test: 0.5050\n",
            "Early stopping:  0.10038661216625626\n",
            "Epoch: 015, Loss: 1.4047, Train: 0.5797, Test: 0.5286\n",
            "Early stopping:  0.09862062357015937\n",
            "Epoch: 016, Loss: 1.3493, Train: 0.5990, Test: 0.5481\n",
            "Early stopping:  0.09623774560607659\n",
            "Epoch: 017, Loss: 1.2961, Train: 0.6126, Test: 0.5625\n",
            "Early stopping:  0.09193585460116795\n",
            "Epoch: 018, Loss: 1.2438, Train: 0.6177, Test: 0.5655\n",
            "Early stopping:  0.08717438330018983\n",
            "Epoch: 019, Loss: 1.1962, Train: 0.6302, Test: 0.5695\n",
            "Early stopping:  0.08264567951169241\n",
            "Epoch: 020, Loss: 1.1529, Train: 0.6444, Test: 0.5768\n",
            "Early stopping:  0.0779704312157729\n",
            "Epoch: 021, Loss: 1.1110, Train: 0.6551, Test: 0.5896\n",
            "Early stopping:  0.07298372894959429\n",
            "Epoch: 022, Loss: 1.0720, Train: 0.6687, Test: 0.5958\n",
            "Early stopping:  0.06784803321186994\n",
            "Epoch: 023, Loss: 1.0361, Train: 0.6829, Test: 0.6005\n",
            "Early stopping:  0.0634759426916702\n",
            "Epoch: 024, Loss: 1.0029, Train: 0.6880, Test: 0.6024\n",
            "Early stopping:  0.05935999748484373\n",
            "Epoch: 025, Loss: 0.9694, Train: 0.6971, Test: 0.6096\n",
            "Early stopping:  0.055737604032779005\n",
            "Epoch: 026, Loss: 0.9361, Train: 0.7050, Test: 0.6119\n",
            "Early stopping:  0.05352798587823066\n",
            "Epoch: 027, Loss: 0.9052, Train: 0.7147, Test: 0.6167\n",
            "Early stopping:  0.051949692799121254\n",
            "Epoch: 028, Loss: 0.8748, Train: 0.7306, Test: 0.6239\n",
            "Early stopping:  0.05066481048758744\n",
            "Epoch: 029, Loss: 0.8440, Train: 0.7487, Test: 0.6255\n",
            "Early stopping:  0.049348773137836865\n",
            "Epoch: 030, Loss: 0.8158, Train: 0.7595, Test: 0.6282\n",
            "Early stopping:  0.04771152084023915\n",
            "Epoch: 031, Loss: 0.7880, Train: 0.7640, Test: 0.6304\n",
            "Early stopping:  0.046396950567679535\n",
            "Epoch: 032, Loss: 0.7597, Train: 0.7788, Test: 0.6333\n",
            "Early stopping:  0.04524948077259575\n",
            "Epoch: 033, Loss: 0.7329, Train: 0.7924, Test: 0.6394\n",
            "Early stopping:  0.04399020597717811\n",
            "Epoch: 034, Loss: 0.7071, Train: 0.8003, Test: 0.6412\n",
            "Early stopping:  0.04308547302417467\n",
            "Epoch: 035, Loss: 0.6817, Train: 0.8111, Test: 0.6432\n",
            "Early stopping:  0.04195235230746691\n",
            "Epoch: 036, Loss: 0.6572, Train: 0.8185, Test: 0.6425\n",
            "Early stopping:  0.040531860654122086\n",
            "Epoch: 037, Loss: 0.6329, Train: 0.8270, Test: 0.6476\n",
            "Early stopping:  0.03954424130293862\n",
            "Epoch: 038, Loss: 0.6094, Train: 0.8349, Test: 0.6494\n",
            "Early stopping:  0.03863722347404022\n",
            "Epoch: 039, Loss: 0.5867, Train: 0.8480, Test: 0.6537\n",
            "Early stopping:  0.03760311594097464\n",
            "Epoch: 040, Loss: 0.5639, Train: 0.8599, Test: 0.6555\n",
            "Early stopping:  0.036790757307132604\n",
            "Epoch: 041, Loss: 0.5421, Train: 0.8673, Test: 0.6559\n",
            "Early stopping:  0.03590731777974805\n",
            "Epoch: 042, Loss: 0.5204, Train: 0.8746, Test: 0.6545\n",
            "Early stopping:  0.035194265433620336\n",
            "Epoch: 043, Loss: 0.4995, Train: 0.8820, Test: 0.6544\n",
            "Early stopping:  0.03445605174098782\n",
            "Epoch: 044, Loss: 0.4789, Train: 0.8900, Test: 0.6554\n",
            "Early stopping:  0.03362132096210318\n",
            "Epoch: 045, Loss: 0.4591, Train: 0.8985, Test: 0.6558\n",
            "Early stopping:  0.03281514484951469\n",
            "Epoch: 046, Loss: 0.4398, Train: 0.9024, Test: 0.6579\n",
            "Early stopping:  0.03191078227909216\n",
            "Epoch: 047, Loss: 0.4208, Train: 0.9081, Test: 0.6589\n",
            "Early stopping:  0.031104682785280287\n",
            "Epoch: 048, Loss: 0.4026, Train: 0.9144, Test: 0.6627\n",
            "Early stopping:  0.030193692755253652\n",
            "Epoch: 049, Loss: 0.3845, Train: 0.9212, Test: 0.6627\n",
            "Early stopping:  0.029463796848980817\n",
            "Epoch: 050, Loss: 0.3672, Train: 0.9325, Test: 0.6635\n",
            "Early stopping:  0.028682636092184976\n",
            "Epoch: 051, Loss: 0.3503, Train: 0.9404, Test: 0.6643\n",
            "Early stopping:  0.027889054796609858\n",
            "Epoch: 052, Loss: 0.3340, Train: 0.9501, Test: 0.6657\n",
            "Early stopping:  0.02710811902941138\n",
            "Epoch: 053, Loss: 0.3182, Train: 0.9501, Test: 0.6633\n",
            "Early stopping:  0.026220028361882884\n",
            "Epoch: 054, Loss: 0.3033, Train: 0.9620, Test: 0.6647\n",
            "Early stopping:  0.025284215975719268\n",
            "Epoch: 055, Loss: 0.2898, Train: 0.9592, Test: 0.6635\n",
            "Early stopping:  0.02399578107219215\n",
            "Epoch: 056, Loss: 0.2784, Train: 0.9711, Test: 0.6606\n",
            "Early stopping:  0.02212571454121347\n",
            "Epoch: 057, Loss: 0.2678, Train: 0.9677, Test: 0.6643\n",
            "Early stopping:  0.019921892278585332\n",
            "Epoch: 058, Loss: 0.2549, Train: 0.9813, Test: 0.6633\n",
            "Early stopping:  0.018801919105344914\n",
            "Epoch: 059, Loss: 0.2397, Train: 0.9841, Test: 0.6602\n",
            "Early stopping:  0.019616077721019955\n",
            "Epoch: 060, Loss: 0.2293, Train: 0.9807, Test: 0.6640\n",
            "Early stopping:  0.020002904919320216\n",
            "Epoch: 061, Loss: 0.2223, Train: 0.9898, Test: 0.6585\n",
            "Early stopping:  0.018605639594262383\n",
            "Epoch: 062, Loss: 0.2118, Train: 0.9898, Test: 0.6632\n",
            "Early stopping:  0.016502624797802318\n",
            "Epoch: 063, Loss: 0.2008, Train: 0.9875, Test: 0.6647\n",
            "Early stopping:  0.015086393922955526\n",
            "Epoch: 064, Loss: 0.1943, Train: 0.9938, Test: 0.6603\n",
            "Early stopping:  0.014522507648927256\n",
            "Epoch: 065, Loss: 0.1876, Train: 0.9926, Test: 0.6632\n",
            "Early stopping:  0.013845611261501634\n",
            "Epoch: 066, Loss: 0.1784, Train: 0.9932, Test: 0.6625\n",
            "Early stopping:  0.01272728220829862\n",
            "Epoch: 067, Loss: 0.1721, Train: 0.9960, Test: 0.6602\n",
            "Early stopping:  0.011632591614443258\n",
            "Epoch: 068, Loss: 0.1672, Train: 0.9960, Test: 0.6637\n",
            "Early stopping:  0.011081039289208698\n",
            "Epoch: 069, Loss: 0.1604, Train: 0.9960, Test: 0.6630\n",
            "Early stopping:  0.010436369897352115\n",
            "Epoch: 070, Loss: 0.1547, Train: 0.9972, Test: 0.6599\n",
            "Early stopping:  0.009350444530585342\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.61      0.67      0.64       758\n",
            "         capital_goods       0.52      0.54      0.53       508\n",
            "conglomerates_industry       0.88      0.26      0.40        80\n",
            "     consumer_cyclical       0.60      0.64      0.62       793\n",
            " consumer_non-cyclical       0.67      0.56      0.61       446\n",
            "                energy       0.75      0.72      0.73       283\n",
            "             financial       0.70      0.69      0.69       767\n",
            "            healthcare       0.75      0.72      0.73       318\n",
            "              services       0.68      0.72      0.70      2076\n",
            "            technology       0.52      0.48      0.50       396\n",
            "        transportation       0.81      0.74      0.77       404\n",
            "             utilities       0.80      0.66      0.72       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.69      0.62      0.64      7054\n",
            "          weighted avg       0.66      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4664, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2612, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1451392477556313\n",
            "Epoch: 003, Loss: 2.1678, Train: 0.2944, Test: 0.2946\n",
            "Early stopping:  0.1527638727183749\n",
            "Epoch: 004, Loss: 2.1198, Train: 0.3057, Test: 0.3055\n",
            "Early stopping:  0.15342971153293325\n",
            "Epoch: 005, Loss: 2.0553, Train: 0.3613, Test: 0.3509\n",
            "Early stopping:  0.15980475569333108\n",
            "Epoch: 006, Loss: 2.0013, Train: 0.3840, Test: 0.3703\n",
            "Early stopping:  0.1005883208281808\n",
            "Epoch: 007, Loss: 1.9406, Train: 0.3823, Test: 0.3706\n",
            "Early stopping:  0.09065159102901421\n",
            "Epoch: 008, Loss: 1.8762, Train: 0.3823, Test: 0.3694\n",
            "Early stopping:  0.09520911858971494\n",
            "Epoch: 009, Loss: 1.8203, Train: 0.3936, Test: 0.3795\n",
            "Early stopping:  0.09414870650239486\n",
            "Epoch: 010, Loss: 1.7646, Train: 0.4260, Test: 0.4081\n",
            "Early stopping:  0.09394466381154731\n",
            "Epoch: 011, Loss: 1.7011, Train: 0.4628, Test: 0.4402\n",
            "Early stopping:  0.09341309189731367\n",
            "Epoch: 012, Loss: 1.6394, Train: 0.4901, Test: 0.4593\n",
            "Early stopping:  0.0937630769441807\n",
            "Epoch: 013, Loss: 1.5791, Train: 0.5054, Test: 0.4692\n",
            "Early stopping:  0.09606590468366759\n",
            "Epoch: 014, Loss: 1.5171, Train: 0.5207, Test: 0.4850\n",
            "Early stopping:  0.0975457080996497\n",
            "Epoch: 015, Loss: 1.4577, Train: 0.5383, Test: 0.4950\n",
            "Early stopping:  0.09629148630385663\n",
            "Epoch: 016, Loss: 1.4011, Train: 0.5598, Test: 0.5170\n",
            "Early stopping:  0.0945488769949114\n",
            "Epoch: 017, Loss: 1.3453, Train: 0.5899, Test: 0.5371\n",
            "Early stopping:  0.09231050140392624\n",
            "Epoch: 018, Loss: 1.2922, Train: 0.6132, Test: 0.5489\n",
            "Early stopping:  0.08892369386064371\n",
            "Epoch: 019, Loss: 1.2412, Train: 0.6268, Test: 0.5612\n",
            "Early stopping:  0.08571262116297325\n",
            "Epoch: 020, Loss: 1.1940, Train: 0.6449, Test: 0.5714\n",
            "Early stopping:  0.08199127951329875\n",
            "Epoch: 021, Loss: 1.1492, Train: 0.6574, Test: 0.5807\n",
            "Early stopping:  0.07758058257992918\n",
            "Epoch: 022, Loss: 1.1055, Train: 0.6608, Test: 0.5893\n",
            "Early stopping:  0.07362310371021383\n",
            "Epoch: 023, Loss: 1.0663, Train: 0.6693, Test: 0.5950\n",
            "Early stopping:  0.06935676974949345\n",
            "Epoch: 024, Loss: 1.0291, Train: 0.6801, Test: 0.6032\n",
            "Early stopping:  0.06529528267152608\n",
            "Epoch: 025, Loss: 0.9928, Train: 0.6869, Test: 0.6073\n",
            "Early stopping:  0.06155529029965837\n",
            "Epoch: 026, Loss: 0.9578, Train: 0.6971, Test: 0.6107\n",
            "Early stopping:  0.05833230256840169\n",
            "Epoch: 027, Loss: 0.9248, Train: 0.7136, Test: 0.6124\n",
            "Early stopping:  0.05603940198805615\n",
            "Epoch: 028, Loss: 0.8922, Train: 0.7283, Test: 0.6194\n",
            "Early stopping:  0.054068312533232854\n",
            "Epoch: 029, Loss: 0.8592, Train: 0.7379, Test: 0.6250\n",
            "Early stopping:  0.05261510220300275\n",
            "Epoch: 030, Loss: 0.8287, Train: 0.7544, Test: 0.6279\n",
            "Early stopping:  0.05119023015959779\n",
            "Epoch: 031, Loss: 0.7977, Train: 0.7691, Test: 0.6280\n",
            "Early stopping:  0.0502240942254455\n",
            "Epoch: 032, Loss: 0.7676, Train: 0.7816, Test: 0.6310\n",
            "Early stopping:  0.04913461002238567\n",
            "Epoch: 033, Loss: 0.7387, Train: 0.7947, Test: 0.6330\n",
            "Early stopping:  0.04777673557872615\n",
            "Epoch: 034, Loss: 0.7108, Train: 0.8037, Test: 0.6378\n",
            "Early stopping:  0.04661115050738058\n",
            "Epoch: 035, Loss: 0.6835, Train: 0.8117, Test: 0.6402\n",
            "Early stopping:  0.04509898713398501\n",
            "Epoch: 036, Loss: 0.6576, Train: 0.8230, Test: 0.6446\n",
            "Early stopping:  0.04352227745928055\n",
            "Epoch: 037, Loss: 0.6317, Train: 0.8293, Test: 0.6463\n",
            "Early stopping:  0.042251029962157995\n",
            "Epoch: 038, Loss: 0.6071, Train: 0.8383, Test: 0.6479\n",
            "Early stopping:  0.04101961642429465\n",
            "Epoch: 039, Loss: 0.5827, Train: 0.8503, Test: 0.6504\n",
            "Early stopping:  0.03989758899072814\n",
            "Epoch: 040, Loss: 0.5589, Train: 0.8599, Test: 0.6548\n",
            "Early stopping:  0.03896766768970368\n",
            "Epoch: 041, Loss: 0.5361, Train: 0.8690, Test: 0.6542\n",
            "Early stopping:  0.037863529793371355\n",
            "Epoch: 042, Loss: 0.5137, Train: 0.8741, Test: 0.6564\n",
            "Early stopping:  0.03689048228974906\n",
            "Epoch: 043, Loss: 0.4921, Train: 0.8866, Test: 0.6572\n",
            "Early stopping:  0.03580066360982008\n",
            "Epoch: 044, Loss: 0.4707, Train: 0.8956, Test: 0.6565\n",
            "Early stopping:  0.034871684180478896\n",
            "Epoch: 045, Loss: 0.4499, Train: 0.9041, Test: 0.6589\n",
            "Early stopping:  0.03406563963053605\n",
            "Epoch: 046, Loss: 0.4294, Train: 0.9121, Test: 0.6583\n",
            "Early stopping:  0.033319368672769294\n",
            "Epoch: 047, Loss: 0.4094, Train: 0.9178, Test: 0.6613\n",
            "Early stopping:  0.03266948188966932\n",
            "Epoch: 048, Loss: 0.3900, Train: 0.9251, Test: 0.6596\n",
            "Early stopping:  0.03190928674495549\n",
            "Epoch: 049, Loss: 0.3711, Train: 0.9336, Test: 0.6605\n",
            "Early stopping:  0.03115552112351644\n",
            "Epoch: 050, Loss: 0.3530, Train: 0.9393, Test: 0.6598\n",
            "Early stopping:  0.030244608870724134\n",
            "Epoch: 051, Loss: 0.3354, Train: 0.9512, Test: 0.6591\n",
            "Early stopping:  0.02925093114466516\n",
            "Epoch: 052, Loss: 0.3187, Train: 0.9529, Test: 0.6635\n",
            "Early stopping:  0.02818482197338832\n",
            "Epoch: 053, Loss: 0.3031, Train: 0.9665, Test: 0.6610\n",
            "Early stopping:  0.026920437349640194\n",
            "Epoch: 054, Loss: 0.2890, Train: 0.9637, Test: 0.6619\n",
            "Early stopping:  0.025369124143927186\n",
            "Epoch: 055, Loss: 0.2782, Train: 0.9739, Test: 0.6579\n",
            "Early stopping:  0.02289179735870059\n",
            "Epoch: 056, Loss: 0.2666, Train: 0.9733, Test: 0.6619\n",
            "Early stopping:  0.02048483462739193\n",
            "Epoch: 057, Loss: 0.2510, Train: 0.9818, Test: 0.6623\n",
            "Early stopping:  0.02006136603423994\n",
            "Epoch: 058, Loss: 0.2363, Train: 0.9796, Test: 0.6602\n",
            "Early stopping:  0.021036544973911324\n",
            "Epoch: 059, Loss: 0.2281, Train: 0.9836, Test: 0.6613\n",
            "Early stopping:  0.020730030403123843\n",
            "Epoch: 060, Loss: 0.2187, Train: 0.9898, Test: 0.6613\n",
            "Early stopping:  0.01897283090154566\n",
            "Epoch: 061, Loss: 0.2073, Train: 0.9881, Test: 0.6609\n",
            "Early stopping:  0.01667440278785363\n",
            "Epoch: 062, Loss: 0.1992, Train: 0.9892, Test: 0.6623\n",
            "Early stopping:  0.015025065961257893\n",
            "Epoch: 063, Loss: 0.1913, Train: 0.9926, Test: 0.6637\n",
            "Early stopping:  0.014760153826177203\n",
            "Epoch: 064, Loss: 0.1840, Train: 0.9915, Test: 0.6601\n",
            "Early stopping:  0.0135625272457076\n",
            "Epoch: 065, Loss: 0.1771, Train: 0.9932, Test: 0.6615\n",
            "Early stopping:  0.011945862607260311\n",
            "Epoch: 066, Loss: 0.1698, Train: 0.9943, Test: 0.6622\n",
            "Early stopping:  0.011558500217203557\n",
            "Epoch: 067, Loss: 0.1654, Train: 0.9932, Test: 0.6601\n",
            "Early stopping:  0.010456384529598918\n",
            "Epoch: 068, Loss: 0.1595, Train: 0.9949, Test: 0.6609\n",
            "Early stopping:  0.00963664961972237\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.68      0.65       758\n",
            "         capital_goods       0.56      0.52      0.54       508\n",
            "conglomerates_industry       0.96      0.30      0.46        80\n",
            "     consumer_cyclical       0.59      0.65      0.62       793\n",
            " consumer_non-cyclical       0.71      0.57      0.63       446\n",
            "                energy       0.76      0.67      0.72       283\n",
            "             financial       0.70      0.68      0.69       767\n",
            "            healthcare       0.73      0.75      0.74       318\n",
            "              services       0.66      0.73      0.69      2076\n",
            "            technology       0.57      0.47      0.51       396\n",
            "        transportation       0.79      0.74      0.76       404\n",
            "             utilities       0.80      0.60      0.69       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.70      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4853, Train: 0.2944, Test: 0.2939\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2582, Train: 0.2944, Test: 0.2939\n",
            "Early stopping:  0.16057089486941004\n",
            "Epoch: 003, Loss: 2.1726, Train: 0.2944, Test: 0.2939\n",
            "Early stopping:  0.1616018605940319\n",
            "Epoch: 004, Loss: 2.1336, Train: 0.2961, Test: 0.2956\n",
            "Early stopping:  0.1574508361331867\n",
            "Epoch: 005, Loss: 2.0730, Train: 0.3386, Test: 0.3299\n",
            "Early stopping:  0.160525855132288\n",
            "Epoch: 006, Loss: 2.0219, Train: 0.3670, Test: 0.3571\n",
            "Early stopping:  0.09107059302796683\n",
            "Epoch: 007, Loss: 1.9638, Train: 0.3800, Test: 0.3650\n",
            "Early stopping:  0.08384111829051924\n",
            "Epoch: 008, Loss: 1.9020, Train: 0.3806, Test: 0.3677\n",
            "Early stopping:  0.09053846771614672\n",
            "Epoch: 009, Loss: 1.8454, Train: 0.3851, Test: 0.3764\n",
            "Early stopping:  0.09098389942013917\n",
            "Epoch: 010, Loss: 1.7913, Train: 0.4107, Test: 0.3918\n",
            "Early stopping:  0.09166893562312411\n",
            "Epoch: 011, Loss: 1.7313, Train: 0.4368, Test: 0.4191\n",
            "Early stopping:  0.0910462958327962\n",
            "Epoch: 012, Loss: 1.6649, Train: 0.4776, Test: 0.4478\n",
            "Early stopping:  0.09310876182020379\n",
            "Epoch: 013, Loss: 1.6005, Train: 0.4991, Test: 0.4660\n",
            "Early stopping:  0.09752121410909861\n",
            "Epoch: 014, Loss: 1.5407, Train: 0.5235, Test: 0.4782\n",
            "Early stopping:  0.09995785391484217\n",
            "Epoch: 015, Loss: 1.4809, Train: 0.5445, Test: 0.4965\n",
            "Early stopping:  0.09884132961707719\n",
            "Epoch: 016, Loss: 1.4190, Train: 0.5570, Test: 0.5122\n",
            "Early stopping:  0.09665204151832281\n",
            "Epoch: 017, Loss: 1.3584, Train: 0.5820, Test: 0.5327\n",
            "Early stopping:  0.09578785579995937\n",
            "Epoch: 018, Loss: 1.3026, Train: 0.6103, Test: 0.5500\n",
            "Early stopping:  0.09466317222091217\n",
            "Epoch: 019, Loss: 1.2509, Train: 0.6341, Test: 0.5671\n",
            "Early stopping:  0.09120945909565277\n",
            "Epoch: 020, Loss: 1.2004, Train: 0.6421, Test: 0.5795\n",
            "Early stopping:  0.0862130302668957\n",
            "Epoch: 021, Loss: 1.1534, Train: 0.6540, Test: 0.5841\n",
            "Early stopping:  0.08103225848935951\n",
            "Epoch: 022, Loss: 1.1108, Train: 0.6614, Test: 0.5902\n",
            "Early stopping:  0.07613346524364706\n",
            "Epoch: 023, Loss: 1.0695, Train: 0.6687, Test: 0.5921\n",
            "Early stopping:  0.07159813240751989\n",
            "Epoch: 024, Loss: 1.0317, Train: 0.6733, Test: 0.5984\n",
            "Early stopping:  0.0666536534492549\n",
            "Epoch: 025, Loss: 0.9972, Train: 0.6812, Test: 0.6038\n",
            "Early stopping:  0.06196150884286494\n",
            "Epoch: 026, Loss: 0.9622, Train: 0.6960, Test: 0.6079\n",
            "Early stopping:  0.058461732592525295\n",
            "Epoch: 027, Loss: 0.9291, Train: 0.7119, Test: 0.6116\n",
            "Early stopping:  0.05540807444615746\n",
            "Epoch: 028, Loss: 0.8975, Train: 0.7215, Test: 0.6172\n",
            "Early stopping:  0.05322883627033426\n",
            "Epoch: 029, Loss: 0.8656, Train: 0.7323, Test: 0.6194\n",
            "Early stopping:  0.051854148093061075\n",
            "Epoch: 030, Loss: 0.8347, Train: 0.7459, Test: 0.6228\n",
            "Early stopping:  0.05035253524540628\n",
            "Epoch: 031, Loss: 0.8053, Train: 0.7589, Test: 0.6273\n",
            "Early stopping:  0.04906534398492857\n",
            "Epoch: 032, Loss: 0.7764, Train: 0.7669, Test: 0.6321\n",
            "Early stopping:  0.04781308320662481\n",
            "Epoch: 033, Loss: 0.7484, Train: 0.7742, Test: 0.6330\n",
            "Early stopping:  0.04628992812869116\n",
            "Epoch: 034, Loss: 0.7209, Train: 0.7930, Test: 0.6350\n",
            "Early stopping:  0.04500397198976818\n",
            "Epoch: 035, Loss: 0.6945, Train: 0.8003, Test: 0.6381\n",
            "Early stopping:  0.043821568746816954\n",
            "Epoch: 036, Loss: 0.6690, Train: 0.8117, Test: 0.6395\n",
            "Early stopping:  0.042483691011823504\n",
            "Epoch: 037, Loss: 0.6437, Train: 0.8196, Test: 0.6429\n",
            "Early stopping:  0.04130624287191112\n",
            "Epoch: 038, Loss: 0.6194, Train: 0.8298, Test: 0.6445\n",
            "Early stopping:  0.04016088340886992\n",
            "Epoch: 039, Loss: 0.5958, Train: 0.8389, Test: 0.6460\n",
            "Early stopping:  0.03908477604528206\n",
            "Epoch: 040, Loss: 0.5730, Train: 0.8548, Test: 0.6477\n",
            "Early stopping:  0.037959667934961284\n",
            "Epoch: 041, Loss: 0.5506, Train: 0.8616, Test: 0.6493\n",
            "Early stopping:  0.036775378301215585\n",
            "Epoch: 042, Loss: 0.5287, Train: 0.8656, Test: 0.6531\n",
            "Early stopping:  0.035814705548808466\n",
            "Epoch: 043, Loss: 0.5073, Train: 0.8763, Test: 0.6561\n",
            "Early stopping:  0.03497599440210543\n",
            "Epoch: 044, Loss: 0.4864, Train: 0.8866, Test: 0.6544\n",
            "Early stopping:  0.03423425621063746\n",
            "Epoch: 045, Loss: 0.4657, Train: 0.8956, Test: 0.6540\n",
            "Early stopping:  0.03354201821279466\n",
            "Epoch: 046, Loss: 0.4459, Train: 0.9070, Test: 0.6555\n",
            "Early stopping:  0.032777401941511776\n",
            "Epoch: 047, Loss: 0.4266, Train: 0.9132, Test: 0.6547\n",
            "Early stopping:  0.03192925330417743\n",
            "Epoch: 048, Loss: 0.4078, Train: 0.9195, Test: 0.6541\n",
            "Early stopping:  0.031029389867225178\n",
            "Epoch: 049, Loss: 0.3896, Train: 0.9257, Test: 0.6559\n",
            "Early stopping:  0.030101667335601493\n",
            "Epoch: 050, Loss: 0.3723, Train: 0.9336, Test: 0.6547\n",
            "Early stopping:  0.02912506732451577\n",
            "Epoch: 051, Loss: 0.3556, Train: 0.9348, Test: 0.6572\n",
            "Early stopping:  0.0280694921709025\n",
            "Epoch: 052, Loss: 0.3406, Train: 0.9455, Test: 0.6549\n",
            "Early stopping:  0.02663627365637167\n",
            "Epoch: 053, Loss: 0.3267, Train: 0.9455, Test: 0.6598\n",
            "Early stopping:  0.02493119460009435\n",
            "Epoch: 054, Loss: 0.3132, Train: 0.9597, Test: 0.6562\n",
            "Early stopping:  0.023295542502660416\n",
            "Epoch: 055, Loss: 0.2970, Train: 0.9631, Test: 0.6591\n",
            "Early stopping:  0.022878412183912976\n",
            "Epoch: 056, Loss: 0.2831, Train: 0.9660, Test: 0.6609\n",
            "Early stopping:  0.022883122313498365\n",
            "Epoch: 057, Loss: 0.2724, Train: 0.9716, Test: 0.6575\n",
            "Early stopping:  0.02195580106562766\n",
            "Epoch: 058, Loss: 0.2613, Train: 0.9756, Test: 0.6598\n",
            "Early stopping:  0.020364667826393987\n",
            "Epoch: 059, Loss: 0.2490, Train: 0.9824, Test: 0.6586\n",
            "Early stopping:  0.01864823684927707\n",
            "Epoch: 060, Loss: 0.2380, Train: 0.9853, Test: 0.6572\n",
            "Early stopping:  0.017975146280844075\n",
            "Epoch: 061, Loss: 0.2294, Train: 0.9853, Test: 0.6588\n",
            "Early stopping:  0.017342872968611825\n",
            "Epoch: 062, Loss: 0.2207, Train: 0.9875, Test: 0.6574\n",
            "Early stopping:  0.016007883645970052\n",
            "Epoch: 063, Loss: 0.2108, Train: 0.9864, Test: 0.6585\n",
            "Early stopping:  0.014834849478910426\n",
            "Epoch: 064, Loss: 0.2029, Train: 0.9892, Test: 0.6558\n",
            "Early stopping:  0.014053744326662163\n",
            "Epoch: 065, Loss: 0.1971, Train: 0.9858, Test: 0.6581\n",
            "Early stopping:  0.013078798368907673\n",
            "Epoch: 066, Loss: 0.1926, Train: 0.9926, Test: 0.6514\n",
            "Early stopping:  0.011197157726088938\n",
            "Epoch: 067, Loss: 0.1889, Train: 0.9858, Test: 0.6599\n",
            "Early stopping:  0.008637807350191215\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.64      0.65       758\n",
            "         capital_goods       0.55      0.49      0.52       508\n",
            "conglomerates_industry       0.91      0.25      0.39        80\n",
            "     consumer_cyclical       0.60      0.63      0.62       793\n",
            " consumer_non-cyclical       0.75      0.55      0.63       446\n",
            "                energy       0.81      0.68      0.74       283\n",
            "             financial       0.75      0.64      0.69       767\n",
            "            healthcare       0.71      0.70      0.71       318\n",
            "              services       0.62      0.80      0.70      2076\n",
            "            technology       0.58      0.45      0.51       396\n",
            "        transportation       0.81      0.68      0.74       404\n",
            "             utilities       0.82      0.60      0.69       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.59      0.63      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5265, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2665, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.18384286733057933\n",
            "Epoch: 003, Loss: 2.2035, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17119746708406466\n",
            "Epoch: 004, Loss: 2.1344, Train: 0.3006, Test: 0.3004\n",
            "Early stopping:  0.17120272582111515\n",
            "Epoch: 005, Loss: 2.0527, Train: 0.3721, Test: 0.3618\n",
            "Early stopping:  0.18046275825441407\n",
            "Epoch: 006, Loss: 2.0098, Train: 0.4010, Test: 0.3835\n",
            "Early stopping:  0.1053653364380861\n",
            "Epoch: 007, Loss: 1.9616, Train: 0.4084, Test: 0.3927\n",
            "Early stopping:  0.09698290252518439\n",
            "Epoch: 008, Loss: 1.8934, Train: 0.3988, Test: 0.3876\n",
            "Early stopping:  0.09117183811492333\n",
            "Epoch: 009, Loss: 1.8200, Train: 0.3936, Test: 0.3835\n",
            "Early stopping:  0.09264337795607079\n",
            "Epoch: 010, Loss: 1.7557, Train: 0.4090, Test: 0.3954\n",
            "Early stopping:  0.10296326244872582\n",
            "Epoch: 011, Loss: 1.6975, Train: 0.4419, Test: 0.4219\n",
            "Early stopping:  0.10537930764833058\n",
            "Epoch: 012, Loss: 1.6334, Train: 0.4821, Test: 0.4491\n",
            "Early stopping:  0.10166040340816872\n",
            "Epoch: 013, Loss: 1.5638, Train: 0.5077, Test: 0.4760\n",
            "Early stopping:  0.10038604042536454\n",
            "Epoch: 014, Loss: 1.4980, Train: 0.5281, Test: 0.4983\n",
            "Early stopping:  0.10267903216453828\n",
            "Epoch: 015, Loss: 1.4392, Train: 0.5649, Test: 0.5170\n",
            "Early stopping:  0.10310704348609569\n",
            "Epoch: 016, Loss: 1.3807, Train: 0.5848, Test: 0.5354\n",
            "Early stopping:  0.09971444452982832\n",
            "Epoch: 017, Loss: 1.3211, Train: 0.5961, Test: 0.5472\n",
            "Early stopping:  0.09532743060283118\n",
            "Epoch: 018, Loss: 1.2673, Train: 0.6069, Test: 0.5567\n",
            "Early stopping:  0.09164352838224853\n",
            "Epoch: 019, Loss: 1.2201, Train: 0.6166, Test: 0.5634\n",
            "Early stopping:  0.0873030821671233\n",
            "Epoch: 020, Loss: 1.1746, Train: 0.6296, Test: 0.5741\n",
            "Early stopping:  0.08126516738721806\n",
            "Epoch: 021, Loss: 1.1308, Train: 0.6489, Test: 0.5868\n",
            "Early stopping:  0.07491313023498568\n",
            "Epoch: 022, Loss: 1.0924, Train: 0.6619, Test: 0.5930\n",
            "Early stopping:  0.06947912875313872\n",
            "Epoch: 023, Loss: 1.0568, Train: 0.6727, Test: 0.5981\n",
            "Early stopping:  0.06474138379720282\n",
            "Epoch: 024, Loss: 1.0208, Train: 0.6801, Test: 0.6011\n",
            "Early stopping:  0.06039476690069419\n",
            "Epoch: 025, Loss: 0.9864, Train: 0.6869, Test: 0.6043\n",
            "Early stopping:  0.05698540817800741\n",
            "Epoch: 026, Loss: 0.9531, Train: 0.6960, Test: 0.6102\n",
            "Early stopping:  0.05519564068999296\n",
            "Epoch: 027, Loss: 0.9189, Train: 0.7119, Test: 0.6153\n",
            "Early stopping:  0.05430176375708716\n",
            "Epoch: 028, Loss: 0.8857, Train: 0.7255, Test: 0.6187\n",
            "Early stopping:  0.0533916894496006\n",
            "Epoch: 029, Loss: 0.8544, Train: 0.7340, Test: 0.6232\n",
            "Early stopping:  0.05241117346715645\n",
            "Epoch: 030, Loss: 0.8236, Train: 0.7442, Test: 0.6284\n",
            "Early stopping:  0.05114953064905904\n",
            "Epoch: 031, Loss: 0.7946, Train: 0.7589, Test: 0.6301\n",
            "Early stopping:  0.04913422323741627\n",
            "Epoch: 032, Loss: 0.7659, Train: 0.7737, Test: 0.6325\n",
            "Early stopping:  0.04734286243864174\n",
            "Epoch: 033, Loss: 0.7376, Train: 0.7856, Test: 0.6343\n",
            "Early stopping:  0.04605596032755112\n",
            "Epoch: 034, Loss: 0.7107, Train: 0.7986, Test: 0.6360\n",
            "Early stopping:  0.044743936506808794\n",
            "Epoch: 035, Loss: 0.6840, Train: 0.8060, Test: 0.6384\n",
            "Early stopping:  0.043717511401088295\n",
            "Epoch: 036, Loss: 0.6583, Train: 0.8179, Test: 0.6435\n",
            "Early stopping:  0.04250935151998004\n",
            "Epoch: 037, Loss: 0.6330, Train: 0.8281, Test: 0.6483\n",
            "Early stopping:  0.04137370848466811\n",
            "Epoch: 038, Loss: 0.6085, Train: 0.8378, Test: 0.6534\n",
            "Early stopping:  0.04036826497530905\n",
            "Epoch: 039, Loss: 0.5843, Train: 0.8474, Test: 0.6521\n",
            "Early stopping:  0.03940638383034939\n",
            "Epoch: 040, Loss: 0.5608, Train: 0.8531, Test: 0.6541\n",
            "Early stopping:  0.038539814829251284\n",
            "Epoch: 041, Loss: 0.5380, Train: 0.8673, Test: 0.6574\n",
            "Early stopping:  0.03760278428063996\n",
            "Epoch: 042, Loss: 0.5154, Train: 0.8735, Test: 0.6578\n",
            "Early stopping:  0.036795547995420036\n",
            "Epoch: 043, Loss: 0.4934, Train: 0.8809, Test: 0.6588\n",
            "Early stopping:  0.03592438943875136\n",
            "Epoch: 044, Loss: 0.4722, Train: 0.8883, Test: 0.6602\n",
            "Early stopping:  0.03505855030985694\n",
            "Epoch: 045, Loss: 0.4513, Train: 0.8990, Test: 0.6603\n",
            "Early stopping:  0.03424663877586012\n",
            "Epoch: 046, Loss: 0.4312, Train: 0.9053, Test: 0.6610\n",
            "Early stopping:  0.03326660695572056\n",
            "Epoch: 047, Loss: 0.4115, Train: 0.9121, Test: 0.6636\n",
            "Early stopping:  0.03238653250601304\n",
            "Epoch: 048, Loss: 0.3924, Train: 0.9223, Test: 0.6642\n",
            "Early stopping:  0.03153264447168157\n",
            "Epoch: 049, Loss: 0.3739, Train: 0.9336, Test: 0.6632\n",
            "Early stopping:  0.030617811694226033\n",
            "Epoch: 050, Loss: 0.3561, Train: 0.9399, Test: 0.6656\n",
            "Early stopping:  0.02972501406286976\n",
            "Epoch: 051, Loss: 0.3392, Train: 0.9461, Test: 0.6623\n",
            "Early stopping:  0.028606341416968124\n",
            "Epoch: 052, Loss: 0.3234, Train: 0.9467, Test: 0.6656\n",
            "Early stopping:  0.02730389751354434\n",
            "Epoch: 053, Loss: 0.3099, Train: 0.9563, Test: 0.6544\n",
            "Early stopping:  0.025417720941073113\n",
            "Epoch: 054, Loss: 0.3000, Train: 0.9518, Test: 0.6657\n",
            "Early stopping:  0.022467218088576598\n",
            "Epoch: 055, Loss: 0.2897, Train: 0.9654, Test: 0.6572\n",
            "Early stopping:  0.01946027734976148\n",
            "Epoch: 056, Loss: 0.2707, Train: 0.9699, Test: 0.6603\n",
            "Early stopping:  0.020041407198098056\n",
            "Epoch: 057, Loss: 0.2556, Train: 0.9660, Test: 0.6632\n",
            "Early stopping:  0.022017176023688582\n",
            "Epoch: 058, Loss: 0.2502, Train: 0.9762, Test: 0.6548\n",
            "Early stopping:  0.021429276608656223\n",
            "Epoch: 059, Loss: 0.2377, Train: 0.9796, Test: 0.6591\n",
            "Early stopping:  0.02003208763775869\n",
            "Epoch: 060, Loss: 0.2245, Train: 0.9784, Test: 0.6615\n",
            "Early stopping:  0.01756284255137728\n",
            "Epoch: 061, Loss: 0.2198, Train: 0.9824, Test: 0.6534\n",
            "Early stopping:  0.01559575930591062\n",
            "Epoch: 062, Loss: 0.2097, Train: 0.9847, Test: 0.6559\n",
            "Early stopping:  0.0158220739573063\n",
            "Epoch: 063, Loss: 0.1993, Train: 0.9858, Test: 0.6616\n",
            "Early stopping:  0.014601770511420298\n",
            "Epoch: 064, Loss: 0.1952, Train: 0.9898, Test: 0.6490\n",
            "Early stopping:  0.012644651103272474\n",
            "Epoch: 065, Loss: 0.1876, Train: 0.9909, Test: 0.6528\n",
            "Early stopping:  0.01261038553077826\n",
            "Epoch: 066, Loss: 0.1786, Train: 0.9921, Test: 0.6576\n",
            "Early stopping:  0.01178634719815588\n",
            "Epoch: 067, Loss: 0.1753, Train: 0.9932, Test: 0.6510\n",
            "Early stopping:  0.010323054650940387\n",
            "Epoch: 068, Loss: 0.1693, Train: 0.9943, Test: 0.6507\n",
            "Early stopping:  0.01022498744586121\n",
            "Epoch: 069, Loss: 0.1621, Train: 0.9938, Test: 0.6575\n",
            "Early stopping:  0.00960824821493072\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.71      0.66       758\n",
            "         capital_goods       0.57      0.51      0.54       508\n",
            "conglomerates_industry       1.00      0.21      0.35        80\n",
            "     consumer_cyclical       0.58      0.58      0.58       793\n",
            " consumer_non-cyclical       0.67      0.53      0.59       446\n",
            "                energy       0.80      0.69      0.74       283\n",
            "             financial       0.73      0.67      0.70       767\n",
            "            healthcare       0.74      0.71      0.73       318\n",
            "              services       0.65      0.76      0.70      2076\n",
            "            technology       0.55      0.45      0.50       396\n",
            "        transportation       0.79      0.72      0.75       404\n",
            "             utilities       0.77      0.62      0.69       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.60      0.63      7054\n",
            "          weighted avg       0.66      0.66      0.65      7054\n",
            "\n",
            "time: 2min 6s (started: 2024-10-16 20:55:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKuelhyAVyh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6cf419-7bdd-45f1-fad0-f78f76bc6bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 407 ms (started: 2024-10-16 20:57:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_u1h2Busrbs"
      },
      "source": [
        "### Training rotulated base = 40% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqZxh6BrU8C-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bd223d-bdc7-4b1f-d6e2-53deadfc6987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 574 µs (started: 2024-10-16 20:57:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRzeKXYxHrm-"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6jERQ-VHrm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fbfab6-007f-4d3d-ebfd-6f9ed4196c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 85.5463, Train: 0.2969, Test: 0.2952\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 350.4079, Train: 0.0752, Test: 0.0741\n",
            "Early stopping:  187.2854461935339\n",
            "Epoch: 003, Loss: 644.9293, Train: 0.1387, Test: 0.1438\n",
            "Early stopping:  279.82254272530844\n",
            "Epoch: 004, Loss: 623.5221, Train: 0.0771, Test: 0.0858\n",
            "Early stopping:  263.6714406960082\n",
            "Epoch: 005, Loss: 509.4789, Train: 0.0559, Test: 0.0580\n",
            "Early stopping:  231.37055440002882\n",
            "Epoch: 006, Loss: 401.6621, Train: 0.0434, Test: 0.0474\n",
            "Early stopping:  130.59498414425684\n",
            "Epoch: 007, Loss: 272.4460, Train: 0.0805, Test: 0.0898\n",
            "Early stopping:  155.99932765316026\n",
            "Epoch: 008, Loss: 165.8453, Train: 0.1293, Test: 0.1291\n",
            "Early stopping:  182.2790868372921\n",
            "Epoch: 009, Loss: 89.4267, Train: 0.1254, Test: 0.1187\n",
            "Early stopping:  170.70937064439053\n",
            "Epoch: 010, Loss: 47.6080, Train: 0.2725, Test: 0.2716\n",
            "Early stopping:  143.5500315950319\n",
            "Epoch: 011, Loss: 37.8439, Train: 0.2782, Test: 0.2807\n",
            "Early stopping:  97.7648387834316\n",
            "Epoch: 012, Loss: 32.0590, Train: 0.2952, Test: 0.2947\n",
            "Early stopping:  55.75582538899115\n",
            "Epoch: 013, Loss: 20.6798, Train: 0.3035, Test: 0.3022\n",
            "Early stopping:  26.406982985268744\n",
            "Epoch: 014, Loss: 9.4531, Train: 0.2737, Test: 0.2682\n",
            "Early stopping:  14.863858420676628\n",
            "Epoch: 015, Loss: 5.0017, Train: 0.2422, Test: 0.2276\n",
            "Early stopping:  14.101406311134328\n",
            "Epoch: 016, Loss: 3.7115, Train: 0.2232, Test: 0.2049\n",
            "Early stopping:  12.02168104985888\n",
            "Epoch: 017, Loss: 3.0401, Train: 0.2175, Test: 0.1988\n",
            "Early stopping:  7.317112497645853\n",
            "Epoch: 018, Loss: 2.6793, Train: 0.1954, Test: 0.1748\n",
            "Early stopping:  2.759925500103233\n",
            "Epoch: 019, Loss: 2.5431, Train: 0.1767, Test: 0.1518\n",
            "Early stopping:  1.0056734747598681\n",
            "Epoch: 020, Loss: 2.4992, Train: 0.1619, Test: 0.1391\n",
            "Early stopping:  0.5036642704480978\n",
            "Epoch: 021, Loss: 2.4816, Train: 0.1554, Test: 0.1336\n",
            "Early stopping:  0.23211357324993134\n",
            "Epoch: 022, Loss: 2.4712, Train: 0.1591, Test: 0.1346\n",
            "Early stopping:  0.0852878171598096\n",
            "Epoch: 023, Loss: 2.4530, Train: 0.1719, Test: 0.1533\n",
            "Early stopping:  0.03425266324221103\n",
            "Epoch: 024, Loss: 2.4154, Train: 0.2019, Test: 0.1792\n",
            "Early stopping:  0.03195225771022232\n",
            "Epoch: 025, Loss: 2.3488, Train: 0.2385, Test: 0.2300\n",
            "Early stopping:  0.0538839140574335\n",
            "Epoch: 026, Loss: 2.3083, Train: 0.2700, Test: 0.2621\n",
            "Early stopping:  0.06918109449381478\n",
            "Epoch: 027, Loss: 2.3262, Train: 0.2731, Test: 0.2629\n",
            "Early stopping:  0.06151024254365054\n",
            "Epoch: 028, Loss: 2.3075, Train: 0.2558, Test: 0.2478\n",
            "Early stopping:  0.044755080305986884\n",
            "Epoch: 029, Loss: 2.2748, Train: 0.2328, Test: 0.2219\n",
            "Early stopping:  0.02722567299318919\n",
            "Epoch: 030, Loss: 2.2532, Train: 0.2238, Test: 0.2068\n",
            "Early stopping:  0.029372621434101052\n",
            "Epoch: 031, Loss: 2.2390, Train: 0.2201, Test: 0.2020\n",
            "Early stopping:  0.03644382342707496\n",
            "Epoch: 032, Loss: 2.2267, Train: 0.2204, Test: 0.2045\n",
            "Early stopping:  0.03189020189110633\n",
            "Epoch: 033, Loss: 2.2088, Train: 0.2340, Test: 0.2257\n",
            "Early stopping:  0.025200220968409102\n",
            "Epoch: 034, Loss: 2.1862, Train: 0.2612, Test: 0.2529\n",
            "Early stopping:  0.026169561734510724\n",
            "Epoch: 035, Loss: 2.1587, Train: 0.2898, Test: 0.2803\n",
            "Early stopping:  0.032150758963933164\n",
            "Epoch: 036, Loss: 2.1288, Train: 0.3313, Test: 0.3228\n",
            "Early stopping:  0.03908622581919441\n",
            "Epoch: 037, Loss: 2.1120, Train: 0.3426, Test: 0.3328\n",
            "Early stopping:  0.03982918719836528\n",
            "Epoch: 038, Loss: 2.1031, Train: 0.3511, Test: 0.3366\n",
            "Early stopping:  0.03438856771392078\n",
            "Epoch: 039, Loss: 2.0745, Train: 0.3434, Test: 0.3238\n",
            "Early stopping:  0.03115361679052829\n",
            "Epoch: 040, Loss: 2.0497, Train: 0.3554, Test: 0.3364\n",
            "Early stopping:  0.031447178910098336\n",
            "Epoch: 041, Loss: 2.0129, Train: 0.3528, Test: 0.3372\n",
            "Early stopping:  0.04047910859534093\n",
            "Epoch: 042, Loss: 1.9999, Train: 0.3500, Test: 0.3342\n",
            "Early stopping:  0.0426999088941976\n",
            "Epoch: 043, Loss: 1.9859, Train: 0.3585, Test: 0.3394\n",
            "Early stopping:  0.03664366552283859\n",
            "Epoch: 044, Loss: 1.9668, Train: 0.3664, Test: 0.3478\n",
            "Early stopping:  0.031190536017076016\n",
            "Epoch: 045, Loss: 1.9428, Train: 0.3766, Test: 0.3566\n",
            "Early stopping:  0.027652196512509636\n",
            "Epoch: 046, Loss: 1.9201, Train: 0.3900, Test: 0.3765\n",
            "Early stopping:  0.0322194083094293\n",
            "Epoch: 047, Loss: 1.9021, Train: 0.3922, Test: 0.3712\n",
            "Early stopping:  0.03393014978629979\n",
            "Epoch: 048, Loss: 1.8848, Train: 0.3976, Test: 0.3697\n",
            "Early stopping:  0.03244763427560129\n",
            "Epoch: 049, Loss: 1.8685, Train: 0.4022, Test: 0.3699\n",
            "Early stopping:  0.029121617750263108\n",
            "Epoch: 050, Loss: 1.8524, Train: 0.4039, Test: 0.3721\n",
            "Early stopping:  0.026701205080930712\n",
            "Epoch: 051, Loss: 1.8334, Train: 0.4149, Test: 0.3808\n",
            "Early stopping:  0.02684020483053562\n",
            "Epoch: 052, Loss: 1.8150, Train: 0.4186, Test: 0.3869\n",
            "Early stopping:  0.02765193050423634\n",
            "Epoch: 053, Loss: 1.7988, Train: 0.4214, Test: 0.3922\n",
            "Early stopping:  0.027964022626823598\n",
            "Epoch: 054, Loss: 1.7857, Train: 0.4223, Test: 0.3927\n",
            "Early stopping:  0.026636946398770754\n",
            "Epoch: 055, Loss: 1.7737, Train: 0.4260, Test: 0.3967\n",
            "Early stopping:  0.023596855771297547\n",
            "Epoch: 056, Loss: 1.7629, Train: 0.4291, Test: 0.3988\n",
            "Early stopping:  0.02050291883158067\n",
            "Epoch: 057, Loss: 1.7511, Train: 0.4319, Test: 0.4033\n",
            "Early stopping:  0.01872443566249558\n",
            "Epoch: 058, Loss: 1.7353, Train: 0.4376, Test: 0.4079\n",
            "Early stopping:  0.01957344476900878\n",
            "Epoch: 059, Loss: 1.7208, Train: 0.4416, Test: 0.4150\n",
            "Early stopping:  0.021173648405810645\n",
            "Epoch: 060, Loss: 1.7051, Train: 0.4430, Test: 0.4143\n",
            "Early stopping:  0.023104395863645166\n",
            "Epoch: 061, Loss: 1.6885, Train: 0.4490, Test: 0.4181\n",
            "Early stopping:  0.024584970872464793\n",
            "Epoch: 062, Loss: 1.6720, Train: 0.4512, Test: 0.4179\n",
            "Early stopping:  0.02512987707270392\n",
            "Epoch: 063, Loss: 1.6587, Train: 0.4549, Test: 0.4218\n",
            "Early stopping:  0.024869339304800172\n",
            "Epoch: 064, Loss: 1.6453, Train: 0.4577, Test: 0.4251\n",
            "Early stopping:  0.023639398381464012\n",
            "Epoch: 065, Loss: 1.6344, Train: 0.4592, Test: 0.4258\n",
            "Early stopping:  0.021377198102066303\n",
            "Epoch: 066, Loss: 1.6219, Train: 0.4623, Test: 0.4254\n",
            "Early stopping:  0.01970551497393654\n",
            "Epoch: 067, Loss: 1.6096, Train: 0.4668, Test: 0.4251\n",
            "Early stopping:  0.01922702296088563\n",
            "Epoch: 068, Loss: 1.5995, Train: 0.4714, Test: 0.4307\n",
            "Early stopping:  0.018410518980964213\n",
            "Epoch: 069, Loss: 1.5889, Train: 0.4711, Test: 0.4321\n",
            "Early stopping:  0.017931528131288357\n",
            "Epoch: 070, Loss: 1.5787, Train: 0.4750, Test: 0.4366\n",
            "Early stopping:  0.016942239655586732\n",
            "Epoch: 071, Loss: 1.5673, Train: 0.4770, Test: 0.4362\n",
            "Early stopping:  0.01668241131344041\n",
            "Epoch: 072, Loss: 1.5559, Train: 0.4753, Test: 0.4379\n",
            "Early stopping:  0.01723690691991984\n",
            "Epoch: 073, Loss: 1.5454, Train: 0.4799, Test: 0.4417\n",
            "Early stopping:  0.017375926684319788\n",
            "Epoch: 074, Loss: 1.5355, Train: 0.4830, Test: 0.4445\n",
            "Early stopping:  0.017109191658530042\n",
            "Epoch: 075, Loss: 1.5263, Train: 0.4858, Test: 0.4474\n",
            "Early stopping:  0.016172000295972946\n",
            "Epoch: 076, Loss: 1.5175, Train: 0.4872, Test: 0.4479\n",
            "Early stopping:  0.015161536506338964\n",
            "Epoch: 077, Loss: 1.5074, Train: 0.4938, Test: 0.4491\n",
            "Early stopping:  0.01488857453708988\n",
            "Epoch: 078, Loss: 1.4989, Train: 0.4940, Test: 0.4494\n",
            "Early stopping:  0.014580705248359783\n",
            "Epoch: 079, Loss: 1.4897, Train: 0.4969, Test: 0.4511\n",
            "Early stopping:  0.014535133849500252\n",
            "Epoch: 080, Loss: 1.4812, Train: 0.5009, Test: 0.4519\n",
            "Early stopping:  0.01428136267398988\n",
            "Epoch: 081, Loss: 1.4709, Train: 0.5043, Test: 0.4527\n",
            "Early stopping:  0.0143573214570314\n",
            "Epoch: 082, Loss: 1.4606, Train: 0.5068, Test: 0.4555\n",
            "Early stopping:  0.015100827180491624\n",
            "Epoch: 083, Loss: 1.4513, Train: 0.5074, Test: 0.4566\n",
            "Early stopping:  0.01538367119974374\n",
            "Epoch: 084, Loss: 1.4406, Train: 0.5079, Test: 0.4583\n",
            "Early stopping:  0.01591711984100501\n",
            "Epoch: 085, Loss: 1.4317, Train: 0.5133, Test: 0.4608\n",
            "Early stopping:  0.01555492984926265\n",
            "Epoch: 086, Loss: 1.4203, Train: 0.5156, Test: 0.4610\n",
            "Early stopping:  0.01588278495062673\n",
            "Epoch: 087, Loss: 1.4145, Train: 0.5162, Test: 0.4655\n",
            "Early stopping:  0.014932067742128022\n",
            "Epoch: 088, Loss: 1.4088, Train: 0.5218, Test: 0.4672\n",
            "Early stopping:  0.01291874080010106\n",
            "Epoch: 089, Loss: 1.3941, Train: 0.5193, Test: 0.4610\n",
            "Early stopping:  0.013925989937326907\n",
            "Epoch: 090, Loss: 1.3991, Train: 0.5241, Test: 0.4723\n",
            "Early stopping:  0.010784541684902529\n",
            "Epoch: 091, Loss: 1.3877, Train: 0.5250, Test: 0.4738\n",
            "Early stopping:  0.010870882048813025\n",
            "Epoch: 092, Loss: 1.3805, Train: 0.5298, Test: 0.4733\n",
            "Early stopping:  0.010785405930639183\n",
            "Epoch: 093, Loss: 1.3642, Train: 0.5335, Test: 0.4717\n",
            "Early stopping:  0.01360492013224083\n",
            "Epoch: 094, Loss: 1.3606, Train: 0.5352, Test: 0.4763\n",
            "Early stopping:  0.016102507838890338\n",
            "Epoch: 095, Loss: 1.3549, Train: 0.5400, Test: 0.4774\n",
            "Early stopping:  0.013911883374675608\n",
            "Epoch: 096, Loss: 1.3407, Train: 0.5357, Test: 0.4759\n",
            "Early stopping:  0.01449143305209554\n",
            "Epoch: 097, Loss: 1.3399, Train: 0.5428, Test: 0.4799\n",
            "Early stopping:  0.011244295457155409\n",
            "Epoch: 098, Loss: 1.3212, Train: 0.5417, Test: 0.4791\n",
            "Early stopping:  0.01533084310155436\n",
            "Epoch: 099, Loss: 1.3229, Train: 0.5448, Test: 0.4804\n",
            "Early stopping:  0.014013886398077671\n",
            "Epoch: 100, Loss: 1.3115, Train: 0.5479, Test: 0.4821\n",
            "Early stopping:  0.01268761816904607\n",
            "Epoch: 101, Loss: 1.3096, Train: 0.5496, Test: 0.4889\n",
            "Early stopping:  0.01205160073103857\n",
            "Epoch: 102, Loss: 1.2996, Train: 0.5510, Test: 0.4857\n",
            "Early stopping:  0.009457688354228526\n",
            "PREDICTIONS -> tensor([ 3,  6,  3,  ..., 11,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.39      0.48      0.43       568\n",
            "         capital_goods       0.27      0.08      0.13       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.37      0.49      0.42       595\n",
            " consumer_non-cyclical       0.58      0.37      0.45       334\n",
            "                energy       0.49      0.40      0.44       213\n",
            "             financial       0.69      0.53      0.60       576\n",
            "            healthcare       0.71      0.15      0.25       238\n",
            "              services       0.50      0.77      0.61      1557\n",
            "            technology       0.19      0.01      0.02       297\n",
            "        transportation       0.60      0.64      0.62       303\n",
            "             utilities       0.36      0.15      0.21       169\n",
            "\n",
            "              accuracy                           0.49      5291\n",
            "             macro avg       0.43      0.34      0.35      5291\n",
            "          weighted avg       0.47      0.49      0.45      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 151.2359, Train: 0.0709, Test: 0.0711\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 334.6982, Train: 0.1117, Test: 0.1121\n",
            "Early stopping:  129.72747642126072\n",
            "Epoch: 003, Loss: 388.5026, Train: 0.3134, Test: 0.3139\n",
            "Early stopping:  124.39776102464516\n",
            "Epoch: 004, Loss: 434.2994, Train: 0.1242, Test: 0.1170\n",
            "Early stopping:  124.16102399509967\n",
            "Epoch: 005, Loss: 586.6841, Train: 0.0726, Test: 0.0663\n",
            "Early stopping:  158.2088246102898\n",
            "Epoch: 006, Loss: 615.4060, Train: 0.1245, Test: 0.1206\n",
            "Early stopping:  123.45305876271122\n",
            "Epoch: 007, Loss: 631.4198, Train: 0.1103, Test: 0.1077\n",
            "Early stopping:  111.764723299568\n",
            "Epoch: 008, Loss: 636.4448, Train: 0.2947, Test: 0.2939\n",
            "Early stopping:  84.19222042823868\n",
            "Epoch: 009, Loss: 655.3168, Train: 0.2853, Test: 0.2833\n",
            "Early stopping:  25.746082645103353\n",
            "Epoch: 010, Loss: 601.1988, Train: 0.1194, Test: 0.1196\n",
            "Early stopping:  20.652625782894997\n",
            "Epoch: 011, Loss: 521.0013, Train: 0.2984, Test: 0.2943\n",
            "Early stopping:  52.92853441515662\n",
            "Epoch: 012, Loss: 367.7342, Train: 0.1699, Test: 0.1676\n",
            "Early stopping:  117.30993506881103\n",
            "Epoch: 013, Loss: 321.3814, Train: 0.2144, Test: 0.2043\n",
            "Early stopping:  144.89802081103184\n",
            "Epoch: 014, Loss: 222.0791, Train: 0.3142, Test: 0.3073\n",
            "Early stopping:  153.11519777596158\n",
            "Epoch: 015, Loss: 132.6605, Train: 0.2632, Test: 0.2710\n",
            "Early stopping:  147.58012239838598\n",
            "Epoch: 016, Loss: 119.5957, Train: 0.2337, Test: 0.2327\n",
            "Early stopping:  110.69316270825574\n",
            "Epoch: 017, Loss: 105.6244, Train: 0.2394, Test: 0.2397\n",
            "Early stopping:  91.07746671355169\n",
            "Epoch: 018, Loss: 81.0874, Train: 0.2212, Test: 0.2255\n",
            "Early stopping:  53.749610616039035\n",
            "Epoch: 019, Loss: 63.9399, Train: 0.2490, Test: 0.2580\n",
            "Early stopping:  28.011877494344564\n",
            "Epoch: 020, Loss: 44.8195, Train: 0.2915, Test: 0.2863\n",
            "Early stopping:  30.31420933821243\n",
            "Epoch: 021, Loss: 27.6954, Train: 0.2584, Test: 0.2521\n",
            "Early stopping:  30.448646403219094\n",
            "Epoch: 022, Loss: 14.7429, Train: 0.2076, Test: 0.1918\n",
            "Early stopping:  26.76462297880956\n",
            "Epoch: 023, Loss: 8.2187, Train: 0.1466, Test: 0.1289\n",
            "Early stopping:  22.728357035643494\n",
            "Epoch: 024, Loss: 5.0996, Train: 0.1273, Test: 0.1087\n",
            "Early stopping:  16.306590398587698\n",
            "Epoch: 025, Loss: 3.4735, Train: 0.1472, Test: 0.1327\n",
            "Early stopping:  9.853508977544498\n",
            "Epoch: 026, Loss: 2.8232, Train: 0.1843, Test: 0.1710\n",
            "Early stopping:  4.869514683804296\n",
            "Epoch: 027, Loss: 2.4773, Train: 0.1866, Test: 0.1720\n",
            "Early stopping:  2.3511630070923926\n",
            "Epoch: 028, Loss: 2.3609, Train: 0.1951, Test: 0.1805\n",
            "Early stopping:  1.122556130634615\n",
            "Epoch: 029, Loss: 2.3314, Train: 0.1903, Test: 0.1731\n",
            "Early stopping:  0.47792568297607047\n",
            "Epoch: 030, Loss: 2.3410, Train: 0.1892, Test: 0.1741\n",
            "Early stopping:  0.20766520206499467\n",
            "Epoch: 031, Loss: 2.3505, Train: 0.2300, Test: 0.2141\n",
            "Early stopping:  0.05976623318039247\n",
            "Epoch: 032, Loss: 2.3523, Train: 0.2348, Test: 0.2228\n",
            "Early stopping:  0.011326556786450332\n",
            "Epoch: 033, Loss: 2.3502, Train: 0.2394, Test: 0.2283\n",
            "Early stopping:  0.008823688180045504\n",
            "PREDICTIONS -> tensor([ 3,  8,  7,  ..., 10,  3,  3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       568\n",
            "         capital_goods       0.00      0.00      0.00       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.14      0.60      0.23       595\n",
            " consumer_non-cyclical       0.55      0.19      0.28       334\n",
            "                energy       0.07      0.04      0.05       213\n",
            "             financial       0.94      0.03      0.05       576\n",
            "            healthcare       0.17      0.58      0.26       238\n",
            "              services       0.40      0.33      0.36      1557\n",
            "            technology       0.00      0.00      0.00       297\n",
            "        transportation       0.30      0.37      0.33       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.23      5291\n",
            "             macro avg       0.21      0.18      0.13      5291\n",
            "          weighted avg       0.30      0.23      0.19      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 62.8202, Train: 0.0570, Test: 0.0578\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 613.8536, Train: 0.2941, Test: 0.2948\n",
            "Early stopping:  389.639414392369\n",
            "Epoch: 003, Loss: 568.1838, Train: 0.0587, Test: 0.0550\n",
            "Early stopping:  305.80925132713764\n",
            "Epoch: 004, Loss: 518.6346, Train: 0.0533, Test: 0.0542\n",
            "Early stopping:  255.017040357249\n",
            "Epoch: 005, Loss: 637.6130, Train: 0.1622, Test: 0.1588\n",
            "Early stopping:  237.73218814700758\n",
            "Epoch: 006, Loss: 536.9227, Train: 0.1220, Test: 0.1191\n",
            "Early stopping:  50.257806795254226\n",
            "Epoch: 007, Loss: 494.7418, Train: 0.0644, Test: 0.0626\n",
            "Early stopping:  55.24387130972411\n",
            "Epoch: 008, Loss: 443.4964, Train: 0.1767, Test: 0.1669\n",
            "Early stopping:  71.43974578475317\n",
            "Epoch: 009, Loss: 352.4517, Train: 0.2981, Test: 0.2956\n",
            "Early stopping:  106.12444185233323\n",
            "Epoch: 010, Loss: 350.9095, Train: 0.2969, Test: 0.2954\n",
            "Early stopping:  83.5350960527516\n",
            "Epoch: 011, Loss: 308.9616, Train: 0.1151, Test: 0.1119\n",
            "Early stopping:  76.38357227827606\n",
            "Epoch: 012, Loss: 259.5575, Train: 0.2584, Test: 0.2533\n",
            "Early stopping:  67.77289556580902\n",
            "Epoch: 013, Loss: 180.5847, Train: 0.3020, Test: 0.3018\n",
            "Early stopping:  72.22700137072017\n",
            "Epoch: 014, Loss: 124.6261, Train: 0.3083, Test: 0.3050\n",
            "Early stopping:  92.41076821440357\n",
            "Epoch: 015, Loss: 66.3144, Train: 0.3071, Test: 0.3018\n",
            "Early stopping:  98.27358487969273\n",
            "Epoch: 016, Loss: 16.7761, Train: 0.1733, Test: 0.1625\n",
            "Early stopping:  95.19376857097096\n",
            "Epoch: 017, Loss: 3.9049, Train: 0.1171, Test: 0.1038\n",
            "Early stopping:  74.28265852705229\n",
            "Epoch: 018, Loss: 3.3146, Train: 0.1185, Test: 0.1040\n",
            "Early stopping:  52.43433132018558\n",
            "Epoch: 019, Loss: 3.0320, Train: 0.1154, Test: 0.0966\n",
            "Early stopping:  27.25766204721953\n",
            "Epoch: 020, Loss: 2.8788, Train: 0.0956, Test: 0.0875\n",
            "Early stopping:  6.047189566829455\n",
            "Epoch: 021, Loss: 2.7746, Train: 0.0882, Test: 0.0777\n",
            "Early stopping:  0.4529265447040052\n",
            "Epoch: 022, Loss: 2.6824, Train: 0.0822, Test: 0.0748\n",
            "Early stopping:  0.24811876327457083\n",
            "Epoch: 023, Loss: 2.5922, Train: 0.0822, Test: 0.0779\n",
            "Early stopping:  0.17131903711110053\n",
            "Epoch: 024, Loss: 2.5738, Train: 0.0803, Test: 0.0745\n",
            "Early stopping:  0.12784759783150812\n",
            "Epoch: 025, Loss: 2.5908, Train: 0.0757, Test: 0.0699\n",
            "Early stopping:  0.08507527361774755\n",
            "Epoch: 026, Loss: 2.5960, Train: 0.0817, Test: 0.0739\n",
            "Early stopping:  0.04296031101657527\n",
            "Epoch: 027, Loss: 2.5831, Train: 0.0817, Test: 0.0739\n",
            "Early stopping:  0.00881223672867265\n",
            "PREDICTIONS -> tensor([ 9, 11,  9,  ...,  4, 11, 11], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       568\n",
            "         capital_goods       0.00      0.00      0.00       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.10      0.00      0.00       595\n",
            " consumer_non-cyclical       0.24      0.36      0.29       334\n",
            "                energy       0.00      0.00      0.00       213\n",
            "             financial       0.33      0.02      0.04       576\n",
            "            healthcare       0.00      0.00      0.00       238\n",
            "              services       0.37      0.04      0.07      1557\n",
            "            technology       0.26      0.15      0.19       297\n",
            "        transportation       0.00      0.00      0.00       303\n",
            "             utilities       0.04      0.93      0.07       169\n",
            "\n",
            "              accuracy                           0.07      5291\n",
            "             macro avg       0.11      0.13      0.05      5291\n",
            "          weighted avg       0.19      0.07      0.05      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 85.4547, Train: 0.0874, Test: 0.0816\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 380.8143, Train: 0.1540, Test: 0.1497\n",
            "Early stopping:  208.85077879341873\n",
            "Epoch: 003, Loss: 425.6783, Train: 0.2952, Test: 0.2954\n",
            "Early stopping:  184.84322851075922\n",
            "Epoch: 004, Loss: 699.9104, Train: 0.2947, Test: 0.2948\n",
            "Early stopping:  251.5921734904811\n",
            "Epoch: 005, Loss: 593.3062, Train: 0.0593, Test: 0.0586\n",
            "Early stopping:  234.74592424217985\n",
            "Epoch: 006, Loss: 675.6668, Train: 0.0749, Test: 0.0663\n",
            "Early stopping:  144.99276027674154\n",
            "Epoch: 007, Loss: 485.5388, Train: 0.1183, Test: 0.1159\n",
            "Early stopping:  118.70828441711858\n",
            "Epoch: 008, Loss: 509.5958, Train: 0.1242, Test: 0.1170\n",
            "Early stopping:  95.8745483304958\n",
            "Epoch: 009, Loss: 369.5695, Train: 0.0683, Test: 0.0709\n",
            "Early stopping:  115.43861027954587\n",
            "Epoch: 010, Loss: 353.9471, Train: 0.0542, Test: 0.0493\n",
            "Early stopping:  129.68216930073024\n",
            "Epoch: 011, Loss: 290.3450, Train: 0.0709, Test: 0.0652\n",
            "Early stopping:  92.71332299293584\n",
            "Epoch: 012, Loss: 169.2320, Train: 0.0953, Test: 0.0900\n",
            "Early stopping:  123.94378197244971\n",
            "Epoch: 013, Loss: 101.1162, Train: 0.2470, Test: 0.2470\n",
            "Early stopping:  117.45783048193387\n",
            "Epoch: 014, Loss: 75.5795, Train: 0.2796, Test: 0.2799\n",
            "Early stopping:  120.46568841901495\n",
            "Epoch: 015, Loss: 57.7366, Train: 0.2898, Test: 0.2878\n",
            "Early stopping:  94.7103034045703\n",
            "Epoch: 016, Loss: 43.4979, Train: 0.2621, Test: 0.2563\n",
            "Early stopping:  49.525245261085836\n",
            "Epoch: 017, Loss: 30.4727, Train: 0.2238, Test: 0.2177\n",
            "Early stopping:  27.696777363927094\n",
            "Epoch: 018, Loss: 19.1828, Train: 0.2672, Test: 0.2504\n",
            "Early stopping:  22.23015831997762\n",
            "Epoch: 019, Loss: 8.1254, Train: 0.2354, Test: 0.2223\n",
            "Early stopping:  19.563915859853513\n",
            "Epoch: 020, Loss: 4.9082, Train: 0.2399, Test: 0.2242\n",
            "Early stopping:  15.996955201206625\n",
            "Epoch: 021, Loss: 4.1152, Train: 0.2558, Test: 0.2374\n",
            "Early stopping:  11.303212353749174\n",
            "Epoch: 022, Loss: 3.5670, Train: 0.2765, Test: 0.2601\n",
            "Early stopping:  6.507145585429322\n",
            "Epoch: 023, Loss: 3.1214, Train: 0.3083, Test: 0.2892\n",
            "Early stopping:  1.991967000382972\n",
            "Epoch: 024, Loss: 2.7919, Train: 0.3406, Test: 0.3185\n",
            "Early stopping:  0.8380431832670411\n",
            "Epoch: 025, Loss: 2.5695, Train: 0.3568, Test: 0.3330\n",
            "Early stopping:  0.6199194744388701\n",
            "Epoch: 026, Loss: 2.4339, Train: 0.3554, Test: 0.3249\n",
            "Early stopping:  0.45607492564074625\n",
            "Epoch: 027, Loss: 2.3379, Train: 0.3321, Test: 0.3067\n",
            "Early stopping:  0.3134243006140792\n",
            "Epoch: 028, Loss: 2.2729, Train: 0.3559, Test: 0.3289\n",
            "Early stopping:  0.20644261048591128\n",
            "Epoch: 029, Loss: 2.2052, Train: 0.3596, Test: 0.3396\n",
            "Early stopping:  0.142574685361518\n",
            "Epoch: 030, Loss: 2.1664, Train: 0.3684, Test: 0.3464\n",
            "Early stopping:  0.10669329791342728\n",
            "Epoch: 031, Loss: 2.1321, Train: 0.3690, Test: 0.3546\n",
            "Early stopping:  0.08287364574094408\n",
            "Epoch: 032, Loss: 2.0999, Train: 0.3667, Test: 0.3538\n",
            "Early stopping:  0.0671685917600496\n",
            "Epoch: 033, Loss: 2.0908, Train: 0.3690, Test: 0.3514\n",
            "Early stopping:  0.04750781263398766\n",
            "Epoch: 034, Loss: 2.0633, Train: 0.3670, Test: 0.3515\n",
            "Early stopping:  0.03974044930962859\n",
            "Epoch: 035, Loss: 2.0520, Train: 0.3670, Test: 0.3532\n",
            "Early stopping:  0.03161130138897971\n",
            "Epoch: 036, Loss: 2.0352, Train: 0.3721, Test: 0.3589\n",
            "Early stopping:  0.02687431738934444\n",
            "Epoch: 037, Loss: 2.0124, Train: 0.3775, Test: 0.3617\n",
            "Early stopping:  0.029470935860703096\n",
            "Epoch: 038, Loss: 1.9959, Train: 0.3800, Test: 0.3623\n",
            "Early stopping:  0.027752950859079816\n",
            "Epoch: 039, Loss: 1.9788, Train: 0.3783, Test: 0.3663\n",
            "Early stopping:  0.029422920200193305\n",
            "Epoch: 040, Loss: 1.9583, Train: 0.3800, Test: 0.3657\n",
            "Early stopping:  0.029686420829992514\n",
            "Epoch: 041, Loss: 1.9476, Train: 0.3820, Test: 0.3644\n",
            "Early stopping:  0.0265277280431095\n",
            "Epoch: 042, Loss: 1.9343, Train: 0.3840, Test: 0.3614\n",
            "Early stopping:  0.02457403633990149\n",
            "Epoch: 043, Loss: 1.9189, Train: 0.3815, Test: 0.3595\n",
            "Early stopping:  0.022847003574423\n",
            "Epoch: 044, Loss: 1.9085, Train: 0.3846, Test: 0.3657\n",
            "Early stopping:  0.020307725972600003\n",
            "Epoch: 045, Loss: 1.8945, Train: 0.3962, Test: 0.3744\n",
            "Early stopping:  0.020893442487489013\n",
            "Epoch: 046, Loss: 1.8796, Train: 0.4013, Test: 0.3810\n",
            "Early stopping:  0.021172293370190453\n",
            "Epoch: 047, Loss: 1.8678, Train: 0.4019, Test: 0.3833\n",
            "Early stopping:  0.020780739369456364\n",
            "Epoch: 048, Loss: 1.8528, Train: 0.4064, Test: 0.3892\n",
            "Early stopping:  0.02185030918911456\n",
            "Epoch: 049, Loss: 1.8417, Train: 0.4121, Test: 0.3935\n",
            "Early stopping:  0.02096138660071811\n",
            "Epoch: 050, Loss: 1.8279, Train: 0.4166, Test: 0.3994\n",
            "Early stopping:  0.020518756968360814\n",
            "Epoch: 051, Loss: 1.8142, Train: 0.4161, Test: 0.4003\n",
            "Early stopping:  0.02091257148718934\n",
            "Epoch: 052, Loss: 1.8028, Train: 0.4192, Test: 0.4033\n",
            "Early stopping:  0.020168379856284384\n",
            "Epoch: 053, Loss: 1.7878, Train: 0.4209, Test: 0.4099\n",
            "Early stopping:  0.021019013114284224\n",
            "Epoch: 054, Loss: 1.7755, Train: 0.4223, Test: 0.4103\n",
            "Early stopping:  0.020732490478823057\n",
            "Epoch: 055, Loss: 1.7627, Train: 0.4234, Test: 0.4133\n",
            "Early stopping:  0.020602539950038728\n",
            "Epoch: 056, Loss: 1.7494, Train: 0.4288, Test: 0.4164\n",
            "Early stopping:  0.020871452878512282\n",
            "Epoch: 057, Loss: 1.7388, Train: 0.4334, Test: 0.4194\n",
            "Early stopping:  0.01961730312862751\n",
            "Epoch: 058, Loss: 1.7266, Train: 0.4339, Test: 0.4205\n",
            "Early stopping:  0.01926731086896803\n",
            "Epoch: 059, Loss: 1.7176, Train: 0.4362, Test: 0.4215\n",
            "Early stopping:  0.017888147043741816\n",
            "Epoch: 060, Loss: 1.7071, Train: 0.4416, Test: 0.4241\n",
            "Early stopping:  0.016736439418848528\n",
            "Epoch: 061, Loss: 1.6979, Train: 0.4441, Test: 0.4243\n",
            "Early stopping:  0.016054959148416093\n",
            "Epoch: 062, Loss: 1.6873, Train: 0.4453, Test: 0.4247\n",
            "Early stopping:  0.01556733436026889\n",
            "Epoch: 063, Loss: 1.6774, Train: 0.4467, Test: 0.4275\n",
            "Early stopping:  0.015881210314630995\n",
            "Epoch: 064, Loss: 1.6676, Train: 0.4498, Test: 0.4313\n",
            "Early stopping:  0.01574355214965654\n",
            "Epoch: 065, Loss: 1.6577, Train: 0.4526, Test: 0.4353\n",
            "Early stopping:  0.01580549702390453\n",
            "Epoch: 066, Loss: 1.6477, Train: 0.4563, Test: 0.4375\n",
            "Early stopping:  0.015612828441086307\n",
            "Epoch: 067, Loss: 1.6382, Train: 0.4600, Test: 0.4402\n",
            "Early stopping:  0.015522829299374936\n",
            "Epoch: 068, Loss: 1.6276, Train: 0.4645, Test: 0.4443\n",
            "Early stopping:  0.01573309790274204\n",
            "Epoch: 069, Loss: 1.6183, Train: 0.4705, Test: 0.4457\n",
            "Early stopping:  0.01564842972703876\n",
            "Epoch: 070, Loss: 1.6077, Train: 0.4731, Test: 0.4493\n",
            "Early stopping:  0.015819417676141707\n",
            "Epoch: 071, Loss: 1.5984, Train: 0.4790, Test: 0.4534\n",
            "Early stopping:  0.01574768824450051\n",
            "Epoch: 072, Loss: 1.5870, Train: 0.4810, Test: 0.4536\n",
            "Early stopping:  0.015986268391169116\n",
            "Epoch: 073, Loss: 1.5766, Train: 0.4816, Test: 0.4551\n",
            "Early stopping:  0.016452841043439314\n",
            "Epoch: 074, Loss: 1.5666, Train: 0.4858, Test: 0.4591\n",
            "Early stopping:  0.016439291271429284\n",
            "Epoch: 075, Loss: 1.5541, Train: 0.4915, Test: 0.4642\n",
            "Early stopping:  0.017240147713330236\n",
            "Epoch: 076, Loss: 1.5457, Train: 0.4974, Test: 0.4670\n",
            "Early stopping:  0.016656234994075703\n",
            "Epoch: 077, Loss: 1.5335, Train: 0.4991, Test: 0.4693\n",
            "Early stopping:  0.016950108270687693\n",
            "Epoch: 078, Loss: 1.5249, Train: 0.5048, Test: 0.4725\n",
            "Early stopping:  0.01646467632366998\n",
            "Epoch: 079, Loss: 1.5182, Train: 0.5068, Test: 0.4770\n",
            "Early stopping:  0.014694336758797532\n",
            "Epoch: 080, Loss: 1.5010, Train: 0.5045, Test: 0.4774\n",
            "Early stopping:  0.01675007086639367\n",
            "Epoch: 081, Loss: 1.4970, Train: 0.5130, Test: 0.4774\n",
            "Early stopping:  0.015601560264130463\n",
            "Epoch: 082, Loss: 1.4876, Train: 0.5187, Test: 0.4835\n",
            "Early stopping:  0.015447714018687709\n",
            "Epoch: 083, Loss: 1.4738, Train: 0.5182, Test: 0.4821\n",
            "Early stopping:  0.016444246482242995\n",
            "Epoch: 084, Loss: 1.4706, Train: 0.5258, Test: 0.4878\n",
            "Early stopping:  0.013540763818151034\n",
            "Epoch: 085, Loss: 1.4516, Train: 0.5275, Test: 0.4876\n",
            "Early stopping:  0.01734849638176861\n",
            "Epoch: 086, Loss: 1.4474, Train: 0.5303, Test: 0.4880\n",
            "Early stopping:  0.016570061269311064\n",
            "Epoch: 087, Loss: 1.4309, Train: 0.5306, Test: 0.4889\n",
            "Early stopping:  0.01763953050756739\n",
            "Epoch: 088, Loss: 1.4274, Train: 0.5346, Test: 0.4910\n",
            "Early stopping:  0.017392056829537546\n",
            "Epoch: 089, Loss: 1.4215, Train: 0.5417, Test: 0.4937\n",
            "Early stopping:  0.013083417357261212\n",
            "Epoch: 090, Loss: 1.4032, Train: 0.5380, Test: 0.4922\n",
            "Early stopping:  0.016026746930066463\n",
            "Epoch: 091, Loss: 1.4022, Train: 0.5434, Test: 0.4984\n",
            "Early stopping:  0.01352911819490692\n",
            "Epoch: 092, Loss: 1.3809, Train: 0.5493, Test: 0.4991\n",
            "Early stopping:  0.018312967234512336\n",
            "Epoch: 093, Loss: 1.3777, Train: 0.5533, Test: 0.4991\n",
            "Early stopping:  0.018001168441566923\n",
            "Epoch: 094, Loss: 1.3626, Train: 0.5542, Test: 0.4991\n",
            "Early stopping:  0.017291825886919375\n",
            "Epoch: 095, Loss: 1.3537, Train: 0.5587, Test: 0.5012\n",
            "Early stopping:  0.018640265209130334\n",
            "Epoch: 096, Loss: 1.3435, Train: 0.5632, Test: 0.5026\n",
            "Early stopping:  0.01584126005039415\n",
            "Epoch: 097, Loss: 1.3326, Train: 0.5664, Test: 0.5024\n",
            "Early stopping:  0.01734923197520518\n",
            "Epoch: 098, Loss: 1.3247, Train: 0.5715, Test: 0.5056\n",
            "Early stopping:  0.015341403347241546\n",
            "Epoch: 099, Loss: 1.3134, Train: 0.5743, Test: 0.5073\n",
            "Early stopping:  0.015707649437159552\n",
            "Epoch: 100, Loss: 1.3066, Train: 0.5777, Test: 0.5065\n",
            "Early stopping:  0.014723041304597718\n",
            "Epoch: 101, Loss: 1.2940, Train: 0.5749, Test: 0.5041\n",
            "Early stopping:  0.015108743994026213\n",
            "Epoch: 102, Loss: 1.2921, Train: 0.5825, Test: 0.5080\n",
            "Early stopping:  0.01360222111675193\n",
            "Epoch: 103, Loss: 1.2787, Train: 0.5831, Test: 0.5143\n",
            "Early stopping:  0.013499433935220108\n",
            "Epoch: 104, Loss: 1.2743, Train: 0.5834, Test: 0.5133\n",
            "Early stopping:  0.012909593427331365\n",
            "Epoch: 105, Loss: 1.2620, Train: 0.5831, Test: 0.5092\n",
            "Early stopping:  0.013262806463055438\n",
            "Epoch: 106, Loss: 1.2604, Train: 0.5902, Test: 0.5152\n",
            "Early stopping:  0.013050398107202066\n",
            "Epoch: 107, Loss: 1.2444, Train: 0.5891, Test: 0.5152\n",
            "Early stopping:  0.013483699051707675\n",
            "Epoch: 108, Loss: 1.2417, Train: 0.5930, Test: 0.5173\n",
            "Early stopping:  0.013503749031779314\n",
            "Epoch: 109, Loss: 1.2318, Train: 0.5967, Test: 0.5216\n",
            "Early stopping:  0.01286606051158974\n",
            "Epoch: 110, Loss: 1.2218, Train: 0.5930, Test: 0.5245\n",
            "Early stopping:  0.014431684800692223\n",
            "Epoch: 111, Loss: 1.2147, Train: 0.5981, Test: 0.5220\n",
            "Early stopping:  0.012693118821933592\n",
            "Epoch: 112, Loss: 1.2041, Train: 0.6012, Test: 0.5222\n",
            "Early stopping:  0.01462327071783993\n",
            "Epoch: 113, Loss: 1.2016, Train: 0.6049, Test: 0.5266\n",
            "Early stopping:  0.012539029041612512\n",
            "Epoch: 114, Loss: 1.1891, Train: 0.6052, Test: 0.5243\n",
            "Early stopping:  0.012584730762899548\n",
            "Epoch: 115, Loss: 1.1982, Train: 0.6047, Test: 0.5230\n",
            "Early stopping:  0.009271194891960084\n",
            "PREDICTIONS -> tensor([ 8,  0,  8,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.46      0.57      0.51       568\n",
            "         capital_goods       0.30      0.07      0.11       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.40      0.46      0.43       595\n",
            " consumer_non-cyclical       0.69      0.37      0.48       334\n",
            "                energy       0.52      0.46      0.49       213\n",
            "             financial       0.69      0.61      0.65       576\n",
            "            healthcare       0.75      0.48      0.59       238\n",
            "              services       0.51      0.72      0.60      1557\n",
            "            technology       0.42      0.20      0.27       297\n",
            "        transportation       0.63      0.59      0.61       303\n",
            "             utilities       0.57      0.54      0.56       169\n",
            "\n",
            "              accuracy                           0.52      5291\n",
            "             macro avg       0.50      0.42      0.44      5291\n",
            "          weighted avg       0.52      0.52      0.50      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 71.7330, Train: 0.1018, Test: 0.1009\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 617.6915, Train: 0.1171, Test: 0.1134\n",
            "Early stopping:  386.05094943950326\n",
            "Epoch: 003, Loss: 818.9528, Train: 0.0448, Test: 0.0406\n",
            "Early stopping:  386.63380136763385\n",
            "Epoch: 004, Loss: 821.1454, Train: 0.0655, Test: 0.0658\n",
            "Early stopping:  353.54531043371975\n",
            "Epoch: 005, Loss: 708.7947, Train: 0.1645, Test: 0.1608\n",
            "Early stopping:  311.35480277481105\n",
            "Epoch: 006, Loss: 629.4444, Train: 0.0459, Test: 0.0450\n",
            "Early stopping:  98.50356318913511\n",
            "Epoch: 007, Loss: 606.5978, Train: 0.1117, Test: 0.1140\n",
            "Early stopping:  101.44283947647357\n",
            "Epoch: 008, Loss: 409.4583, Train: 0.0542, Test: 0.0516\n",
            "Early stopping:  151.49453006317236\n",
            "Epoch: 009, Loss: 279.5617, Train: 0.1066, Test: 0.1041\n",
            "Early stopping:  176.7211791898119\n",
            "Epoch: 010, Loss: 148.2685, Train: 0.2805, Test: 0.2827\n",
            "Early stopping:  207.49429466785782\n",
            "Epoch: 011, Loss: 108.4346, Train: 0.2887, Test: 0.2905\n",
            "Early stopping:  203.45351729506353\n",
            "Epoch: 012, Loss: 77.6155, Train: 0.2969, Test: 0.2975\n",
            "Early stopping:  137.96032979843378\n",
            "Epoch: 013, Loss: 56.5367, Train: 0.2501, Test: 0.2597\n",
            "Early stopping:  88.35314184848464\n",
            "Epoch: 014, Loss: 40.5911, Train: 0.2065, Test: 0.2049\n",
            "Early stopping:  42.955218783572676\n",
            "Epoch: 015, Loss: 26.0672, Train: 0.2521, Test: 0.2449\n",
            "Early stopping:  32.32356753294982\n",
            "Epoch: 016, Loss: 11.0475, Train: 0.2385, Test: 0.2345\n",
            "Early stopping:  25.94700554998041\n",
            "Epoch: 017, Loss: 3.2547, Train: 0.1682, Test: 0.1654\n",
            "Early stopping:  21.65109648025624\n",
            "Epoch: 018, Loss: 2.4631, Train: 0.1583, Test: 0.1478\n",
            "Early stopping:  16.385559209461903\n",
            "Epoch: 019, Loss: 2.3417, Train: 0.1588, Test: 0.1523\n",
            "Early stopping:  10.192506018995491\n",
            "Epoch: 020, Loss: 2.3251, Train: 0.1597, Test: 0.1525\n",
            "Early stopping:  3.799001267261258\n",
            "Epoch: 021, Loss: 2.3284, Train: 0.1639, Test: 0.1578\n",
            "Early stopping:  0.40219116517064124\n",
            "Epoch: 022, Loss: 2.3317, Train: 0.1713, Test: 0.1603\n",
            "Early stopping:  0.05908305599068776\n",
            "Epoch: 023, Loss: 2.3294, Train: 0.1804, Test: 0.1688\n",
            "Early stopping:  0.006322419026983662\n",
            "PREDICTIONS -> tensor([4, 8, 4,  ..., 4, 4, 4], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       568\n",
            "         capital_goods       0.00      0.00      0.00       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.00      0.00      0.00       595\n",
            " consumer_non-cyclical       0.07      0.71      0.13       334\n",
            "                energy       0.00      0.00      0.00       213\n",
            "             financial       0.00      0.00      0.00       576\n",
            "            healthcare       0.34      0.31      0.32       238\n",
            "              services       0.52      0.30      0.38      1557\n",
            "            technology       0.16      0.09      0.12       297\n",
            "        transportation       0.16      0.28      0.20       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.17      5291\n",
            "             macro avg       0.10      0.14      0.10      5291\n",
            "          weighted avg       0.19      0.17      0.15      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 110.5147, Train: 0.0562, Test: 0.0554\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 413.7364, Train: 0.0332, Test: 0.0295\n",
            "Early stopping:  214.41009512021324\n",
            "Epoch: 003, Loss: 360.5261, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  161.90553289207486\n",
            "Epoch: 004, Loss: 418.6209, Train: 0.1736, Test: 0.1627\n",
            "Early stopping:  145.94767085363142\n",
            "Epoch: 005, Loss: 267.5290, Train: 0.1174, Test: 0.1187\n",
            "Early stopping:  129.05734492543817\n",
            "Epoch: 006, Loss: 381.6866, Train: 0.0675, Test: 0.0637\n",
            "Early stopping:  61.21888688715077\n",
            "Epoch: 007, Loss: 339.3041, Train: 0.1886, Test: 0.1756\n",
            "Early stopping:  56.29632907106057\n",
            "Epoch: 008, Loss: 305.5415, Train: 0.2975, Test: 0.2965\n",
            "Early stopping:  59.847221682292236\n",
            "Epoch: 009, Loss: 356.5915, Train: 0.2927, Test: 0.2909\n",
            "Early stopping:  44.62603586849075\n",
            "Epoch: 010, Loss: 339.7581, Train: 0.1894, Test: 0.1864\n",
            "Early stopping:  27.822039397745264\n",
            "Epoch: 011, Loss: 305.8574, Train: 0.2019, Test: 0.1945\n",
            "Early stopping:  22.738941118757825\n",
            "Epoch: 012, Loss: 280.0843, Train: 0.2728, Test: 0.2723\n",
            "Early stopping:  30.422179215744332\n",
            "Epoch: 013, Loss: 191.8868, Train: 0.1906, Test: 0.1761\n",
            "Early stopping:  64.74826815303827\n",
            "Epoch: 014, Loss: 177.6283, Train: 0.1727, Test: 0.1608\n",
            "Early stopping:  71.21719354145165\n",
            "Epoch: 015, Loss: 161.2526, Train: 0.2232, Test: 0.2081\n",
            "Early stopping:  65.12056613526035\n",
            "Epoch: 016, Loss: 124.3507, Train: 0.2459, Test: 0.2376\n",
            "Early stopping:  57.80191697947343\n",
            "Epoch: 017, Loss: 97.2290, Train: 0.2348, Test: 0.2366\n",
            "Early stopping:  39.00644448143079\n",
            "Epoch: 018, Loss: 79.5969, Train: 0.1943, Test: 0.1909\n",
            "Early stopping:  41.44796187034805\n",
            "Epoch: 019, Loss: 66.8906, Train: 0.1894, Test: 0.1882\n",
            "Early stopping:  37.72633078963161\n",
            "Epoch: 020, Loss: 53.9417, Train: 0.3018, Test: 0.2930\n",
            "Early stopping:  27.468172329974966\n",
            "Epoch: 021, Loss: 30.7429, Train: 0.3361, Test: 0.3304\n",
            "Early stopping:  25.24317611061754\n",
            "Epoch: 022, Loss: 19.0716, Train: 0.2873, Test: 0.2790\n",
            "Early stopping:  25.020959121598192\n",
            "Epoch: 023, Loss: 12.1225, Train: 0.2552, Test: 0.2487\n",
            "Early stopping:  23.232220159617967\n",
            "Epoch: 024, Loss: 8.3162, Train: 0.2629, Test: 0.2544\n",
            "Early stopping:  18.369796309748093\n",
            "Epoch: 025, Loss: 6.2982, Train: 0.2918, Test: 0.2867\n",
            "Early stopping:  9.907178522477041\n",
            "Epoch: 026, Loss: 5.0146, Train: 0.3497, Test: 0.3321\n",
            "Early stopping:  5.658086364385628\n",
            "Epoch: 027, Loss: 4.0354, Train: 0.3846, Test: 0.3725\n",
            "Early stopping:  3.2043538946319616\n",
            "Epoch: 028, Loss: 3.3765, Train: 0.4007, Test: 0.3850\n",
            "Early stopping:  1.9631728151048622\n",
            "Epoch: 029, Loss: 2.9483, Train: 0.3996, Test: 0.3795\n",
            "Early stopping:  1.346075268505894\n",
            "Epoch: 030, Loss: 2.6585, Train: 0.3778, Test: 0.3576\n",
            "Early stopping:  0.942277972120879\n",
            "Epoch: 031, Loss: 2.4500, Train: 0.3667, Test: 0.3442\n",
            "Early stopping:  0.6308012666279406\n",
            "Epoch: 032, Loss: 2.3045, Train: 0.3565, Test: 0.3343\n",
            "Early stopping:  0.42680439784256957\n",
            "Epoch: 033, Loss: 2.2127, Train: 0.3400, Test: 0.3230\n",
            "Early stopping:  0.29505533952419344\n",
            "Epoch: 034, Loss: 2.1613, Train: 0.3279, Test: 0.3128\n",
            "Early stopping:  0.20088849257809416\n",
            "Epoch: 035, Loss: 2.1405, Train: 0.3165, Test: 0.3009\n",
            "Early stopping:  0.12664566539754035\n",
            "Epoch: 036, Loss: 2.1311, Train: 0.3086, Test: 0.2914\n",
            "Early stopping:  0.07134553203578048\n",
            "Epoch: 037, Loss: 2.1256, Train: 0.2950, Test: 0.2801\n",
            "Early stopping:  0.03540181543933852\n",
            "Epoch: 038, Loss: 2.1239, Train: 0.2828, Test: 0.2678\n",
            "Early stopping:  0.015315097817369033\n",
            "Epoch: 039, Loss: 2.1264, Train: 0.2768, Test: 0.2589\n",
            "Early stopping:  0.006711129568832067\n",
            "PREDICTIONS -> tensor([3, 8, 3,  ..., 4, 7, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.14      0.00      0.00       568\n",
            "         capital_goods       0.08      0.01      0.01       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.16      0.78      0.27       595\n",
            " consumer_non-cyclical       0.62      0.30      0.40       334\n",
            "                energy       0.00      0.00      0.00       213\n",
            "             financial       0.28      0.27      0.28       576\n",
            "            healthcare       0.15      0.47      0.23       238\n",
            "              services       0.61      0.31      0.42      1557\n",
            "            technology       0.29      0.02      0.04       297\n",
            "        transportation       0.48      0.12      0.19       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.26      5291\n",
            "             macro avg       0.23      0.19      0.15      5291\n",
            "          weighted avg       0.34      0.26      0.23      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 84.6201, Train: 0.0746, Test: 0.0739\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 324.8860, Train: 0.1475, Test: 0.1395\n",
            "Early stopping:  169.8936954893715\n",
            "Epoch: 003, Loss: 244.2099, Train: 0.2944, Test: 0.2941\n",
            "Early stopping:  122.27379442441352\n",
            "Epoch: 004, Loss: 336.2378, Train: 0.2785, Test: 0.2773\n",
            "Early stopping:  116.05126626450095\n",
            "Epoch: 005, Loss: 229.6070, Train: 0.1503, Test: 0.1461\n",
            "Early stopping:  100.82098680004744\n",
            "Epoch: 006, Loss: 170.2043, Train: 0.1225, Test: 0.1215\n",
            "Early stopping:  69.37838755961246\n",
            "Epoch: 007, Loss: 172.8522, Train: 0.1367, Test: 0.1342\n",
            "Early stopping:  67.69048381697624\n",
            "Epoch: 008, Loss: 130.1502, Train: 0.1838, Test: 0.1737\n",
            "Early stopping:  80.05638063615518\n",
            "Epoch: 009, Loss: 76.1653, Train: 0.2113, Test: 0.2007\n",
            "Early stopping:  56.88877724790017\n",
            "Epoch: 010, Loss: 48.1127, Train: 0.2102, Test: 0.2007\n",
            "Early stopping:  55.9125489041166\n",
            "Epoch: 011, Loss: 24.8305, Train: 0.2385, Test: 0.2342\n",
            "Early stopping:  60.557084764387234\n",
            "Epoch: 012, Loss: 17.1567, Train: 0.2175, Test: 0.2081\n",
            "Early stopping:  45.80814418253711\n",
            "Epoch: 013, Loss: 13.4320, Train: 0.1517, Test: 0.1446\n",
            "Early stopping:  26.215903939906237\n",
            "Epoch: 014, Loss: 11.3991, Train: 0.1682, Test: 0.1605\n",
            "Early stopping:  14.952160621701875\n",
            "Epoch: 015, Loss: 8.0380, Train: 0.3242, Test: 0.3204\n",
            "Early stopping:  6.426145664031447\n",
            "Epoch: 016, Loss: 5.1361, Train: 0.3261, Test: 0.3154\n",
            "Early stopping:  4.667225610452866\n",
            "Epoch: 017, Loss: 4.2626, Train: 0.3174, Test: 0.3067\n",
            "Early stopping:  3.9434538122143867\n",
            "Epoch: 018, Loss: 3.5242, Train: 0.3196, Test: 0.3084\n",
            "Early stopping:  3.243606047083339\n",
            "Epoch: 019, Loss: 2.9798, Train: 0.3270, Test: 0.3151\n",
            "Early stopping:  1.989126668798095\n",
            "Epoch: 020, Loss: 2.6845, Train: 0.3176, Test: 0.3033\n",
            "Early stopping:  0.9947677566125952\n",
            "Epoch: 021, Loss: 2.5447, Train: 0.2456, Test: 0.2380\n",
            "Early stopping:  0.7031807260042577\n",
            "Epoch: 022, Loss: 2.5320, Train: 0.2292, Test: 0.2168\n",
            "Early stopping:  0.41624912468460756\n",
            "Epoch: 023, Loss: 2.4701, Train: 0.2328, Test: 0.2208\n",
            "Early stopping:  0.20435319781026368\n",
            "Epoch: 024, Loss: 2.4087, Train: 0.2343, Test: 0.2247\n",
            "Early stopping:  0.10294198911656136\n",
            "Epoch: 025, Loss: 2.3541, Train: 0.2385, Test: 0.2279\n",
            "Early stopping:  0.08107850076839987\n",
            "Epoch: 026, Loss: 2.3221, Train: 0.2382, Test: 0.2262\n",
            "Early stopping:  0.0852603526429321\n",
            "Epoch: 027, Loss: 2.3090, Train: 0.2368, Test: 0.2228\n",
            "Early stopping:  0.0665901184377217\n",
            "Epoch: 028, Loss: 2.3057, Train: 0.2382, Test: 0.2198\n",
            "Early stopping:  0.04291934584511574\n",
            "Epoch: 029, Loss: 2.3050, Train: 0.2348, Test: 0.2204\n",
            "Early stopping:  0.020682145948577455\n",
            "Epoch: 030, Loss: 2.3042, Train: 0.2365, Test: 0.2202\n",
            "Early stopping:  0.007413448646220053\n",
            "PREDICTIONS -> tensor([9, 6, 3,  ..., 8, 6, 3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.60      0.01      0.01       568\n",
            "         capital_goods       0.09      0.01      0.01       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.15      0.21      0.17       595\n",
            " consumer_non-cyclical       0.61      0.25      0.36       334\n",
            "                energy       0.00      0.00      0.00       213\n",
            "             financial       0.16      0.75      0.26       576\n",
            "            healthcare       0.19      0.07      0.10       238\n",
            "              services       0.37      0.29      0.33      1557\n",
            "            technology       0.22      0.17      0.19       297\n",
            "        transportation       0.00      0.00      0.00       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.22      5291\n",
            "             macro avg       0.20      0.15      0.12      5291\n",
            "          weighted avg       0.27      0.22      0.18      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 89.9834, Train: 0.1497, Test: 0.1435\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 314.5561, Train: 0.2933, Test: 0.2926\n",
            "Early stopping:  158.79683084010136\n",
            "Epoch: 003, Loss: 532.1021, Train: 0.0184, Test: 0.0189\n",
            "Early stopping:  221.0686169857767\n",
            "Epoch: 004, Loss: 527.3161, Train: 0.0851, Test: 0.0852\n",
            "Early stopping:  210.11457324017303\n",
            "Epoch: 005, Loss: 639.3101, Train: 0.1611, Test: 0.1535\n",
            "Early stopping:  219.2075327961172\n",
            "Epoch: 006, Loss: 625.4036, Train: 0.1341, Test: 0.1348\n",
            "Early stopping:  129.8582635246002\n",
            "Epoch: 007, Loss: 465.9691, Train: 0.0610, Test: 0.0610\n",
            "Early stopping:  72.86369336013531\n",
            "Epoch: 008, Loss: 443.5504, Train: 0.2816, Test: 0.2841\n",
            "Early stopping:  89.58204141053783\n",
            "Epoch: 009, Loss: 364.2473, Train: 0.2754, Test: 0.2725\n",
            "Early stopping:  120.0111069849398\n",
            "Epoch: 010, Loss: 372.1316, Train: 0.2130, Test: 0.2213\n",
            "Early stopping:  105.34383228574613\n",
            "Epoch: 011, Loss: 321.1339, Train: 0.0993, Test: 0.0988\n",
            "Early stopping:  59.80376767466093\n",
            "Epoch: 012, Loss: 306.2360, Train: 0.2691, Test: 0.2667\n",
            "Early stopping:  53.703769472666515\n",
            "Epoch: 013, Loss: 208.3004, Train: 0.1495, Test: 0.1510\n",
            "Early stopping:  65.54923332220598\n",
            "Epoch: 014, Loss: 174.2286, Train: 0.2711, Test: 0.2782\n",
            "Early stopping:  82.35956397853926\n",
            "Epoch: 015, Loss: 140.3000, Train: 0.2399, Test: 0.2446\n",
            "Early stopping:  80.22578207444326\n",
            "Epoch: 016, Loss: 117.6647, Train: 0.2677, Test: 0.2708\n",
            "Early stopping:  73.81531329639287\n",
            "Epoch: 017, Loss: 93.6251, Train: 0.3148, Test: 0.3120\n",
            "Early stopping:  45.42388392205518\n",
            "Epoch: 018, Loss: 73.7015, Train: 0.2550, Test: 0.2472\n",
            "Early stopping:  39.361857470499196\n",
            "Epoch: 019, Loss: 58.3935, Train: 0.3100, Test: 0.3107\n",
            "Early stopping:  32.96302771146775\n",
            "Epoch: 020, Loss: 35.2210, Train: 0.3111, Test: 0.3015\n",
            "Early stopping:  31.719083706211013\n",
            "Epoch: 021, Loss: 17.4874, Train: 0.2513, Test: 0.2417\n",
            "Early stopping:  30.204364318570633\n",
            "Epoch: 022, Loss: 9.4699, Train: 0.2189, Test: 0.2115\n",
            "Early stopping:  27.06150796055426\n",
            "Epoch: 023, Loss: 4.3104, Train: 0.1526, Test: 0.1399\n",
            "Early stopping:  22.052632304967215\n",
            "Epoch: 024, Loss: 3.4183, Train: 0.1393, Test: 0.1280\n",
            "Early stopping:  13.123178371508558\n",
            "Epoch: 025, Loss: 2.9283, Train: 0.1557, Test: 0.1463\n",
            "Early stopping:  6.151423675293417\n",
            "Epoch: 026, Loss: 2.7209, Train: 0.1605, Test: 0.1508\n",
            "Early stopping:  2.8070022345352106\n",
            "Epoch: 027, Loss: 2.6230, Train: 0.1611, Test: 0.1550\n",
            "Early stopping:  0.6922828814688251\n",
            "Epoch: 028, Loss: 2.5680, Train: 0.1676, Test: 0.1607\n",
            "Early stopping:  0.3452498516049254\n",
            "Epoch: 029, Loss: 2.5117, Train: 0.1736, Test: 0.1646\n",
            "Early stopping:  0.1635346940235757\n",
            "Epoch: 030, Loss: 2.4556, Train: 0.1730, Test: 0.1624\n",
            "Early stopping:  0.1023154447990556\n",
            "Epoch: 031, Loss: 2.4302, Train: 0.1724, Test: 0.1633\n",
            "Early stopping:  0.07931737232043651\n",
            "Epoch: 032, Loss: 2.4287, Train: 0.2223, Test: 0.2085\n",
            "Early stopping:  0.060115512098977975\n",
            "Epoch: 033, Loss: 2.4270, Train: 0.2328, Test: 0.2213\n",
            "Early stopping:  0.03611016106991039\n",
            "Epoch: 034, Loss: 2.4201, Train: 0.2482, Test: 0.2319\n",
            "Early stopping:  0.013598597236725329\n",
            "Epoch: 035, Loss: 2.4057, Train: 0.2513, Test: 0.2483\n",
            "Early stopping:  0.010079675955150863\n",
            "Epoch: 036, Loss: 2.3857, Train: 0.2601, Test: 0.2578\n",
            "Early stopping:  0.017951097900738948\n",
            "Epoch: 037, Loss: 2.3632, Train: 0.2725, Test: 0.2678\n",
            "Early stopping:  0.026097431211162116\n",
            "Epoch: 038, Loss: 2.3385, Train: 0.2864, Test: 0.2776\n",
            "Early stopping:  0.03266038886824825\n",
            "Epoch: 039, Loss: 2.3156, Train: 0.2967, Test: 0.2890\n",
            "Early stopping:  0.03596834933134904\n",
            "Epoch: 040, Loss: 2.2985, Train: 0.3015, Test: 0.2948\n",
            "Early stopping:  0.035163850235171046\n",
            "Epoch: 041, Loss: 2.2849, Train: 0.3074, Test: 0.3041\n",
            "Early stopping:  0.031306094300131225\n",
            "Epoch: 042, Loss: 2.2724, Train: 0.3162, Test: 0.3132\n",
            "Early stopping:  0.025999763354823634\n",
            "Epoch: 043, Loss: 2.2600, Train: 0.3216, Test: 0.3173\n",
            "Early stopping:  0.021773604191183998\n",
            "Epoch: 044, Loss: 2.2485, Train: 0.3216, Test: 0.3188\n",
            "Early stopping:  0.019748427440841744\n",
            "Epoch: 045, Loss: 2.2376, Train: 0.3253, Test: 0.3230\n",
            "Early stopping:  0.01873624696288949\n",
            "Epoch: 046, Loss: 2.2264, Train: 0.3310, Test: 0.3270\n",
            "Early stopping:  0.018098298503521754\n",
            "Epoch: 047, Loss: 2.2147, Train: 0.3298, Test: 0.3308\n",
            "Early stopping:  0.01782550035488613\n",
            "Epoch: 048, Loss: 2.2026, Train: 0.3321, Test: 0.3328\n",
            "Early stopping:  0.01814910562190689\n",
            "Epoch: 049, Loss: 2.1901, Train: 0.3347, Test: 0.3351\n",
            "Early stopping:  0.018794167958659368\n",
            "Epoch: 050, Loss: 2.1783, Train: 0.3369, Test: 0.3387\n",
            "Early stopping:  0.01910602141292787\n",
            "Epoch: 051, Loss: 2.1670, Train: 0.3364, Test: 0.3421\n",
            "Early stopping:  0.018941179632991575\n",
            "Epoch: 052, Loss: 2.1554, Train: 0.3395, Test: 0.3434\n",
            "Early stopping:  0.018573137426686804\n",
            "Epoch: 053, Loss: 2.1439, Train: 0.3395, Test: 0.3455\n",
            "Early stopping:  0.01821432618881415\n",
            "Epoch: 054, Loss: 2.1327, Train: 0.3392, Test: 0.3474\n",
            "Early stopping:  0.018063211922895894\n",
            "Epoch: 055, Loss: 2.1208, Train: 0.3417, Test: 0.3485\n",
            "Early stopping:  0.018197453720740137\n",
            "Epoch: 056, Loss: 2.1097, Train: 0.3420, Test: 0.3510\n",
            "Early stopping:  0.0180883725952695\n",
            "Epoch: 057, Loss: 2.0994, Train: 0.3452, Test: 0.3538\n",
            "Early stopping:  0.017708223927013673\n",
            "Epoch: 058, Loss: 2.0898, Train: 0.3491, Test: 0.3553\n",
            "Early stopping:  0.01694836833470949\n",
            "Epoch: 059, Loss: 2.0810, Train: 0.3511, Test: 0.3568\n",
            "Early stopping:  0.01574731721446678\n",
            "Epoch: 060, Loss: 2.0723, Train: 0.3545, Test: 0.3602\n",
            "Early stopping:  0.01477348381663633\n",
            "Epoch: 061, Loss: 2.0631, Train: 0.3582, Test: 0.3617\n",
            "Early stopping:  0.014254490079572413\n",
            "Epoch: 062, Loss: 2.0536, Train: 0.3627, Test: 0.3644\n",
            "Early stopping:  0.014294635970383045\n",
            "Epoch: 063, Loss: 2.0436, Train: 0.3650, Test: 0.3667\n",
            "Early stopping:  0.014774727387966344\n",
            "Epoch: 064, Loss: 2.0337, Train: 0.3673, Test: 0.3697\n",
            "Early stopping:  0.015294584092888922\n",
            "Epoch: 065, Loss: 2.0235, Train: 0.3707, Test: 0.3721\n",
            "Early stopping:  0.015684409781515335\n",
            "Epoch: 066, Loss: 2.0141, Train: 0.3755, Test: 0.3771\n",
            "Early stopping:  0.01567061609872064\n",
            "Epoch: 067, Loss: 2.0051, Train: 0.3786, Test: 0.3788\n",
            "Early stopping:  0.01528379465130569\n",
            "Epoch: 068, Loss: 1.9959, Train: 0.3798, Test: 0.3822\n",
            "Early stopping:  0.014839841465016159\n",
            "Epoch: 069, Loss: 1.9871, Train: 0.3837, Test: 0.3839\n",
            "Early stopping:  0.014389035062694235\n",
            "Epoch: 070, Loss: 1.9781, Train: 0.3885, Test: 0.3859\n",
            "Early stopping:  0.014246885290017474\n",
            "Epoch: 071, Loss: 1.9685, Train: 0.3914, Test: 0.3893\n",
            "Early stopping:  0.014409504913039339\n",
            "Epoch: 072, Loss: 1.9595, Train: 0.3897, Test: 0.3884\n",
            "Early stopping:  0.01446283704000775\n",
            "Epoch: 073, Loss: 1.9507, Train: 0.3902, Test: 0.3909\n",
            "Early stopping:  0.014431367593455171\n",
            "Epoch: 074, Loss: 1.9416, Train: 0.3931, Test: 0.3918\n",
            "Early stopping:  0.014339683744286704\n",
            "Epoch: 075, Loss: 1.9322, Train: 0.3979, Test: 0.3937\n",
            "Early stopping:  0.014300409762440212\n",
            "Epoch: 076, Loss: 1.9227, Train: 0.4007, Test: 0.3929\n",
            "Early stopping:  0.014569399080894097\n",
            "Epoch: 077, Loss: 1.9132, Train: 0.4030, Test: 0.3948\n",
            "Early stopping:  0.014855490886781487\n",
            "Epoch: 078, Loss: 1.9029, Train: 0.4064, Test: 0.3973\n",
            "Early stopping:  0.015251567508983993\n",
            "Epoch: 079, Loss: 1.8920, Train: 0.4112, Test: 0.3984\n",
            "Early stopping:  0.015867059390918183\n",
            "Epoch: 080, Loss: 1.8813, Train: 0.4158, Test: 0.3994\n",
            "Early stopping:  0.01645770477297443\n",
            "Epoch: 081, Loss: 1.8700, Train: 0.4206, Test: 0.4026\n",
            "Early stopping:  0.017067281186187198\n",
            "Epoch: 082, Loss: 1.8581, Train: 0.4231, Test: 0.4046\n",
            "Early stopping:  0.017642450552516786\n",
            "Epoch: 083, Loss: 1.8459, Train: 0.4246, Test: 0.4086\n",
            "Early stopping:  0.01824092239662551\n",
            "Epoch: 084, Loss: 1.8334, Train: 0.4277, Test: 0.4145\n",
            "Early stopping:  0.018967486482258815\n",
            "Epoch: 085, Loss: 1.8196, Train: 0.4336, Test: 0.4154\n",
            "Early stopping:  0.019846729995886707\n",
            "Epoch: 086, Loss: 1.8101, Train: 0.4359, Test: 0.4222\n",
            "Early stopping:  0.01933617474662783\n",
            "Epoch: 087, Loss: 1.7914, Train: 0.4427, Test: 0.4237\n",
            "Early stopping:  0.020993921894613205\n",
            "Epoch: 088, Loss: 1.7801, Train: 0.4484, Test: 0.4279\n",
            "Early stopping:  0.02141150075978819\n",
            "Epoch: 089, Loss: 1.7648, Train: 0.4546, Test: 0.4317\n",
            "Early stopping:  0.022181298802094407\n",
            "Epoch: 090, Loss: 1.7470, Train: 0.4600, Test: 0.4326\n",
            "Early stopping:  0.024233606386446477\n",
            "Epoch: 091, Loss: 1.7332, Train: 0.4665, Test: 0.4362\n",
            "Early stopping:  0.023682068895484156\n",
            "Epoch: 092, Loss: 1.7166, Train: 0.4697, Test: 0.4404\n",
            "Early stopping:  0.02506621043883012\n",
            "Epoch: 093, Loss: 1.7000, Train: 0.4742, Test: 0.4417\n",
            "Early stopping:  0.02532025838115308\n",
            "Epoch: 094, Loss: 1.6853, Train: 0.4770, Test: 0.4428\n",
            "Early stopping:  0.024776978495601818\n",
            "Epoch: 095, Loss: 1.6669, Train: 0.4762, Test: 0.4423\n",
            "Early stopping:  0.025937298921411948\n",
            "Epoch: 096, Loss: 1.6611, Train: 0.4782, Test: 0.4434\n",
            "Early stopping:  0.023012201939926324\n",
            "Epoch: 097, Loss: 1.6441, Train: 0.4855, Test: 0.4491\n",
            "Early stopping:  0.021709655152324828\n",
            "Epoch: 098, Loss: 1.6300, Train: 0.4906, Test: 0.4504\n",
            "Early stopping:  0.021291229173640035\n",
            "Epoch: 099, Loss: 1.6206, Train: 0.4836, Test: 0.4496\n",
            "Early stopping:  0.019731491825094594\n",
            "Epoch: 100, Loss: 1.6018, Train: 0.4884, Test: 0.4540\n",
            "Early stopping:  0.0225808509000393\n",
            "Epoch: 101, Loss: 1.5885, Train: 0.5043, Test: 0.4593\n",
            "Early stopping:  0.022125381356065097\n",
            "Epoch: 102, Loss: 1.5757, Train: 0.5085, Test: 0.4621\n",
            "Early stopping:  0.022349942315622397\n",
            "Epoch: 103, Loss: 1.5573, Train: 0.5062, Test: 0.4595\n",
            "Early stopping:  0.024205015923133667\n",
            "Epoch: 104, Loss: 1.5468, Train: 0.5102, Test: 0.4655\n",
            "Early stopping:  0.02236381155984523\n",
            "Epoch: 105, Loss: 1.5305, Train: 0.5142, Test: 0.4672\n",
            "Early stopping:  0.022979414748785763\n",
            "Epoch: 106, Loss: 1.5233, Train: 0.5179, Test: 0.4683\n",
            "Early stopping:  0.02099984699873218\n",
            "Epoch: 107, Loss: 1.5093, Train: 0.5196, Test: 0.4702\n",
            "Early stopping:  0.018992434597561638\n",
            "Epoch: 108, Loss: 1.4978, Train: 0.5193, Test: 0.4750\n",
            "Early stopping:  0.01893181416891016\n",
            "Epoch: 109, Loss: 1.4902, Train: 0.5164, Test: 0.4763\n",
            "Early stopping:  0.0168458357706684\n",
            "Epoch: 110, Loss: 1.4776, Train: 0.5241, Test: 0.4782\n",
            "Early stopping:  0.017530557376908482\n",
            "Epoch: 111, Loss: 1.4711, Train: 0.5281, Test: 0.4802\n",
            "Early stopping:  0.015343842262129148\n",
            "Epoch: 112, Loss: 1.4579, Train: 0.5275, Test: 0.4791\n",
            "Early stopping:  0.01572323651311804\n",
            "Epoch: 113, Loss: 1.4492, Train: 0.5323, Test: 0.4842\n",
            "Early stopping:  0.01616950548008588\n",
            "Epoch: 114, Loss: 1.4358, Train: 0.5403, Test: 0.4859\n",
            "Early stopping:  0.016794586848226083\n",
            "Epoch: 115, Loss: 1.4277, Train: 0.5403, Test: 0.4899\n",
            "Early stopping:  0.017271043916373154\n",
            "Epoch: 116, Loss: 1.4191, Train: 0.5465, Test: 0.4931\n",
            "Early stopping:  0.015722526221031683\n",
            "Epoch: 117, Loss: 1.4070, Train: 0.5493, Test: 0.4920\n",
            "Early stopping:  0.016038054622794014\n",
            "Epoch: 118, Loss: 1.3995, Train: 0.5499, Test: 0.4956\n",
            "Early stopping:  0.014797929608039903\n",
            "Epoch: 119, Loss: 1.3899, Train: 0.5562, Test: 0.4974\n",
            "Early stopping:  0.015104369220220001\n",
            "Epoch: 120, Loss: 1.3778, Train: 0.5547, Test: 0.4993\n",
            "Early stopping:  0.015815201631100513\n",
            "Epoch: 121, Loss: 1.3699, Train: 0.5553, Test: 0.5039\n",
            "Early stopping:  0.015206087995262279\n",
            "Epoch: 122, Loss: 1.3597, Train: 0.5587, Test: 0.5037\n",
            "Early stopping:  0.015747491684392902\n",
            "Epoch: 123, Loss: 1.3476, Train: 0.5618, Test: 0.5007\n",
            "Early stopping:  0.016253527018708035\n",
            "Epoch: 124, Loss: 1.3405, Train: 0.5539, Test: 0.5075\n",
            "Early stopping:  0.015380045128425274\n",
            "Epoch: 125, Loss: 1.3590, Train: 0.5615, Test: 0.4990\n",
            "Early stopping:  0.011463818201260082\n",
            "Epoch: 126, Loss: 1.3605, Train: 0.5655, Test: 0.5007\n",
            "Early stopping:  0.008990468529381953\n",
            "PREDICTIONS -> tensor([ 9,  6,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.44      0.54      0.48       568\n",
            "         capital_goods       0.35      0.21      0.26       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.38      0.55      0.45       595\n",
            " consumer_non-cyclical       0.57      0.14      0.22       334\n",
            "                energy       0.64      0.34      0.44       213\n",
            "             financial       0.64      0.55      0.59       576\n",
            "            healthcare       0.63      0.47      0.54       238\n",
            "              services       0.52      0.68      0.59      1557\n",
            "            technology       0.40      0.28      0.33       297\n",
            "        transportation       0.62      0.51      0.56       303\n",
            "             utilities       0.66      0.52      0.58       169\n",
            "\n",
            "              accuracy                           0.50      5291\n",
            "             macro avg       0.49      0.40      0.42      5291\n",
            "          weighted avg       0.51      0.50      0.48      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 73.8669, Train: 0.1157, Test: 0.1185\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 432.9494, Train: 0.2969, Test: 0.2984\n",
            "Early stopping:  253.90969535569099\n",
            "Epoch: 003, Loss: 462.3354, Train: 0.1174, Test: 0.1230\n",
            "Early stopping:  216.29900962157166\n",
            "Epoch: 004, Loss: 574.2357, Train: 0.0533, Test: 0.0535\n",
            "Early stopping:  216.71100799148374\n",
            "Epoch: 005, Loss: 681.4073, Train: 0.0899, Test: 0.0820\n",
            "Early stopping:  229.55160843611253\n",
            "Epoch: 006, Loss: 567.4221, Train: 0.0678, Test: 0.0648\n",
            "Early stopping:  99.177042713777\n",
            "Epoch: 007, Loss: 405.4456, Train: 0.0993, Test: 0.0949\n",
            "Early stopping:  107.28466953486291\n",
            "Epoch: 008, Loss: 358.6193, Train: 0.2686, Test: 0.2480\n",
            "Early stopping:  132.642381156352\n",
            "Epoch: 009, Loss: 244.4176, Train: 0.3054, Test: 0.3009\n",
            "Early stopping:  173.15183618337687\n",
            "Epoch: 010, Loss: 210.4128, Train: 0.3057, Test: 0.2977\n",
            "Early stopping:  142.11729273022587\n",
            "Epoch: 011, Loss: 152.4014, Train: 0.3037, Test: 0.2916\n",
            "Early stopping:  105.05081540830564\n",
            "Epoch: 012, Loss: 82.5012, Train: 0.1897, Test: 0.1771\n",
            "Early stopping:  103.49568399757354\n",
            "Epoch: 013, Loss: 35.8018, Train: 0.1514, Test: 0.1380\n",
            "Early stopping:  86.67143706318629\n",
            "Epoch: 014, Loss: 17.9483, Train: 0.1256, Test: 0.1081\n",
            "Early stopping:  80.77252601522433\n",
            "Epoch: 015, Loss: 9.7683, Train: 0.1339, Test: 0.1157\n",
            "Early stopping:  58.99352699127986\n",
            "Epoch: 016, Loss: 6.9944, Train: 0.1472, Test: 0.1314\n",
            "Early stopping:  31.112891581050466\n",
            "Epoch: 017, Loss: 5.3566, Train: 0.1557, Test: 0.1385\n",
            "Early stopping:  12.50713683167264\n",
            "Epoch: 018, Loss: 4.1760, Train: 0.1548, Test: 0.1370\n",
            "Early stopping:  5.502619527688552\n",
            "Epoch: 019, Loss: 3.4918, Train: 0.1580, Test: 0.1416\n",
            "Early stopping:  2.5105557956093296\n",
            "Epoch: 020, Loss: 3.1445, Train: 0.1526, Test: 0.1368\n",
            "Early stopping:  1.5673555470435319\n",
            "Epoch: 021, Loss: 2.9409, Train: 0.1395, Test: 0.1259\n",
            "Early stopping:  0.9778404653422649\n",
            "Epoch: 022, Loss: 2.8172, Train: 0.1256, Test: 0.1109\n",
            "Early stopping:  0.545306378825676\n",
            "Epoch: 023, Loss: 2.7277, Train: 0.1200, Test: 0.1062\n",
            "Early stopping:  0.3044821787557266\n",
            "Epoch: 024, Loss: 2.6653, Train: 0.1168, Test: 0.1036\n",
            "Early stopping:  0.19023846243058243\n",
            "Epoch: 025, Loss: 2.6245, Train: 0.1160, Test: 0.1009\n",
            "Early stopping:  0.12675555995386065\n",
            "Epoch: 026, Loss: 2.5948, Train: 0.1154, Test: 0.0987\n",
            "Early stopping:  0.08869588299261028\n",
            "Epoch: 027, Loss: 2.5707, Train: 0.1078, Test: 0.0962\n",
            "Early stopping:  0.061960392322038475\n",
            "Epoch: 028, Loss: 2.5506, Train: 0.1044, Test: 0.0947\n",
            "Early stopping:  0.04521995923373048\n",
            "Epoch: 029, Loss: 2.5324, Train: 0.1010, Test: 0.0920\n",
            "Early stopping:  0.036289290503539864\n",
            "Epoch: 030, Loss: 2.5153, Train: 0.0828, Test: 0.0782\n",
            "Early stopping:  0.03127259458117206\n",
            "Epoch: 031, Loss: 2.4978, Train: 0.0865, Test: 0.0828\n",
            "Early stopping:  0.028628744889644564\n",
            "Epoch: 032, Loss: 2.4800, Train: 0.0942, Test: 0.0868\n",
            "Early stopping:  0.027798161407555113\n",
            "Epoch: 033, Loss: 2.4624, Train: 0.0995, Test: 0.0903\n",
            "Early stopping:  0.02772912378278473\n",
            "Epoch: 034, Loss: 2.4459, Train: 0.1024, Test: 0.0954\n",
            "Early stopping:  0.027549817048511737\n",
            "Epoch: 035, Loss: 2.4313, Train: 0.1106, Test: 0.1011\n",
            "Early stopping:  0.026456404055938805\n",
            "Epoch: 036, Loss: 2.4175, Train: 0.1202, Test: 0.1081\n",
            "Early stopping:  0.024721229371944507\n",
            "Epoch: 037, Loss: 2.4016, Train: 0.1410, Test: 0.1380\n",
            "Early stopping:  0.02374174368885704\n",
            "Epoch: 038, Loss: 2.3804, Train: 0.1761, Test: 0.1654\n",
            "Early stopping:  0.025506806295719203\n",
            "Epoch: 039, Loss: 2.3553, Train: 0.2031, Test: 0.2007\n",
            "Early stopping:  0.030128792883402183\n",
            "Epoch: 040, Loss: 2.3341, Train: 0.2181, Test: 0.2257\n",
            "Early stopping:  0.03376469383915879\n",
            "Epoch: 041, Loss: 2.3210, Train: 0.2283, Test: 0.2344\n",
            "Early stopping:  0.03297386487797019\n",
            "Epoch: 042, Loss: 2.3119, Train: 0.2428, Test: 0.2457\n",
            "Early stopping:  0.027617280989398755\n",
            "Epoch: 043, Loss: 2.3008, Train: 0.2510, Test: 0.2506\n",
            "Early stopping:  0.021057452354525535\n",
            "Epoch: 044, Loss: 2.2874, Train: 0.2564, Test: 0.2563\n",
            "Early stopping:  0.01798202797833574\n",
            "Epoch: 045, Loss: 2.2760, Train: 0.2601, Test: 0.2593\n",
            "Early stopping:  0.01812654879029325\n",
            "Epoch: 046, Loss: 2.2686, Train: 0.2686, Test: 0.2665\n",
            "Early stopping:  0.017683520633377216\n",
            "Epoch: 047, Loss: 2.2645, Train: 0.2998, Test: 0.2912\n",
            "Early stopping:  0.014748899868310647\n",
            "Epoch: 048, Loss: 2.2554, Train: 0.2981, Test: 0.2905\n",
            "Early stopping:  0.012088589684335828\n",
            "Epoch: 049, Loss: 2.2438, Train: 0.2978, Test: 0.2903\n",
            "Early stopping:  0.012450509611252826\n",
            "Epoch: 050, Loss: 2.2284, Train: 0.2986, Test: 0.2905\n",
            "Early stopping:  0.01635773803719318\n",
            "Epoch: 051, Loss: 2.2152, Train: 0.2986, Test: 0.2907\n",
            "Early stopping:  0.019967181935009206\n",
            "Epoch: 052, Loss: 2.2091, Train: 0.2986, Test: 0.2909\n",
            "Early stopping:  0.019332047614853683\n",
            "Epoch: 053, Loss: 2.2032, Train: 0.2975, Test: 0.2886\n",
            "Early stopping:  0.01629731993340784\n",
            "Epoch: 054, Loss: 2.1924, Train: 0.2984, Test: 0.2892\n",
            "Early stopping:  0.013422467749522417\n",
            "Epoch: 055, Loss: 2.1818, Train: 0.2978, Test: 0.2901\n",
            "Early stopping:  0.013341528435497274\n",
            "Epoch: 056, Loss: 2.1720, Train: 0.2995, Test: 0.2914\n",
            "Early stopping:  0.015177974126834453\n",
            "Epoch: 057, Loss: 2.1616, Train: 0.2989, Test: 0.2922\n",
            "Early stopping:  0.016363527663767523\n",
            "Epoch: 058, Loss: 2.1519, Train: 0.2992, Test: 0.2935\n",
            "Early stopping:  0.015986157448683584\n",
            "Epoch: 059, Loss: 2.1460, Train: 0.3012, Test: 0.2958\n",
            "Early stopping:  0.014546505643459107\n",
            "Epoch: 060, Loss: 2.1416, Train: 0.3026, Test: 0.2958\n",
            "Early stopping:  0.012261959192777797\n",
            "Epoch: 061, Loss: 2.1346, Train: 0.3020, Test: 0.2956\n",
            "Early stopping:  0.010263165842500668\n",
            "Epoch: 062, Loss: 2.1274, Train: 0.3023, Test: 0.2954\n",
            "Early stopping:  0.009593636207250247\n",
            "PREDICTIONS -> tensor([1, 8, 8,  ..., 6, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.53      0.01      0.03       568\n",
            "         capital_goods       0.11      0.04      0.06       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.06      0.00      0.00       595\n",
            " consumer_non-cyclical       0.68      0.12      0.21       334\n",
            "                energy       0.33      0.00      0.01       213\n",
            "             financial       0.00      0.00      0.00       576\n",
            "            healthcare       0.00      0.00      0.00       238\n",
            "              services       0.30      0.93      0.46      1557\n",
            "            technology       0.12      0.00      0.01       297\n",
            "        transportation       0.18      0.14      0.16       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.30      5291\n",
            "             macro avg       0.19      0.11      0.08      5291\n",
            "          weighted avg       0.24      0.30      0.17      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 74.1299, Train: 0.0621, Test: 0.0588\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 282.7837, Train: 0.2862, Test: 0.2894\n",
            "Early stopping:  147.54050679265072\n",
            "Epoch: 003, Loss: 343.5704, Train: 0.1123, Test: 0.1115\n",
            "Early stopping:  141.32092852281497\n",
            "Epoch: 004, Loss: 496.9581, Train: 0.0176, Test: 0.0168\n",
            "Early stopping:  175.12180597247902\n",
            "Epoch: 005, Loss: 540.8431, Train: 0.0335, Test: 0.0346\n",
            "Early stopping:  186.18136892121785\n",
            "Epoch: 006, Loss: 463.8925, Train: 0.0479, Test: 0.0495\n",
            "Early stopping:  108.35660635053131\n",
            "Epoch: 007, Loss: 370.5264, Train: 0.2116, Test: 0.2053\n",
            "Early stopping:  83.75526825751993\n",
            "Epoch: 008, Loss: 291.0767, Train: 0.3227, Test: 0.3188\n",
            "Early stopping:  100.89561157887259\n",
            "Epoch: 009, Loss: 279.4119, Train: 0.1412, Test: 0.1366\n",
            "Early stopping:  112.47765800222965\n",
            "Epoch: 010, Loss: 266.8419, Train: 0.1617, Test: 0.1487\n",
            "Early stopping:  82.97305607386389\n",
            "Epoch: 011, Loss: 291.8959, Train: 0.1999, Test: 0.1907\n",
            "Early stopping:  40.751059784643466\n",
            "Epoch: 012, Loss: 247.3037, Train: 0.3137, Test: 0.3113\n",
            "Early stopping:  18.685363032631923\n",
            "Epoch: 013, Loss: 227.3221, Train: 0.3242, Test: 0.3238\n",
            "Early stopping:  25.67770530249049\n",
            "Epoch: 014, Loss: 204.3736, Train: 0.3188, Test: 0.3047\n",
            "Early stopping:  33.95645373484004\n",
            "Epoch: 015, Loss: 174.2739, Train: 0.2240, Test: 0.2226\n",
            "Early stopping:  44.42727401075928\n",
            "Epoch: 016, Loss: 155.0237, Train: 0.2510, Test: 0.2463\n",
            "Early stopping:  37.66362945391089\n",
            "Epoch: 017, Loss: 115.2972, Train: 0.3242, Test: 0.3090\n",
            "Early stopping:  43.48588608886517\n",
            "Epoch: 018, Loss: 81.7490, Train: 0.3162, Test: 0.3003\n",
            "Early stopping:  48.371140386700425\n",
            "Epoch: 019, Loss: 67.2907, Train: 0.3366, Test: 0.3309\n",
            "Early stopping:  45.90134117681802\n",
            "Epoch: 020, Loss: 52.5268, Train: 0.3446, Test: 0.3377\n",
            "Early stopping:  41.110349933900565\n",
            "Epoch: 021, Loss: 36.5262, Train: 0.3276, Test: 0.3028\n",
            "Early stopping:  30.086225449677922\n",
            "Epoch: 022, Loss: 22.6445, Train: 0.2368, Test: 0.2198\n",
            "Early stopping:  23.559300086483425\n",
            "Epoch: 023, Loss: 19.9521, Train: 0.1790, Test: 0.1580\n",
            "Early stopping:  20.103975090827483\n",
            "Epoch: 024, Loss: 15.1147, Train: 0.1271, Test: 0.1162\n",
            "Early stopping:  15.203539702887596\n",
            "Epoch: 025, Loss: 10.1299, Train: 0.1262, Test: 0.1126\n",
            "Early stopping:  9.969724468661807\n",
            "Epoch: 026, Loss: 7.7876, Train: 0.1251, Test: 0.1177\n",
            "Early stopping:  6.297489191917525\n",
            "Epoch: 027, Loss: 5.7855, Train: 0.1265, Test: 0.1213\n",
            "Early stopping:  5.755355726557178\n",
            "Epoch: 028, Loss: 4.0993, Train: 0.1347, Test: 0.1302\n",
            "Early stopping:  4.2900324231051\n",
            "Epoch: 029, Loss: 3.0908, Train: 0.1829, Test: 0.1743\n",
            "Early stopping:  2.837869672970765\n",
            "Epoch: 030, Loss: 2.7060, Train: 0.2235, Test: 0.2113\n",
            "Early stopping:  2.09986643370312\n",
            "Epoch: 031, Loss: 2.5598, Train: 0.2555, Test: 0.2455\n",
            "Early stopping:  1.3374011951500024\n",
            "Epoch: 032, Loss: 2.4889, Train: 0.2689, Test: 0.2618\n",
            "Early stopping:  0.6628806458934215\n",
            "Epoch: 033, Loss: 2.4408, Train: 0.2771, Test: 0.2710\n",
            "Early stopping:  0.2622036346898345\n",
            "Epoch: 034, Loss: 2.4046, Train: 0.2828, Test: 0.2784\n",
            "Early stopping:  0.11908809885707435\n",
            "Epoch: 035, Loss: 2.3779, Train: 0.2867, Test: 0.2809\n",
            "Early stopping:  0.07214661136979124\n",
            "Epoch: 036, Loss: 2.3596, Train: 0.2893, Test: 0.2856\n",
            "Early stopping:  0.051661733345521293\n",
            "Epoch: 037, Loss: 2.3476, Train: 0.2924, Test: 0.2875\n",
            "Early stopping:  0.037370413691646426\n",
            "Epoch: 038, Loss: 2.3406, Train: 0.2981, Test: 0.2901\n",
            "Early stopping:  0.02578358783021393\n",
            "Epoch: 039, Loss: 2.3390, Train: 0.2969, Test: 0.2890\n",
            "Early stopping:  0.016162090238314497\n",
            "Epoch: 040, Loss: 2.3429, Train: 0.3006, Test: 0.2928\n",
            "Early stopping:  0.008297057765494945\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 1, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.30      0.05      0.08       568\n",
            "         capital_goods       0.10      0.17      0.13       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.42      0.06      0.11       595\n",
            " consumer_non-cyclical       0.53      0.13      0.21       334\n",
            "                energy       0.54      0.03      0.06       213\n",
            "             financial       0.89      0.01      0.03       576\n",
            "            healthcare       0.19      0.08      0.11       238\n",
            "              services       0.31      0.82      0.45      1557\n",
            "            technology       0.00      0.00      0.00       297\n",
            "        transportation       0.37      0.06      0.10       303\n",
            "             utilities       0.64      0.29      0.40       169\n",
            "\n",
            "              accuracy                           0.29      5291\n",
            "             macro avg       0.36      0.14      0.14      5291\n",
            "          weighted avg       0.38      0.29      0.20      5291\n",
            "\n",
            "time: 1min 41s (started: 2024-10-16 20:57:57 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69rmpa0NHrm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7393a1-207c-44a1-fb0e-da78c872b6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 422 ms (started: 2024-10-16 20:59:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hD3zLW6Hrm-"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuMuiQDfHrm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04470272-994f-4b9f-fbf5-6357a8352139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4703, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2619, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14740236493327227\n",
            "Epoch: 003, Loss: 2.1668, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15526623986188318\n",
            "Epoch: 004, Loss: 2.1286, Train: 0.2958, Test: 0.2975\n",
            "Early stopping:  0.15293551952546114\n",
            "Epoch: 005, Loss: 2.0588, Train: 0.3395, Test: 0.3404\n",
            "Early stopping:  0.15933485305012965\n",
            "Epoch: 006, Loss: 1.9953, Train: 0.3888, Test: 0.3780\n",
            "Early stopping:  0.10204820203415521\n",
            "Epoch: 007, Loss: 1.9357, Train: 0.3999, Test: 0.3901\n",
            "Early stopping:  0.09446956401599091\n",
            "Epoch: 008, Loss: 1.8697, Train: 0.4027, Test: 0.3895\n",
            "Early stopping:  0.1013502569978026\n",
            "Epoch: 009, Loss: 1.8048, Train: 0.4121, Test: 0.4001\n",
            "Early stopping:  0.10018632036780564\n",
            "Epoch: 010, Loss: 1.7447, Train: 0.4353, Test: 0.4213\n",
            "Early stopping:  0.09996677811908394\n",
            "Epoch: 011, Loss: 1.6803, Train: 0.4702, Test: 0.4489\n",
            "Early stopping:  0.10054402858388224\n",
            "Epoch: 012, Loss: 1.6147, Train: 0.4960, Test: 0.4687\n",
            "Early stopping:  0.100356951177899\n",
            "Epoch: 013, Loss: 1.5530, Train: 0.5193, Test: 0.4842\n",
            "Early stopping:  0.10019950054702567\n",
            "Epoch: 014, Loss: 1.4925, Train: 0.5414, Test: 0.5001\n",
            "Early stopping:  0.0998920596028512\n",
            "Epoch: 015, Loss: 1.4341, Train: 0.5564, Test: 0.5162\n",
            "Early stopping:  0.09719502331539914\n",
            "Epoch: 016, Loss: 1.3806, Train: 0.5655, Test: 0.5290\n",
            "Early stopping:  0.09283519507677059\n",
            "Epoch: 017, Loss: 1.3305, Train: 0.5797, Test: 0.5415\n",
            "Early stopping:  0.08810701684443173\n",
            "Epoch: 018, Loss: 1.2831, Train: 0.5981, Test: 0.5528\n",
            "Early stopping:  0.0826521565010359\n",
            "Epoch: 019, Loss: 1.2400, Train: 0.6149, Test: 0.5647\n",
            "Early stopping:  0.07686956461645471\n",
            "Epoch: 020, Loss: 1.2030, Train: 0.6251, Test: 0.5759\n",
            "Early stopping:  0.07059890103670587\n",
            "Epoch: 021, Loss: 1.1694, Train: 0.6336, Test: 0.5844\n",
            "Early stopping:  0.06378413860129868\n",
            "Epoch: 022, Loss: 1.1367, Train: 0.6373, Test: 0.5868\n",
            "Early stopping:  0.05758482015063444\n",
            "Epoch: 023, Loss: 1.1055, Train: 0.6438, Test: 0.5925\n",
            "Early stopping:  0.053041326055163096\n",
            "Epoch: 024, Loss: 1.0767, Train: 0.6512, Test: 0.6014\n",
            "Early stopping:  0.050073697103102145\n",
            "Epoch: 025, Loss: 1.0477, Train: 0.6619, Test: 0.6116\n",
            "Early stopping:  0.047986389426267186\n",
            "Epoch: 026, Loss: 1.0192, Train: 0.6761, Test: 0.6178\n",
            "Early stopping:  0.04631061466970214\n",
            "Epoch: 027, Loss: 0.9912, Train: 0.6852, Test: 0.6269\n",
            "Early stopping:  0.04525505561615984\n",
            "Epoch: 028, Loss: 0.9647, Train: 0.6926, Test: 0.6324\n",
            "Early stopping:  0.044337853977067465\n",
            "Epoch: 029, Loss: 0.9387, Train: 0.7053, Test: 0.6335\n",
            "Early stopping:  0.04309378641606364\n",
            "Epoch: 030, Loss: 0.9146, Train: 0.7178, Test: 0.6384\n",
            "Early stopping:  0.04137685194170791\n",
            "Epoch: 031, Loss: 0.8907, Train: 0.7289, Test: 0.6417\n",
            "Early stopping:  0.03972197770433163\n",
            "Epoch: 032, Loss: 0.8675, Train: 0.7385, Test: 0.6460\n",
            "Early stopping:  0.03833229347290302\n",
            "Epoch: 033, Loss: 0.8463, Train: 0.7465, Test: 0.6494\n",
            "Early stopping:  0.03666932113953571\n",
            "Epoch: 034, Loss: 0.8247, Train: 0.7589, Test: 0.6534\n",
            "Early stopping:  0.03547067694577935\n",
            "Epoch: 035, Loss: 0.8043, Train: 0.7697, Test: 0.6558\n",
            "Early stopping:  0.034087679158018656\n",
            "Epoch: 036, Loss: 0.7843, Train: 0.7760, Test: 0.6590\n",
            "Early stopping:  0.03296564026034701\n",
            "Epoch: 037, Loss: 0.7646, Train: 0.7856, Test: 0.6624\n",
            "Early stopping:  0.03221373802901018\n",
            "Epoch: 038, Loss: 0.7459, Train: 0.7921, Test: 0.6681\n",
            "Early stopping:  0.031202475003294292\n",
            "Epoch: 039, Loss: 0.7273, Train: 0.7975, Test: 0.6713\n",
            "Early stopping:  0.030440325728650817\n",
            "Epoch: 040, Loss: 0.7093, Train: 0.8060, Test: 0.6727\n",
            "Early stopping:  0.02962546621430657\n",
            "Epoch: 041, Loss: 0.6917, Train: 0.8131, Test: 0.6768\n",
            "Early stopping:  0.02884919726619561\n",
            "Epoch: 042, Loss: 0.6745, Train: 0.8188, Test: 0.6793\n",
            "Early stopping:  0.0282025481381102\n",
            "Epoch: 043, Loss: 0.6574, Train: 0.8244, Test: 0.6812\n",
            "Early stopping:  0.027612546628032683\n",
            "Epoch: 044, Loss: 0.6408, Train: 0.8273, Test: 0.6834\n",
            "Early stopping:  0.0270956170002951\n",
            "Epoch: 045, Loss: 0.6245, Train: 0.8352, Test: 0.6832\n",
            "Early stopping:  0.026582026104434633\n",
            "Epoch: 046, Loss: 0.6087, Train: 0.8383, Test: 0.6866\n",
            "Early stopping:  0.025995272655703096\n",
            "Epoch: 047, Loss: 0.5940, Train: 0.8443, Test: 0.6849\n",
            "Early stopping:  0.025125912699540297\n",
            "Epoch: 048, Loss: 0.5807, Train: 0.8412, Test: 0.6912\n",
            "Early stopping:  0.0238573859189316\n",
            "Epoch: 049, Loss: 0.5699, Train: 0.8537, Test: 0.6859\n",
            "Early stopping:  0.021752007497915143\n",
            "Epoch: 050, Loss: 0.5553, Train: 0.8613, Test: 0.6953\n",
            "Early stopping:  0.020732809445344648\n",
            "Epoch: 051, Loss: 0.5367, Train: 0.8678, Test: 0.6968\n",
            "Early stopping:  0.022250218676354603\n",
            "Epoch: 052, Loss: 0.5220, Train: 0.8693, Test: 0.6946\n",
            "Early stopping:  0.023902384192748153\n",
            "Epoch: 053, Loss: 0.5132, Train: 0.8741, Test: 0.7012\n",
            "Early stopping:  0.023359584873964955\n",
            "Epoch: 054, Loss: 0.5014, Train: 0.8829, Test: 0.6999\n",
            "Early stopping:  0.020951733836858458\n",
            "Epoch: 055, Loss: 0.4845, Train: 0.8854, Test: 0.6995\n",
            "Early stopping:  0.01987017335690497\n",
            "Epoch: 056, Loss: 0.4729, Train: 0.8871, Test: 0.7010\n",
            "Early stopping:  0.020168986179752667\n",
            "Epoch: 057, Loss: 0.4647, Train: 0.8931, Test: 0.7002\n",
            "Early stopping:  0.01995620227993141\n",
            "Epoch: 058, Loss: 0.4511, Train: 0.8973, Test: 0.7023\n",
            "Early stopping:  0.019158215409504898\n",
            "Epoch: 059, Loss: 0.4371, Train: 0.9033, Test: 0.7038\n",
            "Early stopping:  0.01850640315784614\n",
            "Epoch: 060, Loss: 0.4279, Train: 0.9061, Test: 0.7029\n",
            "Early stopping:  0.018664753944159073\n",
            "Epoch: 061, Loss: 0.4181, Train: 0.9118, Test: 0.7040\n",
            "Early stopping:  0.018483990306457553\n",
            "Epoch: 062, Loss: 0.4058, Train: 0.9144, Test: 0.7014\n",
            "Early stopping:  0.017356609747548966\n",
            "Epoch: 063, Loss: 0.3938, Train: 0.9180, Test: 0.7023\n",
            "Early stopping:  0.017225596890304142\n",
            "Epoch: 064, Loss: 0.3848, Train: 0.9195, Test: 0.7027\n",
            "Early stopping:  0.017506991357136417\n",
            "Epoch: 065, Loss: 0.3765, Train: 0.9260, Test: 0.7040\n",
            "Early stopping:  0.016556031439183213\n",
            "Epoch: 066, Loss: 0.3665, Train: 0.9257, Test: 0.7031\n",
            "Early stopping:  0.015210858353148364\n",
            "Epoch: 067, Loss: 0.3556, Train: 0.9302, Test: 0.7033\n",
            "Early stopping:  0.014995713798153511\n",
            "Epoch: 068, Loss: 0.3450, Train: 0.9368, Test: 0.7055\n",
            "Early stopping:  0.015903794846086367\n",
            "Epoch: 069, Loss: 0.3363, Train: 0.9339, Test: 0.7048\n",
            "Early stopping:  0.016111425018466966\n",
            "Epoch: 070, Loss: 0.3288, Train: 0.9441, Test: 0.7061\n",
            "Early stopping:  0.015010542873270227\n",
            "Epoch: 071, Loss: 0.3218, Train: 0.9370, Test: 0.7046\n",
            "Early stopping:  0.013306563975813491\n",
            "Epoch: 072, Loss: 0.3148, Train: 0.9475, Test: 0.7063\n",
            "Early stopping:  0.011875427165734906\n",
            "Epoch: 073, Loss: 0.3066, Train: 0.9455, Test: 0.7048\n",
            "Early stopping:  0.011619489681335877\n",
            "Epoch: 074, Loss: 0.2977, Train: 0.9526, Test: 0.7086\n",
            "Early stopping:  0.012239880869221249\n",
            "Epoch: 075, Loss: 0.2881, Train: 0.9541, Test: 0.7089\n",
            "Early stopping:  0.013359237897675976\n",
            "Epoch: 076, Loss: 0.2796, Train: 0.9546, Test: 0.7088\n",
            "Early stopping:  0.014054578971731659\n",
            "Epoch: 077, Loss: 0.2727, Train: 0.9597, Test: 0.7103\n",
            "Early stopping:  0.013617852230728872\n",
            "Epoch: 078, Loss: 0.2670, Train: 0.9546, Test: 0.7089\n",
            "Early stopping:  0.012231712411696297\n",
            "Epoch: 079, Loss: 0.2628, Train: 0.9691, Test: 0.7072\n",
            "Early stopping:  0.010079576016695263\n",
            "Epoch: 080, Loss: 0.2594, Train: 0.9529, Test: 0.7055\n",
            "Early stopping:  0.008026988688025977\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.68      0.70       568\n",
            "         capital_goods       0.66      0.59      0.62       381\n",
            "conglomerates_industry       0.96      0.37      0.53        60\n",
            "     consumer_cyclical       0.68      0.61      0.64       595\n",
            " consumer_non-cyclical       0.73      0.60      0.66       334\n",
            "                energy       0.82      0.71      0.76       213\n",
            "             financial       0.75      0.70      0.72       576\n",
            "            healthcare       0.83      0.74      0.78       238\n",
            "              services       0.65      0.85      0.74      1557\n",
            "            technology       0.69      0.50      0.58       297\n",
            "        transportation       0.85      0.75      0.80       303\n",
            "             utilities       0.83      0.68      0.75       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.65      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.70      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4982, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2643, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16544205903290513\n",
            "Epoch: 003, Loss: 2.1655, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1709063500464597\n",
            "Epoch: 004, Loss: 2.1307, Train: 0.2964, Test: 0.2979\n",
            "Early stopping:  0.1656681521451803\n",
            "Epoch: 005, Loss: 2.0702, Train: 0.3486, Test: 0.3481\n",
            "Early stopping:  0.16778450734894207\n",
            "Epoch: 006, Loss: 2.0123, Train: 0.3988, Test: 0.3893\n",
            "Early stopping:  0.09578844302891511\n",
            "Epoch: 007, Loss: 1.9530, Train: 0.4075, Test: 0.3967\n",
            "Early stopping:  0.08624509184152591\n",
            "Epoch: 008, Loss: 1.8913, Train: 0.4010, Test: 0.3895\n",
            "Early stopping:  0.09424769092608234\n",
            "Epoch: 009, Loss: 1.8312, Train: 0.3990, Test: 0.3865\n",
            "Early stopping:  0.09471156681911728\n",
            "Epoch: 010, Loss: 1.7771, Train: 0.4112, Test: 0.3971\n",
            "Early stopping:  0.09367415427949403\n",
            "Epoch: 011, Loss: 1.7219, Train: 0.4376, Test: 0.4213\n",
            "Early stopping:  0.09116451151509623\n",
            "Epoch: 012, Loss: 1.6600, Train: 0.4671, Test: 0.4457\n",
            "Early stopping:  0.09043219515426257\n",
            "Epoch: 013, Loss: 1.6016, Train: 0.4847, Test: 0.4617\n",
            "Early stopping:  0.09113773913599266\n",
            "Epoch: 014, Loss: 1.5507, Train: 0.5077, Test: 0.4780\n",
            "Early stopping:  0.09066291760112412\n",
            "Epoch: 015, Loss: 1.4986, Train: 0.5312, Test: 0.4933\n",
            "Early stopping:  0.0879940398192638\n",
            "Epoch: 016, Loss: 1.4461, Train: 0.5445, Test: 0.5067\n",
            "Early stopping:  0.08397119863209834\n",
            "Epoch: 017, Loss: 1.3976, Train: 0.5579, Test: 0.5194\n",
            "Early stopping:  0.08106748387099366\n",
            "Epoch: 018, Loss: 1.3500, Train: 0.5777, Test: 0.5360\n",
            "Early stopping:  0.07945038718149981\n",
            "Epoch: 019, Loss: 1.3048, Train: 0.5973, Test: 0.5500\n",
            "Early stopping:  0.07648932775391831\n",
            "Epoch: 020, Loss: 1.2637, Train: 0.6129, Test: 0.5645\n",
            "Early stopping:  0.07238353136162361\n",
            "Epoch: 021, Loss: 1.2242, Train: 0.6202, Test: 0.5774\n",
            "Early stopping:  0.06852477291056795\n",
            "Epoch: 022, Loss: 1.1882, Train: 0.6268, Test: 0.5844\n",
            "Early stopping:  0.06396930079636429\n",
            "Epoch: 023, Loss: 1.1527, Train: 0.6327, Test: 0.5884\n",
            "Early stopping:  0.06005938158596087\n",
            "Epoch: 024, Loss: 1.1188, Train: 0.6410, Test: 0.5954\n",
            "Early stopping:  0.05713574035764142\n",
            "Epoch: 025, Loss: 1.0901, Train: 0.6495, Test: 0.6016\n",
            "Early stopping:  0.053448830117555725\n",
            "Epoch: 026, Loss: 1.0616, Train: 0.6605, Test: 0.6095\n",
            "Early stopping:  0.050025599150652036\n",
            "Epoch: 027, Loss: 1.0334, Train: 0.6730, Test: 0.6163\n",
            "Early stopping:  0.04680622314861403\n",
            "Epoch: 028, Loss: 1.0072, Train: 0.6838, Test: 0.6212\n",
            "Early stopping:  0.044253449612971896\n",
            "Epoch: 029, Loss: 0.9816, Train: 0.6877, Test: 0.6252\n",
            "Early stopping:  0.04293106287583753\n",
            "Epoch: 030, Loss: 0.9569, Train: 0.6960, Test: 0.6309\n",
            "Early stopping:  0.04133243023166597\n",
            "Epoch: 031, Loss: 0.9325, Train: 0.7050, Test: 0.6375\n",
            "Early stopping:  0.039881658254508005\n",
            "Epoch: 032, Loss: 0.9083, Train: 0.7223, Test: 0.6452\n",
            "Early stopping:  0.03902041494956762\n",
            "Epoch: 033, Loss: 0.8855, Train: 0.7357, Test: 0.6486\n",
            "Early stopping:  0.03805468315744585\n",
            "Epoch: 034, Loss: 0.8637, Train: 0.7416, Test: 0.6511\n",
            "Early stopping:  0.03688829434104235\n",
            "Epoch: 035, Loss: 0.8420, Train: 0.7504, Test: 0.6513\n",
            "Early stopping:  0.03566043094003402\n",
            "Epoch: 036, Loss: 0.8222, Train: 0.7581, Test: 0.6551\n",
            "Early stopping:  0.03410784436317747\n",
            "Epoch: 037, Loss: 0.8021, Train: 0.7694, Test: 0.6607\n",
            "Early stopping:  0.032935222904775976\n",
            "Epoch: 038, Loss: 0.7831, Train: 0.7751, Test: 0.6645\n",
            "Early stopping:  0.031814230877212045\n",
            "Epoch: 039, Loss: 0.7640, Train: 0.7828, Test: 0.6689\n",
            "Early stopping:  0.030853526128356748\n",
            "Epoch: 040, Loss: 0.7455, Train: 0.7862, Test: 0.6725\n",
            "Early stopping:  0.030299493596018162\n",
            "Epoch: 041, Loss: 0.7273, Train: 0.7969, Test: 0.6770\n",
            "Early stopping:  0.029605172561989346\n",
            "Epoch: 042, Loss: 0.7088, Train: 0.8049, Test: 0.6827\n",
            "Early stopping:  0.02930694753741724\n",
            "Epoch: 043, Loss: 0.6913, Train: 0.8114, Test: 0.6840\n",
            "Early stopping:  0.0287922192619061\n",
            "Epoch: 044, Loss: 0.6737, Train: 0.8171, Test: 0.6853\n",
            "Early stopping:  0.02839952144775316\n",
            "Epoch: 045, Loss: 0.6565, Train: 0.8213, Test: 0.6881\n",
            "Early stopping:  0.027962961711492235\n",
            "Epoch: 046, Loss: 0.6398, Train: 0.8276, Test: 0.6914\n",
            "Early stopping:  0.027340172398280363\n",
            "Epoch: 047, Loss: 0.6234, Train: 0.8332, Test: 0.6951\n",
            "Early stopping:  0.026841892399037417\n",
            "Epoch: 048, Loss: 0.6073, Train: 0.8392, Test: 0.6963\n",
            "Early stopping:  0.026228507278884528\n",
            "Epoch: 049, Loss: 0.5913, Train: 0.8463, Test: 0.6997\n",
            "Early stopping:  0.02576579687801968\n",
            "Epoch: 050, Loss: 0.5758, Train: 0.8542, Test: 0.7014\n",
            "Early stopping:  0.025316909946177874\n",
            "Epoch: 051, Loss: 0.5605, Train: 0.8602, Test: 0.7033\n",
            "Early stopping:  0.02486872770246432\n",
            "Epoch: 052, Loss: 0.5455, Train: 0.8701, Test: 0.7038\n",
            "Early stopping:  0.024396682912575973\n",
            "Epoch: 053, Loss: 0.5307, Train: 0.8758, Test: 0.7070\n",
            "Early stopping:  0.02392851939972882\n",
            "Epoch: 054, Loss: 0.5160, Train: 0.8783, Test: 0.7063\n",
            "Early stopping:  0.02360687158767896\n",
            "Epoch: 055, Loss: 0.5017, Train: 0.8834, Test: 0.7070\n",
            "Early stopping:  0.02324927240649515\n",
            "Epoch: 056, Loss: 0.4876, Train: 0.8880, Test: 0.7078\n",
            "Early stopping:  0.022880401819579513\n",
            "Epoch: 057, Loss: 0.4738, Train: 0.8917, Test: 0.7074\n",
            "Early stopping:  0.022482313627433553\n",
            "Epoch: 058, Loss: 0.4604, Train: 0.8959, Test: 0.7082\n",
            "Early stopping:  0.022017738354646474\n",
            "Epoch: 059, Loss: 0.4472, Train: 0.9013, Test: 0.7082\n",
            "Early stopping:  0.021566510491548507\n",
            "Epoch: 060, Loss: 0.4342, Train: 0.9058, Test: 0.7082\n",
            "Early stopping:  0.0211007932488648\n",
            "Epoch: 061, Loss: 0.4218, Train: 0.9129, Test: 0.7097\n",
            "Early stopping:  0.02058574176956416\n",
            "Epoch: 062, Loss: 0.4100, Train: 0.9087, Test: 0.7099\n",
            "Early stopping:  0.01995281665310296\n",
            "Epoch: 063, Loss: 0.4004, Train: 0.9189, Test: 0.7101\n",
            "Early stopping:  0.018666578274424885\n",
            "Epoch: 064, Loss: 0.3934, Train: 0.9104, Test: 0.7105\n",
            "Early stopping:  0.016405018448334866\n",
            "Epoch: 065, Loss: 0.3840, Train: 0.9260, Test: 0.7116\n",
            "Early stopping:  0.014622791906501342\n",
            "Epoch: 066, Loss: 0.3669, Train: 0.9299, Test: 0.7120\n",
            "Early stopping:  0.016465928622930923\n",
            "Epoch: 067, Loss: 0.3556, Train: 0.9251, Test: 0.7093\n",
            "Early stopping:  0.01857828700054955\n",
            "Epoch: 068, Loss: 0.3504, Train: 0.9370, Test: 0.7120\n",
            "Early stopping:  0.018351441057419045\n",
            "Epoch: 069, Loss: 0.3375, Train: 0.9393, Test: 0.7129\n",
            "Early stopping:  0.01757068854507009\n",
            "Epoch: 070, Loss: 0.3261, Train: 0.9351, Test: 0.7116\n",
            "Early stopping:  0.01585494379290565\n",
            "Epoch: 071, Loss: 0.3209, Train: 0.9467, Test: 0.7146\n",
            "Early stopping:  0.014951968263257391\n",
            "Epoch: 072, Loss: 0.3099, Train: 0.9478, Test: 0.7125\n",
            "Early stopping:  0.015549858597582521\n",
            "Epoch: 073, Loss: 0.2994, Train: 0.9481, Test: 0.7125\n",
            "Early stopping:  0.014693468936045681\n",
            "Epoch: 074, Loss: 0.2944, Train: 0.9546, Test: 0.7142\n",
            "Early stopping:  0.013531509666365574\n",
            "Epoch: 075, Loss: 0.2851, Train: 0.9572, Test: 0.7133\n",
            "Early stopping:  0.013888167004325463\n",
            "Epoch: 076, Loss: 0.2756, Train: 0.9577, Test: 0.7131\n",
            "Early stopping:  0.013186582647785109\n",
            "Epoch: 077, Loss: 0.2708, Train: 0.9611, Test: 0.7137\n",
            "Early stopping:  0.012115014106017568\n",
            "Epoch: 078, Loss: 0.2627, Train: 0.9665, Test: 0.7131\n",
            "Early stopping:  0.012365201925439549\n",
            "Epoch: 079, Loss: 0.2543, Train: 0.9637, Test: 0.7133\n",
            "Early stopping:  0.01183342602188116\n",
            "Epoch: 080, Loss: 0.2496, Train: 0.9711, Test: 0.7144\n",
            "Early stopping:  0.010889193583837636\n",
            "Epoch: 081, Loss: 0.2428, Train: 0.9702, Test: 0.7150\n",
            "Early stopping:  0.01099878182825761\n",
            "Epoch: 082, Loss: 0.2351, Train: 0.9705, Test: 0.7140\n",
            "Early stopping:  0.010573771471743097\n",
            "Epoch: 083, Loss: 0.2306, Train: 0.9745, Test: 0.7127\n",
            "Early stopping:  0.00981035235547596\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.74      0.70       568\n",
            "         capital_goods       0.67      0.61      0.64       381\n",
            "conglomerates_industry       0.97      0.50      0.66        60\n",
            "     consumer_cyclical       0.67      0.66      0.67       595\n",
            " consumer_non-cyclical       0.72      0.61      0.66       334\n",
            "                energy       0.83      0.69      0.75       213\n",
            "             financial       0.75      0.72      0.74       576\n",
            "            healthcare       0.85      0.76      0.80       238\n",
            "              services       0.69      0.80      0.74      1557\n",
            "            technology       0.66      0.54      0.59       297\n",
            "        transportation       0.79      0.75      0.77       303\n",
            "             utilities       0.87      0.69      0.77       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.67      0.71      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4424, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2501, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1359670819952965\n",
            "Epoch: 003, Loss: 2.1772, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1370111000679412\n",
            "Epoch: 004, Loss: 2.1298, Train: 0.3006, Test: 0.3032\n",
            "Early stopping:  0.13754441325516162\n",
            "Epoch: 005, Loss: 2.0614, Train: 0.3494, Test: 0.3442\n",
            "Early stopping:  0.14592388182720784\n",
            "Epoch: 006, Loss: 2.0010, Train: 0.3763, Test: 0.3714\n",
            "Early stopping:  0.09723320909321734\n",
            "Epoch: 007, Loss: 1.9400, Train: 0.3885, Test: 0.3816\n",
            "Early stopping:  0.09548650818449728\n",
            "Epoch: 008, Loss: 1.8751, Train: 0.3948, Test: 0.3865\n",
            "Early stopping:  0.09975946039375623\n",
            "Epoch: 009, Loss: 1.8082, Train: 0.4087, Test: 0.4041\n",
            "Early stopping:  0.09998812842006279\n",
            "Epoch: 010, Loss: 1.7388, Train: 0.4376, Test: 0.4228\n",
            "Early stopping:  0.10377263298811604\n",
            "Epoch: 011, Loss: 1.6686, Train: 0.4637, Test: 0.4432\n",
            "Early stopping:  0.10738475140665887\n",
            "Epoch: 012, Loss: 1.6036, Train: 0.4810, Test: 0.4598\n",
            "Early stopping:  0.10794843076080626\n",
            "Epoch: 013, Loss: 1.5428, Train: 0.5150, Test: 0.4884\n",
            "Early stopping:  0.1053740707359358\n",
            "Epoch: 014, Loss: 1.4818, Train: 0.5502, Test: 0.5169\n",
            "Early stopping:  0.10122356767965181\n",
            "Epoch: 015, Loss: 1.4245, Train: 0.5732, Test: 0.5392\n",
            "Early stopping:  0.09647799043278328\n",
            "Epoch: 016, Loss: 1.3720, Train: 0.5910, Test: 0.5502\n",
            "Early stopping:  0.09198664648845593\n",
            "Epoch: 017, Loss: 1.3222, Train: 0.5993, Test: 0.5642\n",
            "Early stopping:  0.08719467923066059\n",
            "Epoch: 018, Loss: 1.2747, Train: 0.6007, Test: 0.5693\n",
            "Early stopping:  0.08174001068574115\n",
            "Epoch: 019, Loss: 1.2302, Train: 0.6098, Test: 0.5732\n",
            "Early stopping:  0.07686357035930334\n",
            "Epoch: 020, Loss: 1.1923, Train: 0.6205, Test: 0.5834\n",
            "Early stopping:  0.07146340479800911\n",
            "Epoch: 021, Loss: 1.1585, Train: 0.6302, Test: 0.5874\n",
            "Early stopping:  0.06494970171537418\n",
            "Epoch: 022, Loss: 1.1295, Train: 0.6390, Test: 0.5918\n",
            "Early stopping:  0.05744143560973904\n",
            "Epoch: 023, Loss: 1.1018, Train: 0.6452, Test: 0.5991\n",
            "Early stopping:  0.05065175232471283\n",
            "Epoch: 024, Loss: 1.0730, Train: 0.6566, Test: 0.6048\n",
            "Early stopping:  0.046730905814428146\n",
            "Epoch: 025, Loss: 1.0449, Train: 0.6642, Test: 0.6141\n",
            "Early stopping:  0.044857428629233594\n",
            "Epoch: 026, Loss: 1.0183, Train: 0.6744, Test: 0.6186\n",
            "Early stopping:  0.04414238371280871\n",
            "Epoch: 027, Loss: 0.9915, Train: 0.6858, Test: 0.6243\n",
            "Early stopping:  0.04351019758950056\n",
            "Epoch: 028, Loss: 0.9660, Train: 0.6977, Test: 0.6305\n",
            "Early stopping:  0.04227322169930937\n",
            "Epoch: 029, Loss: 0.9428, Train: 0.7119, Test: 0.6373\n",
            "Early stopping:  0.04058777629294893\n",
            "Epoch: 030, Loss: 0.9194, Train: 0.7161, Test: 0.6390\n",
            "Early stopping:  0.03899687903557216\n",
            "Epoch: 031, Loss: 0.8967, Train: 0.7277, Test: 0.6435\n",
            "Early stopping:  0.03734699854786809\n",
            "Epoch: 032, Loss: 0.8750, Train: 0.7343, Test: 0.6503\n",
            "Early stopping:  0.03607129179812695\n",
            "Epoch: 033, Loss: 0.8541, Train: 0.7442, Test: 0.6517\n",
            "Early stopping:  0.035101299577887324\n",
            "Epoch: 034, Loss: 0.8331, Train: 0.7558, Test: 0.6538\n",
            "Early stopping:  0.03406542310212378\n",
            "Epoch: 035, Loss: 0.8126, Train: 0.7601, Test: 0.6572\n",
            "Early stopping:  0.03324051188481914\n",
            "Epoch: 036, Loss: 0.7926, Train: 0.7677, Test: 0.6606\n",
            "Early stopping:  0.03260773094138075\n",
            "Epoch: 037, Loss: 0.7728, Train: 0.7768, Test: 0.6655\n",
            "Early stopping:  0.03209666563141843\n",
            "Epoch: 038, Loss: 0.7533, Train: 0.7881, Test: 0.6711\n",
            "Early stopping:  0.03149771576971345\n",
            "Epoch: 039, Loss: 0.7348, Train: 0.7952, Test: 0.6738\n",
            "Early stopping:  0.030796745566654224\n",
            "Epoch: 040, Loss: 0.7167, Train: 0.8020, Test: 0.6770\n",
            "Early stopping:  0.03000694557953398\n",
            "Epoch: 041, Loss: 0.6987, Train: 0.8103, Test: 0.6770\n",
            "Early stopping:  0.029240443848066164\n",
            "Epoch: 042, Loss: 0.6820, Train: 0.8097, Test: 0.6819\n",
            "Early stopping:  0.02830087339596021\n",
            "Epoch: 043, Loss: 0.6676, Train: 0.8188, Test: 0.6762\n",
            "Early stopping:  0.02677751884608264\n",
            "Epoch: 044, Loss: 0.6574, Train: 0.8185, Test: 0.6864\n",
            "Early stopping:  0.02378534074482699\n",
            "Epoch: 045, Loss: 0.6427, Train: 0.8321, Test: 0.6880\n",
            "Early stopping:  0.021639267467609705\n",
            "Epoch: 046, Loss: 0.6189, Train: 0.8361, Test: 0.6866\n",
            "Early stopping:  0.024165091632976813\n",
            "Epoch: 047, Loss: 0.6080, Train: 0.8352, Test: 0.6910\n",
            "Early stopping:  0.025195061572649353\n",
            "Epoch: 048, Loss: 0.5981, Train: 0.8486, Test: 0.6931\n",
            "Early stopping:  0.02456627359221413\n",
            "Epoch: 049, Loss: 0.5772, Train: 0.8537, Test: 0.6914\n",
            "Early stopping:  0.024310378883522787\n",
            "Epoch: 050, Loss: 0.5668, Train: 0.8480, Test: 0.6955\n",
            "Early stopping:  0.021525368159484436\n",
            "Epoch: 051, Loss: 0.5567, Train: 0.8625, Test: 0.6942\n",
            "Early stopping:  0.021364056461726358\n",
            "Epoch: 052, Loss: 0.5384, Train: 0.8678, Test: 0.6936\n",
            "Early stopping:  0.022344999196340358\n",
            "Epoch: 053, Loss: 0.5284, Train: 0.8627, Test: 0.6984\n",
            "Early stopping:  0.020063691503113063\n",
            "Epoch: 054, Loss: 0.5186, Train: 0.8775, Test: 0.6950\n",
            "Early stopping:  0.019865650338705274\n",
            "Epoch: 055, Loss: 0.5025, Train: 0.8800, Test: 0.6963\n",
            "Early stopping:  0.020402582700611887\n",
            "Epoch: 056, Loss: 0.4917, Train: 0.8758, Test: 0.6995\n",
            "Early stopping:  0.01893997906926089\n",
            "Epoch: 057, Loss: 0.4833, Train: 0.8866, Test: 0.6959\n",
            "Early stopping:  0.018602446762466734\n",
            "Epoch: 058, Loss: 0.4699, Train: 0.8914, Test: 0.6989\n",
            "Early stopping:  0.018549847270374058\n",
            "Epoch: 059, Loss: 0.4569, Train: 0.8880, Test: 0.7029\n",
            "Early stopping:  0.017924997166281736\n",
            "Epoch: 060, Loss: 0.4490, Train: 0.8996, Test: 0.6967\n",
            "Early stopping:  0.01773306181450084\n",
            "Epoch: 061, Loss: 0.4394, Train: 0.9005, Test: 0.7055\n",
            "Early stopping:  0.017295518418241763\n",
            "Epoch: 062, Loss: 0.4259, Train: 0.9033, Test: 0.7061\n",
            "Early stopping:  0.016746927935738575\n",
            "Epoch: 063, Loss: 0.4155, Train: 0.9112, Test: 0.6993\n",
            "Early stopping:  0.01682668134032332\n",
            "Epoch: 064, Loss: 0.4085, Train: 0.9047, Test: 0.7042\n",
            "Early stopping:  0.01667164489658368\n",
            "Epoch: 065, Loss: 0.3995, Train: 0.9158, Test: 0.7014\n",
            "Early stopping:  0.01546673228132845\n",
            "Epoch: 066, Loss: 0.3884, Train: 0.9178, Test: 0.7050\n",
            "Early stopping:  0.014414924503083262\n",
            "Epoch: 067, Loss: 0.3780, Train: 0.9212, Test: 0.7086\n",
            "Early stopping:  0.015091092076097672\n",
            "Epoch: 068, Loss: 0.3688, Train: 0.9265, Test: 0.7059\n",
            "Early stopping:  0.015966139726822096\n",
            "Epoch: 069, Loss: 0.3595, Train: 0.9271, Test: 0.7091\n",
            "Early stopping:  0.01576971449903464\n",
            "Epoch: 070, Loss: 0.3514, Train: 0.9322, Test: 0.7059\n",
            "Early stopping:  0.014642359674213833\n",
            "Epoch: 071, Loss: 0.3456, Train: 0.9282, Test: 0.7072\n",
            "Early stopping:  0.013040158510036745\n",
            "Epoch: 072, Loss: 0.3416, Train: 0.9334, Test: 0.6999\n",
            "Early stopping:  0.010940654147754737\n",
            "Epoch: 073, Loss: 0.3405, Train: 0.9237, Test: 0.7070\n",
            "Early stopping:  0.007850761585390907\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.70      0.71       568\n",
            "         capital_goods       0.69      0.56      0.62       381\n",
            "conglomerates_industry       0.90      0.43      0.58        60\n",
            "     consumer_cyclical       0.75      0.60      0.67       595\n",
            " consumer_non-cyclical       0.72      0.56      0.63       334\n",
            "                energy       0.81      0.74      0.77       213\n",
            "             financial       0.77      0.74      0.75       576\n",
            "            healthcare       0.81      0.77      0.79       238\n",
            "              services       0.64      0.83      0.72      1557\n",
            "            technology       0.67      0.47      0.55       297\n",
            "        transportation       0.79      0.77      0.78       303\n",
            "             utilities       0.81      0.72      0.76       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.66      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.70      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4894, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2712, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1542893285673464\n",
            "Epoch: 003, Loss: 2.1685, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16386509899631915\n",
            "Epoch: 004, Loss: 2.1126, Train: 0.3151, Test: 0.3143\n",
            "Early stopping:  0.16617124861557148\n",
            "Epoch: 005, Loss: 2.0473, Train: 0.3695, Test: 0.3640\n",
            "Early stopping:  0.17261815381815107\n",
            "Epoch: 006, Loss: 1.9881, Train: 0.3883, Test: 0.3820\n",
            "Early stopping:  0.10945579475170504\n",
            "Epoch: 007, Loss: 1.9226, Train: 0.3919, Test: 0.3859\n",
            "Early stopping:  0.09749168333082332\n",
            "Epoch: 008, Loss: 1.8548, Train: 0.3999, Test: 0.3935\n",
            "Early stopping:  0.1012576312558927\n",
            "Epoch: 009, Loss: 1.7900, Train: 0.4209, Test: 0.4126\n",
            "Early stopping:  0.10247432850294023\n",
            "Epoch: 010, Loss: 1.7234, Train: 0.4487, Test: 0.4305\n",
            "Early stopping:  0.10466953990544196\n",
            "Epoch: 011, Loss: 1.6584, Train: 0.4748, Test: 0.4498\n",
            "Early stopping:  0.10433321371790238\n",
            "Epoch: 012, Loss: 1.5994, Train: 0.4963, Test: 0.4666\n",
            "Early stopping:  0.10161900411808904\n",
            "Epoch: 013, Loss: 1.5388, Train: 0.5187, Test: 0.4863\n",
            "Early stopping:  0.09907552695444649\n",
            "Epoch: 014, Loss: 1.4755, Train: 0.5462, Test: 0.5146\n",
            "Early stopping:  0.09731640938712212\n",
            "Epoch: 015, Loss: 1.4185, Train: 0.5720, Test: 0.5330\n",
            "Early stopping:  0.09543464962051373\n",
            "Epoch: 016, Loss: 1.3691, Train: 0.5865, Test: 0.5462\n",
            "Early stopping:  0.09192260110661848\n",
            "Epoch: 017, Loss: 1.3219, Train: 0.5944, Test: 0.5562\n",
            "Early stopping:  0.0855869652433871\n",
            "Epoch: 018, Loss: 1.2755, Train: 0.6052, Test: 0.5596\n",
            "Early stopping:  0.07861668585649159\n",
            "Epoch: 019, Loss: 1.2340, Train: 0.6106, Test: 0.5668\n",
            "Early stopping:  0.073200378244312\n",
            "Epoch: 020, Loss: 1.2001, Train: 0.6180, Test: 0.5751\n",
            "Early stopping:  0.06747434197273632\n",
            "Epoch: 021, Loss: 1.1679, Train: 0.6265, Test: 0.5817\n",
            "Early stopping:  0.06079215843994392\n",
            "Epoch: 022, Loss: 1.1353, Train: 0.6364, Test: 0.5901\n",
            "Early stopping:  0.054834625751624254\n",
            "Epoch: 023, Loss: 1.1045, Train: 0.6472, Test: 0.5984\n",
            "Early stopping:  0.05121005294065013\n",
            "Epoch: 024, Loss: 1.0752, Train: 0.6568, Test: 0.6082\n",
            "Early stopping:  0.04956171732271709\n",
            "Epoch: 025, Loss: 1.0480, Train: 0.6676, Test: 0.6141\n",
            "Early stopping:  0.04746011192902273\n",
            "Epoch: 026, Loss: 1.0196, Train: 0.6795, Test: 0.6152\n",
            "Early stopping:  0.04553611912867413\n",
            "Epoch: 027, Loss: 0.9922, Train: 0.6894, Test: 0.6182\n",
            "Early stopping:  0.044302945460996786\n",
            "Epoch: 028, Loss: 0.9675, Train: 0.6980, Test: 0.6233\n",
            "Early stopping:  0.042893145038090975\n",
            "Epoch: 029, Loss: 0.9432, Train: 0.7070, Test: 0.6328\n",
            "Early stopping:  0.0414109548297378\n",
            "Epoch: 030, Loss: 0.9197, Train: 0.7192, Test: 0.6384\n",
            "Early stopping:  0.039348435575187475\n",
            "Epoch: 031, Loss: 0.8971, Train: 0.7263, Test: 0.6445\n",
            "Early stopping:  0.037620192137941835\n",
            "Epoch: 032, Loss: 0.8746, Train: 0.7328, Test: 0.6473\n",
            "Early stopping:  0.036665933229298384\n",
            "Epoch: 033, Loss: 0.8530, Train: 0.7425, Test: 0.6509\n",
            "Early stopping:  0.03565524324319684\n",
            "Epoch: 034, Loss: 0.8321, Train: 0.7518, Test: 0.6541\n",
            "Early stopping:  0.03467502999861972\n",
            "Epoch: 035, Loss: 0.8117, Train: 0.7606, Test: 0.6596\n",
            "Early stopping:  0.033725789218527535\n",
            "Epoch: 036, Loss: 0.7919, Train: 0.7723, Test: 0.6640\n",
            "Early stopping:  0.03268025457786659\n",
            "Epoch: 037, Loss: 0.7726, Train: 0.7796, Test: 0.6672\n",
            "Early stopping:  0.03179157425162688\n",
            "Epoch: 038, Loss: 0.7541, Train: 0.7850, Test: 0.6698\n",
            "Early stopping:  0.03085872378337173\n",
            "Epoch: 039, Loss: 0.7360, Train: 0.7876, Test: 0.6742\n",
            "Early stopping:  0.029917528415358653\n",
            "Epoch: 040, Loss: 0.7186, Train: 0.7933, Test: 0.6749\n",
            "Early stopping:  0.02897820713388326\n",
            "Epoch: 041, Loss: 0.7013, Train: 0.8046, Test: 0.6791\n",
            "Early stopping:  0.028165991112890577\n",
            "Epoch: 042, Loss: 0.6840, Train: 0.8140, Test: 0.6821\n",
            "Early stopping:  0.02765487867262142\n",
            "Epoch: 043, Loss: 0.6671, Train: 0.8191, Test: 0.6825\n",
            "Early stopping:  0.02724665387633132\n",
            "Epoch: 044, Loss: 0.6502, Train: 0.8261, Test: 0.6830\n",
            "Early stopping:  0.02703184482448085\n",
            "Epoch: 045, Loss: 0.6337, Train: 0.8344, Test: 0.6853\n",
            "Early stopping:  0.026719661933562434\n",
            "Epoch: 046, Loss: 0.6178, Train: 0.8406, Test: 0.6893\n",
            "Early stopping:  0.026215383023609894\n",
            "Epoch: 047, Loss: 0.6020, Train: 0.8480, Test: 0.6950\n",
            "Early stopping:  0.02570884222103879\n",
            "Epoch: 048, Loss: 0.5864, Train: 0.8537, Test: 0.6961\n",
            "Early stopping:  0.025170642153646946\n",
            "Epoch: 049, Loss: 0.5712, Train: 0.8573, Test: 0.6984\n",
            "Early stopping:  0.02472312641955748\n",
            "Epoch: 050, Loss: 0.5563, Train: 0.8619, Test: 0.7002\n",
            "Early stopping:  0.024313705301613503\n",
            "Epoch: 051, Loss: 0.5419, Train: 0.8647, Test: 0.7018\n",
            "Early stopping:  0.02376497039466232\n",
            "Epoch: 052, Loss: 0.5279, Train: 0.8681, Test: 0.7027\n",
            "Early stopping:  0.023160776929506334\n",
            "Epoch: 053, Loss: 0.5139, Train: 0.8727, Test: 0.7021\n",
            "Early stopping:  0.022650349823066695\n",
            "Epoch: 054, Loss: 0.5004, Train: 0.8769, Test: 0.7046\n",
            "Early stopping:  0.02213152861214302\n",
            "Epoch: 055, Loss: 0.4877, Train: 0.8795, Test: 0.7046\n",
            "Early stopping:  0.021480600938655878\n",
            "Epoch: 056, Loss: 0.4783, Train: 0.8786, Test: 0.6938\n",
            "Early stopping:  0.01985804147184847\n",
            "Epoch: 057, Loss: 0.4760, Train: 0.8783, Test: 0.7016\n",
            "Early stopping:  0.01587739416301272\n",
            "Epoch: 058, Loss: 0.4709, Train: 0.8968, Test: 0.7023\n",
            "Early stopping:  0.011642503752010735\n",
            "Epoch: 059, Loss: 0.4435, Train: 0.8968, Test: 0.6997\n",
            "Early stopping:  0.01668906931032301\n",
            "Epoch: 060, Loss: 0.4354, Train: 0.8917, Test: 0.7050\n",
            "Early stopping:  0.019894250234456862\n",
            "Epoch: 061, Loss: 0.4330, Train: 0.9087, Test: 0.7089\n",
            "Early stopping:  0.02024969074104113\n",
            "Epoch: 062, Loss: 0.4112, Train: 0.9058, Test: 0.7001\n",
            "Early stopping:  0.021560335435466163\n",
            "Epoch: 063, Loss: 0.4090, Train: 0.9073, Test: 0.7044\n",
            "Early stopping:  0.01540817604904692\n",
            "Epoch: 064, Loss: 0.3981, Train: 0.9149, Test: 0.7088\n",
            "Early stopping:  0.01618759351291323\n",
            "Epoch: 065, Loss: 0.3847, Train: 0.9163, Test: 0.7036\n",
            "Early stopping:  0.0178445569706372\n",
            "Epoch: 066, Loss: 0.3815, Train: 0.9220, Test: 0.7091\n",
            "Early stopping:  0.013595638637833347\n",
            "Epoch: 067, Loss: 0.3677, Train: 0.9214, Test: 0.7093\n",
            "Early stopping:  0.01590044971273979\n",
            "Epoch: 068, Loss: 0.3617, Train: 0.9294, Test: 0.7069\n",
            "Early stopping:  0.014431469271780182\n",
            "Epoch: 069, Loss: 0.3534, Train: 0.9336, Test: 0.7078\n",
            "Early stopping:  0.013209772063643961\n",
            "Epoch: 070, Loss: 0.3448, Train: 0.9297, Test: 0.7088\n",
            "Early stopping:  0.01397792979826324\n",
            "Epoch: 071, Loss: 0.3377, Train: 0.9342, Test: 0.7086\n",
            "Early stopping:  0.0121733566338655\n",
            "Epoch: 072, Loss: 0.3303, Train: 0.9399, Test: 0.7069\n",
            "Early stopping:  0.012423918924668901\n",
            "Epoch: 073, Loss: 0.3234, Train: 0.9402, Test: 0.7082\n",
            "Early stopping:  0.011783987041850384\n",
            "Epoch: 074, Loss: 0.3142, Train: 0.9385, Test: 0.7070\n",
            "Early stopping:  0.011921280750060787\n",
            "Epoch: 075, Loss: 0.3104, Train: 0.9453, Test: 0.7097\n",
            "Early stopping:  0.01121755888463492\n",
            "Epoch: 076, Loss: 0.3012, Train: 0.9490, Test: 0.7086\n",
            "Early stopping:  0.011326575852247266\n",
            "Epoch: 077, Loss: 0.2943, Train: 0.9438, Test: 0.7059\n",
            "Early stopping:  0.011341578644996459\n",
            "Epoch: 078, Loss: 0.2897, Train: 0.9498, Test: 0.7080\n",
            "Early stopping:  0.01040847620350694\n",
            "Epoch: 079, Loss: 0.2817, Train: 0.9543, Test: 0.7067\n",
            "Early stopping:  0.010962902777116444\n",
            "Epoch: 080, Loss: 0.2764, Train: 0.9532, Test: 0.7069\n",
            "Early stopping:  0.009853822958942864\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.72      0.70       568\n",
            "         capital_goods       0.67      0.57      0.62       381\n",
            "conglomerates_industry       1.00      0.45      0.62        60\n",
            "     consumer_cyclical       0.63      0.65      0.64       595\n",
            " consumer_non-cyclical       0.67      0.59      0.63       334\n",
            "                energy       0.80      0.76      0.78       213\n",
            "             financial       0.79      0.71      0.75       576\n",
            "            healthcare       0.87      0.76      0.81       238\n",
            "              services       0.68      0.81      0.74      1557\n",
            "            technology       0.66      0.54      0.59       297\n",
            "        transportation       0.83      0.74      0.78       303\n",
            "             utilities       0.83      0.69      0.75       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.66      0.70      5291\n",
            "          weighted avg       0.71      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4782, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2725, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14542736361205177\n",
            "Epoch: 003, Loss: 2.1541, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16400759162607403\n",
            "Epoch: 004, Loss: 2.1325, Train: 0.2978, Test: 0.2984\n",
            "Early stopping:  0.15838097864339262\n",
            "Epoch: 005, Loss: 2.0688, Train: 0.3480, Test: 0.3472\n",
            "Early stopping:  0.16147224971805935\n",
            "Epoch: 006, Loss: 1.9994, Train: 0.3931, Test: 0.3859\n",
            "Early stopping:  0.1019841646499604\n",
            "Epoch: 007, Loss: 1.9333, Train: 0.4064, Test: 0.3933\n",
            "Early stopping:  0.09203748734401068\n",
            "Epoch: 008, Loss: 1.8693, Train: 0.4050, Test: 0.3943\n",
            "Early stopping:  0.10463974722035731\n",
            "Epoch: 009, Loss: 1.8110, Train: 0.4146, Test: 0.4048\n",
            "Early stopping:  0.10214204672314368\n",
            "Epoch: 010, Loss: 1.7536, Train: 0.4402, Test: 0.4245\n",
            "Early stopping:  0.09711967759448001\n",
            "Epoch: 011, Loss: 1.6901, Train: 0.4586, Test: 0.4428\n",
            "Early stopping:  0.09523298283505517\n",
            "Epoch: 012, Loss: 1.6249, Train: 0.4821, Test: 0.4553\n",
            "Early stopping:  0.09647168444716103\n",
            "Epoch: 013, Loss: 1.5661, Train: 0.5011, Test: 0.4642\n",
            "Early stopping:  0.0977936491570412\n",
            "Epoch: 014, Loss: 1.5100, Train: 0.5156, Test: 0.4831\n",
            "Early stopping:  0.09665173557368568\n",
            "Epoch: 015, Loss: 1.4521, Train: 0.5329, Test: 0.5029\n",
            "Early stopping:  0.09342913892438469\n",
            "Epoch: 016, Loss: 1.3959, Train: 0.5624, Test: 0.5237\n",
            "Early stopping:  0.09042414044457721\n",
            "Epoch: 017, Loss: 1.3423, Train: 0.5922, Test: 0.5413\n",
            "Early stopping:  0.08884187463451805\n",
            "Epoch: 018, Loss: 1.2947, Train: 0.5984, Test: 0.5579\n",
            "Early stopping:  0.08551660694555674\n",
            "Epoch: 019, Loss: 1.2537, Train: 0.6086, Test: 0.5702\n",
            "Early stopping:  0.07891461226696743\n",
            "Epoch: 020, Loss: 1.2126, Train: 0.6134, Test: 0.5770\n",
            "Early stopping:  0.07211851974154955\n",
            "Epoch: 021, Loss: 1.1726, Train: 0.6259, Test: 0.5799\n",
            "Early stopping:  0.06667556653161939\n",
            "Epoch: 022, Loss: 1.1373, Train: 0.6367, Test: 0.5865\n",
            "Early stopping:  0.06263149996550758\n",
            "Epoch: 023, Loss: 1.1046, Train: 0.6435, Test: 0.5944\n",
            "Early stopping:  0.05911956667900604\n",
            "Epoch: 024, Loss: 1.0745, Train: 0.6509, Test: 0.6010\n",
            "Early stopping:  0.05451833818770066\n",
            "Epoch: 025, Loss: 1.0442, Train: 0.6634, Test: 0.6057\n",
            "Early stopping:  0.0505667189213682\n",
            "Epoch: 026, Loss: 1.0171, Train: 0.6721, Test: 0.6141\n",
            "Early stopping:  0.04758689536011396\n",
            "Epoch: 027, Loss: 0.9916, Train: 0.6858, Test: 0.6203\n",
            "Early stopping:  0.04484790543419495\n",
            "Epoch: 028, Loss: 0.9667, Train: 0.6912, Test: 0.6245\n",
            "Early stopping:  0.0424348206053259\n",
            "Epoch: 029, Loss: 0.9425, Train: 0.6968, Test: 0.6305\n",
            "Early stopping:  0.040139196657302645\n",
            "Epoch: 030, Loss: 0.9186, Train: 0.7053, Test: 0.6333\n",
            "Early stopping:  0.038896504324381964\n",
            "Epoch: 031, Loss: 0.8956, Train: 0.7201, Test: 0.6390\n",
            "Early stopping:  0.03794312133932848\n",
            "Epoch: 032, Loss: 0.8731, Train: 0.7334, Test: 0.6422\n",
            "Early stopping:  0.036997103001731355\n",
            "Epoch: 033, Loss: 0.8513, Train: 0.7428, Test: 0.6488\n",
            "Early stopping:  0.036020818100789664\n",
            "Epoch: 034, Loss: 0.8293, Train: 0.7544, Test: 0.6532\n",
            "Early stopping:  0.035254565604477824\n",
            "Epoch: 035, Loss: 0.8079, Train: 0.7618, Test: 0.6562\n",
            "Early stopping:  0.03469504701158651\n",
            "Epoch: 036, Loss: 0.7879, Train: 0.7706, Test: 0.6602\n",
            "Early stopping:  0.03383879527903663\n",
            "Epoch: 037, Loss: 0.7683, Train: 0.7765, Test: 0.6645\n",
            "Early stopping:  0.032807972756817495\n",
            "Epoch: 038, Loss: 0.7494, Train: 0.7862, Test: 0.6662\n",
            "Early stopping:  0.03151567284607969\n",
            "Epoch: 039, Loss: 0.7309, Train: 0.7947, Test: 0.6702\n",
            "Early stopping:  0.030428511999940084\n",
            "Epoch: 040, Loss: 0.7133, Train: 0.8018, Test: 0.6764\n",
            "Early stopping:  0.02951018315050762\n",
            "Epoch: 041, Loss: 0.6953, Train: 0.8091, Test: 0.6783\n",
            "Early stopping:  0.028796037208893083\n",
            "Epoch: 042, Loss: 0.6775, Train: 0.8193, Test: 0.6806\n",
            "Early stopping:  0.028365639253717812\n",
            "Epoch: 043, Loss: 0.6605, Train: 0.8250, Test: 0.6846\n",
            "Early stopping:  0.02791125475341253\n",
            "Epoch: 044, Loss: 0.6433, Train: 0.8301, Test: 0.6872\n",
            "Early stopping:  0.027644534190886628\n",
            "Epoch: 045, Loss: 0.6259, Train: 0.8332, Test: 0.6874\n",
            "Early stopping:  0.027354776152975338\n",
            "Epoch: 046, Loss: 0.6096, Train: 0.8415, Test: 0.6912\n",
            "Early stopping:  0.02693598217028973\n",
            "Epoch: 047, Loss: 0.5954, Train: 0.8381, Test: 0.6866\n",
            "Early stopping:  0.02595079019424427\n",
            "Epoch: 048, Loss: 0.5872, Train: 0.8449, Test: 0.6849\n",
            "Early stopping:  0.022754126014356075\n",
            "Epoch: 049, Loss: 0.5797, Train: 0.8520, Test: 0.6927\n",
            "Early stopping:  0.018455929428623504\n",
            "Epoch: 050, Loss: 0.5528, Train: 0.8599, Test: 0.6957\n",
            "Early stopping:  0.021100357057192754\n",
            "Epoch: 051, Loss: 0.5365, Train: 0.8653, Test: 0.6940\n",
            "Early stopping:  0.024756169660439763\n",
            "Epoch: 052, Loss: 0.5331, Train: 0.8690, Test: 0.6995\n",
            "Early stopping:  0.02464871483219673\n",
            "Epoch: 053, Loss: 0.5107, Train: 0.8724, Test: 0.6989\n",
            "Early stopping:  0.025622154374806305\n",
            "Epoch: 054, Loss: 0.4996, Train: 0.8778, Test: 0.6955\n",
            "Early stopping:  0.021279324994677704\n",
            "Epoch: 055, Loss: 0.4929, Train: 0.8829, Test: 0.7006\n",
            "Early stopping:  0.01957428970165188\n",
            "Epoch: 056, Loss: 0.4731, Train: 0.8832, Test: 0.7038\n",
            "Early stopping:  0.022174853492629924\n",
            "Epoch: 057, Loss: 0.4663, Train: 0.8911, Test: 0.7023\n",
            "Early stopping:  0.018463927321520163\n",
            "Epoch: 058, Loss: 0.4564, Train: 0.8959, Test: 0.7016\n",
            "Early stopping:  0.01812767424874241\n",
            "Epoch: 059, Loss: 0.4398, Train: 0.8968, Test: 0.7033\n",
            "Early stopping:  0.019690232954926182\n",
            "Epoch: 060, Loss: 0.4345, Train: 0.9010, Test: 0.7025\n",
            "Early stopping:  0.016601528010568866\n",
            "Epoch: 061, Loss: 0.4241, Train: 0.9109, Test: 0.7059\n",
            "Early stopping:  0.016996287631142883\n",
            "Epoch: 062, Loss: 0.4095, Train: 0.9109, Test: 0.7074\n",
            "Early stopping:  0.01752683356367633\n",
            "Epoch: 063, Loss: 0.4037, Train: 0.9141, Test: 0.7042\n",
            "Early stopping:  0.015574635872343994\n",
            "Epoch: 064, Loss: 0.3949, Train: 0.9197, Test: 0.7057\n",
            "Early stopping:  0.015897175677458693\n",
            "Epoch: 065, Loss: 0.3816, Train: 0.9240, Test: 0.7055\n",
            "Early stopping:  0.01587347208713832\n",
            "Epoch: 066, Loss: 0.3744, Train: 0.9251, Test: 0.7048\n",
            "Early stopping:  0.014723234245069649\n",
            "Epoch: 067, Loss: 0.3681, Train: 0.9271, Test: 0.7044\n",
            "Early stopping:  0.014645975990556207\n",
            "Epoch: 068, Loss: 0.3574, Train: 0.9336, Test: 0.7035\n",
            "Early stopping:  0.01411912888223634\n",
            "Epoch: 069, Loss: 0.3473, Train: 0.9345, Test: 0.7031\n",
            "Early stopping:  0.013613483076957417\n",
            "Epoch: 070, Loss: 0.3405, Train: 0.9342, Test: 0.7042\n",
            "Early stopping:  0.014045705587831464\n",
            "Epoch: 071, Loss: 0.3347, Train: 0.9387, Test: 0.7031\n",
            "Early stopping:  0.013336035750390099\n",
            "Epoch: 072, Loss: 0.3266, Train: 0.9410, Test: 0.7036\n",
            "Early stopping:  0.011804529806643755\n",
            "Epoch: 073, Loss: 0.3168, Train: 0.9438, Test: 0.7029\n",
            "Early stopping:  0.011895164368609858\n",
            "Epoch: 074, Loss: 0.3089, Train: 0.9481, Test: 0.7033\n",
            "Early stopping:  0.012879959593776042\n",
            "Epoch: 075, Loss: 0.3027, Train: 0.9475, Test: 0.7040\n",
            "Early stopping:  0.012954622595836922\n",
            "Epoch: 076, Loss: 0.2977, Train: 0.9504, Test: 0.7031\n",
            "Early stopping:  0.011464292038934133\n",
            "Epoch: 077, Loss: 0.2935, Train: 0.9495, Test: 0.7023\n",
            "Early stopping:  0.00923703413349752\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.70      0.69      0.69       568\n",
            "         capital_goods       0.66      0.59      0.62       381\n",
            "conglomerates_industry       1.00      0.37      0.54        60\n",
            "     consumer_cyclical       0.69      0.63      0.66       595\n",
            " consumer_non-cyclical       0.66      0.59      0.62       334\n",
            "                energy       0.82      0.75      0.78       213\n",
            "             financial       0.73      0.74      0.73       576\n",
            "            healthcare       0.83      0.74      0.78       238\n",
            "              services       0.66      0.80      0.73      1557\n",
            "            technology       0.69      0.53      0.60       297\n",
            "        transportation       0.79      0.74      0.77       303\n",
            "             utilities       0.78      0.66      0.72       169\n",
            "\n",
            "              accuracy                           0.70      5291\n",
            "             macro avg       0.75      0.65      0.69      5291\n",
            "          weighted avg       0.71      0.70      0.70      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4486, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2713, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.12532736296981237\n",
            "Epoch: 003, Loss: 2.1974, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1290580223560966\n",
            "Epoch: 004, Loss: 2.1280, Train: 0.2978, Test: 0.2990\n",
            "Early stopping:  0.13786122473568424\n",
            "Epoch: 005, Loss: 2.0569, Train: 0.3537, Test: 0.3542\n",
            "Early stopping:  0.1503775452745459\n",
            "Epoch: 006, Loss: 2.0125, Train: 0.3908, Test: 0.3840\n",
            "Early stopping:  0.10443956449702471\n",
            "Epoch: 007, Loss: 1.9587, Train: 0.3988, Test: 0.3884\n",
            "Early stopping:  0.0941770984852533\n",
            "Epoch: 008, Loss: 1.8920, Train: 0.3919, Test: 0.3829\n",
            "Early stopping:  0.09039402966010165\n",
            "Epoch: 009, Loss: 1.8290, Train: 0.3948, Test: 0.3873\n",
            "Early stopping:  0.09138704783200025\n",
            "Epoch: 010, Loss: 1.7708, Train: 0.4149, Test: 0.4064\n",
            "Early stopping:  0.09698632007202636\n",
            "Epoch: 011, Loss: 1.7074, Train: 0.4566, Test: 0.4296\n",
            "Early stopping:  0.09865797776432361\n",
            "Epoch: 012, Loss: 1.6440, Train: 0.4892, Test: 0.4583\n",
            "Early stopping:  0.09765570961949936\n",
            "Epoch: 013, Loss: 1.5865, Train: 0.5119, Test: 0.4753\n",
            "Early stopping:  0.09674306694680951\n",
            "Epoch: 014, Loss: 1.5286, Train: 0.5323, Test: 0.4969\n",
            "Early stopping:  0.09573664548644695\n",
            "Epoch: 015, Loss: 1.4681, Train: 0.5459, Test: 0.5145\n",
            "Early stopping:  0.09394086987490684\n",
            "Epoch: 016, Loss: 1.4116, Train: 0.5593, Test: 0.5275\n",
            "Early stopping:  0.09223046367703544\n",
            "Epoch: 017, Loss: 1.3617, Train: 0.5752, Test: 0.5402\n",
            "Early stopping:  0.08964496431483887\n",
            "Epoch: 018, Loss: 1.3128, Train: 0.5953, Test: 0.5555\n",
            "Early stopping:  0.08514700515234509\n",
            "Epoch: 019, Loss: 1.2650, Train: 0.6081, Test: 0.5615\n",
            "Early stopping:  0.07987141821045877\n",
            "Epoch: 020, Loss: 1.2242, Train: 0.6188, Test: 0.5742\n",
            "Early stopping:  0.0745693303817481\n",
            "Epoch: 021, Loss: 1.1874, Train: 0.6254, Test: 0.5834\n",
            "Early stopping:  0.06923509239684497\n",
            "Epoch: 022, Loss: 1.1515, Train: 0.6310, Test: 0.5914\n",
            "Early stopping:  0.06341476291123166\n",
            "Epoch: 023, Loss: 1.1203, Train: 0.6370, Test: 0.5957\n",
            "Early stopping:  0.0573507850181771\n",
            "Epoch: 024, Loss: 1.0923, Train: 0.6449, Test: 0.6006\n",
            "Early stopping:  0.052421251359970694\n",
            "Epoch: 025, Loss: 1.0639, Train: 0.6600, Test: 0.6076\n",
            "Early stopping:  0.04846423501618525\n",
            "Epoch: 026, Loss: 1.0372, Train: 0.6679, Test: 0.6141\n",
            "Early stopping:  0.04507299147932631\n",
            "Epoch: 027, Loss: 1.0106, Train: 0.6758, Test: 0.6209\n",
            "Early stopping:  0.043419668879608866\n",
            "Epoch: 028, Loss: 0.9846, Train: 0.6852, Test: 0.6228\n",
            "Early stopping:  0.04250861516419545\n",
            "Epoch: 029, Loss: 0.9600, Train: 0.6954, Test: 0.6307\n",
            "Early stopping:  0.041183985593398684\n",
            "Epoch: 030, Loss: 0.9351, Train: 0.7031, Test: 0.6366\n",
            "Early stopping:  0.04026155082444562\n",
            "Epoch: 031, Loss: 0.9113, Train: 0.7147, Test: 0.6415\n",
            "Early stopping:  0.039215584221821476\n",
            "Epoch: 032, Loss: 0.8886, Train: 0.7255, Test: 0.6456\n",
            "Early stopping:  0.03806877778103868\n",
            "Epoch: 033, Loss: 0.8656, Train: 0.7362, Test: 0.6500\n",
            "Early stopping:  0.03722417890858769\n",
            "Epoch: 034, Loss: 0.8448, Train: 0.7462, Test: 0.6564\n",
            "Early stopping:  0.035804370086055595\n",
            "Epoch: 035, Loss: 0.8243, Train: 0.7567, Test: 0.6606\n",
            "Early stopping:  0.034448069960271506\n",
            "Epoch: 036, Loss: 0.8045, Train: 0.7677, Test: 0.6655\n",
            "Early stopping:  0.03311650718200867\n",
            "Epoch: 037, Loss: 0.7858, Train: 0.7745, Test: 0.6683\n",
            "Early stopping:  0.031591268757276816\n",
            "Epoch: 038, Loss: 0.7672, Train: 0.7822, Test: 0.6717\n",
            "Early stopping:  0.030629949776661204\n",
            "Epoch: 039, Loss: 0.7490, Train: 0.7910, Test: 0.6738\n",
            "Early stopping:  0.029728638672068007\n",
            "Epoch: 040, Loss: 0.7308, Train: 0.7981, Test: 0.6789\n",
            "Early stopping:  0.029153291740213277\n",
            "Epoch: 041, Loss: 0.7131, Train: 0.8037, Test: 0.6829\n",
            "Early stopping:  0.02875110487515797\n",
            "Epoch: 042, Loss: 0.6951, Train: 0.8060, Test: 0.6810\n",
            "Early stopping:  0.028453347733692548\n",
            "Epoch: 043, Loss: 0.6779, Train: 0.8131, Test: 0.6887\n",
            "Early stopping:  0.028117204372582028\n",
            "Epoch: 044, Loss: 0.6604, Train: 0.8188, Test: 0.6883\n",
            "Early stopping:  0.027820400259312827\n",
            "Epoch: 045, Loss: 0.6438, Train: 0.8219, Test: 0.6904\n",
            "Early stopping:  0.027426240822650282\n",
            "Epoch: 046, Loss: 0.6283, Train: 0.8253, Test: 0.6889\n",
            "Early stopping:  0.026553287837966327\n",
            "Epoch: 047, Loss: 0.6168, Train: 0.8259, Test: 0.6921\n",
            "Early stopping:  0.024486122115860535\n",
            "Epoch: 048, Loss: 0.6095, Train: 0.8341, Test: 0.6923\n",
            "Early stopping:  0.02061102449038377\n",
            "Epoch: 049, Loss: 0.5927, Train: 0.8429, Test: 0.6976\n",
            "Early stopping:  0.01926289057127048\n",
            "Epoch: 050, Loss: 0.5683, Train: 0.8429, Test: 0.6993\n",
            "Early stopping:  0.0233456389425321\n",
            "Epoch: 051, Loss: 0.5629, Train: 0.8497, Test: 0.6955\n",
            "Early stopping:  0.02400765792996232\n",
            "Epoch: 052, Loss: 0.5512, Train: 0.8571, Test: 0.7029\n",
            "Early stopping:  0.023648825227772622\n",
            "Epoch: 053, Loss: 0.5298, Train: 0.8596, Test: 0.7027\n",
            "Early stopping:  0.0230647063747726\n",
            "Epoch: 054, Loss: 0.5243, Train: 0.8667, Test: 0.7018\n",
            "Early stopping:  0.01960000645470277\n",
            "Epoch: 055, Loss: 0.5124, Train: 0.8712, Test: 0.7065\n",
            "Early stopping:  0.02057131049661013\n",
            "Epoch: 056, Loss: 0.4946, Train: 0.8681, Test: 0.7053\n",
            "Early stopping:  0.021007205219305863\n",
            "Epoch: 057, Loss: 0.4894, Train: 0.8809, Test: 0.7025\n",
            "Early stopping:  0.017770889143049588\n",
            "Epoch: 058, Loss: 0.4779, Train: 0.8843, Test: 0.7069\n",
            "Early stopping:  0.018520127308390318\n",
            "Epoch: 059, Loss: 0.4618, Train: 0.8815, Test: 0.7097\n",
            "Early stopping:  0.01888522988227855\n",
            "Epoch: 060, Loss: 0.4563, Train: 0.8942, Test: 0.7033\n",
            "Early stopping:  0.01674029596844402\n",
            "Epoch: 061, Loss: 0.4461, Train: 0.8962, Test: 0.7076\n",
            "Early stopping:  0.01731612039969252\n",
            "Epoch: 062, Loss: 0.4310, Train: 0.8956, Test: 0.7099\n",
            "Early stopping:  0.01750687871365903\n",
            "Epoch: 063, Loss: 0.4244, Train: 0.9039, Test: 0.7042\n",
            "Early stopping:  0.01598387759096695\n",
            "Epoch: 064, Loss: 0.4168, Train: 0.9061, Test: 0.7097\n",
            "Early stopping:  0.016079113616978738\n",
            "Epoch: 065, Loss: 0.4032, Train: 0.9078, Test: 0.7097\n",
            "Early stopping:  0.015984825829557615\n",
            "Epoch: 066, Loss: 0.3940, Train: 0.9178, Test: 0.7044\n",
            "Early stopping:  0.015193740341573305\n",
            "Epoch: 067, Loss: 0.3886, Train: 0.9092, Test: 0.7108\n",
            "Early stopping:  0.015070709156322207\n",
            "Epoch: 068, Loss: 0.3787, Train: 0.9203, Test: 0.7084\n",
            "Early stopping:  0.01451660356778295\n",
            "Epoch: 069, Loss: 0.3671, Train: 0.9231, Test: 0.7080\n",
            "Early stopping:  0.013913513824958966\n",
            "Epoch: 070, Loss: 0.3589, Train: 0.9200, Test: 0.7103\n",
            "Early stopping:  0.014568528686359317\n",
            "Epoch: 071, Loss: 0.3532, Train: 0.9308, Test: 0.7067\n",
            "Early stopping:  0.014416316763972068\n",
            "Epoch: 072, Loss: 0.3468, Train: 0.9260, Test: 0.7112\n",
            "Early stopping:  0.01242157458011471\n",
            "Epoch: 073, Loss: 0.3379, Train: 0.9336, Test: 0.7120\n",
            "Early stopping:  0.011214185736488604\n",
            "Epoch: 074, Loss: 0.3283, Train: 0.9345, Test: 0.7106\n",
            "Early stopping:  0.012207398938993128\n",
            "Epoch: 075, Loss: 0.3195, Train: 0.9385, Test: 0.7116\n",
            "Early stopping:  0.013608915047240943\n",
            "Epoch: 076, Loss: 0.3122, Train: 0.9433, Test: 0.7125\n",
            "Early stopping:  0.013836918020759049\n",
            "Epoch: 077, Loss: 0.3064, Train: 0.9385, Test: 0.7118\n",
            "Early stopping:  0.012555015020347785\n",
            "Epoch: 078, Loss: 0.3025, Train: 0.9450, Test: 0.7065\n",
            "Early stopping:  0.01032555663632098\n",
            "Epoch: 079, Loss: 0.3032, Train: 0.9251, Test: 0.7082\n",
            "Early stopping:  0.0071378324713625586\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.71      0.71       568\n",
            "         capital_goods       0.70      0.56      0.63       381\n",
            "conglomerates_industry       1.00      0.37      0.54        60\n",
            "     consumer_cyclical       0.70      0.59      0.64       595\n",
            " consumer_non-cyclical       0.72      0.60      0.65       334\n",
            "                energy       0.76      0.70      0.73       213\n",
            "             financial       0.74      0.75      0.74       576\n",
            "            healthcare       0.80      0.74      0.76       238\n",
            "              services       0.65      0.84      0.73      1557\n",
            "            technology       0.74      0.53      0.62       297\n",
            "        transportation       0.83      0.74      0.78       303\n",
            "             utilities       0.78      0.72      0.74       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.65      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.70      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4571, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2366, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15596508726413955\n",
            "Epoch: 003, Loss: 2.1856, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14431464629169896\n",
            "Epoch: 004, Loss: 2.1297, Train: 0.3001, Test: 0.3037\n",
            "Early stopping:  0.14337300335505843\n",
            "Epoch: 005, Loss: 2.0731, Train: 0.3613, Test: 0.3559\n",
            "Early stopping:  0.14777931054003696\n",
            "Epoch: 006, Loss: 2.0244, Train: 0.3868, Test: 0.3776\n",
            "Early stopping:  0.08489942575187716\n",
            "Epoch: 007, Loss: 1.9697, Train: 0.3815, Test: 0.3720\n",
            "Early stopping:  0.08495632388380209\n",
            "Epoch: 008, Loss: 1.9089, Train: 0.3701, Test: 0.3631\n",
            "Early stopping:  0.08624119724025482\n",
            "Epoch: 009, Loss: 1.8508, Train: 0.3724, Test: 0.3646\n",
            "Early stopping:  0.08864677115724713\n",
            "Epoch: 010, Loss: 1.7965, Train: 0.4027, Test: 0.3905\n",
            "Early stopping:  0.09090966180028447\n",
            "Epoch: 011, Loss: 1.7340, Train: 0.4498, Test: 0.4309\n",
            "Early stopping:  0.09231072608636759\n",
            "Epoch: 012, Loss: 1.6670, Train: 0.4759, Test: 0.4474\n",
            "Early stopping:  0.09501138319921754\n",
            "Epoch: 013, Loss: 1.6091, Train: 0.4943, Test: 0.4649\n",
            "Early stopping:  0.09692883726835914\n",
            "Epoch: 014, Loss: 1.5541, Train: 0.5167, Test: 0.4784\n",
            "Early stopping:  0.09646930075190785\n",
            "Epoch: 015, Loss: 1.4928, Train: 0.5281, Test: 0.4897\n",
            "Early stopping:  0.09421321446657115\n",
            "Epoch: 016, Loss: 1.4357, Train: 0.5485, Test: 0.5109\n",
            "Early stopping:  0.09157336249218376\n",
            "Epoch: 017, Loss: 1.3866, Train: 0.5791, Test: 0.5271\n",
            "Early stopping:  0.08916298282220864\n",
            "Epoch: 018, Loss: 1.3374, Train: 0.5967, Test: 0.5438\n",
            "Early stopping:  0.0854138203943478\n",
            "Epoch: 019, Loss: 1.2930, Train: 0.6092, Test: 0.5500\n",
            "Early stopping:  0.07879101732547943\n",
            "Epoch: 020, Loss: 1.2541, Train: 0.6140, Test: 0.5621\n",
            "Early stopping:  0.07230973785305951\n",
            "Epoch: 021, Loss: 1.2151, Train: 0.6194, Test: 0.5704\n",
            "Early stopping:  0.06748252906134802\n",
            "Epoch: 022, Loss: 1.1807, Train: 0.6256, Test: 0.5755\n",
            "Early stopping:  0.061918886697075695\n",
            "Epoch: 023, Loss: 1.1498, Train: 0.6358, Test: 0.5827\n",
            "Early stopping:  0.0569242243886839\n",
            "Epoch: 024, Loss: 1.1168, Train: 0.6472, Test: 0.5927\n",
            "Early stopping:  0.053786609000881266\n",
            "Epoch: 025, Loss: 1.0880, Train: 0.6568, Test: 0.6040\n",
            "Early stopping:  0.05033202544637705\n",
            "Epoch: 026, Loss: 1.0601, Train: 0.6622, Test: 0.6091\n",
            "Early stopping:  0.04794884503401578\n",
            "Epoch: 027, Loss: 1.0322, Train: 0.6662, Test: 0.6107\n",
            "Early stopping:  0.04620100813721326\n",
            "Epoch: 028, Loss: 1.0073, Train: 0.6747, Test: 0.6154\n",
            "Early stopping:  0.04345720564765038\n",
            "Epoch: 029, Loss: 0.9820, Train: 0.6880, Test: 0.6211\n",
            "Early stopping:  0.04187361877862992\n",
            "Epoch: 030, Loss: 0.9593, Train: 0.6999, Test: 0.6279\n",
            "Early stopping:  0.03981780925777053\n",
            "Epoch: 031, Loss: 0.9362, Train: 0.7045, Test: 0.6350\n",
            "Early stopping:  0.037954447677640245\n",
            "Epoch: 032, Loss: 0.9150, Train: 0.7161, Test: 0.6415\n",
            "Early stopping:  0.0364576572347872\n",
            "Epoch: 033, Loss: 0.8940, Train: 0.7221, Test: 0.6398\n",
            "Early stopping:  0.03484613976624184\n",
            "Epoch: 034, Loss: 0.8737, Train: 0.7314, Test: 0.6447\n",
            "Early stopping:  0.0337569246819781\n",
            "Epoch: 035, Loss: 0.8538, Train: 0.7425, Test: 0.6477\n",
            "Early stopping:  0.03258123029407553\n",
            "Epoch: 036, Loss: 0.8345, Train: 0.7507, Test: 0.6503\n",
            "Early stopping:  0.03181330098554447\n",
            "Epoch: 037, Loss: 0.8153, Train: 0.7623, Test: 0.6547\n",
            "Early stopping:  0.031088662671667492\n",
            "Epoch: 038, Loss: 0.7967, Train: 0.7740, Test: 0.6579\n",
            "Early stopping:  0.030458808319968948\n",
            "Epoch: 039, Loss: 0.7782, Train: 0.7771, Test: 0.6621\n",
            "Early stopping:  0.02989321295978838\n",
            "Epoch: 040, Loss: 0.7606, Train: 0.7833, Test: 0.6647\n",
            "Early stopping:  0.02923177119668531\n",
            "Epoch: 041, Loss: 0.7430, Train: 0.7913, Test: 0.6655\n",
            "Early stopping:  0.02854524267609521\n",
            "Epoch: 042, Loss: 0.7261, Train: 0.7984, Test: 0.6687\n",
            "Early stopping:  0.02788794278963082\n",
            "Epoch: 043, Loss: 0.7094, Train: 0.8015, Test: 0.6706\n",
            "Early stopping:  0.027243912276550892\n",
            "Epoch: 044, Loss: 0.6932, Train: 0.8097, Test: 0.6725\n",
            "Early stopping:  0.02664583973898203\n",
            "Epoch: 045, Loss: 0.6767, Train: 0.8142, Test: 0.6759\n",
            "Early stopping:  0.026164939204354372\n",
            "Epoch: 046, Loss: 0.6606, Train: 0.8205, Test: 0.6785\n",
            "Early stopping:  0.02587112507232867\n",
            "Epoch: 047, Loss: 0.6447, Train: 0.8264, Test: 0.6810\n",
            "Early stopping:  0.025619929918609723\n",
            "Epoch: 048, Loss: 0.6289, Train: 0.8318, Test: 0.6834\n",
            "Early stopping:  0.02539780042915403\n",
            "Epoch: 049, Loss: 0.6135, Train: 0.8347, Test: 0.6844\n",
            "Early stopping:  0.02501714823905392\n",
            "Epoch: 050, Loss: 0.5987, Train: 0.8412, Test: 0.6876\n",
            "Early stopping:  0.024521629022934817\n",
            "Epoch: 051, Loss: 0.5837, Train: 0.8469, Test: 0.6899\n",
            "Early stopping:  0.024078393334195525\n",
            "Epoch: 052, Loss: 0.5689, Train: 0.8514, Test: 0.6910\n",
            "Early stopping:  0.023689558760973364\n",
            "Epoch: 053, Loss: 0.5546, Train: 0.8539, Test: 0.6931\n",
            "Early stopping:  0.0233196359060842\n",
            "Epoch: 054, Loss: 0.5404, Train: 0.8590, Test: 0.6927\n",
            "Early stopping:  0.02303018312804228\n",
            "Epoch: 055, Loss: 0.5266, Train: 0.8661, Test: 0.6938\n",
            "Early stopping:  0.022571337581298607\n",
            "Epoch: 056, Loss: 0.5130, Train: 0.8664, Test: 0.6974\n",
            "Early stopping:  0.022097780648647754\n",
            "Epoch: 057, Loss: 0.5001, Train: 0.8766, Test: 0.6942\n",
            "Early stopping:  0.02156820547240314\n",
            "Epoch: 058, Loss: 0.4876, Train: 0.8755, Test: 0.6968\n",
            "Early stopping:  0.02087749370418777\n",
            "Epoch: 059, Loss: 0.4758, Train: 0.8832, Test: 0.6976\n",
            "Early stopping:  0.020072435214878784\n",
            "Epoch: 060, Loss: 0.4634, Train: 0.8888, Test: 0.6993\n",
            "Early stopping:  0.01951843568252469\n",
            "Epoch: 061, Loss: 0.4504, Train: 0.8917, Test: 0.6997\n",
            "Early stopping:  0.01951758505845883\n",
            "Epoch: 062, Loss: 0.4384, Train: 0.8985, Test: 0.6963\n",
            "Early stopping:  0.019589761536029742\n",
            "Epoch: 063, Loss: 0.4292, Train: 0.8925, Test: 0.7025\n",
            "Early stopping:  0.018747357121400087\n",
            "Epoch: 064, Loss: 0.4246, Train: 0.9019, Test: 0.6919\n",
            "Early stopping:  0.01586682542168131\n",
            "Epoch: 065, Loss: 0.4195, Train: 0.9047, Test: 0.7038\n",
            "Early stopping:  0.012232849912908765\n",
            "Epoch: 066, Loss: 0.4022, Train: 0.9132, Test: 0.6997\n",
            "Early stopping:  0.013451607769905196\n",
            "Epoch: 067, Loss: 0.3873, Train: 0.9203, Test: 0.6953\n",
            "Early stopping:  0.01742613716526788\n",
            "Epoch: 068, Loss: 0.3849, Train: 0.9175, Test: 0.7050\n",
            "Early stopping:  0.018117961487505475\n",
            "Epoch: 069, Loss: 0.3739, Train: 0.9234, Test: 0.7006\n",
            "Early stopping:  0.01765393905793739\n",
            "Epoch: 070, Loss: 0.3610, Train: 0.9314, Test: 0.6993\n",
            "Early stopping:  0.015411794129123777\n",
            "Epoch: 071, Loss: 0.3567, Train: 0.9268, Test: 0.7025\n",
            "Early stopping:  0.013745891828586352\n",
            "Epoch: 072, Loss: 0.3469, Train: 0.9345, Test: 0.7029\n",
            "Early stopping:  0.014888759839013162\n",
            "Epoch: 073, Loss: 0.3363, Train: 0.9410, Test: 0.7016\n",
            "Early stopping:  0.014262179334869547\n",
            "Epoch: 074, Loss: 0.3318, Train: 0.9376, Test: 0.7052\n",
            "Early stopping:  0.012605396752804672\n",
            "Epoch: 075, Loss: 0.3227, Train: 0.9444, Test: 0.7074\n",
            "Early stopping:  0.013257361148439124\n",
            "Epoch: 076, Loss: 0.3133, Train: 0.9484, Test: 0.7055\n",
            "Early stopping:  0.01282915661380878\n",
            "Epoch: 077, Loss: 0.3084, Train: 0.9438, Test: 0.7055\n",
            "Early stopping:  0.011825168755484203\n",
            "Epoch: 078, Loss: 0.3012, Train: 0.9512, Test: 0.7082\n",
            "Early stopping:  0.011997270783020016\n",
            "Epoch: 079, Loss: 0.2926, Train: 0.9558, Test: 0.7080\n",
            "Early stopping:  0.01146057028512749\n",
            "Epoch: 080, Loss: 0.2855, Train: 0.9532, Test: 0.7082\n",
            "Early stopping:  0.011344203332666171\n",
            "Epoch: 081, Loss: 0.2801, Train: 0.9589, Test: 0.7082\n",
            "Early stopping:  0.011458738859623908\n",
            "Epoch: 082, Loss: 0.2737, Train: 0.9614, Test: 0.7091\n",
            "Early stopping:  0.010714766224476289\n",
            "Epoch: 083, Loss: 0.2658, Train: 0.9580, Test: 0.7091\n",
            "Early stopping:  0.010363661856559472\n",
            "Epoch: 084, Loss: 0.2603, Train: 0.9648, Test: 0.7072\n",
            "Early stopping:  0.010262814074320588\n",
            "Epoch: 085, Loss: 0.2566, Train: 0.9577, Test: 0.7067\n",
            "Early stopping:  0.009628802449310584\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.70      0.71      0.71       568\n",
            "         capital_goods       0.67      0.64      0.65       381\n",
            "conglomerates_industry       1.00      0.43      0.60        60\n",
            "     consumer_cyclical       0.66      0.58      0.62       595\n",
            " consumer_non-cyclical       0.69      0.61      0.65       334\n",
            "                energy       0.85      0.70      0.77       213\n",
            "             financial       0.78      0.70      0.74       576\n",
            "            healthcare       0.86      0.77      0.81       238\n",
            "              services       0.66      0.83      0.73      1557\n",
            "            technology       0.67      0.51      0.58       297\n",
            "        transportation       0.81      0.74      0.77       303\n",
            "             utilities       0.81      0.67      0.74       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.66      0.70      5291\n",
            "          weighted avg       0.71      0.71      0.70      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4558, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2554, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14172400432709653\n",
            "Epoch: 003, Loss: 2.1561, Train: 0.2944, Test: 0.2947\n",
            "Early stopping:  0.15266456298046321\n",
            "Epoch: 004, Loss: 2.1206, Train: 0.3182, Test: 0.3188\n",
            "Early stopping:  0.15044263263054233\n",
            "Epoch: 005, Loss: 2.0518, Train: 0.3602, Test: 0.3597\n",
            "Early stopping:  0.15682490531138285\n",
            "Epoch: 006, Loss: 1.9829, Train: 0.3826, Test: 0.3767\n",
            "Early stopping:  0.10352699202998124\n",
            "Epoch: 007, Loss: 1.9197, Train: 0.3885, Test: 0.3789\n",
            "Early stopping:  0.09704469108421955\n",
            "Epoch: 008, Loss: 1.8562, Train: 0.3973, Test: 0.3880\n",
            "Early stopping:  0.10453129479965186\n",
            "Epoch: 009, Loss: 1.7936, Train: 0.4263, Test: 0.4101\n",
            "Early stopping:  0.10169097533116594\n",
            "Epoch: 010, Loss: 1.7291, Train: 0.4518, Test: 0.4343\n",
            "Early stopping:  0.10017958212899898\n",
            "Epoch: 011, Loss: 1.6650, Train: 0.4716, Test: 0.4481\n",
            "Early stopping:  0.10065279968483484\n",
            "Epoch: 012, Loss: 1.6042, Train: 0.4875, Test: 0.4589\n",
            "Early stopping:  0.10004230369026458\n",
            "Epoch: 013, Loss: 1.5407, Train: 0.5113, Test: 0.4725\n",
            "Early stopping:  0.09972597360232556\n",
            "Epoch: 014, Loss: 1.4803, Train: 0.5445, Test: 0.5007\n",
            "Early stopping:  0.0983434279296293\n",
            "Epoch: 015, Loss: 1.4239, Train: 0.5749, Test: 0.5269\n",
            "Early stopping:  0.09584769274173834\n",
            "Epoch: 016, Loss: 1.3712, Train: 0.5922, Test: 0.5438\n",
            "Early stopping:  0.09222388085988074\n",
            "Epoch: 017, Loss: 1.3234, Train: 0.5998, Test: 0.5577\n",
            "Early stopping:  0.0860559365502505\n",
            "Epoch: 018, Loss: 1.2776, Train: 0.6064, Test: 0.5651\n",
            "Early stopping:  0.0800345128229533\n",
            "Epoch: 019, Loss: 1.2379, Train: 0.6166, Test: 0.5753\n",
            "Early stopping:  0.07369452419603006\n",
            "Epoch: 020, Loss: 1.1998, Train: 0.6350, Test: 0.5789\n",
            "Early stopping:  0.06780997464083585\n",
            "Epoch: 021, Loss: 1.1651, Train: 0.6410, Test: 0.5868\n",
            "Early stopping:  0.06247178275757588\n",
            "Epoch: 022, Loss: 1.1326, Train: 0.6415, Test: 0.5944\n",
            "Early stopping:  0.05743094425487061\n",
            "Epoch: 023, Loss: 1.1036, Train: 0.6486, Test: 0.6016\n",
            "Early stopping:  0.05316531522903663\n",
            "Epoch: 024, Loss: 1.0744, Train: 0.6602, Test: 0.6088\n",
            "Early stopping:  0.049394671493905556\n",
            "Epoch: 025, Loss: 1.0474, Train: 0.6668, Test: 0.6133\n",
            "Early stopping:  0.04643148976275854\n",
            "Epoch: 026, Loss: 1.0204, Train: 0.6790, Test: 0.6212\n",
            "Early stopping:  0.04436768049144173\n",
            "Epoch: 027, Loss: 0.9946, Train: 0.6923, Test: 0.6235\n",
            "Early stopping:  0.0430115310869155\n",
            "Epoch: 028, Loss: 0.9692, Train: 0.7025, Test: 0.6280\n",
            "Early stopping:  0.04163265575075474\n",
            "Epoch: 029, Loss: 0.9457, Train: 0.7099, Test: 0.6335\n",
            "Early stopping:  0.040264841120281385\n",
            "Epoch: 030, Loss: 0.9222, Train: 0.7133, Test: 0.6375\n",
            "Early stopping:  0.038808510158488775\n",
            "Epoch: 031, Loss: 0.8992, Train: 0.7266, Test: 0.6413\n",
            "Early stopping:  0.03759812620713238\n",
            "Epoch: 032, Loss: 0.8779, Train: 0.7379, Test: 0.6479\n",
            "Early stopping:  0.03623689428179143\n",
            "Epoch: 033, Loss: 0.8561, Train: 0.7436, Test: 0.6486\n",
            "Early stopping:  0.03534477035956545\n",
            "Epoch: 034, Loss: 0.8348, Train: 0.7538, Test: 0.6526\n",
            "Early stopping:  0.034430974480236465\n",
            "Epoch: 035, Loss: 0.8145, Train: 0.7609, Test: 0.6564\n",
            "Early stopping:  0.03360896312566007\n",
            "Epoch: 036, Loss: 0.7949, Train: 0.7697, Test: 0.6587\n",
            "Early stopping:  0.03284642834118351\n",
            "Epoch: 037, Loss: 0.7755, Train: 0.7777, Test: 0.6630\n",
            "Early stopping:  0.03181681295245991\n",
            "Epoch: 038, Loss: 0.7564, Train: 0.7853, Test: 0.6674\n",
            "Early stopping:  0.030960760695105057\n",
            "Epoch: 039, Loss: 0.7384, Train: 0.7921, Test: 0.6708\n",
            "Early stopping:  0.03013768922993124\n",
            "Epoch: 040, Loss: 0.7201, Train: 0.8003, Test: 0.6736\n",
            "Early stopping:  0.02952340418390126\n",
            "Epoch: 041, Loss: 0.7018, Train: 0.8097, Test: 0.6774\n",
            "Early stopping:  0.029061155842920107\n",
            "Epoch: 042, Loss: 0.6841, Train: 0.8174, Test: 0.6808\n",
            "Early stopping:  0.028662364650968726\n",
            "Epoch: 043, Loss: 0.6664, Train: 0.8227, Test: 0.6829\n",
            "Early stopping:  0.028462897705097513\n",
            "Epoch: 044, Loss: 0.6489, Train: 0.8279, Test: 0.6842\n",
            "Early stopping:  0.028107276387661716\n",
            "Epoch: 045, Loss: 0.6318, Train: 0.8301, Test: 0.6863\n",
            "Early stopping:  0.027714855926764485\n",
            "Epoch: 046, Loss: 0.6152, Train: 0.8420, Test: 0.6866\n",
            "Early stopping:  0.027247719232411417\n",
            "Epoch: 047, Loss: 0.5994, Train: 0.8369, Test: 0.6931\n",
            "Early stopping:  0.02650205662872433\n",
            "Epoch: 048, Loss: 0.5881, Train: 0.8474, Test: 0.6864\n",
            "Early stopping:  0.02438431455303624\n",
            "Epoch: 049, Loss: 0.5848, Train: 0.8392, Test: 0.6925\n",
            "Early stopping:  0.01959293613676868\n",
            "Epoch: 050, Loss: 0.5701, Train: 0.8590, Test: 0.6946\n",
            "Early stopping:  0.01687713051344363\n",
            "Epoch: 051, Loss: 0.5390, Train: 0.8642, Test: 0.6944\n",
            "Early stopping:  0.02334183683737644\n",
            "Epoch: 052, Loss: 0.5339, Train: 0.8568, Test: 0.6978\n",
            "Early stopping:  0.025406223926017452\n",
            "Epoch: 053, Loss: 0.5239, Train: 0.8755, Test: 0.7012\n",
            "Early stopping:  0.025876034166486744\n",
            "Epoch: 054, Loss: 0.4992, Train: 0.8783, Test: 0.6967\n",
            "Early stopping:  0.02567726706931903\n",
            "Epoch: 055, Loss: 0.4962, Train: 0.8795, Test: 0.7019\n",
            "Early stopping:  0.019725584810222005\n",
            "Epoch: 056, Loss: 0.4809, Train: 0.8880, Test: 0.7021\n",
            "Early stopping:  0.02163290294863789\n",
            "Epoch: 057, Loss: 0.4641, Train: 0.8914, Test: 0.7023\n",
            "Early stopping:  0.022295511423017295\n",
            "Epoch: 058, Loss: 0.4598, Train: 0.8971, Test: 0.7052\n",
            "Early stopping:  0.017977602499694496\n",
            "Epoch: 059, Loss: 0.4426, Train: 0.8948, Test: 0.7042\n",
            "Early stopping:  0.020513293954809796\n",
            "Epoch: 060, Loss: 0.4327, Train: 0.9053, Test: 0.7027\n",
            "Early stopping:  0.018831359977360282\n",
            "Epoch: 061, Loss: 0.4244, Train: 0.9075, Test: 0.7057\n",
            "Early stopping:  0.017050934715431582\n",
            "Epoch: 062, Loss: 0.4092, Train: 0.9041, Test: 0.7076\n",
            "Early stopping:  0.019030662035252322\n",
            "Epoch: 063, Loss: 0.4029, Train: 0.9146, Test: 0.7053\n",
            "Early stopping:  0.01637246892797887\n",
            "Epoch: 064, Loss: 0.3907, Train: 0.9200, Test: 0.7086\n",
            "Early stopping:  0.016783881506811323\n",
            "Epoch: 065, Loss: 0.3780, Train: 0.9129, Test: 0.7072\n",
            "Early stopping:  0.017714377806091287\n",
            "Epoch: 066, Loss: 0.3733, Train: 0.9271, Test: 0.7099\n",
            "Early stopping:  0.015473421391978447\n",
            "Epoch: 067, Loss: 0.3597, Train: 0.9299, Test: 0.7093\n",
            "Early stopping:  0.016557716625892085\n",
            "Epoch: 068, Loss: 0.3489, Train: 0.9231, Test: 0.7099\n",
            "Early stopping:  0.01623544790618597\n",
            "Epoch: 069, Loss: 0.3435, Train: 0.9373, Test: 0.7110\n",
            "Early stopping:  0.01494737441463488\n",
            "Epoch: 070, Loss: 0.3309, Train: 0.9390, Test: 0.7120\n",
            "Early stopping:  0.016090571003363155\n",
            "Epoch: 071, Loss: 0.3217, Train: 0.9334, Test: 0.7095\n",
            "Early stopping:  0.014935278395350431\n",
            "Epoch: 072, Loss: 0.3160, Train: 0.9464, Test: 0.7118\n",
            "Early stopping:  0.013959109191589592\n",
            "Epoch: 073, Loss: 0.3048, Train: 0.9484, Test: 0.7135\n",
            "Early stopping:  0.014693961634279391\n",
            "Epoch: 074, Loss: 0.2963, Train: 0.9430, Test: 0.7088\n",
            "Early stopping:  0.013693468362772012\n",
            "Epoch: 075, Loss: 0.2924, Train: 0.9472, Test: 0.7110\n",
            "Early stopping:  0.012541490538099833\n",
            "Epoch: 076, Loss: 0.2891, Train: 0.9319, Test: 0.6980\n",
            "Early stopping:  0.010843328199983574\n",
            "Epoch: 077, Loss: 0.3167, Train: 0.8965, Test: 0.6914\n",
            "Early stopping:  0.011113156909821162\n",
            "Epoch: 078, Loss: 0.3684, Train: 0.9396, Test: 0.6929\n",
            "Early stopping:  0.03299434032606993\n",
            "Epoch: 079, Loss: 0.3063, Train: 0.9487, Test: 0.6999\n",
            "Early stopping:  0.03204992495339004\n",
            "Epoch: 080, Loss: 0.2869, Train: 0.9280, Test: 0.7035\n",
            "Early stopping:  0.033084630087906915\n",
            "Epoch: 081, Loss: 0.2927, Train: 0.9484, Test: 0.7095\n",
            "Early stopping:  0.03244770645573814\n",
            "Epoch: 082, Loss: 0.2704, Train: 0.9558, Test: 0.7014\n",
            "Early stopping:  0.037728642227232545\n",
            "Epoch: 083, Loss: 0.2663, Train: 0.9543, Test: 0.7046\n",
            "Early stopping:  0.016399473033357655\n",
            "Epoch: 084, Loss: 0.2657, Train: 0.9546, Test: 0.7112\n",
            "Early stopping:  0.012527226235739389\n",
            "Epoch: 085, Loss: 0.2453, Train: 0.9470, Test: 0.7057\n",
            "Early stopping:  0.016887687004970536\n",
            "Epoch: 086, Loss: 0.2619, Train: 0.9677, Test: 0.7127\n",
            "Early stopping:  0.009775583708875872\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.73      0.70       568\n",
            "         capital_goods       0.65      0.59      0.62       381\n",
            "conglomerates_industry       0.93      0.42      0.57        60\n",
            "     consumer_cyclical       0.66      0.67      0.67       595\n",
            " consumer_non-cyclical       0.69      0.60      0.65       334\n",
            "                energy       0.82      0.73      0.77       213\n",
            "             financial       0.75      0.73      0.74       576\n",
            "            healthcare       0.83      0.76      0.79       238\n",
            "              services       0.71      0.79      0.75      1557\n",
            "            technology       0.64      0.53      0.58       297\n",
            "        transportation       0.82      0.76      0.79       303\n",
            "             utilities       0.78      0.72      0.74       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.74      0.67      0.70      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5338, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2893, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  0.1729022198067274\n",
            "Epoch: 003, Loss: 2.1844, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  0.17930194536245428\n",
            "Epoch: 004, Loss: 2.1646, Train: 0.2947, Test: 0.2941\n",
            "Early stopping:  0.16959172227267252\n",
            "Epoch: 005, Loss: 2.0959, Train: 0.3202, Test: 0.3187\n",
            "Early stopping:  0.17129979015657212\n",
            "Epoch: 006, Loss: 2.0292, Train: 0.3636, Test: 0.3600\n",
            "Early stopping:  0.09782143579569104\n",
            "Epoch: 007, Loss: 1.9754, Train: 0.3840, Test: 0.3793\n",
            "Early stopping:  0.08851973457629927\n",
            "Epoch: 008, Loss: 1.9219, Train: 0.3868, Test: 0.3769\n",
            "Early stopping:  0.09600168866815235\n",
            "Epoch: 009, Loss: 1.8647, Train: 0.3868, Test: 0.3761\n",
            "Early stopping:  0.09016754963342156\n",
            "Epoch: 010, Loss: 1.8079, Train: 0.3973, Test: 0.3880\n",
            "Early stopping:  0.08752566498636437\n",
            "Epoch: 011, Loss: 1.7518, Train: 0.4257, Test: 0.4132\n",
            "Early stopping:  0.08873727774416762\n",
            "Epoch: 012, Loss: 1.6926, Train: 0.4589, Test: 0.4379\n",
            "Early stopping:  0.09035582965316788\n",
            "Epoch: 013, Loss: 1.6347, Train: 0.4881, Test: 0.4568\n",
            "Early stopping:  0.0909580358699174\n",
            "Epoch: 014, Loss: 1.5815, Train: 0.5037, Test: 0.4682\n",
            "Early stopping:  0.09011795258278277\n",
            "Epoch: 015, Loss: 1.5279, Train: 0.5213, Test: 0.4872\n",
            "Early stopping:  0.08840181106291256\n",
            "Epoch: 016, Loss: 1.4731, Train: 0.5448, Test: 0.5041\n",
            "Early stopping:  0.0862919875774502\n",
            "Epoch: 017, Loss: 1.4218, Train: 0.5664, Test: 0.5245\n",
            "Early stopping:  0.08444408355853993\n",
            "Epoch: 018, Loss: 1.3759, Train: 0.5771, Test: 0.5392\n",
            "Early stopping:  0.08183758907571353\n",
            "Epoch: 019, Loss: 1.3326, Train: 0.5896, Test: 0.5566\n",
            "Early stopping:  0.07725331237480183\n",
            "Epoch: 020, Loss: 1.2914, Train: 0.6004, Test: 0.5602\n",
            "Early stopping:  0.07165916033185672\n",
            "Epoch: 021, Loss: 1.2525, Train: 0.6086, Test: 0.5640\n",
            "Early stopping:  0.06695108082561559\n",
            "Epoch: 022, Loss: 1.2159, Train: 0.6220, Test: 0.5727\n",
            "Early stopping:  0.063294378812473\n",
            "Epoch: 023, Loss: 1.1824, Train: 0.6282, Test: 0.5785\n",
            "Early stopping:  0.059480036162226545\n",
            "Epoch: 024, Loss: 1.1512, Train: 0.6361, Test: 0.5876\n",
            "Early stopping:  0.05547334285896703\n",
            "Epoch: 025, Loss: 1.1217, Train: 0.6475, Test: 0.5957\n",
            "Early stopping:  0.05162555247291404\n",
            "Epoch: 026, Loss: 1.0946, Train: 0.6543, Test: 0.6023\n",
            "Early stopping:  0.047964783462643906\n",
            "Epoch: 027, Loss: 1.0687, Train: 0.6622, Test: 0.6093\n",
            "Early stopping:  0.04492597157132427\n",
            "Epoch: 028, Loss: 1.0439, Train: 0.6704, Test: 0.6144\n",
            "Early stopping:  0.04232509047304489\n",
            "Epoch: 029, Loss: 1.0190, Train: 0.6778, Test: 0.6211\n",
            "Early stopping:  0.04049210090212182\n",
            "Epoch: 030, Loss: 0.9936, Train: 0.6860, Test: 0.6275\n",
            "Early stopping:  0.03979264546855787\n",
            "Epoch: 031, Loss: 0.9706, Train: 0.6974, Test: 0.6324\n",
            "Early stopping:  0.03898409911554752\n",
            "Epoch: 032, Loss: 0.9478, Train: 0.7048, Test: 0.6356\n",
            "Early stopping:  0.03807340950312133\n",
            "Epoch: 033, Loss: 0.9253, Train: 0.7192, Test: 0.6443\n",
            "Early stopping:  0.036892883901033326\n",
            "Epoch: 034, Loss: 0.9043, Train: 0.7297, Test: 0.6449\n",
            "Early stopping:  0.03540833715312868\n",
            "Epoch: 035, Loss: 0.8836, Train: 0.7351, Test: 0.6469\n",
            "Early stopping:  0.03438546265150727\n",
            "Epoch: 036, Loss: 0.8638, Train: 0.7453, Test: 0.6500\n",
            "Early stopping:  0.03317544992568637\n",
            "Epoch: 037, Loss: 0.8442, Train: 0.7524, Test: 0.6541\n",
            "Early stopping:  0.03206231643025277\n",
            "Epoch: 038, Loss: 0.8253, Train: 0.7638, Test: 0.6579\n",
            "Early stopping:  0.031224159834543738\n",
            "Epoch: 039, Loss: 0.8068, Train: 0.7720, Test: 0.6602\n",
            "Early stopping:  0.03037966184546649\n",
            "Epoch: 040, Loss: 0.7893, Train: 0.7777, Test: 0.6640\n",
            "Early stopping:  0.02948444486622841\n",
            "Epoch: 041, Loss: 0.7721, Train: 0.7825, Test: 0.6649\n",
            "Early stopping:  0.02851165933435347\n",
            "Epoch: 042, Loss: 0.7551, Train: 0.7879, Test: 0.6691\n",
            "Early stopping:  0.027688395577163488\n",
            "Epoch: 043, Loss: 0.7387, Train: 0.7935, Test: 0.6702\n",
            "Early stopping:  0.026923254979582804\n",
            "Epoch: 044, Loss: 0.7222, Train: 0.7989, Test: 0.6721\n",
            "Early stopping:  0.02648558164891725\n",
            "Epoch: 045, Loss: 0.7061, Train: 0.8037, Test: 0.6747\n",
            "Early stopping:  0.026068511436645556\n",
            "Epoch: 046, Loss: 0.6900, Train: 0.8120, Test: 0.6791\n",
            "Early stopping:  0.025746225644184596\n",
            "Epoch: 047, Loss: 0.6744, Train: 0.8145, Test: 0.6793\n",
            "Early stopping:  0.025428559232437295\n",
            "Epoch: 048, Loss: 0.6588, Train: 0.8202, Test: 0.6838\n",
            "Early stopping:  0.025045362858872905\n",
            "Epoch: 049, Loss: 0.6437, Train: 0.8304, Test: 0.6846\n",
            "Early stopping:  0.024672630516484943\n",
            "Epoch: 050, Loss: 0.6286, Train: 0.8347, Test: 0.6868\n",
            "Early stopping:  0.024275341617299058\n",
            "Epoch: 051, Loss: 0.6136, Train: 0.8400, Test: 0.6878\n",
            "Early stopping:  0.023997356380736617\n",
            "Epoch: 052, Loss: 0.5990, Train: 0.8463, Test: 0.6889\n",
            "Early stopping:  0.023669111027784698\n",
            "Epoch: 053, Loss: 0.5843, Train: 0.8514, Test: 0.6904\n",
            "Early stopping:  0.023455967148441566\n",
            "Epoch: 054, Loss: 0.5700, Train: 0.8559, Test: 0.6933\n",
            "Early stopping:  0.0231767462407548\n",
            "Epoch: 055, Loss: 0.5559, Train: 0.8613, Test: 0.6950\n",
            "Early stopping:  0.02285386572000472\n",
            "Epoch: 056, Loss: 0.5419, Train: 0.8667, Test: 0.6968\n",
            "Early stopping:  0.022565883190884962\n",
            "Epoch: 057, Loss: 0.5280, Train: 0.8687, Test: 0.6978\n",
            "Early stopping:  0.022256584211849083\n",
            "Epoch: 058, Loss: 0.5147, Train: 0.8761, Test: 0.6967\n",
            "Early stopping:  0.021901174196855177\n",
            "Epoch: 059, Loss: 0.5018, Train: 0.8752, Test: 0.6991\n",
            "Early stopping:  0.02139010006394999\n",
            "Epoch: 060, Loss: 0.4905, Train: 0.8851, Test: 0.6934\n",
            "Early stopping:  0.020387760726213098\n",
            "Epoch: 061, Loss: 0.4789, Train: 0.8823, Test: 0.6993\n",
            "Early stopping:  0.019346758942190314\n",
            "Epoch: 062, Loss: 0.4653, Train: 0.8919, Test: 0.7001\n",
            "Early stopping:  0.019236197102506125\n",
            "Epoch: 063, Loss: 0.4511, Train: 0.8951, Test: 0.6974\n",
            "Early stopping:  0.020067925371197726\n",
            "Epoch: 064, Loss: 0.4403, Train: 0.8945, Test: 0.7029\n",
            "Early stopping:  0.020298161192098523\n",
            "Epoch: 065, Loss: 0.4310, Train: 0.9050, Test: 0.6968\n",
            "Early stopping:  0.01919508854355758\n",
            "Epoch: 066, Loss: 0.4189, Train: 0.9044, Test: 0.7036\n",
            "Early stopping:  0.01791228438358146\n",
            "Epoch: 067, Loss: 0.4068, Train: 0.9064, Test: 0.7036\n",
            "Early stopping:  0.017415002971984898\n",
            "Epoch: 068, Loss: 0.3966, Train: 0.9144, Test: 0.7004\n",
            "Early stopping:  0.0176527983873834\n",
            "Epoch: 069, Loss: 0.3872, Train: 0.9146, Test: 0.7042\n",
            "Early stopping:  0.01738944659052655\n",
            "Epoch: 070, Loss: 0.3771, Train: 0.9248, Test: 0.7046\n",
            "Early stopping:  0.016330353130441105\n",
            "Epoch: 071, Loss: 0.3663, Train: 0.9274, Test: 0.7023\n",
            "Early stopping:  0.015896352354039918\n",
            "Epoch: 072, Loss: 0.3564, Train: 0.9280, Test: 0.7074\n",
            "Early stopping:  0.01602539747563313\n",
            "Epoch: 073, Loss: 0.3472, Train: 0.9356, Test: 0.7023\n",
            "Early stopping:  0.0159232319434778\n",
            "Epoch: 074, Loss: 0.3382, Train: 0.9334, Test: 0.7089\n",
            "Early stopping:  0.015348208085675736\n",
            "Epoch: 075, Loss: 0.3292, Train: 0.9430, Test: 0.7029\n",
            "Early stopping:  0.014613335719378031\n",
            "Epoch: 076, Loss: 0.3199, Train: 0.9430, Test: 0.7086\n",
            "Early stopping:  0.014375288443375434\n",
            "Epoch: 077, Loss: 0.3110, Train: 0.9461, Test: 0.7076\n",
            "Early stopping:  0.014316209238907023\n",
            "Epoch: 078, Loss: 0.3023, Train: 0.9507, Test: 0.7046\n",
            "Early stopping:  0.014214730131322191\n",
            "Epoch: 079, Loss: 0.2941, Train: 0.9504, Test: 0.7067\n",
            "Early stopping:  0.01388462566612416\n",
            "Epoch: 080, Loss: 0.2864, Train: 0.9586, Test: 0.7038\n",
            "Early stopping:  0.013265670156149554\n",
            "Epoch: 081, Loss: 0.2788, Train: 0.9541, Test: 0.7061\n",
            "Early stopping:  0.012711025946867958\n",
            "Epoch: 082, Loss: 0.2716, Train: 0.9637, Test: 0.7048\n",
            "Early stopping:  0.012134089557857625\n",
            "Epoch: 083, Loss: 0.2644, Train: 0.9597, Test: 0.7052\n",
            "Early stopping:  0.011732843030316072\n",
            "Epoch: 084, Loss: 0.2579, Train: 0.9674, Test: 0.7067\n",
            "Early stopping:  0.011278010116341858\n",
            "Epoch: 085, Loss: 0.2513, Train: 0.9631, Test: 0.7065\n",
            "Early stopping:  0.010846696452685961\n",
            "Epoch: 086, Loss: 0.2451, Train: 0.9688, Test: 0.7069\n",
            "Early stopping:  0.01046516977042879\n",
            "Epoch: 087, Loss: 0.2381, Train: 0.9719, Test: 0.7067\n",
            "Early stopping:  0.01036586689054753\n",
            "Epoch: 088, Loss: 0.2316, Train: 0.9691, Test: 0.7076\n",
            "Early stopping:  0.010440772042249883\n",
            "Epoch: 089, Loss: 0.2264, Train: 0.9767, Test: 0.7069\n",
            "Early stopping:  0.010049174872793016\n",
            "Epoch: 090, Loss: 0.2218, Train: 0.9694, Test: 0.7084\n",
            "Early stopping:  0.009245993631722555\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.71      0.70      0.70       568\n",
            "         capital_goods       0.63      0.57      0.60       381\n",
            "conglomerates_industry       0.97      0.50      0.66        60\n",
            "     consumer_cyclical       0.66      0.66      0.66       595\n",
            " consumer_non-cyclical       0.72      0.58      0.64       334\n",
            "                energy       0.79      0.70      0.75       213\n",
            "             financial       0.78      0.70      0.74       576\n",
            "            healthcare       0.85      0.74      0.79       238\n",
            "              services       0.67      0.83      0.74      1557\n",
            "            technology       0.68      0.53      0.60       297\n",
            "        transportation       0.81      0.73      0.77       303\n",
            "             utilities       0.87      0.67      0.76       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.66      0.70      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5043, Train: 0.2944, Test: 0.2941\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2896, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1517451762038348\n",
            "Epoch: 003, Loss: 2.1636, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17223070444404462\n",
            "Epoch: 004, Loss: 2.1476, Train: 0.2947, Test: 0.2950\n",
            "Early stopping:  0.1647301584170023\n",
            "Epoch: 005, Loss: 2.0980, Train: 0.3120, Test: 0.3145\n",
            "Early stopping:  0.16343075549009003\n",
            "Epoch: 006, Loss: 2.0266, Train: 0.3599, Test: 0.3557\n",
            "Early stopping:  0.09683511730549924\n",
            "Epoch: 007, Loss: 1.9666, Train: 0.3905, Test: 0.3909\n",
            "Early stopping:  0.08305440698548422\n",
            "Epoch: 008, Loss: 1.9129, Train: 0.4067, Test: 0.4009\n",
            "Early stopping:  0.09511358671678348\n",
            "Epoch: 009, Loss: 1.8575, Train: 0.4135, Test: 0.4045\n",
            "Early stopping:  0.09418945529735726\n",
            "Epoch: 010, Loss: 1.7990, Train: 0.4226, Test: 0.4075\n",
            "Early stopping:  0.08924465653763544\n",
            "Epoch: 011, Loss: 1.7388, Train: 0.4331, Test: 0.4166\n",
            "Early stopping:  0.09007191023308289\n",
            "Epoch: 012, Loss: 1.6786, Train: 0.4478, Test: 0.4285\n",
            "Early stopping:  0.09287725873733485\n",
            "Epoch: 013, Loss: 1.6185, Train: 0.4756, Test: 0.4559\n",
            "Early stopping:  0.0946077430008038\n",
            "Epoch: 014, Loss: 1.5605, Train: 0.4991, Test: 0.4691\n",
            "Early stopping:  0.0944248917947389\n",
            "Epoch: 015, Loss: 1.5075, Train: 0.5269, Test: 0.4857\n",
            "Early stopping:  0.09183898887120782\n",
            "Epoch: 016, Loss: 1.4562, Train: 0.5550, Test: 0.5105\n",
            "Early stopping:  0.08792079004632633\n",
            "Epoch: 017, Loss: 1.4069, Train: 0.5689, Test: 0.5277\n",
            "Early stopping:  0.08344555447096745\n",
            "Epoch: 018, Loss: 1.3642, Train: 0.5800, Test: 0.5377\n",
            "Early stopping:  0.0780522603854159\n",
            "Epoch: 019, Loss: 1.3259, Train: 0.5871, Test: 0.5507\n",
            "Early stopping:  0.07211485369962609\n",
            "Epoch: 020, Loss: 1.2874, Train: 0.5976, Test: 0.5589\n",
            "Early stopping:  0.06630116086276829\n",
            "Epoch: 021, Loss: 1.2478, Train: 0.6081, Test: 0.5668\n",
            "Early stopping:  0.062465942736388384\n",
            "Epoch: 022, Loss: 1.2107, Train: 0.6140, Test: 0.5712\n",
            "Early stopping:  0.060877008662326364\n",
            "Epoch: 023, Loss: 1.1784, Train: 0.6211, Test: 0.5810\n",
            "Early stopping:  0.05880469926210354\n",
            "Epoch: 024, Loss: 1.1472, Train: 0.6285, Test: 0.5901\n",
            "Early stopping:  0.055385946225640796\n",
            "Epoch: 025, Loss: 1.1168, Train: 0.6375, Test: 0.5989\n",
            "Early stopping:  0.05151372012758345\n",
            "Epoch: 026, Loss: 1.0892, Train: 0.6461, Test: 0.6059\n",
            "Early stopping:  0.0481992140290272\n",
            "Epoch: 027, Loss: 1.0641, Train: 0.6554, Test: 0.6112\n",
            "Early stopping:  0.04536456694172988\n",
            "Epoch: 028, Loss: 1.0398, Train: 0.6668, Test: 0.6156\n",
            "Early stopping:  0.04235298512950062\n",
            "Epoch: 029, Loss: 1.0144, Train: 0.6764, Test: 0.6190\n",
            "Early stopping:  0.0402188483052627\n",
            "Epoch: 030, Loss: 0.9906, Train: 0.6835, Test: 0.6224\n",
            "Early stopping:  0.03901530083916829\n",
            "Epoch: 031, Loss: 0.9691, Train: 0.6926, Test: 0.6299\n",
            "Early stopping:  0.03783129614778732\n",
            "Epoch: 032, Loss: 0.9462, Train: 0.7045, Test: 0.6379\n",
            "Early stopping:  0.036786923887910494\n",
            "Epoch: 033, Loss: 0.9235, Train: 0.7155, Test: 0.6413\n",
            "Early stopping:  0.035774040992974085\n",
            "Epoch: 034, Loss: 0.9022, Train: 0.7258, Test: 0.6430\n",
            "Early stopping:  0.03518214843243317\n",
            "Epoch: 035, Loss: 0.8805, Train: 0.7382, Test: 0.6460\n",
            "Early stopping:  0.03497927901508587\n",
            "Epoch: 036, Loss: 0.8592, Train: 0.7482, Test: 0.6511\n",
            "Early stopping:  0.0343046813351451\n",
            "Epoch: 037, Loss: 0.8390, Train: 0.7547, Test: 0.6560\n",
            "Early stopping:  0.033508704462030296\n",
            "Epoch: 038, Loss: 0.8190, Train: 0.7626, Test: 0.6626\n",
            "Early stopping:  0.03285634377306196\n",
            "Epoch: 039, Loss: 0.7995, Train: 0.7711, Test: 0.6683\n",
            "Early stopping:  0.031966585715256235\n",
            "Epoch: 040, Loss: 0.7808, Train: 0.7822, Test: 0.6717\n",
            "Early stopping:  0.03103642857387937\n",
            "Epoch: 041, Loss: 0.7619, Train: 0.7879, Test: 0.6761\n",
            "Early stopping:  0.030416175703641964\n",
            "Epoch: 042, Loss: 0.7435, Train: 0.7955, Test: 0.6779\n",
            "Early stopping:  0.02983452617638133\n",
            "Epoch: 043, Loss: 0.7257, Train: 0.8006, Test: 0.6789\n",
            "Early stopping:  0.02924215847314327\n",
            "Epoch: 044, Loss: 0.7074, Train: 0.8057, Test: 0.6796\n",
            "Early stopping:  0.02896086594404316\n",
            "Epoch: 045, Loss: 0.6897, Train: 0.8123, Test: 0.6834\n",
            "Early stopping:  0.02855425478063003\n",
            "Epoch: 046, Loss: 0.6721, Train: 0.8213, Test: 0.6863\n",
            "Early stopping:  0.0282587508892557\n",
            "Epoch: 047, Loss: 0.6543, Train: 0.8259, Test: 0.6912\n",
            "Early stopping:  0.028154589335378635\n",
            "Epoch: 048, Loss: 0.6372, Train: 0.8284, Test: 0.6951\n",
            "Early stopping:  0.027798670069595195\n",
            "Epoch: 049, Loss: 0.6200, Train: 0.8366, Test: 0.6970\n",
            "Early stopping:  0.027561048372589523\n",
            "Epoch: 050, Loss: 0.6032, Train: 0.8420, Test: 0.6976\n",
            "Early stopping:  0.0272304873127975\n",
            "Epoch: 051, Loss: 0.5865, Train: 0.8486, Test: 0.6995\n",
            "Early stopping:  0.026835213854783033\n",
            "Epoch: 052, Loss: 0.5700, Train: 0.8537, Test: 0.7018\n",
            "Early stopping:  0.02655437663784437\n",
            "Epoch: 053, Loss: 0.5541, Train: 0.8616, Test: 0.7018\n",
            "Early stopping:  0.026099483567409917\n",
            "Epoch: 054, Loss: 0.5386, Train: 0.8610, Test: 0.7055\n",
            "Early stopping:  0.025544894405900426\n",
            "Epoch: 055, Loss: 0.5248, Train: 0.8678, Test: 0.7008\n",
            "Early stopping:  0.024479543514588514\n",
            "Epoch: 056, Loss: 0.5129, Train: 0.8687, Test: 0.7046\n",
            "Early stopping:  0.022725442791859785\n",
            "Epoch: 057, Loss: 0.5002, Train: 0.8800, Test: 0.7065\n",
            "Early stopping:  0.02111293166207579\n",
            "Epoch: 058, Loss: 0.4817, Train: 0.8857, Test: 0.7082\n",
            "Early stopping:  0.021942416848271466\n",
            "Epoch: 059, Loss: 0.4662, Train: 0.8866, Test: 0.7080\n",
            "Early stopping:  0.02352480710421183\n",
            "Epoch: 060, Loss: 0.4571, Train: 0.8945, Test: 0.7089\n",
            "Early stopping:  0.023143183171520268\n",
            "Epoch: 061, Loss: 0.4455, Train: 0.8985, Test: 0.7101\n",
            "Early stopping:  0.02141528508416163\n",
            "Epoch: 062, Loss: 0.4301, Train: 0.9007, Test: 0.7112\n",
            "Early stopping:  0.01966701878654578\n",
            "Epoch: 063, Loss: 0.4170, Train: 0.9081, Test: 0.7112\n",
            "Early stopping:  0.019917055679661184\n",
            "Epoch: 064, Loss: 0.4083, Train: 0.9027, Test: 0.7110\n",
            "Early stopping:  0.020013417828116536\n",
            "Epoch: 065, Loss: 0.3987, Train: 0.9138, Test: 0.7108\n",
            "Early stopping:  0.018398867756092292\n",
            "Epoch: 066, Loss: 0.3852, Train: 0.9146, Test: 0.7135\n",
            "Early stopping:  0.017165366909753614\n",
            "Epoch: 067, Loss: 0.3729, Train: 0.9135, Test: 0.7140\n",
            "Early stopping:  0.017645797735817162\n",
            "Epoch: 068, Loss: 0.3641, Train: 0.9248, Test: 0.7103\n",
            "Early stopping:  0.01806138465282302\n",
            "Epoch: 069, Loss: 0.3563, Train: 0.9203, Test: 0.7106\n",
            "Early stopping:  0.016843377346688043\n",
            "Epoch: 070, Loss: 0.3468, Train: 0.9325, Test: 0.7133\n",
            "Early stopping:  0.01480517784021611\n",
            "Epoch: 071, Loss: 0.3354, Train: 0.9356, Test: 0.7135\n",
            "Early stopping:  0.014661541731786357\n",
            "Epoch: 072, Loss: 0.3250, Train: 0.9351, Test: 0.7140\n",
            "Early stopping:  0.015731249797997653\n",
            "Epoch: 073, Loss: 0.3169, Train: 0.9450, Test: 0.7120\n",
            "Early stopping:  0.015928199391231086\n",
            "Epoch: 074, Loss: 0.3103, Train: 0.9390, Test: 0.7108\n",
            "Early stopping:  0.014541717671271942\n",
            "Epoch: 075, Loss: 0.3045, Train: 0.9509, Test: 0.7108\n",
            "Early stopping:  0.012162139377702712\n",
            "Epoch: 076, Loss: 0.2971, Train: 0.9472, Test: 0.7125\n",
            "Early stopping:  0.010794267882934886\n",
            "Epoch: 077, Loss: 0.2878, Train: 0.9538, Test: 0.7110\n",
            "Early stopping:  0.011334355759864268\n",
            "Epoch: 078, Loss: 0.2776, Train: 0.9563, Test: 0.7131\n",
            "Early stopping:  0.013058520710522076\n",
            "Epoch: 079, Loss: 0.2701, Train: 0.9558, Test: 0.7131\n",
            "Early stopping:  0.01398407708512651\n",
            "Epoch: 080, Loss: 0.2653, Train: 0.9651, Test: 0.7123\n",
            "Early stopping:  0.012975633604884393\n",
            "Epoch: 081, Loss: 0.2608, Train: 0.9572, Test: 0.7112\n",
            "Early stopping:  0.01068174841843521\n",
            "Epoch: 082, Loss: 0.2551, Train: 0.9685, Test: 0.7127\n",
            "Early stopping:  0.008637418464577214\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.73      0.70       568\n",
            "         capital_goods       0.63      0.60      0.61       381\n",
            "conglomerates_industry       1.00      0.42      0.59        60\n",
            "     consumer_cyclical       0.63      0.71      0.66       595\n",
            " consumer_non-cyclical       0.73      0.62      0.67       334\n",
            "                energy       0.81      0.74      0.77       213\n",
            "             financial       0.77      0.73      0.75       576\n",
            "            healthcare       0.84      0.76      0.79       238\n",
            "              services       0.71      0.76      0.74      1557\n",
            "            technology       0.65      0.60      0.62       297\n",
            "        transportation       0.79      0.75      0.77       303\n",
            "             utilities       0.86      0.73      0.79       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.68      0.71      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "time: 2min 31s (started: 2024-10-16 20:59:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFmLteHJHrm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fca6a0-9378-4a0d-b8f6-b67a349f063b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 398 ms (started: 2024-10-16 21:02:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71eypyB-tAcN"
      },
      "source": [
        "### Training rotulated base = 60% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxmM92vBU939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5f2422-ec2c-48a6-e4c6-3ff4ab60ee3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 426 µs (started: 2024-10-16 21:02:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjeHM230HtEx"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98LU4CZFHtEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab84d206-f275-4235-ed23-7cff7a87dd86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 40.4308, Train: 0.1287, Test: 0.1222\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 469.9228, Train: 0.1095, Test: 0.1094\n",
            "Early stopping:  303.6967492337501\n",
            "Epoch: 003, Loss: 519.7562, Train: 0.0654, Test: 0.0649\n",
            "Early stopping:  263.5335643106124\n",
            "Epoch: 004, Loss: 248.8673, Train: 0.1053, Test: 0.1004\n",
            "Early stopping:  220.30126356761153\n",
            "Epoch: 005, Loss: 131.3970, Train: 0.1098, Test: 0.1038\n",
            "Early stopping:  208.55315660349794\n",
            "Epoch: 006, Loss: 61.9364, Train: 0.2032, Test: 0.2039\n",
            "Early stopping:  202.4563137276681\n",
            "Epoch: 007, Loss: 62.1564, Train: 0.2520, Test: 0.2450\n",
            "Early stopping:  191.88831146213124\n",
            "Epoch: 008, Loss: 36.2407, Train: 0.2253, Test: 0.2112\n",
            "Early stopping:  86.260985613348\n",
            "Epoch: 009, Loss: 11.3510, Train: 0.1172, Test: 0.1123\n",
            "Early stopping:  44.81984629520991\n",
            "Epoch: 010, Loss: 6.1310, Train: 0.1399, Test: 0.1335\n",
            "Early stopping:  26.719333031669997\n",
            "Epoch: 011, Loss: 4.4831, Train: 0.1974, Test: 0.1908\n",
            "Early stopping:  24.829041269248265\n",
            "Epoch: 012, Loss: 3.5262, Train: 0.2580, Test: 0.2512\n",
            "Early stopping:  13.694754065731601\n",
            "Epoch: 013, Loss: 3.0046, Train: 0.3025, Test: 0.2923\n",
            "Early stopping:  3.376041452348089\n",
            "Epoch: 014, Loss: 2.7203, Train: 0.3316, Test: 0.3150\n",
            "Early stopping:  1.3805864023294034\n",
            "Epoch: 015, Loss: 2.5685, Train: 0.3346, Test: 0.3153\n",
            "Early stopping:  0.7748688238034555\n",
            "Epoch: 016, Loss: 2.5056, Train: 0.3433, Test: 0.3402\n",
            "Early stopping:  0.41674280079668025\n",
            "Epoch: 017, Loss: 2.4080, Train: 0.3344, Test: 0.3320\n",
            "Early stopping:  0.23257490595843072\n",
            "Epoch: 018, Loss: 2.3438, Train: 0.3204, Test: 0.3181\n",
            "Early stopping:  0.14634334106873256\n",
            "Epoch: 019, Loss: 2.3179, Train: 0.3106, Test: 0.3025\n",
            "Early stopping:  0.10647375020610893\n",
            "Epoch: 020, Loss: 2.3111, Train: 0.2992, Test: 0.2883\n",
            "Early stopping:  0.08126724416199621\n",
            "Epoch: 021, Loss: 2.3056, Train: 0.2955, Test: 0.2844\n",
            "Early stopping:  0.042166325334143696\n",
            "Epoch: 022, Loss: 2.2928, Train: 0.3028, Test: 0.2906\n",
            "Early stopping:  0.018937416839650934\n",
            "Epoch: 023, Loss: 2.2701, Train: 0.3136, Test: 0.3000\n",
            "Early stopping:  0.018847280501718886\n",
            "Epoch: 024, Loss: 2.2419, Train: 0.3227, Test: 0.3116\n",
            "Early stopping:  0.028483843126176283\n",
            "Epoch: 025, Loss: 2.2111, Train: 0.3378, Test: 0.3269\n",
            "Early stopping:  0.038351604275664884\n",
            "Epoch: 026, Loss: 2.1794, Train: 0.3582, Test: 0.3351\n",
            "Early stopping:  0.045277905899939755\n",
            "Epoch: 027, Loss: 2.1518, Train: 0.3728, Test: 0.3493\n",
            "Early stopping:  0.047311696926310286\n",
            "Epoch: 028, Loss: 2.1344, Train: 0.3798, Test: 0.3578\n",
            "Early stopping:  0.043592187024508185\n",
            "Epoch: 029, Loss: 2.1168, Train: 0.3898, Test: 0.3689\n",
            "Early stopping:  0.03729616403571901\n",
            "Epoch: 030, Loss: 2.0902, Train: 0.4002, Test: 0.3799\n",
            "Early stopping:  0.033882422234801556\n",
            "Epoch: 031, Loss: 2.0622, Train: 0.4136, Test: 0.3904\n",
            "Early stopping:  0.03554963997730717\n",
            "Epoch: 032, Loss: 2.0383, Train: 0.4166, Test: 0.3986\n",
            "Early stopping:  0.03912310443338833\n",
            "Epoch: 033, Loss: 2.0150, Train: 0.4204, Test: 0.4029\n",
            "Early stopping:  0.040427861308643065\n",
            "Epoch: 034, Loss: 1.9911, Train: 0.4238, Test: 0.4012\n",
            "Early stopping:  0.03883173942800542\n",
            "Epoch: 035, Loss: 1.9710, Train: 0.4195, Test: 0.4003\n",
            "Early stopping:  0.03633344942109799\n",
            "Epoch: 036, Loss: 1.9564, Train: 0.4144, Test: 0.4003\n",
            "Early stopping:  0.03299272086319743\n",
            "Epoch: 037, Loss: 1.9417, Train: 0.4176, Test: 0.4009\n",
            "Early stopping:  0.028838992418683134\n",
            "Epoch: 038, Loss: 1.9244, Train: 0.4238, Test: 0.4148\n",
            "Early stopping:  0.02576791563068029\n",
            "Epoch: 039, Loss: 1.9071, Train: 0.4312, Test: 0.4213\n",
            "Early stopping:  0.025288877965812855\n",
            "Epoch: 040, Loss: 1.8941, Train: 0.4325, Test: 0.4233\n",
            "Early stopping:  0.02519871806148688\n",
            "Epoch: 041, Loss: 1.8779, Train: 0.4323, Test: 0.4199\n",
            "Early stopping:  0.02500741954945646\n",
            "Epoch: 042, Loss: 1.8594, Train: 0.4284, Test: 0.4171\n",
            "Early stopping:  0.025208561808973403\n",
            "Epoch: 043, Loss: 1.8487, Train: 0.4327, Test: 0.4230\n",
            "Early stopping:  0.024028654069726204\n",
            "Epoch: 044, Loss: 1.8344, Train: 0.4395, Test: 0.4273\n",
            "Early stopping:  0.023578020537560168\n",
            "Epoch: 045, Loss: 1.8191, Train: 0.4427, Test: 0.4290\n",
            "Early stopping:  0.02261596865479786\n",
            "Epoch: 046, Loss: 1.8053, Train: 0.4463, Test: 0.4301\n",
            "Early stopping:  0.02181078253963647\n",
            "Epoch: 047, Loss: 1.7926, Train: 0.4507, Test: 0.4301\n",
            "Early stopping:  0.022341291363470957\n",
            "Epoch: 048, Loss: 1.7783, Train: 0.4558, Test: 0.4344\n",
            "Early stopping:  0.021940766923515428\n",
            "Epoch: 049, Loss: 1.7636, Train: 0.4599, Test: 0.4361\n",
            "Early stopping:  0.02182937729595899\n",
            "Epoch: 050, Loss: 1.7529, Train: 0.4631, Test: 0.4395\n",
            "Early stopping:  0.021202776029370415\n",
            "Epoch: 051, Loss: 1.7391, Train: 0.4645, Test: 0.4412\n",
            "Early stopping:  0.020977865351955925\n",
            "Epoch: 052, Loss: 1.7269, Train: 0.4631, Test: 0.4415\n",
            "Early stopping:  0.02013584974841537\n",
            "Epoch: 053, Loss: 1.7180, Train: 0.4660, Test: 0.4415\n",
            "Early stopping:  0.018585146464084098\n",
            "Epoch: 054, Loss: 1.7067, Train: 0.4694, Test: 0.4432\n",
            "Early stopping:  0.01798964518458309\n",
            "Epoch: 055, Loss: 1.6968, Train: 0.4718, Test: 0.4457\n",
            "Early stopping:  0.016582368966764426\n",
            "Epoch: 056, Loss: 1.6858, Train: 0.4749, Test: 0.4477\n",
            "Early stopping:  0.016347124169577933\n",
            "Epoch: 057, Loss: 1.6741, Train: 0.4769, Test: 0.4471\n",
            "Early stopping:  0.01718927815440019\n",
            "Epoch: 058, Loss: 1.6625, Train: 0.4800, Test: 0.4508\n",
            "Early stopping:  0.017597570078012367\n",
            "Epoch: 059, Loss: 1.6521, Train: 0.4871, Test: 0.4602\n",
            "Early stopping:  0.017824112171888166\n",
            "Epoch: 060, Loss: 1.6411, Train: 0.4905, Test: 0.4582\n",
            "Early stopping:  0.01763513508632368\n",
            "Epoch: 061, Loss: 1.6320, Train: 0.4892, Test: 0.4576\n",
            "Early stopping:  0.01668327484564451\n",
            "Epoch: 062, Loss: 1.6241, Train: 0.4962, Test: 0.4602\n",
            "Early stopping:  0.015315562202691266\n",
            "Epoch: 063, Loss: 1.6106, Train: 0.4968, Test: 0.4621\n",
            "Early stopping:  0.015870864539654538\n",
            "Epoch: 064, Loss: 1.6072, Train: 0.4951, Test: 0.4593\n",
            "Early stopping:  0.014260100747358123\n",
            "Epoch: 065, Loss: 1.5969, Train: 0.4960, Test: 0.4607\n",
            "Early stopping:  0.013957492509457287\n",
            "Epoch: 066, Loss: 1.5909, Train: 0.4977, Test: 0.4627\n",
            "Early stopping:  0.012865008809498438\n",
            "Epoch: 067, Loss: 1.5815, Train: 0.4989, Test: 0.4621\n",
            "Early stopping:  0.01188554014243624\n",
            "Epoch: 068, Loss: 1.5742, Train: 0.5042, Test: 0.4681\n",
            "Early stopping:  0.012919609357882505\n",
            "Epoch: 069, Loss: 1.5595, Train: 0.5064, Test: 0.4647\n",
            "Early stopping:  0.014640967777499694\n",
            "Epoch: 070, Loss: 1.5567, Train: 0.5057, Test: 0.4675\n",
            "Early stopping:  0.014497911633284536\n",
            "Epoch: 071, Loss: 1.5504, Train: 0.5115, Test: 0.4724\n",
            "Early stopping:  0.012907902321635974\n",
            "Epoch: 072, Loss: 1.5354, Train: 0.5132, Test: 0.4752\n",
            "Early stopping:  0.014106074511461562\n",
            "Epoch: 073, Loss: 1.5293, Train: 0.5159, Test: 0.4755\n",
            "Early stopping:  0.01331005635877844\n",
            "Epoch: 074, Loss: 1.5223, Train: 0.5153, Test: 0.4755\n",
            "Early stopping:  0.014391389765574678\n",
            "Epoch: 075, Loss: 1.5119, Train: 0.5172, Test: 0.4760\n",
            "Early stopping:  0.014439892541199097\n",
            "Epoch: 076, Loss: 1.5058, Train: 0.5208, Test: 0.4792\n",
            "Early stopping:  0.012175719550693737\n",
            "Epoch: 077, Loss: 1.4992, Train: 0.5236, Test: 0.4777\n",
            "Early stopping:  0.012187812936292019\n",
            "Epoch: 078, Loss: 1.4943, Train: 0.5270, Test: 0.4794\n",
            "Early stopping:  0.010976946051368675\n",
            "Epoch: 079, Loss: 1.4819, Train: 0.5263, Test: 0.4817\n",
            "Early stopping:  0.011472669280120697\n",
            "Epoch: 080, Loss: 1.4772, Train: 0.5267, Test: 0.4843\n",
            "Early stopping:  0.011914373069570704\n",
            "Epoch: 081, Loss: 1.4725, Train: 0.5310, Test: 0.4868\n",
            "Early stopping:  0.011326748637553846\n",
            "Epoch: 082, Loss: 1.4590, Train: 0.5346, Test: 0.4845\n",
            "Early stopping:  0.012922722663685313\n",
            "Epoch: 083, Loss: 1.4572, Train: 0.5365, Test: 0.4857\n",
            "Early stopping:  0.010993903819573117\n",
            "Epoch: 084, Loss: 1.4504, Train: 0.5376, Test: 0.4899\n",
            "Early stopping:  0.011181673824694134\n",
            "Epoch: 085, Loss: 1.4399, Train: 0.5363, Test: 0.4899\n",
            "Early stopping:  0.011989312550015285\n",
            "Epoch: 086, Loss: 1.4331, Train: 0.5412, Test: 0.4905\n",
            "Early stopping:  0.011162697056772086\n",
            "Epoch: 087, Loss: 1.4243, Train: 0.5395, Test: 0.4931\n",
            "Early stopping:  0.013159639599336767\n",
            "Epoch: 088, Loss: 1.4218, Train: 0.5427, Test: 0.4925\n",
            "Early stopping:  0.011710398365172214\n",
            "Epoch: 089, Loss: 1.4117, Train: 0.5442, Test: 0.4914\n",
            "Early stopping:  0.010822977861902774\n",
            "Epoch: 090, Loss: 1.4041, Train: 0.5456, Test: 0.4919\n",
            "Early stopping:  0.011304588081313906\n",
            "Epoch: 091, Loss: 1.3943, Train: 0.5442, Test: 0.4950\n",
            "Early stopping:  0.012446780018188336\n",
            "Epoch: 092, Loss: 1.3898, Train: 0.5474, Test: 0.4942\n",
            "Early stopping:  0.012923942072221313\n",
            "Epoch: 093, Loss: 1.3826, Train: 0.5491, Test: 0.4939\n",
            "Early stopping:  0.011510297603655392\n",
            "Epoch: 094, Loss: 1.3764, Train: 0.5509, Test: 0.4928\n",
            "Early stopping:  0.010676534029799819\n",
            "Epoch: 095, Loss: 1.3673, Train: 0.5522, Test: 0.4965\n",
            "Early stopping:  0.010749927334015313\n",
            "Epoch: 096, Loss: 1.3586, Train: 0.5560, Test: 0.4967\n",
            "Early stopping:  0.012328942071209605\n",
            "Epoch: 097, Loss: 1.3498, Train: 0.5588, Test: 0.5007\n",
            "Early stopping:  0.013202177476266506\n",
            "Epoch: 098, Loss: 1.3440, Train: 0.5588, Test: 0.4990\n",
            "Early stopping:  0.013044314166275267\n",
            "Epoch: 099, Loss: 1.3363, Train: 0.5629, Test: 0.5013\n",
            "Early stopping:  0.012151549968788311\n",
            "Epoch: 100, Loss: 1.3301, Train: 0.5656, Test: 0.5050\n",
            "Early stopping:  0.011193155629660613\n",
            "Epoch: 101, Loss: 1.3231, Train: 0.5641, Test: 0.5112\n",
            "Early stopping:  0.010654136295353776\n",
            "Epoch: 102, Loss: 1.3215, Train: 0.5681, Test: 0.5064\n",
            "Early stopping:  0.009336301250783942\n",
            "PREDICTIONS -> tensor([0, 0, 1,  ..., 6, 5, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.52      0.57      0.54       379\n",
            "         capital_goods       0.37      0.26      0.30       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.41      0.54      0.46       396\n",
            " consumer_non-cyclical       0.64      0.38      0.48       223\n",
            "                energy       0.63      0.35      0.45       141\n",
            "             financial       0.54      0.52      0.53       384\n",
            "            healthcare       0.66      0.47      0.55       159\n",
            "              services       0.52      0.78      0.62      1038\n",
            "            technology       0.18      0.04      0.06       198\n",
            "        transportation       0.61      0.36      0.45       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.51      3527\n",
            "             macro avg       0.42      0.35      0.37      3527\n",
            "          weighted avg       0.48      0.51      0.47      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 100.1794, Train: 0.2879, Test: 0.2883\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 341.1819, Train: 0.0934, Test: 0.0961\n",
            "Early stopping:  170.41446060087296\n",
            "Epoch: 003, Loss: 376.4346, Train: 0.1172, Test: 0.1157\n",
            "Early stopping:  150.3561459612515\n",
            "Epoch: 004, Loss: 275.3536, Train: 0.0762, Test: 0.0731\n",
            "Early stopping:  122.77300672833198\n",
            "Epoch: 005, Loss: 197.2526, Train: 0.0575, Test: 0.0573\n",
            "Early stopping:  111.62958563380847\n",
            "Epoch: 006, Loss: 190.2493, Train: 0.0588, Test: 0.0598\n",
            "Early stopping:  83.50220968389088\n",
            "Epoch: 007, Loss: 126.5772, Train: 0.0775, Test: 0.0777\n",
            "Early stopping:  95.91461524090852\n",
            "Epoch: 008, Loss: 70.9712, Train: 0.2125, Test: 0.2092\n",
            "Early stopping:  77.33483599352822\n",
            "Epoch: 009, Loss: 56.6070, Train: 0.3019, Test: 0.2969\n",
            "Early stopping:  65.23348113820043\n",
            "Epoch: 010, Loss: 57.5756, Train: 0.2919, Test: 0.2866\n",
            "Early stopping:  57.82724710433973\n",
            "Epoch: 011, Loss: 28.7462, Train: 0.2662, Test: 0.2589\n",
            "Early stopping:  36.122715907389065\n",
            "Epoch: 012, Loss: 12.8668, Train: 0.1947, Test: 0.1891\n",
            "Early stopping:  23.787988288904973\n",
            "Epoch: 013, Loss: 7.4277, Train: 0.2072, Test: 0.2044\n",
            "Early stopping:  23.653741640645013\n",
            "Epoch: 014, Loss: 4.8638, Train: 0.2265, Test: 0.2220\n",
            "Early stopping:  21.795138003332077\n",
            "Epoch: 015, Loss: 3.4875, Train: 0.2233, Test: 0.2081\n",
            "Early stopping:  10.297091959486355\n",
            "Epoch: 016, Loss: 2.9944, Train: 0.2200, Test: 0.2010\n",
            "Early stopping:  4.04014879114746\n",
            "Epoch: 017, Loss: 2.7328, Train: 0.2386, Test: 0.2183\n",
            "Early stopping:  1.9314153932626712\n",
            "Epoch: 018, Loss: 2.5259, Train: 0.2730, Test: 0.2509\n",
            "Early stopping:  0.9344270732804233\n",
            "Epoch: 019, Loss: 2.3785, Train: 0.3282, Test: 0.3105\n",
            "Early stopping:  0.4372956782831876\n",
            "Epoch: 020, Loss: 2.2878, Train: 0.3652, Test: 0.3456\n",
            "Early stopping:  0.28455494609698373\n",
            "Epoch: 021, Loss: 2.2304, Train: 0.3620, Test: 0.3451\n",
            "Early stopping:  0.20222420948126532\n",
            "Epoch: 022, Loss: 2.2000, Train: 0.3533, Test: 0.3408\n",
            "Early stopping:  0.13152987780184605\n",
            "Epoch: 023, Loss: 2.1951, Train: 0.3367, Test: 0.3272\n",
            "Early stopping:  0.0766393284313916\n",
            "Epoch: 024, Loss: 2.2024, Train: 0.3304, Test: 0.3278\n",
            "Early stopping:  0.038708329625326464\n",
            "Epoch: 025, Loss: 2.2076, Train: 0.3287, Test: 0.3241\n",
            "Early stopping:  0.013793198482841992\n",
            "Epoch: 026, Loss: 2.2058, Train: 0.3369, Test: 0.3300\n",
            "Early stopping:  0.004942562492287966\n",
            "PREDICTIONS -> tensor([3, 8, 3,  ..., 8, 1, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.37      0.23      0.29       379\n",
            "         capital_goods       0.09      0.17      0.12       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.20      0.50      0.28       396\n",
            " consumer_non-cyclical       0.56      0.15      0.24       223\n",
            "                energy       0.50      0.04      0.07       141\n",
            "             financial       0.68      0.16      0.25       384\n",
            "            healthcare       0.53      0.39      0.45       159\n",
            "              services       0.44      0.65      0.52      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.00      0.00      0.00       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.33      3527\n",
            "             macro avg       0.28      0.19      0.19      3527\n",
            "          weighted avg       0.35      0.33      0.29      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 83.8132, Train: 0.2943, Test: 0.2900\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 361.1487, Train: 0.1146, Test: 0.1154\n",
            "Early stopping:  196.1058199643684\n",
            "Epoch: 003, Loss: 568.0828, Train: 0.0635, Test: 0.0629\n",
            "Early stopping:  242.9861777201259\n",
            "Epoch: 004, Loss: 751.5924, Train: 0.0743, Test: 0.0740\n",
            "Early stopping:  286.6915925309604\n",
            "Epoch: 005, Loss: 714.3441, Train: 0.0696, Test: 0.0644\n",
            "Early stopping:  276.71292674048874\n",
            "Epoch: 006, Loss: 460.6747, Train: 0.1274, Test: 0.1324\n",
            "Early stopping:  165.36229929302257\n",
            "Epoch: 007, Loss: 347.0760, Train: 0.1180, Test: 0.1177\n",
            "Early stopping:  169.8884256175462\n",
            "Epoch: 008, Loss: 201.2636, Train: 0.1193, Test: 0.1145\n",
            "Early stopping:  236.26909344170676\n",
            "Epoch: 009, Loss: 117.1193, Train: 0.2658, Test: 0.2591\n",
            "Early stopping:  234.3729683839389\n",
            "Epoch: 010, Loss: 91.0065, Train: 0.2204, Test: 0.2203\n",
            "Early stopping:  157.24427658811865\n",
            "Epoch: 011, Loss: 72.1464, Train: 0.2149, Test: 0.2081\n",
            "Early stopping:  112.75357081885932\n",
            "Epoch: 012, Loss: 43.0422, Train: 0.2637, Test: 0.2614\n",
            "Early stopping:  60.26496561281105\n",
            "Epoch: 013, Loss: 15.4866, Train: 0.2520, Test: 0.2563\n",
            "Early stopping:  39.810389859414336\n",
            "Epoch: 014, Loss: 6.1091, Train: 0.1943, Test: 0.1959\n",
            "Early stopping:  36.19168550328168\n",
            "Epoch: 015, Loss: 4.8048, Train: 0.1798, Test: 0.1800\n",
            "Early stopping:  28.933067645935413\n",
            "Epoch: 016, Loss: 4.2938, Train: 0.1794, Test: 0.1761\n",
            "Early stopping:  16.461283042059616\n",
            "Epoch: 017, Loss: 3.8172, Train: 0.1822, Test: 0.1741\n",
            "Early stopping:  4.874429151257791\n",
            "Epoch: 018, Loss: 3.3990, Train: 0.1922, Test: 0.1775\n",
            "Early stopping:  1.0490061630203213\n",
            "Epoch: 019, Loss: 3.0891, Train: 0.1924, Test: 0.1783\n",
            "Early stopping:  0.6868762789120959\n",
            "Epoch: 020, Loss: 2.9022, Train: 0.1958, Test: 0.1849\n",
            "Early stopping:  0.5628361004809704\n",
            "Epoch: 021, Loss: 2.8097, Train: 0.2006, Test: 0.1928\n",
            "Early stopping:  0.41043263106407407\n",
            "Epoch: 022, Loss: 2.7407, Train: 0.2078, Test: 0.2007\n",
            "Early stopping:  0.26434179896596294\n",
            "Epoch: 023, Loss: 2.6738, Train: 0.2091, Test: 0.2030\n",
            "Early stopping:  0.16147055366082116\n",
            "Epoch: 024, Loss: 2.6091, Train: 0.2132, Test: 0.2078\n",
            "Early stopping:  0.11448280976620155\n",
            "Epoch: 025, Loss: 2.5503, Train: 0.2189, Test: 0.2124\n",
            "Early stopping:  0.10287760048702592\n",
            "Epoch: 026, Loss: 2.4856, Train: 0.2276, Test: 0.2206\n",
            "Early stopping:  0.10023375796070426\n",
            "Epoch: 027, Loss: 2.4196, Train: 0.2512, Test: 0.2404\n",
            "Early stopping:  0.09996120791688728\n",
            "Epoch: 028, Loss: 2.3525, Train: 0.2722, Test: 0.2648\n",
            "Early stopping:  0.10185214057867667\n",
            "Epoch: 029, Loss: 2.2951, Train: 0.2756, Test: 0.2705\n",
            "Early stopping:  0.10176122141192186\n",
            "Epoch: 030, Loss: 2.2423, Train: 0.2769, Test: 0.2657\n",
            "Early stopping:  0.09671498223209514\n",
            "Epoch: 031, Loss: 2.1991, Train: 0.2815, Test: 0.2716\n",
            "Early stopping:  0.087406437995347\n",
            "Epoch: 032, Loss: 2.1646, Train: 0.2907, Test: 0.2824\n",
            "Early stopping:  0.07495194156888446\n",
            "Epoch: 033, Loss: 2.1364, Train: 0.2896, Test: 0.2796\n",
            "Early stopping:  0.06295618716801481\n",
            "Epoch: 034, Loss: 2.1153, Train: 0.2892, Test: 0.2756\n",
            "Early stopping:  0.050533993490061706\n",
            "Epoch: 035, Loss: 2.0979, Train: 0.2962, Test: 0.2798\n",
            "Early stopping:  0.040181692022738955\n",
            "Epoch: 036, Loss: 2.0800, Train: 0.3096, Test: 0.2977\n",
            "Early stopping:  0.033029231760774894\n",
            "Epoch: 037, Loss: 2.0594, Train: 0.3123, Test: 0.2994\n",
            "Early stopping:  0.029957793050515134\n",
            "Epoch: 038, Loss: 2.0401, Train: 0.3183, Test: 0.3022\n",
            "Early stopping:  0.029883060141888403\n",
            "Epoch: 039, Loss: 2.0190, Train: 0.3180, Test: 0.3056\n",
            "Early stopping:  0.03125811186745709\n",
            "Epoch: 040, Loss: 1.9983, Train: 0.3219, Test: 0.3099\n",
            "Early stopping:  0.032211108640135044\n",
            "Epoch: 041, Loss: 1.9761, Train: 0.3355, Test: 0.3215\n",
            "Early stopping:  0.03295040173113582\n",
            "Epoch: 042, Loss: 1.9574, Train: 0.3399, Test: 0.3317\n",
            "Early stopping:  0.032951453379931904\n",
            "Epoch: 043, Loss: 1.9379, Train: 0.3342, Test: 0.3249\n",
            "Early stopping:  0.032137223336503457\n",
            "Epoch: 044, Loss: 1.9206, Train: 0.3355, Test: 0.3249\n",
            "Early stopping:  0.030661972510113537\n",
            "Epoch: 045, Loss: 1.9044, Train: 0.3355, Test: 0.3258\n",
            "Early stopping:  0.02852287391066064\n",
            "Epoch: 046, Loss: 1.8841, Train: 0.3507, Test: 0.3391\n",
            "Early stopping:  0.028502998945221646\n",
            "Epoch: 047, Loss: 1.8644, Train: 0.3514, Test: 0.3439\n",
            "Early stopping:  0.029049213312828663\n",
            "Epoch: 048, Loss: 1.8471, Train: 0.3567, Test: 0.3439\n",
            "Early stopping:  0.029587591814244193\n",
            "Epoch: 049, Loss: 1.8322, Train: 0.3667, Test: 0.3516\n",
            "Early stopping:  0.028749145642620096\n",
            "Epoch: 050, Loss: 1.8144, Train: 0.3546, Test: 0.3538\n",
            "Early stopping:  0.02716841356778799\n",
            "Epoch: 051, Loss: 1.7977, Train: 0.3656, Test: 0.3572\n",
            "Early stopping:  0.026256463251371417\n",
            "Epoch: 052, Loss: 1.7842, Train: 0.3949, Test: 0.3811\n",
            "Early stopping:  0.025362284907331095\n",
            "Epoch: 053, Loss: 1.7707, Train: 0.4113, Test: 0.3958\n",
            "Early stopping:  0.02426338743495412\n",
            "Epoch: 054, Loss: 1.7552, Train: 0.4144, Test: 0.3952\n",
            "Early stopping:  0.02298740417468603\n",
            "Epoch: 055, Loss: 1.7452, Train: 0.4200, Test: 0.4001\n",
            "Early stopping:  0.021226998877110676\n",
            "Epoch: 056, Loss: 1.7334, Train: 0.4238, Test: 0.4043\n",
            "Early stopping:  0.020145777968309738\n",
            "Epoch: 057, Loss: 1.7212, Train: 0.4248, Test: 0.4037\n",
            "Early stopping:  0.01915307742584568\n",
            "Epoch: 058, Loss: 1.7115, Train: 0.4253, Test: 0.4037\n",
            "Early stopping:  0.017629649474057808\n",
            "Epoch: 059, Loss: 1.7017, Train: 0.4297, Test: 0.4060\n",
            "Early stopping:  0.01721583608741363\n",
            "Epoch: 060, Loss: 1.6912, Train: 0.4321, Test: 0.4080\n",
            "Early stopping:  0.016422059854616335\n",
            "Epoch: 061, Loss: 1.6796, Train: 0.4380, Test: 0.4117\n",
            "Early stopping:  0.01639145552863426\n",
            "Epoch: 062, Loss: 1.6696, Train: 0.4480, Test: 0.4171\n",
            "Early stopping:  0.0167690041243458\n",
            "Epoch: 063, Loss: 1.6604, Train: 0.4476, Test: 0.4159\n",
            "Early stopping:  0.01651592848206104\n",
            "Epoch: 064, Loss: 1.6497, Train: 0.4505, Test: 0.4179\n",
            "Early stopping:  0.01616265065561068\n",
            "Epoch: 065, Loss: 1.6398, Train: 0.4550, Test: 0.4179\n",
            "Early stopping:  0.015702107221509397\n",
            "Epoch: 066, Loss: 1.6324, Train: 0.4573, Test: 0.4199\n",
            "Early stopping:  0.015024564753865323\n",
            "Epoch: 067, Loss: 1.6208, Train: 0.4635, Test: 0.4244\n",
            "Early stopping:  0.01528438293561879\n",
            "Epoch: 068, Loss: 1.6190, Train: 0.4682, Test: 0.4210\n",
            "Early stopping:  0.012942852674384301\n",
            "Epoch: 069, Loss: 1.6074, Train: 0.4688, Test: 0.4259\n",
            "Early stopping:  0.012574915159171312\n",
            "Epoch: 070, Loss: 1.6015, Train: 0.4728, Test: 0.4295\n",
            "Early stopping:  0.012112853451631805\n",
            "Epoch: 071, Loss: 1.5926, Train: 0.4735, Test: 0.4363\n",
            "Early stopping:  0.011876226057043025\n",
            "Epoch: 072, Loss: 1.5797, Train: 0.4756, Test: 0.4318\n",
            "Early stopping:  0.014894832207205068\n",
            "Epoch: 073, Loss: 1.5756, Train: 0.4798, Test: 0.4349\n",
            "Early stopping:  0.013659434621212094\n",
            "Epoch: 074, Loss: 1.5749, Train: 0.4775, Test: 0.4426\n",
            "Early stopping:  0.011710373257263786\n",
            "Epoch: 075, Loss: 1.5668, Train: 0.4873, Test: 0.4485\n",
            "Early stopping:  0.009436943957310047\n",
            "PREDICTIONS -> tensor([8, 8, 5,  ..., 6, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.36      0.48      0.41       379\n",
            "         capital_goods       0.23      0.07      0.10       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.39      0.22      0.29       396\n",
            " consumer_non-cyclical       0.70      0.28      0.40       223\n",
            "                energy       0.50      0.30      0.38       141\n",
            "             financial       0.54      0.54      0.54       384\n",
            "            healthcare       0.41      0.37      0.39       159\n",
            "              services       0.44      0.80      0.57      1038\n",
            "            technology       0.33      0.03      0.05       198\n",
            "        transportation       0.62      0.46      0.53       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.45      3527\n",
            "             macro avg       0.38      0.30      0.30      3527\n",
            "          weighted avg       0.43      0.45      0.40      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 119.3362, Train: 0.0839, Test: 0.0814\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 153.3951, Train: 0.2977, Test: 0.2960\n",
            "Early stopping:  24.083272995370532\n",
            "Epoch: 003, Loss: 280.8817, Train: 0.1078, Test: 0.1086\n",
            "Early stopping:  85.15646826105241\n",
            "Epoch: 004, Loss: 383.1480, Train: 0.1074, Test: 0.1024\n",
            "Early stopping:  121.2267822389569\n",
            "Epoch: 005, Loss: 551.6326, Train: 0.1140, Test: 0.1128\n",
            "Early stopping:  176.5669324058092\n",
            "Epoch: 006, Loss: 589.5466, Train: 0.0510, Test: 0.0471\n",
            "Early stopping:  182.93768647317552\n",
            "Epoch: 007, Loss: 570.8870, Train: 0.1198, Test: 0.1182\n",
            "Early stopping:  136.2959027208408\n",
            "Epoch: 008, Loss: 377.6218, Train: 0.0488, Test: 0.0491\n",
            "Early stopping:  105.11034021882635\n",
            "Epoch: 009, Loss: 382.9055, Train: 0.2140, Test: 0.2141\n",
            "Early stopping:  105.17463835150092\n",
            "Epoch: 010, Loss: 380.7198, Train: 0.3013, Test: 0.3014\n",
            "Early stopping:  109.65032661137435\n",
            "Epoch: 011, Loss: 393.9432, Train: 0.2991, Test: 0.2974\n",
            "Early stopping:  83.89474544129025\n",
            "Epoch: 012, Loss: 350.2759, Train: 0.3091, Test: 0.3068\n",
            "Early stopping:  16.20419205577558\n",
            "Epoch: 013, Loss: 266.8348, Train: 0.3093, Test: 0.3017\n",
            "Early stopping:  51.84637994586902\n",
            "Epoch: 014, Loss: 178.7746, Train: 0.2868, Test: 0.2750\n",
            "Early stopping:  90.3725414061281\n",
            "Epoch: 015, Loss: 107.2717, Train: 0.1811, Test: 0.1781\n",
            "Early stopping:  118.38396972638216\n",
            "Epoch: 016, Loss: 102.9040, Train: 0.1904, Test: 0.1809\n",
            "Early stopping:  106.67601556711726\n",
            "Epoch: 017, Loss: 73.6388, Train: 0.1681, Test: 0.1554\n",
            "Early stopping:  77.89168966486517\n",
            "Epoch: 018, Loss: 43.7087, Train: 0.2138, Test: 0.2039\n",
            "Early stopping:  50.30517715734202\n",
            "Epoch: 019, Loss: 20.0498, Train: 0.3023, Test: 0.2923\n",
            "Early stopping:  37.65025948560711\n",
            "Epoch: 020, Loss: 8.4430, Train: 0.3784, Test: 0.3652\n",
            "Early stopping:  38.7972021495108\n",
            "Epoch: 021, Loss: 4.2508, Train: 0.3754, Test: 0.3672\n",
            "Early stopping:  28.805381863462777\n",
            "Epoch: 022, Loss: 3.7131, Train: 0.3671, Test: 0.3572\n",
            "Early stopping:  16.80986274477654\n",
            "Epoch: 023, Loss: 3.4245, Train: 0.3652, Test: 0.3521\n",
            "Early stopping:  7.049133041723347\n",
            "Epoch: 024, Loss: 3.1801, Train: 0.3622, Test: 0.3479\n",
            "Early stopping:  2.1837506146616983\n",
            "Epoch: 025, Loss: 2.9491, Train: 0.3561, Test: 0.3422\n",
            "Early stopping:  0.505123444573796\n",
            "Epoch: 026, Loss: 2.7409, Train: 0.3469, Test: 0.3374\n",
            "Early stopping:  0.38331849366442855\n",
            "Epoch: 027, Loss: 2.5660, Train: 0.3318, Test: 0.3210\n",
            "Early stopping:  0.3415973893302045\n",
            "Epoch: 028, Loss: 2.4318, Train: 0.3093, Test: 0.3051\n",
            "Early stopping:  0.29874001275716255\n",
            "Epoch: 029, Loss: 2.3437, Train: 0.2907, Test: 0.2869\n",
            "Early stopping:  0.24324145300540936\n",
            "Epoch: 030, Loss: 2.2921, Train: 0.2656, Test: 0.2634\n",
            "Early stopping:  0.18134360685922474\n",
            "Epoch: 031, Loss: 2.2620, Train: 0.2526, Test: 0.2572\n",
            "Early stopping:  0.12273380735138162\n",
            "Epoch: 032, Loss: 2.2426, Train: 0.2427, Test: 0.2438\n",
            "Early stopping:  0.07592016757187132\n",
            "Epoch: 033, Loss: 2.2309, Train: 0.2310, Test: 0.2322\n",
            "Early stopping:  0.045214797247750065\n",
            "Epoch: 034, Loss: 2.2259, Train: 0.2242, Test: 0.2254\n",
            "Early stopping:  0.026996953549652175\n",
            "Epoch: 035, Loss: 2.2249, Train: 0.2147, Test: 0.2231\n",
            "Early stopping:  0.015520178094147208\n",
            "Epoch: 036, Loss: 2.2228, Train: 0.2164, Test: 0.2231\n",
            "Early stopping:  0.007943812857646447\n",
            "PREDICTIONS -> tensor([10,  6,  7,  ...,  4,  7,  7], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.19      0.01      0.02       379\n",
            "         capital_goods       0.12      0.05      0.07       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.22      0.04      0.06       396\n",
            " consumer_non-cyclical       0.60      0.34      0.43       223\n",
            "                energy       0.08      0.06      0.07       141\n",
            "             financial       0.52      0.42      0.47       384\n",
            "            healthcare       0.06      0.69      0.11       159\n",
            "              services       0.66      0.29      0.40      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.20      0.35      0.25       202\n",
            "             utilities       0.21      0.24      0.22       113\n",
            "\n",
            "              accuracy                           0.22      3527\n",
            "             macro avg       0.24      0.21      0.18      3527\n",
            "          weighted avg       0.36      0.22      0.24      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 50.6492, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 686.7196, Train: 0.0327, Test: 0.0295\n",
            "Early stopping:  449.7697229917388\n",
            "Epoch: 003, Loss: 275.2458, Train: 0.1420, Test: 0.1316\n",
            "Early stopping:  322.5781467514098\n",
            "Epoch: 004, Loss: 172.6651, Train: 0.1176, Test: 0.1177\n",
            "Early stopping:  275.98348882372625\n",
            "Epoch: 005, Loss: 333.4167, Train: 0.0896, Test: 0.0896\n",
            "Early stopping:  239.58380295102242\n",
            "Epoch: 006, Loss: 299.5485, Train: 0.1297, Test: 0.1350\n",
            "Early stopping:  195.6874922325603\n",
            "Epoch: 007, Loss: 313.4633, Train: 0.0537, Test: 0.0584\n",
            "Early stopping:  63.03222789412232\n",
            "Epoch: 008, Loss: 361.1995, Train: 0.3025, Test: 0.2974\n",
            "Early stopping:  72.76676408241524\n",
            "Epoch: 009, Loss: 359.7299, Train: 0.3036, Test: 0.2952\n",
            "Early stopping:  27.429207506378035\n",
            "Epoch: 010, Loss: 346.3508, Train: 0.0809, Test: 0.0777\n",
            "Early stopping:  28.026141890399813\n",
            "Epoch: 011, Loss: 364.4741, Train: 0.2726, Test: 0.2648\n",
            "Early stopping:  21.05510352640866\n",
            "Epoch: 012, Loss: 258.0114, Train: 0.1526, Test: 0.1514\n",
            "Early stopping:  45.21940636644006\n",
            "Epoch: 013, Loss: 219.8722, Train: 0.2384, Test: 0.2314\n",
            "Early stopping:  66.308423109269\n",
            "Epoch: 014, Loss: 161.6427, Train: 0.2561, Test: 0.2461\n",
            "Early stopping:  85.37066666867825\n",
            "Epoch: 015, Loss: 114.8762, Train: 0.2478, Test: 0.2455\n",
            "Early stopping:  95.7901094229029\n",
            "Epoch: 016, Loss: 72.7324, Train: 0.2227, Test: 0.2251\n",
            "Early stopping:  75.32953284869575\n",
            "Epoch: 017, Loss: 38.7728, Train: 0.2788, Test: 0.2793\n",
            "Early stopping:  71.68409596887578\n",
            "Epoch: 018, Loss: 15.5748, Train: 0.1921, Test: 0.1897\n",
            "Early stopping:  58.6992478673053\n",
            "Epoch: 019, Loss: 6.4148, Train: 0.1376, Test: 0.1330\n",
            "Early stopping:  44.54202633485289\n",
            "Epoch: 020, Loss: 4.4425, Train: 0.1280, Test: 0.1262\n",
            "Early stopping:  28.68558990028043\n",
            "Epoch: 021, Loss: 3.7752, Train: 0.1357, Test: 0.1355\n",
            "Early stopping:  14.742721315941004\n",
            "Epoch: 022, Loss: 3.2846, Train: 0.1471, Test: 0.1466\n",
            "Early stopping:  5.102908626329245\n",
            "Epoch: 023, Loss: 2.9067, Train: 0.1692, Test: 0.1718\n",
            "Early stopping:  1.3828407538176795\n",
            "Epoch: 024, Loss: 2.6595, Train: 0.2178, Test: 0.2149\n",
            "Early stopping:  0.7126666975262239\n",
            "Epoch: 025, Loss: 2.4236, Train: 0.2406, Test: 0.2410\n",
            "Early stopping:  0.5334518459263312\n",
            "Epoch: 026, Loss: 2.3339, Train: 0.2603, Test: 0.2518\n",
            "Early stopping:  0.3854080709656459\n",
            "Epoch: 027, Loss: 2.3170, Train: 0.2875, Test: 0.2813\n",
            "Early stopping:  0.2518800247742886\n",
            "Epoch: 028, Loss: 2.2953, Train: 0.3189, Test: 0.3170\n",
            "Early stopping:  0.14997252525550087\n",
            "Epoch: 029, Loss: 2.2659, Train: 0.3312, Test: 0.3266\n",
            "Early stopping:  0.05962529922235714\n",
            "Epoch: 030, Loss: 2.2403, Train: 0.3359, Test: 0.3292\n",
            "Early stopping:  0.03787362063902878\n",
            "Epoch: 031, Loss: 2.2264, Train: 0.3365, Test: 0.3292\n",
            "Early stopping:  0.0375599805531216\n",
            "Epoch: 032, Loss: 2.2235, Train: 0.3359, Test: 0.3312\n",
            "Early stopping:  0.030223212702904557\n",
            "Epoch: 033, Loss: 2.2213, Train: 0.3355, Test: 0.3289\n",
            "Early stopping:  0.018528914932464936\n",
            "Epoch: 034, Loss: 2.2107, Train: 0.3380, Test: 0.3314\n",
            "Early stopping:  0.01067469988514009\n",
            "Epoch: 035, Loss: 2.1908, Train: 0.3412, Test: 0.3351\n",
            "Early stopping:  0.014522295146426972\n",
            "Epoch: 036, Loss: 2.1668, Train: 0.3405, Test: 0.3377\n",
            "Early stopping:  0.023843651895977938\n",
            "Epoch: 037, Loss: 2.1447, Train: 0.3439, Test: 0.3399\n",
            "Early stopping:  0.03144055565108617\n",
            "Epoch: 038, Loss: 2.1280, Train: 0.3450, Test: 0.3399\n",
            "Early stopping:  0.03349310851256824\n",
            "Epoch: 039, Loss: 2.1165, Train: 0.3474, Test: 0.3422\n",
            "Early stopping:  0.029920719330140583\n",
            "Epoch: 040, Loss: 2.1081, Train: 0.3473, Test: 0.3428\n",
            "Early stopping:  0.023412391965818978\n",
            "Epoch: 041, Loss: 2.1000, Train: 0.3484, Test: 0.3453\n",
            "Early stopping:  0.017488168508105806\n",
            "Epoch: 042, Loss: 2.0892, Train: 0.3478, Test: 0.3439\n",
            "Early stopping:  0.014899201084538117\n",
            "Epoch: 043, Loss: 2.0771, Train: 0.3526, Test: 0.3442\n",
            "Early stopping:  0.015502209498641298\n",
            "Epoch: 044, Loss: 2.0615, Train: 0.3565, Test: 0.3482\n",
            "Early stopping:  0.0185137275526517\n",
            "Epoch: 045, Loss: 2.0432, Train: 0.3590, Test: 0.3502\n",
            "Early stopping:  0.022480226957936868\n",
            "Epoch: 046, Loss: 2.0272, Train: 0.3599, Test: 0.3502\n",
            "Early stopping:  0.02504011390445591\n",
            "Epoch: 047, Loss: 2.0130, Train: 0.3620, Test: 0.3544\n",
            "Early stopping:  0.025707624376884092\n",
            "Epoch: 048, Loss: 1.9994, Train: 0.3635, Test: 0.3550\n",
            "Early stopping:  0.02444590554710172\n",
            "Epoch: 049, Loss: 1.9879, Train: 0.3665, Test: 0.3550\n",
            "Early stopping:  0.02194314480949769\n",
            "Epoch: 050, Loss: 1.9778, Train: 0.3681, Test: 0.3564\n",
            "Early stopping:  0.019630488712661542\n",
            "Epoch: 051, Loss: 1.9677, Train: 0.3686, Test: 0.3587\n",
            "Early stopping:  0.017777150955502648\n",
            "Epoch: 052, Loss: 1.9580, Train: 0.3696, Test: 0.3587\n",
            "Early stopping:  0.016299256752797094\n",
            "Epoch: 053, Loss: 1.9463, Train: 0.3699, Test: 0.3592\n",
            "Early stopping:  0.016295119308744783\n",
            "Epoch: 054, Loss: 1.9329, Train: 0.3715, Test: 0.3589\n",
            "Early stopping:  0.017624141687614357\n",
            "Epoch: 055, Loss: 1.9195, Train: 0.3703, Test: 0.3629\n",
            "Early stopping:  0.019229091610987445\n",
            "Epoch: 056, Loss: 1.9069, Train: 0.3705, Test: 0.3626\n",
            "Early stopping:  0.020372902893305896\n",
            "Epoch: 057, Loss: 1.8959, Train: 0.3730, Test: 0.3643\n",
            "Early stopping:  0.02007740301011214\n",
            "Epoch: 058, Loss: 1.8855, Train: 0.3741, Test: 0.3652\n",
            "Early stopping:  0.01877354867689053\n",
            "Epoch: 059, Loss: 1.8744, Train: 0.3735, Test: 0.3638\n",
            "Early stopping:  0.0176621437027938\n",
            "Epoch: 060, Loss: 1.8620, Train: 0.3752, Test: 0.3632\n",
            "Early stopping:  0.017584704200484563\n",
            "Epoch: 061, Loss: 1.8495, Train: 0.3807, Test: 0.3694\n",
            "Early stopping:  0.018390628060174507\n",
            "Epoch: 062, Loss: 1.8351, Train: 0.3870, Test: 0.3748\n",
            "Early stopping:  0.019922518638382476\n",
            "Epoch: 063, Loss: 1.8226, Train: 0.3887, Test: 0.3768\n",
            "Early stopping:  0.02067804273263016\n",
            "Epoch: 064, Loss: 1.8120, Train: 0.3924, Test: 0.3799\n",
            "Early stopping:  0.020120257359887195\n",
            "Epoch: 065, Loss: 1.8011, Train: 0.3932, Test: 0.3828\n",
            "Early stopping:  0.019008396674747324\n",
            "Epoch: 066, Loss: 1.7905, Train: 0.3955, Test: 0.3833\n",
            "Early stopping:  0.017509044426256327\n",
            "Epoch: 067, Loss: 1.7826, Train: 0.3953, Test: 0.3828\n",
            "Early stopping:  0.01607714220858838\n",
            "Epoch: 068, Loss: 1.7699, Train: 0.3951, Test: 0.3825\n",
            "Early stopping:  0.016240145159368342\n",
            "Epoch: 069, Loss: 1.7595, Train: 0.4049, Test: 0.3913\n",
            "Early stopping:  0.016423327232107675\n",
            "Epoch: 070, Loss: 1.7473, Train: 0.4138, Test: 0.3952\n",
            "Early stopping:  0.01735484827205353\n",
            "Epoch: 071, Loss: 1.7355, Train: 0.4187, Test: 0.3975\n",
            "Early stopping:  0.018461867056306463\n",
            "Epoch: 072, Loss: 1.7242, Train: 0.4187, Test: 0.3975\n",
            "Early stopping:  0.018253540708870693\n",
            "Epoch: 073, Loss: 1.7139, Train: 0.4187, Test: 0.4012\n",
            "Early stopping:  0.01806724769718389\n",
            "Epoch: 074, Loss: 1.7043, Train: 0.4276, Test: 0.4086\n",
            "Early stopping:  0.017019902519651808\n",
            "Epoch: 075, Loss: 1.6945, Train: 0.4340, Test: 0.4145\n",
            "Early stopping:  0.016124620362783348\n",
            "Epoch: 076, Loss: 1.6851, Train: 0.4403, Test: 0.4162\n",
            "Early stopping:  0.01544343424760118\n",
            "Epoch: 077, Loss: 1.6764, Train: 0.4388, Test: 0.4171\n",
            "Early stopping:  0.014898761345494869\n",
            "Epoch: 078, Loss: 1.6689, Train: 0.4440, Test: 0.4202\n",
            "Early stopping:  0.014080638712744281\n",
            "Epoch: 079, Loss: 1.6577, Train: 0.4448, Test: 0.4242\n",
            "Early stopping:  0.01422822788485669\n",
            "Epoch: 080, Loss: 1.6554, Train: 0.4422, Test: 0.4182\n",
            "Early stopping:  0.012502694165440597\n",
            "Epoch: 081, Loss: 1.6504, Train: 0.4446, Test: 0.4202\n",
            "Early stopping:  0.01061489814515586\n",
            "Epoch: 082, Loss: 1.6423, Train: 0.4546, Test: 0.4278\n",
            "Early stopping:  0.009778654576663093\n",
            "PREDICTIONS -> tensor([0, 0, 0,  ..., 6, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.32      0.50      0.39       379\n",
            "         capital_goods       0.12      0.02      0.03       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.45      0.24      0.31       396\n",
            " consumer_non-cyclical       0.64      0.33      0.43       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.61      0.56      0.58       384\n",
            "            healthcare       0.74      0.37      0.49       159\n",
            "              services       0.41      0.84      0.55      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.00      0.00      0.00       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.43      3527\n",
            "             macro avg       0.27      0.24      0.23      3527\n",
            "          weighted avg       0.35      0.43      0.35      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 77.7500, Train: 0.0612, Test: 0.0595\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 370.1201, Train: 0.1376, Test: 0.1350\n",
            "Early stopping:  206.73690866397664\n",
            "Epoch: 003, Loss: 446.3051, Train: 0.0773, Test: 0.0771\n",
            "Early stopping:  194.55819272456716\n",
            "Epoch: 004, Loss: 391.3170, Train: 0.1234, Test: 0.1245\n",
            "Early stopping:  165.55829681260116\n",
            "Epoch: 005, Loss: 449.1006, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  154.33738164150202\n",
            "Epoch: 006, Loss: 558.9160, Train: 0.2953, Test: 0.2954\n",
            "Early stopping:  73.25834707065675\n",
            "Epoch: 007, Loss: 526.7197, Train: 0.0444, Test: 0.0451\n",
            "Early stopping:  67.47673959469556\n",
            "Epoch: 008, Loss: 497.0157, Train: 0.0724, Test: 0.0720\n",
            "Early stopping:  65.97918082980661\n",
            "Epoch: 009, Loss: 369.0774, Train: 0.1223, Test: 0.1219\n",
            "Early stopping:  74.09194003914594\n",
            "Epoch: 010, Loss: 304.6462, Train: 0.1618, Test: 0.1582\n",
            "Early stopping:  109.11821878916933\n",
            "Epoch: 011, Loss: 253.2632, Train: 0.1947, Test: 0.1939\n",
            "Early stopping:  118.91649553705066\n",
            "Epoch: 012, Loss: 196.9227, Train: 0.2972, Test: 0.2940\n",
            "Early stopping:  115.64293946256186\n",
            "Epoch: 013, Loss: 155.9671, Train: 0.3102, Test: 0.3042\n",
            "Early stopping:  84.64211839695582\n",
            "Epoch: 014, Loss: 129.1153, Train: 0.3112, Test: 0.3090\n",
            "Early stopping:  71.48807502344243\n",
            "Epoch: 015, Loss: 95.0789, Train: 0.2208, Test: 0.2189\n",
            "Early stopping:  61.36366073078855\n",
            "Epoch: 016, Loss: 72.3502, Train: 0.2236, Test: 0.2172\n",
            "Early stopping:  49.23574644724783\n",
            "Epoch: 017, Loss: 44.9667, Train: 0.2505, Test: 0.2549\n",
            "Early stopping:  44.147866806344766\n",
            "Epoch: 018, Loss: 18.6559, Train: 0.1004, Test: 0.1035\n",
            "Early stopping:  42.928160461888595\n",
            "Epoch: 019, Loss: 13.1992, Train: 0.1108, Test: 0.1109\n",
            "Early stopping:  34.95396192941134\n",
            "Epoch: 020, Loss: 11.1194, Train: 0.1868, Test: 0.1868\n",
            "Early stopping:  26.28588854363274\n",
            "Epoch: 021, Loss: 8.0224, Train: 0.2321, Test: 0.2274\n",
            "Early stopping:  14.920297169977522\n",
            "Epoch: 022, Loss: 6.2119, Train: 0.2401, Test: 0.2348\n",
            "Early stopping:  4.854865647636015\n",
            "Epoch: 023, Loss: 4.9064, Train: 0.2427, Test: 0.2407\n",
            "Early stopping:  3.432813479770809\n",
            "Epoch: 024, Loss: 4.0888, Train: 0.2565, Test: 0.2433\n",
            "Early stopping:  2.8022090498054006\n",
            "Epoch: 025, Loss: 3.4830, Train: 0.2802, Test: 0.2685\n",
            "Early stopping:  1.8136416609748405\n",
            "Epoch: 026, Loss: 3.0373, Train: 0.3057, Test: 0.3017\n",
            "Early stopping:  1.2569147501075484\n",
            "Epoch: 027, Loss: 2.7760, Train: 0.3197, Test: 0.3122\n",
            "Early stopping:  0.8570221482958822\n",
            "Epoch: 028, Loss: 2.6481, Train: 0.3140, Test: 0.3071\n",
            "Early stopping:  0.5875062623321128\n",
            "Epoch: 029, Loss: 2.5601, Train: 0.3117, Test: 0.3065\n",
            "Early stopping:  0.3718524595730996\n",
            "Epoch: 030, Loss: 2.4801, Train: 0.3066, Test: 0.3079\n",
            "Early stopping:  0.21802383028095043\n",
            "Epoch: 031, Loss: 2.4034, Train: 0.3025, Test: 0.3082\n",
            "Early stopping:  0.14527364220276606\n",
            "Epoch: 032, Loss: 2.3384, Train: 0.3068, Test: 0.3105\n",
            "Early stopping:  0.12292205383774013\n",
            "Epoch: 033, Loss: 2.2913, Train: 0.3112, Test: 0.3195\n",
            "Early stopping:  0.10793422377940583\n",
            "Epoch: 034, Loss: 2.2653, Train: 0.3244, Test: 0.3246\n",
            "Early stopping:  0.08714115114424076\n",
            "Epoch: 035, Loss: 2.2577, Train: 0.3183, Test: 0.3184\n",
            "Early stopping:  0.06041324603782517\n",
            "Epoch: 036, Loss: 2.2599, Train: 0.3081, Test: 0.3085\n",
            "Early stopping:  0.033985243291369226\n",
            "Epoch: 037, Loss: 2.2638, Train: 0.2972, Test: 0.2988\n",
            "Early stopping:  0.01360384488019505\n",
            "Epoch: 038, Loss: 2.2676, Train: 0.3244, Test: 0.3229\n",
            "Early stopping:  0.004014331343407463\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 8, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       379\n",
            "         capital_goods       0.00      0.00      0.00       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.27      0.18      0.21       396\n",
            " consumer_non-cyclical       0.61      0.10      0.17       223\n",
            "                energy       1.00      0.02      0.04       141\n",
            "             financial       0.80      0.12      0.20       384\n",
            "            healthcare       0.48      0.25      0.32       159\n",
            "              services       0.32      0.92      0.48      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.00      0.00      0.00       202\n",
            "             utilities       0.03      0.02      0.02       113\n",
            "\n",
            "              accuracy                           0.32      3527\n",
            "             macro avg       0.29      0.13      0.12      3527\n",
            "          weighted avg       0.31      0.32      0.21      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 81.7017, Train: 0.2924, Test: 0.2920\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 422.3623, Train: 0.0639, Test: 0.0598\n",
            "Early stopping:  240.883435647037\n",
            "Epoch: 003, Loss: 283.6071, Train: 0.1363, Test: 0.1369\n",
            "Early stopping:  171.30307153701398\n",
            "Epoch: 004, Loss: 270.5439, Train: 0.0739, Test: 0.0740\n",
            "Early stopping:  139.92536978087784\n",
            "Epoch: 005, Loss: 238.4581, Train: 0.1473, Test: 0.1466\n",
            "Early stopping:  121.73959354743175\n",
            "Epoch: 006, Loss: 187.2885, Train: 0.1233, Test: 0.1191\n",
            "Early stopping:  87.59221758853604\n",
            "Epoch: 007, Loss: 147.6606, Train: 0.1433, Test: 0.1384\n",
            "Early stopping:  57.21096588800875\n",
            "Epoch: 008, Loss: 143.9400, Train: 0.2958, Test: 0.2935\n",
            "Early stopping:  55.83425828177942\n",
            "Epoch: 009, Loss: 110.0589, Train: 0.1021, Test: 0.0930\n",
            "Early stopping:  49.12842827769506\n",
            "Epoch: 010, Loss: 117.7638, Train: 0.2924, Test: 0.2929\n",
            "Early stopping:  30.38224084033603\n",
            "Epoch: 011, Loss: 88.3981, Train: 0.2881, Test: 0.2889\n",
            "Early stopping:  24.639924957275614\n",
            "Epoch: 012, Loss: 76.6022, Train: 0.3121, Test: 0.3071\n",
            "Early stopping:  26.264105474966094\n",
            "Epoch: 013, Loss: 60.1001, Train: 0.3159, Test: 0.3048\n",
            "Early stopping:  23.704178788214794\n",
            "Epoch: 014, Loss: 42.2437, Train: 0.2537, Test: 0.2421\n",
            "Early stopping:  28.659070747434427\n",
            "Epoch: 015, Loss: 30.3791, Train: 0.1473, Test: 0.1361\n",
            "Early stopping:  23.84190372835861\n",
            "Epoch: 016, Loss: 21.7466, Train: 0.1488, Test: 0.1401\n",
            "Early stopping:  22.256606464753908\n",
            "Epoch: 017, Loss: 10.7204, Train: 0.1745, Test: 0.1627\n",
            "Early stopping:  19.037711826075466\n",
            "Epoch: 018, Loss: 6.9808, Train: 0.2011, Test: 0.1837\n",
            "Early stopping:  14.422442748587212\n",
            "Epoch: 019, Loss: 5.5886, Train: 0.2134, Test: 0.2013\n",
            "Early stopping:  10.644198986286273\n",
            "Epoch: 020, Loss: 4.2963, Train: 0.2493, Test: 0.2297\n",
            "Early stopping:  7.062617124948167\n",
            "Epoch: 021, Loss: 3.2512, Train: 0.2762, Test: 0.2662\n",
            "Early stopping:  2.9039749819160616\n",
            "Epoch: 022, Loss: 2.6887, Train: 0.2960, Test: 0.2929\n",
            "Early stopping:  1.7466231697405032\n",
            "Epoch: 023, Loss: 2.4781, Train: 0.3021, Test: 0.2966\n",
            "Early stopping:  1.2876179535628298\n",
            "Epoch: 024, Loss: 2.4006, Train: 0.3008, Test: 0.2949\n",
            "Early stopping:  0.7856982155257568\n",
            "Epoch: 025, Loss: 2.3519, Train: 0.2960, Test: 0.2909\n",
            "Early stopping:  0.3681738559671955\n",
            "Epoch: 026, Loss: 2.2954, Train: 0.2957, Test: 0.2889\n",
            "Early stopping:  0.1528516161737897\n",
            "Epoch: 027, Loss: 2.2421, Train: 0.2979, Test: 0.2954\n",
            "Early stopping:  0.09156240518263828\n",
            "Epoch: 028, Loss: 2.2026, Train: 0.3060, Test: 0.3037\n",
            "Early stopping:  0.08011098100585655\n",
            "Epoch: 029, Loss: 2.1749, Train: 0.3096, Test: 0.3051\n",
            "Early stopping:  0.07131829796749557\n",
            "Epoch: 030, Loss: 2.1552, Train: 0.3146, Test: 0.3082\n",
            "Early stopping:  0.055977326780106744\n",
            "Epoch: 031, Loss: 2.1418, Train: 0.3181, Test: 0.3099\n",
            "Early stopping:  0.040030291377310866\n",
            "Epoch: 032, Loss: 2.1299, Train: 0.3219, Test: 0.3173\n",
            "Early stopping:  0.02868504399571958\n",
            "Epoch: 033, Loss: 2.1137, Train: 0.3319, Test: 0.3238\n",
            "Early stopping:  0.023430799550479664\n",
            "Epoch: 034, Loss: 2.0913, Train: 0.3376, Test: 0.3283\n",
            "Early stopping:  0.024863383739504304\n",
            "Epoch: 035, Loss: 2.0839, Train: 0.3423, Test: 0.3331\n",
            "Early stopping:  0.024643532814980124\n",
            "Epoch: 036, Loss: 2.0700, Train: 0.3520, Test: 0.3391\n",
            "Early stopping:  0.023944858688921933\n",
            "Epoch: 037, Loss: 2.0429, Train: 0.3554, Test: 0.3436\n",
            "Early stopping:  0.02625110088340605\n",
            "Epoch: 038, Loss: 2.0311, Train: 0.3599, Test: 0.3490\n",
            "Early stopping:  0.02601952519923061\n",
            "Epoch: 039, Loss: 2.0172, Train: 0.3631, Test: 0.3521\n",
            "Early stopping:  0.02751720398454636\n",
            "Epoch: 040, Loss: 1.9967, Train: 0.3688, Test: 0.3581\n",
            "Early stopping:  0.027509532353181026\n",
            "Epoch: 041, Loss: 1.9800, Train: 0.3713, Test: 0.3632\n",
            "Early stopping:  0.02547090496931345\n",
            "Epoch: 042, Loss: 1.9709, Train: 0.3730, Test: 0.3666\n",
            "Early stopping:  0.0250992705557482\n",
            "Epoch: 043, Loss: 1.9510, Train: 0.3794, Test: 0.3717\n",
            "Early stopping:  0.025182272485079565\n",
            "Epoch: 044, Loss: 1.9291, Train: 0.3809, Test: 0.3765\n",
            "Early stopping:  0.02618713813026153\n",
            "Epoch: 045, Loss: 1.9145, Train: 0.3892, Test: 0.3822\n",
            "Early stopping:  0.027499641778435866\n",
            "Epoch: 046, Loss: 1.8965, Train: 0.3896, Test: 0.3864\n",
            "Early stopping:  0.02934367721431694\n",
            "Epoch: 047, Loss: 1.8780, Train: 0.3917, Test: 0.3904\n",
            "Early stopping:  0.028281625083578763\n",
            "Epoch: 048, Loss: 1.8638, Train: 0.3972, Test: 0.3836\n",
            "Early stopping:  0.026456626442808977\n",
            "Epoch: 049, Loss: 1.8491, Train: 0.4002, Test: 0.3862\n",
            "Early stopping:  0.025885896360545405\n",
            "Epoch: 050, Loss: 1.8398, Train: 0.4060, Test: 0.3944\n",
            "Early stopping:  0.022623069690621302\n",
            "Epoch: 051, Loss: 1.8256, Train: 0.4066, Test: 0.3989\n",
            "Early stopping:  0.02040098041372289\n",
            "Epoch: 052, Loss: 1.8142, Train: 0.4095, Test: 0.4037\n",
            "Early stopping:  0.019424485043476748\n",
            "Epoch: 053, Loss: 1.8018, Train: 0.4132, Test: 0.4103\n",
            "Early stopping:  0.01904798604637867\n",
            "Epoch: 054, Loss: 1.7878, Train: 0.4159, Test: 0.4145\n",
            "Early stopping:  0.020232281820449033\n",
            "Epoch: 055, Loss: 1.7754, Train: 0.4214, Test: 0.4151\n",
            "Early stopping:  0.02008471323965696\n",
            "Epoch: 056, Loss: 1.7607, Train: 0.4219, Test: 0.4196\n",
            "Early stopping:  0.021127047598786766\n",
            "Epoch: 057, Loss: 1.7494, Train: 0.4272, Test: 0.4242\n",
            "Early stopping:  0.020866728131138605\n",
            "Epoch: 058, Loss: 1.7346, Train: 0.4323, Test: 0.4318\n",
            "Early stopping:  0.020932302830481333\n",
            "Epoch: 059, Loss: 1.7250, Train: 0.4384, Test: 0.4329\n",
            "Early stopping:  0.02009576016626263\n",
            "Epoch: 060, Loss: 1.7139, Train: 0.4454, Test: 0.4352\n",
            "Early stopping:  0.018696227301050077\n",
            "Epoch: 061, Loss: 1.6995, Train: 0.4505, Test: 0.4375\n",
            "Early stopping:  0.019115763589196887\n",
            "Epoch: 062, Loss: 1.6853, Train: 0.4592, Test: 0.4457\n",
            "Early stopping:  0.019714912505420075\n",
            "Epoch: 063, Loss: 1.6688, Train: 0.4618, Test: 0.4485\n",
            "Early stopping:  0.02233781538475325\n",
            "Epoch: 064, Loss: 1.6604, Train: 0.4664, Test: 0.4519\n",
            "Early stopping:  0.021860518126744975\n",
            "Epoch: 065, Loss: 1.6473, Train: 0.4698, Test: 0.4505\n",
            "Early stopping:  0.020542562676177108\n",
            "Epoch: 066, Loss: 1.6331, Train: 0.4735, Test: 0.4528\n",
            "Early stopping:  0.019959055160463448\n",
            "Epoch: 067, Loss: 1.6203, Train: 0.4769, Test: 0.4553\n",
            "Early stopping:  0.019726714353924195\n",
            "Epoch: 068, Loss: 1.6106, Train: 0.4775, Test: 0.4596\n",
            "Early stopping:  0.02006517724187283\n",
            "Epoch: 069, Loss: 1.5973, Train: 0.4824, Test: 0.4616\n",
            "Early stopping:  0.019405557558300254\n",
            "Epoch: 070, Loss: 1.5844, Train: 0.4809, Test: 0.4658\n",
            "Early stopping:  0.019080896789172713\n",
            "Epoch: 071, Loss: 1.5745, Train: 0.4864, Test: 0.4684\n",
            "Early stopping:  0.018644224348112075\n",
            "Epoch: 072, Loss: 1.5628, Train: 0.4888, Test: 0.4695\n",
            "Early stopping:  0.018726873631947003\n",
            "Epoch: 073, Loss: 1.5484, Train: 0.4941, Test: 0.4741\n",
            "Early stopping:  0.018894255257162623\n",
            "Epoch: 074, Loss: 1.5385, Train: 0.5021, Test: 0.4743\n",
            "Early stopping:  0.01866407070350563\n",
            "Epoch: 075, Loss: 1.5262, Train: 0.5045, Test: 0.4769\n",
            "Early stopping:  0.019153694407279605\n",
            "Epoch: 076, Loss: 1.5135, Train: 0.5079, Test: 0.4814\n",
            "Early stopping:  0.0191305893967607\n",
            "Epoch: 077, Loss: 1.5022, Train: 0.5095, Test: 0.4851\n",
            "Early stopping:  0.01858467076743374\n",
            "Epoch: 078, Loss: 1.4918, Train: 0.5112, Test: 0.4911\n",
            "Early stopping:  0.01859473699109744\n",
            "Epoch: 079, Loss: 1.4786, Train: 0.5157, Test: 0.4902\n",
            "Early stopping:  0.01850387557805612\n",
            "Epoch: 080, Loss: 1.4677, Train: 0.5161, Test: 0.4902\n",
            "Early stopping:  0.018233685378216395\n",
            "Epoch: 081, Loss: 1.4567, Train: 0.5214, Test: 0.4945\n",
            "Early stopping:  0.01820453170737444\n",
            "Epoch: 082, Loss: 1.4474, Train: 0.5270, Test: 0.5001\n",
            "Early stopping:  0.017533666905331098\n",
            "Epoch: 083, Loss: 1.4433, Train: 0.5319, Test: 0.4990\n",
            "Early stopping:  0.014548224690781542\n",
            "Epoch: 084, Loss: 1.4256, Train: 0.5261, Test: 0.4967\n",
            "Early stopping:  0.015706595423738645\n",
            "Epoch: 085, Loss: 1.4334, Train: 0.5336, Test: 0.5021\n",
            "Early stopping:  0.012093629007825799\n",
            "Epoch: 086, Loss: 1.4212, Train: 0.5355, Test: 0.4982\n",
            "Early stopping:  0.011185465857138205\n",
            "Epoch: 087, Loss: 1.4190, Train: 0.5416, Test: 0.5075\n",
            "Early stopping:  0.009938566694054515\n",
            "PREDICTIONS -> tensor([ 9,  6,  3,  ..., 11,  8,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.49      0.31      0.38       379\n",
            "         capital_goods       0.45      0.15      0.22       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.40      0.44      0.42       396\n",
            " consumer_non-cyclical       0.78      0.33      0.47       223\n",
            "                energy       0.47      0.48      0.48       141\n",
            "             financial       0.63      0.60      0.61       384\n",
            "            healthcare       0.62      0.50      0.56       159\n",
            "              services       0.48      0.80      0.60      1038\n",
            "            technology       0.33      0.18      0.24       198\n",
            "        transportation       0.78      0.45      0.57       202\n",
            "             utilities       0.55      0.50      0.53       113\n",
            "\n",
            "              accuracy                           0.51      3527\n",
            "             macro avg       0.50      0.40      0.42      3527\n",
            "          weighted avg       0.52      0.51      0.48      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 64.7002, Train: 0.1380, Test: 0.1355\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 567.5276, Train: 0.0620, Test: 0.0627\n",
            "Early stopping:  355.55268603232554\n",
            "Epoch: 003, Loss: 592.7505, Train: 0.1085, Test: 0.1089\n",
            "Early stopping:  297.8559046259859\n",
            "Epoch: 004, Loss: 467.8149, Train: 0.0992, Test: 0.0953\n",
            "Early stopping:  245.01052473816114\n",
            "Epoch: 005, Loss: 253.7062, Train: 0.0503, Test: 0.0488\n",
            "Early stopping:  225.31784756207807\n",
            "Epoch: 006, Loss: 164.3209, Train: 0.1282, Test: 0.1228\n",
            "Early stopping:  191.27089897737474\n",
            "Epoch: 007, Loss: 55.5768, Train: 0.0991, Test: 0.0995\n",
            "Early stopping:  220.18506795472123\n",
            "Epoch: 008, Loss: 31.6712, Train: 0.0715, Test: 0.0683\n",
            "Early stopping:  176.7272019967706\n",
            "Epoch: 009, Loss: 11.3979, Train: 0.0911, Test: 0.0910\n",
            "Early stopping:  102.71289708561112\n",
            "Epoch: 010, Loss: 3.9708, Train: 0.1161, Test: 0.1225\n",
            "Early stopping:  65.16853213388525\n",
            "Epoch: 011, Loss: 3.6005, Train: 0.1240, Test: 0.1284\n",
            "Early stopping:  22.327505034309485\n",
            "Epoch: 012, Loss: 3.2919, Train: 0.1382, Test: 0.1395\n",
            "Early stopping:  12.153247089816956\n",
            "Epoch: 013, Loss: 3.0507, Train: 0.1501, Test: 0.1520\n",
            "Early stopping:  3.5584104957804525\n",
            "Epoch: 014, Loss: 2.8212, Train: 0.3181, Test: 0.3207\n",
            "Early stopping:  0.45296603345523706\n",
            "Epoch: 015, Loss: 2.6305, Train: 0.3142, Test: 0.3127\n",
            "Early stopping:  0.3826512779529813\n",
            "Epoch: 016, Loss: 2.4929, Train: 0.3060, Test: 0.3000\n",
            "Early stopping:  0.3208416751900448\n",
            "Epoch: 017, Loss: 2.3863, Train: 0.2966, Test: 0.2906\n",
            "Early stopping:  0.265027189728501\n",
            "Epoch: 018, Loss: 2.3170, Train: 0.2883, Test: 0.2858\n",
            "Early stopping:  0.20140929738599828\n",
            "Epoch: 019, Loss: 2.2955, Train: 0.3134, Test: 0.3150\n",
            "Early stopping:  0.13853786105379656\n",
            "Epoch: 020, Loss: 2.3019, Train: 0.3312, Test: 0.3272\n",
            "Early stopping:  0.08325667913348196\n",
            "Epoch: 021, Loss: 2.3153, Train: 0.3348, Test: 0.3300\n",
            "Early stopping:  0.036416588187261165\n",
            "Epoch: 022, Loss: 2.3228, Train: 0.3367, Test: 0.3300\n",
            "Early stopping:  0.011344116505895938\n",
            "Epoch: 023, Loss: 2.3232, Train: 0.3367, Test: 0.3323\n",
            "Early stopping:  0.012505539386727823\n",
            "Epoch: 024, Loss: 2.3185, Train: 0.3378, Test: 0.3314\n",
            "Early stopping:  0.008703446370904455\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.02      0.04       379\n",
            "         capital_goods       0.00      0.00      0.00       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.00      0.00      0.00       396\n",
            " consumer_non-cyclical       1.00      0.02      0.04       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.50      0.46      0.48       384\n",
            "            healthcare       0.68      0.19      0.30       159\n",
            "              services       0.31      0.90      0.46      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.33      0.05      0.09       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.33      3527\n",
            "             macro avg       0.27      0.14      0.12      3527\n",
            "          weighted avg       0.30      0.33      0.21      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 66.9890, Train: 0.2930, Test: 0.2926\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 386.9687, Train: 0.0461, Test: 0.0462\n",
            "Early stopping:  226.25983600510403\n",
            "Epoch: 003, Loss: 531.6615, Train: 0.0972, Test: 0.0904\n",
            "Early stopping:  237.78266237022518\n",
            "Epoch: 004, Loss: 512.0292, Train: 0.1401, Test: 0.1341\n",
            "Early stopping:  214.7343213857863\n",
            "Epoch: 005, Loss: 381.9022, Train: 0.0688, Test: 0.0649\n",
            "Early stopping:  185.99554278668478\n",
            "Epoch: 006, Loss: 352.5399, Train: 0.0964, Test: 0.0879\n",
            "Early stopping:  82.43695463808655\n",
            "Epoch: 007, Loss: 302.7993, Train: 0.1189, Test: 0.1168\n",
            "Early stopping:  100.75126035675834\n",
            "Epoch: 008, Loss: 268.7824, Train: 0.0510, Test: 0.0499\n",
            "Early stopping:  93.77583426449996\n",
            "Epoch: 009, Loss: 212.3766, Train: 0.1393, Test: 0.1403\n",
            "Early stopping:  67.19547978537976\n",
            "Epoch: 010, Loss: 130.7694, Train: 0.3132, Test: 0.3136\n",
            "Early stopping:  85.47924031891682\n",
            "Epoch: 011, Loss: 128.4002, Train: 0.3236, Test: 0.3181\n",
            "Early stopping:  79.05585653452563\n",
            "Epoch: 012, Loss: 117.0118, Train: 0.3127, Test: 0.3045\n",
            "Early stopping:  66.37075677239503\n",
            "Epoch: 013, Loss: 100.9597, Train: 0.2832, Test: 0.2773\n",
            "Early stopping:  43.26880926488833\n",
            "Epoch: 014, Loss: 81.5423, Train: 0.2936, Test: 0.2917\n",
            "Early stopping:  20.588758545689085\n",
            "Epoch: 015, Loss: 54.8435, Train: 0.3325, Test: 0.3193\n",
            "Early stopping:  29.228541434868912\n",
            "Epoch: 016, Loss: 28.2736, Train: 0.3248, Test: 0.3178\n",
            "Early stopping:  35.565745231144405\n",
            "Epoch: 017, Loss: 12.0832, Train: 0.3427, Test: 0.3428\n",
            "Early stopping:  36.64526080842692\n",
            "Epoch: 018, Loss: 5.4743, Train: 0.3384, Test: 0.3377\n",
            "Early stopping:  31.590024318721458\n",
            "Epoch: 019, Loss: 3.7373, Train: 0.2943, Test: 0.2872\n",
            "Early stopping:  21.31180479002159\n",
            "Epoch: 020, Loss: 3.3324, Train: 0.2578, Test: 0.2512\n",
            "Early stopping:  10.496926949863191\n",
            "Epoch: 021, Loss: 3.2127, Train: 0.2529, Test: 0.2458\n",
            "Early stopping:  3.753440545833209\n",
            "Epoch: 022, Loss: 3.0242, Train: 0.2573, Test: 0.2464\n",
            "Early stopping:  0.9953976838058616\n",
            "Epoch: 023, Loss: 2.8733, Train: 0.2550, Test: 0.2461\n",
            "Early stopping:  0.330755818211896\n",
            "Epoch: 024, Loss: 2.7423, Train: 0.2401, Test: 0.2382\n",
            "Early stopping:  0.2408087991328207\n",
            "Epoch: 025, Loss: 2.6208, Train: 0.2280, Test: 0.2274\n",
            "Early stopping:  0.23270535451752164\n",
            "Epoch: 026, Loss: 2.5018, Train: 0.2259, Test: 0.2231\n",
            "Early stopping:  0.2053696756297934\n",
            "Epoch: 027, Loss: 2.4027, Train: 0.2212, Test: 0.2152\n",
            "Early stopping:  0.1870668921491774\n",
            "Epoch: 028, Loss: 2.3387, Train: 0.2151, Test: 0.2064\n",
            "Early stopping:  0.1631874420008674\n",
            "Epoch: 029, Loss: 2.3239, Train: 0.2130, Test: 0.2002\n",
            "Early stopping:  0.12410477022939762\n",
            "Epoch: 030, Loss: 2.3294, Train: 0.2153, Test: 0.2024\n",
            "Early stopping:  0.07542966437074018\n",
            "Epoch: 031, Loss: 2.3345, Train: 0.2123, Test: 0.2013\n",
            "Early stopping:  0.032274636232006125\n",
            "Epoch: 032, Loss: 2.3371, Train: 0.2136, Test: 0.2058\n",
            "Early stopping:  0.006064572096422664\n",
            "PREDICTIONS -> tensor([ 1,  6,  3,  ..., 11, 11, 11], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       379\n",
            "         capital_goods       0.19      0.24      0.21       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.17      0.59      0.26       396\n",
            " consumer_non-cyclical       0.64      0.12      0.20       223\n",
            "                energy       0.67      0.03      0.05       141\n",
            "             financial       0.38      0.40      0.39       384\n",
            "            healthcare       0.39      0.31      0.35       159\n",
            "              services       0.71      0.01      0.02      1038\n",
            "            technology       0.05      0.02      0.02       198\n",
            "        transportation       0.61      0.52      0.56       202\n",
            "             utilities       0.08      0.71      0.14       113\n",
            "\n",
            "              accuracy                           0.21      3527\n",
            "             macro avg       0.32      0.25      0.18      3527\n",
            "          weighted avg       0.41      0.21      0.16      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 80.4751, Train: 0.1121, Test: 0.1117\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 263.5161, Train: 0.1163, Test: 0.1154\n",
            "Early stopping:  129.42951101493065\n",
            "Epoch: 003, Loss: 324.1829, Train: 0.2066, Test: 0.2141\n",
            "Early stopping:  126.87128640124548\n",
            "Epoch: 004, Loss: 342.9884, Train: 0.2949, Test: 0.2926\n",
            "Early stopping:  119.77781045734537\n",
            "Epoch: 005, Loss: 386.2307, Train: 0.1074, Test: 0.1038\n",
            "Early stopping:  119.67160914210012\n",
            "Epoch: 006, Loss: 497.4266, Train: 0.0410, Test: 0.0417\n",
            "Early stopping:  87.19858483853321\n",
            "Epoch: 007, Loss: 535.9965, Train: 0.0185, Test: 0.0190\n",
            "Early stopping:  94.42911144560938\n",
            "Epoch: 008, Loss: 518.7327, Train: 0.0730, Test: 0.0692\n",
            "Early stopping:  86.15392108605006\n",
            "Epoch: 009, Loss: 419.5856, Train: 0.1000, Test: 0.0978\n",
            "Early stopping:  65.24727343747078\n",
            "Epoch: 010, Loss: 343.5058, Train: 0.2994, Test: 0.2940\n",
            "Early stopping:  80.28616087949459\n",
            "Epoch: 011, Loss: 315.1766, Train: 0.2998, Test: 0.2997\n",
            "Early stopping:  99.78106979577255\n",
            "Epoch: 012, Loss: 316.4116, Train: 0.3030, Test: 0.3025\n",
            "Early stopping:  87.11369885287714\n",
            "Epoch: 013, Loss: 281.0244, Train: 0.3274, Test: 0.3173\n",
            "Early stopping:  52.15083803794513\n",
            "Epoch: 014, Loss: 223.9497, Train: 0.1732, Test: 0.1678\n",
            "Early stopping:  45.98011095405589\n",
            "Epoch: 015, Loss: 180.8114, Train: 0.1626, Test: 0.1593\n",
            "Early stopping:  59.53744446278836\n",
            "Epoch: 016, Loss: 130.0226, Train: 0.1274, Test: 0.1239\n",
            "Early stopping:  74.93132516624965\n",
            "Epoch: 017, Loss: 84.8939, Train: 0.2270, Test: 0.2299\n",
            "Early stopping:  76.94313020627938\n",
            "Epoch: 018, Loss: 38.7793, Train: 0.2219, Test: 0.2183\n",
            "Early stopping:  73.7391304724142\n",
            "Epoch: 019, Loss: 19.0765, Train: 0.1172, Test: 0.1072\n",
            "Early stopping:  66.19241543431508\n",
            "Epoch: 020, Loss: 14.6824, Train: 0.1318, Test: 0.1270\n",
            "Early stopping:  49.168423265672175\n",
            "Epoch: 021, Loss: 7.0549, Train: 0.1866, Test: 0.1846\n",
            "Early stopping:  31.340333806303697\n",
            "Epoch: 022, Loss: 4.5761, Train: 0.2098, Test: 0.2104\n",
            "Early stopping:  13.575144602154309\n",
            "Epoch: 023, Loss: 3.7530, Train: 0.2240, Test: 0.2206\n",
            "Early stopping:  6.731971273397924\n",
            "Epoch: 024, Loss: 3.1736, Train: 0.2397, Test: 0.2325\n",
            "Early stopping:  4.729253491706813\n",
            "Epoch: 025, Loss: 2.7797, Train: 0.2507, Test: 0.2413\n",
            "Early stopping:  1.6985384620176551\n",
            "Epoch: 026, Loss: 2.5298, Train: 0.2488, Test: 0.2387\n",
            "Early stopping:  0.8206785200747673\n",
            "Epoch: 027, Loss: 2.3818, Train: 0.2422, Test: 0.2302\n",
            "Early stopping:  0.5521934343370393\n",
            "Epoch: 028, Loss: 2.2917, Train: 0.2361, Test: 0.2271\n",
            "Early stopping:  0.3549388057598476\n",
            "Epoch: 029, Loss: 2.2461, Train: 0.2384, Test: 0.2265\n",
            "Early stopping:  0.21578031431246095\n",
            "Epoch: 030, Loss: 2.2263, Train: 0.2382, Test: 0.2308\n",
            "Early stopping:  0.12424231522503387\n",
            "Epoch: 031, Loss: 2.2183, Train: 0.2363, Test: 0.2319\n",
            "Early stopping:  0.06723845166145484\n",
            "Epoch: 032, Loss: 2.2212, Train: 0.2280, Test: 0.2220\n",
            "Early stopping:  0.03047917688385512\n",
            "Epoch: 033, Loss: 2.2257, Train: 0.2240, Test: 0.2203\n",
            "Early stopping:  0.010897947805707773\n",
            "Epoch: 034, Loss: 2.2313, Train: 0.2208, Test: 0.2203\n",
            "Early stopping:  0.00500530442377917\n",
            "PREDICTIONS -> tensor([10,  8,  1,  ...,  8,  6,  5], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       379\n",
            "         capital_goods       0.19      0.11      0.14       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.13      0.01      0.02       396\n",
            " consumer_non-cyclical       0.18      0.23      0.20       223\n",
            "                energy       0.04      0.13      0.06       141\n",
            "             financial       0.15      0.66      0.24       384\n",
            "            healthcare       0.72      0.24      0.36       159\n",
            "              services       0.50      0.31      0.38      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.44      0.29      0.35       202\n",
            "             utilities       0.04      0.01      0.01       113\n",
            "\n",
            "              accuracy                           0.22      3527\n",
            "             macro avg       0.20      0.17      0.15      3527\n",
            "          weighted avg       0.26      0.22      0.20      3527\n",
            "\n",
            "time: 1min 31s (started: 2024-10-16 21:02:11 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwLt_Fs8HtEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4adb33f6-762c-4948-f4f9-35e4383b1ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 417 ms (started: 2024-10-16 21:03:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86xKwHiHtEx"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D6w1i_NHtEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe70c55-56bd-421e-ac0d-c07014ea5142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4839, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2826, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  0.14234845203463395\n",
            "Epoch: 003, Loss: 2.1696, Train: 0.2947, Test: 0.2943\n",
            "Early stopping:  0.15924659944152372\n",
            "Epoch: 004, Loss: 2.1468, Train: 0.3034, Test: 0.3022\n",
            "Early stopping:  0.15404092501684624\n",
            "Epoch: 005, Loss: 2.0768, Train: 0.3406, Test: 0.3377\n",
            "Early stopping:  0.15911985379341978\n",
            "Epoch: 006, Loss: 2.0139, Train: 0.3675, Test: 0.3632\n",
            "Early stopping:  0.10142767049267519\n",
            "Epoch: 007, Loss: 1.9600, Train: 0.3733, Test: 0.3680\n",
            "Early stopping:  0.08811231978173045\n",
            "Epoch: 008, Loss: 1.9028, Train: 0.3749, Test: 0.3689\n",
            "Early stopping:  0.09578522282733015\n",
            "Epoch: 009, Loss: 1.8453, Train: 0.3856, Test: 0.3828\n",
            "Early stopping:  0.0907882696213658\n",
            "Epoch: 010, Loss: 1.7892, Train: 0.4142, Test: 0.4086\n",
            "Early stopping:  0.08916965662252974\n",
            "Epoch: 011, Loss: 1.7295, Train: 0.4488, Test: 0.4409\n",
            "Early stopping:  0.09085070056199453\n",
            "Epoch: 012, Loss: 1.6691, Train: 0.4730, Test: 0.4585\n",
            "Early stopping:  0.09222736292962762\n",
            "Epoch: 013, Loss: 1.6153, Train: 0.4849, Test: 0.4704\n",
            "Early stopping:  0.09175043589545119\n",
            "Epoch: 014, Loss: 1.5625, Train: 0.4981, Test: 0.4854\n",
            "Early stopping:  0.08978837968052714\n",
            "Epoch: 015, Loss: 1.5074, Train: 0.5212, Test: 0.5030\n",
            "Early stopping:  0.08709595837786825\n",
            "Epoch: 016, Loss: 1.4576, Train: 0.5414, Test: 0.5231\n",
            "Early stopping:  0.08395047940235116\n",
            "Epoch: 017, Loss: 1.4128, Train: 0.5635, Test: 0.5370\n",
            "Early stopping:  0.08070676373768969\n",
            "Epoch: 018, Loss: 1.3691, Train: 0.5805, Test: 0.5571\n",
            "Early stopping:  0.07623970928515823\n",
            "Epoch: 019, Loss: 1.3284, Train: 0.5851, Test: 0.5693\n",
            "Early stopping:  0.070652110350713\n",
            "Epoch: 020, Loss: 1.2910, Train: 0.5947, Test: 0.5727\n",
            "Early stopping:  0.06604612552651747\n",
            "Epoch: 021, Loss: 1.2551, Train: 0.6059, Test: 0.5815\n",
            "Early stopping:  0.06226240759160168\n",
            "Epoch: 022, Loss: 1.2224, Train: 0.6193, Test: 0.5903\n",
            "Early stopping:  0.05801225578635058\n",
            "Epoch: 023, Loss: 1.1918, Train: 0.6295, Test: 0.5999\n",
            "Early stopping:  0.054079770322587074\n",
            "Epoch: 024, Loss: 1.1622, Train: 0.6348, Test: 0.6062\n",
            "Early stopping:  0.050766044304806315\n",
            "Epoch: 025, Loss: 1.1356, Train: 0.6414, Test: 0.6138\n",
            "Early stopping:  0.04734021509760719\n",
            "Epoch: 026, Loss: 1.1093, Train: 0.6488, Test: 0.6215\n",
            "Early stopping:  0.04471198563880871\n",
            "Epoch: 027, Loss: 1.0851, Train: 0.6569, Test: 0.6252\n",
            "Early stopping:  0.04215805637436786\n",
            "Epoch: 028, Loss: 1.0617, Train: 0.6664, Test: 0.6286\n",
            "Early stopping:  0.039788041167785546\n",
            "Epoch: 029, Loss: 1.0393, Train: 0.6749, Test: 0.6337\n",
            "Early stopping:  0.0379824731966483\n",
            "Epoch: 030, Loss: 1.0173, Train: 0.6828, Test: 0.6445\n",
            "Early stopping:  0.03633034780480817\n",
            "Epoch: 031, Loss: 0.9951, Train: 0.6879, Test: 0.6498\n",
            "Early stopping:  0.03546670049575564\n",
            "Epoch: 032, Loss: 0.9744, Train: 0.6981, Test: 0.6547\n",
            "Early stopping:  0.03457647029372786\n",
            "Epoch: 033, Loss: 0.9531, Train: 0.7093, Test: 0.6569\n",
            "Early stopping:  0.034027997929075376\n",
            "Epoch: 034, Loss: 0.9329, Train: 0.7174, Test: 0.6629\n",
            "Early stopping:  0.03333523156349393\n",
            "Epoch: 035, Loss: 0.9138, Train: 0.7263, Test: 0.6654\n",
            "Early stopping:  0.03227848837004795\n",
            "Epoch: 036, Loss: 0.8959, Train: 0.7352, Test: 0.6674\n",
            "Early stopping:  0.03105996340141022\n",
            "Epoch: 037, Loss: 0.8774, Train: 0.7427, Test: 0.6714\n",
            "Early stopping:  0.029814977035746825\n",
            "Epoch: 038, Loss: 0.8604, Train: 0.7491, Test: 0.6759\n",
            "Early stopping:  0.028699706325379118\n",
            "Epoch: 039, Loss: 0.8438, Train: 0.7563, Test: 0.6782\n",
            "Early stopping:  0.02776794910301338\n",
            "Epoch: 040, Loss: 0.8272, Train: 0.7656, Test: 0.6790\n",
            "Early stopping:  0.027032496446600083\n",
            "Epoch: 041, Loss: 0.8106, Train: 0.7711, Test: 0.6790\n",
            "Early stopping:  0.026359831269071442\n",
            "Epoch: 042, Loss: 0.7948, Train: 0.7775, Test: 0.6842\n",
            "Early stopping:  0.025986347299985068\n",
            "Epoch: 043, Loss: 0.7784, Train: 0.7805, Test: 0.6887\n",
            "Early stopping:  0.02580824575162012\n",
            "Epoch: 044, Loss: 0.7622, Train: 0.7881, Test: 0.6910\n",
            "Early stopping:  0.025643261784273737\n",
            "Epoch: 045, Loss: 0.7465, Train: 0.7921, Test: 0.6955\n",
            "Early stopping:  0.025433551018514876\n",
            "Epoch: 046, Loss: 0.7305, Train: 0.7994, Test: 0.6986\n",
            "Early stopping:  0.025386471821448035\n",
            "Epoch: 047, Loss: 0.7152, Train: 0.8034, Test: 0.7020\n",
            "Early stopping:  0.02499628591434604\n",
            "Epoch: 048, Loss: 0.6999, Train: 0.8089, Test: 0.7020\n",
            "Early stopping:  0.024662534432672842\n",
            "Epoch: 049, Loss: 0.6846, Train: 0.8125, Test: 0.7105\n",
            "Early stopping:  0.02442558521298975\n",
            "Epoch: 050, Loss: 0.6703, Train: 0.8176, Test: 0.7060\n",
            "Early stopping:  0.023902273887114654\n",
            "Epoch: 051, Loss: 0.6577, Train: 0.8134, Test: 0.7125\n",
            "Early stopping:  0.022898760723578503\n",
            "Epoch: 052, Loss: 0.6482, Train: 0.8212, Test: 0.7057\n",
            "Early stopping:  0.020675014078715272\n",
            "Epoch: 053, Loss: 0.6352, Train: 0.8293, Test: 0.7170\n",
            "Early stopping:  0.01914765984862893\n",
            "Epoch: 054, Loss: 0.6151, Train: 0.8325, Test: 0.7196\n",
            "Early stopping:  0.021215917234099286\n",
            "Epoch: 055, Loss: 0.6026, Train: 0.8376, Test: 0.7148\n",
            "Early stopping:  0.022805920493382384\n",
            "Epoch: 056, Loss: 0.5952, Train: 0.8376, Test: 0.7244\n",
            "Early stopping:  0.022153032168401088\n",
            "Epoch: 057, Loss: 0.5792, Train: 0.8459, Test: 0.7272\n",
            "Early stopping:  0.02108189029290079\n",
            "Epoch: 058, Loss: 0.5646, Train: 0.8510, Test: 0.7272\n",
            "Early stopping:  0.019813526940007736\n",
            "Epoch: 059, Loss: 0.5569, Train: 0.8482, Test: 0.7326\n",
            "Early stopping:  0.019457442717572056\n",
            "Epoch: 060, Loss: 0.5442, Train: 0.8571, Test: 0.7352\n",
            "Early stopping:  0.019785365554371916\n",
            "Epoch: 061, Loss: 0.5297, Train: 0.8658, Test: 0.7349\n",
            "Early stopping:  0.01894587658670796\n",
            "Epoch: 062, Loss: 0.5207, Train: 0.8605, Test: 0.7352\n",
            "Early stopping:  0.01826870075159423\n",
            "Epoch: 063, Loss: 0.5108, Train: 0.8724, Test: 0.7389\n",
            "Early stopping:  0.018378861823784728\n",
            "Epoch: 064, Loss: 0.4975, Train: 0.8760, Test: 0.7411\n",
            "Early stopping:  0.017831848502023333\n",
            "Epoch: 065, Loss: 0.4864, Train: 0.8760, Test: 0.7420\n",
            "Early stopping:  0.01740528831298864\n",
            "Epoch: 066, Loss: 0.4779, Train: 0.8851, Test: 0.7437\n",
            "Early stopping:  0.01741816362695551\n",
            "Epoch: 067, Loss: 0.4678, Train: 0.8854, Test: 0.7409\n",
            "Early stopping:  0.016754549387255003\n",
            "Epoch: 068, Loss: 0.4561, Train: 0.8904, Test: 0.7426\n",
            "Early stopping:  0.016047889689272243\n",
            "Epoch: 069, Loss: 0.4458, Train: 0.8957, Test: 0.7434\n",
            "Early stopping:  0.01630655690982338\n",
            "Epoch: 070, Loss: 0.4377, Train: 0.8938, Test: 0.7414\n",
            "Early stopping:  0.016235661980837625\n",
            "Epoch: 071, Loss: 0.4293, Train: 0.9034, Test: 0.7434\n",
            "Early stopping:  0.015135190377496482\n",
            "Epoch: 072, Loss: 0.4188, Train: 0.9028, Test: 0.7437\n",
            "Early stopping:  0.014422880077410747\n",
            "Epoch: 073, Loss: 0.4081, Train: 0.9062, Test: 0.7409\n",
            "Early stopping:  0.014931109702118356\n",
            "Epoch: 074, Loss: 0.3997, Train: 0.9149, Test: 0.7471\n",
            "Early stopping:  0.015357264632527382\n",
            "Epoch: 075, Loss: 0.3937, Train: 0.9030, Test: 0.7409\n",
            "Early stopping:  0.014366321965369236\n",
            "Epoch: 076, Loss: 0.3913, Train: 0.9091, Test: 0.7386\n",
            "Early stopping:  0.011284459756960328\n",
            "Epoch: 077, Loss: 0.3931, Train: 0.9060, Test: 0.7431\n",
            "Early stopping:  0.006899192895027175\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.84      0.71      0.77       379\n",
            "         capital_goods       0.71      0.67      0.69       254\n",
            "conglomerates_industry       1.00      0.57      0.73        40\n",
            "     consumer_cyclical       0.64      0.78      0.70       396\n",
            " consumer_non-cyclical       0.72      0.61      0.66       223\n",
            "                energy       0.86      0.77      0.81       141\n",
            "             financial       0.88      0.68      0.77       384\n",
            "            healthcare       0.84      0.70      0.76       159\n",
            "              services       0.68      0.85      0.76      1038\n",
            "            technology       0.81      0.53      0.64       198\n",
            "        transportation       0.84      0.80      0.82       202\n",
            "             utilities       0.88      0.73      0.80       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.81      0.70      0.74      3527\n",
            "          weighted avg       0.76      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4867, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2664, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.15577171752317157\n",
            "Epoch: 003, Loss: 2.1618, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.16584017158429196\n",
            "Epoch: 004, Loss: 2.1525, Train: 0.2953, Test: 0.2943\n",
            "Early stopping:  0.15538013088965577\n",
            "Epoch: 005, Loss: 2.0912, Train: 0.3129, Test: 0.3090\n",
            "Early stopping:  0.15580329307356386\n",
            "Epoch: 006, Loss: 2.0248, Train: 0.3546, Test: 0.3544\n",
            "Early stopping:  0.08977743620350835\n",
            "Epoch: 007, Loss: 1.9744, Train: 0.3849, Test: 0.3813\n",
            "Early stopping:  0.08101052797027428\n",
            "Epoch: 008, Loss: 1.9251, Train: 0.3991, Test: 0.3921\n",
            "Early stopping:  0.09055952188457705\n",
            "Epoch: 009, Loss: 1.8712, Train: 0.4070, Test: 0.4009\n",
            "Early stopping:  0.08548472066903837\n",
            "Epoch: 010, Loss: 1.8152, Train: 0.4193, Test: 0.4131\n",
            "Early stopping:  0.08264718928650004\n",
            "Epoch: 011, Loss: 1.7585, Train: 0.4386, Test: 0.4259\n",
            "Early stopping:  0.0856844019292121\n",
            "Epoch: 012, Loss: 1.7036, Train: 0.4488, Test: 0.4366\n",
            "Early stopping:  0.08786923661351853\n",
            "Epoch: 013, Loss: 1.6520, Train: 0.4597, Test: 0.4474\n",
            "Early stopping:  0.08695762879083532\n",
            "Epoch: 014, Loss: 1.5988, Train: 0.4747, Test: 0.4582\n",
            "Early stopping:  0.08529535635084552\n",
            "Epoch: 015, Loss: 1.5447, Train: 0.5023, Test: 0.4826\n",
            "Early stopping:  0.08420124895296631\n",
            "Epoch: 016, Loss: 1.4963, Train: 0.5268, Test: 0.5052\n",
            "Early stopping:  0.082547312458433\n",
            "Epoch: 017, Loss: 1.4496, Train: 0.5527, Test: 0.5234\n",
            "Early stopping:  0.08027306950086545\n",
            "Epoch: 018, Loss: 1.4040, Train: 0.5690, Test: 0.5390\n",
            "Early stopping:  0.07666371480884684\n",
            "Epoch: 019, Loss: 1.3619, Train: 0.5726, Test: 0.5492\n",
            "Early stopping:  0.07242408412254676\n",
            "Epoch: 020, Loss: 1.3224, Train: 0.5851, Test: 0.5611\n",
            "Early stopping:  0.0688884759772372\n",
            "Epoch: 021, Loss: 1.2858, Train: 0.5907, Test: 0.5736\n",
            "Early stopping:  0.0647521326036311\n",
            "Epoch: 022, Loss: 1.2493, Train: 0.6045, Test: 0.5843\n",
            "Early stopping:  0.06096773179899978\n",
            "Epoch: 023, Loss: 1.2163, Train: 0.6140, Test: 0.5909\n",
            "Early stopping:  0.05761227800882698\n",
            "Epoch: 024, Loss: 1.1868, Train: 0.6255, Test: 0.6022\n",
            "Early stopping:  0.05394281427154902\n",
            "Epoch: 025, Loss: 1.1591, Train: 0.6340, Test: 0.6082\n",
            "Early stopping:  0.05004700347159596\n",
            "Epoch: 026, Loss: 1.1321, Train: 0.6406, Test: 0.6136\n",
            "Early stopping:  0.04616513686114598\n",
            "Epoch: 027, Loss: 1.1068, Train: 0.6482, Test: 0.6184\n",
            "Early stopping:  0.04327506103967768\n",
            "Epoch: 028, Loss: 1.0816, Train: 0.6597, Test: 0.6255\n",
            "Early stopping:  0.041539997023410415\n",
            "Epoch: 029, Loss: 1.0561, Train: 0.6703, Test: 0.6308\n",
            "Early stopping:  0.04057371180042309\n",
            "Epoch: 030, Loss: 1.0318, Train: 0.6796, Test: 0.6365\n",
            "Early stopping:  0.03973873045668557\n",
            "Epoch: 031, Loss: 1.0090, Train: 0.6860, Test: 0.6413\n",
            "Early stopping:  0.038827137123829875\n",
            "Epoch: 032, Loss: 0.9858, Train: 0.6936, Test: 0.6462\n",
            "Early stopping:  0.03775432703714657\n",
            "Epoch: 033, Loss: 0.9643, Train: 0.7006, Test: 0.6513\n",
            "Early stopping:  0.03630205606083404\n",
            "Epoch: 034, Loss: 0.9442, Train: 0.7123, Test: 0.6583\n",
            "Early stopping:  0.03477814519985872\n",
            "Epoch: 035, Loss: 0.9236, Train: 0.7225, Test: 0.6660\n",
            "Early stopping:  0.03358564116876716\n",
            "Epoch: 036, Loss: 0.9041, Train: 0.7312, Test: 0.6720\n",
            "Early stopping:  0.032270639089773366\n",
            "Epoch: 037, Loss: 0.8854, Train: 0.7367, Test: 0.6714\n",
            "Early stopping:  0.03129274085956349\n",
            "Epoch: 038, Loss: 0.8666, Train: 0.7435, Test: 0.6722\n",
            "Early stopping:  0.03056800532325448\n",
            "Epoch: 039, Loss: 0.8486, Train: 0.7529, Test: 0.6739\n",
            "Early stopping:  0.029630837643092697\n",
            "Epoch: 040, Loss: 0.8313, Train: 0.7582, Test: 0.6785\n",
            "Early stopping:  0.028849460342267035\n",
            "Epoch: 041, Loss: 0.8136, Train: 0.7664, Test: 0.6776\n",
            "Early stopping:  0.028310725010238735\n",
            "Epoch: 042, Loss: 0.7963, Train: 0.7679, Test: 0.6796\n",
            "Early stopping:  0.02777017980615985\n",
            "Epoch: 043, Loss: 0.7795, Train: 0.7769, Test: 0.6830\n",
            "Early stopping:  0.027375478865156674\n",
            "Epoch: 044, Loss: 0.7624, Train: 0.7807, Test: 0.6884\n",
            "Early stopping:  0.02718145961242832\n",
            "Epoch: 045, Loss: 0.7465, Train: 0.7917, Test: 0.6853\n",
            "Early stopping:  0.02660450390412654\n",
            "Epoch: 046, Loss: 0.7314, Train: 0.7898, Test: 0.6915\n",
            "Early stopping:  0.025761620016044668\n",
            "Epoch: 047, Loss: 0.7171, Train: 0.8021, Test: 0.6927\n",
            "Early stopping:  0.02464005736802979\n",
            "Epoch: 048, Loss: 0.7021, Train: 0.8025, Test: 0.6969\n",
            "Early stopping:  0.023712002365474768\n",
            "Epoch: 049, Loss: 0.6854, Train: 0.8087, Test: 0.7040\n",
            "Early stopping:  0.023950074902666985\n",
            "Epoch: 050, Loss: 0.6697, Train: 0.8140, Test: 0.7048\n",
            "Early stopping:  0.024548322468312768\n",
            "Epoch: 051, Loss: 0.6569, Train: 0.8140, Test: 0.7048\n",
            "Early stopping:  0.02419251717109213\n",
            "Epoch: 052, Loss: 0.6455, Train: 0.8236, Test: 0.7085\n",
            "Early stopping:  0.022480069987519186\n",
            "Epoch: 053, Loss: 0.6336, Train: 0.8282, Test: 0.7083\n",
            "Early stopping:  0.020255933655203732\n",
            "Epoch: 054, Loss: 0.6197, Train: 0.8336, Test: 0.7142\n",
            "Early stopping:  0.019501221345905397\n",
            "Epoch: 055, Loss: 0.6050, Train: 0.8397, Test: 0.7165\n",
            "Early stopping:  0.02053861866580557\n",
            "Epoch: 056, Loss: 0.5919, Train: 0.8444, Test: 0.7153\n",
            "Early stopping:  0.021496839689830446\n",
            "Epoch: 057, Loss: 0.5811, Train: 0.8480, Test: 0.7187\n",
            "Early stopping:  0.021040554612155427\n",
            "Epoch: 058, Loss: 0.5717, Train: 0.8459, Test: 0.7170\n",
            "Early stopping:  0.01903469533125299\n",
            "Epoch: 059, Loss: 0.5629, Train: 0.8527, Test: 0.7210\n",
            "Early stopping:  0.016556065744625283\n",
            "Epoch: 060, Loss: 0.5532, Train: 0.8535, Test: 0.7224\n",
            "Early stopping:  0.015109331184795315\n",
            "Epoch: 061, Loss: 0.5402, Train: 0.8637, Test: 0.7264\n",
            "Early stopping:  0.015883169769800486\n",
            "Epoch: 062, Loss: 0.5258, Train: 0.8684, Test: 0.7312\n",
            "Early stopping:  0.018193845782680514\n",
            "Epoch: 063, Loss: 0.5149, Train: 0.8688, Test: 0.7326\n",
            "Early stopping:  0.01955785203622989\n",
            "Epoch: 064, Loss: 0.5074, Train: 0.8739, Test: 0.7278\n",
            "Early stopping:  0.018621664068074584\n",
            "Epoch: 065, Loss: 0.5010, Train: 0.8737, Test: 0.7292\n",
            "Early stopping:  0.015542926079628391\n",
            "Epoch: 066, Loss: 0.4922, Train: 0.8805, Test: 0.7295\n",
            "Early stopping:  0.012861215992875097\n",
            "Epoch: 067, Loss: 0.4798, Train: 0.8841, Test: 0.7355\n",
            "Early stopping:  0.013599274363571529\n",
            "Epoch: 068, Loss: 0.4669, Train: 0.8870, Test: 0.7369\n",
            "Early stopping:  0.016313428610179866\n",
            "Epoch: 069, Loss: 0.4574, Train: 0.8917, Test: 0.7332\n",
            "Early stopping:  0.017839073254281978\n",
            "Epoch: 070, Loss: 0.4510, Train: 0.8900, Test: 0.7366\n",
            "Early stopping:  0.016742824311419616\n",
            "Epoch: 071, Loss: 0.4456, Train: 0.8970, Test: 0.7332\n",
            "Early stopping:  0.013571896203056547\n",
            "Epoch: 072, Loss: 0.4390, Train: 0.8947, Test: 0.7386\n",
            "Early stopping:  0.010738368787612598\n",
            "Epoch: 073, Loss: 0.4291, Train: 0.9009, Test: 0.7394\n",
            "Early stopping:  0.010917100145045168\n",
            "Epoch: 074, Loss: 0.4166, Train: 0.9045, Test: 0.7423\n",
            "Early stopping:  0.013693381595891604\n",
            "Epoch: 075, Loss: 0.4054, Train: 0.9060, Test: 0.7414\n",
            "Early stopping:  0.016358550954374454\n",
            "Epoch: 076, Loss: 0.3977, Train: 0.9106, Test: 0.7420\n",
            "Early stopping:  0.016848168974032025\n",
            "Epoch: 077, Loss: 0.3929, Train: 0.9045, Test: 0.7411\n",
            "Early stopping:  0.014629442258392135\n",
            "Epoch: 078, Loss: 0.3898, Train: 0.9144, Test: 0.7394\n",
            "Early stopping:  0.010738547201862887\n",
            "Epoch: 079, Loss: 0.3873, Train: 0.9042, Test: 0.7380\n",
            "Early stopping:  0.007164185323171763\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.81      0.71      0.75       379\n",
            "         capital_goods       0.72      0.59      0.65       254\n",
            "conglomerates_industry       0.88      0.57      0.70        40\n",
            "     consumer_cyclical       0.71      0.70      0.71       396\n",
            " consumer_non-cyclical       0.71      0.63      0.67       223\n",
            "                energy       0.86      0.75      0.80       141\n",
            "             financial       0.81      0.76      0.78       384\n",
            "            healthcare       0.76      0.73      0.74       159\n",
            "              services       0.68      0.84      0.75      1038\n",
            "            technology       0.70      0.59      0.64       198\n",
            "        transportation       0.87      0.80      0.83       202\n",
            "             utilities       0.81      0.69      0.75       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.78      0.70      0.73      3527\n",
            "          weighted avg       0.75      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4893, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2715, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15406797331896718\n",
            "Epoch: 003, Loss: 2.1710, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16272120927234443\n",
            "Epoch: 004, Loss: 2.1405, Train: 0.2983, Test: 0.2971\n",
            "Early stopping:  0.15774444528416018\n",
            "Epoch: 005, Loss: 2.0792, Train: 0.3423, Test: 0.3388\n",
            "Early stopping:  0.16061968339503171\n",
            "Epoch: 006, Loss: 2.0227, Train: 0.3826, Test: 0.3760\n",
            "Early stopping:  0.09440942323787134\n",
            "Epoch: 007, Loss: 1.9672, Train: 0.3856, Test: 0.3771\n",
            "Early stopping:  0.08348130267326288\n",
            "Epoch: 008, Loss: 1.9091, Train: 0.3856, Test: 0.3757\n",
            "Early stopping:  0.09088885846141265\n",
            "Epoch: 009, Loss: 1.8524, Train: 0.3940, Test: 0.3822\n",
            "Early stopping:  0.08966691805778128\n",
            "Epoch: 010, Loss: 1.7967, Train: 0.4204, Test: 0.4049\n",
            "Early stopping:  0.0896187223659751\n",
            "Epoch: 011, Loss: 1.7380, Train: 0.4478, Test: 0.4361\n",
            "Early stopping:  0.09026610679101123\n",
            "Epoch: 012, Loss: 1.6797, Train: 0.4635, Test: 0.4531\n",
            "Early stopping:  0.09064082535557703\n",
            "Epoch: 013, Loss: 1.6268, Train: 0.4800, Test: 0.4641\n",
            "Early stopping:  0.08987908989802117\n",
            "Epoch: 014, Loss: 1.5740, Train: 0.4953, Test: 0.4775\n",
            "Early stopping:  0.08804128394866259\n",
            "Epoch: 015, Loss: 1.5193, Train: 0.5176, Test: 0.4982\n",
            "Early stopping:  0.08590538753459662\n",
            "Epoch: 016, Loss: 1.4676, Train: 0.5435, Test: 0.5214\n",
            "Early stopping:  0.08407730454976266\n",
            "Epoch: 017, Loss: 1.4200, Train: 0.5628, Test: 0.5359\n",
            "Early stopping:  0.08222538575132118\n",
            "Epoch: 018, Loss: 1.3771, Train: 0.5752, Test: 0.5478\n",
            "Early stopping:  0.07804721054826037\n",
            "Epoch: 019, Loss: 1.3354, Train: 0.5888, Test: 0.5551\n",
            "Early stopping:  0.0725353646850606\n",
            "Epoch: 020, Loss: 1.2932, Train: 0.5949, Test: 0.5668\n",
            "Early stopping:  0.0685749197324497\n",
            "Epoch: 021, Loss: 1.2561, Train: 0.6043, Test: 0.5739\n",
            "Early stopping:  0.06513930069398798\n",
            "Epoch: 022, Loss: 1.2226, Train: 0.6142, Test: 0.5832\n",
            "Early stopping:  0.061467032315691186\n",
            "Epoch: 023, Loss: 1.1898, Train: 0.6259, Test: 0.5929\n",
            "Early stopping:  0.057258151748304204\n",
            "Epoch: 024, Loss: 1.1591, Train: 0.6367, Test: 0.6056\n",
            "Early stopping:  0.05291605084104867\n",
            "Epoch: 025, Loss: 1.1313, Train: 0.6433, Test: 0.6104\n",
            "Early stopping:  0.049552637365047794\n",
            "Epoch: 026, Loss: 1.1058, Train: 0.6505, Test: 0.6161\n",
            "Early stopping:  0.046271924287962854\n",
            "Epoch: 027, Loss: 1.0814, Train: 0.6582, Test: 0.6206\n",
            "Early stopping:  0.04275622479742025\n",
            "Epoch: 028, Loss: 1.0579, Train: 0.6660, Test: 0.6274\n",
            "Early stopping:  0.03989575867844345\n",
            "Epoch: 029, Loss: 1.0356, Train: 0.6737, Test: 0.6331\n",
            "Early stopping:  0.03783147221342657\n",
            "Epoch: 030, Loss: 1.0146, Train: 0.6796, Test: 0.6408\n",
            "Early stopping:  0.03607573582263537\n",
            "Epoch: 031, Loss: 0.9922, Train: 0.6919, Test: 0.6445\n",
            "Early stopping:  0.03504555541400996\n",
            "Epoch: 032, Loss: 0.9713, Train: 0.6977, Test: 0.6496\n",
            "Early stopping:  0.034242815720065885\n",
            "Epoch: 033, Loss: 0.9511, Train: 0.7051, Test: 0.6510\n",
            "Early stopping:  0.03359534691135319\n",
            "Epoch: 034, Loss: 0.9309, Train: 0.7142, Test: 0.6530\n",
            "Early stopping:  0.03300584361927844\n",
            "Epoch: 035, Loss: 0.9108, Train: 0.7233, Test: 0.6541\n",
            "Early stopping:  0.03214070711882039\n",
            "Epoch: 036, Loss: 0.8924, Train: 0.7287, Test: 0.6575\n",
            "Early stopping:  0.03133216269145352\n",
            "Epoch: 037, Loss: 0.8741, Train: 0.7376, Test: 0.6635\n",
            "Early stopping:  0.03043022599365137\n",
            "Epoch: 038, Loss: 0.8558, Train: 0.7473, Test: 0.6694\n",
            "Early stopping:  0.029549964873710675\n",
            "Epoch: 039, Loss: 0.8389, Train: 0.7535, Test: 0.6708\n",
            "Early stopping:  0.028527604564775818\n",
            "Epoch: 040, Loss: 0.8218, Train: 0.7647, Test: 0.6751\n",
            "Early stopping:  0.02789768105155903\n",
            "Epoch: 041, Loss: 0.8048, Train: 0.7694, Test: 0.6793\n",
            "Early stopping:  0.02730825896157204\n",
            "Epoch: 042, Loss: 0.7883, Train: 0.7747, Test: 0.6768\n",
            "Early stopping:  0.026733639681098792\n",
            "Epoch: 043, Loss: 0.7720, Train: 0.7807, Test: 0.6836\n",
            "Early stopping:  0.026433747573137236\n",
            "Epoch: 044, Loss: 0.7559, Train: 0.7881, Test: 0.6844\n",
            "Early stopping:  0.026020504475750102\n",
            "Epoch: 045, Loss: 0.7404, Train: 0.7930, Test: 0.6878\n",
            "Early stopping:  0.02549696321503209\n",
            "Epoch: 046, Loss: 0.7247, Train: 0.7981, Test: 0.6912\n",
            "Early stopping:  0.025125804579742124\n",
            "Epoch: 047, Loss: 0.7094, Train: 0.8034, Test: 0.6958\n",
            "Early stopping:  0.024730530178049978\n",
            "Epoch: 048, Loss: 0.6941, Train: 0.8085, Test: 0.6972\n",
            "Early stopping:  0.02441845172242084\n",
            "Epoch: 049, Loss: 0.6792, Train: 0.8098, Test: 0.6975\n",
            "Early stopping:  0.024185052466164095\n",
            "Epoch: 050, Loss: 0.6654, Train: 0.8193, Test: 0.6997\n",
            "Early stopping:  0.023545964949414747\n",
            "Epoch: 051, Loss: 0.6532, Train: 0.8130, Test: 0.7054\n",
            "Early stopping:  0.022365487366467977\n",
            "Epoch: 052, Loss: 0.6435, Train: 0.8265, Test: 0.7051\n",
            "Early stopping:  0.020204510084736005\n",
            "Epoch: 053, Loss: 0.6297, Train: 0.8268, Test: 0.7111\n",
            "Early stopping:  0.01913914050281727\n",
            "Epoch: 054, Loss: 0.6116, Train: 0.8318, Test: 0.7125\n",
            "Early stopping:  0.020856168546005145\n",
            "Epoch: 055, Loss: 0.5982, Train: 0.8423, Test: 0.7119\n",
            "Early stopping:  0.022542867960531482\n",
            "Epoch: 056, Loss: 0.5907, Train: 0.8340, Test: 0.7151\n",
            "Early stopping:  0.021872451195356503\n",
            "Epoch: 057, Loss: 0.5798, Train: 0.8497, Test: 0.7193\n",
            "Early stopping:  0.019320190159142922\n",
            "Epoch: 058, Loss: 0.5632, Train: 0.8529, Test: 0.7210\n",
            "Early stopping:  0.01833460225680975\n",
            "Epoch: 059, Loss: 0.5520, Train: 0.8467, Test: 0.7159\n",
            "Early stopping:  0.019098321654069047\n",
            "Epoch: 060, Loss: 0.5452, Train: 0.8561, Test: 0.7219\n",
            "Early stopping:  0.01895916010680351\n",
            "Epoch: 061, Loss: 0.5336, Train: 0.8601, Test: 0.7244\n",
            "Early stopping:  0.01766205334386136\n",
            "Epoch: 062, Loss: 0.5194, Train: 0.8648, Test: 0.7250\n",
            "Early stopping:  0.01687311216244463\n",
            "Epoch: 063, Loss: 0.5089, Train: 0.8682, Test: 0.7287\n",
            "Early stopping:  0.0178156594981611\n",
            "Epoch: 064, Loss: 0.5015, Train: 0.8673, Test: 0.7241\n",
            "Early stopping:  0.01782641766942104\n",
            "Epoch: 065, Loss: 0.4926, Train: 0.8767, Test: 0.7315\n",
            "Early stopping:  0.015917329086249616\n",
            "Epoch: 066, Loss: 0.4800, Train: 0.8802, Test: 0.7298\n",
            "Early stopping:  0.015105127781864635\n",
            "Epoch: 067, Loss: 0.4682, Train: 0.8820, Test: 0.7301\n",
            "Early stopping:  0.016371197428770388\n",
            "Epoch: 068, Loss: 0.4594, Train: 0.8864, Test: 0.7343\n",
            "Early stopping:  0.017216216026064577\n",
            "Epoch: 069, Loss: 0.4521, Train: 0.8853, Test: 0.7289\n",
            "Early stopping:  0.016163158413284497\n",
            "Epoch: 070, Loss: 0.4452, Train: 0.8941, Test: 0.7329\n",
            "Early stopping:  0.013622708254577669\n",
            "Epoch: 071, Loss: 0.4357, Train: 0.8932, Test: 0.7315\n",
            "Early stopping:  0.012540033718307832\n",
            "Epoch: 072, Loss: 0.4252, Train: 0.9013, Test: 0.7341\n",
            "Early stopping:  0.013461770755852524\n",
            "Epoch: 073, Loss: 0.4144, Train: 0.9008, Test: 0.7338\n",
            "Early stopping:  0.015122049161683731\n",
            "Epoch: 074, Loss: 0.4061, Train: 0.9055, Test: 0.7343\n",
            "Early stopping:  0.015748474873006544\n",
            "Epoch: 075, Loss: 0.3998, Train: 0.9040, Test: 0.7349\n",
            "Early stopping:  0.014465017981782553\n",
            "Epoch: 076, Loss: 0.3948, Train: 0.9095, Test: 0.7352\n",
            "Early stopping:  0.012074798233565179\n",
            "Epoch: 077, Loss: 0.3869, Train: 0.9121, Test: 0.7369\n",
            "Early stopping:  0.010517285116992866\n",
            "Epoch: 078, Loss: 0.3760, Train: 0.9110, Test: 0.7369\n",
            "Early stopping:  0.011668880680836824\n",
            "Epoch: 079, Loss: 0.3701, Train: 0.9178, Test: 0.7324\n",
            "Early stopping:  0.012449160271748947\n",
            "Epoch: 080, Loss: 0.3719, Train: 0.9006, Test: 0.7306\n",
            "Early stopping:  0.010570912983365665\n",
            "Epoch: 081, Loss: 0.3729, Train: 0.9208, Test: 0.7298\n",
            "Early stopping:  0.006696923228411368\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11, 11], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.71      0.76      0.74       379\n",
            "         capital_goods       0.62      0.64      0.63       254\n",
            "conglomerates_industry       0.96      0.55      0.70        40\n",
            "     consumer_cyclical       0.61      0.78      0.68       396\n",
            " consumer_non-cyclical       0.67      0.66      0.67       223\n",
            "                energy       0.85      0.77      0.81       141\n",
            "             financial       0.76      0.78      0.77       384\n",
            "            healthcare       0.82      0.75      0.79       159\n",
            "              services       0.77      0.73      0.75      1038\n",
            "            technology       0.65      0.60      0.62       198\n",
            "        transportation       0.86      0.77      0.81       202\n",
            "             utilities       0.88      0.74      0.80       113\n",
            "\n",
            "              accuracy                           0.73      3527\n",
            "             macro avg       0.76      0.71      0.73      3527\n",
            "          weighted avg       0.74      0.73      0.73      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4836, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2660, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15387510934018134\n",
            "Epoch: 003, Loss: 2.1676, Train: 0.2949, Test: 0.2946\n",
            "Early stopping:  0.16170672710932935\n",
            "Epoch: 004, Loss: 2.1319, Train: 0.3070, Test: 0.3062\n",
            "Early stopping:  0.15808517776028194\n",
            "Epoch: 005, Loss: 2.0654, Train: 0.3488, Test: 0.3448\n",
            "Early stopping:  0.16277259755310747\n",
            "Epoch: 006, Loss: 2.0042, Train: 0.3843, Test: 0.3825\n",
            "Early stopping:  0.09984805316641975\n",
            "Epoch: 007, Loss: 1.9439, Train: 0.3951, Test: 0.3876\n",
            "Early stopping:  0.09128224815099407\n",
            "Epoch: 008, Loss: 1.8847, Train: 0.4032, Test: 0.3935\n",
            "Early stopping:  0.0973972059841919\n",
            "Epoch: 009, Loss: 1.8253, Train: 0.4142, Test: 0.4080\n",
            "Early stopping:  0.09483359879884853\n",
            "Epoch: 010, Loss: 1.7652, Train: 0.4293, Test: 0.4205\n",
            "Early stopping:  0.09434839151251925\n",
            "Epoch: 011, Loss: 1.7074, Train: 0.4495, Test: 0.4335\n",
            "Early stopping:  0.09371054144103423\n",
            "Epoch: 012, Loss: 1.6553, Train: 0.4641, Test: 0.4485\n",
            "Early stopping:  0.09122232823592953\n",
            "Epoch: 013, Loss: 1.6033, Train: 0.4843, Test: 0.4630\n",
            "Early stopping:  0.087632350908582\n",
            "Epoch: 014, Loss: 1.5493, Train: 0.5043, Test: 0.4831\n",
            "Early stopping:  0.08474056339749274\n",
            "Epoch: 015, Loss: 1.4999, Train: 0.5359, Test: 0.5101\n",
            "Early stopping:  0.08237867248511176\n",
            "Epoch: 016, Loss: 1.4539, Train: 0.5601, Test: 0.5316\n",
            "Early stopping:  0.08006948530238035\n",
            "Epoch: 017, Loss: 1.4095, Train: 0.5709, Test: 0.5503\n",
            "Early stopping:  0.07642690668422046\n",
            "Epoch: 018, Loss: 1.3670, Train: 0.5752, Test: 0.5574\n",
            "Early stopping:  0.0719630655041112\n",
            "Epoch: 019, Loss: 1.3254, Train: 0.5826, Test: 0.5631\n",
            "Early stopping:  0.06892099935860677\n",
            "Epoch: 020, Loss: 1.2866, Train: 0.5919, Test: 0.5739\n",
            "Early stopping:  0.0662134154929466\n",
            "Epoch: 021, Loss: 1.2505, Train: 0.6057, Test: 0.5824\n",
            "Early stopping:  0.06303299608107465\n",
            "Epoch: 022, Loss: 1.2161, Train: 0.6191, Test: 0.5923\n",
            "Early stopping:  0.059629846815466535\n",
            "Epoch: 023, Loss: 1.1857, Train: 0.6276, Test: 0.5999\n",
            "Early stopping:  0.05537299836597472\n",
            "Epoch: 024, Loss: 1.1575, Train: 0.6374, Test: 0.6042\n",
            "Early stopping:  0.05111304850822337\n",
            "Epoch: 025, Loss: 1.1294, Train: 0.6422, Test: 0.6073\n",
            "Early stopping:  0.047600203922016866\n",
            "Epoch: 026, Loss: 1.1032, Train: 0.6467, Test: 0.6136\n",
            "Early stopping:  0.04460592395344804\n",
            "Epoch: 027, Loss: 1.0799, Train: 0.6535, Test: 0.6175\n",
            "Early stopping:  0.04207980397492974\n",
            "Epoch: 028, Loss: 1.0557, Train: 0.6629, Test: 0.6283\n",
            "Early stopping:  0.040043807185865414\n",
            "Epoch: 029, Loss: 1.0325, Train: 0.6767, Test: 0.6325\n",
            "Early stopping:  0.03813693942855747\n",
            "Epoch: 030, Loss: 1.0115, Train: 0.6847, Test: 0.6413\n",
            "Early stopping:  0.03650910343400737\n",
            "Epoch: 031, Loss: 0.9898, Train: 0.6919, Test: 0.6453\n",
            "Early stopping:  0.03549857390225718\n",
            "Epoch: 032, Loss: 0.9679, Train: 0.7008, Test: 0.6490\n",
            "Early stopping:  0.03454340761162177\n",
            "Epoch: 033, Loss: 0.9477, Train: 0.7106, Test: 0.6575\n",
            "Early stopping:  0.03374041345436638\n",
            "Epoch: 034, Loss: 0.9277, Train: 0.7180, Test: 0.6629\n",
            "Early stopping:  0.03318136670868649\n",
            "Epoch: 035, Loss: 0.9082, Train: 0.7272, Test: 0.6646\n",
            "Early stopping:  0.03215954044959363\n",
            "Epoch: 036, Loss: 0.8903, Train: 0.7363, Test: 0.6677\n",
            "Early stopping:  0.030769763263498687\n",
            "Epoch: 037, Loss: 0.8723, Train: 0.7435, Test: 0.6703\n",
            "Early stopping:  0.029770861750169395\n",
            "Epoch: 038, Loss: 0.8547, Train: 0.7497, Test: 0.6731\n",
            "Early stopping:  0.028769502107486154\n",
            "Epoch: 039, Loss: 0.8379, Train: 0.7569, Test: 0.6745\n",
            "Early stopping:  0.027859336414203163\n",
            "Epoch: 040, Loss: 0.8208, Train: 0.7641, Test: 0.6776\n",
            "Early stopping:  0.027398395176188802\n",
            "Epoch: 041, Loss: 0.8041, Train: 0.7690, Test: 0.6813\n",
            "Early stopping:  0.026901366551974115\n",
            "Epoch: 042, Loss: 0.7878, Train: 0.7750, Test: 0.6833\n",
            "Early stopping:  0.0265150618858008\n",
            "Epoch: 043, Loss: 0.7715, Train: 0.7803, Test: 0.6844\n",
            "Early stopping:  0.026248060333413478\n",
            "Epoch: 044, Loss: 0.7555, Train: 0.7877, Test: 0.6856\n",
            "Early stopping:  0.025815165240373236\n",
            "Epoch: 045, Loss: 0.7398, Train: 0.7915, Test: 0.6912\n",
            "Early stopping:  0.02543880643793934\n",
            "Epoch: 046, Loss: 0.7247, Train: 0.8013, Test: 0.6915\n",
            "Early stopping:  0.024951677351505004\n",
            "Epoch: 047, Loss: 0.7108, Train: 0.7907, Test: 0.6983\n",
            "Early stopping:  0.024062746860208414\n",
            "Epoch: 048, Loss: 0.7034, Train: 0.7998, Test: 0.6850\n",
            "Early stopping:  0.021230530317289786\n",
            "Epoch: 049, Loss: 0.7006, Train: 0.8026, Test: 0.7040\n",
            "Early stopping:  0.016321496190767535\n",
            "Epoch: 050, Loss: 0.6769, Train: 0.8146, Test: 0.7060\n",
            "Early stopping:  0.017449835466830346\n",
            "Epoch: 051, Loss: 0.6573, Train: 0.8159, Test: 0.6961\n",
            "Early stopping:  0.022172872795400655\n",
            "Epoch: 052, Loss: 0.6575, Train: 0.8189, Test: 0.7091\n",
            "Early stopping:  0.022349296040137783\n",
            "Epoch: 053, Loss: 0.6363, Train: 0.8223, Test: 0.7105\n",
            "Early stopping:  0.024236774532555155\n",
            "Epoch: 054, Loss: 0.6253, Train: 0.8299, Test: 0.7037\n",
            "Early stopping:  0.020193679232826812\n",
            "Epoch: 055, Loss: 0.6208, Train: 0.8355, Test: 0.7117\n",
            "Early stopping:  0.017337878078597574\n",
            "Epoch: 056, Loss: 0.6006, Train: 0.8312, Test: 0.7165\n",
            "Early stopping:  0.02090755453577478\n",
            "Epoch: 057, Loss: 0.5964, Train: 0.8420, Test: 0.7108\n",
            "Early stopping:  0.016883580056738165\n",
            "Epoch: 058, Loss: 0.5858, Train: 0.8480, Test: 0.7165\n",
            "Early stopping:  0.016725388418323037\n",
            "Epoch: 059, Loss: 0.5703, Train: 0.8406, Test: 0.7224\n",
            "Early stopping:  0.0186636245491981\n",
            "Epoch: 060, Loss: 0.5671, Train: 0.8527, Test: 0.7176\n",
            "Early stopping:  0.015049428682025308\n",
            "Epoch: 061, Loss: 0.5542, Train: 0.8582, Test: 0.7213\n",
            "Early stopping:  0.016536883210545413\n",
            "Epoch: 062, Loss: 0.5421, Train: 0.8529, Test: 0.7261\n",
            "Early stopping:  0.01661847965809684\n",
            "Epoch: 063, Loss: 0.5381, Train: 0.8639, Test: 0.7247\n",
            "Early stopping:  0.01442428543934407\n",
            "Epoch: 064, Loss: 0.5258, Train: 0.8679, Test: 0.7244\n",
            "Early stopping:  0.015787246436044497\n",
            "Epoch: 065, Loss: 0.5146, Train: 0.8648, Test: 0.7255\n",
            "Early stopping:  0.015238240274045285\n",
            "Epoch: 066, Loss: 0.5106, Train: 0.8728, Test: 0.7278\n",
            "Early stopping:  0.013887804351586222\n",
            "Epoch: 067, Loss: 0.5001, Train: 0.8775, Test: 0.7326\n",
            "Early stopping:  0.014587134504608886\n",
            "Epoch: 068, Loss: 0.4879, Train: 0.8756, Test: 0.7306\n",
            "Early stopping:  0.014426510573601267\n",
            "Epoch: 069, Loss: 0.4837, Train: 0.8805, Test: 0.7298\n",
            "Early stopping:  0.013552698037049107\n",
            "Epoch: 070, Loss: 0.4768, Train: 0.8843, Test: 0.7363\n",
            "Early stopping:  0.013517641579902229\n",
            "Epoch: 071, Loss: 0.4641, Train: 0.8871, Test: 0.7355\n",
            "Early stopping:  0.013351651203196008\n",
            "Epoch: 072, Loss: 0.4561, Train: 0.8915, Test: 0.7338\n",
            "Early stopping:  0.013355571593075356\n",
            "Epoch: 073, Loss: 0.4517, Train: 0.8911, Test: 0.7358\n",
            "Early stopping:  0.013558818056784508\n",
            "Epoch: 074, Loss: 0.4446, Train: 0.8991, Test: 0.7352\n",
            "Early stopping:  0.012357408326396552\n",
            "Epoch: 075, Loss: 0.4342, Train: 0.9015, Test: 0.7386\n",
            "Early stopping:  0.011385301633553686\n",
            "Epoch: 076, Loss: 0.4244, Train: 0.9021, Test: 0.7394\n",
            "Early stopping:  0.012949556663620207\n",
            "Epoch: 077, Loss: 0.4187, Train: 0.9051, Test: 0.7366\n",
            "Early stopping:  0.013687163266464198\n",
            "Epoch: 078, Loss: 0.4142, Train: 0.9049, Test: 0.7403\n",
            "Early stopping:  0.012245169669655836\n",
            "Epoch: 079, Loss: 0.4085, Train: 0.9076, Test: 0.7369\n",
            "Early stopping:  0.009853465679675568\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.79      0.75       379\n",
            "         capital_goods       0.65      0.61      0.63       254\n",
            "conglomerates_industry       0.97      0.70      0.81        40\n",
            "     consumer_cyclical       0.65      0.77      0.70       396\n",
            " consumer_non-cyclical       0.75      0.66      0.70       223\n",
            "                energy       0.82      0.77      0.79       141\n",
            "             financial       0.84      0.75      0.80       384\n",
            "            healthcare       0.80      0.76      0.78       159\n",
            "              services       0.73      0.76      0.75      1038\n",
            "            technology       0.63      0.58      0.60       198\n",
            "        transportation       0.84      0.79      0.82       202\n",
            "             utilities       0.86      0.73      0.79       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.77      0.72      0.74      3527\n",
            "          weighted avg       0.74      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4580, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2504, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.14681416351545423\n",
            "Epoch: 003, Loss: 2.1650, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15072119061879724\n",
            "Epoch: 004, Loss: 2.1350, Train: 0.3019, Test: 0.3008\n",
            "Early stopping:  0.14572749132906826\n",
            "Epoch: 005, Loss: 2.0809, Train: 0.3359, Test: 0.3329\n",
            "Early stopping:  0.14761169929379994\n",
            "Epoch: 006, Loss: 2.0235, Train: 0.3554, Test: 0.3524\n",
            "Early stopping:  0.08587781699043232\n",
            "Epoch: 007, Loss: 1.9647, Train: 0.3582, Test: 0.3575\n",
            "Early stopping:  0.08145995867814897\n",
            "Epoch: 008, Loss: 1.9085, Train: 0.3673, Test: 0.3629\n",
            "Early stopping:  0.09000944444025279\n",
            "Epoch: 009, Loss: 1.8558, Train: 0.3922, Test: 0.3847\n",
            "Early stopping:  0.08936299748440661\n",
            "Epoch: 010, Loss: 1.8017, Train: 0.4244, Test: 0.4165\n",
            "Early stopping:  0.08735812597290397\n",
            "Epoch: 011, Loss: 1.7450, Train: 0.4480, Test: 0.4335\n",
            "Early stopping:  0.08635968873333959\n",
            "Epoch: 012, Loss: 1.6883, Train: 0.4620, Test: 0.4494\n",
            "Early stopping:  0.08718233435926546\n",
            "Epoch: 013, Loss: 1.6319, Train: 0.4711, Test: 0.4604\n",
            "Early stopping:  0.08874477223208178\n",
            "Epoch: 014, Loss: 1.5760, Train: 0.4868, Test: 0.4780\n",
            "Early stopping:  0.08925404106000212\n",
            "Epoch: 015, Loss: 1.5224, Train: 0.5144, Test: 0.4987\n",
            "Early stopping:  0.08815210840760777\n",
            "Epoch: 016, Loss: 1.4721, Train: 0.5401, Test: 0.5197\n",
            "Early stopping:  0.08569662432780446\n",
            "Epoch: 017, Loss: 1.4237, Train: 0.5603, Test: 0.5344\n",
            "Early stopping:  0.08230893675383182\n",
            "Epoch: 018, Loss: 1.3766, Train: 0.5730, Test: 0.5478\n",
            "Early stopping:  0.07868444071107195\n",
            "Epoch: 019, Loss: 1.3317, Train: 0.5813, Test: 0.5574\n",
            "Early stopping:  0.07542580289381302\n",
            "Epoch: 020, Loss: 1.2929, Train: 0.5892, Test: 0.5690\n",
            "Early stopping:  0.07126663625133489\n",
            "Epoch: 021, Loss: 1.2572, Train: 0.6004, Test: 0.5787\n",
            "Early stopping:  0.06600629512063053\n",
            "Epoch: 022, Loss: 1.2229, Train: 0.6121, Test: 0.5849\n",
            "Early stopping:  0.06047472065071224\n",
            "Epoch: 023, Loss: 1.1934, Train: 0.6193, Test: 0.5971\n",
            "Early stopping:  0.05488629570673084\n",
            "Epoch: 024, Loss: 1.1651, Train: 0.6265, Test: 0.6036\n",
            "Early stopping:  0.05060060062592975\n",
            "Epoch: 025, Loss: 1.1394, Train: 0.6325, Test: 0.6096\n",
            "Early stopping:  0.04646690384540545\n",
            "Epoch: 026, Loss: 1.1148, Train: 0.6423, Test: 0.6133\n",
            "Early stopping:  0.04274658685453977\n",
            "Epoch: 027, Loss: 1.0911, Train: 0.6497, Test: 0.6189\n",
            "Early stopping:  0.04030836130313081\n",
            "Epoch: 028, Loss: 1.0671, Train: 0.6620, Test: 0.6266\n",
            "Early stopping:  0.03862196559807887\n",
            "Epoch: 029, Loss: 1.0438, Train: 0.6701, Test: 0.6323\n",
            "Early stopping:  0.03777283954858058\n",
            "Epoch: 030, Loss: 1.0218, Train: 0.6796, Test: 0.6340\n",
            "Early stopping:  0.03688497683257936\n",
            "Epoch: 031, Loss: 0.9996, Train: 0.6896, Test: 0.6351\n",
            "Early stopping:  0.03608522229500822\n",
            "Epoch: 032, Loss: 0.9783, Train: 0.6985, Test: 0.6428\n",
            "Early stopping:  0.03505708652291604\n",
            "Epoch: 033, Loss: 0.9580, Train: 0.7068, Test: 0.6487\n",
            "Early stopping:  0.03401107664271154\n",
            "Epoch: 034, Loss: 0.9376, Train: 0.7151, Test: 0.6532\n",
            "Early stopping:  0.03323376790490496\n",
            "Epoch: 035, Loss: 0.9178, Train: 0.7195, Test: 0.6552\n",
            "Early stopping:  0.03233233932803766\n",
            "Epoch: 036, Loss: 0.8985, Train: 0.7272, Test: 0.6598\n",
            "Early stopping:  0.03162373410118932\n",
            "Epoch: 037, Loss: 0.8795, Train: 0.7372, Test: 0.6646\n",
            "Early stopping:  0.031004809878909745\n",
            "Epoch: 038, Loss: 0.8610, Train: 0.7471, Test: 0.6669\n",
            "Early stopping:  0.03027021007134208\n",
            "Epoch: 039, Loss: 0.8428, Train: 0.7537, Test: 0.6700\n",
            "Early stopping:  0.029644151445035617\n",
            "Epoch: 040, Loss: 0.8254, Train: 0.7628, Test: 0.6731\n",
            "Early stopping:  0.02890888151023866\n",
            "Epoch: 041, Loss: 0.8079, Train: 0.7707, Test: 0.6790\n",
            "Early stopping:  0.028267967063752654\n",
            "Epoch: 042, Loss: 0.7911, Train: 0.7737, Test: 0.6847\n",
            "Early stopping:  0.027621175428338886\n",
            "Epoch: 043, Loss: 0.7749, Train: 0.7832, Test: 0.6844\n",
            "Early stopping:  0.026916883968489038\n",
            "Epoch: 044, Loss: 0.7585, Train: 0.7877, Test: 0.6870\n",
            "Early stopping:  0.026408776855594136\n",
            "Epoch: 045, Loss: 0.7422, Train: 0.7928, Test: 0.6907\n",
            "Early stopping:  0.025949090621944396\n",
            "Epoch: 046, Loss: 0.7260, Train: 0.7975, Test: 0.6932\n",
            "Early stopping:  0.025746725242571166\n",
            "Epoch: 047, Loss: 0.7101, Train: 0.8038, Test: 0.7000\n",
            "Early stopping:  0.025605952217493722\n",
            "Epoch: 048, Loss: 0.6944, Train: 0.8095, Test: 0.7000\n",
            "Early stopping:  0.025317603577609194\n",
            "Epoch: 049, Loss: 0.6789, Train: 0.8117, Test: 0.7043\n",
            "Early stopping:  0.025003261226132584\n",
            "Epoch: 050, Loss: 0.6637, Train: 0.8198, Test: 0.7074\n",
            "Early stopping:  0.02463644638232774\n",
            "Epoch: 051, Loss: 0.6487, Train: 0.8244, Test: 0.7125\n",
            "Early stopping:  0.024295306238032808\n",
            "Epoch: 052, Loss: 0.6338, Train: 0.8314, Test: 0.7142\n",
            "Early stopping:  0.02395258858068323\n",
            "Epoch: 053, Loss: 0.6193, Train: 0.8323, Test: 0.7179\n",
            "Early stopping:  0.023574551107308217\n",
            "Epoch: 054, Loss: 0.6060, Train: 0.8395, Test: 0.7122\n",
            "Early stopping:  0.022886100495769363\n",
            "Epoch: 055, Loss: 0.5951, Train: 0.8352, Test: 0.7185\n",
            "Early stopping:  0.021352073510747048\n",
            "Epoch: 056, Loss: 0.5888, Train: 0.8452, Test: 0.7139\n",
            "Early stopping:  0.01823993527298771\n",
            "Epoch: 057, Loss: 0.5751, Train: 0.8501, Test: 0.7238\n",
            "Early stopping:  0.016818844519515243\n",
            "Epoch: 058, Loss: 0.5542, Train: 0.8533, Test: 0.7281\n",
            "Early stopping:  0.019988868562226765\n",
            "Epoch: 059, Loss: 0.5439, Train: 0.8586, Test: 0.7213\n",
            "Early stopping:  0.021953895409832336\n",
            "Epoch: 060, Loss: 0.5383, Train: 0.8603, Test: 0.7295\n",
            "Early stopping:  0.021307495677244968\n",
            "Epoch: 061, Loss: 0.5227, Train: 0.8671, Test: 0.7324\n",
            "Early stopping:  0.019463611365749586\n",
            "Epoch: 062, Loss: 0.5096, Train: 0.8701, Test: 0.7284\n",
            "Early stopping:  0.017669030803041108\n",
            "Epoch: 063, Loss: 0.5045, Train: 0.8711, Test: 0.7363\n",
            "Early stopping:  0.01725639868229435\n",
            "Epoch: 064, Loss: 0.4932, Train: 0.8773, Test: 0.7366\n",
            "Early stopping:  0.01737073516873357\n",
            "Epoch: 065, Loss: 0.4792, Train: 0.8803, Test: 0.7315\n",
            "Early stopping:  0.0164948304178072\n",
            "Epoch: 066, Loss: 0.4730, Train: 0.8796, Test: 0.7409\n",
            "Early stopping:  0.01574792745835915\n",
            "Epoch: 067, Loss: 0.4656, Train: 0.8883, Test: 0.7366\n",
            "Early stopping:  0.01567695782231016\n",
            "Epoch: 068, Loss: 0.4519, Train: 0.8919, Test: 0.7375\n",
            "Early stopping:  0.015400191464236367\n",
            "Epoch: 069, Loss: 0.4423, Train: 0.8900, Test: 0.7437\n",
            "Early stopping:  0.015164889574986426\n",
            "Epoch: 070, Loss: 0.4374, Train: 0.8955, Test: 0.7360\n",
            "Early stopping:  0.015107555622907982\n",
            "Epoch: 071, Loss: 0.4286, Train: 0.8983, Test: 0.7440\n",
            "Early stopping:  0.01419878785553617\n",
            "Epoch: 072, Loss: 0.4170, Train: 0.9025, Test: 0.7423\n",
            "Early stopping:  0.013289134991745865\n",
            "Epoch: 073, Loss: 0.4085, Train: 0.9034, Test: 0.7366\n",
            "Early stopping:  0.014007805158645078\n",
            "Epoch: 074, Loss: 0.4038, Train: 0.9036, Test: 0.7460\n",
            "Early stopping:  0.013914541767616074\n",
            "Epoch: 075, Loss: 0.3967, Train: 0.9108, Test: 0.7420\n",
            "Early stopping:  0.012331706057205358\n",
            "Epoch: 076, Loss: 0.3847, Train: 0.9130, Test: 0.7448\n",
            "Early stopping:  0.012234562727239526\n",
            "Epoch: 077, Loss: 0.3759, Train: 0.9121, Test: 0.7477\n",
            "Early stopping:  0.013491288315803356\n",
            "Epoch: 078, Loss: 0.3714, Train: 0.9183, Test: 0.7420\n",
            "Early stopping:  0.013671732060051123\n",
            "Epoch: 079, Loss: 0.3640, Train: 0.9219, Test: 0.7454\n",
            "Early stopping:  0.012625979971640406\n",
            "Epoch: 080, Loss: 0.3542, Train: 0.9234, Test: 0.7485\n",
            "Early stopping:  0.011629897973225006\n",
            "Epoch: 081, Loss: 0.3470, Train: 0.9287, Test: 0.7445\n",
            "Early stopping:  0.011956599053549863\n",
            "Epoch: 082, Loss: 0.3412, Train: 0.9274, Test: 0.7477\n",
            "Early stopping:  0.012268034839532437\n",
            "Epoch: 083, Loss: 0.3345, Train: 0.9316, Test: 0.7423\n",
            "Early stopping:  0.011436702724351185\n",
            "Epoch: 084, Loss: 0.3275, Train: 0.9335, Test: 0.7468\n",
            "Early stopping:  0.01042283924684787\n",
            "Epoch: 085, Loss: 0.3219, Train: 0.9259, Test: 0.7386\n",
            "Early stopping:  0.01010327487138627\n",
            "Epoch: 086, Loss: 0.3338, Train: 0.8565, Test: 0.7233\n",
            "Early stopping:  0.007363390214220958\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.82      0.65      0.73       379\n",
            "         capital_goods       0.76      0.55      0.64       254\n",
            "conglomerates_industry       0.96      0.65      0.78        40\n",
            "     consumer_cyclical       0.85      0.53      0.65       396\n",
            " consumer_non-cyclical       0.78      0.59      0.67       223\n",
            "                energy       0.90      0.69      0.78       141\n",
            "             financial       0.71      0.80      0.75       384\n",
            "            healthcare       0.88      0.75      0.81       159\n",
            "              services       0.63      0.87      0.73      1038\n",
            "            technology       0.69      0.62      0.65       198\n",
            "        transportation       0.77      0.81      0.79       202\n",
            "             utilities       0.84      0.72      0.77       113\n",
            "\n",
            "              accuracy                           0.72      3527\n",
            "             macro avg       0.80      0.69      0.73      3527\n",
            "          weighted avg       0.75      0.72      0.72      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5145, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2862, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16142630730678714\n",
            "Epoch: 003, Loss: 2.1747, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.1732293976821124\n",
            "Epoch: 004, Loss: 2.1481, Train: 0.2985, Test: 0.2991\n",
            "Early stopping:  0.16687125856568263\n",
            "Epoch: 005, Loss: 2.1005, Train: 0.3397, Test: 0.3368\n",
            "Early stopping:  0.16549313905912807\n",
            "Epoch: 006, Loss: 2.0483, Train: 0.3665, Test: 0.3626\n",
            "Early stopping:  0.0893821823843891\n",
            "Epoch: 007, Loss: 1.9875, Train: 0.3671, Test: 0.3615\n",
            "Early stopping:  0.0756376729165255\n",
            "Epoch: 008, Loss: 1.9294, Train: 0.3652, Test: 0.3601\n",
            "Early stopping:  0.08712476294377929\n",
            "Epoch: 009, Loss: 1.8808, Train: 0.3741, Test: 0.3677\n",
            "Early stopping:  0.0883377149741204\n",
            "Epoch: 010, Loss: 1.8323, Train: 0.3955, Test: 0.3833\n",
            "Early stopping:  0.08530054344853709\n",
            "Epoch: 011, Loss: 1.7768, Train: 0.4240, Test: 0.4125\n",
            "Early stopping:  0.08202007177803822\n",
            "Epoch: 012, Loss: 1.7199, Train: 0.4493, Test: 0.4369\n",
            "Early stopping:  0.08274148130181866\n",
            "Epoch: 013, Loss: 1.6653, Train: 0.4654, Test: 0.4511\n",
            "Early stopping:  0.08594851999695545\n",
            "Epoch: 014, Loss: 1.6118, Train: 0.4777, Test: 0.4624\n",
            "Early stopping:  0.08735262351582164\n",
            "Epoch: 015, Loss: 1.5591, Train: 0.4915, Test: 0.4775\n",
            "Early stopping:  0.08596968356439524\n",
            "Epoch: 016, Loss: 1.5074, Train: 0.5117, Test: 0.4959\n",
            "Early stopping:  0.08399360959445638\n",
            "Epoch: 017, Loss: 1.4574, Train: 0.5342, Test: 0.5169\n",
            "Early stopping:  0.08224657694791859\n",
            "Epoch: 018, Loss: 1.4108, Train: 0.5580, Test: 0.5325\n",
            "Early stopping:  0.0796673009813425\n",
            "Epoch: 019, Loss: 1.3698, Train: 0.5747, Test: 0.5447\n",
            "Early stopping:  0.07519673783411487\n",
            "Epoch: 020, Loss: 1.3310, Train: 0.5871, Test: 0.5568\n",
            "Early stopping:  0.0697540865213952\n",
            "Epoch: 021, Loss: 1.2925, Train: 0.5985, Test: 0.5648\n",
            "Early stopping:  0.0648276828486642\n",
            "Epoch: 022, Loss: 1.2572, Train: 0.6047, Test: 0.5770\n",
            "Early stopping:  0.06081415643043323\n",
            "Epoch: 023, Loss: 1.2247, Train: 0.6125, Test: 0.5846\n",
            "Early stopping:  0.057595104489010104\n",
            "Epoch: 024, Loss: 1.1959, Train: 0.6202, Test: 0.5909\n",
            "Early stopping:  0.05350997243504274\n",
            "Epoch: 025, Loss: 1.1677, Train: 0.6289, Test: 0.6005\n",
            "Early stopping:  0.049198578400362424\n",
            "Epoch: 026, Loss: 1.1402, Train: 0.6363, Test: 0.6019\n",
            "Early stopping:  0.04604326977188177\n",
            "Epoch: 027, Loss: 1.1142, Train: 0.6442, Test: 0.6102\n",
            "Early stopping:  0.04378071693334135\n",
            "Epoch: 028, Loss: 1.0897, Train: 0.6541, Test: 0.6161\n",
            "Early stopping:  0.04208844302764553\n",
            "Epoch: 029, Loss: 1.0665, Train: 0.6628, Test: 0.6212\n",
            "Early stopping:  0.04002574384436604\n",
            "Epoch: 030, Loss: 1.0427, Train: 0.6715, Test: 0.6323\n",
            "Early stopping:  0.03837756800483589\n",
            "Epoch: 031, Loss: 1.0190, Train: 0.6832, Test: 0.6323\n",
            "Early stopping:  0.03753527086057932\n",
            "Epoch: 032, Loss: 0.9969, Train: 0.6913, Test: 0.6382\n",
            "Early stopping:  0.036846798877641626\n",
            "Epoch: 033, Loss: 0.9762, Train: 0.6972, Test: 0.6436\n",
            "Early stopping:  0.03580317495112254\n",
            "Epoch: 034, Loss: 0.9550, Train: 0.7064, Test: 0.6493\n",
            "Early stopping:  0.03449555968950547\n",
            "Epoch: 035, Loss: 0.9345, Train: 0.7198, Test: 0.6541\n",
            "Early stopping:  0.03332930867877497\n",
            "Epoch: 036, Loss: 0.9154, Train: 0.7289, Test: 0.6578\n",
            "Early stopping:  0.03236109597079654\n",
            "Epoch: 037, Loss: 0.8972, Train: 0.7371, Test: 0.6609\n",
            "Early stopping:  0.03125411640071863\n",
            "Epoch: 038, Loss: 0.8794, Train: 0.7465, Test: 0.6663\n",
            "Early stopping:  0.029831379363368257\n",
            "Epoch: 039, Loss: 0.8619, Train: 0.7505, Test: 0.6669\n",
            "Early stopping:  0.028688205651914725\n",
            "Epoch: 040, Loss: 0.8452, Train: 0.7565, Test: 0.6728\n",
            "Early stopping:  0.027805374818364845\n",
            "Epoch: 041, Loss: 0.8286, Train: 0.7656, Test: 0.6771\n",
            "Early stopping:  0.027107160710425918\n",
            "Epoch: 042, Loss: 0.8118, Train: 0.7707, Test: 0.6771\n",
            "Early stopping:  0.026629788193805702\n",
            "Epoch: 043, Loss: 0.7956, Train: 0.7764, Test: 0.6816\n",
            "Early stopping:  0.026250023664582234\n",
            "Epoch: 044, Loss: 0.7793, Train: 0.7803, Test: 0.6822\n",
            "Early stopping:  0.02604881578551975\n",
            "Epoch: 045, Loss: 0.7633, Train: 0.7864, Test: 0.6847\n",
            "Early stopping:  0.02578160653602436\n",
            "Epoch: 046, Loss: 0.7477, Train: 0.7902, Test: 0.6878\n",
            "Early stopping:  0.02536018400597108\n",
            "Epoch: 047, Loss: 0.7323, Train: 0.7966, Test: 0.6912\n",
            "Early stopping:  0.024995676353930962\n",
            "Epoch: 048, Loss: 0.7168, Train: 0.8028, Test: 0.6944\n",
            "Early stopping:  0.0246594330833034\n",
            "Epoch: 049, Loss: 0.7019, Train: 0.8112, Test: 0.6963\n",
            "Early stopping:  0.02430771589952661\n",
            "Epoch: 050, Loss: 0.6871, Train: 0.8147, Test: 0.6995\n",
            "Early stopping:  0.023988135534640614\n",
            "Epoch: 051, Loss: 0.6724, Train: 0.8202, Test: 0.7009\n",
            "Early stopping:  0.023638904175236973\n",
            "Epoch: 052, Loss: 0.6581, Train: 0.8240, Test: 0.7043\n",
            "Early stopping:  0.02324605078243894\n",
            "Epoch: 053, Loss: 0.6440, Train: 0.8297, Test: 0.7074\n",
            "Early stopping:  0.0228910197540396\n",
            "Epoch: 054, Loss: 0.6301, Train: 0.8329, Test: 0.7091\n",
            "Early stopping:  0.022522380353592504\n",
            "Epoch: 055, Loss: 0.6164, Train: 0.8378, Test: 0.7156\n",
            "Early stopping:  0.02213922249660311\n",
            "Epoch: 056, Loss: 0.6032, Train: 0.8425, Test: 0.7142\n",
            "Early stopping:  0.02172211260650806\n",
            "Epoch: 057, Loss: 0.5905, Train: 0.8437, Test: 0.7182\n",
            "Early stopping:  0.02119163025504421\n",
            "Epoch: 058, Loss: 0.5791, Train: 0.8474, Test: 0.7190\n",
            "Early stopping:  0.02022577847200608\n",
            "Epoch: 059, Loss: 0.5685, Train: 0.8503, Test: 0.7264\n",
            "Early stopping:  0.018980188650130077\n",
            "Epoch: 060, Loss: 0.5578, Train: 0.8582, Test: 0.7224\n",
            "Early stopping:  0.017830107882207066\n",
            "Epoch: 061, Loss: 0.5430, Train: 0.8603, Test: 0.7309\n",
            "Early stopping:  0.018419098375414607\n",
            "Epoch: 062, Loss: 0.5305, Train: 0.8665, Test: 0.7253\n",
            "Early stopping:  0.019441844827180043\n",
            "Epoch: 063, Loss: 0.5269, Train: 0.8454, Test: 0.7244\n",
            "Early stopping:  0.017713230962898084\n",
            "Epoch: 064, Loss: 0.5384, Train: 0.8679, Test: 0.7151\n",
            "Early stopping:  0.012134747814867747\n",
            "Epoch: 065, Loss: 0.5306, Train: 0.8694, Test: 0.7392\n",
            "Early stopping:  0.006595726640959928\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.74      0.79      0.76       379\n",
            "         capital_goods       0.66      0.57      0.61       254\n",
            "conglomerates_industry       1.00      0.47      0.64        40\n",
            "     consumer_cyclical       0.73      0.66      0.70       396\n",
            " consumer_non-cyclical       0.71      0.61      0.66       223\n",
            "                energy       0.82      0.77      0.79       141\n",
            "             financial       0.86      0.72      0.79       384\n",
            "            healthcare       0.80      0.72      0.76       159\n",
            "              services       0.68      0.85      0.76      1038\n",
            "            technology       0.71      0.59      0.64       198\n",
            "        transportation       0.85      0.81      0.83       202\n",
            "             utilities       0.89      0.75      0.82       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.79      0.69      0.73      3527\n",
            "          weighted avg       0.75      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4681, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2415, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1602411379266608\n",
            "Epoch: 003, Loss: 2.1470, Train: 0.2964, Test: 0.2952\n",
            "Early stopping:  0.16501599949348703\n",
            "Epoch: 004, Loss: 2.1057, Train: 0.3348, Test: 0.3297\n",
            "Early stopping:  0.16198191587530805\n",
            "Epoch: 005, Loss: 2.0355, Train: 0.3743, Test: 0.3666\n",
            "Early stopping:  0.16759896146121223\n",
            "Epoch: 006, Loss: 1.9658, Train: 0.3875, Test: 0.3785\n",
            "Early stopping:  0.10541370530649251\n",
            "Epoch: 007, Loss: 1.9039, Train: 0.3928, Test: 0.3816\n",
            "Early stopping:  0.0993559813177259\n",
            "Epoch: 008, Loss: 1.8481, Train: 0.4049, Test: 0.3916\n",
            "Early stopping:  0.10239872813157595\n",
            "Epoch: 009, Loss: 1.7871, Train: 0.4267, Test: 0.4105\n",
            "Early stopping:  0.09723038348418495\n",
            "Epoch: 010, Loss: 1.7214, Train: 0.4501, Test: 0.4307\n",
            "Early stopping:  0.09576428286217209\n",
            "Epoch: 011, Loss: 1.6610, Train: 0.4722, Test: 0.4536\n",
            "Early stopping:  0.0968754302287464\n",
            "Epoch: 012, Loss: 1.6037, Train: 0.4892, Test: 0.4701\n",
            "Early stopping:  0.09724852801849929\n",
            "Epoch: 013, Loss: 1.5450, Train: 0.5078, Test: 0.4897\n",
            "Early stopping:  0.09518464236443377\n",
            "Epoch: 014, Loss: 1.4888, Train: 0.5295, Test: 0.5075\n",
            "Early stopping:  0.09189948165234554\n",
            "Epoch: 015, Loss: 1.4373, Train: 0.5541, Test: 0.5344\n",
            "Early stopping:  0.08892998297079309\n",
            "Epoch: 016, Loss: 1.3917, Train: 0.5777, Test: 0.5523\n",
            "Early stopping:  0.08419973949931091\n",
            "Epoch: 017, Loss: 1.3498, Train: 0.5913, Test: 0.5634\n",
            "Early stopping:  0.07723261263716084\n",
            "Epoch: 018, Loss: 1.3076, Train: 0.6017, Test: 0.5727\n",
            "Early stopping:  0.07120805793929592\n",
            "Epoch: 019, Loss: 1.2678, Train: 0.6106, Test: 0.5826\n",
            "Early stopping:  0.06692794428203931\n",
            "Epoch: 020, Loss: 1.2325, Train: 0.6248, Test: 0.5926\n",
            "Early stopping:  0.06333049399765431\n",
            "Epoch: 021, Loss: 1.2003, Train: 0.6318, Test: 0.5971\n",
            "Early stopping:  0.05923182601117702\n",
            "Epoch: 022, Loss: 1.1694, Train: 0.6371, Test: 0.6016\n",
            "Early stopping:  0.05442992767317931\n",
            "Epoch: 023, Loss: 1.1401, Train: 0.6444, Test: 0.6087\n",
            "Early stopping:  0.05039143742457398\n",
            "Epoch: 024, Loss: 1.1141, Train: 0.6505, Test: 0.6147\n",
            "Early stopping:  0.04702366862713681\n",
            "Epoch: 025, Loss: 1.0881, Train: 0.6599, Test: 0.6218\n",
            "Early stopping:  0.044294153098034736\n",
            "Epoch: 026, Loss: 1.0625, Train: 0.6660, Test: 0.6266\n",
            "Early stopping:  0.04205197941680409\n",
            "Epoch: 027, Loss: 1.0393, Train: 0.6745, Test: 0.6320\n",
            "Early stopping:  0.040040574542735456\n",
            "Epoch: 028, Loss: 1.0172, Train: 0.6843, Test: 0.6365\n",
            "Early stopping:  0.038387955113733226\n",
            "Epoch: 029, Loss: 0.9934, Train: 0.6951, Test: 0.6408\n",
            "Early stopping:  0.03711257686578364\n",
            "Epoch: 030, Loss: 0.9714, Train: 0.7015, Test: 0.6450\n",
            "Early stopping:  0.036066761764995715\n",
            "Epoch: 031, Loss: 0.9510, Train: 0.7081, Test: 0.6467\n",
            "Early stopping:  0.03515118696127344\n",
            "Epoch: 032, Loss: 0.9300, Train: 0.7180, Test: 0.6496\n",
            "Early stopping:  0.03428922535588655\n",
            "Epoch: 033, Loss: 0.9100, Train: 0.7282, Test: 0.6541\n",
            "Early stopping:  0.032943117360143125\n",
            "Epoch: 034, Loss: 0.8914, Train: 0.7369, Test: 0.6561\n",
            "Early stopping:  0.03180209948488618\n",
            "Epoch: 035, Loss: 0.8728, Train: 0.7457, Test: 0.6601\n",
            "Early stopping:  0.03085669625775276\n",
            "Epoch: 036, Loss: 0.8541, Train: 0.7522, Test: 0.6657\n",
            "Early stopping:  0.02988075568684862\n",
            "Epoch: 037, Loss: 0.8366, Train: 0.7573, Test: 0.6697\n",
            "Early stopping:  0.02911989248561883\n",
            "Epoch: 038, Loss: 0.8189, Train: 0.7645, Test: 0.6756\n",
            "Early stopping:  0.02866692067173202\n",
            "Epoch: 039, Loss: 0.8014, Train: 0.7667, Test: 0.6802\n",
            "Early stopping:  0.02815571455136937\n",
            "Epoch: 040, Loss: 0.7844, Train: 0.7720, Test: 0.6822\n",
            "Early stopping:  0.02760936035055156\n",
            "Epoch: 041, Loss: 0.7676, Train: 0.7807, Test: 0.6867\n",
            "Early stopping:  0.027271413278307167\n",
            "Epoch: 042, Loss: 0.7511, Train: 0.7847, Test: 0.6915\n",
            "Early stopping:  0.026773122349263618\n",
            "Epoch: 043, Loss: 0.7348, Train: 0.7926, Test: 0.6924\n",
            "Early stopping:  0.026314581817153213\n",
            "Epoch: 044, Loss: 0.7187, Train: 0.7989, Test: 0.6958\n",
            "Early stopping:  0.02595549513200519\n",
            "Epoch: 045, Loss: 0.7031, Train: 0.8042, Test: 0.7023\n",
            "Early stopping:  0.025527069280690234\n",
            "Epoch: 046, Loss: 0.6874, Train: 0.8060, Test: 0.7048\n",
            "Early stopping:  0.02516018130081777\n",
            "Epoch: 047, Loss: 0.6724, Train: 0.8161, Test: 0.7097\n",
            "Early stopping:  0.024675961374840842\n",
            "Epoch: 048, Loss: 0.6578, Train: 0.8157, Test: 0.7068\n",
            "Early stopping:  0.024090454249335386\n",
            "Epoch: 049, Loss: 0.6444, Train: 0.8255, Test: 0.7071\n",
            "Early stopping:  0.023217899637405462\n",
            "Epoch: 050, Loss: 0.6335, Train: 0.8204, Test: 0.7102\n",
            "Early stopping:  0.021504299370067335\n",
            "Epoch: 051, Loss: 0.6241, Train: 0.8348, Test: 0.7094\n",
            "Early stopping:  0.019188932083263402\n",
            "Epoch: 052, Loss: 0.6096, Train: 0.8367, Test: 0.7153\n",
            "Early stopping:  0.018494835618683942\n",
            "Epoch: 053, Loss: 0.5920, Train: 0.8380, Test: 0.7213\n",
            "Early stopping:  0.020543899989327923\n",
            "Epoch: 054, Loss: 0.5820, Train: 0.8440, Test: 0.7153\n",
            "Early stopping:  0.021472090516431157\n",
            "Epoch: 055, Loss: 0.5731, Train: 0.8480, Test: 0.7238\n",
            "Early stopping:  0.020699702478210513\n",
            "Epoch: 056, Loss: 0.5581, Train: 0.8467, Test: 0.7272\n",
            "Early stopping:  0.019412023921443464\n",
            "Epoch: 057, Loss: 0.5480, Train: 0.8558, Test: 0.7210\n",
            "Early stopping:  0.017732337707827298\n",
            "Epoch: 058, Loss: 0.5390, Train: 0.8571, Test: 0.7329\n",
            "Early stopping:  0.017629203010221948\n",
            "Epoch: 059, Loss: 0.5249, Train: 0.8618, Test: 0.7318\n",
            "Early stopping:  0.018338030730394736\n",
            "Epoch: 060, Loss: 0.5146, Train: 0.8654, Test: 0.7261\n",
            "Early stopping:  0.01748522978505092\n",
            "Epoch: 061, Loss: 0.5080, Train: 0.8681, Test: 0.7352\n",
            "Early stopping:  0.016623972244324492\n",
            "Epoch: 062, Loss: 0.4975, Train: 0.8718, Test: 0.7335\n",
            "Early stopping:  0.015916254446209838\n",
            "Epoch: 063, Loss: 0.4861, Train: 0.8766, Test: 0.7304\n",
            "Early stopping:  0.015020922890454166\n",
            "Epoch: 064, Loss: 0.4796, Train: 0.8737, Test: 0.7377\n",
            "Early stopping:  0.014586525822165427\n",
            "Epoch: 065, Loss: 0.4704, Train: 0.8845, Test: 0.7380\n",
            "Early stopping:  0.014790172994385146\n",
            "Epoch: 066, Loss: 0.4588, Train: 0.8877, Test: 0.7324\n",
            "Early stopping:  0.014771290147220653\n",
            "Epoch: 067, Loss: 0.4519, Train: 0.8841, Test: 0.7394\n",
            "Early stopping:  0.014153228319363612\n",
            "Epoch: 068, Loss: 0.4462, Train: 0.8913, Test: 0.7363\n",
            "Early stopping:  0.013608857381949484\n",
            "Epoch: 069, Loss: 0.4361, Train: 0.8987, Test: 0.7335\n",
            "Early stopping:  0.012933323217333289\n",
            "Epoch: 070, Loss: 0.4300, Train: 0.8885, Test: 0.7409\n",
            "Early stopping:  0.01167890188147734\n",
            "Epoch: 071, Loss: 0.4263, Train: 0.9032, Test: 0.7335\n",
            "Early stopping:  0.010813417918461665\n",
            "Epoch: 072, Loss: 0.4175, Train: 0.9011, Test: 0.7400\n",
            "Early stopping:  0.010767146232142486\n",
            "Epoch: 073, Loss: 0.4056, Train: 0.9036, Test: 0.7465\n",
            "Early stopping:  0.011882594318443362\n",
            "Epoch: 074, Loss: 0.3977, Train: 0.9123, Test: 0.7375\n",
            "Early stopping:  0.013639645215133573\n",
            "Epoch: 075, Loss: 0.3892, Train: 0.9134, Test: 0.7443\n",
            "Early stopping:  0.01489259724392911\n",
            "Epoch: 076, Loss: 0.3822, Train: 0.9121, Test: 0.7448\n",
            "Early stopping:  0.013818743990742217\n",
            "Epoch: 077, Loss: 0.3799, Train: 0.9153, Test: 0.7403\n",
            "Early stopping:  0.010729922031316792\n",
            "Epoch: 078, Loss: 0.3742, Train: 0.9187, Test: 0.7448\n",
            "Early stopping:  0.009068921899032287\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.74      0.80      0.77       379\n",
            "         capital_goods       0.67      0.68      0.68       254\n",
            "conglomerates_industry       1.00      0.57      0.73        40\n",
            "     consumer_cyclical       0.67      0.72      0.69       396\n",
            " consumer_non-cyclical       0.74      0.69      0.71       223\n",
            "                energy       0.87      0.76      0.81       141\n",
            "             financial       0.85      0.73      0.79       384\n",
            "            healthcare       0.83      0.76      0.79       159\n",
            "              services       0.72      0.79      0.75      1038\n",
            "            technology       0.68      0.64      0.66       198\n",
            "        transportation       0.85      0.77      0.81       202\n",
            "             utilities       0.85      0.74      0.79       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.79      0.72      0.75      3527\n",
            "          weighted avg       0.75      0.74      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5322, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2975, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1659680517023211\n",
            "Epoch: 003, Loss: 2.2010, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.17036830924017252\n",
            "Epoch: 004, Loss: 2.1572, Train: 0.2992, Test: 0.2988\n",
            "Early stopping:  0.16742231559586865\n",
            "Epoch: 005, Loss: 2.0983, Train: 0.3429, Test: 0.3397\n",
            "Early stopping:  0.17004156530448136\n",
            "Epoch: 006, Loss: 2.0490, Train: 0.3648, Test: 0.3538\n",
            "Early stopping:  0.09581968959282376\n",
            "Epoch: 007, Loss: 1.9980, Train: 0.3578, Test: 0.3521\n",
            "Early stopping:  0.08136874665341122\n",
            "Epoch: 008, Loss: 1.9446, Train: 0.3518, Test: 0.3470\n",
            "Early stopping:  0.0831486576531921\n",
            "Epoch: 009, Loss: 1.8960, Train: 0.3588, Test: 0.3538\n",
            "Early stopping:  0.08050151225924941\n",
            "Epoch: 010, Loss: 1.8503, Train: 0.3854, Test: 0.3774\n",
            "Early stopping:  0.07898583072585749\n",
            "Epoch: 011, Loss: 1.7991, Train: 0.4195, Test: 0.4103\n",
            "Early stopping:  0.07783132882877991\n",
            "Epoch: 012, Loss: 1.7481, Train: 0.4461, Test: 0.4324\n",
            "Early stopping:  0.07748702304649802\n",
            "Epoch: 013, Loss: 1.7013, Train: 0.4612, Test: 0.4457\n",
            "Early stopping:  0.07774735920674489\n",
            "Epoch: 014, Loss: 1.6504, Train: 0.4754, Test: 0.4621\n",
            "Early stopping:  0.07866481718585419\n",
            "Epoch: 015, Loss: 1.5940, Train: 0.4858, Test: 0.4755\n",
            "Early stopping:  0.08034166114945276\n",
            "Epoch: 016, Loss: 1.5408, Train: 0.5070, Test: 0.4914\n",
            "Early stopping:  0.08254584769769364\n",
            "Epoch: 017, Loss: 1.4924, Train: 0.5369, Test: 0.5180\n",
            "Early stopping:  0.08341230046412856\n",
            "Epoch: 018, Loss: 1.4433, Train: 0.5541, Test: 0.5347\n",
            "Early stopping:  0.08160692613147608\n",
            "Epoch: 019, Loss: 1.3955, Train: 0.5694, Test: 0.5472\n",
            "Early stopping:  0.07818523714047755\n",
            "Epoch: 020, Loss: 1.3508, Train: 0.5792, Test: 0.5580\n",
            "Early stopping:  0.07540975154439376\n",
            "Epoch: 021, Loss: 1.3090, Train: 0.5871, Test: 0.5654\n",
            "Early stopping:  0.07266211434112123\n",
            "Epoch: 022, Loss: 1.2730, Train: 0.5985, Test: 0.5736\n",
            "Early stopping:  0.0676314468877029\n",
            "Epoch: 023, Loss: 1.2409, Train: 0.6095, Test: 0.5852\n",
            "Early stopping:  0.06134492986651342\n",
            "Epoch: 024, Loss: 1.2095, Train: 0.6208, Test: 0.5926\n",
            "Early stopping:  0.055573942924754365\n",
            "Epoch: 025, Loss: 1.1813, Train: 0.6304, Test: 0.5971\n",
            "Early stopping:  0.050479544379087735\n",
            "Epoch: 026, Loss: 1.1554, Train: 0.6355, Test: 0.6065\n",
            "Early stopping:  0.04668567423439171\n",
            "Epoch: 027, Loss: 1.1295, Train: 0.6423, Test: 0.6090\n",
            "Early stopping:  0.04383078995038869\n",
            "Epoch: 028, Loss: 1.1040, Train: 0.6476, Test: 0.6104\n",
            "Early stopping:  0.04155867101405701\n",
            "Epoch: 029, Loss: 1.0800, Train: 0.6541, Test: 0.6158\n",
            "Early stopping:  0.0401515633869479\n",
            "Epoch: 030, Loss: 1.0576, Train: 0.6662, Test: 0.6218\n",
            "Early stopping:  0.03875895083711641\n",
            "Epoch: 031, Loss: 1.0353, Train: 0.6773, Test: 0.6277\n",
            "Early stopping:  0.03712091999745071\n",
            "Epoch: 032, Loss: 1.0138, Train: 0.6807, Test: 0.6277\n",
            "Early stopping:  0.0355761062198788\n",
            "Epoch: 033, Loss: 0.9926, Train: 0.6917, Test: 0.6348\n",
            "Early stopping:  0.03454824264605388\n",
            "Epoch: 034, Loss: 0.9717, Train: 0.6998, Test: 0.6377\n",
            "Early stopping:  0.03392563671966767\n",
            "Epoch: 035, Loss: 0.9512, Train: 0.7070, Test: 0.6436\n",
            "Early stopping:  0.033272529130211476\n",
            "Epoch: 036, Loss: 0.9316, Train: 0.7161, Test: 0.6484\n",
            "Early stopping:  0.03254091791352491\n",
            "Epoch: 037, Loss: 0.9116, Train: 0.7242, Test: 0.6479\n",
            "Early stopping:  0.03196412965096555\n",
            "Epoch: 038, Loss: 0.8930, Train: 0.7329, Test: 0.6544\n",
            "Early stopping:  0.031127233955779408\n",
            "Epoch: 039, Loss: 0.8753, Train: 0.7391, Test: 0.6581\n",
            "Early stopping:  0.030122151919456316\n",
            "Epoch: 040, Loss: 0.8579, Train: 0.7456, Test: 0.6615\n",
            "Early stopping:  0.029072624860715556\n",
            "Epoch: 041, Loss: 0.8401, Train: 0.7533, Test: 0.6669\n",
            "Early stopping:  0.028156255856086245\n",
            "Epoch: 042, Loss: 0.8231, Train: 0.7609, Test: 0.6717\n",
            "Early stopping:  0.02768499338861256\n",
            "Epoch: 043, Loss: 0.8064, Train: 0.7694, Test: 0.6754\n",
            "Early stopping:  0.027294980675670124\n",
            "Epoch: 044, Loss: 0.7894, Train: 0.7764, Test: 0.6768\n",
            "Early stopping:  0.02700720624328292\n",
            "Epoch: 045, Loss: 0.7730, Train: 0.7820, Test: 0.6790\n",
            "Early stopping:  0.026566754502885113\n",
            "Epoch: 046, Loss: 0.7568, Train: 0.7885, Test: 0.6830\n",
            "Early stopping:  0.026244094377862643\n",
            "Epoch: 047, Loss: 0.7406, Train: 0.7936, Test: 0.6847\n",
            "Early stopping:  0.025948585192185826\n",
            "Epoch: 048, Loss: 0.7246, Train: 0.7979, Test: 0.6856\n",
            "Early stopping:  0.025590574191617373\n",
            "Epoch: 049, Loss: 0.7084, Train: 0.8034, Test: 0.6915\n",
            "Early stopping:  0.025500800266856305\n",
            "Epoch: 050, Loss: 0.6925, Train: 0.8074, Test: 0.6969\n",
            "Early stopping:  0.0254108821406136\n",
            "Epoch: 051, Loss: 0.6770, Train: 0.8136, Test: 0.6997\n",
            "Early stopping:  0.025173732955662186\n",
            "Epoch: 052, Loss: 0.6616, Train: 0.8189, Test: 0.7003\n",
            "Early stopping:  0.024894188428601915\n",
            "Epoch: 053, Loss: 0.6464, Train: 0.8229, Test: 0.7046\n",
            "Early stopping:  0.02451022916048759\n",
            "Epoch: 054, Loss: 0.6322, Train: 0.8278, Test: 0.7026\n",
            "Early stopping:  0.02393079461619782\n",
            "Epoch: 055, Loss: 0.6220, Train: 0.8142, Test: 0.7031\n",
            "Early stopping:  0.022113863338886184\n",
            "Epoch: 056, Loss: 0.6223, Train: 0.8285, Test: 0.6949\n",
            "Early stopping:  0.016995615054123257\n",
            "Epoch: 057, Loss: 0.6129, Train: 0.8344, Test: 0.7085\n",
            "Early stopping:  0.012731882867575513\n",
            "Epoch: 058, Loss: 0.5830, Train: 0.8301, Test: 0.7122\n",
            "Early stopping:  0.018862885905541624\n",
            "Epoch: 059, Loss: 0.5815, Train: 0.8467, Test: 0.7012\n",
            "Early stopping:  0.020507208833426833\n",
            "Epoch: 060, Loss: 0.5720, Train: 0.8497, Test: 0.7111\n",
            "Early stopping:  0.021894778856494542\n",
            "Epoch: 061, Loss: 0.5509, Train: 0.8399, Test: 0.7139\n",
            "Early stopping:  0.022390194185190275\n",
            "Epoch: 062, Loss: 0.5535, Train: 0.8586, Test: 0.7134\n",
            "Early stopping:  0.01524528596759332\n",
            "Epoch: 063, Loss: 0.5328, Train: 0.8639, Test: 0.7097\n",
            "Early stopping:  0.019100777038741346\n",
            "Epoch: 064, Loss: 0.5277, Train: 0.8529, Test: 0.7202\n",
            "Early stopping:  0.017736865476676435\n",
            "Epoch: 065, Loss: 0.5192, Train: 0.8595, Test: 0.7204\n",
            "Early stopping:  0.014879102509384245\n",
            "Epoch: 066, Loss: 0.5048, Train: 0.8701, Test: 0.7128\n",
            "Early stopping:  0.017934387247445442\n",
            "Epoch: 067, Loss: 0.5033, Train: 0.8713, Test: 0.7210\n",
            "Early stopping:  0.013239806073377073\n",
            "Epoch: 068, Loss: 0.4880, Train: 0.8658, Test: 0.7224\n",
            "Early stopping:  0.015353875806800869\n",
            "Epoch: 069, Loss: 0.4837, Train: 0.8802, Test: 0.7168\n",
            "Early stopping:  0.014255059132115708\n",
            "Epoch: 070, Loss: 0.4749, Train: 0.8843, Test: 0.7196\n",
            "Early stopping:  0.012875567910162072\n",
            "Epoch: 071, Loss: 0.4646, Train: 0.8779, Test: 0.7233\n",
            "Early stopping:  0.01449804748058899\n",
            "Epoch: 072, Loss: 0.4594, Train: 0.8879, Test: 0.7250\n",
            "Early stopping:  0.012171880499905863\n",
            "Epoch: 073, Loss: 0.4504, Train: 0.8930, Test: 0.7193\n",
            "Early stopping:  0.013054408241655154\n",
            "Epoch: 074, Loss: 0.4418, Train: 0.8909, Test: 0.7230\n",
            "Early stopping:  0.01277204913682934\n",
            "Epoch: 075, Loss: 0.4356, Train: 0.8934, Test: 0.7284\n",
            "Early stopping:  0.011999727559240914\n",
            "Epoch: 076, Loss: 0.4282, Train: 0.9008, Test: 0.7255\n",
            "Early stopping:  0.012205674870703986\n",
            "Epoch: 077, Loss: 0.4190, Train: 0.9025, Test: 0.7275\n",
            "Early stopping:  0.01208129782043922\n",
            "Epoch: 078, Loss: 0.4142, Train: 0.9034, Test: 0.7318\n",
            "Early stopping:  0.011398085161544596\n",
            "Epoch: 079, Loss: 0.4066, Train: 0.9064, Test: 0.7301\n",
            "Early stopping:  0.011436086846841751\n",
            "Epoch: 080, Loss: 0.3982, Train: 0.9119, Test: 0.7272\n",
            "Early stopping:  0.011503588051567905\n",
            "Epoch: 081, Loss: 0.3928, Train: 0.9132, Test: 0.7326\n",
            "Early stopping:  0.010870282635060076\n",
            "Epoch: 082, Loss: 0.3858, Train: 0.9108, Test: 0.7355\n",
            "Early stopping:  0.011192719655947278\n",
            "Epoch: 083, Loss: 0.3795, Train: 0.9198, Test: 0.7289\n",
            "Early stopping:  0.010543527889485791\n",
            "Epoch: 084, Loss: 0.3724, Train: 0.9197, Test: 0.7335\n",
            "Early stopping:  0.010286953076527156\n",
            "Epoch: 085, Loss: 0.3649, Train: 0.9180, Test: 0.7366\n",
            "Early stopping:  0.01095028473293626\n",
            "Epoch: 086, Loss: 0.3599, Train: 0.9259, Test: 0.7346\n",
            "Early stopping:  0.010534528453917124\n",
            "Epoch: 087, Loss: 0.3541, Train: 0.9225, Test: 0.7329\n",
            "Early stopping:  0.010040613122963843\n",
            "Epoch: 088, Loss: 0.3487, Train: 0.9278, Test: 0.7360\n",
            "Early stopping:  0.009204997983029718\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.78      0.75       379\n",
            "         capital_goods       0.64      0.69      0.67       254\n",
            "conglomerates_industry       1.00      0.68      0.81        40\n",
            "     consumer_cyclical       0.69      0.69      0.69       396\n",
            " consumer_non-cyclical       0.71      0.65      0.68       223\n",
            "                energy       0.83      0.74      0.79       141\n",
            "             financial       0.84      0.74      0.78       384\n",
            "            healthcare       0.83      0.73      0.78       159\n",
            "              services       0.72      0.78      0.75      1038\n",
            "            technology       0.67      0.64      0.65       198\n",
            "        transportation       0.81      0.76      0.79       202\n",
            "             utilities       0.78      0.73      0.75       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.77      0.72      0.74      3527\n",
            "          weighted avg       0.74      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4854, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2722, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.15075354515207048\n",
            "Epoch: 003, Loss: 2.1761, Train: 0.2943, Test: 0.2937\n",
            "Early stopping:  0.15832416469055455\n",
            "Epoch: 004, Loss: 2.1278, Train: 0.3051, Test: 0.3037\n",
            "Early stopping:  0.15851142560262818\n",
            "Epoch: 005, Loss: 2.0631, Train: 0.3520, Test: 0.3496\n",
            "Early stopping:  0.16439771265077693\n",
            "Epoch: 006, Loss: 2.0110, Train: 0.3773, Test: 0.3723\n",
            "Early stopping:  0.10122991736226598\n",
            "Epoch: 007, Loss: 1.9574, Train: 0.3777, Test: 0.3706\n",
            "Early stopping:  0.08768166760344089\n",
            "Epoch: 008, Loss: 1.8987, Train: 0.3732, Test: 0.3689\n",
            "Early stopping:  0.08921106888567346\n",
            "Epoch: 009, Loss: 1.8425, Train: 0.3824, Test: 0.3754\n",
            "Early stopping:  0.08753241273209034\n",
            "Epoch: 010, Loss: 1.7895, Train: 0.4030, Test: 0.3964\n",
            "Early stopping:  0.08823667731171667\n",
            "Epoch: 011, Loss: 1.7331, Train: 0.4361, Test: 0.4256\n",
            "Early stopping:  0.08823141989041977\n",
            "Epoch: 012, Loss: 1.6773, Train: 0.4647, Test: 0.4457\n",
            "Early stopping:  0.08733028133156254\n",
            "Epoch: 013, Loss: 1.6241, Train: 0.4813, Test: 0.4726\n",
            "Early stopping:  0.08680825319162916\n",
            "Epoch: 014, Loss: 1.5705, Train: 0.5009, Test: 0.4911\n",
            "Early stopping:  0.08648092398694333\n",
            "Epoch: 015, Loss: 1.5197, Train: 0.5197, Test: 0.5033\n",
            "Early stopping:  0.08435623865860409\n",
            "Epoch: 016, Loss: 1.4726, Train: 0.5357, Test: 0.5203\n",
            "Early stopping:  0.08126355165425032\n",
            "Epoch: 017, Loss: 1.4250, Train: 0.5484, Test: 0.5293\n",
            "Early stopping:  0.07846746493228887\n",
            "Epoch: 018, Loss: 1.3795, Train: 0.5612, Test: 0.5401\n",
            "Early stopping:  0.07538723191991564\n",
            "Epoch: 019, Loss: 1.3385, Train: 0.5722, Test: 0.5532\n",
            "Early stopping:  0.07205360323895062\n",
            "Epoch: 020, Loss: 1.2989, Train: 0.5877, Test: 0.5668\n",
            "Early stopping:  0.06864104201068776\n",
            "Epoch: 021, Loss: 1.2619, Train: 0.6015, Test: 0.5787\n",
            "Early stopping:  0.06435785843132664\n",
            "Epoch: 022, Loss: 1.2280, Train: 0.6104, Test: 0.5880\n",
            "Early stopping:  0.06005428634512691\n",
            "Epoch: 023, Loss: 1.1969, Train: 0.6210, Test: 0.5912\n",
            "Early stopping:  0.05606321535068591\n",
            "Epoch: 024, Loss: 1.1680, Train: 0.6276, Test: 0.6031\n",
            "Early stopping:  0.0517351416464384\n",
            "Epoch: 025, Loss: 1.1385, Train: 0.6376, Test: 0.6087\n",
            "Early stopping:  0.048550833568676885\n",
            "Epoch: 026, Loss: 1.1115, Train: 0.6439, Test: 0.6153\n",
            "Early stopping:  0.046103193116925525\n",
            "Epoch: 027, Loss: 1.0864, Train: 0.6537, Test: 0.6170\n",
            "Early stopping:  0.04390384149309319\n",
            "Epoch: 028, Loss: 1.0614, Train: 0.6631, Test: 0.6223\n",
            "Early stopping:  0.041964972919545984\n",
            "Epoch: 029, Loss: 1.0372, Train: 0.6701, Test: 0.6283\n",
            "Early stopping:  0.03994762827606896\n",
            "Epoch: 030, Loss: 1.0150, Train: 0.6798, Test: 0.6334\n",
            "Early stopping:  0.03830341978190619\n",
            "Epoch: 031, Loss: 0.9931, Train: 0.6896, Test: 0.6396\n",
            "Early stopping:  0.03686343615990238\n",
            "Epoch: 032, Loss: 0.9715, Train: 0.6981, Test: 0.6481\n",
            "Early stopping:  0.035424493884638246\n",
            "Epoch: 033, Loss: 0.9509, Train: 0.7057, Test: 0.6504\n",
            "Early stopping:  0.03416191036195595\n",
            "Epoch: 034, Loss: 0.9314, Train: 0.7176, Test: 0.6527\n",
            "Early stopping:  0.033125646107021296\n",
            "Epoch: 035, Loss: 0.9125, Train: 0.7276, Test: 0.6589\n",
            "Early stopping:  0.031856796097210476\n",
            "Epoch: 036, Loss: 0.8936, Train: 0.7346, Test: 0.6646\n",
            "Early stopping:  0.03070947474035916\n",
            "Epoch: 037, Loss: 0.8758, Train: 0.7405, Test: 0.6649\n",
            "Early stopping:  0.029729153694691757\n",
            "Epoch: 038, Loss: 0.8586, Train: 0.7486, Test: 0.6705\n",
            "Early stopping:  0.028827456690865866\n",
            "Epoch: 039, Loss: 0.8414, Train: 0.7569, Test: 0.6748\n",
            "Early stopping:  0.028029338066034172\n",
            "Epoch: 040, Loss: 0.8248, Train: 0.7639, Test: 0.6771\n",
            "Early stopping:  0.027201216686229785\n",
            "Epoch: 041, Loss: 0.8089, Train: 0.7718, Test: 0.6793\n",
            "Early stopping:  0.026515527272622946\n",
            "Epoch: 042, Loss: 0.7930, Train: 0.7773, Test: 0.6827\n",
            "Early stopping:  0.025882574972492915\n",
            "Epoch: 043, Loss: 0.7773, Train: 0.7836, Test: 0.6873\n",
            "Early stopping:  0.025296920609937736\n",
            "Epoch: 044, Loss: 0.7618, Train: 0.7902, Test: 0.6884\n",
            "Early stopping:  0.024910298232877184\n",
            "Epoch: 045, Loss: 0.7462, Train: 0.7962, Test: 0.6910\n",
            "Early stopping:  0.02472266736770483\n",
            "Epoch: 046, Loss: 0.7308, Train: 0.7989, Test: 0.6912\n",
            "Early stopping:  0.024565530271765616\n",
            "Epoch: 047, Loss: 0.7158, Train: 0.8047, Test: 0.6927\n",
            "Early stopping:  0.024366958082738567\n",
            "Epoch: 048, Loss: 0.7008, Train: 0.8068, Test: 0.6952\n",
            "Early stopping:  0.02413897021164407\n",
            "Epoch: 049, Loss: 0.6862, Train: 0.8121, Test: 0.6963\n",
            "Early stopping:  0.02373643855215884\n",
            "Epoch: 050, Loss: 0.6719, Train: 0.8140, Test: 0.6997\n",
            "Early stopping:  0.023313550266450905\n",
            "Epoch: 051, Loss: 0.6579, Train: 0.8219, Test: 0.7003\n",
            "Early stopping:  0.02285408316634732\n",
            "Epoch: 052, Loss: 0.6444, Train: 0.8180, Test: 0.6992\n",
            "Early stopping:  0.022308128471482714\n",
            "Epoch: 053, Loss: 0.6326, Train: 0.8291, Test: 0.7083\n",
            "Early stopping:  0.021324053559612775\n",
            "Epoch: 054, Loss: 0.6226, Train: 0.8229, Test: 0.7029\n",
            "Early stopping:  0.019637278967064294\n",
            "Epoch: 055, Loss: 0.6128, Train: 0.8372, Test: 0.7119\n",
            "Early stopping:  0.017756374636784224\n",
            "Epoch: 056, Loss: 0.5976, Train: 0.8388, Test: 0.7134\n",
            "Early stopping:  0.01798186331899303\n",
            "Epoch: 057, Loss: 0.5809, Train: 0.8405, Test: 0.7136\n",
            "Early stopping:  0.02046081972041882\n",
            "Epoch: 058, Loss: 0.5715, Train: 0.8478, Test: 0.7176\n",
            "Early stopping:  0.021300235418974252\n",
            "Epoch: 059, Loss: 0.5646, Train: 0.8478, Test: 0.7176\n",
            "Early stopping:  0.01966987332395139\n",
            "Epoch: 060, Loss: 0.5515, Train: 0.8584, Test: 0.7190\n",
            "Early stopping:  0.017331978790599733\n",
            "Epoch: 061, Loss: 0.5368, Train: 0.8616, Test: 0.7210\n",
            "Early stopping:  0.017300871157260877\n",
            "Epoch: 062, Loss: 0.5281, Train: 0.8567, Test: 0.7261\n",
            "Early stopping:  0.018220571271754527\n",
            "Epoch: 063, Loss: 0.5216, Train: 0.8701, Test: 0.7244\n",
            "Early stopping:  0.017502435094899366\n",
            "Epoch: 064, Loss: 0.5108, Train: 0.8711, Test: 0.7272\n",
            "Early stopping:  0.015435140710788197\n",
            "Epoch: 065, Loss: 0.4976, Train: 0.8733, Test: 0.7278\n",
            "Early stopping:  0.015259040406616188\n",
            "Epoch: 066, Loss: 0.4875, Train: 0.8794, Test: 0.7253\n",
            "Early stopping:  0.016736082136411493\n",
            "Epoch: 067, Loss: 0.4805, Train: 0.8739, Test: 0.7301\n",
            "Early stopping:  0.016745688383562884\n",
            "Epoch: 068, Loss: 0.4731, Train: 0.8856, Test: 0.7304\n",
            "Early stopping:  0.014756437235296618\n",
            "Epoch: 069, Loss: 0.4630, Train: 0.8890, Test: 0.7304\n",
            "Early stopping:  0.013263382617212289\n",
            "Epoch: 070, Loss: 0.4520, Train: 0.8900, Test: 0.7321\n",
            "Early stopping:  0.014068489615628345\n",
            "Epoch: 071, Loss: 0.4430, Train: 0.8974, Test: 0.7253\n",
            "Early stopping:  0.015233973561506818\n",
            "Epoch: 072, Loss: 0.4347, Train: 0.8940, Test: 0.7358\n",
            "Early stopping:  0.015317532714153662\n",
            "Epoch: 073, Loss: 0.4265, Train: 0.9038, Test: 0.7318\n",
            "Early stopping:  0.014301794962824046\n",
            "Epoch: 074, Loss: 0.4183, Train: 0.9002, Test: 0.7360\n",
            "Early stopping:  0.013257369258221975\n",
            "Epoch: 075, Loss: 0.4119, Train: 0.9060, Test: 0.7343\n",
            "Early stopping:  0.012414710026475154\n",
            "Epoch: 076, Loss: 0.4063, Train: 0.9072, Test: 0.7377\n",
            "Early stopping:  0.011316939102294572\n",
            "Epoch: 077, Loss: 0.3966, Train: 0.9132, Test: 0.7326\n",
            "Early stopping:  0.01139106966843764\n",
            "Epoch: 078, Loss: 0.3853, Train: 0.9144, Test: 0.7386\n",
            "Early stopping:  0.013027679298645197\n",
            "Epoch: 079, Loss: 0.3761, Train: 0.9191, Test: 0.7352\n",
            "Early stopping:  0.014736403959875187\n",
            "Epoch: 080, Loss: 0.3695, Train: 0.9208, Test: 0.7360\n",
            "Early stopping:  0.014940693212309451\n",
            "Epoch: 081, Loss: 0.3635, Train: 0.9225, Test: 0.7417\n",
            "Early stopping:  0.013078112392780642\n",
            "Epoch: 082, Loss: 0.3552, Train: 0.9250, Test: 0.7358\n",
            "Early stopping:  0.011545878169202694\n",
            "Epoch: 083, Loss: 0.3469, Train: 0.9242, Test: 0.7428\n",
            "Early stopping:  0.011547856163913127\n",
            "Epoch: 084, Loss: 0.3427, Train: 0.9261, Test: 0.7321\n",
            "Early stopping:  0.011179072642778962\n",
            "Epoch: 085, Loss: 0.3432, Train: 0.9129, Test: 0.7369\n",
            "Early stopping:  0.008939057459444968\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.83      0.71      0.77       379\n",
            "         capital_goods       0.69      0.59      0.64       254\n",
            "conglomerates_industry       0.96      0.60      0.74        40\n",
            "     consumer_cyclical       0.72      0.69      0.70       396\n",
            " consumer_non-cyclical       0.73      0.63      0.68       223\n",
            "                energy       0.84      0.74      0.79       141\n",
            "             financial       0.80      0.74      0.77       384\n",
            "            healthcare       0.91      0.70      0.79       159\n",
            "              services       0.66      0.85      0.75      1038\n",
            "            technology       0.68      0.56      0.61       198\n",
            "        transportation       0.83      0.80      0.82       202\n",
            "             utilities       0.85      0.73      0.78       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.79      0.70      0.74      3527\n",
            "          weighted avg       0.75      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4779, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2595, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15446095053448275\n",
            "Epoch: 003, Loss: 2.1787, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15479789372522532\n",
            "Epoch: 004, Loss: 2.1270, Train: 0.3034, Test: 0.3045\n",
            "Early stopping:  0.15468303448999837\n",
            "Epoch: 005, Loss: 2.0698, Train: 0.3518, Test: 0.3485\n",
            "Early stopping:  0.15887808233014591\n",
            "Epoch: 006, Loss: 2.0154, Train: 0.3629, Test: 0.3558\n",
            "Early stopping:  0.09473600683950908\n",
            "Epoch: 007, Loss: 1.9532, Train: 0.3573, Test: 0.3507\n",
            "Early stopping:  0.08900843059518582\n",
            "Epoch: 008, Loss: 1.8933, Train: 0.3639, Test: 0.3550\n",
            "Early stopping:  0.09235991420245394\n",
            "Epoch: 009, Loss: 1.8387, Train: 0.3911, Test: 0.3811\n",
            "Early stopping:  0.09241062859534363\n",
            "Epoch: 010, Loss: 1.7762, Train: 0.4346, Test: 0.4208\n",
            "Early stopping:  0.0937735378554983\n",
            "Epoch: 011, Loss: 1.7068, Train: 0.4716, Test: 0.4621\n",
            "Early stopping:  0.09650949051353264\n",
            "Epoch: 012, Loss: 1.6430, Train: 0.4936, Test: 0.4848\n",
            "Early stopping:  0.10010720642871701\n",
            "Epoch: 013, Loss: 1.5830, Train: 0.5062, Test: 0.4933\n",
            "Early stopping:  0.10196018968621631\n",
            "Epoch: 014, Loss: 1.5206, Train: 0.5138, Test: 0.5084\n",
            "Early stopping:  0.1004561544761815\n",
            "Epoch: 015, Loss: 1.4637, Train: 0.5348, Test: 0.5220\n",
            "Early stopping:  0.09625321571703734\n",
            "Epoch: 016, Loss: 1.4132, Train: 0.5578, Test: 0.5353\n",
            "Early stopping:  0.0915782759046462\n",
            "Epoch: 017, Loss: 1.3644, Train: 0.5752, Test: 0.5526\n",
            "Early stopping:  0.08621903558174951\n",
            "Epoch: 018, Loss: 1.3234, Train: 0.5860, Test: 0.5634\n",
            "Early stopping:  0.07819362118017917\n",
            "Epoch: 019, Loss: 1.2859, Train: 0.5947, Test: 0.5693\n",
            "Early stopping:  0.07056763529527887\n",
            "Epoch: 020, Loss: 1.2492, Train: 0.5983, Test: 0.5719\n",
            "Early stopping:  0.06438889886113318\n",
            "Epoch: 021, Loss: 1.2188, Train: 0.6066, Test: 0.5804\n",
            "Early stopping:  0.05785172446505279\n",
            "Epoch: 022, Loss: 1.1883, Train: 0.6155, Test: 0.5892\n",
            "Early stopping:  0.053419921567886185\n",
            "Epoch: 023, Loss: 1.1584, Train: 0.6251, Test: 0.6011\n",
            "Early stopping:  0.0499894874886744\n",
            "Epoch: 024, Loss: 1.1315, Train: 0.6353, Test: 0.6062\n",
            "Early stopping:  0.0467682041617944\n",
            "Epoch: 025, Loss: 1.1048, Train: 0.6395, Test: 0.6107\n",
            "Early stopping:  0.045047072900739706\n",
            "Epoch: 026, Loss: 1.0809, Train: 0.6486, Test: 0.6175\n",
            "Early stopping:  0.04246666915648937\n",
            "Epoch: 027, Loss: 1.0565, Train: 0.6637, Test: 0.6240\n",
            "Early stopping:  0.04025237641178222\n",
            "Epoch: 028, Loss: 1.0337, Train: 0.6703, Test: 0.6314\n",
            "Early stopping:  0.038587687458582126\n",
            "Epoch: 029, Loss: 1.0119, Train: 0.6779, Test: 0.6360\n",
            "Early stopping:  0.03684799562694991\n",
            "Epoch: 030, Loss: 0.9899, Train: 0.6830, Test: 0.6391\n",
            "Early stopping:  0.03584763169530594\n",
            "Epoch: 031, Loss: 0.9688, Train: 0.6955, Test: 0.6447\n",
            "Early stopping:  0.03465154085922636\n",
            "Epoch: 032, Loss: 0.9486, Train: 0.7049, Test: 0.6507\n",
            "Early stopping:  0.03372283153822636\n",
            "Epoch: 033, Loss: 0.9287, Train: 0.7144, Test: 0.6569\n",
            "Early stopping:  0.03283415003515641\n",
            "Epoch: 034, Loss: 0.9094, Train: 0.7242, Test: 0.6564\n",
            "Early stopping:  0.03178352989487043\n",
            "Epoch: 035, Loss: 0.8909, Train: 0.7308, Test: 0.6603\n",
            "Early stopping:  0.030837461246422747\n",
            "Epoch: 036, Loss: 0.8725, Train: 0.7416, Test: 0.6640\n",
            "Early stopping:  0.030044418708455606\n",
            "Epoch: 037, Loss: 0.8546, Train: 0.7474, Test: 0.6680\n",
            "Early stopping:  0.029275908522578366\n",
            "Epoch: 038, Loss: 0.8372, Train: 0.7533, Test: 0.6694\n",
            "Early stopping:  0.028591363226723572\n",
            "Epoch: 039, Loss: 0.8203, Train: 0.7603, Test: 0.6737\n",
            "Early stopping:  0.027903889820839346\n",
            "Epoch: 040, Loss: 0.8032, Train: 0.7669, Test: 0.6756\n",
            "Early stopping:  0.02733384869225436\n",
            "Epoch: 041, Loss: 0.7867, Train: 0.7769, Test: 0.6802\n",
            "Early stopping:  0.026843004279527627\n",
            "Epoch: 042, Loss: 0.7702, Train: 0.7779, Test: 0.6813\n",
            "Early stopping:  0.026481186152186288\n",
            "Epoch: 043, Loss: 0.7540, Train: 0.7849, Test: 0.6876\n",
            "Early stopping:  0.026184366525447592\n",
            "Epoch: 044, Loss: 0.7378, Train: 0.7900, Test: 0.6921\n",
            "Early stopping:  0.025867753310271573\n",
            "Epoch: 045, Loss: 0.7218, Train: 0.7955, Test: 0.6958\n",
            "Early stopping:  0.025674047127239957\n",
            "Epoch: 046, Loss: 0.7061, Train: 0.8025, Test: 0.6963\n",
            "Early stopping:  0.025387596667801198\n",
            "Epoch: 047, Loss: 0.6907, Train: 0.8057, Test: 0.7029\n",
            "Early stopping:  0.025048909356080156\n",
            "Epoch: 048, Loss: 0.6754, Train: 0.8108, Test: 0.7057\n",
            "Early stopping:  0.024626027658319723\n",
            "Epoch: 049, Loss: 0.6608, Train: 0.8172, Test: 0.7114\n",
            "Early stopping:  0.024125473099387573\n",
            "Epoch: 050, Loss: 0.6462, Train: 0.8227, Test: 0.7142\n",
            "Early stopping:  0.023670675135020643\n",
            "Epoch: 051, Loss: 0.6321, Train: 0.8250, Test: 0.7176\n",
            "Early stopping:  0.023158790043565657\n",
            "Epoch: 052, Loss: 0.6189, Train: 0.8336, Test: 0.7153\n",
            "Early stopping:  0.022440019278302734\n",
            "Epoch: 053, Loss: 0.6071, Train: 0.8297, Test: 0.7221\n",
            "Early stopping:  0.021332070256513902\n",
            "Epoch: 054, Loss: 0.5978, Train: 0.8427, Test: 0.7139\n",
            "Early stopping:  0.019325314525677657\n",
            "Epoch: 055, Loss: 0.5870, Train: 0.8414, Test: 0.7255\n",
            "Early stopping:  0.01761285629895251\n",
            "Epoch: 056, Loss: 0.5706, Train: 0.8490, Test: 0.7289\n",
            "Early stopping:  0.018542495366577143\n",
            "Epoch: 057, Loss: 0.5555, Train: 0.8560, Test: 0.7250\n",
            "Early stopping:  0.020742930865304522\n",
            "Epoch: 058, Loss: 0.5480, Train: 0.8518, Test: 0.7295\n",
            "Early stopping:  0.02085053610652208\n",
            "Epoch: 059, Loss: 0.5394, Train: 0.8635, Test: 0.7292\n",
            "Early stopping:  0.018916371983220506\n",
            "Epoch: 060, Loss: 0.5245, Train: 0.8679, Test: 0.7346\n",
            "Early stopping:  0.017247176164338386\n",
            "Epoch: 061, Loss: 0.5128, Train: 0.8614, Test: 0.7326\n",
            "Early stopping:  0.017371227477812588\n",
            "Epoch: 062, Loss: 0.5064, Train: 0.8737, Test: 0.7312\n",
            "Early stopping:  0.01749626818875894\n",
            "Epoch: 063, Loss: 0.4970, Train: 0.8741, Test: 0.7386\n",
            "Early stopping:  0.01646431721970499\n",
            "Epoch: 064, Loss: 0.4843, Train: 0.8784, Test: 0.7386\n",
            "Early stopping:  0.015288488922823817\n",
            "Epoch: 065, Loss: 0.4745, Train: 0.8839, Test: 0.7349\n",
            "Early stopping:  0.0156996772090581\n",
            "Epoch: 066, Loss: 0.4679, Train: 0.8817, Test: 0.7383\n",
            "Early stopping:  0.015805905361900272\n",
            "Epoch: 067, Loss: 0.4598, Train: 0.8909, Test: 0.7369\n",
            "Early stopping:  0.01446634919094996\n",
            "Epoch: 068, Loss: 0.4488, Train: 0.8932, Test: 0.7366\n",
            "Early stopping:  0.013612797768751708\n",
            "Epoch: 069, Loss: 0.4389, Train: 0.8955, Test: 0.7389\n",
            "Early stopping:  0.0143542729069317\n",
            "Epoch: 070, Loss: 0.4320, Train: 0.8972, Test: 0.7397\n",
            "Early stopping:  0.014717367333727692\n",
            "Epoch: 071, Loss: 0.4253, Train: 0.8991, Test: 0.7409\n",
            "Early stopping:  0.01367101868920643\n",
            "Epoch: 072, Loss: 0.4173, Train: 0.9028, Test: 0.7397\n",
            "Early stopping:  0.01213626545042863\n",
            "Epoch: 073, Loss: 0.4077, Train: 0.9078, Test: 0.7380\n",
            "Early stopping:  0.012218211628072833\n",
            "Epoch: 074, Loss: 0.3986, Train: 0.9117, Test: 0.7411\n",
            "Early stopping:  0.013356648550412211\n",
            "Epoch: 075, Loss: 0.3922, Train: 0.9104, Test: 0.7426\n",
            "Early stopping:  0.013427307577053316\n",
            "Epoch: 076, Loss: 0.3882, Train: 0.9164, Test: 0.7420\n",
            "Early stopping:  0.011780231542524762\n",
            "Epoch: 077, Loss: 0.3835, Train: 0.9076, Test: 0.7372\n",
            "Early stopping:  0.009420357074652712\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.79      0.72      0.76       379\n",
            "         capital_goods       0.67      0.56      0.61       254\n",
            "conglomerates_industry       1.00      0.65      0.79        40\n",
            "     consumer_cyclical       0.66      0.69      0.67       396\n",
            " consumer_non-cyclical       0.76      0.64      0.69       223\n",
            "                energy       0.83      0.77      0.80       141\n",
            "             financial       0.82      0.76      0.79       384\n",
            "            healthcare       0.82      0.75      0.78       159\n",
            "              services       0.68      0.83      0.75      1038\n",
            "            technology       0.69      0.59      0.64       198\n",
            "        transportation       0.89      0.79      0.83       202\n",
            "             utilities       0.83      0.74      0.79       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.79      0.71      0.74      3527\n",
            "          weighted avg       0.74      0.74      0.74      3527\n",
            "\n",
            "time: 2min 28s (started: 2024-10-16 21:03:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R55L-Ii5HtEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd760763-388c-40c5-f167-b608c568a2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 437 ms (started: 2024-10-16 21:06:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWUrgueftApV"
      },
      "source": [
        "### Training rotulated base = 80% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziy-LYEgU_Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd9c681-ba1e-4e09-a9cd-3c032318d47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.89 ms (started: 2024-10-16 21:06:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVAiqJH7Hugb"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2JV8dUpHugb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af032ae-944e-4f23-b40a-176e3119b2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 75.6125, Train: 0.1636, Test: 0.1548\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 776.0373, Train: 0.1133, Test: 0.1122\n",
            "Early stopping:  495.2750859474215\n",
            "Epoch: 003, Loss: 713.9493, Train: 0.0577, Test: 0.0573\n",
            "Early stopping:  387.7120055042322\n",
            "Epoch: 004, Loss: 768.6353, Train: 0.0563, Test: 0.0499\n",
            "Early stopping:  339.7608833995177\n",
            "Epoch: 005, Loss: 415.7242, Train: 0.0503, Test: 0.0459\n",
            "Early stopping:  303.6639185986307\n",
            "Epoch: 006, Loss: 241.8154, Train: 0.0818, Test: 0.0737\n",
            "Early stopping:  241.48472447705026\n",
            "Epoch: 007, Loss: 114.1308, Train: 0.0654, Test: 0.0646\n",
            "Early stopping:  286.582395310687\n",
            "Epoch: 008, Loss: 35.6068, Train: 0.0715, Test: 0.0692\n",
            "Early stopping:  291.41686191837067\n",
            "Epoch: 009, Loss: 10.1635, Train: 0.0968, Test: 0.0907\n",
            "Early stopping:  167.37412225682058\n",
            "Epoch: 010, Loss: 3.5755, Train: 0.1411, Test: 0.1264\n",
            "Early stopping:  100.04014325946417\n",
            "Epoch: 011, Loss: 2.6748, Train: 0.1255, Test: 0.1139\n",
            "Early stopping:  47.15842132443538\n",
            "Epoch: 012, Loss: 2.4540, Train: 0.1158, Test: 0.1100\n",
            "Early stopping:  14.173981997238842\n",
            "Epoch: 013, Loss: 2.3899, Train: 0.1122, Test: 0.1145\n",
            "Early stopping:  3.3388310962308028\n",
            "Epoch: 014, Loss: 2.3881, Train: 0.1147, Test: 0.1111\n",
            "Early stopping:  0.5052360456664629\n",
            "Epoch: 015, Loss: 2.3968, Train: 0.1160, Test: 0.1094\n",
            "Early stopping:  0.12271910073038739\n",
            "Epoch: 016, Loss: 2.3972, Train: 0.1224, Test: 0.1162\n",
            "Early stopping:  0.027571939926532587\n",
            "Epoch: 017, Loss: 2.3874, Train: 0.1309, Test: 0.1196\n",
            "Early stopping:  0.004768127820817712\n",
            "PREDICTIONS -> tensor([0, 4, 4,  ..., 4, 5, 4], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.43      0.32      0.37       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.00      0.00      0.00       198\n",
            " consumer_non-cyclical       0.07      0.79      0.13       112\n",
            "                energy       0.05      0.07      0.06        71\n",
            "             financial       0.00      0.00      0.00       192\n",
            "            healthcare       0.00      0.00      0.00        79\n",
            "              services       0.00      0.00      0.00       519\n",
            "            technology       0.08      0.01      0.02        99\n",
            "        transportation       0.25      0.55      0.34       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.12      1764\n",
            "             macro avg       0.07      0.15      0.08      1764\n",
            "          weighted avg       0.07      0.12      0.07      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 57.6118, Train: 0.0320, Test: 0.0317\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 778.1760, Train: 0.0727, Test: 0.0731\n",
            "Early stopping:  509.5158735193501\n",
            "Epoch: 003, Loss: 605.1738, Train: 0.1224, Test: 0.1247\n",
            "Early stopping:  376.15750734254925\n",
            "Epoch: 004, Loss: 378.1602, Train: 0.0832, Test: 0.0839\n",
            "Early stopping:  311.35001418909434\n",
            "Epoch: 005, Loss: 361.2312, Train: 0.1331, Test: 0.1264\n",
            "Early stopping:  272.86336709292243\n",
            "Epoch: 006, Loss: 280.0626, Train: 0.1076, Test: 0.1066\n",
            "Early stopping:  205.56520059552204\n",
            "Epoch: 007, Loss: 166.7249, Train: 0.1002, Test: 0.1037\n",
            "Early stopping:  161.37830123264465\n",
            "Epoch: 008, Loss: 142.6786, Train: 0.1484, Test: 0.1468\n",
            "Early stopping:  108.29370320720331\n",
            "Epoch: 009, Loss: 119.5859, Train: 0.2994, Test: 0.2971\n",
            "Early stopping:  102.80683124890359\n",
            "Epoch: 010, Loss: 139.4163, Train: 0.2990, Test: 0.3010\n",
            "Early stopping:  63.92854601423803\n",
            "Epoch: 011, Loss: 129.0179, Train: 0.3026, Test: 0.2993\n",
            "Early stopping:  17.732388205317854\n",
            "Epoch: 012, Loss: 94.5120, Train: 0.2556, Test: 0.2489\n",
            "Early stopping:  19.334731223191138\n",
            "Epoch: 013, Loss: 62.3738, Train: 0.2171, Test: 0.2052\n",
            "Early stopping:  30.910493177183422\n",
            "Epoch: 014, Loss: 43.9609, Train: 0.2091, Test: 0.1967\n",
            "Early stopping:  41.21039279056476\n",
            "Epoch: 015, Loss: 19.0255, Train: 0.2990, Test: 0.2920\n",
            "Early stopping:  43.06476500349184\n",
            "Epoch: 016, Loss: 7.5797, Train: 0.2881, Test: 0.2778\n",
            "Early stopping:  34.742322476046866\n",
            "Epoch: 017, Loss: 5.2713, Train: 0.2823, Test: 0.2681\n",
            "Early stopping:  24.747663312419295\n",
            "Epoch: 018, Loss: 4.1948, Train: 0.2704, Test: 0.2511\n",
            "Early stopping:  16.70578245246898\n",
            "Epoch: 019, Loss: 3.5159, Train: 0.2442, Test: 0.2290\n",
            "Early stopping:  6.39798111651717\n",
            "Epoch: 020, Loss: 3.0902, Train: 0.2493, Test: 0.2307\n",
            "Early stopping:  1.7932834219498708\n",
            "Epoch: 021, Loss: 2.8140, Train: 0.2399, Test: 0.2251\n",
            "Early stopping:  0.984225261924019\n",
            "Epoch: 022, Loss: 2.6149, Train: 0.2117, Test: 0.1956\n",
            "Early stopping:  0.6289335719289116\n",
            "Epoch: 023, Loss: 2.6398, Train: 0.1812, Test: 0.1667\n",
            "Early stopping:  0.3761019406162052\n",
            "Epoch: 024, Loss: 2.7113, Train: 0.1621, Test: 0.1457\n",
            "Early stopping:  0.192902556751297\n",
            "Epoch: 025, Loss: 2.6792, Train: 0.1517, Test: 0.1270\n",
            "Early stopping:  0.07759578719968546\n",
            "Epoch: 026, Loss: 2.5772, Train: 0.1428, Test: 0.1213\n",
            "Early stopping:  0.05267283387162531\n",
            "Epoch: 027, Loss: 2.4495, Train: 0.1338, Test: 0.1213\n",
            "Early stopping:  0.1034116400413042\n",
            "Epoch: 028, Loss: 2.4893, Train: 0.1296, Test: 0.1145\n",
            "Early stopping:  0.11438774180660677\n",
            "Epoch: 029, Loss: 2.5022, Train: 0.1303, Test: 0.1128\n",
            "Early stopping:  0.09077213566286561\n",
            "Epoch: 030, Loss: 2.4756, Train: 0.1283, Test: 0.1128\n",
            "Early stopping:  0.04799519279080522\n",
            "Epoch: 031, Loss: 2.4771, Train: 0.1290, Test: 0.1168\n",
            "Early stopping:  0.01955567338842602\n",
            "Epoch: 032, Loss: 2.5032, Train: 0.1286, Test: 0.1224\n",
            "Early stopping:  0.013156666555931635\n",
            "Epoch: 033, Loss: 2.4660, Train: 0.1344, Test: 0.1315\n",
            "Early stopping:  0.016850023701334798\n",
            "Epoch: 034, Loss: 2.4045, Train: 0.1340, Test: 0.1332\n",
            "Early stopping:  0.03668435280669473\n",
            "Epoch: 035, Loss: 2.3983, Train: 0.1406, Test: 0.1406\n",
            "Early stopping:  0.04625979083255342\n",
            "Epoch: 036, Loss: 2.3999, Train: 0.1619, Test: 0.1553\n",
            "Early stopping:  0.04772618991585851\n",
            "Epoch: 037, Loss: 2.3909, Train: 0.1747, Test: 0.1655\n",
            "Early stopping:  0.03061244115672027\n",
            "Epoch: 038, Loss: 2.3704, Train: 0.1969, Test: 0.1854\n",
            "Early stopping:  0.013455994158623032\n",
            "Epoch: 039, Loss: 2.3287, Train: 0.2240, Test: 0.2137\n",
            "Early stopping:  0.029793155909088473\n",
            "Epoch: 040, Loss: 2.2839, Train: 0.2610, Test: 0.2489\n",
            "Early stopping:  0.04819917728229839\n",
            "Epoch: 041, Loss: 2.2622, Train: 0.2660, Test: 0.2585\n",
            "Early stopping:  0.05482802835466009\n",
            "Epoch: 042, Loss: 2.2392, Train: 0.2622, Test: 0.2511\n",
            "Early stopping:  0.0526928582058574\n",
            "Epoch: 043, Loss: 2.2217, Train: 0.2734, Test: 0.2608\n",
            "Early stopping:  0.04165079640544159\n",
            "Epoch: 044, Loss: 2.1879, Train: 0.3047, Test: 0.2885\n",
            "Early stopping:  0.03696046184580805\n",
            "Epoch: 045, Loss: 2.1447, Train: 0.3350, Test: 0.3192\n",
            "Early stopping:  0.04597638315366077\n",
            "Epoch: 046, Loss: 2.1077, Train: 0.3542, Test: 0.3373\n",
            "Early stopping:  0.05425403654668805\n",
            "Epoch: 047, Loss: 2.0919, Train: 0.3505, Test: 0.3424\n",
            "Early stopping:  0.0542170218398948\n",
            "Epoch: 048, Loss: 2.0501, Train: 0.3566, Test: 0.3458\n",
            "Early stopping:  0.05241230274902692\n",
            "Epoch: 049, Loss: 2.0197, Train: 0.3610, Test: 0.3600\n",
            "Early stopping:  0.048966016235064504\n",
            "Epoch: 050, Loss: 1.9982, Train: 0.3698, Test: 0.3713\n",
            "Early stopping:  0.046444890439958225\n",
            "Epoch: 051, Loss: 1.9697, Train: 0.4015, Test: 0.4014\n",
            "Early stopping:  0.04720000672256674\n",
            "Epoch: 052, Loss: 1.9508, Train: 0.4088, Test: 0.4076\n",
            "Early stopping:  0.039431094521013295\n",
            "Epoch: 053, Loss: 1.9347, Train: 0.4157, Test: 0.4178\n",
            "Early stopping:  0.034545565532050085\n",
            "Epoch: 054, Loss: 1.9120, Train: 0.4211, Test: 0.4178\n",
            "Early stopping:  0.0329195512750924\n",
            "Epoch: 055, Loss: 1.8939, Train: 0.4254, Test: 0.4178\n",
            "Early stopping:  0.030123069836610804\n",
            "Epoch: 056, Loss: 1.8768, Train: 0.4313, Test: 0.4229\n",
            "Early stopping:  0.02987671573023189\n",
            "Epoch: 057, Loss: 1.8555, Train: 0.4360, Test: 0.4291\n",
            "Early stopping:  0.030667259462659937\n",
            "Epoch: 058, Loss: 1.8438, Train: 0.4374, Test: 0.4212\n",
            "Early stopping:  0.027732869395427937\n",
            "Epoch: 059, Loss: 1.8248, Train: 0.4421, Test: 0.4325\n",
            "Early stopping:  0.02714905581549411\n",
            "Epoch: 060, Loss: 1.8104, Train: 0.4419, Test: 0.4297\n",
            "Early stopping:  0.025951207605790092\n",
            "Epoch: 061, Loss: 1.8011, Train: 0.4431, Test: 0.4303\n",
            "Early stopping:  0.02261909068956124\n",
            "Epoch: 062, Loss: 1.7847, Train: 0.4442, Test: 0.4286\n",
            "Early stopping:  0.022555162026620796\n",
            "Epoch: 063, Loss: 1.7757, Train: 0.4504, Test: 0.4382\n",
            "Early stopping:  0.019660752103457007\n",
            "Epoch: 064, Loss: 1.7545, Train: 0.4519, Test: 0.4439\n",
            "Early stopping:  0.021906099471866756\n",
            "Epoch: 065, Loss: 1.7404, Train: 0.4536, Test: 0.4399\n",
            "Early stopping:  0.024084087560273016\n",
            "Epoch: 066, Loss: 1.7269, Train: 0.4560, Test: 0.4410\n",
            "Early stopping:  0.02401002704360406\n",
            "Epoch: 067, Loss: 1.7157, Train: 0.4595, Test: 0.4439\n",
            "Early stopping:  0.023505570154427387\n",
            "Epoch: 068, Loss: 1.7053, Train: 0.4599, Test: 0.4478\n",
            "Early stopping:  0.019527925419397723\n",
            "Epoch: 069, Loss: 1.6956, Train: 0.4597, Test: 0.4484\n",
            "Early stopping:  0.017627013111576835\n",
            "Epoch: 070, Loss: 1.6824, Train: 0.4614, Test: 0.4490\n",
            "Early stopping:  0.017249654419005705\n",
            "Epoch: 071, Loss: 1.6712, Train: 0.4652, Test: 0.4569\n",
            "Early stopping:  0.017727971085261956\n",
            "Epoch: 072, Loss: 1.6593, Train: 0.4670, Test: 0.4535\n",
            "Early stopping:  0.018413080391669882\n",
            "Epoch: 073, Loss: 1.6476, Train: 0.4648, Test: 0.4495\n",
            "Early stopping:  0.018840862524569083\n",
            "Epoch: 074, Loss: 1.6397, Train: 0.4666, Test: 0.4541\n",
            "Early stopping:  0.01726931533030446\n",
            "Epoch: 075, Loss: 1.6290, Train: 0.4658, Test: 0.4507\n",
            "Early stopping:  0.01647031623628111\n",
            "Epoch: 076, Loss: 1.6207, Train: 0.4789, Test: 0.4632\n",
            "Early stopping:  0.015186182695758828\n",
            "Epoch: 077, Loss: 1.6122, Train: 0.4916, Test: 0.4711\n",
            "Early stopping:  0.014205992882083176\n",
            "Epoch: 078, Loss: 1.6006, Train: 0.4921, Test: 0.4705\n",
            "Early stopping:  0.015034237020043561\n",
            "Epoch: 079, Loss: 1.5930, Train: 0.4988, Test: 0.4768\n",
            "Early stopping:  0.014572644179513228\n",
            "Epoch: 080, Loss: 1.5821, Train: 0.4974, Test: 0.4847\n",
            "Early stopping:  0.015272159058790848\n",
            "Epoch: 081, Loss: 1.5746, Train: 0.5067, Test: 0.4921\n",
            "Early stopping:  0.014866221730194652\n",
            "Epoch: 082, Loss: 1.5580, Train: 0.5074, Test: 0.4847\n",
            "Early stopping:  0.016553982747094992\n",
            "Epoch: 083, Loss: 1.5481, Train: 0.5135, Test: 0.4881\n",
            "Early stopping:  0.018107651593798162\n",
            "Epoch: 084, Loss: 1.5341, Train: 0.5179, Test: 0.4938\n",
            "Early stopping:  0.019448510241889795\n",
            "Epoch: 085, Loss: 1.5247, Train: 0.5209, Test: 0.4955\n",
            "Early stopping:  0.019643886483439146\n",
            "Epoch: 086, Loss: 1.5138, Train: 0.5300, Test: 0.4989\n",
            "Early stopping:  0.017727736612041786\n",
            "Epoch: 087, Loss: 1.5007, Train: 0.5315, Test: 0.4932\n",
            "Early stopping:  0.018246907665150847\n",
            "Epoch: 088, Loss: 1.4974, Train: 0.5303, Test: 0.4994\n",
            "Early stopping:  0.015583517399697398\n",
            "Epoch: 089, Loss: 1.4854, Train: 0.5313, Test: 0.5006\n",
            "Early stopping:  0.015220333690102537\n",
            "Epoch: 090, Loss: 1.4788, Train: 0.5389, Test: 0.5017\n",
            "Early stopping:  0.013663174577217474\n",
            "Epoch: 091, Loss: 1.4660, Train: 0.5409, Test: 0.5028\n",
            "Early stopping:  0.014142713826553372\n",
            "Epoch: 092, Loss: 1.4587, Train: 0.5412, Test: 0.5034\n",
            "Early stopping:  0.01538863516623313\n",
            "Epoch: 093, Loss: 1.4501, Train: 0.5425, Test: 0.5034\n",
            "Early stopping:  0.014387902154706913\n",
            "Epoch: 094, Loss: 1.4396, Train: 0.5437, Test: 0.5045\n",
            "Early stopping:  0.014972101840635047\n",
            "Epoch: 095, Loss: 1.4327, Train: 0.5481, Test: 0.5125\n",
            "Early stopping:  0.013578588308446698\n",
            "Epoch: 096, Loss: 1.4232, Train: 0.5480, Test: 0.5119\n",
            "Early stopping:  0.014003903003086592\n",
            "Epoch: 097, Loss: 1.4133, Train: 0.5500, Test: 0.5119\n",
            "Early stopping:  0.014266187584927575\n",
            "Epoch: 098, Loss: 1.4050, Train: 0.5517, Test: 0.5153\n",
            "Early stopping:  0.01402488768439612\n",
            "Epoch: 099, Loss: 1.3994, Train: 0.5578, Test: 0.5181\n",
            "Early stopping:  0.01348208398550924\n",
            "Epoch: 100, Loss: 1.3899, Train: 0.5634, Test: 0.5198\n",
            "Early stopping:  0.012765980633379683\n",
            "Epoch: 101, Loss: 1.3833, Train: 0.5684, Test: 0.5238\n",
            "Early stopping:  0.011883527993831473\n",
            "Epoch: 102, Loss: 1.3693, Train: 0.5698, Test: 0.5261\n",
            "Early stopping:  0.013990636415134847\n",
            "Epoch: 103, Loss: 1.3663, Train: 0.5727, Test: 0.5272\n",
            "Early stopping:  0.013904059426562924\n",
            "Epoch: 104, Loss: 1.3563, Train: 0.5792, Test: 0.5312\n",
            "Early stopping:  0.013520650507492652\n",
            "Epoch: 105, Loss: 1.3478, Train: 0.5812, Test: 0.5278\n",
            "Early stopping:  0.013491749306462063\n",
            "Epoch: 106, Loss: 1.3418, Train: 0.5812, Test: 0.5300\n",
            "Early stopping:  0.011741322536157587\n",
            "Epoch: 107, Loss: 1.3392, Train: 0.5864, Test: 0.5346\n",
            "Early stopping:  0.011090155564202004\n",
            "Epoch: 108, Loss: 1.3253, Train: 0.5915, Test: 0.5380\n",
            "Early stopping:  0.011448630150307886\n",
            "Epoch: 109, Loss: 1.3178, Train: 0.5908, Test: 0.5385\n",
            "Early stopping:  0.01241017197523525\n",
            "Epoch: 110, Loss: 1.3146, Train: 0.5941, Test: 0.5340\n",
            "Early stopping:  0.012321361615273491\n",
            "Epoch: 111, Loss: 1.3021, Train: 0.5935, Test: 0.5351\n",
            "Early stopping:  0.013719035819593266\n",
            "Epoch: 112, Loss: 1.2988, Train: 0.5969, Test: 0.5437\n",
            "Early stopping:  0.011071821958170042\n",
            "Epoch: 113, Loss: 1.2974, Train: 0.5944, Test: 0.5363\n",
            "Early stopping:  0.009414010011235529\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.47      0.58      0.52       190\n",
            "         capital_goods       0.45      0.08      0.13       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.48      0.62      0.54       198\n",
            " consumer_non-cyclical       0.58      0.22      0.32       112\n",
            "                energy       0.57      0.56      0.57        71\n",
            "             financial       0.65      0.52      0.58       192\n",
            "            healthcare       0.62      0.49      0.55        79\n",
            "              services       0.53      0.75      0.62       519\n",
            "            technology       0.49      0.23      0.32        99\n",
            "        transportation       0.59      0.56      0.58       101\n",
            "             utilities       0.64      0.52      0.57        56\n",
            "\n",
            "              accuracy                           0.54      1764\n",
            "             macro avg       0.51      0.43      0.44      1764\n",
            "          weighted avg       0.53      0.54      0.51      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 87.4226, Train: 0.2819, Test: 0.2789\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 338.4048, Train: 0.1116, Test: 0.1077\n",
            "Early stopping:  177.47124838611336\n",
            "Epoch: 003, Loss: 776.4841, Train: 0.1076, Test: 0.1083\n",
            "Early stopping:  348.7385057756239\n",
            "Epoch: 004, Loss: 1009.3166, Train: 0.0727, Test: 0.0731\n",
            "Early stopping:  416.72667370631075\n",
            "Epoch: 005, Loss: 868.3992, Train: 0.0493, Test: 0.0442\n",
            "Early stopping:  387.4956785538777\n",
            "Epoch: 006, Loss: 585.4974, Train: 0.0595, Test: 0.0539\n",
            "Early stopping:  261.02049203647243\n",
            "Epoch: 007, Loss: 592.6985, Train: 0.0689, Test: 0.0618\n",
            "Early stopping:  181.9422522436258\n",
            "Epoch: 008, Loss: 499.3116, Train: 0.0842, Test: 0.0805\n",
            "Early stopping:  216.98237569808168\n",
            "Epoch: 009, Loss: 325.5820, Train: 0.0581, Test: 0.0533\n",
            "Early stopping:  196.49561590910335\n",
            "Epoch: 010, Loss: 252.9663, Train: 0.0817, Test: 0.0737\n",
            "Early stopping:  154.47394571526783\n",
            "Epoch: 011, Loss: 162.4672, Train: 0.3028, Test: 0.2942\n",
            "Early stopping:  176.77540815264962\n",
            "Epoch: 012, Loss: 98.5844, Train: 0.1942, Test: 0.1814\n",
            "Early stopping:  155.63205691924213\n",
            "Epoch: 013, Loss: 72.4207, Train: 0.1642, Test: 0.1593\n",
            "Early stopping:  106.06426345932397\n",
            "Epoch: 014, Loss: 51.9910, Train: 0.2270, Test: 0.2211\n",
            "Early stopping:  81.44904180712344\n",
            "Epoch: 015, Loss: 25.4531, Train: 0.3050, Test: 0.3033\n",
            "Early stopping:  52.296496958185855\n",
            "Epoch: 016, Loss: 6.9562, Train: 0.3017, Test: 0.3005\n",
            "Early stopping:  36.457383678445716\n",
            "Epoch: 017, Loss: 5.8075, Train: 0.3006, Test: 0.3010\n",
            "Early stopping:  29.10408558926739\n",
            "Epoch: 018, Loss: 5.0116, Train: 0.3026, Test: 0.3016\n",
            "Early stopping:  20.278123179554004\n",
            "Epoch: 019, Loss: 4.1847, Train: 0.3046, Test: 0.3056\n",
            "Early stopping:  8.986173264512791\n",
            "Epoch: 020, Loss: 3.4172, Train: 0.3030, Test: 0.3039\n",
            "Early stopping:  1.3802779709186133\n",
            "Epoch: 021, Loss: 2.8450, Train: 0.2928, Test: 0.2874\n",
            "Early stopping:  1.1913994807333677\n",
            "Epoch: 022, Loss: 2.4983, Train: 0.2644, Test: 0.2523\n",
            "Early stopping:  1.0187525616255408\n",
            "Epoch: 023, Loss: 2.3439, Train: 0.2286, Test: 0.2188\n",
            "Early stopping:  0.7528311238151145\n",
            "Epoch: 024, Loss: 2.3367, Train: 0.1891, Test: 0.1859\n",
            "Early stopping:  0.4566697142477203\n",
            "Epoch: 025, Loss: 2.3545, Train: 0.1629, Test: 0.1610\n",
            "Early stopping:  0.2169502889756083\n",
            "Epoch: 026, Loss: 2.3630, Train: 0.1492, Test: 0.1446\n",
            "Early stopping:  0.0672773488383189\n",
            "Epoch: 027, Loss: 2.3655, Train: 0.1419, Test: 0.1412\n",
            "Early stopping:  0.012322440951714288\n",
            "Epoch: 028, Loss: 2.3614, Train: 0.1421, Test: 0.1395\n",
            "Early stopping:  0.011666902603661534\n",
            "Epoch: 029, Loss: 2.3494, Train: 0.1467, Test: 0.1468\n",
            "Early stopping:  0.006620633335542587\n",
            "PREDICTIONS -> tensor([10,  8, 10,  ...,  4, 10, 10], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       190\n",
            "         capital_goods       0.11      0.02      0.04       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.19      0.02      0.03       198\n",
            " consumer_non-cyclical       0.11      0.59      0.19       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.15      0.03      0.05       192\n",
            "            healthcare       0.00      0.00      0.00        79\n",
            "              services       0.47      0.22      0.30       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.08      0.64      0.14       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.15      1764\n",
            "             macro avg       0.09      0.13      0.06      1764\n",
            "          weighted avg       0.20      0.15      0.12      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 139.1967, Train: 0.0783, Test: 0.0816\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 343.7299, Train: 0.2952, Test: 0.2937\n",
            "Early stopping:  144.62682569708628\n",
            "Epoch: 003, Loss: 536.6469, Train: 0.2997, Test: 0.2937\n",
            "Early stopping:  198.7534105424026\n",
            "Epoch: 004, Loss: 399.1025, Train: 0.1083, Test: 0.1049\n",
            "Early stopping:  164.962913639369\n",
            "Epoch: 005, Loss: 470.5249, Train: 0.0533, Test: 0.0465\n",
            "Early stopping:  151.96739336854353\n",
            "Epoch: 006, Loss: 494.9352, Train: 0.0678, Test: 0.0607\n",
            "Early stopping:  77.17981765311207\n",
            "Epoch: 007, Loss: 463.2068, Train: 0.0981, Test: 0.0896\n",
            "Early stopping:  50.23664826105123\n",
            "Epoch: 008, Loss: 378.9115, Train: 0.0844, Test: 0.0816\n",
            "Early stopping:  49.708347428914166\n",
            "Epoch: 009, Loss: 255.4839, Train: 0.0768, Test: 0.0737\n",
            "Early stopping:  98.12720033796916\n",
            "Epoch: 010, Loss: 209.0508, Train: 0.0798, Test: 0.0754\n",
            "Early stopping:  125.42482221874612\n",
            "Epoch: 011, Loss: 120.8809, Train: 0.0997, Test: 0.0924\n",
            "Early stopping:  136.1527451646223\n",
            "Epoch: 012, Loss: 64.6923, Train: 0.1242, Test: 0.1332\n",
            "Early stopping:  122.03408968821628\n",
            "Epoch: 013, Loss: 37.4960, Train: 0.2364, Test: 0.2137\n",
            "Early stopping:  93.02913800217443\n",
            "Epoch: 014, Loss: 18.8507, Train: 0.1095, Test: 0.1185\n",
            "Early stopping:  76.78852851751762\n",
            "Epoch: 015, Loss: 11.9937, Train: 0.1555, Test: 0.1531\n",
            "Early stopping:  44.181478109462944\n",
            "Epoch: 016, Loss: 5.4972, Train: 0.2847, Test: 0.2806\n",
            "Early stopping:  23.887008119127746\n",
            "Epoch: 017, Loss: 3.6103, Train: 0.2970, Test: 0.2891\n",
            "Early stopping:  13.681800718831274\n",
            "Epoch: 018, Loss: 3.2312, Train: 0.2787, Test: 0.2761\n",
            "Early stopping:  6.707028542947175\n",
            "Epoch: 019, Loss: 3.0610, Train: 0.2612, Test: 0.2591\n",
            "Early stopping:  3.7694060757009926\n",
            "Epoch: 020, Loss: 2.8803, Train: 0.2407, Test: 0.2370\n",
            "Early stopping:  1.0639955458199777\n",
            "Epoch: 021, Loss: 2.6983, Train: 0.2273, Test: 0.2290\n",
            "Early stopping:  0.34952773688726757\n",
            "Epoch: 022, Loss: 2.5534, Train: 0.2127, Test: 0.2132\n",
            "Early stopping:  0.2718812142314162\n",
            "Epoch: 023, Loss: 2.4333, Train: 0.2141, Test: 0.2007\n",
            "Early stopping:  0.2511140542194407\n",
            "Epoch: 024, Loss: 2.3374, Train: 0.2188, Test: 0.2080\n",
            "Early stopping:  0.2151773717225606\n",
            "Epoch: 025, Loss: 2.2761, Train: 0.2281, Test: 0.2166\n",
            "Early stopping:  0.1695831232561847\n",
            "Epoch: 026, Loss: 2.2445, Train: 0.2373, Test: 0.2330\n",
            "Early stopping:  0.1257308027442745\n",
            "Epoch: 027, Loss: 2.2318, Train: 0.2354, Test: 0.2319\n",
            "Early stopping:  0.08272118408580133\n",
            "Epoch: 028, Loss: 2.2250, Train: 0.2328, Test: 0.2239\n",
            "Early stopping:  0.046023940249733705\n",
            "Epoch: 029, Loss: 2.2195, Train: 0.2277, Test: 0.2126\n",
            "Early stopping:  0.02256249639513526\n",
            "Epoch: 030, Loss: 2.2133, Train: 0.2271, Test: 0.2188\n",
            "Early stopping:  0.012006335505697087\n",
            "Epoch: 031, Loss: 2.2048, Train: 0.2315, Test: 0.2319\n",
            "Early stopping:  0.010398558471610914\n",
            "Epoch: 032, Loss: 2.1948, Train: 0.2378, Test: 0.2307\n",
            "Early stopping:  0.01199337898444323\n",
            "Epoch: 033, Loss: 2.1832, Train: 0.2399, Test: 0.2409\n",
            "Early stopping:  0.014528759892122304\n",
            "Epoch: 034, Loss: 2.1709, Train: 0.2439, Test: 0.2421\n",
            "Early stopping:  0.01688804390808303\n",
            "Epoch: 035, Loss: 2.1590, Train: 0.2490, Test: 0.2477\n",
            "Early stopping:  0.018279972161933774\n",
            "Epoch: 036, Loss: 2.1472, Train: 0.2538, Test: 0.2551\n",
            "Early stopping:  0.01885683189987214\n",
            "Epoch: 037, Loss: 2.1367, Train: 0.2663, Test: 0.2664\n",
            "Early stopping:  0.018436773193428048\n",
            "Epoch: 038, Loss: 2.1284, Train: 0.2739, Test: 0.2727\n",
            "Early stopping:  0.017007120219659354\n",
            "Epoch: 039, Loss: 2.1214, Train: 0.2895, Test: 0.2823\n",
            "Early stopping:  0.014952038216455542\n",
            "Epoch: 040, Loss: 2.1139, Train: 0.3194, Test: 0.3027\n",
            "Early stopping:  0.01299098385700382\n",
            "Epoch: 041, Loss: 2.1050, Train: 0.3443, Test: 0.3214\n",
            "Early stopping:  0.012317486673092364\n",
            "Epoch: 042, Loss: 2.0951, Train: 0.3535, Test: 0.3418\n",
            "Early stopping:  0.013148868351070672\n",
            "Epoch: 043, Loss: 2.0819, Train: 0.3535, Test: 0.3401\n",
            "Early stopping:  0.015545459620095264\n",
            "Epoch: 044, Loss: 2.0659, Train: 0.3407, Test: 0.3339\n",
            "Early stopping:  0.01898763638638043\n",
            "Epoch: 045, Loss: 2.0492, Train: 0.3414, Test: 0.3328\n",
            "Early stopping:  0.022376039411321443\n",
            "Epoch: 046, Loss: 2.0346, Train: 0.3435, Test: 0.3401\n",
            "Early stopping:  0.024338070771015484\n",
            "Epoch: 047, Loss: 2.0225, Train: 0.3472, Test: 0.3413\n",
            "Early stopping:  0.0238047772371303\n",
            "Epoch: 048, Loss: 2.0116, Train: 0.3468, Test: 0.3384\n",
            "Early stopping:  0.021490496100500984\n",
            "Epoch: 049, Loss: 2.0004, Train: 0.3434, Test: 0.3350\n",
            "Early stopping:  0.019115788250886983\n",
            "Epoch: 050, Loss: 1.9916, Train: 0.3460, Test: 0.3373\n",
            "Early stopping:  0.017115930196380386\n",
            "Epoch: 051, Loss: 1.9784, Train: 0.3506, Test: 0.3367\n",
            "Early stopping:  0.017130207143654267\n",
            "Epoch: 052, Loss: 1.9661, Train: 0.3518, Test: 0.3390\n",
            "Early stopping:  0.0178832087269681\n",
            "Epoch: 053, Loss: 1.9535, Train: 0.3525, Test: 0.3418\n",
            "Early stopping:  0.018886788559840724\n",
            "Epoch: 054, Loss: 1.9430, Train: 0.3570, Test: 0.3469\n",
            "Early stopping:  0.01931398905182407\n",
            "Epoch: 055, Loss: 1.9314, Train: 0.3584, Test: 0.3492\n",
            "Early stopping:  0.018525892150691604\n",
            "Epoch: 056, Loss: 1.9223, Train: 0.3618, Test: 0.3492\n",
            "Early stopping:  0.017368487148391416\n",
            "Epoch: 057, Loss: 1.9135, Train: 0.3613, Test: 0.3503\n",
            "Early stopping:  0.015929596678629115\n",
            "Epoch: 058, Loss: 1.9067, Train: 0.3705, Test: 0.3577\n",
            "Early stopping:  0.014382021428725815\n",
            "Epoch: 059, Loss: 1.8987, Train: 0.3630, Test: 0.3605\n",
            "Early stopping:  0.012821784890297923\n",
            "Epoch: 060, Loss: 1.9305, Train: 0.3793, Test: 0.3696\n",
            "Early stopping:  0.012517152138836037\n",
            "Epoch: 061, Loss: 1.8845, Train: 0.3788, Test: 0.3662\n",
            "Early stopping:  0.017086932476414005\n",
            "Epoch: 062, Loss: 1.8818, Train: 0.3805, Test: 0.3719\n",
            "Early stopping:  0.019645557288204666\n",
            "Epoch: 063, Loss: 1.8752, Train: 0.3825, Test: 0.3770\n",
            "Early stopping:  0.022047933124968798\n",
            "Epoch: 064, Loss: 1.8683, Train: 0.3844, Test: 0.3793\n",
            "Early stopping:  0.0245173551926863\n",
            "Epoch: 065, Loss: 1.8612, Train: 0.3842, Test: 0.3759\n",
            "Early stopping:  0.00960087566467302\n",
            "PREDICTIONS -> tensor([3, 8, 8,  ..., 8, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.39      0.05      0.08       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.26      0.23      0.25       198\n",
            " consumer_non-cyclical       0.69      0.28      0.39       112\n",
            "                energy       0.33      0.01      0.03        71\n",
            "             financial       0.59      0.46      0.52       192\n",
            "            healthcare       0.35      0.47      0.40        79\n",
            "              services       0.34      0.80      0.48       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.70      0.37      0.48       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.38      1764\n",
            "             macro avg       0.30      0.22      0.22      1764\n",
            "          weighted avg       0.35      0.38      0.31      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 78.2611, Train: 0.0563, Test: 0.0578\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 269.6455, Train: 0.2885, Test: 0.2868\n",
            "Early stopping:  135.32924979737118\n",
            "Epoch: 003, Loss: 334.4513, Train: 0.1102, Test: 0.1060\n",
            "Early stopping:  133.20487007454045\n",
            "Epoch: 004, Loss: 321.9276, Train: 0.0634, Test: 0.0635\n",
            "Early stopping:  118.57657337125035\n",
            "Epoch: 005, Loss: 280.8557, Train: 0.1117, Test: 0.1083\n",
            "Early stopping:  103.5505893688778\n",
            "Epoch: 006, Loss: 277.6482, Train: 0.0414, Test: 0.0368\n",
            "Early stopping:  29.186192265146843\n",
            "Epoch: 007, Loss: 226.9465, Train: 0.2125, Test: 0.2029\n",
            "Early stopping:  42.40833592097921\n",
            "Epoch: 008, Loss: 264.7614, Train: 0.2904, Test: 0.2920\n",
            "Early stopping:  34.116840998455224\n",
            "Epoch: 009, Loss: 295.4836, Train: 0.1707, Test: 0.1655\n",
            "Early stopping:  25.99566611245269\n",
            "Epoch: 010, Loss: 298.0408, Train: 0.2471, Test: 0.2438\n",
            "Early stopping:  28.90528975681579\n",
            "Epoch: 011, Loss: 262.2368, Train: 0.2629, Test: 0.2489\n",
            "Early stopping:  29.05054185946151\n",
            "Epoch: 012, Loss: 222.5959, Train: 0.1662, Test: 0.1610\n",
            "Early stopping:  30.663844111301177\n",
            "Epoch: 013, Loss: 184.9753, Train: 0.2882, Test: 0.2800\n",
            "Early stopping:  48.65711603476359\n",
            "Epoch: 014, Loss: 134.2131, Train: 0.2681, Test: 0.2591\n",
            "Early stopping:  64.16057211882882\n",
            "Epoch: 015, Loss: 90.9896, Train: 0.1133, Test: 0.0958\n",
            "Early stopping:  68.22090407308609\n",
            "Epoch: 016, Loss: 65.7628, Train: 0.1089, Test: 0.0907\n",
            "Early stopping:  64.79024067263464\n",
            "Epoch: 017, Loss: 39.7118, Train: 0.1469, Test: 0.1281\n",
            "Early stopping:  57.511686981062105\n",
            "Epoch: 018, Loss: 19.7205, Train: 0.2103, Test: 0.2069\n",
            "Early stopping:  44.79736834630057\n",
            "Epoch: 019, Loss: 10.8359, Train: 0.2175, Test: 0.2143\n",
            "Early stopping:  33.08899518030418\n",
            "Epoch: 020, Loss: 7.8030, Train: 0.1876, Test: 0.1780\n",
            "Early stopping:  24.141950302962425\n",
            "Epoch: 021, Loss: 6.2173, Train: 0.1772, Test: 0.1655\n",
            "Early stopping:  13.801925429760239\n",
            "Epoch: 022, Loss: 4.8950, Train: 0.1792, Test: 0.1672\n",
            "Early stopping:  5.922999191832778\n",
            "Epoch: 023, Loss: 3.8662, Train: 0.1819, Test: 0.1633\n",
            "Early stopping:  2.7313332145698235\n",
            "Epoch: 024, Loss: 3.1536, Train: 0.1779, Test: 0.1638\n",
            "Early stopping:  1.8620822941824742\n",
            "Epoch: 025, Loss: 2.7296, Train: 0.1726, Test: 0.1599\n",
            "Early stopping:  1.4068495405132342\n",
            "Epoch: 026, Loss: 2.4944, Train: 0.1781, Test: 0.1689\n",
            "Early stopping:  0.9719421235491943\n",
            "Epoch: 027, Loss: 2.3762, Train: 0.1876, Test: 0.1825\n",
            "Early stopping:  0.6047610777679374\n",
            "Epoch: 028, Loss: 2.3281, Train: 0.1870, Test: 0.1865\n",
            "Early stopping:  0.3380542719985486\n",
            "Epoch: 029, Loss: 2.3182, Train: 0.1887, Test: 0.1876\n",
            "Early stopping:  0.17165276917953107\n",
            "Epoch: 030, Loss: 2.3185, Train: 0.2812, Test: 0.2766\n",
            "Early stopping:  0.07508824513615668\n",
            "Epoch: 031, Loss: 2.3191, Train: 0.2921, Test: 0.2971\n",
            "Early stopping:  0.025043738277825148\n",
            "Epoch: 032, Loss: 2.3177, Train: 0.2966, Test: 0.3005\n",
            "Early stopping:  0.004375703972125858\n",
            "PREDICTIONS -> tensor([1, 8, 1,  ..., 8, 8, 6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.29      0.08      0.13       190\n",
            "         capital_goods       0.19      0.11      0.14       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.35      0.05      0.08       198\n",
            " consumer_non-cyclical       0.21      0.27      0.23       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.30      0.46      0.36       192\n",
            "            healthcare       0.16      0.04      0.06        79\n",
            "              services       0.32      0.71      0.44       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.00      0.00      0.00       101\n",
            "             utilities       0.12      0.05      0.07        56\n",
            "\n",
            "              accuracy                           0.30      1764\n",
            "             macro avg       0.16      0.15      0.13      1764\n",
            "          weighted avg       0.24      0.30      0.22      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 127.1477, Train: 0.0428, Test: 0.0420\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 374.8980, Train: 0.0448, Test: 0.0471\n",
            "Early stopping:  175.18594241001796\n",
            "Epoch: 003, Loss: 478.0485, Train: 0.2938, Test: 0.2931\n",
            "Early stopping:  180.34763055424546\n",
            "Epoch: 004, Loss: 454.7039, Train: 0.0827, Test: 0.0816\n",
            "Early stopping:  160.56117061903987\n",
            "Epoch: 005, Loss: 404.9065, Train: 0.2269, Test: 0.2200\n",
            "Early stopping:  140.57714591001061\n",
            "Epoch: 006, Loss: 266.0715, Train: 0.0880, Test: 0.0850\n",
            "Early stopping:  83.04179896855605\n",
            "Epoch: 007, Loss: 317.5446, Train: 0.1771, Test: 0.1820\n",
            "Early stopping:  90.28339603341311\n",
            "Epoch: 008, Loss: 319.4683, Train: 0.2551, Test: 0.2619\n",
            "Early stopping:  75.78742621552166\n",
            "Epoch: 009, Loss: 370.8611, Train: 0.2823, Test: 0.2778\n",
            "Early stopping:  53.542116799210326\n",
            "Epoch: 010, Loss: 395.6666, Train: 0.1014, Test: 0.0947\n",
            "Early stopping:  50.64018403834152\n",
            "Epoch: 011, Loss: 395.1049, Train: 0.2877, Test: 0.2863\n",
            "Early stopping:  38.94655126770251\n",
            "Epoch: 012, Loss: 310.3674, Train: 0.1408, Test: 0.1366\n",
            "Early stopping:  40.969753578175215\n",
            "Epoch: 013, Loss: 344.5421, Train: 0.2035, Test: 0.1910\n",
            "Early stopping:  36.297525716943355\n",
            "Epoch: 014, Loss: 263.6524, Train: 0.2722, Test: 0.2766\n",
            "Early stopping:  56.6691678286485\n",
            "Epoch: 015, Loss: 186.8141, Train: 0.2989, Test: 0.2931\n",
            "Early stopping:  79.48045355906949\n",
            "Epoch: 016, Loss: 155.1649, Train: 0.2739, Test: 0.2749\n",
            "Early stopping:  80.20715733393337\n",
            "Epoch: 017, Loss: 130.7825, Train: 0.2396, Test: 0.2375\n",
            "Early stopping:  87.47627001045291\n",
            "Epoch: 018, Loss: 133.9015, Train: 0.1806, Test: 0.1746\n",
            "Early stopping:  54.845545917873686\n",
            "Epoch: 019, Loss: 121.6230, Train: 0.1962, Test: 0.1910\n",
            "Early stopping:  26.08344417131533\n",
            "Epoch: 020, Loss: 91.0586, Train: 0.3138, Test: 0.3027\n",
            "Early stopping:  23.316692640584726\n",
            "Epoch: 021, Loss: 58.5961, Train: 0.3386, Test: 0.3282\n",
            "Early stopping:  32.0156604594111\n",
            "Epoch: 022, Loss: 34.6901, Train: 0.3345, Test: 0.3305\n",
            "Early stopping:  41.693335720707594\n",
            "Epoch: 023, Loss: 13.4539, Train: 0.2362, Test: 0.2194\n",
            "Early stopping:  43.28883646243777\n",
            "Epoch: 024, Loss: 9.2919, Train: 0.1877, Test: 0.1672\n",
            "Early stopping:  33.98023465125183\n",
            "Epoch: 025, Loss: 8.5056, Train: 0.1761, Test: 0.1599\n",
            "Early stopping:  21.648772957941965\n",
            "Epoch: 026, Loss: 7.3274, Train: 0.1752, Test: 0.1599\n",
            "Early stopping:  11.435912021383876\n",
            "Epoch: 027, Loss: 6.1460, Train: 0.1781, Test: 0.1633\n",
            "Early stopping:  2.788104841227\n",
            "Epoch: 028, Loss: 5.0177, Train: 0.1893, Test: 0.1718\n",
            "Early stopping:  1.7286878221778474\n",
            "Epoch: 029, Loss: 3.9855, Train: 0.2342, Test: 0.2273\n",
            "Early stopping:  1.7952379568316974\n",
            "Epoch: 030, Loss: 3.0950, Train: 0.2841, Test: 0.2596\n",
            "Early stopping:  1.682506361733652\n",
            "Epoch: 031, Loss: 2.4524, Train: 0.3360, Test: 0.3101\n",
            "Early stopping:  1.4797060923564542\n",
            "Epoch: 032, Loss: 2.1942, Train: 0.3655, Test: 0.3413\n",
            "Early stopping:  1.161004920560204\n",
            "Epoch: 033, Loss: 2.1760, Train: 0.3716, Test: 0.3526\n",
            "Early stopping:  0.769388339224445\n",
            "Epoch: 034, Loss: 2.2192, Train: 0.3681, Test: 0.3509\n",
            "Early stopping:  0.3896403922540107\n",
            "Epoch: 035, Loss: 2.2536, Train: 0.3689, Test: 0.3526\n",
            "Early stopping:  0.11193705409109751\n",
            "Epoch: 036, Loss: 2.2659, Train: 0.3684, Test: 0.3537\n",
            "Early stopping:  0.0381346818594333\n",
            "Epoch: 037, Loss: 2.2521, Train: 0.3710, Test: 0.3605\n",
            "Early stopping:  0.03642777474124922\n",
            "Epoch: 038, Loss: 2.2179, Train: 0.3747, Test: 0.3673\n",
            "Early stopping:  0.02183039299753665\n",
            "Epoch: 039, Loss: 2.1727, Train: 0.3797, Test: 0.3673\n",
            "Early stopping:  0.037858648019275104\n",
            "Epoch: 040, Loss: 2.1262, Train: 0.3857, Test: 0.3645\n",
            "Early stopping:  0.05770466283628967\n",
            "Epoch: 041, Loss: 2.0869, Train: 0.3871, Test: 0.3651\n",
            "Early stopping:  0.06681251804065956\n",
            "Epoch: 042, Loss: 2.0572, Train: 0.3855, Test: 0.3594\n",
            "Early stopping:  0.06459535059409593\n",
            "Epoch: 043, Loss: 2.0374, Train: 0.3849, Test: 0.3543\n",
            "Early stopping:  0.05434385457448549\n",
            "Epoch: 044, Loss: 2.0257, Train: 0.3791, Test: 0.3503\n",
            "Early stopping:  0.040566003976202136\n",
            "Epoch: 045, Loss: 2.0199, Train: 0.3743, Test: 0.3498\n",
            "Early stopping:  0.027221393079508306\n",
            "Epoch: 046, Loss: 2.0178, Train: 0.3699, Test: 0.3475\n",
            "Early stopping:  0.01621364627530337\n",
            "Epoch: 047, Loss: 2.0171, Train: 0.3630, Test: 0.3435\n",
            "Early stopping:  0.008412599619343377\n",
            "PREDICTIONS -> tensor([0, 8, 6,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.24      0.15      0.19       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.22      0.11      0.14       198\n",
            " consumer_non-cyclical       0.45      0.12      0.20       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.23      0.69      0.35       192\n",
            "            healthcare       0.52      0.32      0.39        79\n",
            "              services       0.42      0.66      0.52       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.48      0.43      0.45       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.34      1764\n",
            "             macro avg       0.21      0.21      0.19      1764\n",
            "          weighted avg       0.28      0.34      0.28      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 52.2051, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 474.7749, Train: 0.0469, Test: 0.0448\n",
            "Early stopping:  298.80196665069906\n",
            "Epoch: 003, Loss: 508.8186, Train: 0.1225, Test: 0.1213\n",
            "Early stopping:  254.36852787452202\n",
            "Epoch: 004, Loss: 568.3224, Train: 0.1090, Test: 0.1071\n",
            "Early stopping:  235.74156166680007\n",
            "Epoch: 005, Loss: 572.7109, Train: 0.1181, Test: 0.1111\n",
            "Early stopping:  218.1178854600395\n",
            "Epoch: 006, Loss: 347.1458, Train: 0.1416, Test: 0.1372\n",
            "Early stopping:  92.02439471609266\n",
            "Epoch: 007, Loss: 237.5185, Train: 0.3038, Test: 0.2982\n",
            "Early stopping:  148.4901057681723\n",
            "Epoch: 008, Loss: 263.8845, Train: 0.2928, Test: 0.2755\n",
            "Early stopping:  162.68147949212607\n",
            "Epoch: 009, Loss: 246.9262, Train: 0.1042, Test: 0.0975\n",
            "Early stopping:  140.50069157863317\n",
            "Epoch: 010, Loss: 232.9861, Train: 0.0408, Test: 0.0357\n",
            "Early stopping:  47.04669662148675\n",
            "Epoch: 011, Loss: 231.3759, Train: 0.2613, Test: 0.2506\n",
            "Early stopping:  13.378678040160917\n",
            "Epoch: 012, Loss: 168.2881, Train: 0.2949, Test: 0.2897\n",
            "Early stopping:  36.212120999617234\n",
            "Epoch: 013, Loss: 130.7398, Train: 0.2921, Test: 0.2812\n",
            "Early stopping:  50.13963413029776\n",
            "Epoch: 014, Loss: 91.5089, Train: 0.1528, Test: 0.1480\n",
            "Early stopping:  62.11801325878431\n",
            "Epoch: 015, Loss: 56.1529, Train: 0.1170, Test: 0.1117\n",
            "Early stopping:  68.04597290715748\n",
            "Epoch: 016, Loss: 33.3650, Train: 0.1249, Test: 0.1287\n",
            "Early stopping:  54.68895791128845\n",
            "Epoch: 017, Loss: 16.4784, Train: 0.1357, Test: 0.1264\n",
            "Early stopping:  45.977533528201995\n",
            "Epoch: 018, Loss: 7.6268, Train: 0.1024, Test: 0.0947\n",
            "Early stopping:  33.74215839700272\n",
            "Epoch: 019, Loss: 5.8877, Train: 0.1124, Test: 0.1003\n",
            "Early stopping:  21.06048514978737\n",
            "Epoch: 020, Loss: 4.5726, Train: 0.1431, Test: 0.1355\n",
            "Early stopping:  11.995763321017616\n",
            "Epoch: 021, Loss: 3.5620, Train: 0.1744, Test: 0.1672\n",
            "Early stopping:  5.177501610851717\n",
            "Epoch: 022, Loss: 2.8676, Train: 0.1940, Test: 0.1825\n",
            "Early stopping:  1.8999349299024704\n",
            "Epoch: 023, Loss: 2.5509, Train: 0.1893, Test: 0.1797\n",
            "Early stopping:  1.360410509817292\n",
            "Epoch: 024, Loss: 2.5092, Train: 0.1850, Test: 0.1814\n",
            "Early stopping:  0.869363426283442\n",
            "Epoch: 025, Loss: 2.5029, Train: 0.1886, Test: 0.1893\n",
            "Early stopping:  0.45276650086649023\n",
            "Epoch: 026, Loss: 2.4698, Train: 0.2047, Test: 0.2035\n",
            "Early stopping:  0.1633087095703657\n",
            "Epoch: 027, Loss: 2.4244, Train: 0.2247, Test: 0.2234\n",
            "Early stopping:  0.047300781428549835\n",
            "Epoch: 028, Loss: 2.3901, Train: 0.2311, Test: 0.2313\n",
            "Early stopping:  0.05129054393717361\n",
            "Epoch: 029, Loss: 2.3594, Train: 0.2300, Test: 0.2313\n",
            "Early stopping:  0.05810688332162354\n",
            "Epoch: 030, Loss: 2.3315, Train: 0.2287, Test: 0.2307\n",
            "Early stopping:  0.0542537129897624\n",
            "Epoch: 031, Loss: 2.3084, Train: 0.2351, Test: 0.2341\n",
            "Early stopping:  0.04603946740635524\n",
            "Epoch: 032, Loss: 2.2920, Train: 0.2392, Test: 0.2336\n",
            "Early stopping:  0.039331973777562625\n",
            "Epoch: 033, Loss: 2.2779, Train: 0.2447, Test: 0.2398\n",
            "Early stopping:  0.032355055930803615\n",
            "Epoch: 034, Loss: 2.2623, Train: 0.2521, Test: 0.2426\n",
            "Early stopping:  0.026827771408697836\n",
            "Epoch: 035, Loss: 2.2448, Train: 0.2561, Test: 0.2443\n",
            "Early stopping:  0.024831887537992\n",
            "Epoch: 036, Loss: 2.2269, Train: 0.2680, Test: 0.2540\n",
            "Early stopping:  0.02584715167304709\n",
            "Epoch: 037, Loss: 2.2097, Train: 0.2790, Test: 0.2698\n",
            "Early stopping:  0.02718619713918925\n",
            "Epoch: 038, Loss: 2.1942, Train: 0.3013, Test: 0.2942\n",
            "Early stopping:  0.02709960519034081\n",
            "Epoch: 039, Loss: 2.1812, Train: 0.3072, Test: 0.3095\n",
            "Early stopping:  0.025317052521733937\n",
            "Epoch: 040, Loss: 2.1717, Train: 0.3136, Test: 0.3095\n",
            "Early stopping:  0.022088168596980578\n",
            "Epoch: 041, Loss: 2.1658, Train: 0.3218, Test: 0.3231\n",
            "Early stopping:  0.017708160378828226\n",
            "Epoch: 042, Loss: 2.1623, Train: 0.3241, Test: 0.3254\n",
            "Early stopping:  0.012897274345623988\n",
            "Epoch: 043, Loss: 2.1586, Train: 0.3247, Test: 0.3265\n",
            "Early stopping:  0.008866313975305531\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.00      0.00      0.00       198\n",
            " consumer_non-cyclical       0.49      0.21      0.30       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.24      0.59      0.34       192\n",
            "            healthcare       0.00      0.00      0.00        79\n",
            "              services       0.34      0.78      0.47       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.78      0.35      0.48       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.33      1764\n",
            "             macro avg       0.15      0.16      0.13      1764\n",
            "          weighted avg       0.20      0.33      0.22      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 74.8167, Train: 0.1151, Test: 0.1151\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 375.5587, Train: 0.2935, Test: 0.2840\n",
            "Early stopping:  212.6567484585838\n",
            "Epoch: 003, Loss: 275.0435, Train: 0.1019, Test: 0.0952\n",
            "Early stopping:  153.10121150919358\n",
            "Epoch: 004, Loss: 491.7093, Train: 0.1097, Test: 0.1100\n",
            "Early stopping:  176.7470586298819\n",
            "Epoch: 005, Loss: 621.0549, Train: 0.0576, Test: 0.0533\n",
            "Early stopping:  208.56328698875222\n",
            "Epoch: 006, Loss: 585.1196, Train: 0.0571, Test: 0.0567\n",
            "Early stopping:  144.45422342947066\n",
            "Epoch: 007, Loss: 498.8492, Train: 0.3226, Test: 0.3175\n",
            "Early stopping:  134.55089562879783\n",
            "Epoch: 008, Loss: 492.6275, Train: 0.1951, Test: 0.1848\n",
            "Early stopping:  60.93555723290282\n",
            "Epoch: 009, Loss: 521.5767, Train: 0.2736, Test: 0.2687\n",
            "Early stopping:  56.58774482633351\n",
            "Epoch: 010, Loss: 507.7722, Train: 0.2056, Test: 0.1973\n",
            "Early stopping:  37.35674306653837\n",
            "Epoch: 011, Loss: 480.1642, Train: 0.2593, Test: 0.2483\n",
            "Early stopping:  15.611690113903336\n",
            "Epoch: 012, Loss: 432.7081, Train: 0.2890, Test: 0.2834\n",
            "Early stopping:  34.10656313517432\n",
            "Epoch: 013, Loss: 368.3334, Train: 0.0832, Test: 0.0816\n",
            "Early stopping:  62.46158840079085\n",
            "Epoch: 014, Loss: 305.5626, Train: 0.3119, Test: 0.3039\n",
            "Early stopping:  82.52969753538434\n",
            "Epoch: 015, Loss: 223.3059, Train: 0.1657, Test: 0.1616\n",
            "Early stopping:  101.76417321063767\n",
            "Epoch: 016, Loss: 187.2601, Train: 0.1283, Test: 0.1202\n",
            "Early stopping:  101.05092442014681\n",
            "Epoch: 017, Loss: 162.8835, Train: 0.1689, Test: 0.1519\n",
            "Early stopping:  85.62526692265862\n",
            "Epoch: 018, Loss: 118.2125, Train: 0.1967, Test: 0.1757\n",
            "Early stopping:  70.55887147406243\n",
            "Epoch: 019, Loss: 83.5049, Train: 0.1662, Test: 0.1463\n",
            "Early stopping:  55.30241036314436\n",
            "Epoch: 020, Loss: 66.5236, Train: 0.1146, Test: 0.1043\n",
            "Early stopping:  51.21067008163608\n",
            "Epoch: 021, Loss: 55.5459, Train: 0.1229, Test: 0.1173\n",
            "Early stopping:  43.64462079273821\n",
            "Epoch: 022, Loss: 34.6927, Train: 0.1588, Test: 0.1582\n",
            "Early stopping:  31.464470852864576\n",
            "Epoch: 023, Loss: 15.8090, Train: 0.0971, Test: 0.0986\n",
            "Early stopping:  26.562743883234692\n",
            "Epoch: 024, Loss: 13.7274, Train: 0.0949, Test: 0.0930\n",
            "Early stopping:  23.51198869735257\n",
            "Epoch: 025, Loss: 10.9363, Train: 0.1045, Test: 0.1037\n",
            "Early stopping:  18.906429682023376\n",
            "Epoch: 026, Loss: 8.4713, Train: 0.1198, Test: 0.1145\n",
            "Early stopping:  10.419892116803428\n",
            "Epoch: 027, Loss: 6.6533, Train: 0.1436, Test: 0.1327\n",
            "Early stopping:  3.734494665177122\n",
            "Epoch: 028, Loss: 5.4638, Train: 0.1686, Test: 0.1661\n",
            "Early stopping:  3.3307777177055313\n",
            "Epoch: 029, Loss: 4.4873, Train: 0.1880, Test: 0.1848\n",
            "Early stopping:  2.5616544750152093\n",
            "Epoch: 030, Loss: 3.6429, Train: 0.2101, Test: 0.2018\n",
            "Early stopping:  1.893222336744664\n",
            "Epoch: 031, Loss: 3.0165, Train: 0.2466, Test: 0.2387\n",
            "Early stopping:  1.4477904716294299\n",
            "Epoch: 032, Loss: 2.5883, Train: 0.2839, Test: 0.2937\n",
            "Early stopping:  1.1553666570558085\n",
            "Epoch: 033, Loss: 2.3531, Train: 0.3074, Test: 0.3090\n",
            "Early stopping:  0.862692614161607\n",
            "Epoch: 034, Loss: 2.3154, Train: 0.3148, Test: 0.3158\n",
            "Early stopping:  0.5557562570180256\n",
            "Epoch: 035, Loss: 2.3175, Train: 0.3189, Test: 0.3192\n",
            "Early stopping:  0.3008028133806044\n",
            "Epoch: 036, Loss: 2.3144, Train: 0.3272, Test: 0.3260\n",
            "Early stopping:  0.11881344567257672\n",
            "Epoch: 037, Loss: 2.3047, Train: 0.3316, Test: 0.3299\n",
            "Early stopping:  0.018571809422651516\n",
            "Epoch: 038, Loss: 2.2898, Train: 0.3363, Test: 0.3339\n",
            "Early stopping:  0.011455328192240847\n",
            "Epoch: 039, Loss: 2.2722, Train: 0.3390, Test: 0.3288\n",
            "Early stopping:  0.01878443760084331\n",
            "Epoch: 040, Loss: 2.2531, Train: 0.3403, Test: 0.3316\n",
            "Early stopping:  0.024722552134056712\n",
            "Epoch: 041, Loss: 2.2343, Train: 0.3391, Test: 0.3345\n",
            "Early stopping:  0.028125333279589885\n",
            "Epoch: 042, Loss: 2.2172, Train: 0.3424, Test: 0.3345\n",
            "Early stopping:  0.028960170939267622\n",
            "Epoch: 043, Loss: 2.2030, Train: 0.3417, Test: 0.3367\n",
            "Early stopping:  0.027610196616476086\n",
            "Epoch: 044, Loss: 2.1924, Train: 0.3434, Test: 0.3390\n",
            "Early stopping:  0.024284881408231672\n",
            "Epoch: 045, Loss: 2.1843, Train: 0.3441, Test: 0.3367\n",
            "Early stopping:  0.019948230417312984\n",
            "Epoch: 046, Loss: 2.1778, Train: 0.3450, Test: 0.3367\n",
            "Early stopping:  0.015612903793485733\n",
            "Epoch: 047, Loss: 2.1720, Train: 0.3489, Test: 0.3362\n",
            "Early stopping:  0.012180256207032015\n",
            "Epoch: 048, Loss: 2.1665, Train: 0.3589, Test: 0.3503\n",
            "Early stopping:  0.0101617750233848\n",
            "Epoch: 049, Loss: 2.1607, Train: 0.3611, Test: 0.3498\n",
            "Early stopping:  0.00927359823315528\n",
            "PREDICTIONS -> tensor([1, 8, 8,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.55      0.09      0.16       190\n",
            "         capital_goods       0.19      0.13      0.16       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       1.00      0.01      0.01       198\n",
            " consumer_non-cyclical       0.48      0.13      0.21       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.45      0.50      0.47       192\n",
            "            healthcare       0.25      0.01      0.02        79\n",
            "              services       0.33      0.88      0.48       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.50      0.14      0.22       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.35      1764\n",
            "             macro avg       0.31      0.16      0.14      1764\n",
            "          weighted avg       0.40      0.35      0.25      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 87.5115, Train: 0.0601, Test: 0.0595\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 399.6353, Train: 0.0424, Test: 0.0408\n",
            "Early stopping:  220.70486788437668\n",
            "Epoch: 003, Loss: 497.4770, Train: 0.1153, Test: 0.1145\n",
            "Early stopping:  214.11291318954326\n",
            "Epoch: 004, Loss: 514.6811, Train: 0.1072, Test: 0.1088\n",
            "Early stopping:  198.13115253505092\n",
            "Epoch: 005, Loss: 400.1791, Train: 0.2968, Test: 0.2948\n",
            "Early stopping:  171.9608066665553\n",
            "Epoch: 006, Loss: 391.6611, Train: 0.2956, Test: 0.2942\n",
            "Early stopping:  60.06225602421492\n",
            "Epoch: 007, Loss: 368.5255, Train: 0.1231, Test: 0.1224\n",
            "Early stopping:  66.63492106714443\n",
            "Epoch: 008, Loss: 337.8420, Train: 0.1935, Test: 0.1865\n",
            "Early stopping:  67.16092888001339\n",
            "Epoch: 009, Loss: 274.0680, Train: 0.2783, Test: 0.2710\n",
            "Early stopping:  51.017228961081216\n",
            "Epoch: 010, Loss: 218.3507, Train: 0.1839, Test: 0.1684\n",
            "Early stopping:  71.12041650432678\n",
            "Epoch: 011, Loss: 184.7065, Train: 0.2583, Test: 0.2409\n",
            "Early stopping:  77.53167873349109\n",
            "Epoch: 012, Loss: 128.4238, Train: 0.2517, Test: 0.2466\n",
            "Early stopping:  80.72806656458052\n",
            "Epoch: 013, Loss: 82.8994, Train: 0.1253, Test: 0.1207\n",
            "Early stopping:  74.83925144710717\n",
            "Epoch: 014, Loss: 57.2428, Train: 0.1201, Test: 0.1224\n",
            "Early stopping:  67.49017469207077\n",
            "Epoch: 015, Loss: 35.1673, Train: 0.1755, Test: 0.1808\n",
            "Early stopping:  59.75001131128786\n",
            "Epoch: 016, Loss: 14.9914, Train: 0.1961, Test: 0.1876\n",
            "Early stopping:  44.119476661134826\n",
            "Epoch: 017, Loss: 5.3431, Train: 0.1618, Test: 0.1542\n",
            "Early stopping:  31.558516090393343\n",
            "Epoch: 018, Loss: 3.5554, Train: 0.1625, Test: 0.1514\n",
            "Early stopping:  22.77208171225732\n",
            "Epoch: 019, Loss: 2.9788, Train: 0.1699, Test: 0.1531\n",
            "Early stopping:  13.61849347646556\n",
            "Epoch: 020, Loss: 2.6649, Train: 0.1740, Test: 0.1599\n",
            "Early stopping:  5.183144928612224\n",
            "Epoch: 021, Loss: 2.4935, Train: 0.1717, Test: 0.1582\n",
            "Early stopping:  1.1553090657592495\n",
            "Epoch: 022, Loss: 2.4690, Train: 0.1480, Test: 0.1361\n",
            "Early stopping:  0.4525164217873539\n",
            "Epoch: 023, Loss: 2.4844, Train: 0.1355, Test: 0.1270\n",
            "Early stopping:  0.21675038812074512\n",
            "Epoch: 024, Loss: 2.4783, Train: 0.1276, Test: 0.1230\n",
            "Early stopping:  0.08260522248242748\n",
            "Epoch: 025, Loss: 2.4562, Train: 0.1282, Test: 0.1298\n",
            "Early stopping:  0.014326757519345628\n",
            "Epoch: 026, Loss: 2.4443, Train: 0.1188, Test: 0.1105\n",
            "Early stopping:  0.016304453935872973\n",
            "Epoch: 027, Loss: 2.4369, Train: 0.1199, Test: 0.1077\n",
            "Early stopping:  0.020759667346449653\n",
            "Epoch: 028, Loss: 2.4256, Train: 0.1242, Test: 0.1134\n",
            "Early stopping:  0.020138608491557432\n",
            "Epoch: 029, Loss: 2.4124, Train: 0.1395, Test: 0.1276\n",
            "Early stopping:  0.01687633736106324\n",
            "Epoch: 030, Loss: 2.3978, Train: 0.1555, Test: 0.1502\n",
            "Early stopping:  0.018703638013753715\n",
            "Epoch: 031, Loss: 2.3814, Train: 0.1795, Test: 0.1712\n",
            "Early stopping:  0.022006150909600303\n",
            "Epoch: 032, Loss: 2.3623, Train: 0.2070, Test: 0.1995\n",
            "Early stopping:  0.025001413961931743\n",
            "Epoch: 033, Loss: 2.3406, Train: 0.2383, Test: 0.2415\n",
            "Early stopping:  0.02843911467108289\n",
            "Epoch: 034, Loss: 2.3191, Train: 0.2861, Test: 0.2823\n",
            "Early stopping:  0.03140660399137936\n",
            "Epoch: 035, Loss: 2.2913, Train: 0.3179, Test: 0.3112\n",
            "Early stopping:  0.035393712126912766\n",
            "Epoch: 036, Loss: 2.3019, Train: 0.3244, Test: 0.3112\n",
            "Early stopping:  0.02880016934810598\n",
            "Epoch: 037, Loss: 2.2388, Train: 0.3213, Test: 0.3141\n",
            "Early stopping:  0.03813600024857454\n",
            "Epoch: 038, Loss: 2.2141, Train: 0.3468, Test: 0.3362\n",
            "Early stopping:  0.04451330334831726\n",
            "Epoch: 039, Loss: 2.1893, Train: 0.3479, Test: 0.3373\n",
            "Early stopping:  0.048600840010825784\n",
            "Epoch: 040, Loss: 2.1598, Train: 0.3513, Test: 0.3430\n",
            "Early stopping:  0.05398980573444612\n",
            "Epoch: 041, Loss: 2.1258, Train: 0.3503, Test: 0.3373\n",
            "Early stopping:  0.0444406201903428\n",
            "Epoch: 042, Loss: 2.0968, Train: 0.3498, Test: 0.3396\n",
            "Early stopping:  0.04718797538739105\n",
            "Epoch: 043, Loss: 2.0936, Train: 0.3525, Test: 0.3413\n",
            "Early stopping:  0.04121228980954317\n",
            "Epoch: 044, Loss: 2.0838, Train: 0.3579, Test: 0.3486\n",
            "Early stopping:  0.030962566898603494\n",
            "Epoch: 045, Loss: 2.0563, Train: 0.3671, Test: 0.3600\n",
            "Early stopping:  0.02505749205053729\n",
            "Epoch: 046, Loss: 2.0450, Train: 0.3674, Test: 0.3639\n",
            "Early stopping:  0.023199573334463063\n",
            "Epoch: 047, Loss: 2.0412, Train: 0.3699, Test: 0.3617\n",
            "Early stopping:  0.023507107963491675\n",
            "Epoch: 048, Loss: 2.0265, Train: 0.3786, Test: 0.3707\n",
            "Early stopping:  0.021400331656731303\n",
            "Epoch: 049, Loss: 2.0026, Train: 0.3804, Test: 0.3673\n",
            "Early stopping:  0.02067488294424713\n",
            "Epoch: 050, Loss: 1.9880, Train: 0.3796, Test: 0.3707\n",
            "Early stopping:  0.024707745181102695\n",
            "Epoch: 051, Loss: 1.9806, Train: 0.3801, Test: 0.3679\n",
            "Early stopping:  0.025638248959071897\n",
            "Epoch: 052, Loss: 1.9613, Train: 0.3811, Test: 0.3690\n",
            "Early stopping:  0.024476965198064\n",
            "Epoch: 053, Loss: 1.9424, Train: 0.3837, Test: 0.3679\n",
            "Early stopping:  0.023521561991253964\n",
            "Epoch: 054, Loss: 1.9341, Train: 0.3824, Test: 0.3719\n",
            "Early stopping:  0.023346965111900805\n",
            "Epoch: 055, Loss: 1.9224, Train: 0.3811, Test: 0.3696\n",
            "Early stopping:  0.022992298197009094\n",
            "Epoch: 056, Loss: 1.9071, Train: 0.3777, Test: 0.3645\n",
            "Early stopping:  0.02043032264610248\n",
            "Epoch: 057, Loss: 1.8987, Train: 0.3788, Test: 0.3662\n",
            "Early stopping:  0.01816987830621015\n",
            "Epoch: 058, Loss: 1.8892, Train: 0.3845, Test: 0.3730\n",
            "Early stopping:  0.01804139269237431\n",
            "Epoch: 059, Loss: 1.8744, Train: 0.3891, Test: 0.3804\n",
            "Early stopping:  0.01810731785790153\n",
            "Epoch: 060, Loss: 1.8674, Train: 0.3937, Test: 0.3793\n",
            "Early stopping:  0.016488607046590425\n",
            "Epoch: 061, Loss: 1.8563, Train: 0.3953, Test: 0.3781\n",
            "Early stopping:  0.016959344875217313\n",
            "Epoch: 062, Loss: 1.8447, Train: 0.3946, Test: 0.3770\n",
            "Early stopping:  0.017028906311531075\n",
            "Epoch: 063, Loss: 1.8394, Train: 0.3986, Test: 0.3866\n",
            "Early stopping:  0.014782712228846306\n",
            "Epoch: 064, Loss: 1.8293, Train: 0.4012, Test: 0.3900\n",
            "Early stopping:  0.014824024702442444\n",
            "Epoch: 065, Loss: 1.8221, Train: 0.4028, Test: 0.3900\n",
            "Early stopping:  0.013300214773758444\n",
            "Epoch: 066, Loss: 1.8144, Train: 0.4052, Test: 0.3940\n",
            "Early stopping:  0.012359360561208796\n",
            "Epoch: 067, Loss: 1.8053, Train: 0.4064, Test: 0.3957\n",
            "Early stopping:  0.013153292030202574\n",
            "Epoch: 068, Loss: 1.7985, Train: 0.4090, Test: 0.3997\n",
            "Early stopping:  0.012422219197952796\n",
            "Epoch: 069, Loss: 1.7913, Train: 0.4122, Test: 0.4031\n",
            "Early stopping:  0.01228611744712258\n",
            "Epoch: 070, Loss: 1.7843, Train: 0.4151, Test: 0.4019\n",
            "Early stopping:  0.011735229791162047\n",
            "Epoch: 071, Loss: 1.7782, Train: 0.4156, Test: 0.4031\n",
            "Early stopping:  0.01081377956123522\n",
            "Epoch: 072, Loss: 1.7708, Train: 0.4153, Test: 0.3991\n",
            "Early stopping:  0.010825582125805823\n",
            "Epoch: 073, Loss: 1.7649, Train: 0.4171, Test: 0.4019\n",
            "Early stopping:  0.010495067917735506\n",
            "Epoch: 074, Loss: 1.7581, Train: 0.4180, Test: 0.3997\n",
            "Early stopping:  0.010408873216782081\n",
            "Epoch: 075, Loss: 1.7519, Train: 0.4205, Test: 0.4042\n",
            "Early stopping:  0.010335204488620476\n",
            "Epoch: 076, Loss: 1.7450, Train: 0.4224, Test: 0.4070\n",
            "Early stopping:  0.010203490803994485\n",
            "Epoch: 077, Loss: 1.7380, Train: 0.4244, Test: 0.4065\n",
            "Early stopping:  0.01054572590688491\n",
            "Epoch: 078, Loss: 1.7328, Train: 0.4248, Test: 0.4138\n",
            "Early stopping:  0.010171948096114072\n",
            "Epoch: 079, Loss: 1.7265, Train: 0.4275, Test: 0.4104\n",
            "Early stopping:  0.009948867761105464\n",
            "PREDICTIONS -> tensor([0, 6, 8,  ..., 4, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.35      0.56      0.43       190\n",
            "         capital_goods       0.25      0.01      0.02       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.30      0.11      0.16       198\n",
            " consumer_non-cyclical       0.29      0.24      0.26       112\n",
            "                energy       0.50      0.04      0.08        71\n",
            "             financial       0.55      0.57      0.56       192\n",
            "            healthcare       0.88      0.18      0.29        79\n",
            "              services       0.41      0.85      0.56       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.00      0.00      0.00       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.41      1764\n",
            "             macro avg       0.29      0.21      0.20      1764\n",
            "          weighted avg       0.35      0.41      0.32      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 50.1527, Train: 0.1137, Test: 0.1173\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 416.4834, Train: 0.2942, Test: 0.2937\n",
            "Early stopping:  259.03493016290383\n",
            "Epoch: 003, Loss: 503.5924, Train: 0.0770, Test: 0.0816\n",
            "Early stopping:  240.6220235536702\n",
            "Epoch: 004, Loss: 510.2979, Train: 0.0810, Test: 0.0782\n",
            "Early stopping:  217.55717074998188\n",
            "Epoch: 005, Loss: 560.9454, Train: 0.1348, Test: 0.1327\n",
            "Early stopping:  206.83404367566308\n",
            "Epoch: 006, Loss: 604.5966, Train: 0.1082, Test: 0.1122\n",
            "Early stopping:  70.54925891285508\n",
            "Epoch: 007, Loss: 561.4997, Train: 0.0702, Test: 0.0652\n",
            "Early stopping:  41.672248225850275\n",
            "Epoch: 008, Loss: 524.5591, Train: 0.0688, Test: 0.0720\n",
            "Early stopping:  36.83750831575918\n",
            "Epoch: 009, Loss: 468.1175, Train: 0.1243, Test: 0.1423\n",
            "Early stopping:  50.99366078021521\n",
            "Epoch: 010, Loss: 477.0154, Train: 0.2938, Test: 0.2937\n",
            "Early stopping:  57.40839203569291\n",
            "Epoch: 011, Loss: 505.4342, Train: 0.2935, Test: 0.2925\n",
            "Early stopping:  37.7206385769013\n",
            "Epoch: 012, Loss: 464.2848, Train: 0.2769, Test: 0.2755\n",
            "Early stopping:  26.07123608790086\n",
            "Epoch: 013, Loss: 368.3515, Train: 0.0861, Test: 0.0884\n",
            "Early stopping:  51.9159481475139\n",
            "Epoch: 014, Loss: 300.2979, Train: 0.2410, Test: 0.2489\n",
            "Early stopping:  85.81918522294804\n",
            "Epoch: 015, Loss: 165.7192, Train: 0.2473, Test: 0.2404\n",
            "Early stopping:  135.4641939326596\n",
            "Epoch: 016, Loss: 103.0430, Train: 0.2666, Test: 0.2494\n",
            "Early stopping:  147.0987613935241\n",
            "Epoch: 017, Loss: 52.8768, Train: 0.1674, Test: 0.1559\n",
            "Early stopping:  132.85716753026756\n",
            "Epoch: 018, Loss: 25.6080, Train: 0.1968, Test: 0.1865\n",
            "Early stopping:  109.37136662449703\n",
            "Epoch: 019, Loss: 6.7330, Train: 0.1618, Test: 0.1621\n",
            "Early stopping:  64.26003550212495\n",
            "Epoch: 020, Loss: 3.6576, Train: 0.1351, Test: 0.1321\n",
            "Early stopping:  41.11497664401319\n",
            "Epoch: 021, Loss: 3.2109, Train: 0.1361, Test: 0.1270\n",
            "Early stopping:  21.35867175090587\n",
            "Epoch: 022, Loss: 2.8927, Train: 0.1346, Test: 0.1264\n",
            "Early stopping:  9.729361670833262\n",
            "Epoch: 023, Loss: 2.6854, Train: 0.1258, Test: 0.1156\n",
            "Early stopping:  1.660500068509709\n",
            "Epoch: 024, Loss: 2.5597, Train: 0.1219, Test: 0.1190\n",
            "Early stopping:  0.44193885614535905\n",
            "Epoch: 025, Loss: 2.4957, Train: 0.1154, Test: 0.1111\n",
            "Early stopping:  0.28985072220665004\n",
            "Epoch: 026, Loss: 2.4659, Train: 0.1061, Test: 0.1032\n",
            "Early stopping:  0.17424149743055925\n",
            "Epoch: 027, Loss: 2.4479, Train: 0.0984, Test: 0.0952\n",
            "Early stopping:  0.09623368247206976\n",
            "Epoch: 028, Loss: 2.4385, Train: 0.0946, Test: 0.0947\n",
            "Early stopping:  0.04883775698276197\n",
            "Epoch: 029, Loss: 2.4364, Train: 0.0920, Test: 0.0930\n",
            "Early stopping:  0.02460696656680957\n",
            "Epoch: 030, Loss: 2.4373, Train: 0.0896, Test: 0.0896\n",
            "Early stopping:  0.012455391066731667\n",
            "Epoch: 031, Loss: 2.4367, Train: 0.0888, Test: 0.0862\n",
            "Early stopping:  0.004822963868945583\n",
            "PREDICTIONS -> tensor([0, 7, 2,  ..., 4, 7, 7], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.10      0.01      0.01       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.20      0.03      0.04       198\n",
            " consumer_non-cyclical       0.42      0.13      0.20       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.00      0.00      0.00       192\n",
            "            healthcare       0.05      0.82      0.09        79\n",
            "              services       0.83      0.06      0.10       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.21      0.37      0.27       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.09      1764\n",
            "             macro avg       0.15      0.12      0.06      1764\n",
            "          weighted avg       0.32      0.09      0.07      1764\n",
            "\n",
            "time: 1min 26s (started: 2024-10-16 21:06:11 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im8tt38XHugb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60607c5f-8cf7-44ce-ca17-a27ccd0866f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 488 ms (started: 2024-10-16 21:07:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y5_EcxHugb"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM_16W6CHugb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da1d953-4692-4c64-cff6-7d3c6df4e8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5130, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3115, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  0.14250439537412424\n",
            "Epoch: 003, Loss: 2.1918, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.16234692044629728\n",
            "Epoch: 004, Loss: 2.1541, Train: 0.3004, Test: 0.2993\n",
            "Early stopping:  0.16154336358839\n",
            "Epoch: 005, Loss: 2.0995, Train: 0.3391, Test: 0.3362\n",
            "Early stopping:  0.16440392452614266\n",
            "Epoch: 006, Loss: 2.0321, Train: 0.3754, Test: 0.3702\n",
            "Early stopping:  0.10486620594515077\n",
            "Epoch: 007, Loss: 1.9733, Train: 0.3910, Test: 0.3815\n",
            "Early stopping:  0.0887950502427705\n",
            "Epoch: 008, Loss: 1.9230, Train: 0.3866, Test: 0.3815\n",
            "Early stopping:  0.09312488687286205\n",
            "Epoch: 009, Loss: 1.8753, Train: 0.3892, Test: 0.3770\n",
            "Early stopping:  0.0883672000006449\n",
            "Epoch: 010, Loss: 1.8276, Train: 0.4029, Test: 0.3929\n",
            "Early stopping:  0.08021449561659852\n",
            "Epoch: 011, Loss: 1.7754, Train: 0.4310, Test: 0.4110\n",
            "Early stopping:  0.077652604072661\n",
            "Epoch: 012, Loss: 1.7175, Train: 0.4507, Test: 0.4325\n",
            "Early stopping:  0.08087586379830466\n",
            "Epoch: 013, Loss: 1.6626, Train: 0.4744, Test: 0.4450\n",
            "Early stopping:  0.08475362951889841\n",
            "Epoch: 014, Loss: 1.6152, Train: 0.4872, Test: 0.4615\n",
            "Early stopping:  0.08507299846965959\n",
            "Epoch: 015, Loss: 1.5663, Train: 0.4987, Test: 0.4734\n",
            "Early stopping:  0.08237616114053747\n",
            "Epoch: 016, Loss: 1.5134, Train: 0.5121, Test: 0.4983\n",
            "Early stopping:  0.07976276493400772\n",
            "Epoch: 017, Loss: 1.4637, Train: 0.5341, Test: 0.5221\n",
            "Early stopping:  0.07898256774949973\n",
            "Epoch: 018, Loss: 1.4179, Train: 0.5574, Test: 0.5300\n",
            "Early stopping:  0.0786275722642452\n",
            "Epoch: 019, Loss: 1.3756, Train: 0.5798, Test: 0.5442\n",
            "Early stopping:  0.07548941515539304\n",
            "Epoch: 020, Loss: 1.3391, Train: 0.5890, Test: 0.5550\n",
            "Early stopping:  0.06919333858227497\n",
            "Epoch: 021, Loss: 1.3034, Train: 0.5912, Test: 0.5692\n",
            "Early stopping:  0.06325623436390732\n",
            "Epoch: 022, Loss: 1.2683, Train: 0.5944, Test: 0.5743\n",
            "Early stopping:  0.05877052231431517\n",
            "Epoch: 023, Loss: 1.2391, Train: 0.6019, Test: 0.5879\n",
            "Early stopping:  0.05440724559424552\n",
            "Epoch: 024, Loss: 1.2127, Train: 0.6178, Test: 0.5924\n",
            "Early stopping:  0.050245240936709915\n",
            "Epoch: 025, Loss: 1.1858, Train: 0.6315, Test: 0.5981\n",
            "Early stopping:  0.046082846778549603\n",
            "Epoch: 026, Loss: 1.1602, Train: 0.6417, Test: 0.6094\n",
            "Early stopping:  0.0426115449781378\n",
            "Epoch: 027, Loss: 1.1342, Train: 0.6482, Test: 0.6196\n",
            "Early stopping:  0.04145896714300186\n",
            "Epoch: 028, Loss: 1.1101, Train: 0.6550, Test: 0.6247\n",
            "Early stopping:  0.04060786692521112\n",
            "Epoch: 029, Loss: 1.0868, Train: 0.6613, Test: 0.6293\n",
            "Early stopping:  0.039248559165787275\n",
            "Epoch: 030, Loss: 1.0634, Train: 0.6678, Test: 0.6327\n",
            "Early stopping:  0.038133990245897746\n",
            "Epoch: 031, Loss: 1.0432, Train: 0.6783, Test: 0.6349\n",
            "Early stopping:  0.03619375970077491\n",
            "Epoch: 032, Loss: 1.0233, Train: 0.6838, Test: 0.6429\n",
            "Early stopping:  0.034353351169124524\n",
            "Epoch: 033, Loss: 1.0043, Train: 0.6923, Test: 0.6480\n",
            "Early stopping:  0.032446576833649425\n",
            "Epoch: 034, Loss: 0.9852, Train: 0.7023, Test: 0.6525\n",
            "Early stopping:  0.030872172084090183\n",
            "Epoch: 035, Loss: 0.9671, Train: 0.7092, Test: 0.6604\n",
            "Early stopping:  0.030092490854568995\n",
            "Epoch: 036, Loss: 0.9489, Train: 0.7125, Test: 0.6559\n",
            "Early stopping:  0.02941123733409098\n",
            "Epoch: 037, Loss: 0.9309, Train: 0.7186, Test: 0.6604\n",
            "Early stopping:  0.028944806826519856\n",
            "Epoch: 038, Loss: 0.9136, Train: 0.7235, Test: 0.6621\n",
            "Early stopping:  0.02835675040208803\n",
            "Epoch: 039, Loss: 0.8971, Train: 0.7303, Test: 0.6593\n",
            "Early stopping:  0.02770899132957839\n",
            "Epoch: 040, Loss: 0.8804, Train: 0.7343, Test: 0.6621\n",
            "Early stopping:  0.02701685526109728\n",
            "Epoch: 041, Loss: 0.8645, Train: 0.7427, Test: 0.6701\n",
            "Early stopping:  0.02624896735944247\n",
            "Epoch: 042, Loss: 0.8489, Train: 0.7505, Test: 0.6763\n",
            "Early stopping:  0.02564106165363105\n",
            "Epoch: 043, Loss: 0.8334, Train: 0.7544, Test: 0.6757\n",
            "Early stopping:  0.02513084950927194\n",
            "Epoch: 044, Loss: 0.8182, Train: 0.7593, Test: 0.6808\n",
            "Early stopping:  0.024574772816678126\n",
            "Epoch: 045, Loss: 0.8036, Train: 0.7658, Test: 0.6848\n",
            "Early stopping:  0.024108496662595885\n",
            "Epoch: 046, Loss: 0.7890, Train: 0.7688, Test: 0.6859\n",
            "Early stopping:  0.023654244020213232\n",
            "Epoch: 047, Loss: 0.7746, Train: 0.7737, Test: 0.6927\n",
            "Early stopping:  0.023229993908823136\n",
            "Epoch: 048, Loss: 0.7605, Train: 0.7794, Test: 0.6961\n",
            "Early stopping:  0.022841740360352657\n",
            "Epoch: 049, Loss: 0.7464, Train: 0.7839, Test: 0.6978\n",
            "Early stopping:  0.022586801679203274\n",
            "Epoch: 050, Loss: 0.7328, Train: 0.7906, Test: 0.7001\n",
            "Early stopping:  0.022222995452735504\n",
            "Epoch: 051, Loss: 0.7193, Train: 0.7948, Test: 0.7063\n",
            "Early stopping:  0.021851639275558554\n",
            "Epoch: 052, Loss: 0.7061, Train: 0.7998, Test: 0.7086\n",
            "Early stopping:  0.021485549686018228\n",
            "Epoch: 053, Loss: 0.6932, Train: 0.8033, Test: 0.7143\n",
            "Early stopping:  0.021063874076466642\n",
            "Epoch: 054, Loss: 0.6802, Train: 0.8073, Test: 0.7166\n",
            "Early stopping:  0.020783522089133512\n",
            "Epoch: 055, Loss: 0.6675, Train: 0.8126, Test: 0.7171\n",
            "Early stopping:  0.020482299522263124\n",
            "Epoch: 056, Loss: 0.6551, Train: 0.8195, Test: 0.7205\n",
            "Early stopping:  0.020186396509603896\n",
            "Epoch: 057, Loss: 0.6430, Train: 0.8212, Test: 0.7251\n",
            "Early stopping:  0.019849760346647982\n",
            "Epoch: 058, Loss: 0.6314, Train: 0.8283, Test: 0.7239\n",
            "Early stopping:  0.01932447419944578\n",
            "Epoch: 059, Loss: 0.6212, Train: 0.8223, Test: 0.7228\n",
            "Early stopping:  0.018404944926046562\n",
            "Epoch: 060, Loss: 0.6126, Train: 0.8362, Test: 0.7262\n",
            "Early stopping:  0.01690671281646137\n",
            "Epoch: 061, Loss: 0.6010, Train: 0.8331, Test: 0.7364\n",
            "Early stopping:  0.016265407227260546\n",
            "Epoch: 062, Loss: 0.5898, Train: 0.8369, Test: 0.7296\n",
            "Early stopping:  0.01636621951635106\n",
            "Epoch: 063, Loss: 0.5864, Train: 0.8327, Test: 0.7336\n",
            "Early stopping:  0.014791461618605692\n",
            "Epoch: 064, Loss: 0.5814, Train: 0.8455, Test: 0.7375\n",
            "Early stopping:  0.012533384815860459\n",
            "Epoch: 065, Loss: 0.5619, Train: 0.8460, Test: 0.7387\n",
            "Early stopping:  0.014320217034064246\n",
            "Epoch: 066, Loss: 0.5547, Train: 0.8481, Test: 0.7415\n",
            "Early stopping:  0.015575444566783303\n",
            "Epoch: 067, Loss: 0.5487, Train: 0.8572, Test: 0.7489\n",
            "Early stopping:  0.01653558964297759\n",
            "Epoch: 068, Loss: 0.5380, Train: 0.8523, Test: 0.7443\n",
            "Early stopping:  0.016246111168047327\n",
            "Epoch: 069, Loss: 0.5314, Train: 0.8601, Test: 0.7466\n",
            "Early stopping:  0.01234279552248623\n",
            "Epoch: 070, Loss: 0.5205, Train: 0.8646, Test: 0.7494\n",
            "Early stopping:  0.013625953554470886\n",
            "Epoch: 071, Loss: 0.5140, Train: 0.8613, Test: 0.7511\n",
            "Early stopping:  0.013791764959839439\n",
            "Epoch: 072, Loss: 0.5070, Train: 0.8693, Test: 0.7483\n",
            "Early stopping:  0.012613328119538886\n",
            "Epoch: 073, Loss: 0.4989, Train: 0.8735, Test: 0.7551\n",
            "Early stopping:  0.012485590938120449\n",
            "Epoch: 074, Loss: 0.4892, Train: 0.8704, Test: 0.7562\n",
            "Early stopping:  0.012311453495851443\n",
            "Epoch: 075, Loss: 0.4844, Train: 0.8772, Test: 0.7517\n",
            "Early stopping:  0.01220472180107942\n",
            "Epoch: 076, Loss: 0.4763, Train: 0.8774, Test: 0.7562\n",
            "Early stopping:  0.012054320239791794\n",
            "Epoch: 077, Loss: 0.4688, Train: 0.8827, Test: 0.7608\n",
            "Early stopping:  0.011595563953777243\n",
            "Epoch: 078, Loss: 0.4649, Train: 0.8850, Test: 0.7608\n",
            "Early stopping:  0.010233420754879522\n",
            "Epoch: 079, Loss: 0.4538, Train: 0.8887, Test: 0.7523\n",
            "Early stopping:  0.011593128250083658\n",
            "Epoch: 080, Loss: 0.4493, Train: 0.8890, Test: 0.7681\n",
            "Early stopping:  0.010999445110050847\n",
            "Epoch: 081, Loss: 0.4421, Train: 0.8938, Test: 0.7630\n",
            "Early stopping:  0.011026088327040327\n",
            "Epoch: 082, Loss: 0.4339, Train: 0.8956, Test: 0.7585\n",
            "Early stopping:  0.01173282220659155\n",
            "Epoch: 083, Loss: 0.4292, Train: 0.8948, Test: 0.7670\n",
            "Early stopping:  0.010271741084764297\n",
            "Epoch: 084, Loss: 0.4225, Train: 0.9029, Test: 0.7647\n",
            "Early stopping:  0.010549582771471394\n",
            "Epoch: 085, Loss: 0.4167, Train: 0.8952, Test: 0.7630\n",
            "Early stopping:  0.009861167119583663\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.79      0.77      0.78       190\n",
            "         capital_goods       0.82      0.61      0.70       127\n",
            "conglomerates_industry       1.00      0.85      0.92        20\n",
            "     consumer_cyclical       0.73      0.74      0.73       198\n",
            " consumer_non-cyclical       0.77      0.65      0.71       112\n",
            "                energy       0.85      0.75      0.80        71\n",
            "             financial       0.74      0.79      0.76       192\n",
            "            healthcare       0.91      0.73      0.81        79\n",
            "              services       0.71      0.82      0.76       519\n",
            "            technology       0.72      0.70      0.71        99\n",
            "        transportation       0.93      0.81      0.87       101\n",
            "             utilities       0.86      0.77      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.82      0.75      0.78      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5102, Train: 0.2958, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2628, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.17497618192825148\n",
            "Epoch: 003, Loss: 2.1791, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1721902961943562\n",
            "Epoch: 004, Loss: 2.1348, Train: 0.3002, Test: 0.3005\n",
            "Early stopping:  0.1676241403094731\n",
            "Epoch: 005, Loss: 2.0570, Train: 0.3574, Test: 0.3560\n",
            "Early stopping:  0.17405296154021652\n",
            "Epoch: 006, Loss: 2.0003, Train: 0.3893, Test: 0.3804\n",
            "Early stopping:  0.10266339478074915\n",
            "Epoch: 007, Loss: 1.9482, Train: 0.3899, Test: 0.3855\n",
            "Early stopping:  0.09458367980222006\n",
            "Epoch: 008, Loss: 1.8879, Train: 0.3842, Test: 0.3759\n",
            "Early stopping:  0.09556433983459045\n",
            "Epoch: 009, Loss: 1.8256, Train: 0.3927, Test: 0.3872\n",
            "Early stopping:  0.0909996045672779\n",
            "Epoch: 010, Loss: 1.7654, Train: 0.4164, Test: 0.4138\n",
            "Early stopping:  0.09369670153326143\n",
            "Epoch: 011, Loss: 1.7032, Train: 0.4445, Test: 0.4325\n",
            "Early stopping:  0.09683345919981162\n",
            "Epoch: 012, Loss: 1.6428, Train: 0.4679, Test: 0.4490\n",
            "Early stopping:  0.09686941314350074\n",
            "Epoch: 013, Loss: 1.5884, Train: 0.4846, Test: 0.4677\n",
            "Early stopping:  0.09442791034605434\n",
            "Epoch: 014, Loss: 1.5345, Train: 0.5055, Test: 0.4892\n",
            "Early stopping:  0.09123439142883648\n",
            "Epoch: 015, Loss: 1.4798, Train: 0.5361, Test: 0.5147\n",
            "Early stopping:  0.08778346136669314\n",
            "Epoch: 016, Loss: 1.4274, Train: 0.5598, Test: 0.5374\n",
            "Early stopping:  0.08529053959658754\n",
            "Epoch: 017, Loss: 1.3804, Train: 0.5690, Test: 0.5522\n",
            "Early stopping:  0.08275489612491557\n",
            "Epoch: 018, Loss: 1.3396, Train: 0.5772, Test: 0.5618\n",
            "Early stopping:  0.07745547901773961\n",
            "Epoch: 019, Loss: 1.3018, Train: 0.5876, Test: 0.5697\n",
            "Early stopping:  0.07031074324361306\n",
            "Epoch: 020, Loss: 1.2646, Train: 0.5989, Test: 0.5731\n",
            "Early stopping:  0.06397456251375895\n",
            "Epoch: 021, Loss: 1.2318, Train: 0.6090, Test: 0.5799\n",
            "Early stopping:  0.05889664324801292\n",
            "Epoch: 022, Loss: 1.2029, Train: 0.6169, Test: 0.5896\n",
            "Early stopping:  0.054398914368644205\n",
            "Epoch: 023, Loss: 1.1760, Train: 0.6265, Test: 0.5992\n",
            "Early stopping:  0.04965661650969593\n",
            "Epoch: 024, Loss: 1.1506, Train: 0.6322, Test: 0.6049\n",
            "Early stopping:  0.04491385590381458\n",
            "Epoch: 025, Loss: 1.1261, Train: 0.6403, Test: 0.6105\n",
            "Early stopping:  0.04171691917030575\n",
            "Epoch: 026, Loss: 1.1023, Train: 0.6491, Test: 0.6134\n",
            "Early stopping:  0.03972910167541954\n",
            "Epoch: 027, Loss: 1.0792, Train: 0.6580, Test: 0.6224\n",
            "Early stopping:  0.03826419193676517\n",
            "Epoch: 028, Loss: 1.0588, Train: 0.6652, Test: 0.6236\n",
            "Early stopping:  0.03645872614729076\n",
            "Epoch: 029, Loss: 1.0375, Train: 0.6732, Test: 0.6276\n",
            "Early stopping:  0.034885814206287225\n",
            "Epoch: 030, Loss: 1.0178, Train: 0.6797, Test: 0.6310\n",
            "Early stopping:  0.03330585313367186\n",
            "Epoch: 031, Loss: 0.9987, Train: 0.6886, Test: 0.6378\n",
            "Early stopping:  0.031967069851898774\n",
            "Epoch: 032, Loss: 0.9791, Train: 0.6963, Test: 0.6457\n",
            "Early stopping:  0.031380699387301876\n",
            "Epoch: 033, Loss: 0.9594, Train: 0.7034, Test: 0.6508\n",
            "Early stopping:  0.030830253059716838\n",
            "Epoch: 034, Loss: 0.9406, Train: 0.7112, Test: 0.6576\n",
            "Early stopping:  0.03062685498126078\n",
            "Epoch: 035, Loss: 0.9220, Train: 0.7174, Test: 0.6616\n",
            "Early stopping:  0.030323142325796016\n",
            "Epoch: 036, Loss: 0.9049, Train: 0.7211, Test: 0.6650\n",
            "Early stopping:  0.029389791497668058\n",
            "Epoch: 037, Loss: 0.8886, Train: 0.7276, Test: 0.6684\n",
            "Early stopping:  0.028054994059355076\n",
            "Epoch: 038, Loss: 0.8729, Train: 0.7347, Test: 0.6689\n",
            "Early stopping:  0.026693526712693448\n",
            "Epoch: 039, Loss: 0.8580, Train: 0.7412, Test: 0.6740\n",
            "Early stopping:  0.025291240170360274\n",
            "Epoch: 040, Loss: 0.8429, Train: 0.7482, Test: 0.6786\n",
            "Early stopping:  0.02442363092713025\n",
            "Epoch: 041, Loss: 0.8286, Train: 0.7547, Test: 0.6842\n",
            "Early stopping:  0.02372303458872814\n",
            "Epoch: 042, Loss: 0.8139, Train: 0.7612, Test: 0.6871\n",
            "Early stopping:  0.023307237585481706\n",
            "Epoch: 043, Loss: 0.7993, Train: 0.7665, Test: 0.6876\n",
            "Early stopping:  0.023157914009703448\n",
            "Epoch: 044, Loss: 0.7850, Train: 0.7731, Test: 0.6916\n",
            "Early stopping:  0.022957832434885618\n",
            "Epoch: 045, Loss: 0.7708, Train: 0.7785, Test: 0.6944\n",
            "Early stopping:  0.02285405378224117\n",
            "Epoch: 046, Loss: 0.7567, Train: 0.7855, Test: 0.6995\n",
            "Early stopping:  0.02259906107534302\n",
            "Epoch: 047, Loss: 0.7429, Train: 0.7890, Test: 0.7029\n",
            "Early stopping:  0.022309994588500397\n",
            "Epoch: 048, Loss: 0.7293, Train: 0.7917, Test: 0.7046\n",
            "Early stopping:  0.022039104097284844\n",
            "Epoch: 049, Loss: 0.7160, Train: 0.7974, Test: 0.7075\n",
            "Early stopping:  0.021682959940266793\n",
            "Epoch: 050, Loss: 0.7026, Train: 0.8018, Test: 0.7109\n",
            "Early stopping:  0.021357776350493102\n",
            "Epoch: 051, Loss: 0.6897, Train: 0.8079, Test: 0.7160\n",
            "Early stopping:  0.02102245701643857\n",
            "Epoch: 052, Loss: 0.6767, Train: 0.8099, Test: 0.7211\n",
            "Early stopping:  0.020789625636749336\n",
            "Epoch: 053, Loss: 0.6643, Train: 0.8157, Test: 0.7200\n",
            "Early stopping:  0.02043550856261851\n",
            "Epoch: 054, Loss: 0.6526, Train: 0.8137, Test: 0.7234\n",
            "Early stopping:  0.01983384971165655\n",
            "Epoch: 055, Loss: 0.6429, Train: 0.8216, Test: 0.7239\n",
            "Early stopping:  0.018624944480461737\n",
            "Epoch: 056, Loss: 0.6355, Train: 0.8168, Test: 0.7234\n",
            "Early stopping:  0.01647415681124115\n",
            "Epoch: 057, Loss: 0.6277, Train: 0.8306, Test: 0.7336\n",
            "Early stopping:  0.01436667416213797\n",
            "Epoch: 058, Loss: 0.6107, Train: 0.8306, Test: 0.7381\n",
            "Early stopping:  0.015884549909430203\n",
            "Epoch: 059, Loss: 0.5958, Train: 0.8292, Test: 0.7392\n",
            "Early stopping:  0.019148054875120765\n",
            "Epoch: 060, Loss: 0.5910, Train: 0.8378, Test: 0.7375\n",
            "Early stopping:  0.01940020289043213\n",
            "Epoch: 061, Loss: 0.5834, Train: 0.8395, Test: 0.7426\n",
            "Early stopping:  0.0176212195702246\n",
            "Epoch: 062, Loss: 0.5683, Train: 0.8438, Test: 0.7426\n",
            "Early stopping:  0.015655245331073077\n",
            "Epoch: 063, Loss: 0.5585, Train: 0.8483, Test: 0.7415\n",
            "Early stopping:  0.015633418945178282\n",
            "Epoch: 064, Loss: 0.5539, Train: 0.8460, Test: 0.7477\n",
            "Early stopping:  0.015880181248067733\n",
            "Epoch: 065, Loss: 0.5446, Train: 0.8554, Test: 0.7489\n",
            "Early stopping:  0.014795774306432433\n",
            "Epoch: 066, Loss: 0.5318, Train: 0.8578, Test: 0.7517\n",
            "Early stopping:  0.013889080007787123\n",
            "Epoch: 067, Loss: 0.5246, Train: 0.8547, Test: 0.7477\n",
            "Early stopping:  0.014357868414749498\n",
            "Epoch: 068, Loss: 0.5197, Train: 0.8635, Test: 0.7540\n",
            "Early stopping:  0.014167436652713227\n",
            "Epoch: 069, Loss: 0.5103, Train: 0.8625, Test: 0.7562\n",
            "Early stopping:  0.012921026204162784\n",
            "Epoch: 070, Loss: 0.5007, Train: 0.8664, Test: 0.7528\n",
            "Early stopping:  0.012193309126960494\n",
            "Epoch: 071, Loss: 0.4928, Train: 0.8747, Test: 0.7579\n",
            "Early stopping:  0.013115606161724868\n",
            "Epoch: 072, Loss: 0.4863, Train: 0.8715, Test: 0.7568\n",
            "Early stopping:  0.013344204487517391\n",
            "Epoch: 073, Loss: 0.4788, Train: 0.8775, Test: 0.7579\n",
            "Early stopping:  0.012274030831006956\n",
            "Epoch: 074, Loss: 0.4712, Train: 0.8768, Test: 0.7602\n",
            "Early stopping:  0.011547872416892414\n",
            "Epoch: 075, Loss: 0.4667, Train: 0.8757, Test: 0.7545\n",
            "Early stopping:  0.01070661606201541\n",
            "Epoch: 076, Loss: 0.4624, Train: 0.8788, Test: 0.7602\n",
            "Early stopping:  0.009574490758030829\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.78      0.76       190\n",
            "         capital_goods       0.72      0.63      0.67       127\n",
            "conglomerates_industry       1.00      0.75      0.86        20\n",
            "     consumer_cyclical       0.81      0.71      0.76       198\n",
            " consumer_non-cyclical       0.72      0.65      0.68       112\n",
            "                energy       0.91      0.75      0.82        71\n",
            "             financial       0.81      0.77      0.79       192\n",
            "            healthcare       0.88      0.65      0.74        79\n",
            "              services       0.70      0.86      0.77       519\n",
            "            technology       0.71      0.63      0.67        99\n",
            "        transportation       0.88      0.83      0.86       101\n",
            "             utilities       0.81      0.77      0.79        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.81      0.73      0.76      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4634, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2358, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.16094650758333912\n",
            "Epoch: 003, Loss: 2.1769, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1512821049960093\n",
            "Epoch: 004, Loss: 2.1290, Train: 0.3109, Test: 0.3050\n",
            "Early stopping:  0.14800391575575042\n",
            "Epoch: 005, Loss: 2.0566, Train: 0.3465, Test: 0.3458\n",
            "Early stopping:  0.1549491492974976\n",
            "Epoch: 006, Loss: 1.9950, Train: 0.3604, Test: 0.3554\n",
            "Early stopping:  0.09533548750190163\n",
            "Epoch: 007, Loss: 1.9352, Train: 0.3650, Test: 0.3605\n",
            "Early stopping:  0.09775035976690463\n",
            "Epoch: 008, Loss: 1.8762, Train: 0.3779, Test: 0.3685\n",
            "Early stopping:  0.09919285215393969\n",
            "Epoch: 009, Loss: 1.8183, Train: 0.4018, Test: 0.3883\n",
            "Early stopping:  0.09412071329571847\n",
            "Epoch: 010, Loss: 1.7573, Train: 0.4346, Test: 0.4167\n",
            "Early stopping:  0.09364530543862432\n",
            "Epoch: 011, Loss: 1.6919, Train: 0.4609, Test: 0.4456\n",
            "Early stopping:  0.09577368400306698\n",
            "Epoch: 012, Loss: 1.6305, Train: 0.4788, Test: 0.4558\n",
            "Early stopping:  0.0977240840098485\n",
            "Epoch: 013, Loss: 1.5771, Train: 0.4960, Test: 0.4728\n",
            "Early stopping:  0.0963932934202161\n",
            "Epoch: 014, Loss: 1.5214, Train: 0.5127, Test: 0.4938\n",
            "Early stopping:  0.09283139670255813\n",
            "Epoch: 015, Loss: 1.4645, Train: 0.5374, Test: 0.5164\n",
            "Early stopping:  0.08918215560469682\n",
            "Epoch: 016, Loss: 1.4137, Train: 0.5578, Test: 0.5351\n",
            "Early stopping:  0.08639067333277209\n",
            "Epoch: 017, Loss: 1.3662, Train: 0.5768, Test: 0.5505\n",
            "Early stopping:  0.08378385089641209\n",
            "Epoch: 018, Loss: 1.3232, Train: 0.5887, Test: 0.5618\n",
            "Early stopping:  0.0783125622218302\n",
            "Epoch: 019, Loss: 1.2859, Train: 0.5961, Test: 0.5697\n",
            "Early stopping:  0.07089502758005546\n",
            "Epoch: 020, Loss: 1.2497, Train: 0.6006, Test: 0.5754\n",
            "Early stopping:  0.0646736262856167\n",
            "Epoch: 021, Loss: 1.2170, Train: 0.6058, Test: 0.5816\n",
            "Early stopping:  0.058900124680901694\n",
            "Epoch: 022, Loss: 1.1901, Train: 0.6162, Test: 0.5958\n",
            "Early stopping:  0.05310548153367113\n",
            "Epoch: 023, Loss: 1.1639, Train: 0.6271, Test: 0.5952\n",
            "Early stopping:  0.04812402344211097\n",
            "Epoch: 024, Loss: 1.1388, Train: 0.6353, Test: 0.6020\n",
            "Early stopping:  0.043518426298729034\n",
            "Epoch: 025, Loss: 1.1142, Train: 0.6448, Test: 0.6128\n",
            "Early stopping:  0.040620077143678136\n",
            "Epoch: 026, Loss: 1.0904, Train: 0.6526, Test: 0.6224\n",
            "Early stopping:  0.03939029885806531\n",
            "Epoch: 027, Loss: 1.0674, Train: 0.6606, Test: 0.6236\n",
            "Early stopping:  0.03816663563956087\n",
            "Epoch: 028, Loss: 1.0441, Train: 0.6696, Test: 0.6276\n",
            "Early stopping:  0.037336272365875146\n",
            "Epoch: 029, Loss: 1.0229, Train: 0.6796, Test: 0.6338\n",
            "Early stopping:  0.03619273631404521\n",
            "Epoch: 030, Loss: 1.0016, Train: 0.6851, Test: 0.6389\n",
            "Early stopping:  0.035123243236400495\n",
            "Epoch: 031, Loss: 0.9808, Train: 0.6920, Test: 0.6468\n",
            "Early stopping:  0.03412546867244264\n",
            "Epoch: 032, Loss: 0.9612, Train: 0.7008, Test: 0.6508\n",
            "Early stopping:  0.03288947644918238\n",
            "Epoch: 033, Loss: 0.9425, Train: 0.7096, Test: 0.6525\n",
            "Early stopping:  0.03183436035899204\n",
            "Epoch: 034, Loss: 0.9240, Train: 0.7157, Test: 0.6616\n",
            "Early stopping:  0.03061088172800634\n",
            "Epoch: 035, Loss: 0.9060, Train: 0.7215, Test: 0.6667\n",
            "Early stopping:  0.029528320534713763\n",
            "Epoch: 036, Loss: 0.8896, Train: 0.7299, Test: 0.6689\n",
            "Early stopping:  0.028399466906447055\n",
            "Epoch: 037, Loss: 0.8730, Train: 0.7373, Test: 0.6695\n",
            "Early stopping:  0.02741393109239046\n",
            "Epoch: 038, Loss: 0.8569, Train: 0.7425, Test: 0.6706\n",
            "Early stopping:  0.026447158313107212\n",
            "Epoch: 039, Loss: 0.8417, Train: 0.7498, Test: 0.6757\n",
            "Early stopping:  0.025537890747210246\n",
            "Epoch: 040, Loss: 0.8263, Train: 0.7578, Test: 0.6808\n",
            "Early stopping:  0.024973892931578288\n",
            "Epoch: 041, Loss: 0.8112, Train: 0.7628, Test: 0.6848\n",
            "Early stopping:  0.024379642965444374\n",
            "Epoch: 042, Loss: 0.7963, Train: 0.7676, Test: 0.6888\n",
            "Early stopping:  0.02398687503471916\n",
            "Epoch: 043, Loss: 0.7816, Train: 0.7737, Test: 0.6927\n",
            "Early stopping:  0.023762133805546717\n",
            "Epoch: 044, Loss: 0.7669, Train: 0.7780, Test: 0.6967\n",
            "Early stopping:  0.02348805603490912\n",
            "Epoch: 045, Loss: 0.7527, Train: 0.7808, Test: 0.6961\n",
            "Early stopping:  0.023142594346057373\n",
            "Epoch: 046, Loss: 0.7386, Train: 0.7842, Test: 0.6978\n",
            "Early stopping:  0.02278854072955362\n",
            "Epoch: 047, Loss: 0.7249, Train: 0.7929, Test: 0.7024\n",
            "Early stopping:  0.02238348018594695\n",
            "Epoch: 048, Loss: 0.7114, Train: 0.7955, Test: 0.7029\n",
            "Early stopping:  0.021931282245780368\n",
            "Epoch: 049, Loss: 0.6982, Train: 0.7998, Test: 0.7063\n",
            "Early stopping:  0.021527117590143037\n",
            "Epoch: 050, Loss: 0.6853, Train: 0.8072, Test: 0.7086\n",
            "Early stopping:  0.021084419520578056\n",
            "Epoch: 051, Loss: 0.6729, Train: 0.8052, Test: 0.7137\n",
            "Early stopping:  0.02057224919205887\n",
            "Epoch: 052, Loss: 0.6623, Train: 0.8174, Test: 0.7103\n",
            "Early stopping:  0.01957093472126911\n",
            "Epoch: 053, Loss: 0.6530, Train: 0.8138, Test: 0.7211\n",
            "Early stopping:  0.018005436691710945\n",
            "Epoch: 054, Loss: 0.6433, Train: 0.8242, Test: 0.7211\n",
            "Early stopping:  0.016471157869221223\n",
            "Epoch: 055, Loss: 0.6271, Train: 0.8303, Test: 0.7171\n",
            "Early stopping:  0.0176042658803964\n",
            "Epoch: 056, Loss: 0.6149, Train: 0.8282, Test: 0.7279\n",
            "Early stopping:  0.019165307439063094\n",
            "Epoch: 057, Loss: 0.6082, Train: 0.8351, Test: 0.7273\n",
            "Early stopping:  0.01878316978648262\n",
            "Epoch: 058, Loss: 0.5968, Train: 0.8396, Test: 0.7341\n",
            "Early stopping:  0.01787210550835828\n",
            "Epoch: 059, Loss: 0.5835, Train: 0.8411, Test: 0.7398\n",
            "Early stopping:  0.016743753253914\n",
            "Epoch: 060, Loss: 0.5751, Train: 0.8481, Test: 0.7347\n",
            "Early stopping:  0.016591689691725038\n",
            "Epoch: 061, Loss: 0.5670, Train: 0.8496, Test: 0.7421\n",
            "Early stopping:  0.016553870327083928\n",
            "Epoch: 062, Loss: 0.5552, Train: 0.8533, Test: 0.7466\n",
            "Early stopping:  0.01580775242166878\n",
            "Epoch: 063, Loss: 0.5448, Train: 0.8596, Test: 0.7449\n",
            "Early stopping:  0.01543050223083623\n",
            "Epoch: 064, Loss: 0.5373, Train: 0.8585, Test: 0.7551\n",
            "Early stopping:  0.015501617484303279\n",
            "Epoch: 065, Loss: 0.5284, Train: 0.8625, Test: 0.7483\n",
            "Early stopping:  0.015109094353053406\n",
            "Epoch: 066, Loss: 0.5173, Train: 0.8660, Test: 0.7545\n",
            "Early stopping:  0.014630442904595187\n",
            "Epoch: 067, Loss: 0.5089, Train: 0.8667, Test: 0.7591\n",
            "Early stopping:  0.014555308259443964\n",
            "Epoch: 068, Loss: 0.5018, Train: 0.8715, Test: 0.7551\n",
            "Early stopping:  0.014354045530224947\n",
            "Epoch: 069, Loss: 0.4923, Train: 0.8741, Test: 0.7591\n",
            "Early stopping:  0.013879051649110618\n",
            "Epoch: 070, Loss: 0.4829, Train: 0.8751, Test: 0.7636\n",
            "Early stopping:  0.013493961718154004\n",
            "Epoch: 071, Loss: 0.4753, Train: 0.8788, Test: 0.7664\n",
            "Early stopping:  0.013623203643515222\n",
            "Epoch: 072, Loss: 0.4683, Train: 0.8766, Test: 0.7636\n",
            "Early stopping:  0.013325541040674412\n",
            "Epoch: 073, Loss: 0.4619, Train: 0.8863, Test: 0.7698\n",
            "Early stopping:  0.011951858968839785\n",
            "Epoch: 074, Loss: 0.4555, Train: 0.8825, Test: 0.7602\n",
            "Early stopping:  0.0107825128480612\n",
            "Epoch: 075, Loss: 0.4487, Train: 0.8878, Test: 0.7681\n",
            "Early stopping:  0.010410435157419125\n",
            "Epoch: 076, Loss: 0.4386, Train: 0.8918, Test: 0.7676\n",
            "Early stopping:  0.011553666550941017\n",
            "Epoch: 077, Loss: 0.4290, Train: 0.8925, Test: 0.7670\n",
            "Early stopping:  0.013182201928064265\n",
            "Epoch: 078, Loss: 0.4235, Train: 0.8954, Test: 0.7727\n",
            "Early stopping:  0.013325337120488952\n",
            "Epoch: 079, Loss: 0.4187, Train: 0.8975, Test: 0.7619\n",
            "Early stopping:  0.01206667392385852\n",
            "Epoch: 080, Loss: 0.4109, Train: 0.9016, Test: 0.7687\n",
            "Early stopping:  0.010443067479823943\n",
            "Epoch: 081, Loss: 0.4022, Train: 0.9023, Test: 0.7744\n",
            "Early stopping:  0.010514255715144888\n",
            "Epoch: 082, Loss: 0.3961, Train: 0.9033, Test: 0.7698\n",
            "Early stopping:  0.011306934768664697\n",
            "Epoch: 083, Loss: 0.3897, Train: 0.9085, Test: 0.7732\n",
            "Early stopping:  0.01154952552886945\n",
            "Epoch: 084, Loss: 0.3826, Train: 0.9094, Test: 0.7755\n",
            "Early stopping:  0.010978834638872973\n",
            "Epoch: 085, Loss: 0.3758, Train: 0.9112, Test: 0.7727\n",
            "Early stopping:  0.010502250478686236\n",
            "Epoch: 086, Loss: 0.3702, Train: 0.9111, Test: 0.7715\n",
            "Early stopping:  0.010392347103855226\n",
            "Epoch: 087, Loss: 0.3656, Train: 0.9188, Test: 0.7710\n",
            "Early stopping:  0.009596940453922167\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.79      0.78      0.79       190\n",
            "         capital_goods       0.74      0.64      0.69       127\n",
            "conglomerates_industry       0.95      0.90      0.92        20\n",
            "     consumer_cyclical       0.68      0.78      0.72       198\n",
            " consumer_non-cyclical       0.70      0.76      0.73       112\n",
            "                energy       0.87      0.76      0.81        71\n",
            "             financial       0.79      0.78      0.79       192\n",
            "            healthcare       0.90      0.76      0.82        79\n",
            "              services       0.76      0.79      0.78       519\n",
            "            technology       0.72      0.72      0.72        99\n",
            "        transportation       0.90      0.83      0.87       101\n",
            "             utilities       0.88      0.77      0.82        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.81      0.77      0.79      1764\n",
            "          weighted avg       0.78      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4595, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2652, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.137373438036408\n",
            "Epoch: 003, Loss: 2.1689, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1480151710873651\n",
            "Epoch: 004, Loss: 2.1393, Train: 0.2999, Test: 0.2988\n",
            "Early stopping:  0.1445536752758984\n",
            "Epoch: 005, Loss: 2.0896, Train: 0.3444, Test: 0.3384\n",
            "Early stopping:  0.1461493567019819\n",
            "Epoch: 006, Loss: 2.0370, Train: 0.3685, Test: 0.3600\n",
            "Early stopping:  0.08612994201576707\n",
            "Epoch: 007, Loss: 1.9798, Train: 0.3725, Test: 0.3622\n",
            "Early stopping:  0.07642569733677486\n",
            "Epoch: 008, Loss: 1.9225, Train: 0.3732, Test: 0.3617\n",
            "Early stopping:  0.08594323186233008\n",
            "Epoch: 009, Loss: 1.8690, Train: 0.3827, Test: 0.3713\n",
            "Early stopping:  0.08785937152266374\n",
            "Epoch: 010, Loss: 1.8147, Train: 0.4090, Test: 0.3968\n",
            "Early stopping:  0.087822454816779\n",
            "Epoch: 011, Loss: 1.7549, Train: 0.4395, Test: 0.4195\n",
            "Early stopping:  0.08818115475782368\n",
            "Epoch: 012, Loss: 1.6957, Train: 0.4582, Test: 0.4371\n",
            "Early stopping:  0.08980906339817624\n",
            "Epoch: 013, Loss: 1.6419, Train: 0.4771, Test: 0.4478\n",
            "Early stopping:  0.09064881501946982\n",
            "Epoch: 014, Loss: 1.5878, Train: 0.4896, Test: 0.4626\n",
            "Early stopping:  0.08963772970205289\n",
            "Epoch: 015, Loss: 1.5323, Train: 0.5121, Test: 0.4887\n",
            "Early stopping:  0.08745981389128288\n",
            "Epoch: 016, Loss: 1.4784, Train: 0.5429, Test: 0.5210\n",
            "Early stopping:  0.08603079320611112\n",
            "Epoch: 017, Loss: 1.4276, Train: 0.5636, Test: 0.5420\n",
            "Early stopping:  0.0850882008347595\n",
            "Epoch: 018, Loss: 1.3841, Train: 0.5765, Test: 0.5618\n",
            "Early stopping:  0.08105753209196069\n",
            "Epoch: 019, Loss: 1.3446, Train: 0.5847, Test: 0.5726\n",
            "Early stopping:  0.07442150208256818\n",
            "Epoch: 020, Loss: 1.3050, Train: 0.5945, Test: 0.5777\n",
            "Early stopping:  0.06806073751752473\n",
            "Epoch: 021, Loss: 1.2686, Train: 0.6041, Test: 0.5867\n",
            "Early stopping:  0.06281963331747958\n",
            "Epoch: 022, Loss: 1.2362, Train: 0.6141, Test: 0.5907\n",
            "Early stopping:  0.05885876787766542\n",
            "Epoch: 023, Loss: 1.2081, Train: 0.6197, Test: 0.5958\n",
            "Early stopping:  0.054178009068248094\n",
            "Epoch: 024, Loss: 1.1795, Train: 0.6260, Test: 0.6043\n",
            "Early stopping:  0.049318221307270725\n",
            "Epoch: 025, Loss: 1.1517, Train: 0.6363, Test: 0.6083\n",
            "Early stopping:  0.04594247656270146\n",
            "Epoch: 026, Loss: 1.1273, Train: 0.6475, Test: 0.6196\n",
            "Early stopping:  0.04337380040636101\n",
            "Epoch: 027, Loss: 1.1043, Train: 0.6556, Test: 0.6219\n",
            "Early stopping:  0.04111812072444992\n",
            "Epoch: 028, Loss: 1.0811, Train: 0.6627, Test: 0.6281\n",
            "Early stopping:  0.03863939239801797\n",
            "Epoch: 029, Loss: 1.0580, Train: 0.6675, Test: 0.6293\n",
            "Early stopping:  0.036915338630753344\n",
            "Epoch: 030, Loss: 1.0373, Train: 0.6766, Test: 0.6355\n",
            "Early stopping:  0.03578781205002053\n",
            "Epoch: 031, Loss: 1.0164, Train: 0.6845, Test: 0.6338\n",
            "Early stopping:  0.034739570657643874\n",
            "Epoch: 032, Loss: 0.9954, Train: 0.6936, Test: 0.6355\n",
            "Early stopping:  0.03367639862641113\n",
            "Epoch: 033, Loss: 0.9753, Train: 0.7007, Test: 0.6434\n",
            "Early stopping:  0.03278123973965917\n",
            "Epoch: 034, Loss: 0.9561, Train: 0.7061, Test: 0.6468\n",
            "Early stopping:  0.032189538983500306\n",
            "Epoch: 035, Loss: 0.9363, Train: 0.7143, Test: 0.6525\n",
            "Early stopping:  0.03156551522042569\n",
            "Epoch: 036, Loss: 0.9175, Train: 0.7191, Test: 0.6519\n",
            "Early stopping:  0.030820398655366205\n",
            "Epoch: 037, Loss: 0.8996, Train: 0.7268, Test: 0.6542\n",
            "Early stopping:  0.03003861072419331\n",
            "Epoch: 038, Loss: 0.8818, Train: 0.7357, Test: 0.6582\n",
            "Early stopping:  0.02928549718393344\n",
            "Epoch: 039, Loss: 0.8648, Train: 0.7438, Test: 0.6678\n",
            "Early stopping:  0.02824254624769412\n",
            "Epoch: 040, Loss: 0.8486, Train: 0.7485, Test: 0.6712\n",
            "Early stopping:  0.02729225603731305\n",
            "Epoch: 041, Loss: 0.8324, Train: 0.7556, Test: 0.6752\n",
            "Early stopping:  0.026522142304200143\n",
            "Epoch: 042, Loss: 0.8163, Train: 0.7629, Test: 0.6746\n",
            "Early stopping:  0.02587151002683726\n",
            "Epoch: 043, Loss: 0.8009, Train: 0.7672, Test: 0.6780\n",
            "Early stopping:  0.025344728415204503\n",
            "Epoch: 044, Loss: 0.7854, Train: 0.7739, Test: 0.6803\n",
            "Early stopping:  0.024967708832577873\n",
            "Epoch: 045, Loss: 0.7699, Train: 0.7792, Test: 0.6848\n",
            "Early stopping:  0.02463277642472488\n",
            "Epoch: 046, Loss: 0.7551, Train: 0.7852, Test: 0.6871\n",
            "Early stopping:  0.024243255185576344\n",
            "Epoch: 047, Loss: 0.7400, Train: 0.7903, Test: 0.6939\n",
            "Early stopping:  0.02403206913041771\n",
            "Epoch: 048, Loss: 0.7253, Train: 0.7968, Test: 0.6956\n",
            "Early stopping:  0.023728938442627846\n",
            "Epoch: 049, Loss: 0.7113, Train: 0.7982, Test: 0.7001\n",
            "Early stopping:  0.023241518361332664\n",
            "Epoch: 050, Loss: 0.6984, Train: 0.8052, Test: 0.6990\n",
            "Early stopping:  0.022480031159282824\n",
            "Epoch: 051, Loss: 0.6873, Train: 0.7999, Test: 0.7080\n",
            "Early stopping:  0.020962321464469996\n",
            "Epoch: 052, Loss: 0.6778, Train: 0.8130, Test: 0.7058\n",
            "Early stopping:  0.018867956205724068\n",
            "Epoch: 053, Loss: 0.6645, Train: 0.8153, Test: 0.7132\n",
            "Early stopping:  0.018088219544814918\n",
            "Epoch: 054, Loss: 0.6477, Train: 0.8182, Test: 0.7245\n",
            "Early stopping:  0.019780193516213303\n",
            "Epoch: 055, Loss: 0.6358, Train: 0.8255, Test: 0.7154\n",
            "Early stopping:  0.02114441254048846\n",
            "Epoch: 056, Loss: 0.6273, Train: 0.8218, Test: 0.7279\n",
            "Early stopping:  0.020625753201982646\n",
            "Epoch: 057, Loss: 0.6162, Train: 0.8296, Test: 0.7307\n",
            "Early stopping:  0.018619668825499057\n",
            "Epoch: 058, Loss: 0.6046, Train: 0.8347, Test: 0.7239\n",
            "Early stopping:  0.01674334751596647\n",
            "Epoch: 059, Loss: 0.5950, Train: 0.8340, Test: 0.7353\n",
            "Early stopping:  0.01652605233969438\n",
            "Epoch: 060, Loss: 0.5833, Train: 0.8423, Test: 0.7375\n",
            "Early stopping:  0.017294892907748247\n",
            "Epoch: 061, Loss: 0.5728, Train: 0.8413, Test: 0.7330\n",
            "Early stopping:  0.017085138125967562\n",
            "Epoch: 062, Loss: 0.5668, Train: 0.8466, Test: 0.7421\n",
            "Early stopping:  0.01551936614097784\n",
            "Epoch: 063, Loss: 0.5586, Train: 0.8477, Test: 0.7438\n",
            "Early stopping:  0.014216088540960388\n",
            "Epoch: 064, Loss: 0.5475, Train: 0.8551, Test: 0.7421\n",
            "Early stopping:  0.013612101844687058\n",
            "Epoch: 065, Loss: 0.5391, Train: 0.8534, Test: 0.7483\n",
            "Early stopping:  0.013792896313540413\n",
            "Epoch: 066, Loss: 0.5295, Train: 0.8642, Test: 0.7500\n",
            "Early stopping:  0.014918870091417428\n",
            "Epoch: 067, Loss: 0.5197, Train: 0.8613, Test: 0.7477\n",
            "Early stopping:  0.015178698180085526\n",
            "Epoch: 068, Loss: 0.5118, Train: 0.8654, Test: 0.7534\n",
            "Early stopping:  0.014383663488471726\n",
            "Epoch: 069, Loss: 0.5023, Train: 0.8727, Test: 0.7591\n",
            "Early stopping:  0.014417571059557157\n",
            "Epoch: 070, Loss: 0.4949, Train: 0.8683, Test: 0.7557\n",
            "Early stopping:  0.013686609407458724\n",
            "Epoch: 071, Loss: 0.4880, Train: 0.8781, Test: 0.7585\n",
            "Early stopping:  0.01269125664139066\n",
            "Epoch: 072, Loss: 0.4806, Train: 0.8652, Test: 0.7551\n",
            "Early stopping:  0.012154808380252086\n",
            "Epoch: 073, Loss: 0.4873, Train: 0.8552, Test: 0.7336\n",
            "Early stopping:  0.008273639090363707\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.75      0.74       190\n",
            "         capital_goods       0.64      0.76      0.69       127\n",
            "conglomerates_industry       1.00      0.70      0.82        20\n",
            "     consumer_cyclical       0.61      0.80      0.69       198\n",
            " consumer_non-cyclical       0.77      0.54      0.63       112\n",
            "                energy       0.90      0.77      0.83        71\n",
            "             financial       0.90      0.65      0.75       192\n",
            "            healthcare       0.91      0.67      0.77        79\n",
            "              services       0.72      0.76      0.74       519\n",
            "            technology       0.67      0.74      0.70        99\n",
            "        transportation       0.77      0.85      0.81       101\n",
            "             utilities       0.91      0.73      0.81        56\n",
            "\n",
            "              accuracy                           0.73      1764\n",
            "             macro avg       0.79      0.73      0.75      1764\n",
            "          weighted avg       0.75      0.73      0.73      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4734, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2404, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1647444443963528\n",
            "Epoch: 003, Loss: 2.1900, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15120148269649525\n",
            "Epoch: 004, Loss: 2.1281, Train: 0.3201, Test: 0.3226\n",
            "Early stopping:  0.15078906214684062\n",
            "Epoch: 005, Loss: 2.0698, Train: 0.3648, Test: 0.3566\n",
            "Early stopping:  0.1553531650470079\n",
            "Epoch: 006, Loss: 2.0167, Train: 0.3787, Test: 0.3713\n",
            "Early stopping:  0.08978496713788489\n",
            "Epoch: 007, Loss: 1.9593, Train: 0.3726, Test: 0.3628\n",
            "Early stopping:  0.09058845951508197\n",
            "Epoch: 008, Loss: 1.9023, Train: 0.3695, Test: 0.3588\n",
            "Early stopping:  0.08889955741258976\n",
            "Epoch: 009, Loss: 1.8501, Train: 0.3797, Test: 0.3713\n",
            "Early stopping:  0.08757915486281999\n",
            "Epoch: 010, Loss: 1.7976, Train: 0.4082, Test: 0.3934\n",
            "Early stopping:  0.08658257829204792\n",
            "Epoch: 011, Loss: 1.7370, Train: 0.4414, Test: 0.4286\n",
            "Early stopping:  0.0868746902974286\n",
            "Epoch: 012, Loss: 1.6737, Train: 0.4642, Test: 0.4439\n",
            "Early stopping:  0.09024326652791757\n",
            "Epoch: 013, Loss: 1.6166, Train: 0.4822, Test: 0.4592\n",
            "Early stopping:  0.09346945812145643\n",
            "Epoch: 014, Loss: 1.5636, Train: 0.4954, Test: 0.4745\n",
            "Early stopping:  0.09308658224873825\n",
            "Epoch: 015, Loss: 1.5099, Train: 0.5130, Test: 0.4915\n",
            "Early stopping:  0.08930166874125951\n",
            "Epoch: 016, Loss: 1.4583, Train: 0.5314, Test: 0.5102\n",
            "Early stopping:  0.08502506034088539\n",
            "Epoch: 017, Loss: 1.4116, Train: 0.5488, Test: 0.5198\n",
            "Early stopping:  0.08151867188841548\n",
            "Epoch: 018, Loss: 1.3672, Train: 0.5693, Test: 0.5374\n",
            "Early stopping:  0.0777294086040368\n",
            "Epoch: 019, Loss: 1.3255, Train: 0.5870, Test: 0.5516\n",
            "Early stopping:  0.07279004339755335\n",
            "Epoch: 020, Loss: 1.2878, Train: 0.5982, Test: 0.5618\n",
            "Early stopping:  0.06757546094390207\n",
            "Epoch: 021, Loss: 1.2535, Train: 0.6040, Test: 0.5669\n",
            "Early stopping:  0.06261918639694809\n",
            "Epoch: 022, Loss: 1.2210, Train: 0.6108, Test: 0.5777\n",
            "Early stopping:  0.05766074392895704\n",
            "Epoch: 023, Loss: 1.1918, Train: 0.6182, Test: 0.5845\n",
            "Early stopping:  0.05287299741860457\n",
            "Epoch: 024, Loss: 1.1653, Train: 0.6282, Test: 0.5890\n",
            "Early stopping:  0.048537778278201035\n",
            "Epoch: 025, Loss: 1.1394, Train: 0.6380, Test: 0.5935\n",
            "Early stopping:  0.04492212325566797\n",
            "Epoch: 026, Loss: 1.1162, Train: 0.6464, Test: 0.6032\n",
            "Early stopping:  0.041475968135261145\n",
            "Epoch: 027, Loss: 1.0927, Train: 0.6556, Test: 0.6088\n",
            "Early stopping:  0.03913884235178095\n",
            "Epoch: 028, Loss: 1.0699, Train: 0.6633, Test: 0.6117\n",
            "Early stopping:  0.03758406332258395\n",
            "Epoch: 029, Loss: 1.0482, Train: 0.6701, Test: 0.6179\n",
            "Early stopping:  0.036166461882741294\n",
            "Epoch: 030, Loss: 1.0274, Train: 0.6794, Test: 0.6259\n",
            "Early stopping:  0.03512050831385084\n",
            "Epoch: 031, Loss: 1.0064, Train: 0.6894, Test: 0.6338\n",
            "Early stopping:  0.03401724282377309\n",
            "Epoch: 032, Loss: 0.9851, Train: 0.6972, Test: 0.6372\n",
            "Early stopping:  0.033416790977511705\n",
            "Epoch: 033, Loss: 0.9659, Train: 0.7014, Test: 0.6395\n",
            "Early stopping:  0.03272074035884455\n",
            "Epoch: 034, Loss: 0.9471, Train: 0.7095, Test: 0.6508\n",
            "Early stopping:  0.0318003563157361\n",
            "Epoch: 035, Loss: 0.9281, Train: 0.7174, Test: 0.6627\n",
            "Early stopping:  0.030768468870527418\n",
            "Epoch: 036, Loss: 0.9110, Train: 0.7251, Test: 0.6650\n",
            "Early stopping:  0.029413259207157826\n",
            "Epoch: 037, Loss: 0.8941, Train: 0.7293, Test: 0.6701\n",
            "Early stopping:  0.028431779219205464\n",
            "Epoch: 038, Loss: 0.8777, Train: 0.7364, Test: 0.6723\n",
            "Early stopping:  0.027336811626440288\n",
            "Epoch: 039, Loss: 0.8616, Train: 0.7445, Test: 0.6774\n",
            "Early stopping:  0.026303320472136814\n",
            "Epoch: 040, Loss: 0.8458, Train: 0.7499, Test: 0.6774\n",
            "Early stopping:  0.02576929177439351\n",
            "Epoch: 041, Loss: 0.8305, Train: 0.7557, Test: 0.6837\n",
            "Early stopping:  0.02515598849963071\n",
            "Epoch: 042, Loss: 0.8154, Train: 0.7612, Test: 0.6854\n",
            "Early stopping:  0.024637485142216142\n",
            "Epoch: 043, Loss: 0.8002, Train: 0.7689, Test: 0.6865\n",
            "Early stopping:  0.024215787116488672\n",
            "Epoch: 044, Loss: 0.7854, Train: 0.7741, Test: 0.6871\n",
            "Early stopping:  0.023875534966424892\n",
            "Epoch: 045, Loss: 0.7706, Train: 0.7754, Test: 0.6876\n",
            "Early stopping:  0.02367315612634354\n",
            "Epoch: 046, Loss: 0.7557, Train: 0.7826, Test: 0.6910\n",
            "Early stopping:  0.023540978903463527\n",
            "Epoch: 047, Loss: 0.7411, Train: 0.7890, Test: 0.7012\n",
            "Early stopping:  0.023404572630036688\n",
            "Epoch: 048, Loss: 0.7266, Train: 0.7913, Test: 0.7035\n",
            "Early stopping:  0.023288516762491867\n",
            "Epoch: 049, Loss: 0.7123, Train: 0.7988, Test: 0.7035\n",
            "Early stopping:  0.023053304072777506\n",
            "Epoch: 050, Loss: 0.6982, Train: 0.8046, Test: 0.7080\n",
            "Early stopping:  0.02273849463296115\n",
            "Epoch: 051, Loss: 0.6844, Train: 0.8089, Test: 0.7109\n",
            "Early stopping:  0.022402144872370345\n",
            "Epoch: 052, Loss: 0.6709, Train: 0.8092, Test: 0.7160\n",
            "Early stopping:  0.022008483582867052\n",
            "Epoch: 053, Loss: 0.6590, Train: 0.8191, Test: 0.7160\n",
            "Early stopping:  0.02118687227415503\n",
            "Epoch: 054, Loss: 0.6484, Train: 0.8130, Test: 0.7239\n",
            "Early stopping:  0.01981287838560704\n",
            "Epoch: 055, Loss: 0.6385, Train: 0.8282, Test: 0.7251\n",
            "Early stopping:  0.01811588194478098\n",
            "Epoch: 056, Loss: 0.6235, Train: 0.8309, Test: 0.7256\n",
            "Early stopping:  0.018299093372678252\n",
            "Epoch: 057, Loss: 0.6072, Train: 0.8299, Test: 0.7347\n",
            "Early stopping:  0.020431256241522974\n",
            "Epoch: 058, Loss: 0.5972, Train: 0.8402, Test: 0.7302\n",
            "Early stopping:  0.021188127026631587\n",
            "Epoch: 059, Loss: 0.5894, Train: 0.8350, Test: 0.7381\n",
            "Early stopping:  0.019891354029312906\n",
            "Epoch: 060, Loss: 0.5777, Train: 0.8450, Test: 0.7381\n",
            "Early stopping:  0.017421482679709128\n",
            "Epoch: 061, Loss: 0.5629, Train: 0.8493, Test: 0.7415\n",
            "Early stopping:  0.017208741373512696\n",
            "Epoch: 062, Loss: 0.5529, Train: 0.8450, Test: 0.7404\n",
            "Early stopping:  0.018292524857375182\n",
            "Epoch: 063, Loss: 0.5459, Train: 0.8562, Test: 0.7341\n",
            "Early stopping:  0.017800107891615163\n",
            "Epoch: 064, Loss: 0.5359, Train: 0.8541, Test: 0.7438\n",
            "Early stopping:  0.01605189082934192\n",
            "Epoch: 065, Loss: 0.5240, Train: 0.8598, Test: 0.7460\n",
            "Early stopping:  0.015044592719988424\n",
            "Epoch: 066, Loss: 0.5124, Train: 0.8642, Test: 0.7443\n",
            "Early stopping:  0.01633240226470944\n",
            "Epoch: 067, Loss: 0.5043, Train: 0.8620, Test: 0.7472\n",
            "Early stopping:  0.01692146320722719\n",
            "Epoch: 068, Loss: 0.4980, Train: 0.8713, Test: 0.7443\n",
            "Early stopping:  0.015235635179172258\n",
            "Epoch: 069, Loss: 0.4891, Train: 0.8700, Test: 0.7511\n",
            "Early stopping:  0.013389787738589303\n",
            "Epoch: 070, Loss: 0.4788, Train: 0.8792, Test: 0.7489\n",
            "Early stopping:  0.013071490107635272\n",
            "Epoch: 071, Loss: 0.4685, Train: 0.8782, Test: 0.7517\n",
            "Early stopping:  0.014393278866579276\n",
            "Epoch: 072, Loss: 0.4597, Train: 0.8857, Test: 0.7540\n",
            "Early stopping:  0.015392862668600906\n",
            "Epoch: 073, Loss: 0.4512, Train: 0.8893, Test: 0.7534\n",
            "Early stopping:  0.015048949747194675\n",
            "Epoch: 074, Loss: 0.4435, Train: 0.8861, Test: 0.7523\n",
            "Early stopping:  0.013937823407501033\n",
            "Epoch: 075, Loss: 0.4375, Train: 0.8955, Test: 0.7540\n",
            "Early stopping:  0.012404632105821293\n",
            "Epoch: 076, Loss: 0.4333, Train: 0.8826, Test: 0.7494\n",
            "Early stopping:  0.010595483163240538\n",
            "Epoch: 077, Loss: 0.4329, Train: 0.8930, Test: 0.7602\n",
            "Early stopping:  0.0077109145964663565\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.78      0.77       190\n",
            "         capital_goods       0.73      0.67      0.70       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.65      0.82      0.72       198\n",
            " consumer_non-cyclical       0.71      0.72      0.72       112\n",
            "                energy       0.84      0.75      0.79        71\n",
            "             financial       0.72      0.80      0.76       192\n",
            "            healthcare       0.80      0.75      0.77        79\n",
            "              services       0.79      0.75      0.77       519\n",
            "            technology       0.75      0.72      0.73        99\n",
            "        transportation       0.92      0.80      0.86       101\n",
            "             utilities       0.86      0.77      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.79      0.76      0.77      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4837, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2615, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15715463991651102\n",
            "Epoch: 003, Loss: 2.1747, Train: 0.2948, Test: 0.2942\n",
            "Early stopping:  0.15938383878803442\n",
            "Epoch: 004, Loss: 2.1225, Train: 0.3132, Test: 0.3112\n",
            "Early stopping:  0.15941268808508882\n",
            "Epoch: 005, Loss: 2.0532, Train: 0.3720, Test: 0.3571\n",
            "Early stopping:  0.16631886870370097\n",
            "Epoch: 006, Loss: 2.0028, Train: 0.3905, Test: 0.3696\n",
            "Early stopping:  0.10145523871517202\n",
            "Epoch: 007, Loss: 1.9453, Train: 0.3892, Test: 0.3747\n",
            "Early stopping:  0.09157372689547372\n",
            "Epoch: 008, Loss: 1.8790, Train: 0.3879, Test: 0.3713\n",
            "Early stopping:  0.09417175812049842\n",
            "Epoch: 009, Loss: 1.8159, Train: 0.4068, Test: 0.3906\n",
            "Early stopping:  0.09473329782024309\n",
            "Epoch: 010, Loss: 1.7551, Train: 0.4364, Test: 0.4240\n",
            "Early stopping:  0.09880499832913092\n",
            "Epoch: 011, Loss: 1.6904, Train: 0.4676, Test: 0.4478\n",
            "Early stopping:  0.10021571992358375\n",
            "Epoch: 012, Loss: 1.6287, Train: 0.4865, Test: 0.4643\n",
            "Early stopping:  0.09902011131558985\n",
            "Epoch: 013, Loss: 1.5721, Train: 0.5019, Test: 0.4853\n",
            "Early stopping:  0.097104623117081\n",
            "Epoch: 014, Loss: 1.5149, Train: 0.5253, Test: 0.5011\n",
            "Early stopping:  0.0946789033791676\n",
            "Epoch: 015, Loss: 1.4590, Train: 0.5401, Test: 0.5170\n",
            "Early stopping:  0.09115950744486873\n",
            "Epoch: 016, Loss: 1.4097, Train: 0.5585, Test: 0.5385\n",
            "Early stopping:  0.08715837794665912\n",
            "Epoch: 017, Loss: 1.3627, Train: 0.5783, Test: 0.5522\n",
            "Early stopping:  0.08296130480175413\n",
            "Epoch: 018, Loss: 1.3177, Train: 0.5888, Test: 0.5652\n",
            "Early stopping:  0.07768689009294477\n",
            "Epoch: 019, Loss: 1.2782, Train: 0.6009, Test: 0.5737\n",
            "Early stopping:  0.07180326445588658\n",
            "Epoch: 020, Loss: 1.2408, Train: 0.6102, Test: 0.5799\n",
            "Early stopping:  0.06685773596417875\n",
            "Epoch: 021, Loss: 1.2057, Train: 0.6227, Test: 0.5884\n",
            "Early stopping:  0.06184684625122201\n",
            "Epoch: 022, Loss: 1.1757, Train: 0.6284, Test: 0.5941\n",
            "Early stopping:  0.056436979099109955\n",
            "Epoch: 023, Loss: 1.1479, Train: 0.6359, Test: 0.5986\n",
            "Early stopping:  0.051611021969067854\n",
            "Epoch: 024, Loss: 1.1214, Train: 0.6477, Test: 0.6071\n",
            "Early stopping:  0.04698016684133857\n",
            "Epoch: 025, Loss: 1.0964, Train: 0.6512, Test: 0.6151\n",
            "Early stopping:  0.04319058021346736\n",
            "Epoch: 026, Loss: 1.0738, Train: 0.6576, Test: 0.6236\n",
            "Early stopping:  0.04038731819887066\n",
            "Epoch: 027, Loss: 1.0515, Train: 0.6688, Test: 0.6236\n",
            "Early stopping:  0.038038516888589086\n",
            "Epoch: 028, Loss: 1.0287, Train: 0.6766, Test: 0.6315\n",
            "Early stopping:  0.03644186857741439\n",
            "Epoch: 029, Loss: 1.0067, Train: 0.6844, Test: 0.6412\n",
            "Early stopping:  0.0354889879012472\n",
            "Epoch: 030, Loss: 0.9850, Train: 0.6935, Test: 0.6429\n",
            "Early stopping:  0.03515359995183291\n",
            "Epoch: 031, Loss: 0.9642, Train: 0.6989, Test: 0.6491\n",
            "Early stopping:  0.03450994480114249\n",
            "Epoch: 032, Loss: 0.9439, Train: 0.7065, Test: 0.6514\n",
            "Early stopping:  0.0335312681847701\n",
            "Epoch: 033, Loss: 0.9245, Train: 0.7130, Test: 0.6582\n",
            "Early stopping:  0.032501689767312485\n",
            "Epoch: 034, Loss: 0.9063, Train: 0.7213, Test: 0.6616\n",
            "Early stopping:  0.031177083730916978\n",
            "Epoch: 035, Loss: 0.8888, Train: 0.7317, Test: 0.6650\n",
            "Early stopping:  0.02980705141737294\n",
            "Epoch: 036, Loss: 0.8714, Train: 0.7363, Test: 0.6701\n",
            "Early stopping:  0.02859622143218685\n",
            "Epoch: 037, Loss: 0.8553, Train: 0.7418, Test: 0.6712\n",
            "Early stopping:  0.027428311574643458\n",
            "Epoch: 038, Loss: 0.8389, Train: 0.7505, Test: 0.6774\n",
            "Early stopping:  0.026618264735982142\n",
            "Epoch: 039, Loss: 0.8228, Train: 0.7570, Test: 0.6842\n",
            "Early stopping:  0.025997754249529423\n",
            "Epoch: 040, Loss: 0.8072, Train: 0.7608, Test: 0.6865\n",
            "Early stopping:  0.025437970508551955\n",
            "Epoch: 041, Loss: 0.7916, Train: 0.7690, Test: 0.6916\n",
            "Early stopping:  0.025142344693921063\n",
            "Epoch: 042, Loss: 0.7765, Train: 0.7753, Test: 0.6950\n",
            "Early stopping:  0.0246747724005773\n",
            "Epoch: 043, Loss: 0.7612, Train: 0.7798, Test: 0.6973\n",
            "Early stopping:  0.02433868206690815\n",
            "Epoch: 044, Loss: 0.7464, Train: 0.7841, Test: 0.7029\n",
            "Early stopping:  0.024022681720362808\n",
            "Epoch: 045, Loss: 0.7316, Train: 0.7906, Test: 0.7063\n",
            "Early stopping:  0.02373449856294944\n",
            "Epoch: 046, Loss: 0.7169, Train: 0.7950, Test: 0.7103\n",
            "Early stopping:  0.023516860152191177\n",
            "Epoch: 047, Loss: 0.7025, Train: 0.8024, Test: 0.7120\n",
            "Early stopping:  0.023242412062378842\n",
            "Epoch: 048, Loss: 0.6884, Train: 0.8011, Test: 0.7132\n",
            "Early stopping:  0.022968525787957647\n",
            "Epoch: 049, Loss: 0.6753, Train: 0.8100, Test: 0.7137\n",
            "Early stopping:  0.022329050352243213\n",
            "Epoch: 050, Loss: 0.6657, Train: 0.8008, Test: 0.7137\n",
            "Early stopping:  0.020553890025049166\n",
            "Epoch: 051, Loss: 0.6650, Train: 0.8151, Test: 0.7143\n",
            "Early stopping:  0.016006938164586596\n",
            "Epoch: 052, Loss: 0.6566, Train: 0.8202, Test: 0.7279\n",
            "Early stopping:  0.012127710671973423\n",
            "Epoch: 053, Loss: 0.6265, Train: 0.8179, Test: 0.7290\n",
            "Early stopping:  0.018693622808496953\n",
            "Epoch: 054, Loss: 0.6226, Train: 0.8282, Test: 0.7251\n",
            "Early stopping:  0.021086703019375\n",
            "Epoch: 055, Loss: 0.6174, Train: 0.8335, Test: 0.7353\n",
            "Early stopping:  0.02159161145899843\n",
            "Epoch: 056, Loss: 0.5934, Train: 0.8263, Test: 0.7353\n",
            "Early stopping:  0.022629518604191004\n",
            "Epoch: 057, Loss: 0.5943, Train: 0.8435, Test: 0.7392\n",
            "Early stopping:  0.01585256583700784\n",
            "Epoch: 058, Loss: 0.5807, Train: 0.8472, Test: 0.7466\n",
            "Early stopping:  0.0176755575816822\n",
            "Epoch: 059, Loss: 0.5660, Train: 0.8338, Test: 0.7387\n",
            "Early stopping:  0.01900849077521307\n",
            "Epoch: 060, Loss: 0.5660, Train: 0.8514, Test: 0.7489\n",
            "Early stopping:  0.013943812760383523\n",
            "Epoch: 061, Loss: 0.5482, Train: 0.8542, Test: 0.7460\n",
            "Early stopping:  0.01736866583066244\n",
            "Epoch: 062, Loss: 0.5419, Train: 0.8497, Test: 0.7449\n",
            "Early stopping:  0.015517322809432726\n",
            "Epoch: 063, Loss: 0.5357, Train: 0.8613, Test: 0.7489\n",
            "Early stopping:  0.013883575434753704\n",
            "Epoch: 064, Loss: 0.5213, Train: 0.8630, Test: 0.7528\n",
            "Early stopping:  0.01642289553529539\n",
            "Epoch: 065, Loss: 0.5168, Train: 0.8594, Test: 0.7466\n",
            "Early stopping:  0.013395042739135646\n",
            "Epoch: 066, Loss: 0.5075, Train: 0.8683, Test: 0.7494\n",
            "Early stopping:  0.014031225387775991\n",
            "Epoch: 067, Loss: 0.4973, Train: 0.8730, Test: 0.7534\n",
            "Early stopping:  0.014474425261175665\n",
            "Epoch: 068, Loss: 0.4915, Train: 0.8720, Test: 0.7483\n",
            "Early stopping:  0.012579958973194512\n",
            "Epoch: 069, Loss: 0.4822, Train: 0.8791, Test: 0.7511\n",
            "Early stopping:  0.013519987594584736\n",
            "Epoch: 070, Loss: 0.4745, Train: 0.8798, Test: 0.7534\n",
            "Early stopping:  0.01288037246594318\n",
            "Epoch: 071, Loss: 0.4674, Train: 0.8825, Test: 0.7540\n",
            "Early stopping:  0.012197114822386664\n",
            "Epoch: 072, Loss: 0.4579, Train: 0.8881, Test: 0.7585\n",
            "Early stopping:  0.012980093316728685\n",
            "Epoch: 073, Loss: 0.4513, Train: 0.8863, Test: 0.7562\n",
            "Early stopping:  0.012379401043064736\n",
            "Epoch: 074, Loss: 0.4455, Train: 0.8910, Test: 0.7608\n",
            "Early stopping:  0.011730051462633024\n",
            "Epoch: 075, Loss: 0.4361, Train: 0.8924, Test: 0.7630\n",
            "Early stopping:  0.011894643332747364\n",
            "Epoch: 076, Loss: 0.4277, Train: 0.8949, Test: 0.7579\n",
            "Early stopping:  0.012019759678264078\n",
            "Epoch: 077, Loss: 0.4223, Train: 0.9002, Test: 0.7602\n",
            "Early stopping:  0.012033346384345119\n",
            "Epoch: 078, Loss: 0.4154, Train: 0.9000, Test: 0.7608\n",
            "Early stopping:  0.011779630587570792\n",
            "Epoch: 079, Loss: 0.4070, Train: 0.9042, Test: 0.7659\n",
            "Early stopping:  0.01118692584543665\n",
            "Epoch: 080, Loss: 0.4008, Train: 0.9006, Test: 0.7591\n",
            "Early stopping:  0.010956306899534425\n",
            "Epoch: 081, Loss: 0.3982, Train: 0.9027, Test: 0.7551\n",
            "Early stopping:  0.010071486497661857\n",
            "Epoch: 082, Loss: 0.4019, Train: 0.8782, Test: 0.7506\n",
            "Early stopping:  0.006784908961161034\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.84      0.68      0.75       190\n",
            "         capital_goods       0.76      0.61      0.68       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.74      0.70      0.72       198\n",
            " consumer_non-cyclical       0.78      0.55      0.65       112\n",
            "                energy       0.98      0.72      0.83        71\n",
            "             financial       0.82      0.70      0.76       192\n",
            "            healthcare       0.95      0.73      0.83        79\n",
            "              services       0.65      0.90      0.75       519\n",
            "            technology       0.77      0.64      0.70        99\n",
            "        transportation       0.88      0.83      0.85       101\n",
            "             utilities       0.87      0.73      0.80        56\n",
            "\n",
            "              accuracy                           0.75      1764\n",
            "             macro avg       0.84      0.72      0.77      1764\n",
            "          weighted avg       0.77      0.75      0.75      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5272, Train: 0.2969, Test: 0.2920\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3020, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  0.15924225761695263\n",
            "Epoch: 003, Loss: 2.1798, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1762447117223595\n",
            "Epoch: 004, Loss: 2.1562, Train: 0.2945, Test: 0.2937\n",
            "Early stopping:  0.16974762111631017\n",
            "Epoch: 005, Loss: 2.1067, Train: 0.3125, Test: 0.3141\n",
            "Early stopping:  0.16859001607798035\n",
            "Epoch: 006, Loss: 2.0491, Train: 0.3553, Test: 0.3469\n",
            "Early stopping:  0.09446282843621476\n",
            "Epoch: 007, Loss: 2.0002, Train: 0.3804, Test: 0.3702\n",
            "Early stopping:  0.07433793342129562\n",
            "Epoch: 008, Loss: 1.9529, Train: 0.3912, Test: 0.3855\n",
            "Early stopping:  0.08118580090815217\n",
            "Epoch: 009, Loss: 1.9032, Train: 0.3953, Test: 0.3861\n",
            "Early stopping:  0.0796290561320282\n",
            "Epoch: 010, Loss: 1.8510, Train: 0.4024, Test: 0.3968\n",
            "Early stopping:  0.07801303389165813\n",
            "Epoch: 011, Loss: 1.7972, Train: 0.4181, Test: 0.4065\n",
            "Early stopping:  0.08036152405690944\n",
            "Epoch: 012, Loss: 1.7429, Train: 0.4330, Test: 0.4155\n",
            "Early stopping:  0.08318718509622325\n",
            "Epoch: 013, Loss: 1.6900, Train: 0.4492, Test: 0.4308\n",
            "Early stopping:  0.08449753970670092\n",
            "Epoch: 014, Loss: 1.6406, Train: 0.4714, Test: 0.4512\n",
            "Early stopping:  0.0834718018554897\n",
            "Epoch: 015, Loss: 1.5895, Train: 0.4964, Test: 0.4785\n",
            "Early stopping:  0.0818566778664393\n",
            "Epoch: 016, Loss: 1.5352, Train: 0.5208, Test: 0.5034\n",
            "Early stopping:  0.08160282938958019\n",
            "Epoch: 017, Loss: 1.4859, Train: 0.5443, Test: 0.5198\n",
            "Early stopping:  0.08122031667574674\n",
            "Epoch: 018, Loss: 1.4434, Train: 0.5626, Test: 0.5323\n",
            "Early stopping:  0.07881183683339611\n",
            "Epoch: 019, Loss: 1.4008, Train: 0.5739, Test: 0.5471\n",
            "Early stopping:  0.07431172811648352\n",
            "Epoch: 020, Loss: 1.3575, Train: 0.5823, Test: 0.5595\n",
            "Early stopping:  0.06968880341289534\n",
            "Epoch: 021, Loss: 1.3172, Train: 0.5902, Test: 0.5646\n",
            "Early stopping:  0.06694625031814419\n",
            "Epoch: 022, Loss: 1.2806, Train: 0.5976, Test: 0.5675\n",
            "Early stopping:  0.06474125770247562\n",
            "Epoch: 023, Loss: 1.2476, Train: 0.6027, Test: 0.5782\n",
            "Early stopping:  0.06067935713797307\n",
            "Epoch: 024, Loss: 1.2191, Train: 0.6145, Test: 0.5867\n",
            "Early stopping:  0.05487365478019491\n",
            "Epoch: 025, Loss: 1.1946, Train: 0.6214, Test: 0.5918\n",
            "Early stopping:  0.04864838400551371\n",
            "Epoch: 026, Loss: 1.1712, Train: 0.6302, Test: 0.6032\n",
            "Early stopping:  0.04308191499997928\n",
            "Epoch: 027, Loss: 1.1480, Train: 0.6368, Test: 0.6066\n",
            "Early stopping:  0.03911516724513666\n",
            "Epoch: 028, Loss: 1.1267, Train: 0.6431, Test: 0.6094\n",
            "Early stopping:  0.03660113498796655\n",
            "Epoch: 029, Loss: 1.1050, Train: 0.6506, Test: 0.6162\n",
            "Early stopping:  0.035350753044777646\n",
            "Epoch: 030, Loss: 1.0839, Train: 0.6579, Test: 0.6190\n",
            "Early stopping:  0.03439475893624711\n",
            "Epoch: 031, Loss: 1.0627, Train: 0.6698, Test: 0.6264\n",
            "Early stopping:  0.03374305655467019\n",
            "Epoch: 032, Loss: 1.0426, Train: 0.6773, Test: 0.6310\n",
            "Early stopping:  0.03329398066324604\n",
            "Epoch: 033, Loss: 1.0248, Train: 0.6821, Test: 0.6332\n",
            "Early stopping:  0.031921302654356346\n",
            "Epoch: 034, Loss: 1.0069, Train: 0.6886, Test: 0.6429\n",
            "Early stopping:  0.030372312043341874\n",
            "Epoch: 035, Loss: 0.9887, Train: 0.6936, Test: 0.6491\n",
            "Early stopping:  0.029039529320174857\n",
            "Epoch: 036, Loss: 0.9714, Train: 0.7013, Test: 0.6553\n",
            "Early stopping:  0.028214229098117775\n",
            "Epoch: 037, Loss: 0.9544, Train: 0.7093, Test: 0.6565\n",
            "Early stopping:  0.02787999343884408\n",
            "Epoch: 038, Loss: 0.9376, Train: 0.7147, Test: 0.6599\n",
            "Early stopping:  0.027348469094210254\n",
            "Epoch: 039, Loss: 0.9205, Train: 0.7170, Test: 0.6627\n",
            "Early stopping:  0.02691724237976271\n",
            "Epoch: 040, Loss: 0.9050, Train: 0.7259, Test: 0.6672\n",
            "Early stopping:  0.02636428757399539\n",
            "Epoch: 041, Loss: 0.8894, Train: 0.7327, Test: 0.6718\n",
            "Early stopping:  0.025721013360612496\n",
            "Epoch: 042, Loss: 0.8741, Train: 0.7376, Test: 0.6746\n",
            "Early stopping:  0.025019067183082867\n",
            "Epoch: 043, Loss: 0.8594, Train: 0.7434, Test: 0.6786\n",
            "Early stopping:  0.024217132675731508\n",
            "Epoch: 044, Loss: 0.8451, Train: 0.7475, Test: 0.6842\n",
            "Early stopping:  0.023662339077138488\n",
            "Epoch: 045, Loss: 0.8301, Train: 0.7527, Test: 0.6871\n",
            "Early stopping:  0.023302367677284185\n",
            "Epoch: 046, Loss: 0.8153, Train: 0.7601, Test: 0.6888\n",
            "Early stopping:  0.02321017019053023\n",
            "Epoch: 047, Loss: 0.8008, Train: 0.7644, Test: 0.6944\n",
            "Early stopping:  0.023266021388222806\n",
            "Epoch: 048, Loss: 0.7861, Train: 0.7702, Test: 0.6956\n",
            "Early stopping:  0.0233277905951407\n",
            "Epoch: 049, Loss: 0.7717, Train: 0.7736, Test: 0.7001\n",
            "Early stopping:  0.023101727346568433\n",
            "Epoch: 050, Loss: 0.7578, Train: 0.7807, Test: 0.7001\n",
            "Early stopping:  0.022783245616994265\n",
            "Epoch: 051, Loss: 0.7440, Train: 0.7818, Test: 0.7046\n",
            "Early stopping:  0.022405379651033674\n",
            "Epoch: 052, Loss: 0.7308, Train: 0.7909, Test: 0.6995\n",
            "Early stopping:  0.021840433606920024\n",
            "Epoch: 053, Loss: 0.7181, Train: 0.7921, Test: 0.7149\n",
            "Early stopping:  0.021214042585846766\n",
            "Epoch: 054, Loss: 0.7055, Train: 0.8004, Test: 0.7109\n",
            "Early stopping:  0.020641826766342065\n",
            "Epoch: 055, Loss: 0.6926, Train: 0.8014, Test: 0.7160\n",
            "Early stopping:  0.020269289888867628\n",
            "Epoch: 056, Loss: 0.6795, Train: 0.8099, Test: 0.7183\n",
            "Early stopping:  0.020264849126396988\n",
            "Epoch: 057, Loss: 0.6666, Train: 0.8143, Test: 0.7239\n",
            "Early stopping:  0.02040206062257489\n",
            "Epoch: 058, Loss: 0.6546, Train: 0.8153, Test: 0.7268\n",
            "Early stopping:  0.020216390258662564\n",
            "Epoch: 059, Loss: 0.6433, Train: 0.8262, Test: 0.7245\n",
            "Early stopping:  0.01956138630819173\n",
            "Epoch: 060, Loss: 0.6330, Train: 0.8202, Test: 0.7285\n",
            "Early stopping:  0.0184267795605986\n",
            "Epoch: 061, Loss: 0.6245, Train: 0.8350, Test: 0.7256\n",
            "Early stopping:  0.016762662592250403\n",
            "Epoch: 062, Loss: 0.6164, Train: 0.8279, Test: 0.7341\n",
            "Early stopping:  0.015069038075975935\n",
            "Epoch: 063, Loss: 0.6048, Train: 0.8395, Test: 0.7398\n",
            "Early stopping:  0.014794794010951436\n",
            "Epoch: 064, Loss: 0.5896, Train: 0.8432, Test: 0.7398\n",
            "Early stopping:  0.01699514663360425\n",
            "Epoch: 065, Loss: 0.5791, Train: 0.8388, Test: 0.7381\n",
            "Early stopping:  0.01869980580474103\n",
            "Epoch: 066, Loss: 0.5732, Train: 0.8486, Test: 0.7438\n",
            "Early stopping:  0.01794409913170033\n",
            "Epoch: 067, Loss: 0.5651, Train: 0.8491, Test: 0.7415\n",
            "Early stopping:  0.01542592529497063\n",
            "Epoch: 068, Loss: 0.5525, Train: 0.8554, Test: 0.7438\n",
            "Early stopping:  0.014045270341648729\n",
            "Epoch: 069, Loss: 0.5411, Train: 0.8608, Test: 0.7477\n",
            "Early stopping:  0.01541914582356007\n",
            "Epoch: 070, Loss: 0.5343, Train: 0.8548, Test: 0.7460\n",
            "Early stopping:  0.016149835083573233\n",
            "Epoch: 071, Loss: 0.5283, Train: 0.8657, Test: 0.7494\n",
            "Early stopping:  0.014713718952865036\n",
            "Epoch: 072, Loss: 0.5189, Train: 0.8640, Test: 0.7483\n",
            "Early stopping:  0.012742878361958546\n",
            "Epoch: 073, Loss: 0.5074, Train: 0.8689, Test: 0.7511\n",
            "Early stopping:  0.013220286470520131\n",
            "Epoch: 074, Loss: 0.4978, Train: 0.8744, Test: 0.7551\n",
            "Early stopping:  0.0148987554586704\n",
            "Epoch: 075, Loss: 0.4914, Train: 0.8708, Test: 0.7517\n",
            "Early stopping:  0.015057588830313181\n",
            "Epoch: 076, Loss: 0.4860, Train: 0.8774, Test: 0.7528\n",
            "Early stopping:  0.01308828617335872\n",
            "Epoch: 077, Loss: 0.4796, Train: 0.8761, Test: 0.7523\n",
            "Early stopping:  0.01071550878755169\n",
            "Epoch: 078, Loss: 0.4709, Train: 0.8846, Test: 0.7551\n",
            "Early stopping:  0.01041298667798506\n",
            "Epoch: 079, Loss: 0.4604, Train: 0.8847, Test: 0.7562\n",
            "Early stopping:  0.012286758730241647\n",
            "Epoch: 080, Loss: 0.4513, Train: 0.8876, Test: 0.7579\n",
            "Early stopping:  0.014062534762286463\n",
            "Epoch: 081, Loss: 0.4451, Train: 0.8901, Test: 0.7511\n",
            "Early stopping:  0.014052359568552547\n",
            "Epoch: 082, Loss: 0.4411, Train: 0.8891, Test: 0.7557\n",
            "Early stopping:  0.012035130597146929\n",
            "Epoch: 083, Loss: 0.4369, Train: 0.8954, Test: 0.7523\n",
            "Early stopping:  0.009209946361470805\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.79      0.75       190\n",
            "         capital_goods       0.72      0.62      0.67       127\n",
            "conglomerates_industry       1.00      0.85      0.92        20\n",
            "     consumer_cyclical       0.68      0.79      0.73       198\n",
            " consumer_non-cyclical       0.72      0.62      0.67       112\n",
            "                energy       0.92      0.76      0.83        71\n",
            "             financial       0.79      0.76      0.77       192\n",
            "            healthcare       0.86      0.75      0.80        79\n",
            "              services       0.74      0.79      0.76       519\n",
            "            technology       0.69      0.65      0.67        99\n",
            "        transportation       0.86      0.82      0.84       101\n",
            "             utilities       0.85      0.73      0.79        56\n",
            "\n",
            "              accuracy                           0.75      1764\n",
            "             macro avg       0.79      0.74      0.77      1764\n",
            "          weighted avg       0.76      0.75      0.75      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5290, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3191, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.14840545992783155\n",
            "Epoch: 003, Loss: 2.1802, Train: 0.2945, Test: 0.2942\n",
            "Early stopping:  0.17556996863186233\n",
            "Epoch: 004, Loss: 2.1603, Train: 0.3004, Test: 0.2988\n",
            "Early stopping:  0.16992052167857935\n",
            "Epoch: 005, Loss: 2.1038, Train: 0.3270, Test: 0.3243\n",
            "Early stopping:  0.17068919587651\n",
            "Epoch: 006, Loss: 2.0310, Train: 0.3563, Test: 0.3537\n",
            "Early stopping:  0.10668863605752518\n",
            "Epoch: 007, Loss: 1.9742, Train: 0.3737, Test: 0.3645\n",
            "Early stopping:  0.0868466597418903\n",
            "Epoch: 008, Loss: 1.9280, Train: 0.3902, Test: 0.3815\n",
            "Early stopping:  0.09418228372808644\n",
            "Epoch: 009, Loss: 1.8776, Train: 0.4066, Test: 0.3951\n",
            "Early stopping:  0.08814508349428272\n",
            "Epoch: 010, Loss: 1.8183, Train: 0.4242, Test: 0.4138\n",
            "Early stopping:  0.08258442035872056\n",
            "Epoch: 011, Loss: 1.7581, Train: 0.4407, Test: 0.4246\n",
            "Early stopping:  0.08584572942150775\n",
            "Epoch: 012, Loss: 1.7047, Train: 0.4495, Test: 0.4274\n",
            "Early stopping:  0.08956347707274391\n",
            "Epoch: 013, Loss: 1.6568, Train: 0.4580, Test: 0.4382\n",
            "Early stopping:  0.0879011947800598\n",
            "Epoch: 014, Loss: 1.6071, Train: 0.4714, Test: 0.4541\n",
            "Early stopping:  0.08288752263732244\n",
            "Epoch: 015, Loss: 1.5541, Train: 0.4934, Test: 0.4779\n",
            "Early stopping:  0.0799534345293537\n",
            "Epoch: 016, Loss: 1.5036, Train: 0.5249, Test: 0.5062\n",
            "Early stopping:  0.07986429978709776\n",
            "Epoch: 017, Loss: 1.4599, Train: 0.5518, Test: 0.5300\n",
            "Early stopping:  0.07865535390480365\n",
            "Epoch: 018, Loss: 1.4195, Train: 0.5663, Test: 0.5346\n",
            "Early stopping:  0.07432617800434307\n",
            "Epoch: 019, Loss: 1.3762, Train: 0.5751, Test: 0.5499\n",
            "Early stopping:  0.06960242805150421\n",
            "Epoch: 020, Loss: 1.3317, Train: 0.5806, Test: 0.5601\n",
            "Early stopping:  0.06761248823043739\n",
            "Epoch: 021, Loss: 1.2921, Train: 0.5884, Test: 0.5703\n",
            "Early stopping:  0.06699323942496543\n",
            "Epoch: 022, Loss: 1.2578, Train: 0.5978, Test: 0.5805\n",
            "Early stopping:  0.0645416208497458\n",
            "Epoch: 023, Loss: 1.2253, Train: 0.6095, Test: 0.5873\n",
            "Early stopping:  0.05953862289193253\n",
            "Epoch: 024, Loss: 1.1947, Train: 0.6230, Test: 0.5986\n",
            "Early stopping:  0.05394851440851247\n",
            "Epoch: 025, Loss: 1.1688, Train: 0.6294, Test: 0.6032\n",
            "Early stopping:  0.04900954297361512\n",
            "Epoch: 026, Loss: 1.1460, Train: 0.6366, Test: 0.6077\n",
            "Early stopping:  0.04438940365576807\n",
            "Epoch: 027, Loss: 1.1217, Train: 0.6416, Test: 0.6088\n",
            "Early stopping:  0.04051899968320805\n",
            "Epoch: 028, Loss: 1.0981, Train: 0.6454, Test: 0.6105\n",
            "Early stopping:  0.0379940829775186\n",
            "Epoch: 029, Loss: 1.0769, Train: 0.6533, Test: 0.6156\n",
            "Early stopping:  0.036642261097347396\n",
            "Epoch: 030, Loss: 1.0545, Train: 0.6658, Test: 0.6304\n",
            "Early stopping:  0.03600342013803354\n",
            "Epoch: 031, Loss: 1.0320, Train: 0.6756, Test: 0.6366\n",
            "Early stopping:  0.03527197718874443\n",
            "Epoch: 032, Loss: 1.0121, Train: 0.6834, Test: 0.6389\n",
            "Early stopping:  0.034312874506912006\n",
            "Epoch: 033, Loss: 0.9926, Train: 0.6877, Test: 0.6423\n",
            "Early stopping:  0.03340159897649379\n",
            "Epoch: 034, Loss: 0.9725, Train: 0.6949, Test: 0.6485\n",
            "Early stopping:  0.03216832917931291\n",
            "Epoch: 035, Loss: 0.9548, Train: 0.6997, Test: 0.6553\n",
            "Early stopping:  0.030664210656644136\n",
            "Epoch: 036, Loss: 0.9376, Train: 0.7102, Test: 0.6604\n",
            "Early stopping:  0.02955397707611147\n",
            "Epoch: 037, Loss: 0.9198, Train: 0.7196, Test: 0.6706\n",
            "Early stopping:  0.028544977792088238\n",
            "Epoch: 038, Loss: 0.9038, Train: 0.7259, Test: 0.6780\n",
            "Early stopping:  0.02729059037159651\n",
            "Epoch: 039, Loss: 0.8879, Train: 0.7326, Test: 0.6797\n",
            "Early stopping:  0.02652013463275983\n",
            "Epoch: 040, Loss: 0.8719, Train: 0.7366, Test: 0.6814\n",
            "Early stopping:  0.025844516662824106\n",
            "Epoch: 041, Loss: 0.8567, Train: 0.7451, Test: 0.6808\n",
            "Early stopping:  0.0249997744327884\n",
            "Epoch: 042, Loss: 0.8415, Train: 0.7530, Test: 0.6825\n",
            "Early stopping:  0.024632164113368632\n",
            "Epoch: 043, Loss: 0.8264, Train: 0.7600, Test: 0.6848\n",
            "Early stopping:  0.024255603901740724\n",
            "Epoch: 044, Loss: 0.8119, Train: 0.7632, Test: 0.6899\n",
            "Early stopping:  0.023748200400849434\n",
            "Epoch: 045, Loss: 0.7974, Train: 0.7668, Test: 0.6910\n",
            "Early stopping:  0.023433629789164877\n",
            "Epoch: 046, Loss: 0.7834, Train: 0.7690, Test: 0.6922\n",
            "Early stopping:  0.02295972121918213\n",
            "Epoch: 047, Loss: 0.7695, Train: 0.7783, Test: 0.6922\n",
            "Early stopping:  0.022506013889724907\n",
            "Epoch: 048, Loss: 0.7557, Train: 0.7842, Test: 0.6939\n",
            "Early stopping:  0.022175689226798034\n",
            "Epoch: 049, Loss: 0.7424, Train: 0.7882, Test: 0.6973\n",
            "Early stopping:  0.021768778882156985\n",
            "Epoch: 050, Loss: 0.7290, Train: 0.7926, Test: 0.6990\n",
            "Early stopping:  0.02147971169731672\n",
            "Epoch: 051, Loss: 0.7158, Train: 0.7961, Test: 0.7001\n",
            "Early stopping:  0.021218258161425977\n",
            "Epoch: 052, Loss: 0.7027, Train: 0.8016, Test: 0.7029\n",
            "Early stopping:  0.02096771144920562\n",
            "Epoch: 053, Loss: 0.6896, Train: 0.8066, Test: 0.7041\n",
            "Early stopping:  0.02083270161573839\n",
            "Epoch: 054, Loss: 0.6769, Train: 0.8092, Test: 0.7092\n",
            "Early stopping:  0.02062545943023928\n",
            "Epoch: 055, Loss: 0.6642, Train: 0.8119, Test: 0.7092\n",
            "Early stopping:  0.02038788487598119\n",
            "Epoch: 056, Loss: 0.6529, Train: 0.8177, Test: 0.7149\n",
            "Early stopping:  0.019771908587798155\n",
            "Epoch: 057, Loss: 0.6459, Train: 0.8043, Test: 0.7154\n",
            "Early stopping:  0.01770399752968755\n",
            "Epoch: 058, Loss: 0.6468, Train: 0.8236, Test: 0.7194\n",
            "Early stopping:  0.013128723279224968\n",
            "Epoch: 059, Loss: 0.6302, Train: 0.8303, Test: 0.7239\n",
            "Early stopping:  0.01235179639984248\n",
            "Epoch: 060, Loss: 0.6065, Train: 0.8188, Test: 0.7313\n",
            "Early stopping:  0.018748289473153722\n",
            "Epoch: 061, Loss: 0.6096, Train: 0.8360, Test: 0.7285\n",
            "Early stopping:  0.019242160292484344\n",
            "Epoch: 062, Loss: 0.5941, Train: 0.8415, Test: 0.7307\n",
            "Early stopping:  0.020943487521804336\n",
            "Epoch: 063, Loss: 0.5790, Train: 0.8313, Test: 0.7336\n",
            "Early stopping:  0.019030162288848615\n",
            "Epoch: 064, Loss: 0.5788, Train: 0.8472, Test: 0.7381\n",
            "Early stopping:  0.014628483500750748\n",
            "Epoch: 065, Loss: 0.5603, Train: 0.8508, Test: 0.7364\n",
            "Early stopping:  0.01851457219787835\n",
            "Epoch: 066, Loss: 0.5553, Train: 0.8432, Test: 0.7387\n",
            "Early stopping:  0.015711877539918557\n",
            "Epoch: 067, Loss: 0.5478, Train: 0.8538, Test: 0.7455\n",
            "Early stopping:  0.014087362788256216\n",
            "Epoch: 068, Loss: 0.5338, Train: 0.8586, Test: 0.7404\n",
            "Early stopping:  0.016545071078769874\n",
            "Epoch: 069, Loss: 0.5308, Train: 0.8567, Test: 0.7426\n",
            "Early stopping:  0.012982826669505674\n",
            "Epoch: 070, Loss: 0.5193, Train: 0.8595, Test: 0.7477\n",
            "Early stopping:  0.014254868637144357\n",
            "Epoch: 071, Loss: 0.5108, Train: 0.8663, Test: 0.7421\n",
            "Early stopping:  0.014186960025766204\n",
            "Epoch: 072, Loss: 0.5053, Train: 0.8671, Test: 0.7438\n",
            "Early stopping:  0.012302142367273443\n",
            "Epoch: 073, Loss: 0.4951, Train: 0.8680, Test: 0.7511\n",
            "Early stopping:  0.013556728718275547\n",
            "Epoch: 074, Loss: 0.4881, Train: 0.8758, Test: 0.7489\n",
            "Early stopping:  0.01238682019571663\n",
            "Epoch: 075, Loss: 0.4815, Train: 0.8776, Test: 0.7460\n",
            "Early stopping:  0.012050492296756248\n",
            "Epoch: 076, Loss: 0.4729, Train: 0.8778, Test: 0.7528\n",
            "Early stopping:  0.012444706682394212\n",
            "Epoch: 077, Loss: 0.4656, Train: 0.8842, Test: 0.7562\n",
            "Early stopping:  0.01173394484740566\n",
            "Epoch: 078, Loss: 0.4590, Train: 0.8846, Test: 0.7523\n",
            "Early stopping:  0.01170809839885642\n",
            "Epoch: 079, Loss: 0.4523, Train: 0.8856, Test: 0.7568\n",
            "Early stopping:  0.011458005220311939\n",
            "Epoch: 080, Loss: 0.4443, Train: 0.8908, Test: 0.7574\n",
            "Early stopping:  0.01118128600391095\n",
            "Epoch: 081, Loss: 0.4380, Train: 0.8905, Test: 0.7523\n",
            "Early stopping:  0.011056916597963217\n",
            "Epoch: 082, Loss: 0.4327, Train: 0.8930, Test: 0.7579\n",
            "Early stopping:  0.010593009217265742\n",
            "Epoch: 083, Loss: 0.4247, Train: 0.8981, Test: 0.7579\n",
            "Early stopping:  0.010563445646621119\n",
            "Epoch: 084, Loss: 0.4178, Train: 0.8969, Test: 0.7540\n",
            "Early stopping:  0.010491982841434364\n",
            "Epoch: 085, Loss: 0.4132, Train: 0.9009, Test: 0.7534\n",
            "Early stopping:  0.010245179921651163\n",
            "Epoch: 086, Loss: 0.4071, Train: 0.9012, Test: 0.7585\n",
            "Early stopping:  0.00997796838592062\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.78      0.77      0.78       190\n",
            "         capital_goods       0.73      0.65      0.69       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.73      0.71      0.72       198\n",
            " consumer_non-cyclical       0.71      0.69      0.70       112\n",
            "                energy       0.91      0.73      0.81        71\n",
            "             financial       0.78      0.76      0.77       192\n",
            "            healthcare       0.87      0.75      0.80        79\n",
            "              services       0.72      0.81      0.76       519\n",
            "            technology       0.74      0.71      0.73        99\n",
            "        transportation       0.84      0.82      0.83       101\n",
            "             utilities       0.83      0.79      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.80      0.75      0.77      1764\n",
            "          weighted avg       0.76      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4797, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2438, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1667971645062274\n",
            "Epoch: 003, Loss: 2.1762, Train: 0.2946, Test: 0.2942\n",
            "Early stopping:  0.15932118464029596\n",
            "Epoch: 004, Loss: 2.1236, Train: 0.3109, Test: 0.3067\n",
            "Early stopping:  0.1571439215814212\n",
            "Epoch: 005, Loss: 2.0615, Train: 0.3417, Test: 0.3401\n",
            "Early stopping:  0.1614575474495217\n",
            "Epoch: 006, Loss: 2.0055, Train: 0.3736, Test: 0.3707\n",
            "Early stopping:  0.09354752648153263\n",
            "Epoch: 007, Loss: 1.9521, Train: 0.3868, Test: 0.3787\n",
            "Early stopping:  0.08957165059981512\n",
            "Epoch: 008, Loss: 1.8972, Train: 0.3920, Test: 0.3832\n",
            "Early stopping:  0.08892188088267056\n",
            "Epoch: 009, Loss: 1.8388, Train: 0.4018, Test: 0.3889\n",
            "Early stopping:  0.0875603500886087\n",
            "Epoch: 010, Loss: 1.7797, Train: 0.4159, Test: 0.4031\n",
            "Early stopping:  0.08932942762543178\n",
            "Epoch: 011, Loss: 1.7218, Train: 0.4370, Test: 0.4178\n",
            "Early stopping:  0.09141539130167019\n",
            "Epoch: 012, Loss: 1.6669, Train: 0.4534, Test: 0.4325\n",
            "Early stopping:  0.0913326890997133\n",
            "Epoch: 013, Loss: 1.6159, Train: 0.4798, Test: 0.4580\n",
            "Early stopping:  0.08837044824531831\n",
            "Epoch: 014, Loss: 1.5635, Train: 0.5065, Test: 0.4790\n",
            "Early stopping:  0.08515107999130679\n",
            "Epoch: 015, Loss: 1.5099, Train: 0.5344, Test: 0.5136\n",
            "Early stopping:  0.08337623470911786\n",
            "Epoch: 016, Loss: 1.4595, Train: 0.5531, Test: 0.5289\n",
            "Early stopping:  0.08236067778112223\n",
            "Epoch: 017, Loss: 1.4138, Train: 0.5664, Test: 0.5454\n",
            "Early stopping:  0.08036928575061784\n",
            "Epoch: 018, Loss: 1.3706, Train: 0.5735, Test: 0.5516\n",
            "Early stopping:  0.07624902298968403\n",
            "Epoch: 019, Loss: 1.3280, Train: 0.5829, Test: 0.5601\n",
            "Early stopping:  0.07161446123491379\n",
            "Epoch: 020, Loss: 1.2869, Train: 0.5918, Test: 0.5714\n",
            "Early stopping:  0.06815420183209858\n",
            "Epoch: 021, Loss: 1.2518, Train: 0.6017, Test: 0.5839\n",
            "Early stopping:  0.06450534076808842\n",
            "Epoch: 022, Loss: 1.2199, Train: 0.6100, Test: 0.5890\n",
            "Early stopping:  0.05980883871324694\n",
            "Epoch: 023, Loss: 1.1882, Train: 0.6206, Test: 0.5981\n",
            "Early stopping:  0.0548886011052877\n",
            "Epoch: 024, Loss: 1.1596, Train: 0.6339, Test: 0.6043\n",
            "Early stopping:  0.05037544428222891\n",
            "Epoch: 025, Loss: 1.1338, Train: 0.6420, Test: 0.6049\n",
            "Early stopping:  0.046929515517982315\n",
            "Epoch: 026, Loss: 1.1094, Train: 0.6489, Test: 0.6111\n",
            "Early stopping:  0.04362546149428496\n",
            "Epoch: 027, Loss: 1.0848, Train: 0.6617, Test: 0.6100\n",
            "Early stopping:  0.040644120390438715\n",
            "Epoch: 028, Loss: 1.0618, Train: 0.6651, Test: 0.6145\n",
            "Early stopping:  0.038664948899901165\n",
            "Epoch: 029, Loss: 1.0409, Train: 0.6738, Test: 0.6202\n",
            "Early stopping:  0.03691435217582091\n",
            "Epoch: 030, Loss: 1.0184, Train: 0.6857, Test: 0.6293\n",
            "Early stopping:  0.03572610407515599\n",
            "Epoch: 031, Loss: 0.9968, Train: 0.6902, Test: 0.6349\n",
            "Early stopping:  0.034698431483809904\n",
            "Epoch: 032, Loss: 0.9770, Train: 0.6967, Test: 0.6395\n",
            "Early stopping:  0.03380516535921004\n",
            "Epoch: 033, Loss: 0.9573, Train: 0.7040, Test: 0.6451\n",
            "Early stopping:  0.03298913847452556\n",
            "Epoch: 034, Loss: 0.9383, Train: 0.7109, Test: 0.6525\n",
            "Early stopping:  0.03157695125838298\n",
            "Epoch: 035, Loss: 0.9206, Train: 0.7176, Test: 0.6559\n",
            "Early stopping:  0.030205919953568323\n",
            "Epoch: 036, Loss: 0.9036, Train: 0.7258, Test: 0.6633\n",
            "Early stopping:  0.02901172929544034\n",
            "Epoch: 037, Loss: 0.8866, Train: 0.7320, Test: 0.6667\n",
            "Early stopping:  0.027852558916948295\n",
            "Epoch: 038, Loss: 0.8710, Train: 0.7394, Test: 0.6689\n",
            "Early stopping:  0.02666295580518604\n",
            "Epoch: 039, Loss: 0.8559, Train: 0.7466, Test: 0.6729\n",
            "Early stopping:  0.02563018316900113\n",
            "Epoch: 040, Loss: 0.8408, Train: 0.7499, Test: 0.6740\n",
            "Early stopping:  0.024720435106975747\n",
            "Epoch: 041, Loss: 0.8264, Train: 0.7533, Test: 0.6752\n",
            "Early stopping:  0.023826760091456133\n",
            "Epoch: 042, Loss: 0.8119, Train: 0.7607, Test: 0.6814\n",
            "Early stopping:  0.023368165478232606\n",
            "Epoch: 043, Loss: 0.7974, Train: 0.7676, Test: 0.6814\n",
            "Early stopping:  0.023083862880821295\n",
            "Epoch: 044, Loss: 0.7834, Train: 0.7710, Test: 0.6859\n",
            "Early stopping:  0.022751500946670316\n",
            "Epoch: 045, Loss: 0.7694, Train: 0.7736, Test: 0.6837\n",
            "Early stopping:  0.022535718831004845\n",
            "Epoch: 046, Loss: 0.7559, Train: 0.7787, Test: 0.6876\n",
            "Early stopping:  0.022141606698276542\n",
            "Epoch: 047, Loss: 0.7426, Train: 0.7851, Test: 0.6905\n",
            "Early stopping:  0.021669305122912257\n",
            "Epoch: 048, Loss: 0.7294, Train: 0.7909, Test: 0.6944\n",
            "Early stopping:  0.021287525503426778\n",
            "Epoch: 049, Loss: 0.7167, Train: 0.7950, Test: 0.6973\n",
            "Early stopping:  0.020845178381286404\n",
            "Epoch: 050, Loss: 0.7041, Train: 0.7980, Test: 0.6984\n",
            "Early stopping:  0.020482939536051167\n",
            "Epoch: 051, Loss: 0.6919, Train: 0.8029, Test: 0.7018\n",
            "Early stopping:  0.020061986773715144\n",
            "Epoch: 052, Loss: 0.6801, Train: 0.8052, Test: 0.7041\n",
            "Early stopping:  0.01953328702866416\n",
            "Epoch: 053, Loss: 0.6687, Train: 0.8109, Test: 0.7075\n",
            "Early stopping:  0.018972713311844358\n",
            "Epoch: 054, Loss: 0.6586, Train: 0.8109, Test: 0.7046\n",
            "Early stopping:  0.01807028143641815\n",
            "Epoch: 055, Loss: 0.6508, Train: 0.8153, Test: 0.7137\n",
            "Early stopping:  0.016441751281835358\n",
            "Epoch: 056, Loss: 0.6444, Train: 0.8221, Test: 0.7177\n",
            "Early stopping:  0.014215219992452581\n",
            "Epoch: 057, Loss: 0.6276, Train: 0.8245, Test: 0.7194\n",
            "Early stopping:  0.01546277692160911\n",
            "Epoch: 058, Loss: 0.6166, Train: 0.8289, Test: 0.7256\n",
            "Early stopping:  0.017183427183981297\n",
            "Epoch: 059, Loss: 0.6115, Train: 0.8306, Test: 0.7222\n",
            "Early stopping:  0.017073805313805562\n",
            "Epoch: 060, Loss: 0.5991, Train: 0.8378, Test: 0.7268\n",
            "Early stopping:  0.017125334322122956\n",
            "Epoch: 061, Loss: 0.5889, Train: 0.8371, Test: 0.7319\n",
            "Early stopping:  0.015100200488772218\n",
            "Epoch: 062, Loss: 0.5839, Train: 0.8446, Test: 0.7330\n",
            "Early stopping:  0.014061892043340634\n",
            "Epoch: 063, Loss: 0.5719, Train: 0.8436, Test: 0.7330\n",
            "Early stopping:  0.015055529994010755\n",
            "Epoch: 064, Loss: 0.5634, Train: 0.8500, Test: 0.7415\n",
            "Early stopping:  0.01406315704600787\n",
            "Epoch: 065, Loss: 0.5574, Train: 0.8507, Test: 0.7358\n",
            "Early stopping:  0.013313268238940225\n",
            "Epoch: 066, Loss: 0.5473, Train: 0.8562, Test: 0.7387\n",
            "Early stopping:  0.013955448249064988\n",
            "Epoch: 067, Loss: 0.5402, Train: 0.8538, Test: 0.7432\n",
            "Early stopping:  0.012597357148574916\n",
            "Epoch: 068, Loss: 0.5349, Train: 0.8601, Test: 0.7460\n",
            "Early stopping:  0.011792970506743888\n",
            "Epoch: 069, Loss: 0.5239, Train: 0.8588, Test: 0.7426\n",
            "Early stopping:  0.012618298383142736\n",
            "Epoch: 070, Loss: 0.5177, Train: 0.8662, Test: 0.7517\n",
            "Early stopping:  0.01198684279677238\n",
            "Epoch: 071, Loss: 0.5098, Train: 0.8677, Test: 0.7500\n",
            "Early stopping:  0.012390774930777303\n",
            "Epoch: 072, Loss: 0.5000, Train: 0.8677, Test: 0.7466\n",
            "Early stopping:  0.01331095212799713\n",
            "Epoch: 073, Loss: 0.4946, Train: 0.8730, Test: 0.7574\n",
            "Early stopping:  0.012118496836807493\n",
            "Epoch: 074, Loss: 0.4874, Train: 0.8758, Test: 0.7534\n",
            "Early stopping:  0.012015579570583143\n",
            "Epoch: 075, Loss: 0.4790, Train: 0.8755, Test: 0.7511\n",
            "Early stopping:  0.01177324750449508\n",
            "Epoch: 076, Loss: 0.4741, Train: 0.8774, Test: 0.7551\n",
            "Early stopping:  0.010693764664078863\n",
            "Epoch: 077, Loss: 0.4688, Train: 0.8839, Test: 0.7551\n",
            "Early stopping:  0.01030947901535944\n",
            "Epoch: 078, Loss: 0.4632, Train: 0.8701, Test: 0.7466\n",
            "Early stopping:  0.009315588840033315\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.82      0.69      0.75       190\n",
            "         capital_goods       0.77      0.63      0.69       127\n",
            "conglomerates_industry       1.00      0.70      0.82        20\n",
            "     consumer_cyclical       0.70      0.71      0.70       198\n",
            " consumer_non-cyclical       0.72      0.62      0.67       112\n",
            "                energy       0.89      0.79      0.84        71\n",
            "             financial       0.80      0.74      0.77       192\n",
            "            healthcare       0.87      0.68      0.77        79\n",
            "              services       0.66      0.86      0.75       519\n",
            "            technology       0.78      0.61      0.68        99\n",
            "        transportation       0.90      0.79      0.84       101\n",
            "             utilities       0.91      0.73      0.81        56\n",
            "\n",
            "              accuracy                           0.75      1764\n",
            "             macro avg       0.82      0.71      0.76      1764\n",
            "          weighted avg       0.76      0.75      0.75      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4704, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2602, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.14862664658881672\n",
            "Epoch: 003, Loss: 2.1701, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15410475681975525\n",
            "Epoch: 004, Loss: 2.1320, Train: 0.3020, Test: 0.3005\n",
            "Early stopping:  0.15134354432202726\n",
            "Epoch: 005, Loss: 2.0765, Train: 0.3511, Test: 0.3509\n",
            "Early stopping:  0.15421713270371365\n",
            "Epoch: 006, Loss: 2.0231, Train: 0.3720, Test: 0.3634\n",
            "Early stopping:  0.09060820842509647\n",
            "Epoch: 007, Loss: 1.9642, Train: 0.3664, Test: 0.3566\n",
            "Early stopping:  0.0825405040714931\n",
            "Epoch: 008, Loss: 1.9067, Train: 0.3637, Test: 0.3532\n",
            "Early stopping:  0.08901220448394645\n",
            "Epoch: 009, Loss: 1.8561, Train: 0.3771, Test: 0.3634\n",
            "Early stopping:  0.08809603637372962\n",
            "Epoch: 010, Loss: 1.8047, Train: 0.4098, Test: 0.3883\n",
            "Early stopping:  0.08618374800772134\n",
            "Epoch: 011, Loss: 1.7453, Train: 0.4407, Test: 0.4178\n",
            "Early stopping:  0.08539491148013631\n",
            "Epoch: 012, Loss: 1.6851, Train: 0.4625, Test: 0.4371\n",
            "Early stopping:  0.08769916369532053\n",
            "Epoch: 013, Loss: 1.6301, Train: 0.4794, Test: 0.4507\n",
            "Early stopping:  0.09042190121288413\n",
            "Epoch: 014, Loss: 1.5763, Train: 0.4900, Test: 0.4677\n",
            "Early stopping:  0.09047748413721787\n",
            "Epoch: 015, Loss: 1.5213, Train: 0.5045, Test: 0.4813\n",
            "Early stopping:  0.0880622375071904\n",
            "Epoch: 016, Loss: 1.4693, Train: 0.5348, Test: 0.5068\n",
            "Early stopping:  0.08544210903718233\n",
            "Epoch: 017, Loss: 1.4192, Train: 0.5606, Test: 0.5397\n",
            "Early stopping:  0.08362261047893271\n",
            "Epoch: 018, Loss: 1.3713, Train: 0.5809, Test: 0.5595\n",
            "Early stopping:  0.0809803026666232\n",
            "Epoch: 019, Loss: 1.3329, Train: 0.5911, Test: 0.5680\n",
            "Early stopping:  0.07517556613615417\n",
            "Epoch: 020, Loss: 1.2963, Train: 0.5966, Test: 0.5726\n",
            "Early stopping:  0.06854430090552863\n",
            "Epoch: 021, Loss: 1.2592, Train: 0.6033, Test: 0.5754\n",
            "Early stopping:  0.06256671276725428\n",
            "Epoch: 022, Loss: 1.2272, Train: 0.6092, Test: 0.5822\n",
            "Early stopping:  0.05726966956141799\n",
            "Epoch: 023, Loss: 1.1955, Train: 0.6175, Test: 0.5935\n",
            "Early stopping:  0.05442801320906331\n",
            "Epoch: 024, Loss: 1.1689, Train: 0.6267, Test: 0.5981\n",
            "Early stopping:  0.05044600929836755\n",
            "Epoch: 025, Loss: 1.1441, Train: 0.6345, Test: 0.6066\n",
            "Early stopping:  0.045686895527128214\n",
            "Epoch: 026, Loss: 1.1193, Train: 0.6386, Test: 0.6128\n",
            "Early stopping:  0.0422845808661701\n",
            "Epoch: 027, Loss: 1.0977, Train: 0.6512, Test: 0.6173\n",
            "Early stopping:  0.03876702182727675\n",
            "Epoch: 028, Loss: 1.0764, Train: 0.6611, Test: 0.6253\n",
            "Early stopping:  0.03660309149313432\n",
            "Epoch: 029, Loss: 1.0564, Train: 0.6679, Test: 0.6281\n",
            "Early stopping:  0.034551776157037535\n",
            "Epoch: 030, Loss: 1.0348, Train: 0.6694, Test: 0.6281\n",
            "Early stopping:  0.033269921019168036\n",
            "Epoch: 031, Loss: 1.0156, Train: 0.6757, Test: 0.6310\n",
            "Early stopping:  0.032568722321297025\n",
            "Epoch: 032, Loss: 0.9965, Train: 0.6855, Test: 0.6383\n",
            "Early stopping:  0.03174013585737913\n",
            "Epoch: 033, Loss: 0.9778, Train: 0.6946, Test: 0.6468\n",
            "Early stopping:  0.030930916752140977\n",
            "Epoch: 034, Loss: 0.9588, Train: 0.7013, Test: 0.6463\n",
            "Early stopping:  0.03002855295169544\n",
            "Epoch: 035, Loss: 0.9415, Train: 0.7106, Test: 0.6502\n",
            "Early stopping:  0.029400640154549445\n",
            "Epoch: 036, Loss: 0.9238, Train: 0.7187, Test: 0.6514\n",
            "Early stopping:  0.028740211454756817\n",
            "Epoch: 037, Loss: 0.9065, Train: 0.7232, Test: 0.6559\n",
            "Early stopping:  0.0280769120881941\n",
            "Epoch: 038, Loss: 0.8897, Train: 0.7291, Test: 0.6621\n",
            "Early stopping:  0.027360251054197187\n",
            "Epoch: 039, Loss: 0.8741, Train: 0.7380, Test: 0.6661\n",
            "Early stopping:  0.026699124026266584\n",
            "Epoch: 040, Loss: 0.8579, Train: 0.7461, Test: 0.6757\n",
            "Early stopping:  0.02597261029790092\n",
            "Epoch: 041, Loss: 0.8419, Train: 0.7516, Test: 0.6797\n",
            "Early stopping:  0.02545517280671587\n",
            "Epoch: 042, Loss: 0.8266, Train: 0.7595, Test: 0.6825\n",
            "Early stopping:  0.025054932666327627\n",
            "Epoch: 043, Loss: 0.8106, Train: 0.7673, Test: 0.6876\n",
            "Early stopping:  0.02502803996688007\n",
            "Epoch: 044, Loss: 0.7949, Train: 0.7697, Test: 0.6916\n",
            "Early stopping:  0.024871246628667597\n",
            "Epoch: 045, Loss: 0.7797, Train: 0.7774, Test: 0.6939\n",
            "Early stopping:  0.02469320065064716\n",
            "Epoch: 046, Loss: 0.7641, Train: 0.7819, Test: 0.6950\n",
            "Early stopping:  0.024632725134388307\n",
            "Epoch: 047, Loss: 0.7487, Train: 0.7858, Test: 0.7007\n",
            "Early stopping:  0.024436990359109897\n",
            "Epoch: 048, Loss: 0.7338, Train: 0.7920, Test: 0.7007\n",
            "Early stopping:  0.024211683359689113\n",
            "Epoch: 049, Loss: 0.7191, Train: 0.7951, Test: 0.7098\n",
            "Early stopping:  0.02394789914999831\n",
            "Epoch: 050, Loss: 0.7046, Train: 0.8031, Test: 0.7092\n",
            "Early stopping:  0.02348542167295581\n",
            "Epoch: 051, Loss: 0.6907, Train: 0.8067, Test: 0.7143\n",
            "Early stopping:  0.02297240988925212\n",
            "Epoch: 052, Loss: 0.6775, Train: 0.8141, Test: 0.7200\n",
            "Early stopping:  0.02232064275208457\n",
            "Epoch: 053, Loss: 0.6652, Train: 0.8137, Test: 0.7228\n",
            "Early stopping:  0.02136136399059987\n",
            "Epoch: 054, Loss: 0.6540, Train: 0.8189, Test: 0.7234\n",
            "Early stopping:  0.02005216736475521\n",
            "Epoch: 055, Loss: 0.6439, Train: 0.8165, Test: 0.7251\n",
            "Early stopping:  0.018530215935665385\n",
            "Epoch: 056, Loss: 0.6323, Train: 0.8282, Test: 0.7330\n",
            "Early stopping:  0.017680629019377475\n",
            "Epoch: 057, Loss: 0.6174, Train: 0.8316, Test: 0.7375\n",
            "Early stopping:  0.018586348005381042\n",
            "Epoch: 058, Loss: 0.6038, Train: 0.8321, Test: 0.7353\n",
            "Early stopping:  0.020122938086405123\n",
            "Epoch: 059, Loss: 0.5952, Train: 0.8372, Test: 0.7370\n",
            "Early stopping:  0.019959777742241818\n",
            "Epoch: 060, Loss: 0.5880, Train: 0.8350, Test: 0.7415\n",
            "Early stopping:  0.017720465680363756\n",
            "Epoch: 061, Loss: 0.5795, Train: 0.8429, Test: 0.7409\n",
            "Early stopping:  0.014593351715161\n",
            "Epoch: 062, Loss: 0.5711, Train: 0.8422, Test: 0.7404\n",
            "Early stopping:  0.012827884889239248\n",
            "Epoch: 063, Loss: 0.5595, Train: 0.8517, Test: 0.7477\n",
            "Early stopping:  0.014012974516769246\n",
            "Epoch: 064, Loss: 0.5463, Train: 0.8552, Test: 0.7500\n",
            "Early stopping:  0.016446199619884604\n",
            "Epoch: 065, Loss: 0.5384, Train: 0.8489, Test: 0.7517\n",
            "Early stopping:  0.016968771639131415\n",
            "Epoch: 066, Loss: 0.5344, Train: 0.8584, Test: 0.7517\n",
            "Early stopping:  0.015221042750687475\n",
            "Epoch: 067, Loss: 0.5281, Train: 0.8558, Test: 0.7557\n",
            "Early stopping:  0.012139950800943\n",
            "Epoch: 068, Loss: 0.5203, Train: 0.8616, Test: 0.7455\n",
            "Early stopping:  0.009924782983657239\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.78      0.76       190\n",
            "         capital_goods       0.77      0.65      0.70       127\n",
            "conglomerates_industry       1.00      0.75      0.86        20\n",
            "     consumer_cyclical       0.61      0.76      0.68       198\n",
            " consumer_non-cyclical       0.72      0.65      0.69       112\n",
            "                energy       0.86      0.77      0.81        71\n",
            "             financial       0.83      0.71      0.77       192\n",
            "            healthcare       0.92      0.71      0.80        79\n",
            "              services       0.72      0.78      0.75       519\n",
            "            technology       0.65      0.72      0.68        99\n",
            "        transportation       0.87      0.80      0.84       101\n",
            "             utilities       0.88      0.79      0.83        56\n",
            "\n",
            "              accuracy                           0.75      1764\n",
            "             macro avg       0.80      0.74      0.76      1764\n",
            "          weighted avg       0.76      0.75      0.75      1764\n",
            "\n",
            "time: 2min 27s (started: 2024-10-16 21:07:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk21vlU9Hugb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5cc6a8-76eb-47fc-a442-c3e4f2a7f565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 437 ms (started: 2024-10-16 21:10:06 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQn_F5KEXNpC"
      },
      "source": [
        "--------------------------------------\n",
        "# Hetrogenous Graph for Keyphrase = 3\n",
        "--------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upPyCTFBXNpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23382cd5-c99f-4ac4-ce43-a670510fd224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basic_materials' 'capital_goods' 'conglomerates_industry'\n",
            " 'consumer_cyclical' 'consumer_non-cyclical' 'energy' 'financial'\n",
            " 'healthcare' 'services' 'technology' 'transportation' 'utilities']\n",
            "12\n",
            "time: 2.66 ms (started: 2024-10-16 21:10:07 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(df[\"class\"].unique())\n",
        "class_number = len(df[\"class\"].unique())\n",
        "print(class_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ZkbthBXNpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9add5eef-70a8-43e5-fe47-513288aa387e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 468 µs (started: 2024-10-16 21:10:07 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Change here to change wich keypharse to use\n",
        "keyphrase = \"keyphrase3\"\n",
        "\n",
        "model_name = dataset_name+\"_\"+keyphrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-p0Ugv8eQpg"
      },
      "source": [
        "## Creating the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5eCUuXkeQpg"
      },
      "source": [
        "### Defining Graph Nodes and Edges 👀\n",
        "\n",
        "- `Nodes` - documents and contexts\n",
        "- `Edges`\n",
        "  - document <- has -> context\n",
        "- `Labels` - documents classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_uP4E2seQph"
      },
      "source": [
        "#### Nodes and Edges 👀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNqpuD_qeQph"
      },
      "source": [
        "##### Defining Docmuent nodes, Context nodes and edges between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rluwFHD8eQph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae26ffb-171c-4f3d-ad28-d38c904783af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 17.1 s (started: 2024-10-16 21:10:07 +00:00)\n"
          ]
        }
      ],
      "source": [
        "all_contexts_list =[]\n",
        "edges1,edges2 = [],[]\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes = []\n",
        "context_nodes = []\n",
        "\n",
        "edges_tuple = []\n",
        "\n",
        "sentences = []\n",
        "cont_sentences = 0\n",
        "dit_sentences = {}\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding document nodes for every doc in df\n",
        "    document_nodes.append(df[\"text_embeddings\"][i])\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list: # if NOT\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes.append(df[keyphrase+\"_embeddings\"][i][j])\n",
        "            # add a new edge between doc and new context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(new_edge_cont)\n",
        "            edges_tuple.append((df[\"id\"][i],new_edge_cont))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(all_contexts_list.index(key[0]))\n",
        "            edges_tuple.append((df[\"id\"][i],all_contexts_list.index(key[0])))\n",
        "            cont+=1\n",
        "\n",
        "    # organize sentences, sentences_embeddings, and a dict with the corresponding document for each sentence\n",
        "    aux = df['sentences_embeddings'][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        sentences.append(key)\n",
        "        dit_sentences[cont_sentences] = df[\"id\"][i]\n",
        "        cont_sentences += 1\n",
        "\n",
        "\n",
        "document_nodes = np.array(document_nodes)\n",
        "context_nodes = np.array(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEEarV3OeQpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ae66d9-e170-4a09-d0da-bb9f75dd5b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 8817\n",
            "number of context nodes: 35984\n",
            "number of shared contexts: 6890\n",
            "number of direct edges (first dimension): 42874\n",
            "number of direct edges (second dimension): 42874\n",
            "time: 6.57 ms (started: 2024-10-16 21:10:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"number of document nodes:\",len(document_nodes))\n",
        "print(\"number of context nodes:\",len(context_nodes))\n",
        "print(\"number of shared contexts:\",cont)\n",
        "print(\"number of direct edges (first dimension):\",len(edges1))\n",
        "print(\"number of direct edges (second dimension):\",len(edges2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLeSewX5eQpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb34e01d-4645-4d2a-8741-da681bdab4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 7s (started: 2024-10-16 21:10:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=15, metric='cosine').fit(context_nodes)\n",
        "\n",
        "neighbors_list = nbrs.kneighbors(sentences, return_distance=False)\n",
        "\n",
        "# cria aresta para cada vizinho encontrado\n",
        "for i,neighbors in enumerate(neighbors_list):\n",
        "        for n in neighbors:\n",
        "            edges1.append(dit_sentences[i])\n",
        "            edges2.append(n)\n",
        "            edges_tuple.append((dit_sentences[i],n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wvV-vByeQpi"
      },
      "source": [
        "##### Ajusting everything to Tensor Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcLYlutFeQpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03a7e0a-f1e1-4e90-cfcb-ce38427b06cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 197 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# transforms egdes to tensor\n",
        "edges = np.array([edges1,edges2])\n",
        "edges = torch.tensor(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3EFPPQfeQpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1851f666-bb71-4ddd-c11e-a435d315ef63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 58.1 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# transforms nodes to tensor\n",
        "document_nodes = np.array(document_nodes)\n",
        "document_nodes = torch.tensor(document_nodes)\n",
        "context_nodes = np.array(context_nodes)\n",
        "context_nodes = torch.tensor(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMc-Rgc4eQpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adef26d2-d3d0-4324-ea8b-8c08efd1d193"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.8602e-02,  1.4557e-02,  1.7552e-02,  ..., -1.8911e-02,\n",
              "         -1.8681e-02, -2.2412e-02],\n",
              "        [-1.2433e-01, -1.2993e-02,  1.4001e-02,  ..., -1.7944e-02,\n",
              "          1.1914e-02, -8.1062e-03],\n",
              "        [-8.5865e-02,  4.3530e-02, -7.1712e-02,  ...,  5.6326e-02,\n",
              "         -4.5610e-02,  3.1375e-02],\n",
              "        ...,\n",
              "        [-1.6584e-02, -3.9770e-02, -2.2835e-02,  ..., -4.2101e-05,\n",
              "          1.2257e-02,  2.9048e-02],\n",
              "        [ 3.1864e-02, -1.0648e-02, -3.8576e-02,  ..., -4.5610e-03,\n",
              "         -1.7202e-02,  4.2433e-02],\n",
              "        [ 4.3771e-02,  3.8128e-02,  2.0526e-02,  ..., -1.0110e-02,\n",
              "          3.8847e-02,  4.4935e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 274
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.14 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show documents nodes\n",
        "document_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO-MZZgkeQpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65093c94-7f7d-4b58-8714-dfe869cb8403"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 275
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.71 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of document nodes\n",
        "len(document_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PQJp_6oeQpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29a6e0b-5a06-4eb1-804a-73101be63927"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0714,  0.0858, -0.0434,  ...,  0.0149,  0.0713,  0.0182],\n",
              "        [-0.0430,  0.0382,  0.0263,  ...,  0.0298,  0.0570, -0.0218],\n",
              "        [-0.0816,  0.0714, -0.0450,  ...,  0.0140,  0.0811,  0.0313],\n",
              "        ...,\n",
              "        [-0.0464,  0.0437,  0.0029,  ...,  0.0401,  0.0498,  0.0721],\n",
              "        [-0.0457, -0.0080, -0.0750,  ...,  0.0681, -0.0170,  0.0555],\n",
              "        [-0.0652,  0.0149, -0.0086,  ...,  0.0067, -0.0111,  0.0143]])"
            ]
          },
          "metadata": {},
          "execution_count": 276
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show context nodes\n",
        "context_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBB5kzRseQpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f7a27a-3770-4b9d-caa6-83066bb42c16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35984"
            ]
          },
          "metadata": {},
          "execution_count": 277
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.82 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of context nodes\n",
        "len(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ERr5YqreQpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5d17d2-8d28-4872-bc1f-d52274186f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1946479\n",
            "1946479\n",
            "time: 481 µs (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of edges\n",
        "print(len(edges[0]))\n",
        "print(len(edges[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuWKiX2ceQpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ed8b17-eb55-49a2-96ae-dd16832563b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ...,  8816,  8816,  8816],\n",
            "        [    0,     1,     2,  ..., 32218, 12105, 23313]])\n",
            "time: 1.85 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# showing edges\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpojxPL5eQpk"
      },
      "source": [
        "#### Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z6PwJeteQpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1799c594-3d8e-40cb-c4f1-19e90648eb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basic_materials' 'capital_goods' 'conglomerates_industry'\n",
            " 'consumer_cyclical' 'consumer_non-cyclical' 'energy' 'financial'\n",
            " 'healthcare' 'services' 'technology' 'transportation' 'utilities']\n",
            "time: 1.68 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# All different classes\n",
        "print(df[\"class\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOPXet4DeQpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bea9e01-1aa3-49c5-a5c9-3a0ecbbbc32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'basic_materials': 0, 'capital_goods': 1, 'conglomerates_industry': 2, 'consumer_cyclical': 3, 'consumer_non-cyclical': 4, 'energy': 5, 'financial': 6, 'healthcare': 7, 'services': 8, 'technology': 9, 'transportation': 10, 'utilities': 11} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-281-a018e46c97dd>:12: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
            "<ipython-input-281-a018e46c97dd>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class\n",
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6915951a-d2b7-4352-ab3d-8da15266e649\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6915951a-d2b7-4352-ab3d-8da15266e649')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6915951a-d2b7-4352-ab3d-8da15266e649 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6915951a-d2b7-4352-ab3d-8da15266e649');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce569cd1-2243-4e5e-b35b-cbf42bfff9fb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce569cd1-2243-4e5e-b35b-cbf42bfff9fb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce569cd1-2243-4e5e-b35b-cbf42bfff9fb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels",
              "summary": "{\n  \"name\": \"labels\",\n  \"rows\": 8817,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10,\n          9,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 281
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 841 ms (started: 2024-10-16 21:12:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Creating labels for classification\n",
        "# the dictionary is a numeric representation for each possible \"document\" class\n",
        "\n",
        "dit = {}\n",
        "for i,classe in enumerate(df[\"class\"].unique()):\n",
        "  dit[classe] = i\n",
        "\n",
        "print(dit,'\\n')\n",
        "\n",
        "labels = df[[\"class\"]]\n",
        "for i in range(len(df[[\"class\"]])):\n",
        "    labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za7OEJ9IeQpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37a0c88-8baf-4237-dcfe-3149364163ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  ..., 11, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 282
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.26 ms (started: 2024-10-16 21:12:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# tranfors class dataframe into tensor\n",
        "y = labels[\"class\"].tolist()\n",
        "y = x_np = torch.tensor(y)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eFpsDwAeQpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d9a1f7-29ed-41b3-aa72-6bb91a7b5746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 283
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.17 ms (started: 2024-10-16 21:12:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p97x2PZeQpl"
      },
      "source": [
        "### Testing Graph with Networkx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4aNru07eQpl"
      },
      "source": [
        "#### Defining overal graph in networkx maner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OerHW_ZueQpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf3f076-1a0d-410a-9c61-9056553ce899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.6 s (started: 2024-10-16 21:12:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Run graph Representation for networkx\n",
        "all_contexts_list =[]\n",
        "edges_test = []\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes_test = []\n",
        "context_nodes_test = []\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding new documents for every node\n",
        "    document_nodes_test.append(\"doc_\"+str(i)) # in the actual graph nodes -> documents embeddings\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list:\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes_test.append(\"contx_\"+str(new_edge_cont)) # in the actual graph nodes -> context embeddings\n",
        "\n",
        "            # add a new edge between doc and new context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(new_edge_cont)))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(all_contexts_list.index(key[0]))))\n",
        "            cont+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWwW2UhteQpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d03020-8bbf-42f5-c6e9-d80155f0bfaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.17 s (started: 2024-10-16 21:12:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "edges_test = [(\"doc_\"+str(i[0]),\"contx_\"+str(i[1])) for i in edges_tuple]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZOoC3pkeQpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475b299a-0557-4170-c0f8-9b065bd9ef29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 8817\n",
            "number of context nodes: 35984\n",
            "number of edges: 1946479\n",
            "time: 2.71 ms (started: 2024-10-16 21:12:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"number of document nodes:\",len(document_nodes_test))\n",
        "print(\"number of context nodes:\",len(context_nodes_test))\n",
        "print(\"number of edges:\",len(edges_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj_plNzUeQpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad18291-de02-4650-a32d-739336124da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('doc_0', 'contx_0'), ('doc_0', 'contx_1'), ('doc_0', 'contx_2'), ('doc_0', 'contx_3'), ('doc_0', 'contx_4'), ('doc_1', 'contx_5'), ('doc_1', 'contx_6'), ('doc_1', 'contx_7'), ('doc_1', 'contx_8'), ('doc_1', 'contx_9'), ('doc_2', 'contx_10'), ('doc_2', 'contx_11'), ('doc_2', 'contx_12'), ('doc_2', 'contx_13'), ('doc_2', 'contx_14'), ('doc_3', 'contx_15'), ('doc_3', 'contx_16'), ('doc_3', 'contx_17'), ('doc_3', 'contx_18'), ('doc_3', 'contx_19'), ('doc_4', 'contx_20'), ('doc_4', 'contx_21'), ('doc_4', 'contx_22'), ('doc_4', 'contx_23'), ('doc_4', 'contx_24'), ('doc_5', 'contx_25'), ('doc_5', 'contx_26'), ('doc_5', 'contx_27'), ('doc_5', 'contx_28'), ('doc_5', 'contx_29'), ('doc_6', 'contx_30'), ('doc_6', 'contx_31'), ('doc_6', 'contx_32'), ('doc_6', 'contx_33'), ('doc_6', 'contx_34'), ('doc_7', 'contx_35'), ('doc_7', 'contx_36'), ('doc_7', 'contx_37'), ('doc_7', 'contx_38'), ('doc_7', 'contx_39'), ('doc_8', 'contx_40'), ('doc_8', 'contx_41'), ('doc_8', 'contx_42'), ('doc_8', 'contx_43'), ('doc_8', 'contx_44'), ('doc_9', 'contx_45'), ('doc_9', 'contx_46'), ('doc_9', 'contx_47'), ('doc_9', 'contx_48'), ('doc_9', 'contx_49'), ('doc_10', 'contx_50'), ('doc_10', 'contx_51'), ('doc_10', 'contx_52'), ('doc_10', 'contx_53'), ('doc_10', 'contx_54'), ('doc_11', 'contx_55'), ('doc_11', 'contx_56'), ('doc_11', 'contx_57'), ('doc_11', 'contx_58'), ('doc_11', 'contx_59'), ('doc_12', 'contx_60'), ('doc_12', 'contx_61'), ('doc_12', 'contx_62'), ('doc_12', 'contx_63'), ('doc_12', 'contx_64'), ('doc_13', 'contx_65'), ('doc_13', 'contx_66'), ('doc_13', 'contx_67'), ('doc_13', 'contx_68'), ('doc_13', 'contx_69'), ('doc_14', 'contx_70'), ('doc_14', 'contx_71'), ('doc_14', 'contx_72'), ('doc_14', 'contx_73'), ('doc_14', 'contx_74'), ('doc_15', 'contx_75'), ('doc_15', 'contx_76'), ('doc_15', 'contx_77'), ('doc_15', 'contx_78'), ('doc_15', 'contx_79'), ('doc_16', 'contx_80'), ('doc_16', 'contx_81'), ('doc_16', 'contx_82'), ('doc_16', 'contx_83'), ('doc_16', 'contx_84'), ('doc_17', 'contx_85'), ('doc_17', 'contx_86'), ('doc_17', 'contx_87'), ('doc_17', 'contx_88'), ('doc_17', 'contx_89'), ('doc_18', 'contx_90'), ('doc_18', 'contx_91'), ('doc_18', 'contx_92'), ('doc_18', 'contx_93'), ('doc_18', 'contx_94'), ('doc_19', 'contx_95'), ('doc_19', 'contx_96'), ('doc_19', 'contx_97'), ('doc_19', 'contx_98'), ('doc_19', 'contx_99')]\n",
            "time: 1.25 ms (started: 2024-10-16 21:12:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(edges_test[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgiblP-UeQpm"
      },
      "source": [
        "#### Test graph Conectivity with networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuUkEAGLeQpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0de8507-2e67-477e-ca63-0ed43ab1735e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.11 s (started: 2024-10-16 21:12:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define Bipartide graph\n",
        "B = nx.Graph()\n",
        "B.add_nodes_from(document_nodes_test, bipartite=0)\n",
        "B.add_nodes_from(context_nodes_test, bipartite=1)\n",
        "B.add_edges_from(edges_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFDpkX2oeQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8ec71b-b7f5-45c9-8e86-b91e1d6cf886"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 289
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 508 ms (started: 2024-10-16 21:12:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nx.is_connected(B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvLEAfMUeQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c054c59a-3bf8-4f9d-a916-8bb8eb9380b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "time: 747 ms (started: 2024-10-16 21:12:51 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Number of conected elements\n",
        "# if == 1 -> all elements of the graph are conected\n",
        "print(nx.number_connected_components(B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhhBhwAAeQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a8ae71d-9315-46bd-e087-4bc7f73b47c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44792\n",
            "time: 736 ms (started: 2024-10-16 21:12:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# size of a cluster\n",
        "print(len(nx.node_connected_component(B,'doc_0')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKQJ0GpkeQpn"
      },
      "source": [
        "### Creating Graph HeteroData Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr8Q3wkreQpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a438abc-9925-4e63-ade6-38c69325d734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 912 µs (started: 2024-10-16 21:12:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Defining nodes, edges and class labels\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# nodes\n",
        "data['document'].x = document_nodes\n",
        "data['concept'].x = context_nodes\n",
        "\n",
        "# edges\n",
        "data['document', 'has', 'concept'].edge_index = edges\n",
        "\n",
        "#class labels\n",
        "data['document'].y = y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQpYAViGeQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fa5623-2dea-45d6-aebb-a1247835a76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.74 ms (started: 2024-10-16 21:12:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Setting graph to undirected\n",
        "data = T.ToUndirected()(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHrIrV2geQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2aa4d0-6b3f-4319-b145-773383bb28a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 687 ms (started: 2024-10-16 21:12:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# removing duplicate edges\n",
        "data = T.RemoveDuplicatedEdges()(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSgA_wDDeQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8f2e90-204d-4a19-a3b8-7fd78b6c0f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 34.8 ms (started: 2024-10-16 21:12:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Ensure date in using gpu\n",
        "data = data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RM4Ioq_eQpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816c5d4b-ed20-40e6-943e-3025042f4ede"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  document={\n",
              "    x=[8817, 384],\n",
              "    y=[8817],\n",
              "  },\n",
              "  concept={ x=[35984, 384] },\n",
              "  (document, has, concept)={ edge_index=[2, 1104343] },\n",
              "  (concept, rev_has, document)={ edge_index=[2, 1104343] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 296
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.43 ms (started: 2024-10-16 21:12:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzrLTAjVgSl9"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HAAys2cgSl9"
      },
      "source": [
        "### Training rotulated base = 20% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOQ_Ta9rgSl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d1c9be-2b92-40e1-ad65-f27913995cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 332 µs (started: 2024-10-16 21:12:53 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg9zWmgSgSl9"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGL3JW7fgSl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bfb408-4285-4cce-aab1-1fa7e6f7d71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 61.1458, Train: 0.1055, Test: 0.1002\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 337.4022, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  195.34279611632547\n",
            "Epoch: 003, Loss: 379.3685, Train: 0.1180, Test: 0.1145\n",
            "Early stopping:  172.88939404578926\n",
            "Epoch: 004, Loss: 383.5331, Train: 0.0510, Test: 0.0517\n",
            "Early stopping:  154.2247998004672\n",
            "Epoch: 005, Loss: 464.8856, Train: 0.1225, Test: 0.1206\n",
            "Early stopping:  154.69530414284856\n",
            "Epoch: 006, Loss: 463.4489, Train: 0.0681, Test: 0.0658\n",
            "Early stopping:  56.31868349834021\n",
            "Epoch: 007, Loss: 424.4395, Train: 0.0675, Test: 0.0641\n",
            "Early stopping:  41.39397260041529\n",
            "Epoch: 008, Loss: 388.9366, Train: 0.3023, Test: 0.2967\n",
            "Early stopping:  39.017805003863636\n",
            "Epoch: 009, Loss: 279.5083, Train: 0.2972, Test: 0.2964\n",
            "Early stopping:  76.45881961173907\n",
            "Epoch: 010, Loss: 274.3239, Train: 0.1815, Test: 0.1758\n",
            "Early stopping:  85.61949972190847\n",
            "Epoch: 011, Loss: 258.5367, Train: 0.1469, Test: 0.1403\n",
            "Early stopping:  75.87971221689408\n",
            "Epoch: 012, Loss: 208.6285, Train: 0.1940, Test: 0.1873\n",
            "Early stopping:  66.01980291631907\n",
            "Epoch: 013, Loss: 167.4671, Train: 0.3267, Test: 0.3201\n",
            "Early stopping:  48.221405299163486\n",
            "Epoch: 014, Loss: 123.9867, Train: 0.2660, Test: 0.2667\n",
            "Early stopping:  62.53876232205555\n",
            "Epoch: 015, Loss: 106.1972, Train: 0.2569, Test: 0.2535\n",
            "Early stopping:  62.212203170870595\n",
            "Epoch: 016, Loss: 76.4268, Train: 0.3210, Test: 0.3201\n",
            "Early stopping:  52.06993510612939\n",
            "Epoch: 017, Loss: 46.8843, Train: 0.2184, Test: 0.2145\n",
            "Early stopping:  45.974640964801345\n",
            "Epoch: 018, Loss: 28.7828, Train: 0.1872, Test: 0.1765\n",
            "Early stopping:  39.65769683506529\n",
            "Epoch: 019, Loss: 12.4009, Train: 0.1860, Test: 0.1861\n",
            "Early stopping:  37.56474352037028\n",
            "Epoch: 020, Loss: 5.9068, Train: 0.1639, Test: 0.1656\n",
            "Early stopping:  28.499884862874918\n",
            "Epoch: 021, Loss: 6.3456, Train: 0.1656, Test: 0.1678\n",
            "Early stopping:  17.625901259429348\n",
            "Epoch: 022, Loss: 5.6445, Train: 0.1770, Test: 0.1788\n",
            "Early stopping:  9.888694892009946\n",
            "Epoch: 023, Loss: 4.4511, Train: 0.2025, Test: 0.2041\n",
            "Early stopping:  3.127080310116071\n",
            "Epoch: 024, Loss: 3.3584, Train: 0.2740, Test: 0.2764\n",
            "Early stopping:  1.2191008665228222\n",
            "Epoch: 025, Loss: 2.7148, Train: 0.3505, Test: 0.3455\n",
            "Early stopping:  1.5173088962009529\n",
            "Epoch: 026, Loss: 2.4558, Train: 0.3744, Test: 0.3711\n",
            "Early stopping:  1.3207488690711686\n",
            "Epoch: 027, Loss: 2.3659, Train: 0.3908, Test: 0.3785\n",
            "Early stopping:  0.8645442635720083\n",
            "Epoch: 028, Loss: 2.3210, Train: 0.3908, Test: 0.3750\n",
            "Early stopping:  0.42790807750548915\n",
            "Epoch: 029, Loss: 2.2733, Train: 0.3965, Test: 0.3716\n",
            "Early stopping:  0.17480917273982752\n",
            "Epoch: 030, Loss: 2.2298, Train: 0.3908, Test: 0.3639\n",
            "Early stopping:  0.08724687586806074\n",
            "Epoch: 031, Loss: 2.2106, Train: 0.3891, Test: 0.3631\n",
            "Early stopping:  0.06409988562027245\n",
            "Epoch: 032, Loss: 2.1975, Train: 0.3885, Test: 0.3608\n",
            "Early stopping:  0.050583568950925896\n",
            "Epoch: 033, Loss: 2.1850, Train: 0.3846, Test: 0.3608\n",
            "Early stopping:  0.03447963921245542\n",
            "Epoch: 034, Loss: 2.1755, Train: 0.3778, Test: 0.3570\n",
            "Early stopping:  0.02138241560711446\n",
            "Epoch: 035, Loss: 2.1685, Train: 0.3766, Test: 0.3554\n",
            "Early stopping:  0.01691830059276851\n",
            "Epoch: 036, Loss: 2.1559, Train: 0.3823, Test: 0.3531\n",
            "Early stopping:  0.01584155733804809\n",
            "Epoch: 037, Loss: 2.1377, Train: 0.3851, Test: 0.3560\n",
            "Early stopping:  0.018380695656970987\n",
            "Epoch: 038, Loss: 2.1147, Train: 0.3857, Test: 0.3594\n",
            "Early stopping:  0.024618629181889756\n",
            "Epoch: 039, Loss: 2.0905, Train: 0.3897, Test: 0.3619\n",
            "Early stopping:  0.031413123816080364\n",
            "Epoch: 040, Loss: 2.0683, Train: 0.3948, Test: 0.3655\n",
            "Early stopping:  0.035201833457740764\n",
            "Epoch: 041, Loss: 2.0495, Train: 0.3953, Test: 0.3693\n",
            "Early stopping:  0.035262884054658536\n",
            "Epoch: 042, Loss: 2.0298, Train: 0.4073, Test: 0.3726\n",
            "Early stopping:  0.03337369243913172\n",
            "Epoch: 043, Loss: 2.0040, Train: 0.4033, Test: 0.3748\n",
            "Early stopping:  0.033500821346243055\n",
            "Epoch: 044, Loss: 1.9796, Train: 0.4005, Test: 0.3704\n",
            "Early stopping:  0.03534706092372912\n",
            "Epoch: 045, Loss: 1.9700, Train: 0.4073, Test: 0.3757\n",
            "Early stopping:  0.03337766210759512\n",
            "Epoch: 046, Loss: 1.9490, Train: 0.4226, Test: 0.3894\n",
            "Early stopping:  0.03124425754773195\n",
            "Epoch: 047, Loss: 1.9367, Train: 0.4180, Test: 0.3896\n",
            "Early stopping:  0.026314895532359096\n",
            "Epoch: 048, Loss: 1.9083, Train: 0.4299, Test: 0.3982\n",
            "Early stopping:  0.028198609642560126\n",
            "Epoch: 049, Loss: 1.8904, Train: 0.4334, Test: 0.4029\n",
            "Early stopping:  0.031774103211806806\n",
            "Epoch: 050, Loss: 1.8746, Train: 0.4351, Test: 0.4093\n",
            "Early stopping:  0.031068739178054083\n",
            "Epoch: 051, Loss: 1.8609, Train: 0.4390, Test: 0.4137\n",
            "Early stopping:  0.029625722971625794\n",
            "Epoch: 052, Loss: 1.8462, Train: 0.4447, Test: 0.4155\n",
            "Early stopping:  0.024334447596981237\n",
            "Epoch: 053, Loss: 1.8254, Train: 0.4458, Test: 0.4175\n",
            "Early stopping:  0.025140704887382723\n",
            "Epoch: 054, Loss: 1.8017, Train: 0.4475, Test: 0.4219\n",
            "Early stopping:  0.028878043556398603\n",
            "Epoch: 055, Loss: 1.7847, Train: 0.4549, Test: 0.4291\n",
            "Early stopping:  0.031215871552544756\n",
            "Epoch: 056, Loss: 1.7677, Train: 0.4560, Test: 0.4288\n",
            "Early stopping:  0.031312420912539336\n",
            "Epoch: 057, Loss: 1.7518, Train: 0.4543, Test: 0.4284\n",
            "Early stopping:  0.028728250576240674\n",
            "Epoch: 058, Loss: 1.7334, Train: 0.4611, Test: 0.4334\n",
            "Early stopping:  0.026827056137438205\n",
            "Epoch: 059, Loss: 1.7174, Train: 0.4543, Test: 0.4271\n",
            "Early stopping:  0.026737617636710988\n",
            "Epoch: 060, Loss: 1.7036, Train: 0.4538, Test: 0.4277\n",
            "Early stopping:  0.025755020788547293\n",
            "Epoch: 061, Loss: 1.6914, Train: 0.4549, Test: 0.4266\n",
            "Early stopping:  0.023893795163494554\n",
            "Epoch: 062, Loss: 1.6783, Train: 0.4589, Test: 0.4257\n",
            "Early stopping:  0.02155793594368957\n",
            "Epoch: 063, Loss: 1.6669, Train: 0.4628, Test: 0.4300\n",
            "Early stopping:  0.01997491480535193\n",
            "Epoch: 064, Loss: 1.6545, Train: 0.4714, Test: 0.4407\n",
            "Early stopping:  0.019419704244803805\n",
            "Epoch: 065, Loss: 1.6428, Train: 0.4787, Test: 0.4487\n",
            "Early stopping:  0.01912972227886402\n",
            "Epoch: 066, Loss: 1.6309, Train: 0.4799, Test: 0.4512\n",
            "Early stopping:  0.01879559259987049\n",
            "Epoch: 067, Loss: 1.6168, Train: 0.4810, Test: 0.4539\n",
            "Early stopping:  0.019570301037719368\n",
            "Epoch: 068, Loss: 1.5998, Train: 0.4833, Test: 0.4529\n",
            "Early stopping:  0.021484129180630283\n",
            "Epoch: 069, Loss: 1.5900, Train: 0.4810, Test: 0.4531\n",
            "Early stopping:  0.02169612197552774\n",
            "Epoch: 070, Loss: 1.5753, Train: 0.4821, Test: 0.4562\n",
            "Early stopping:  0.021881673827179027\n",
            "Epoch: 071, Loss: 1.5585, Train: 0.4923, Test: 0.4634\n",
            "Early stopping:  0.022377185009630114\n",
            "Epoch: 072, Loss: 1.5456, Train: 0.4969, Test: 0.4690\n",
            "Early stopping:  0.0221869298416166\n",
            "Epoch: 073, Loss: 1.5332, Train: 0.5031, Test: 0.4716\n",
            "Early stopping:  0.02270638260188503\n",
            "Epoch: 074, Loss: 1.5201, Train: 0.5031, Test: 0.4746\n",
            "Early stopping:  0.021517090257301304\n",
            "Epoch: 075, Loss: 1.5073, Train: 0.5014, Test: 0.4773\n",
            "Early stopping:  0.020216519582306698\n",
            "Epoch: 076, Loss: 1.4973, Train: 0.5054, Test: 0.4793\n",
            "Early stopping:  0.019348916443816498\n",
            "Epoch: 077, Loss: 1.4856, Train: 0.5111, Test: 0.4814\n",
            "Early stopping:  0.018641191226578934\n",
            "Epoch: 078, Loss: 1.4743, Train: 0.5179, Test: 0.4817\n",
            "Early stopping:  0.017905962145964596\n",
            "Epoch: 079, Loss: 1.4632, Train: 0.5207, Test: 0.4868\n",
            "Early stopping:  0.017612634211549898\n",
            "Epoch: 080, Loss: 1.4496, Train: 0.5264, Test: 0.4884\n",
            "Early stopping:  0.018656537677544684\n",
            "Epoch: 081, Loss: 1.4390, Train: 0.5309, Test: 0.4888\n",
            "Early stopping:  0.01865310893170475\n",
            "Epoch: 082, Loss: 1.4254, Train: 0.5394, Test: 0.4915\n",
            "Early stopping:  0.0192954822996539\n",
            "Epoch: 083, Loss: 1.4152, Train: 0.5428, Test: 0.4926\n",
            "Early stopping:  0.0190218546183585\n",
            "Epoch: 084, Loss: 1.4030, Train: 0.5468, Test: 0.4918\n",
            "Early stopping:  0.018532833749503465\n",
            "Epoch: 085, Loss: 1.3937, Train: 0.5513, Test: 0.4939\n",
            "Early stopping:  0.017926009879412272\n",
            "Epoch: 086, Loss: 1.3827, Train: 0.5559, Test: 0.4967\n",
            "Early stopping:  0.01690337422136757\n",
            "Epoch: 087, Loss: 1.3741, Train: 0.5576, Test: 0.4972\n",
            "Early stopping:  0.01622472800286512\n",
            "Epoch: 088, Loss: 1.3639, Train: 0.5604, Test: 0.4980\n",
            "Early stopping:  0.015469119839973922\n",
            "Epoch: 089, Loss: 1.3533, Train: 0.5598, Test: 0.4993\n",
            "Early stopping:  0.01574425417146119\n",
            "Epoch: 090, Loss: 1.3428, Train: 0.5632, Test: 0.5024\n",
            "Early stopping:  0.01593326124352126\n",
            "Epoch: 091, Loss: 1.3331, Train: 0.5644, Test: 0.5035\n",
            "Early stopping:  0.01628515274684887\n",
            "Epoch: 092, Loss: 1.3238, Train: 0.5661, Test: 0.5033\n",
            "Early stopping:  0.01588293911423998\n",
            "Epoch: 093, Loss: 1.3146, Train: 0.5683, Test: 0.5050\n",
            "Early stopping:  0.01525377756100084\n",
            "Epoch: 094, Loss: 1.3054, Train: 0.5837, Test: 0.5105\n",
            "Early stopping:  0.014758500193976051\n",
            "Epoch: 095, Loss: 1.2974, Train: 0.5740, Test: 0.5077\n",
            "Early stopping:  0.014205203547713662\n",
            "Epoch: 096, Loss: 1.2885, Train: 0.5837, Test: 0.5123\n",
            "Early stopping:  0.013880230423768397\n",
            "Epoch: 097, Loss: 1.2787, Train: 0.5814, Test: 0.5112\n",
            "Early stopping:  0.014020125787110531\n",
            "Epoch: 098, Loss: 1.2667, Train: 0.5865, Test: 0.5152\n",
            "Early stopping:  0.015246432699925046\n",
            "Epoch: 099, Loss: 1.2567, Train: 0.5922, Test: 0.5193\n",
            "Early stopping:  0.016365883795291587\n",
            "Epoch: 100, Loss: 1.2492, Train: 0.5882, Test: 0.5167\n",
            "Early stopping:  0.015951525051006363\n",
            "Epoch: 101, Loss: 1.2414, Train: 0.6018, Test: 0.5223\n",
            "Early stopping:  0.01464206896415124\n",
            "Epoch: 102, Loss: 1.2318, Train: 0.5967, Test: 0.5197\n",
            "Early stopping:  0.013459872817867225\n",
            "Epoch: 103, Loss: 1.2199, Train: 0.6029, Test: 0.5211\n",
            "Early stopping:  0.01445827506799189\n",
            "Epoch: 104, Loss: 1.2150, Train: 0.6109, Test: 0.5207\n",
            "Early stopping:  0.014302342269024037\n",
            "Epoch: 105, Loss: 1.2054, Train: 0.6007, Test: 0.5159\n",
            "Early stopping:  0.014126091006286277\n",
            "Epoch: 106, Loss: 1.2179, Train: 0.6160, Test: 0.5259\n",
            "Early stopping:  0.00949537759371795\n",
            "PREDICTIONS -> tensor([ 9,  0,  8,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.56      0.49      0.52       758\n",
            "         capital_goods       0.37      0.13      0.19       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.50      0.44      0.47       793\n",
            " consumer_non-cyclical       0.68      0.28      0.40       446\n",
            "                energy       0.59      0.43      0.50       283\n",
            "             financial       0.59      0.59      0.59       767\n",
            "            healthcare       0.66      0.56      0.61       318\n",
            "              services       0.49      0.79      0.60      2076\n",
            "            technology       0.15      0.03      0.05       396\n",
            "        transportation       0.58      0.69      0.63       404\n",
            "             utilities       0.61      0.56      0.58       225\n",
            "\n",
            "              accuracy                           0.53      7054\n",
            "             macro avg       0.48      0.41      0.43      7054\n",
            "          weighted avg       0.51      0.53      0.49      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 49.6500, Train: 0.1129, Test: 0.1111\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 261.4294, Train: 0.0391, Test: 0.0424\n",
            "Early stopping:  149.75063897539252\n",
            "Epoch: 003, Loss: 390.1833, Train: 0.2796, Test: 0.2841\n",
            "Early stopping:  171.94528404737233\n",
            "Epoch: 004, Loss: 250.7992, Train: 0.1299, Test: 0.1297\n",
            "Early stopping:  140.65117480397635\n",
            "Epoch: 005, Loss: 320.3696, Train: 0.0868, Test: 0.0815\n",
            "Early stopping:  127.25370570089842\n",
            "Epoch: 006, Loss: 273.7035, Train: 0.1202, Test: 0.1072\n",
            "Early stopping:  57.32671614156111\n",
            "Epoch: 007, Loss: 218.7594, Train: 0.3125, Test: 0.3116\n",
            "Early stopping:  66.77127025228512\n",
            "Epoch: 008, Loss: 166.3525, Train: 0.3137, Test: 0.3098\n",
            "Early stopping:  57.89456645007269\n",
            "Epoch: 009, Loss: 150.3812, Train: 0.1134, Test: 0.1120\n",
            "Early stopping:  71.60596275153706\n",
            "Epoch: 010, Loss: 143.1105, Train: 0.2666, Test: 0.2580\n",
            "Early stopping:  55.135717491988125\n",
            "Epoch: 011, Loss: 102.0418, Train: 0.0766, Test: 0.0763\n",
            "Early stopping:  42.29371226115113\n",
            "Epoch: 012, Loss: 122.9455, Train: 0.1044, Test: 0.1002\n",
            "Early stopping:  24.98781871258987\n",
            "Epoch: 013, Loss: 95.2470, Train: 0.3256, Test: 0.3185\n",
            "Early stopping:  24.306966892070992\n",
            "Epoch: 014, Loss: 60.5309, Train: 0.3193, Test: 0.3156\n",
            "Early stopping:  31.04662159150713\n",
            "Epoch: 015, Loss: 47.2493, Train: 0.2462, Test: 0.2338\n",
            "Early stopping:  31.053679034102814\n",
            "Epoch: 016, Loss: 34.3887, Train: 0.1872, Test: 0.1717\n",
            "Early stopping:  36.381986224792804\n",
            "Epoch: 017, Loss: 24.3007, Train: 0.1747, Test: 0.1697\n",
            "Early stopping:  27.57930298575503\n",
            "Epoch: 018, Loss: 10.8227, Train: 0.1384, Test: 0.1239\n",
            "Early stopping:  19.36627469738863\n",
            "Epoch: 019, Loss: 7.7744, Train: 0.1254, Test: 0.1162\n",
            "Early stopping:  16.44187969062346\n",
            "Epoch: 020, Loss: 6.0517, Train: 0.1566, Test: 0.1470\n",
            "Early stopping:  12.229589733753762\n",
            "Epoch: 021, Loss: 4.9242, Train: 0.1843, Test: 0.1778\n",
            "Early stopping:  7.881827432138194\n",
            "Epoch: 022, Loss: 4.2844, Train: 0.2104, Test: 0.2047\n",
            "Early stopping:  2.6236623849208613\n",
            "Epoch: 023, Loss: 3.8236, Train: 0.2036, Test: 0.2012\n",
            "Early stopping:  1.582266816017388\n",
            "Epoch: 024, Loss: 3.3759, Train: 0.2093, Test: 0.1936\n",
            "Early stopping:  1.0434312548673588\n",
            "Epoch: 025, Loss: 2.9645, Train: 0.2082, Test: 0.1922\n",
            "Early stopping:  0.7663664550195569\n",
            "Epoch: 026, Loss: 2.6425, Train: 0.2138, Test: 0.1972\n",
            "Early stopping:  0.6565086113704723\n",
            "Epoch: 027, Loss: 2.4610, Train: 0.2252, Test: 0.2152\n",
            "Early stopping:  0.5533597959661382\n",
            "Epoch: 028, Loss: 2.3687, Train: 0.2354, Test: 0.2226\n",
            "Early stopping:  0.4115223703679233\n",
            "Epoch: 029, Loss: 2.2975, Train: 0.2484, Test: 0.2393\n",
            "Early stopping:  0.2668496129902221\n",
            "Epoch: 030, Loss: 2.2384, Train: 0.2689, Test: 0.2499\n",
            "Early stopping:  0.15822013481739014\n",
            "Epoch: 031, Loss: 2.2001, Train: 0.2757, Test: 0.2627\n",
            "Early stopping:  0.10435824602290322\n",
            "Epoch: 032, Loss: 2.1681, Train: 0.2802, Test: 0.2711\n",
            "Early stopping:  0.07997073707793097\n",
            "Epoch: 033, Loss: 2.1398, Train: 0.2927, Test: 0.2820\n",
            "Early stopping:  0.061721543181279495\n",
            "Epoch: 034, Loss: 2.1122, Train: 0.3006, Test: 0.2929\n",
            "Early stopping:  0.049545194768474944\n",
            "Epoch: 035, Loss: 2.0853, Train: 0.3159, Test: 0.3044\n",
            "Early stopping:  0.045155246938169066\n",
            "Epoch: 036, Loss: 2.0652, Train: 0.3545, Test: 0.3502\n",
            "Early stopping:  0.0412334153763584\n",
            "Epoch: 037, Loss: 2.0406, Train: 0.3687, Test: 0.3687\n",
            "Early stopping:  0.038875113520834954\n",
            "Epoch: 038, Loss: 2.0146, Train: 0.3715, Test: 0.3772\n",
            "Early stopping:  0.03798236572462653\n",
            "Epoch: 039, Loss: 1.9980, Train: 0.3772, Test: 0.3860\n",
            "Early stopping:  0.03568440666407765\n",
            "Epoch: 040, Loss: 1.9836, Train: 0.3953, Test: 0.3862\n",
            "Early stopping:  0.03279362163110978\n",
            "Epoch: 041, Loss: 1.9672, Train: 0.3942, Test: 0.3917\n",
            "Early stopping:  0.02831180254605707\n",
            "Epoch: 042, Loss: 1.9528, Train: 0.4022, Test: 0.3950\n",
            "Early stopping:  0.024418004787892206\n",
            "Epoch: 043, Loss: 1.9421, Train: 0.4039, Test: 0.3971\n",
            "Early stopping:  0.02261289301526303\n",
            "Epoch: 044, Loss: 1.9233, Train: 0.4061, Test: 0.4025\n",
            "Early stopping:  0.023105421491592636\n",
            "Epoch: 045, Loss: 1.9046, Train: 0.4044, Test: 0.4047\n",
            "Early stopping:  0.02457386424546088\n",
            "Epoch: 046, Loss: 1.8841, Train: 0.4146, Test: 0.4105\n",
            "Early stopping:  0.027795099034218628\n",
            "Epoch: 047, Loss: 1.8595, Train: 0.4260, Test: 0.4145\n",
            "Early stopping:  0.03237084716045027\n",
            "Epoch: 048, Loss: 1.8347, Train: 0.4288, Test: 0.4181\n",
            "Early stopping:  0.03522542736706555\n",
            "Epoch: 049, Loss: 1.8146, Train: 0.4328, Test: 0.4226\n",
            "Early stopping:  0.036289984083101565\n",
            "Epoch: 050, Loss: 1.7980, Train: 0.4373, Test: 0.4267\n",
            "Early stopping:  0.034437088801412885\n",
            "Epoch: 051, Loss: 1.7825, Train: 0.4481, Test: 0.4317\n",
            "Early stopping:  0.03031573043686804\n",
            "Epoch: 052, Loss: 1.7667, Train: 0.4583, Test: 0.4342\n",
            "Early stopping:  0.02664351249111087\n",
            "Epoch: 053, Loss: 1.7495, Train: 0.4623, Test: 0.4378\n",
            "Early stopping:  0.025569379496622698\n",
            "Epoch: 054, Loss: 1.7345, Train: 0.4674, Test: 0.4420\n",
            "Early stopping:  0.025310420646560406\n",
            "Epoch: 055, Loss: 1.7188, Train: 0.4719, Test: 0.4453\n",
            "Early stopping:  0.02524462501658628\n",
            "Epoch: 056, Loss: 1.7008, Train: 0.4770, Test: 0.4467\n",
            "Early stopping:  0.025693561117013156\n",
            "Epoch: 057, Loss: 1.6856, Train: 0.4833, Test: 0.4495\n",
            "Early stopping:  0.025533600655664648\n",
            "Epoch: 058, Loss: 1.6691, Train: 0.4872, Test: 0.4525\n",
            "Early stopping:  0.025931228546872978\n",
            "Epoch: 059, Loss: 1.6521, Train: 0.4889, Test: 0.4562\n",
            "Early stopping:  0.026103478818573257\n",
            "Epoch: 060, Loss: 1.6341, Train: 0.4923, Test: 0.4603\n",
            "Early stopping:  0.0264108944825381\n",
            "Epoch: 061, Loss: 1.6161, Train: 0.4969, Test: 0.4633\n",
            "Early stopping:  0.0275250497121921\n",
            "Epoch: 062, Loss: 1.5982, Train: 0.4997, Test: 0.4663\n",
            "Early stopping:  0.028103851869142916\n",
            "Epoch: 063, Loss: 1.5818, Train: 0.5048, Test: 0.4698\n",
            "Early stopping:  0.02791000192267588\n",
            "Epoch: 064, Loss: 1.5660, Train: 0.5133, Test: 0.4739\n",
            "Early stopping:  0.026961704973823476\n",
            "Epoch: 065, Loss: 1.5511, Train: 0.5224, Test: 0.4787\n",
            "Early stopping:  0.025657809413403513\n",
            "Epoch: 066, Loss: 1.5360, Train: 0.5303, Test: 0.4821\n",
            "Early stopping:  0.02453725681239352\n",
            "Epoch: 067, Loss: 1.5199, Train: 0.5372, Test: 0.4862\n",
            "Early stopping:  0.024324792159706794\n",
            "Epoch: 068, Loss: 1.5031, Train: 0.5309, Test: 0.4826\n",
            "Early stopping:  0.02482915305823765\n",
            "Epoch: 069, Loss: 1.4889, Train: 0.5286, Test: 0.4820\n",
            "Early stopping:  0.024874472061328496\n",
            "Epoch: 070, Loss: 1.4744, Train: 0.5292, Test: 0.4833\n",
            "Early stopping:  0.02438777512575763\n",
            "Epoch: 071, Loss: 1.4606, Train: 0.5343, Test: 0.4860\n",
            "Early stopping:  0.02329323985800918\n",
            "Epoch: 072, Loss: 1.4458, Train: 0.5372, Test: 0.4898\n",
            "Early stopping:  0.022589462715450256\n",
            "Epoch: 073, Loss: 1.4329, Train: 0.5491, Test: 0.4963\n",
            "Early stopping:  0.022218470996242797\n",
            "Epoch: 074, Loss: 1.4191, Train: 0.5496, Test: 0.4973\n",
            "Early stopping:  0.021863177719673634\n",
            "Epoch: 075, Loss: 1.4074, Train: 0.5570, Test: 0.4987\n",
            "Early stopping:  0.02105690541001801\n",
            "Epoch: 076, Loss: 1.3941, Train: 0.5593, Test: 0.5006\n",
            "Early stopping:  0.020398906229640787\n",
            "Epoch: 077, Loss: 1.3817, Train: 0.5598, Test: 0.4999\n",
            "Early stopping:  0.020172089790627876\n",
            "Epoch: 078, Loss: 1.3692, Train: 0.5604, Test: 0.5013\n",
            "Early stopping:  0.019851628620976298\n",
            "Epoch: 079, Loss: 1.3588, Train: 0.5632, Test: 0.5020\n",
            "Early stopping:  0.019318691378972828\n",
            "Epoch: 080, Loss: 1.3463, Train: 0.5683, Test: 0.5043\n",
            "Early stopping:  0.01872887860782985\n",
            "Epoch: 081, Loss: 1.3358, Train: 0.5706, Test: 0.5084\n",
            "Early stopping:  0.018130099120718467\n",
            "Epoch: 082, Loss: 1.3242, Train: 0.5752, Test: 0.5084\n",
            "Early stopping:  0.017873332206210207\n",
            "Epoch: 083, Loss: 1.3134, Train: 0.5769, Test: 0.5078\n",
            "Early stopping:  0.017870499605041966\n",
            "Epoch: 084, Loss: 1.3027, Train: 0.5865, Test: 0.5098\n",
            "Early stopping:  0.0173573025552163\n",
            "Epoch: 085, Loss: 1.2907, Train: 0.5905, Test: 0.5118\n",
            "Early stopping:  0.017678666563889545\n",
            "Epoch: 086, Loss: 1.2803, Train: 0.5950, Test: 0.5135\n",
            "Early stopping:  0.017461022563199806\n",
            "Epoch: 087, Loss: 1.2693, Train: 0.6001, Test: 0.5147\n",
            "Early stopping:  0.017461022563199806\n",
            "Epoch: 088, Loss: 1.2588, Train: 0.6052, Test: 0.5149\n",
            "Early stopping:  0.017253595641401123\n",
            "Epoch: 089, Loss: 1.2500, Train: 0.6109, Test: 0.5157\n",
            "Early stopping:  0.01629182668754237\n",
            "Epoch: 090, Loss: 1.2395, Train: 0.6154, Test: 0.5156\n",
            "Early stopping:  0.015971360608402204\n",
            "Epoch: 091, Loss: 1.2291, Train: 0.6211, Test: 0.5160\n",
            "Early stopping:  0.015776764158754137\n",
            "Epoch: 092, Loss: 1.2201, Train: 0.6222, Test: 0.5172\n",
            "Early stopping:  0.01556114970305866\n",
            "Epoch: 093, Loss: 1.2106, Train: 0.6256, Test: 0.5196\n",
            "Early stopping:  0.015536612618920906\n",
            "Epoch: 094, Loss: 1.2004, Train: 0.6262, Test: 0.5200\n",
            "Early stopping:  0.015296423083670414\n",
            "Epoch: 095, Loss: 1.1912, Train: 0.6273, Test: 0.5208\n",
            "Early stopping:  0.015107597696877629\n",
            "Epoch: 096, Loss: 1.1812, Train: 0.6307, Test: 0.5230\n",
            "Early stopping:  0.015376226162400343\n",
            "Epoch: 097, Loss: 1.1691, Train: 0.6330, Test: 0.5234\n",
            "Early stopping:  0.016176644814076397\n",
            "Epoch: 098, Loss: 1.1592, Train: 0.6347, Test: 0.5261\n",
            "Early stopping:  0.016547371190393595\n",
            "Epoch: 099, Loss: 1.1488, Train: 0.6410, Test: 0.5261\n",
            "Early stopping:  0.01688598409529261\n",
            "Epoch: 100, Loss: 1.1379, Train: 0.6449, Test: 0.5261\n",
            "Early stopping:  0.016919423112006112\n",
            "Epoch: 101, Loss: 1.1275, Train: 0.6478, Test: 0.5264\n",
            "Early stopping:  0.01651841814829706\n",
            "Epoch: 102, Loss: 1.1156, Train: 0.6489, Test: 0.5279\n",
            "Early stopping:  0.017137395642775416\n",
            "Epoch: 103, Loss: 1.1098, Train: 0.6597, Test: 0.5296\n",
            "Early stopping:  0.015939374932865808\n",
            "Epoch: 104, Loss: 1.0998, Train: 0.6608, Test: 0.5308\n",
            "Early stopping:  0.014917545111905581\n",
            "Epoch: 105, Loss: 1.0903, Train: 0.6670, Test: 0.5327\n",
            "Early stopping:  0.014339072620065531\n",
            "Epoch: 106, Loss: 1.0782, Train: 0.6676, Test: 0.5319\n",
            "Early stopping:  0.015022490824940093\n",
            "Epoch: 107, Loss: 1.0682, Train: 0.6659, Test: 0.5299\n",
            "Early stopping:  0.016573453990575814\n",
            "Epoch: 108, Loss: 1.0603, Train: 0.6733, Test: 0.5301\n",
            "Early stopping:  0.016007353426213525\n",
            "Epoch: 109, Loss: 1.0510, Train: 0.6807, Test: 0.5326\n",
            "Early stopping:  0.01527606183982782\n",
            "Epoch: 110, Loss: 1.0435, Train: 0.6784, Test: 0.5337\n",
            "Early stopping:  0.013700877246870703\n",
            "Epoch: 111, Loss: 1.0324, Train: 0.6790, Test: 0.5342\n",
            "Early stopping:  0.014009668923358039\n",
            "Epoch: 112, Loss: 1.0244, Train: 0.6829, Test: 0.5359\n",
            "Early stopping:  0.014314413571677785\n",
            "Epoch: 113, Loss: 1.0182, Train: 0.6852, Test: 0.5364\n",
            "Early stopping:  0.013466276849323397\n",
            "Epoch: 114, Loss: 1.0053, Train: 0.6841, Test: 0.5364\n",
            "Early stopping:  0.014409516567223458\n",
            "Epoch: 115, Loss: 1.0024, Train: 0.6909, Test: 0.5360\n",
            "Early stopping:  0.012687955424907298\n",
            "Epoch: 116, Loss: 0.9932, Train: 0.6965, Test: 0.5388\n",
            "Early stopping:  0.01254446742093275\n",
            "Epoch: 117, Loss: 0.9839, Train: 0.6920, Test: 0.5403\n",
            "Early stopping:  0.01293280253144686\n",
            "Epoch: 118, Loss: 0.9728, Train: 0.6931, Test: 0.5384\n",
            "Early stopping:  0.013420090666101973\n",
            "Epoch: 119, Loss: 0.9628, Train: 0.6971, Test: 0.5413\n",
            "Early stopping:  0.01574129509437986\n",
            "Epoch: 120, Loss: 0.9572, Train: 0.7011, Test: 0.5411\n",
            "Early stopping:  0.01477096400505111\n",
            "Epoch: 121, Loss: 0.9471, Train: 0.6977, Test: 0.5415\n",
            "Early stopping:  0.014156449409629077\n",
            "Epoch: 122, Loss: 0.9415, Train: 0.7067, Test: 0.5431\n",
            "Early stopping:  0.0124235701813444\n",
            "Epoch: 123, Loss: 0.9337, Train: 0.7113, Test: 0.5430\n",
            "Early stopping:  0.01173926036487126\n",
            "Epoch: 124, Loss: 0.9245, Train: 0.7039, Test: 0.5415\n",
            "Early stopping:  0.012505270711039032\n",
            "Epoch: 125, Loss: 0.9217, Train: 0.7073, Test: 0.5417\n",
            "Early stopping:  0.010820259200576963\n",
            "Epoch: 126, Loss: 0.9231, Train: 0.7164, Test: 0.5437\n",
            "Early stopping:  0.008468460701097332\n",
            "PREDICTIONS -> tensor([ 8, 10,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.53      0.50      0.51       758\n",
            "         capital_goods       0.44      0.37      0.40       508\n",
            "conglomerates_industry       0.33      0.03      0.05        80\n",
            "     consumer_cyclical       0.46      0.47      0.46       793\n",
            " consumer_non-cyclical       0.77      0.32      0.45       446\n",
            "                energy       0.57      0.41      0.48       283\n",
            "             financial       0.59      0.57      0.58       767\n",
            "            healthcare       0.58      0.57      0.58       318\n",
            "              services       0.53      0.75      0.63      2076\n",
            "            technology       0.52      0.17      0.26       396\n",
            "        transportation       0.68      0.62      0.65       404\n",
            "             utilities       0.58      0.58      0.58       225\n",
            "\n",
            "              accuracy                           0.54      7054\n",
            "             macro avg       0.55      0.45      0.47      7054\n",
            "          weighted avg       0.55      0.54      0.53      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 37.4441, Train: 0.3159, Test: 0.3107\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 336.7445, Train: 0.1078, Test: 0.1072\n",
            "Early stopping:  211.63734769127262\n",
            "Epoch: 003, Loss: 568.6292, Train: 0.0596, Test: 0.0611\n",
            "Early stopping:  266.3045958841625\n",
            "Epoch: 004, Loss: 450.0452, Train: 0.0839, Test: 0.0807\n",
            "Early stopping:  227.78786721888792\n",
            "Epoch: 005, Loss: 312.7730, Train: 0.1231, Test: 0.1196\n",
            "Early stopping:  197.90584030077298\n",
            "Epoch: 006, Loss: 352.4736, Train: 0.0743, Test: 0.0760\n",
            "Early stopping:  105.73962160385167\n",
            "Epoch: 007, Loss: 248.6864, Train: 0.0993, Test: 0.1050\n",
            "Early stopping:  125.2947136785105\n",
            "Epoch: 008, Loss: 157.5781, Train: 0.2898, Test: 0.2900\n",
            "Early stopping:  109.83435115576543\n",
            "Epoch: 009, Loss: 156.3371, Train: 0.2859, Test: 0.2861\n",
            "Early stopping:  88.965460352147\n",
            "Epoch: 010, Loss: 148.8197, Train: 0.1583, Test: 0.1649\n",
            "Early stopping:  88.21500130543798\n",
            "Epoch: 011, Loss: 115.5835, Train: 0.0584, Test: 0.0590\n",
            "Early stopping:  49.589658482531526\n",
            "Epoch: 012, Loss: 85.6837, Train: 0.0669, Test: 0.0617\n",
            "Early stopping:  31.388426660941537\n",
            "Epoch: 013, Loss: 57.7558, Train: 0.1061, Test: 0.0998\n",
            "Early stopping:  41.73067039971991\n",
            "Epoch: 014, Loss: 36.1976, Train: 0.1265, Test: 0.1216\n",
            "Early stopping:  44.889256076649936\n",
            "Epoch: 015, Loss: 17.2190, Train: 0.1997, Test: 0.1959\n",
            "Early stopping:  39.11532156790464\n",
            "Epoch: 016, Loss: 4.5267, Train: 0.2167, Test: 0.2092\n",
            "Early stopping:  32.37928398963464\n",
            "Epoch: 017, Loss: 2.8762, Train: 0.2002, Test: 0.2019\n",
            "Early stopping:  23.233785140304857\n",
            "Epoch: 018, Loss: 2.7297, Train: 0.2138, Test: 0.2019\n",
            "Early stopping:  14.450560512045408\n",
            "Epoch: 019, Loss: 2.6116, Train: 0.2212, Test: 0.2111\n",
            "Early stopping:  6.323996135430372\n",
            "Epoch: 020, Loss: 2.4695, Train: 0.2343, Test: 0.2257\n",
            "Early stopping:  0.8429689108300196\n",
            "Epoch: 021, Loss: 2.3690, Train: 0.2462, Test: 0.2328\n",
            "Early stopping:  0.2018952250725174\n",
            "Epoch: 022, Loss: 2.3271, Train: 0.2433, Test: 0.2321\n",
            "Early stopping:  0.16820769213010967\n",
            "Epoch: 023, Loss: 2.3054, Train: 0.2348, Test: 0.2258\n",
            "Early stopping:  0.12593939341360463\n",
            "Epoch: 024, Loss: 2.3013, Train: 0.2292, Test: 0.2203\n",
            "Early stopping:  0.06967571254194338\n",
            "Epoch: 025, Loss: 2.3030, Train: 0.2246, Test: 0.2183\n",
            "Early stopping:  0.02867752819226755\n",
            "Epoch: 026, Loss: 2.3020, Train: 0.2229, Test: 0.2118\n",
            "Early stopping:  0.010935142866521965\n",
            "Epoch: 027, Loss: 2.2947, Train: 0.2144, Test: 0.2047\n",
            "Early stopping:  0.00399832315554831\n",
            "PREDICTIONS -> tensor([ 3,  3,  3,  ...,  4, 11,  3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       758\n",
            "         capital_goods       0.00      0.00      0.00       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.16      0.65      0.26       793\n",
            " consumer_non-cyclical       0.21      0.44      0.28       446\n",
            "                energy       0.03      0.01      0.02       283\n",
            "             financial       0.43      0.17      0.24       767\n",
            "            healthcare       0.69      0.10      0.17       318\n",
            "              services       0.50      0.23      0.31      2076\n",
            "            technology       0.08      0.14      0.10       396\n",
            "        transportation       0.15      0.02      0.04       404\n",
            "             utilities       0.05      0.16      0.08       225\n",
            "\n",
            "              accuracy                           0.20      7054\n",
            "             macro avg       0.19      0.16      0.12      7054\n",
            "          weighted avg       0.27      0.20      0.18      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 58.8070, Train: 0.2082, Test: 0.2053\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 154.9793, Train: 0.0743, Test: 0.0773\n",
            "Early stopping:  68.00411135711987\n",
            "Epoch: 003, Loss: 508.1554, Train: 0.1061, Test: 0.1072\n",
            "Early stopping:  236.60674848945087\n",
            "Epoch: 004, Loss: 617.7375, Train: 0.0136, Test: 0.0159\n",
            "Early stopping:  269.94649775862297\n",
            "Epoch: 005, Loss: 643.9929, Train: 0.0635, Test: 0.0652\n",
            "Early stopping:  271.58529173424284\n",
            "Epoch: 006, Loss: 519.5800, Train: 0.2995, Test: 0.2976\n",
            "Early stopping:  195.8749536700454\n",
            "Epoch: 007, Loss: 420.2508, Train: 0.1367, Test: 0.1337\n",
            "Early stopping:  90.29077483409463\n",
            "Epoch: 008, Loss: 443.0203, Train: 0.2978, Test: 0.2991\n",
            "Early stopping:  100.50538573097965\n",
            "Epoch: 009, Loss: 408.0964, Train: 0.1622, Test: 0.1622\n",
            "Early stopping:  97.88217403396555\n",
            "Epoch: 010, Loss: 413.5359, Train: 0.2575, Test: 0.2461\n",
            "Early stopping:  45.95186441456756\n",
            "Epoch: 011, Loss: 352.1096, Train: 0.0868, Test: 0.0795\n",
            "Early stopping:  33.649345866217594\n",
            "Epoch: 012, Loss: 360.1830, Train: 0.2950, Test: 0.2852\n",
            "Early stopping:  38.314805815964974\n",
            "Epoch: 013, Loss: 311.7503, Train: 0.2870, Test: 0.2791\n",
            "Early stopping:  42.28607240412106\n",
            "Epoch: 014, Loss: 273.5200, Train: 0.2609, Test: 0.2607\n",
            "Early stopping:  52.8075152774353\n",
            "Epoch: 015, Loss: 222.9846, Train: 0.2445, Test: 0.2440\n",
            "Early stopping:  57.060486226059815\n",
            "Epoch: 016, Loss: 169.3083, Train: 0.1225, Test: 0.1151\n",
            "Early stopping:  74.51122627462351\n",
            "Epoch: 017, Loss: 133.5175, Train: 0.2025, Test: 0.1928\n",
            "Early stopping:  72.99954176371803\n",
            "Epoch: 018, Loss: 96.8742, Train: 0.2229, Test: 0.2056\n",
            "Early stopping:  70.31291043479564\n",
            "Epoch: 019, Loss: 71.3772, Train: 0.1991, Test: 0.1807\n",
            "Early stopping:  59.894314768769235\n",
            "Epoch: 020, Loss: 47.5222, Train: 0.2275, Test: 0.2068\n",
            "Early stopping:  48.581969373062044\n",
            "Epoch: 021, Loss: 25.1289, Train: 0.2955, Test: 0.2684\n",
            "Early stopping:  42.30243739092218\n",
            "Epoch: 022, Loss: 9.6901, Train: 0.2740, Test: 0.2597\n",
            "Early stopping:  35.01320806744779\n",
            "Epoch: 023, Loss: 5.3914, Train: 0.2314, Test: 0.2173\n",
            "Early stopping:  27.587592336140286\n",
            "Epoch: 024, Loss: 4.3960, Train: 0.2201, Test: 0.2044\n",
            "Early stopping:  18.26591122131405\n",
            "Epoch: 025, Loss: 3.6944, Train: 0.2042, Test: 0.1985\n",
            "Early stopping:  8.955555799141347\n",
            "Epoch: 026, Loss: 3.2294, Train: 0.2070, Test: 0.1971\n",
            "Early stopping:  2.596259348475399\n",
            "Epoch: 027, Loss: 2.9533, Train: 0.2082, Test: 0.1969\n",
            "Early stopping:  0.981453855668128\n",
            "Epoch: 028, Loss: 2.7551, Train: 0.2138, Test: 0.2097\n",
            "Early stopping:  0.6563067696278643\n",
            "Epoch: 029, Loss: 2.5834, Train: 0.2201, Test: 0.2221\n",
            "Early stopping:  0.4362385339050465\n",
            "Epoch: 030, Loss: 2.4506, Train: 0.2206, Test: 0.2223\n",
            "Early stopping:  0.307725951223078\n",
            "Epoch: 031, Loss: 2.3495, Train: 0.2382, Test: 0.2295\n",
            "Early stopping:  0.24111404060837777\n",
            "Epoch: 032, Loss: 2.2753, Train: 0.2501, Test: 0.2325\n",
            "Early stopping:  0.19114587868130806\n",
            "Epoch: 033, Loss: 2.2279, Train: 0.2581, Test: 0.2468\n",
            "Early stopping:  0.14261641524055707\n",
            "Epoch: 034, Loss: 2.1957, Train: 0.2666, Test: 0.2549\n",
            "Early stopping:  0.10226059828860856\n",
            "Epoch: 035, Loss: 2.1760, Train: 0.2757, Test: 0.2671\n",
            "Early stopping:  0.0695287171096166\n",
            "Epoch: 036, Loss: 2.1616, Train: 0.2791, Test: 0.2797\n",
            "Early stopping:  0.045419684325797356\n",
            "Epoch: 037, Loss: 2.1452, Train: 0.2967, Test: 0.2920\n",
            "Early stopping:  0.0320069204013132\n",
            "Epoch: 038, Loss: 2.1263, Train: 0.3125, Test: 0.3048\n",
            "Early stopping:  0.026834791624913472\n",
            "Epoch: 039, Loss: 2.0964, Train: 0.3171, Test: 0.3193\n",
            "Early stopping:  0.031097715557100558\n",
            "Epoch: 040, Loss: 2.1003, Train: 0.3341, Test: 0.3191\n",
            "Early stopping:  0.028143916890357383\n",
            "Epoch: 041, Loss: 2.0553, Train: 0.3392, Test: 0.3188\n",
            "Early stopping:  0.03402337669722417\n",
            "Epoch: 042, Loss: 2.0303, Train: 0.3386, Test: 0.3253\n",
            "Early stopping:  0.03836503512896892\n",
            "Epoch: 043, Loss: 2.0129, Train: 0.3443, Test: 0.3371\n",
            "Early stopping:  0.038953924469204745\n",
            "Epoch: 044, Loss: 1.9899, Train: 0.3573, Test: 0.3436\n",
            "Early stopping:  0.042407218951611404\n",
            "Epoch: 045, Loss: 1.9612, Train: 0.3630, Test: 0.3521\n",
            "Early stopping:  0.03627733141289666\n",
            "Epoch: 046, Loss: 1.9494, Train: 0.3659, Test: 0.3540\n",
            "Early stopping:  0.03398823617849429\n",
            "Epoch: 047, Loss: 1.9268, Train: 0.3715, Test: 0.3510\n",
            "Early stopping:  0.0338488474471449\n",
            "Epoch: 048, Loss: 1.8976, Train: 0.3766, Test: 0.3509\n",
            "Early stopping:  0.03489217327115706\n",
            "Epoch: 049, Loss: 1.8854, Train: 0.3772, Test: 0.3541\n",
            "Early stopping:  0.03249446393359898\n",
            "Epoch: 050, Loss: 1.8670, Train: 0.3783, Test: 0.3614\n",
            "Early stopping:  0.03288691526850823\n",
            "Epoch: 051, Loss: 1.8420, Train: 0.3902, Test: 0.3682\n",
            "Early stopping:  0.03191356310542213\n",
            "Epoch: 052, Loss: 1.8246, Train: 0.4033, Test: 0.3711\n",
            "Early stopping:  0.030134442685585735\n",
            "Epoch: 053, Loss: 1.8116, Train: 0.4152, Test: 0.3808\n",
            "Early stopping:  0.03021188093176921\n",
            "Epoch: 054, Loss: 1.7970, Train: 0.4197, Test: 0.3857\n",
            "Early stopping:  0.02719802967928664\n",
            "Epoch: 055, Loss: 1.7835, Train: 0.4192, Test: 0.3941\n",
            "Early stopping:  0.022868413621233385\n",
            "Epoch: 056, Loss: 1.7695, Train: 0.4248, Test: 0.3975\n",
            "Early stopping:  0.021848919797009307\n",
            "Epoch: 057, Loss: 1.7556, Train: 0.4288, Test: 0.4053\n",
            "Early stopping:  0.022066154133997457\n",
            "Epoch: 058, Loss: 1.7347, Train: 0.4356, Test: 0.4142\n",
            "Early stopping:  0.024216854731385084\n",
            "Epoch: 059, Loss: 1.7137, Train: 0.4419, Test: 0.4213\n",
            "Early stopping:  0.02775444580396834\n",
            "Epoch: 060, Loss: 1.7030, Train: 0.4396, Test: 0.4259\n",
            "Early stopping:  0.02780803351706976\n",
            "Epoch: 061, Loss: 1.6866, Train: 0.4481, Test: 0.4287\n",
            "Early stopping:  0.027026471919426438\n",
            "Epoch: 062, Loss: 1.6716, Train: 0.4509, Test: 0.4328\n",
            "Early stopping:  0.02435493969814085\n",
            "Epoch: 063, Loss: 1.6512, Train: 0.4657, Test: 0.4382\n",
            "Early stopping:  0.024841796653842348\n",
            "Epoch: 064, Loss: 1.6271, Train: 0.4782, Test: 0.4415\n",
            "Early stopping:  0.029732208064619507\n",
            "Epoch: 065, Loss: 1.6094, Train: 0.4821, Test: 0.4419\n",
            "Early stopping:  0.03152539220517497\n",
            "Epoch: 066, Loss: 1.5910, Train: 0.4816, Test: 0.4441\n",
            "Early stopping:  0.03214097502837579\n",
            "Epoch: 067, Loss: 1.5774, Train: 0.4850, Test: 0.4504\n",
            "Early stopping:  0.029181177260523505\n",
            "Epoch: 068, Loss: 1.5592, Train: 0.4957, Test: 0.4501\n",
            "Early stopping:  0.02654440181664517\n",
            "Epoch: 069, Loss: 1.5429, Train: 0.5048, Test: 0.4527\n",
            "Early stopping:  0.026063047740076926\n",
            "Epoch: 070, Loss: 1.5227, Train: 0.5105, Test: 0.4555\n",
            "Early stopping:  0.02713472875324674\n",
            "Epoch: 071, Loss: 1.5050, Train: 0.5111, Test: 0.4585\n",
            "Early stopping:  0.02870086919178611\n",
            "Epoch: 072, Loss: 1.4888, Train: 0.5128, Test: 0.4630\n",
            "Early stopping:  0.028287611113127296\n",
            "Epoch: 073, Loss: 1.4744, Train: 0.5150, Test: 0.4674\n",
            "Early stopping:  0.027073917632452198\n",
            "Epoch: 074, Loss: 1.4615, Train: 0.5167, Test: 0.4668\n",
            "Early stopping:  0.02421519436170857\n",
            "Epoch: 075, Loss: 1.4499, Train: 0.5201, Test: 0.4705\n",
            "Early stopping:  0.02177843323499657\n",
            "Epoch: 076, Loss: 1.4395, Train: 0.5241, Test: 0.4739\n",
            "Early stopping:  0.01950079728823869\n",
            "Epoch: 077, Loss: 1.4284, Train: 0.5269, Test: 0.4752\n",
            "Early stopping:  0.018047987770499985\n",
            "Epoch: 078, Loss: 1.4131, Train: 0.5286, Test: 0.4776\n",
            "Early stopping:  0.01877469734575512\n",
            "Epoch: 079, Loss: 1.4030, Train: 0.5372, Test: 0.4793\n",
            "Early stopping:  0.019042863921445424\n",
            "Epoch: 080, Loss: 1.3872, Train: 0.5417, Test: 0.4794\n",
            "Early stopping:  0.02059831341640595\n",
            "Epoch: 081, Loss: 1.3734, Train: 0.5530, Test: 0.4810\n",
            "Early stopping:  0.021533606476138428\n",
            "Epoch: 082, Loss: 1.3646, Train: 0.5564, Test: 0.4809\n",
            "Early stopping:  0.020118105644946444\n",
            "Epoch: 083, Loss: 1.3458, Train: 0.5581, Test: 0.4813\n",
            "Early stopping:  0.021784760893549554\n",
            "Epoch: 084, Loss: 1.3359, Train: 0.5615, Test: 0.4844\n",
            "Early stopping:  0.02069503830487501\n",
            "Epoch: 085, Loss: 1.3205, Train: 0.5638, Test: 0.4830\n",
            "Early stopping:  0.02138107242291046\n",
            "Epoch: 086, Loss: 1.3086, Train: 0.5706, Test: 0.4844\n",
            "Early stopping:  0.02178376100199261\n",
            "Epoch: 087, Loss: 1.2982, Train: 0.5712, Test: 0.4871\n",
            "Early stopping:  0.019397267082518817\n",
            "Epoch: 088, Loss: 1.2851, Train: 0.5712, Test: 0.4867\n",
            "Early stopping:  0.019623773124894828\n",
            "Epoch: 089, Loss: 1.2799, Train: 0.5763, Test: 0.4921\n",
            "Early stopping:  0.016675766753225737\n",
            "Epoch: 090, Loss: 1.2659, Train: 0.5820, Test: 0.4948\n",
            "Early stopping:  0.016523073914656402\n",
            "Epoch: 091, Loss: 1.2563, Train: 0.5927, Test: 0.4945\n",
            "Early stopping:  0.01640662230602437\n",
            "Epoch: 092, Loss: 1.2410, Train: 0.5848, Test: 0.4967\n",
            "Early stopping:  0.017866302970324023\n",
            "Epoch: 093, Loss: 1.2329, Train: 0.5899, Test: 0.4987\n",
            "Early stopping:  0.01886873070319517\n",
            "Epoch: 094, Loss: 1.2213, Train: 0.5990, Test: 0.5030\n",
            "Early stopping:  0.01787576925827763\n",
            "Epoch: 095, Loss: 1.2101, Train: 0.6098, Test: 0.5016\n",
            "Early stopping:  0.017769257574477542\n",
            "Epoch: 096, Loss: 1.2026, Train: 0.6069, Test: 0.5034\n",
            "Early stopping:  0.015772585214719997\n",
            "Epoch: 097, Loss: 1.1881, Train: 0.6012, Test: 0.5062\n",
            "Early stopping:  0.017188368875318834\n",
            "Epoch: 098, Loss: 1.1837, Train: 0.6103, Test: 0.5067\n",
            "Early stopping:  0.015509351936232003\n",
            "Epoch: 099, Loss: 1.1675, Train: 0.6217, Test: 0.5045\n",
            "Early stopping:  0.016683263280931324\n",
            "Epoch: 100, Loss: 1.1607, Train: 0.6239, Test: 0.5082\n",
            "Early stopping:  0.016728522451281824\n",
            "Epoch: 101, Loss: 1.1445, Train: 0.6200, Test: 0.5112\n",
            "Early stopping:  0.017714627629839958\n",
            "Epoch: 102, Loss: 1.1398, Train: 0.6251, Test: 0.5072\n",
            "Early stopping:  0.017777484281304302\n",
            "Epoch: 103, Loss: 1.1343, Train: 0.6347, Test: 0.5077\n",
            "Early stopping:  0.01413981777924458\n",
            "Epoch: 104, Loss: 1.1243, Train: 0.6330, Test: 0.5106\n",
            "Early stopping:  0.013451522095947644\n",
            "Epoch: 105, Loss: 1.1108, Train: 0.6398, Test: 0.5108\n",
            "Early stopping:  0.013453401302100881\n",
            "Epoch: 106, Loss: 1.1038, Train: 0.6370, Test: 0.5060\n",
            "Early stopping:  0.015251048713548713\n",
            "Epoch: 107, Loss: 1.1051, Train: 0.6444, Test: 0.5067\n",
            "Early stopping:  0.013200693404364153\n",
            "Epoch: 108, Loss: 1.0943, Train: 0.6551, Test: 0.5088\n",
            "Early stopping:  0.011016774009628333\n",
            "Epoch: 109, Loss: 1.0832, Train: 0.6444, Test: 0.5138\n",
            "Early stopping:  0.010825230975120921\n",
            "Epoch: 110, Loss: 1.0889, Train: 0.6461, Test: 0.5099\n",
            "Early stopping:  0.009432509028564875\n",
            "PREDICTIONS -> tensor([ 9, 11,  1,  ..., 11,  7,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.44      0.45      0.45       758\n",
            "         capital_goods       0.31      0.17      0.22       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.45      0.53      0.48       793\n",
            " consumer_non-cyclical       0.69      0.34      0.45       446\n",
            "                energy       0.62      0.52      0.57       283\n",
            "             financial       0.64      0.54      0.58       767\n",
            "            healthcare       0.44      0.58      0.50       318\n",
            "              services       0.53      0.69      0.60      2076\n",
            "            technology       0.40      0.28      0.33       396\n",
            "        transportation       0.62      0.63      0.62       404\n",
            "             utilities       0.44      0.28      0.34       225\n",
            "\n",
            "              accuracy                           0.51      7054\n",
            "             macro avg       0.46      0.42      0.43      7054\n",
            "          weighted avg       0.50      0.51      0.50      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 94.2396, Train: 0.1095, Test: 0.1121\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 465.2692, Train: 0.2950, Test: 0.2960\n",
            "Early stopping:  262.3574990778841\n",
            "Epoch: 003, Loss: 542.4835, Train: 0.0675, Test: 0.0672\n",
            "Early stopping:  239.6342891543607\n",
            "Epoch: 004, Loss: 604.3899, Train: 0.0539, Test: 0.0527\n",
            "Early stopping:  228.7625806535284\n",
            "Epoch: 005, Loss: 574.9668, Train: 0.1344, Test: 0.1283\n",
            "Early stopping:  208.93071570551825\n",
            "Epoch: 006, Loss: 436.4589, Train: 0.0510, Test: 0.0526\n",
            "Early stopping:  71.6100639636091\n",
            "Epoch: 007, Loss: 430.1808, Train: 0.1702, Test: 0.1708\n",
            "Early stopping:  80.10708701031534\n",
            "Epoch: 008, Loss: 325.6125, Train: 0.2944, Test: 0.2950\n",
            "Early stopping:  114.61195034018056\n",
            "Epoch: 009, Loss: 333.8239, Train: 0.2927, Test: 0.2926\n",
            "Early stopping:  100.90135863174706\n",
            "Epoch: 010, Loss: 300.7767, Train: 0.2592, Test: 0.2565\n",
            "Early stopping:  63.24982806020734\n",
            "Epoch: 011, Loss: 234.0803, Train: 0.1004, Test: 0.1011\n",
            "Early stopping:  70.7006664178874\n",
            "Epoch: 012, Loss: 220.1199, Train: 0.0607, Test: 0.0580\n",
            "Early stopping:  52.587752910395395\n",
            "Epoch: 013, Loss: 168.8857, Train: 0.1015, Test: 0.0990\n",
            "Early stopping:  65.80027960512659\n",
            "Epoch: 014, Loss: 104.3835, Train: 0.1123, Test: 0.1148\n",
            "Early stopping:  73.60924808882399\n",
            "Epoch: 015, Loss: 57.7445, Train: 0.1713, Test: 0.1803\n",
            "Early stopping:  75.31088960768642\n",
            "Epoch: 016, Loss: 31.2465, Train: 0.1838, Test: 0.1918\n",
            "Early stopping:  78.0200627543148\n",
            "Epoch: 017, Loss: 18.5363, Train: 0.2518, Test: 0.2478\n",
            "Early stopping:  61.38293422257571\n",
            "Epoch: 018, Loss: 8.8712, Train: 0.2706, Test: 0.2594\n",
            "Early stopping:  38.34069026937834\n",
            "Epoch: 019, Loss: 6.6244, Train: 0.2768, Test: 0.2674\n",
            "Early stopping:  20.916599119256276\n",
            "Epoch: 020, Loss: 5.4101, Train: 0.2734, Test: 0.2671\n",
            "Early stopping:  10.867555015270991\n",
            "Epoch: 021, Loss: 4.5284, Train: 0.2689, Test: 0.2620\n",
            "Early stopping:  5.684854031756167\n",
            "Epoch: 022, Loss: 3.8448, Train: 0.2672, Test: 0.2582\n",
            "Early stopping:  1.980411005593859\n",
            "Epoch: 023, Loss: 3.3321, Train: 0.2649, Test: 0.2528\n",
            "Early stopping:  1.3065258310209058\n",
            "Epoch: 024, Loss: 2.9441, Train: 0.2604, Test: 0.2467\n",
            "Early stopping:  0.9813351724652289\n",
            "Epoch: 025, Loss: 2.6453, Train: 0.2711, Test: 0.2494\n",
            "Early stopping:  0.7476523018130479\n",
            "Epoch: 026, Loss: 2.4234, Train: 0.2791, Test: 0.2528\n",
            "Early stopping:  0.5652823108153614\n",
            "Epoch: 027, Loss: 2.2856, Train: 0.2796, Test: 0.2601\n",
            "Early stopping:  0.4203831213458023\n",
            "Epoch: 028, Loss: 2.2246, Train: 0.2757, Test: 0.2576\n",
            "Early stopping:  0.2940597634870195\n",
            "Epoch: 029, Loss: 2.2030, Train: 0.2830, Test: 0.2607\n",
            "Early stopping:  0.18293164537823117\n",
            "Epoch: 030, Loss: 2.1949, Train: 0.2706, Test: 0.2618\n",
            "Early stopping:  0.09476590039969242\n",
            "Epoch: 031, Loss: 2.1883, Train: 0.2677, Test: 0.2627\n",
            "Early stopping:  0.039550135499861125\n",
            "Epoch: 032, Loss: 2.1802, Train: 0.2660, Test: 0.2624\n",
            "Early stopping:  0.016980378301626173\n",
            "Epoch: 033, Loss: 2.1726, Train: 0.2677, Test: 0.2645\n",
            "Early stopping:  0.011920679600491901\n",
            "Epoch: 034, Loss: 2.1648, Train: 0.2768, Test: 0.2676\n",
            "Early stopping:  0.011989646024396278\n",
            "Epoch: 035, Loss: 2.1594, Train: 0.2813, Test: 0.2675\n",
            "Early stopping:  0.011607399725061009\n",
            "Epoch: 036, Loss: 2.1526, Train: 0.2904, Test: 0.2764\n",
            "Early stopping:  0.010864154463194647\n",
            "Epoch: 037, Loss: 2.1395, Train: 0.3125, Test: 0.2925\n",
            "Early stopping:  0.012575480502772822\n",
            "Epoch: 038, Loss: 2.1127, Train: 0.3352, Test: 0.3191\n",
            "Early stopping:  0.020780784385201773\n",
            "Epoch: 039, Loss: 2.0749, Train: 0.3670, Test: 0.3486\n",
            "Early stopping:  0.034537024260496485\n",
            "Epoch: 040, Loss: 2.0352, Train: 0.3800, Test: 0.3595\n",
            "Early stopping:  0.04814069179483562\n",
            "Epoch: 041, Loss: 2.0016, Train: 0.3971, Test: 0.3765\n",
            "Early stopping:  0.055975433006467554\n",
            "Epoch: 042, Loss: 1.9663, Train: 0.4175, Test: 0.3907\n",
            "Early stopping:  0.057918981712934324\n",
            "Epoch: 043, Loss: 1.9369, Train: 0.4248, Test: 0.3976\n",
            "Early stopping:  0.054611352570631995\n",
            "Epoch: 044, Loss: 1.9050, Train: 0.4356, Test: 0.4001\n",
            "Early stopping:  0.05142757374510566\n",
            "Epoch: 045, Loss: 1.8801, Train: 0.4487, Test: 0.4076\n",
            "Early stopping:  0.04818195079860047\n",
            "Epoch: 046, Loss: 1.8519, Train: 0.4583, Test: 0.4230\n",
            "Early stopping:  0.0451649919425299\n",
            "Epoch: 047, Loss: 1.8265, Train: 0.4611, Test: 0.4295\n",
            "Early stopping:  0.043332839610428706\n",
            "Epoch: 048, Loss: 1.8010, Train: 0.4776, Test: 0.4395\n",
            "Early stopping:  0.0413691041881407\n",
            "Epoch: 049, Loss: 1.7793, Train: 0.4799, Test: 0.4461\n",
            "Early stopping:  0.03997368915670701\n",
            "Epoch: 050, Loss: 1.7583, Train: 0.4850, Test: 0.4509\n",
            "Early stopping:  0.037131642395114475\n",
            "Epoch: 051, Loss: 1.7412, Train: 0.4850, Test: 0.4538\n",
            "Early stopping:  0.03379969905529945\n",
            "Epoch: 052, Loss: 1.7235, Train: 0.4912, Test: 0.4578\n",
            "Early stopping:  0.030576368091863093\n",
            "Epoch: 053, Loss: 1.7021, Train: 0.4974, Test: 0.4614\n",
            "Early stopping:  0.029943353708196733\n",
            "Epoch: 054, Loss: 1.6810, Train: 0.5060, Test: 0.4636\n",
            "Early stopping:  0.030667825246641127\n",
            "Epoch: 055, Loss: 1.6600, Train: 0.5088, Test: 0.4658\n",
            "Early stopping:  0.03242982798440064\n",
            "Epoch: 056, Loss: 1.6410, Train: 0.5094, Test: 0.4636\n",
            "Early stopping:  0.032730792843271676\n",
            "Epoch: 057, Loss: 1.6224, Train: 0.5116, Test: 0.4654\n",
            "Early stopping:  0.03151673610835324\n",
            "Epoch: 058, Loss: 1.6039, Train: 0.5179, Test: 0.4648\n",
            "Early stopping:  0.030310403564695852\n",
            "Epoch: 059, Loss: 1.5862, Train: 0.5145, Test: 0.4657\n",
            "Early stopping:  0.029187051865756042\n",
            "Epoch: 060, Loss: 1.5672, Train: 0.5139, Test: 0.4650\n",
            "Early stopping:  0.029063675572996567\n",
            "Epoch: 061, Loss: 1.5502, Train: 0.5196, Test: 0.4648\n",
            "Early stopping:  0.028638788930334506\n",
            "Epoch: 062, Loss: 1.5338, Train: 0.5230, Test: 0.4678\n",
            "Early stopping:  0.027890998185218695\n",
            "Epoch: 063, Loss: 1.5174, Train: 0.5224, Test: 0.4704\n",
            "Early stopping:  0.02708013490377817\n",
            "Epoch: 064, Loss: 1.5011, Train: 0.5281, Test: 0.4681\n",
            "Early stopping:  0.026123159204582518\n",
            "Epoch: 065, Loss: 1.4895, Train: 0.5355, Test: 0.4739\n",
            "Early stopping:  0.024416300528344898\n",
            "Epoch: 066, Loss: 1.4693, Train: 0.5326, Test: 0.4742\n",
            "Early stopping:  0.024837138144390123\n",
            "Epoch: 067, Loss: 1.4510, Train: 0.5451, Test: 0.4719\n",
            "Early stopping:  0.026118742504844708\n",
            "Epoch: 068, Loss: 1.4416, Train: 0.5474, Test: 0.4766\n",
            "Early stopping:  0.025086272758290874\n",
            "Epoch: 069, Loss: 1.4216, Train: 0.5434, Test: 0.4790\n",
            "Early stopping:  0.02603804126543322\n",
            "Epoch: 070, Loss: 1.4131, Train: 0.5547, Test: 0.4810\n",
            "Early stopping:  0.022613185728446894\n",
            "Epoch: 071, Loss: 1.3932, Train: 0.5519, Test: 0.4768\n",
            "Early stopping:  0.02296036337849344\n",
            "Epoch: 072, Loss: 1.3874, Train: 0.5587, Test: 0.4838\n",
            "Early stopping:  0.021936257024514356\n",
            "Epoch: 073, Loss: 1.3623, Train: 0.5632, Test: 0.4828\n",
            "Early stopping:  0.023255384693430976\n",
            "Epoch: 074, Loss: 1.3550, Train: 0.5638, Test: 0.4823\n",
            "Early stopping:  0.023643849751982908\n",
            "Epoch: 075, Loss: 1.3376, Train: 0.5780, Test: 0.4885\n",
            "Early stopping:  0.023071762249805033\n",
            "Epoch: 076, Loss: 1.3243, Train: 0.5735, Test: 0.4959\n",
            "Early stopping:  0.02413246388571446\n",
            "Epoch: 077, Loss: 1.3080, Train: 0.5735, Test: 0.4939\n",
            "Early stopping:  0.022151567570185174\n",
            "Epoch: 078, Loss: 1.2997, Train: 0.5763, Test: 0.4942\n",
            "Early stopping:  0.022286146508125267\n",
            "Epoch: 079, Loss: 1.2928, Train: 0.5871, Test: 0.4997\n",
            "Early stopping:  0.01832439870144055\n",
            "Epoch: 080, Loss: 1.2686, Train: 0.5882, Test: 0.4999\n",
            "Early stopping:  0.020521453163365445\n",
            "Epoch: 081, Loss: 1.2683, Train: 0.5973, Test: 0.5003\n",
            "Early stopping:  0.01818998057076563\n",
            "Epoch: 082, Loss: 1.2453, Train: 0.5961, Test: 0.4990\n",
            "Early stopping:  0.021766045151835493\n",
            "Epoch: 083, Loss: 1.2395, Train: 0.5950, Test: 0.5017\n",
            "Early stopping:  0.02128314189424095\n",
            "Epoch: 084, Loss: 1.2295, Train: 0.5990, Test: 0.5028\n",
            "Early stopping:  0.017575921401860213\n",
            "Epoch: 085, Loss: 1.2103, Train: 0.6041, Test: 0.5041\n",
            "Early stopping:  0.021275927540342875\n",
            "Epoch: 086, Loss: 1.2094, Train: 0.6052, Test: 0.5058\n",
            "Early stopping:  0.016466949263657292\n",
            "Epoch: 087, Loss: 1.1932, Train: 0.6120, Test: 0.5052\n",
            "Early stopping:  0.018213649348264485\n",
            "Epoch: 088, Loss: 1.1799, Train: 0.6137, Test: 0.5057\n",
            "Early stopping:  0.018789885995580395\n",
            "Epoch: 089, Loss: 1.1763, Train: 0.6086, Test: 0.5105\n",
            "Early stopping:  0.015938716620634133\n",
            "Epoch: 090, Loss: 1.1740, Train: 0.6239, Test: 0.5091\n",
            "Early stopping:  0.014771357761950281\n",
            "Epoch: 091, Loss: 1.1542, Train: 0.6262, Test: 0.5096\n",
            "Early stopping:  0.01406924170120718\n",
            "Epoch: 092, Loss: 1.1439, Train: 0.6228, Test: 0.5153\n",
            "Early stopping:  0.0157454136785112\n",
            "Epoch: 093, Loss: 1.1311, Train: 0.6285, Test: 0.5170\n",
            "Early stopping:  0.01938730652573491\n",
            "Epoch: 094, Loss: 1.1242, Train: 0.6336, Test: 0.5164\n",
            "Early stopping:  0.019696994471291154\n",
            "Epoch: 095, Loss: 1.1078, Train: 0.6381, Test: 0.5142\n",
            "Early stopping:  0.01790618760071604\n",
            "Epoch: 096, Loss: 1.0999, Train: 0.6398, Test: 0.5163\n",
            "Early stopping:  0.017715852186632696\n",
            "Epoch: 097, Loss: 1.0889, Train: 0.6415, Test: 0.5183\n",
            "Early stopping:  0.017322756452386134\n",
            "Epoch: 098, Loss: 1.0791, Train: 0.6455, Test: 0.5152\n",
            "Early stopping:  0.01736547578896512\n",
            "Epoch: 099, Loss: 1.0727, Train: 0.6489, Test: 0.5211\n",
            "Early stopping:  0.014437270853531522\n",
            "Epoch: 100, Loss: 1.0603, Train: 0.6557, Test: 0.5184\n",
            "Early stopping:  0.015145868912778207\n",
            "Epoch: 101, Loss: 1.0569, Train: 0.6529, Test: 0.5201\n",
            "Early stopping:  0.013210972554206349\n",
            "Epoch: 102, Loss: 1.0426, Train: 0.6517, Test: 0.5232\n",
            "Early stopping:  0.014247401108586046\n",
            "Epoch: 103, Loss: 1.0370, Train: 0.6574, Test: 0.5252\n",
            "Early stopping:  0.014303454337416291\n",
            "Epoch: 104, Loss: 1.0224, Train: 0.6636, Test: 0.5180\n",
            "Early stopping:  0.01543187477331937\n",
            "Epoch: 105, Loss: 1.0152, Train: 0.6744, Test: 0.5179\n",
            "Early stopping:  0.01652415319011312\n",
            "Epoch: 106, Loss: 1.0054, Train: 0.6710, Test: 0.5267\n",
            "Early stopping:  0.015292778258547761\n",
            "Epoch: 107, Loss: 1.0014, Train: 0.6733, Test: 0.5258\n",
            "Early stopping:  0.014189451094963813\n",
            "Epoch: 108, Loss: 0.9880, Train: 0.6761, Test: 0.5240\n",
            "Early stopping:  0.013219671617338677\n",
            "Epoch: 109, Loss: 0.9793, Train: 0.6670, Test: 0.5289\n",
            "Early stopping:  0.014252654845263647\n",
            "Epoch: 110, Loss: 0.9802, Train: 0.6773, Test: 0.5285\n",
            "Early stopping:  0.012010619974253052\n",
            "Epoch: 111, Loss: 0.9663, Train: 0.6852, Test: 0.5259\n",
            "Early stopping:  0.012853045186747855\n",
            "Epoch: 112, Loss: 0.9586, Train: 0.6818, Test: 0.5292\n",
            "Early stopping:  0.011815057176854155\n",
            "Epoch: 113, Loss: 0.9553, Train: 0.6852, Test: 0.5316\n",
            "Early stopping:  0.011535498529728667\n",
            "Epoch: 114, Loss: 0.9460, Train: 0.6931, Test: 0.5251\n",
            "Early stopping:  0.012872078669248695\n",
            "Epoch: 115, Loss: 0.9346, Train: 0.6965, Test: 0.5279\n",
            "Early stopping:  0.012228140087685817\n",
            "Epoch: 116, Loss: 0.9245, Train: 0.6824, Test: 0.5308\n",
            "Early stopping:  0.014231504396131738\n",
            "Epoch: 117, Loss: 0.9272, Train: 0.6897, Test: 0.5305\n",
            "Early stopping:  0.012948287198083677\n",
            "Epoch: 118, Loss: 0.9190, Train: 0.6835, Test: 0.5347\n",
            "Early stopping:  0.01042435112567958\n",
            "Epoch: 119, Loss: 0.9184, Train: 0.7022, Test: 0.5344\n",
            "Early stopping:  0.006640875656786691\n",
            "PREDICTIONS -> tensor([ 9, 11,  9,  ...,  5,  1,  9], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.43      0.57      0.49       758\n",
            "         capital_goods       0.23      0.09      0.13       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.51      0.49      0.50       793\n",
            " consumer_non-cyclical       0.64      0.38      0.48       446\n",
            "                energy       0.52      0.56      0.54       283\n",
            "             financial       0.65      0.57      0.61       767\n",
            "            healthcare       0.55      0.59      0.57       318\n",
            "              services       0.58      0.73      0.64      2076\n",
            "            technology       0.35      0.34      0.34       396\n",
            "        transportation       0.66      0.58      0.62       404\n",
            "             utilities       0.49      0.32      0.39       225\n",
            "\n",
            "              accuracy                           0.53      7054\n",
            "             macro avg       0.47      0.43      0.44      7054\n",
            "          weighted avg       0.52      0.53      0.52      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 79.5347, Train: 0.2910, Test: 0.2919\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 287.5445, Train: 0.0664, Test: 0.0628\n",
            "Early stopping:  147.0851050368687\n",
            "Epoch: 003, Loss: 480.5490, Train: 0.1157, Test: 0.1097\n",
            "Early stopping:  200.5539171011613\n",
            "Epoch: 004, Loss: 453.9270, Train: 0.0868, Test: 0.0885\n",
            "Early stopping:  184.81809468828834\n",
            "Epoch: 005, Loss: 451.0452, Train: 0.1106, Test: 0.1096\n",
            "Early stopping:  169.63549806478122\n",
            "Epoch: 006, Loss: 423.9687, Train: 0.0635, Test: 0.0662\n",
            "Early stopping:  76.3863322621251\n",
            "Epoch: 007, Loss: 383.3425, Train: 0.0953, Test: 0.0926\n",
            "Early stopping:  36.80006725733987\n",
            "Epoch: 008, Loss: 260.7696, Train: 0.0851, Test: 0.0753\n",
            "Early stopping:  80.00873712127016\n",
            "Epoch: 009, Loss: 199.2702, Train: 0.3165, Test: 0.3092\n",
            "Early stopping:  108.71405731279205\n",
            "Epoch: 010, Loss: 171.9268, Train: 0.3103, Test: 0.3018\n",
            "Early stopping:  111.42825031881515\n",
            "Epoch: 011, Loss: 157.7592, Train: 0.3040, Test: 0.2949\n",
            "Early stopping:  92.04592855692808\n",
            "Epoch: 012, Loss: 123.2222, Train: 0.0652, Test: 0.0675\n",
            "Early stopping:  51.58749864753318\n",
            "Epoch: 013, Loss: 115.5599, Train: 0.0664, Test: 0.0706\n",
            "Early stopping:  34.672714431173155\n",
            "Epoch: 014, Loss: 76.6541, Train: 0.1327, Test: 0.1299\n",
            "Early stopping:  37.496348410063696\n",
            "Epoch: 015, Loss: 33.7997, Train: 0.2093, Test: 0.2095\n",
            "Early stopping:  47.526894700441005\n",
            "Epoch: 016, Loss: 8.6725, Train: 0.1673, Test: 0.1613\n",
            "Early stopping:  50.028994729533565\n",
            "Epoch: 017, Loss: 5.1122, Train: 0.1826, Test: 0.1725\n",
            "Early stopping:  47.363107727249435\n",
            "Epoch: 018, Loss: 4.2082, Train: 0.2201, Test: 0.2263\n",
            "Early stopping:  30.974277864519717\n",
            "Epoch: 019, Loss: 3.4542, Train: 0.3091, Test: 0.3065\n",
            "Early stopping:  12.874069580006484\n",
            "Epoch: 020, Loss: 2.9031, Train: 0.3630, Test: 0.3452\n",
            "Early stopping:  2.2819208083145455\n",
            "Epoch: 021, Loss: 2.7434, Train: 0.3607, Test: 0.3419\n",
            "Early stopping:  0.982569651409115\n",
            "Epoch: 022, Loss: 2.6706, Train: 0.3795, Test: 0.3650\n",
            "Early stopping:  0.6438093471542994\n",
            "Epoch: 023, Loss: 2.4923, Train: 0.3914, Test: 0.3720\n",
            "Early stopping:  0.3672014905606456\n",
            "Epoch: 024, Loss: 2.3416, Train: 0.3914, Test: 0.3761\n",
            "Early stopping:  0.21862462102153607\n",
            "Epoch: 025, Loss: 2.2499, Train: 0.3806, Test: 0.3683\n",
            "Early stopping:  0.20977354902938672\n",
            "Epoch: 026, Loss: 2.1864, Train: 0.3766, Test: 0.3683\n",
            "Early stopping:  0.19530840207186326\n",
            "Epoch: 027, Loss: 2.1230, Train: 0.3823, Test: 0.3694\n",
            "Early stopping:  0.14421997142431764\n",
            "Epoch: 028, Loss: 2.0628, Train: 0.3857, Test: 0.3724\n",
            "Early stopping:  0.10866616295139196\n",
            "Epoch: 029, Loss: 2.0154, Train: 0.3925, Test: 0.3767\n",
            "Early stopping:  0.09383454976504998\n",
            "Epoch: 030, Loss: 1.9884, Train: 0.3891, Test: 0.3730\n",
            "Early stopping:  0.0804794516613769\n",
            "Epoch: 031, Loss: 1.9702, Train: 0.3851, Test: 0.3733\n",
            "Early stopping:  0.06167282395371932\n",
            "Epoch: 032, Loss: 1.9583, Train: 0.3778, Test: 0.3662\n",
            "Early stopping:  0.0416307211485238\n",
            "Epoch: 033, Loss: 1.9478, Train: 0.3727, Test: 0.3587\n",
            "Early stopping:  0.026704865148458597\n",
            "Epoch: 034, Loss: 1.9383, Train: 0.3698, Test: 0.3578\n",
            "Early stopping:  0.019590275422509847\n",
            "Epoch: 035, Loss: 1.9249, Train: 0.3710, Test: 0.3578\n",
            "Early stopping:  0.01750649489886152\n",
            "Epoch: 036, Loss: 1.9063, Train: 0.3772, Test: 0.3604\n",
            "Early stopping:  0.020258987832219643\n",
            "Epoch: 037, Loss: 1.8800, Train: 0.3800, Test: 0.3686\n",
            "Early stopping:  0.027009333310924224\n",
            "Epoch: 038, Loss: 1.8483, Train: 0.3897, Test: 0.3816\n",
            "Early stopping:  0.0360544051989228\n",
            "Epoch: 039, Loss: 1.8199, Train: 0.4084, Test: 0.3935\n",
            "Early stopping:  0.0425424788883798\n",
            "Epoch: 040, Loss: 1.7938, Train: 0.4209, Test: 0.4026\n",
            "Early stopping:  0.045081404435111515\n",
            "Epoch: 041, Loss: 1.7694, Train: 0.4288, Test: 0.4080\n",
            "Early stopping:  0.043632343106824206\n",
            "Epoch: 042, Loss: 1.7463, Train: 0.4351, Test: 0.4107\n",
            "Early stopping:  0.04026868580009668\n",
            "Epoch: 043, Loss: 1.7245, Train: 0.4368, Test: 0.4147\n",
            "Early stopping:  0.037711538856715535\n",
            "Epoch: 044, Loss: 1.7035, Train: 0.4498, Test: 0.4175\n",
            "Early stopping:  0.03567655417129685\n",
            "Epoch: 045, Loss: 1.6770, Train: 0.4572, Test: 0.4212\n",
            "Early stopping:  0.03602502327413045\n",
            "Epoch: 046, Loss: 1.6488, Train: 0.4640, Test: 0.4239\n",
            "Early stopping:  0.0384546007418718\n",
            "Epoch: 047, Loss: 1.6248, Train: 0.4759, Test: 0.4312\n",
            "Early stopping:  0.04025141795179106\n",
            "Epoch: 048, Loss: 1.6011, Train: 0.4810, Test: 0.4348\n",
            "Early stopping:  0.0406676029205593\n",
            "Epoch: 049, Loss: 1.5823, Train: 0.4872, Test: 0.4365\n",
            "Early stopping:  0.037564844424875156\n",
            "Epoch: 050, Loss: 1.5652, Train: 0.4929, Test: 0.4366\n",
            "Early stopping:  0.03324076936208602\n",
            "Epoch: 051, Loss: 1.5493, Train: 0.4946, Test: 0.4383\n",
            "Early stopping:  0.029641913571324827\n",
            "Epoch: 052, Loss: 1.5325, Train: 0.5003, Test: 0.4430\n",
            "Early stopping:  0.02691958852165401\n",
            "Epoch: 053, Loss: 1.5155, Train: 0.5020, Test: 0.4447\n",
            "Early stopping:  0.026298103005897463\n",
            "Epoch: 054, Loss: 1.4997, Train: 0.5054, Test: 0.4458\n",
            "Early stopping:  0.026063684634696025\n",
            "Epoch: 055, Loss: 1.4822, Train: 0.5077, Test: 0.4491\n",
            "Early stopping:  0.02640635449771473\n",
            "Epoch: 056, Loss: 1.4662, Train: 0.5116, Test: 0.4507\n",
            "Early stopping:  0.026248617457553466\n",
            "Epoch: 057, Loss: 1.4500, Train: 0.5184, Test: 0.4561\n",
            "Early stopping:  0.026015532858077978\n",
            "Epoch: 058, Loss: 1.4345, Train: 0.5258, Test: 0.4590\n",
            "Early stopping:  0.025731835458761593\n",
            "Epoch: 059, Loss: 1.4191, Train: 0.5309, Test: 0.4617\n",
            "Early stopping:  0.02498522770256339\n",
            "Epoch: 060, Loss: 1.4041, Train: 0.5389, Test: 0.4624\n",
            "Early stopping:  0.0245074772410599\n",
            "Epoch: 061, Loss: 1.3905, Train: 0.5468, Test: 0.4673\n",
            "Early stopping:  0.02359447205042597\n",
            "Epoch: 062, Loss: 1.3738, Train: 0.5474, Test: 0.4736\n",
            "Early stopping:  0.02370436852995469\n",
            "Epoch: 063, Loss: 1.3574, Train: 0.5519, Test: 0.4692\n",
            "Early stopping:  0.024299126961227345\n",
            "Epoch: 064, Loss: 1.3432, Train: 0.5615, Test: 0.4714\n",
            "Early stopping:  0.02452056271535794\n",
            "Epoch: 065, Loss: 1.3250, Train: 0.5649, Test: 0.4735\n",
            "Early stopping:  0.025568941097782273\n",
            "Epoch: 066, Loss: 1.3086, Train: 0.5672, Test: 0.4704\n",
            "Early stopping:  0.025766470308786094\n",
            "Epoch: 067, Loss: 1.2942, Train: 0.5661, Test: 0.4733\n",
            "Early stopping:  0.025491062362023073\n",
            "Epoch: 068, Loss: 1.2789, Train: 0.5701, Test: 0.4760\n",
            "Early stopping:  0.025216717732546582\n",
            "Epoch: 069, Loss: 1.2633, Train: 0.5718, Test: 0.4816\n",
            "Early stopping:  0.024211752793951945\n",
            "Epoch: 070, Loss: 1.2505, Train: 0.5769, Test: 0.4830\n",
            "Early stopping:  0.023269102978653533\n",
            "Epoch: 071, Loss: 1.2347, Train: 0.5808, Test: 0.4830\n",
            "Early stopping:  0.023341202670591546\n",
            "Epoch: 072, Loss: 1.2227, Train: 0.5905, Test: 0.4881\n",
            "Early stopping:  0.022316852084802004\n",
            "Epoch: 073, Loss: 1.2080, Train: 0.5967, Test: 0.4881\n",
            "Early stopping:  0.021872357401510164\n",
            "Epoch: 074, Loss: 1.1965, Train: 0.6047, Test: 0.4949\n",
            "Early stopping:  0.02129498287610566\n",
            "Epoch: 075, Loss: 1.1791, Train: 0.6115, Test: 0.4962\n",
            "Early stopping:  0.0217776087854667\n",
            "Epoch: 076, Loss: 1.1644, Train: 0.6194, Test: 0.4991\n",
            "Early stopping:  0.023064664042313655\n",
            "Epoch: 077, Loss: 1.1507, Train: 0.6251, Test: 0.5061\n",
            "Early stopping:  0.02324707453417081\n",
            "Epoch: 078, Loss: 1.1376, Train: 0.6313, Test: 0.5071\n",
            "Early stopping:  0.023157483310712303\n",
            "Epoch: 079, Loss: 1.1307, Train: 0.6387, Test: 0.5135\n",
            "Early stopping:  0.019675193127760292\n",
            "Epoch: 080, Loss: 1.1114, Train: 0.6421, Test: 0.5146\n",
            "Early stopping:  0.020079041207225496\n",
            "Epoch: 081, Loss: 1.0963, Train: 0.6421, Test: 0.5130\n",
            "Early stopping:  0.02155504472285619\n",
            "Epoch: 082, Loss: 1.0869, Train: 0.6398, Test: 0.5186\n",
            "Early stopping:  0.021667949734978445\n",
            "Epoch: 083, Loss: 1.0735, Train: 0.6546, Test: 0.5196\n",
            "Early stopping:  0.022120101093147836\n",
            "Epoch: 084, Loss: 1.0602, Train: 0.6540, Test: 0.5235\n",
            "Early stopping:  0.01982398226582371\n",
            "Epoch: 085, Loss: 1.0445, Train: 0.6585, Test: 0.5242\n",
            "Early stopping:  0.020676815470693093\n",
            "Epoch: 086, Loss: 1.0318, Train: 0.6591, Test: 0.5197\n",
            "Early stopping:  0.022048776913752358\n",
            "Epoch: 087, Loss: 1.0221, Train: 0.6682, Test: 0.5271\n",
            "Early stopping:  0.02085157513670398\n",
            "Epoch: 088, Loss: 1.0105, Train: 0.6710, Test: 0.5308\n",
            "Early stopping:  0.019355720286849523\n",
            "Epoch: 089, Loss: 1.0064, Train: 0.6750, Test: 0.5337\n",
            "Early stopping:  0.01559693451547502\n",
            "Epoch: 090, Loss: 0.9903, Train: 0.6773, Test: 0.5265\n",
            "Early stopping:  0.015770673333137863\n",
            "Epoch: 091, Loss: 0.9744, Train: 0.6761, Test: 0.5213\n",
            "Early stopping:  0.018580358241052173\n",
            "Epoch: 092, Loss: 0.9768, Train: 0.6710, Test: 0.5274\n",
            "Early stopping:  0.016505253429307063\n",
            "Epoch: 093, Loss: 0.9666, Train: 0.6875, Test: 0.5316\n",
            "Early stopping:  0.015651695769030714\n",
            "Epoch: 094, Loss: 0.9555, Train: 0.6909, Test: 0.5310\n",
            "Early stopping:  0.01289834639539616\n",
            "Epoch: 095, Loss: 0.9452, Train: 0.6892, Test: 0.5336\n",
            "Early stopping:  0.013275287302317767\n",
            "Epoch: 096, Loss: 0.9383, Train: 0.6943, Test: 0.5322\n",
            "Early stopping:  0.015593284902014965\n",
            "Epoch: 097, Loss: 0.9154, Train: 0.6909, Test: 0.5289\n",
            "Early stopping:  0.019309067182387417\n",
            "Epoch: 098, Loss: 0.9173, Train: 0.6994, Test: 0.5342\n",
            "Early stopping:  0.01753644040157199\n",
            "Epoch: 099, Loss: 0.8982, Train: 0.7085, Test: 0.5360\n",
            "Early stopping:  0.018931377523505738\n",
            "Epoch: 100, Loss: 0.8924, Train: 0.7090, Test: 0.5366\n",
            "Early stopping:  0.01808912392825845\n",
            "Epoch: 101, Loss: 0.8829, Train: 0.7141, Test: 0.5361\n",
            "Early stopping:  0.014852755363214376\n",
            "Epoch: 102, Loss: 0.8688, Train: 0.7073, Test: 0.5319\n",
            "Early stopping:  0.018012184253940192\n",
            "Epoch: 103, Loss: 0.8628, Train: 0.7170, Test: 0.5346\n",
            "Early stopping:  0.015065753520386022\n",
            "Epoch: 104, Loss: 0.8509, Train: 0.7215, Test: 0.5352\n",
            "Early stopping:  0.016365685977404985\n",
            "Epoch: 105, Loss: 0.8423, Train: 0.7311, Test: 0.5380\n",
            "Early stopping:  0.015754597926145256\n",
            "Epoch: 106, Loss: 0.8327, Train: 0.7379, Test: 0.5373\n",
            "Early stopping:  0.014714588078232524\n",
            "Epoch: 107, Loss: 0.8244, Train: 0.7402, Test: 0.5381\n",
            "Early stopping:  0.015073316017493262\n",
            "Epoch: 108, Loss: 0.8123, Train: 0.7379, Test: 0.5388\n",
            "Early stopping:  0.015076332579257959\n",
            "Epoch: 109, Loss: 0.8073, Train: 0.7448, Test: 0.5400\n",
            "Early stopping:  0.014389977751992725\n",
            "Epoch: 110, Loss: 0.7969, Train: 0.7533, Test: 0.5407\n",
            "Early stopping:  0.014094508608244955\n",
            "Epoch: 111, Loss: 0.7885, Train: 0.7487, Test: 0.5401\n",
            "Early stopping:  0.013840376406534903\n",
            "Epoch: 112, Loss: 0.7807, Train: 0.7465, Test: 0.5430\n",
            "Early stopping:  0.013018196550177122\n",
            "Epoch: 113, Loss: 0.7725, Train: 0.7527, Test: 0.5415\n",
            "Early stopping:  0.013575109302702294\n",
            "Epoch: 114, Loss: 0.7649, Train: 0.7538, Test: 0.5411\n",
            "Early stopping:  0.012647998792158848\n",
            "Epoch: 115, Loss: 0.7562, Train: 0.7612, Test: 0.5421\n",
            "Early stopping:  0.012702649506317465\n",
            "Epoch: 116, Loss: 0.7487, Train: 0.7635, Test: 0.5431\n",
            "Early stopping:  0.01268602099816452\n",
            "Epoch: 117, Loss: 0.7431, Train: 0.7697, Test: 0.5438\n",
            "Early stopping:  0.011912455468595658\n",
            "Epoch: 118, Loss: 0.7348, Train: 0.7720, Test: 0.5390\n",
            "Early stopping:  0.011654828704937246\n",
            "Epoch: 119, Loss: 0.7266, Train: 0.7697, Test: 0.5430\n",
            "Early stopping:  0.011613629430600899\n",
            "Epoch: 120, Loss: 0.7209, Train: 0.7714, Test: 0.5425\n",
            "Early stopping:  0.011419213639734728\n",
            "Epoch: 121, Loss: 0.7126, Train: 0.7805, Test: 0.5359\n",
            "Early stopping:  0.011835015831238605\n",
            "Epoch: 122, Loss: 0.7113, Train: 0.7760, Test: 0.5407\n",
            "Early stopping:  0.009787857142905716\n",
            "PREDICTIONS -> tensor([ 9,  8,  1,  ..., 11,  7,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.46      0.58      0.51       758\n",
            "         capital_goods       0.39      0.29      0.33       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.51      0.50      0.51       793\n",
            " consumer_non-cyclical       0.59      0.39      0.47       446\n",
            "                energy       0.65      0.54      0.59       283\n",
            "             financial       0.60      0.54      0.57       767\n",
            "            healthcare       0.51      0.54      0.52       318\n",
            "              services       0.58      0.70      0.63      2076\n",
            "            technology       0.43      0.27      0.33       396\n",
            "        transportation       0.58      0.59      0.58       404\n",
            "             utilities       0.62      0.49      0.55       225\n",
            "\n",
            "              accuracy                           0.54      7054\n",
            "             macro avg       0.49      0.45      0.47      7054\n",
            "          weighted avg       0.53      0.54      0.53      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 39.5876, Train: 0.3012, Test: 0.2971\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 273.2590, Train: 0.1129, Test: 0.1124\n",
            "Early stopping:  165.23065178947363\n",
            "Epoch: 003, Loss: 516.8199, Train: 0.0431, Test: 0.0445\n",
            "Early stopping:  238.63321751352888\n",
            "Epoch: 004, Loss: 614.8008, Train: 0.0788, Test: 0.0788\n",
            "Early stopping:  258.0045373187866\n",
            "Epoch: 005, Loss: 440.5042, Train: 0.1134, Test: 0.1114\n",
            "Early stopping:  226.2415200858753\n",
            "Epoch: 006, Loss: 326.0753, Train: 0.0658, Test: 0.0644\n",
            "Early stopping:  138.8122317802711\n",
            "Epoch: 007, Loss: 281.2862, Train: 0.0834, Test: 0.0863\n",
            "Early stopping:  136.51156391712306\n",
            "Epoch: 008, Loss: 191.2868, Train: 0.1781, Test: 0.1888\n",
            "Early stopping:  163.27359439768196\n",
            "Epoch: 009, Loss: 165.8307, Train: 0.3125, Test: 0.3089\n",
            "Early stopping:  110.43760950904735\n",
            "Epoch: 010, Loss: 139.8889, Train: 0.3103, Test: 0.3071\n",
            "Early stopping:  79.34212061168063\n",
            "Epoch: 011, Loss: 109.3424, Train: 0.1027, Test: 0.0981\n",
            "Early stopping:  65.4940270143043\n",
            "Epoch: 012, Loss: 81.3839, Train: 0.1968, Test: 0.1904\n",
            "Early stopping:  43.71275185667861\n",
            "Epoch: 013, Loss: 44.4521, Train: 0.2700, Test: 0.2603\n",
            "Early stopping:  47.722336050986094\n",
            "Epoch: 014, Loss: 16.3326, Train: 0.2195, Test: 0.2180\n",
            "Early stopping:  49.37601965189699\n",
            "Epoch: 015, Loss: 5.7891, Train: 0.1815, Test: 0.1817\n",
            "Early stopping:  43.631073230535236\n",
            "Epoch: 016, Loss: 4.3266, Train: 0.1923, Test: 0.1938\n",
            "Early stopping:  32.711648099466984\n",
            "Epoch: 017, Loss: 3.6850, Train: 0.2082, Test: 0.2053\n",
            "Early stopping:  17.29132591953501\n",
            "Epoch: 018, Loss: 3.1444, Train: 0.2065, Test: 0.2085\n",
            "Early stopping:  5.499386901618932\n",
            "Epoch: 019, Loss: 2.7594, Train: 0.2065, Test: 0.2050\n",
            "Early stopping:  1.1895887497218243\n",
            "Epoch: 020, Loss: 2.5324, Train: 0.2087, Test: 0.2024\n",
            "Early stopping:  0.7258246677236361\n",
            "Epoch: 021, Loss: 2.4219, Train: 0.2087, Test: 0.2031\n",
            "Early stopping:  0.5145376659544985\n",
            "Epoch: 022, Loss: 2.3656, Train: 0.2116, Test: 0.2088\n",
            "Early stopping:  0.3174001070711832\n",
            "Epoch: 023, Loss: 2.3327, Train: 0.2263, Test: 0.2142\n",
            "Early stopping:  0.17243547581795599\n",
            "Epoch: 024, Loss: 2.3076, Train: 0.2371, Test: 0.2278\n",
            "Early stopping:  0.08938385053097751\n",
            "Epoch: 025, Loss: 2.2845, Train: 0.2428, Test: 0.2359\n",
            "Early stopping:  0.05365371465365143\n",
            "Epoch: 026, Loss: 2.2628, Train: 0.2587, Test: 0.2448\n",
            "Early stopping:  0.040258506870516735\n",
            "Epoch: 027, Loss: 2.2426, Train: 0.2621, Test: 0.2496\n",
            "Early stopping:  0.035581642354909744\n",
            "Epoch: 028, Loss: 2.2240, Train: 0.2706, Test: 0.2563\n",
            "Early stopping:  0.033066234659512986\n",
            "Epoch: 029, Loss: 2.2078, Train: 0.2836, Test: 0.2692\n",
            "Early stopping:  0.030453409575546684\n",
            "Epoch: 030, Loss: 2.1940, Train: 0.2847, Test: 0.2757\n",
            "Early stopping:  0.02736860766892857\n",
            "Epoch: 031, Loss: 2.1831, Train: 0.2898, Test: 0.2830\n",
            "Early stopping:  0.02370049110935314\n",
            "Epoch: 032, Loss: 2.1743, Train: 0.2972, Test: 0.2851\n",
            "Early stopping:  0.01978429811988687\n",
            "Epoch: 033, Loss: 2.1652, Train: 0.2984, Test: 0.2866\n",
            "Early stopping:  0.01664100141893739\n",
            "Epoch: 034, Loss: 2.1569, Train: 0.3057, Test: 0.2898\n",
            "Early stopping:  0.014558218248161166\n",
            "Epoch: 035, Loss: 2.1481, Train: 0.3035, Test: 0.2919\n",
            "Early stopping:  0.013814538375867526\n",
            "Epoch: 036, Loss: 2.1373, Train: 0.3052, Test: 0.2927\n",
            "Early stopping:  0.014412518218229963\n",
            "Epoch: 037, Loss: 2.1246, Train: 0.3046, Test: 0.2930\n",
            "Early stopping:  0.016017150222464447\n",
            "Epoch: 038, Loss: 2.1111, Train: 0.3097, Test: 0.2936\n",
            "Early stopping:  0.018266080499159787\n",
            "Epoch: 039, Loss: 2.0972, Train: 0.3069, Test: 0.2966\n",
            "Early stopping:  0.020273433378272494\n",
            "Epoch: 040, Loss: 2.0837, Train: 0.3148, Test: 0.3018\n",
            "Early stopping:  0.021301765482100044\n",
            "Epoch: 041, Loss: 2.0713, Train: 0.3137, Test: 0.3054\n",
            "Early stopping:  0.021206145030436184\n",
            "Epoch: 042, Loss: 2.0599, Train: 0.3159, Test: 0.3090\n",
            "Early stopping:  0.02029193318798086\n",
            "Epoch: 043, Loss: 2.0480, Train: 0.3171, Test: 0.3184\n",
            "Early stopping:  0.0193178533261552\n",
            "Epoch: 044, Loss: 2.0352, Train: 0.3261, Test: 0.3185\n",
            "Early stopping:  0.019011853205840885\n",
            "Epoch: 045, Loss: 2.0219, Train: 0.3398, Test: 0.3262\n",
            "Early stopping:  0.01952431812777717\n",
            "Epoch: 046, Loss: 2.0079, Train: 0.3415, Test: 0.3292\n",
            "Early stopping:  0.020595364399126643\n",
            "Epoch: 047, Loss: 1.9947, Train: 0.3443, Test: 0.3344\n",
            "Early stopping:  0.02116528621892602\n",
            "Epoch: 048, Loss: 1.9825, Train: 0.3539, Test: 0.3356\n",
            "Early stopping:  0.020969020180571788\n",
            "Epoch: 049, Loss: 1.9710, Train: 0.3494, Test: 0.3378\n",
            "Early stopping:  0.020135687715216035\n",
            "Epoch: 050, Loss: 1.9606, Train: 0.3590, Test: 0.3375\n",
            "Early stopping:  0.018730402149736036\n",
            "Epoch: 051, Loss: 1.9508, Train: 0.3653, Test: 0.3459\n",
            "Early stopping:  0.017385727413611322\n",
            "Epoch: 052, Loss: 1.9406, Train: 0.3698, Test: 0.3462\n",
            "Early stopping:  0.01644371221427419\n",
            "Epoch: 053, Loss: 1.9302, Train: 0.3749, Test: 0.3519\n",
            "Early stopping:  0.016044878199801056\n",
            "Epoch: 054, Loss: 1.9175, Train: 0.3834, Test: 0.3589\n",
            "Early stopping:  0.01690957811207233\n",
            "Epoch: 055, Loss: 1.9031, Train: 0.3931, Test: 0.3679\n",
            "Early stopping:  0.018817638845239903\n",
            "Epoch: 056, Loss: 1.8894, Train: 0.4016, Test: 0.3785\n",
            "Early stopping:  0.020535504983634233\n",
            "Epoch: 057, Loss: 1.8765, Train: 0.4095, Test: 0.3835\n",
            "Early stopping:  0.021425328713936286\n",
            "Epoch: 058, Loss: 1.8639, Train: 0.4163, Test: 0.3879\n",
            "Early stopping:  0.02115726834049273\n",
            "Epoch: 059, Loss: 1.8515, Train: 0.4237, Test: 0.3901\n",
            "Early stopping:  0.020335465886943516\n",
            "Epoch: 060, Loss: 1.8391, Train: 0.4282, Test: 0.3945\n",
            "Early stopping:  0.01985029250010249\n",
            "Epoch: 061, Loss: 1.8253, Train: 0.4311, Test: 0.3981\n",
            "Early stopping:  0.020111847547797636\n",
            "Epoch: 062, Loss: 1.8109, Train: 0.4328, Test: 0.4029\n",
            "Early stopping:  0.020898979509266294\n",
            "Epoch: 063, Loss: 1.7959, Train: 0.4368, Test: 0.4071\n",
            "Early stopping:  0.02205580146680238\n",
            "Epoch: 064, Loss: 1.7810, Train: 0.4419, Test: 0.4115\n",
            "Early stopping:  0.02303173162701037\n",
            "Epoch: 065, Loss: 1.7661, Train: 0.4368, Test: 0.4132\n",
            "Early stopping:  0.02345940005203563\n",
            "Epoch: 066, Loss: 1.7511, Train: 0.4441, Test: 0.4161\n",
            "Early stopping:  0.023620001550892396\n",
            "Epoch: 067, Loss: 1.7353, Train: 0.4441, Test: 0.4200\n",
            "Early stopping:  0.02389078581980599\n",
            "Epoch: 068, Loss: 1.7187, Train: 0.4509, Test: 0.4253\n",
            "Early stopping:  0.024561279751534108\n",
            "Epoch: 069, Loss: 1.7023, Train: 0.4555, Test: 0.4310\n",
            "Early stopping:  0.025311939701499534\n",
            "Epoch: 070, Loss: 1.6867, Train: 0.4577, Test: 0.4345\n",
            "Early stopping:  0.025593519714789972\n",
            "Epoch: 071, Loss: 1.6725, Train: 0.4617, Test: 0.4386\n",
            "Early stopping:  0.02493738643687884\n",
            "Epoch: 072, Loss: 1.6587, Train: 0.4668, Test: 0.4405\n",
            "Early stopping:  0.02370375658371275\n",
            "Epoch: 073, Loss: 1.6442, Train: 0.4674, Test: 0.4412\n",
            "Early stopping:  0.022803866699202396\n",
            "Epoch: 074, Loss: 1.6293, Train: 0.4759, Test: 0.4409\n",
            "Early stopping:  0.022627952103028936\n",
            "Epoch: 075, Loss: 1.6140, Train: 0.4838, Test: 0.4444\n",
            "Early stopping:  0.023151418792703733\n",
            "Epoch: 076, Loss: 1.5994, Train: 0.4918, Test: 0.4466\n",
            "Early stopping:  0.02353684657755877\n",
            "Epoch: 077, Loss: 1.5858, Train: 0.4878, Test: 0.4447\n",
            "Early stopping:  0.023199454174867632\n",
            "Epoch: 078, Loss: 1.5724, Train: 0.4923, Test: 0.4457\n",
            "Early stopping:  0.02245660049612149\n",
            "Epoch: 079, Loss: 1.5631, Train: 0.4997, Test: 0.4471\n",
            "Early stopping:  0.020414468755252605\n",
            "Epoch: 080, Loss: 1.5460, Train: 0.4997, Test: 0.4492\n",
            "Early stopping:  0.02052365126297033\n",
            "Epoch: 081, Loss: 1.5338, Train: 0.5048, Test: 0.4524\n",
            "Early stopping:  0.020679829227767885\n",
            "Epoch: 082, Loss: 1.5204, Train: 0.5077, Test: 0.4525\n",
            "Early stopping:  0.02113732339091281\n",
            "Epoch: 083, Loss: 1.5067, Train: 0.5139, Test: 0.4558\n",
            "Early stopping:  0.02189654613151331\n",
            "Epoch: 084, Loss: 1.4941, Train: 0.5207, Test: 0.4603\n",
            "Early stopping:  0.020714184150705375\n",
            "Epoch: 085, Loss: 1.4807, Train: 0.5264, Test: 0.4626\n",
            "Early stopping:  0.02095658640869004\n",
            "Epoch: 086, Loss: 1.4675, Train: 0.5315, Test: 0.4660\n",
            "Early stopping:  0.020850599870542972\n",
            "Epoch: 087, Loss: 1.4539, Train: 0.5343, Test: 0.4673\n",
            "Early stopping:  0.0209230874966997\n",
            "Epoch: 088, Loss: 1.4432, Train: 0.5411, Test: 0.4691\n",
            "Early stopping:  0.020358830044101012\n",
            "Epoch: 089, Loss: 1.4287, Train: 0.5462, Test: 0.4724\n",
            "Early stopping:  0.02031362505420579\n",
            "Epoch: 090, Loss: 1.4180, Train: 0.5525, Test: 0.4748\n",
            "Early stopping:  0.019669742027275065\n",
            "Epoch: 091, Loss: 1.4046, Train: 0.5547, Test: 0.4773\n",
            "Early stopping:  0.019589493646104197\n",
            "Epoch: 092, Loss: 1.3911, Train: 0.5570, Test: 0.4775\n",
            "Early stopping:  0.02030785334185358\n",
            "Epoch: 093, Loss: 1.3798, Train: 0.5621, Test: 0.4797\n",
            "Early stopping:  0.019729360555304647\n",
            "Epoch: 094, Loss: 1.3655, Train: 0.5649, Test: 0.4782\n",
            "Early stopping:  0.020507544655941738\n",
            "Epoch: 095, Loss: 1.3569, Train: 0.5666, Test: 0.4799\n",
            "Early stopping:  0.019155167029406322\n",
            "Epoch: 096, Loss: 1.3396, Train: 0.5701, Test: 0.4827\n",
            "Early stopping:  0.019965676960213067\n",
            "Epoch: 097, Loss: 1.3304, Train: 0.5683, Test: 0.4811\n",
            "Early stopping:  0.019801290754403823\n",
            "Epoch: 098, Loss: 1.3235, Train: 0.5763, Test: 0.4809\n",
            "Early stopping:  0.017683630242730394\n",
            "Epoch: 099, Loss: 1.3082, Train: 0.5859, Test: 0.4838\n",
            "Early stopping:  0.018170197840362976\n",
            "Epoch: 100, Loss: 1.3046, Train: 0.5774, Test: 0.4851\n",
            "Early stopping:  0.014798843548591964\n",
            "Epoch: 101, Loss: 1.2909, Train: 0.5854, Test: 0.4865\n",
            "Early stopping:  0.015678047608717202\n",
            "Epoch: 102, Loss: 1.2792, Train: 0.5888, Test: 0.4862\n",
            "Early stopping:  0.016915849161415704\n",
            "Epoch: 103, Loss: 1.2701, Train: 0.5933, Test: 0.4902\n",
            "Early stopping:  0.01623203900020305\n",
            "Epoch: 104, Loss: 1.2548, Train: 0.5944, Test: 0.4960\n",
            "Early stopping:  0.019083033563274947\n",
            "Epoch: 105, Loss: 1.2503, Train: 0.5978, Test: 0.4970\n",
            "Early stopping:  0.01685252801140989\n",
            "Epoch: 106, Loss: 1.2349, Train: 0.6012, Test: 0.4960\n",
            "Early stopping:  0.017303840604817378\n",
            "Epoch: 107, Loss: 1.2297, Train: 0.6018, Test: 0.4990\n",
            "Early stopping:  0.016176517746051177\n",
            "Epoch: 108, Loss: 1.2176, Train: 0.6007, Test: 0.5003\n",
            "Early stopping:  0.015250009983882575\n",
            "Epoch: 109, Loss: 1.2094, Train: 0.6081, Test: 0.5026\n",
            "Early stopping:  0.015855774822366484\n",
            "Epoch: 110, Loss: 1.2035, Train: 0.6109, Test: 0.5041\n",
            "Early stopping:  0.013246672514144023\n",
            "Epoch: 111, Loss: 1.1902, Train: 0.6103, Test: 0.5033\n",
            "Early stopping:  0.014817586583922944\n",
            "Epoch: 112, Loss: 1.1846, Train: 0.6177, Test: 0.5037\n",
            "Early stopping:  0.013578909175107332\n",
            "Epoch: 113, Loss: 1.1725, Train: 0.6205, Test: 0.5071\n",
            "Early stopping:  0.014764230413354475\n",
            "Epoch: 114, Loss: 1.1650, Train: 0.6194, Test: 0.5086\n",
            "Early stopping:  0.01508772241067547\n",
            "Epoch: 115, Loss: 1.1578, Train: 0.6285, Test: 0.5074\n",
            "Early stopping:  0.01342139071039608\n",
            "Epoch: 116, Loss: 1.1479, Train: 0.6251, Test: 0.5078\n",
            "Early stopping:  0.013977532905363391\n",
            "Epoch: 117, Loss: 1.1405, Train: 0.6296, Test: 0.5096\n",
            "Early stopping:  0.012854331705096323\n",
            "Epoch: 118, Loss: 1.1320, Train: 0.6228, Test: 0.5123\n",
            "Early stopping:  0.013167433805743704\n",
            "Epoch: 119, Loss: 1.1250, Train: 0.6336, Test: 0.5113\n",
            "Early stopping:  0.012892276325131924\n",
            "Epoch: 120, Loss: 1.1146, Train: 0.6381, Test: 0.5119\n",
            "Early stopping:  0.013019652849515827\n",
            "Epoch: 121, Loss: 1.1075, Train: 0.6353, Test: 0.5103\n",
            "Early stopping:  0.013215746598893683\n",
            "Epoch: 122, Loss: 1.1001, Train: 0.6438, Test: 0.5140\n",
            "Early stopping:  0.012905432340739025\n",
            "Epoch: 123, Loss: 1.0939, Train: 0.6427, Test: 0.5153\n",
            "Early stopping:  0.012186190802107255\n",
            "Epoch: 124, Loss: 1.0827, Train: 0.6381, Test: 0.5152\n",
            "Early stopping:  0.012287676533850643\n",
            "Epoch: 125, Loss: 1.0790, Train: 0.6489, Test: 0.5170\n",
            "Early stopping:  0.0118708213540015\n",
            "Epoch: 126, Loss: 1.0746, Train: 0.6449, Test: 0.5172\n",
            "Early stopping:  0.010620281891356111\n",
            "Epoch: 127, Loss: 1.0624, Train: 0.6387, Test: 0.5155\n",
            "Early stopping:  0.011523260463702058\n",
            "Epoch: 128, Loss: 1.0624, Train: 0.6506, Test: 0.5162\n",
            "Early stopping:  0.009417692287612653\n",
            "PREDICTIONS -> tensor([ 3,  6, 11,  ...,  5,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.46      0.53      0.49       758\n",
            "         capital_goods       0.46      0.31      0.37       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.39      0.44      0.41       793\n",
            " consumer_non-cyclical       0.54      0.43      0.47       446\n",
            "                energy       0.59      0.47      0.52       283\n",
            "             financial       0.60      0.55      0.58       767\n",
            "            healthcare       0.65      0.25      0.36       318\n",
            "              services       0.52      0.74      0.61      2076\n",
            "            technology       0.00      0.00      0.00       396\n",
            "        transportation       0.61      0.67      0.64       404\n",
            "             utilities       0.66      0.44      0.52       225\n",
            "\n",
            "              accuracy                           0.52      7054\n",
            "             macro avg       0.46      0.40      0.42      7054\n",
            "          weighted avg       0.49      0.52      0.49      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 86.9046, Train: 0.0556, Test: 0.0580\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 341.4275, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  179.97482783302488\n",
            "Epoch: 003, Loss: 324.2917, Train: 0.1083, Test: 0.1087\n",
            "Early stopping:  142.26038684333358\n",
            "Epoch: 004, Loss: 390.9323, Train: 0.0874, Test: 0.0822\n",
            "Early stopping:  135.63203273157208\n",
            "Epoch: 005, Loss: 421.8562, Train: 0.0652, Test: 0.0645\n",
            "Early stopping:  132.26659278512997\n",
            "Epoch: 006, Loss: 404.9690, Train: 0.1123, Test: 0.1103\n",
            "Early stopping:  41.927198232395334\n",
            "Epoch: 007, Loss: 385.2526, Train: 0.1055, Test: 0.1049\n",
            "Early stopping:  37.00283475451511\n",
            "Epoch: 008, Loss: 422.0914, Train: 0.0794, Test: 0.0808\n",
            "Early stopping:  17.05952221866755\n",
            "Epoch: 009, Loss: 360.8373, Train: 0.3137, Test: 0.3085\n",
            "Early stopping:  26.156417775502618\n",
            "Epoch: 010, Loss: 312.2841, Train: 0.3057, Test: 0.3061\n",
            "Early stopping:  42.816296323697216\n",
            "Epoch: 011, Loss: 320.5601, Train: 0.2933, Test: 0.2950\n",
            "Early stopping:  45.622996453027255\n",
            "Epoch: 012, Loss: 277.4991, Train: 0.1821, Test: 0.1805\n",
            "Early stopping:  55.26430003551457\n",
            "Epoch: 013, Loss: 241.6842, Train: 0.2161, Test: 0.2080\n",
            "Early stopping:  45.13511590147911\n",
            "Epoch: 014, Loss: 182.8525, Train: 0.2933, Test: 0.2916\n",
            "Early stopping:  56.46072803440298\n",
            "Epoch: 015, Loss: 144.6638, Train: 0.2881, Test: 0.2895\n",
            "Early stopping:  70.78028295651927\n",
            "Epoch: 016, Loss: 116.9722, Train: 0.3018, Test: 0.2920\n",
            "Early stopping:  66.57606279438104\n",
            "Epoch: 017, Loss: 92.1864, Train: 0.2354, Test: 0.2285\n",
            "Early stopping:  58.70722414154227\n",
            "Epoch: 018, Loss: 83.1304, Train: 0.2836, Test: 0.2823\n",
            "Early stopping:  40.689142631480166\n",
            "Epoch: 019, Loss: 56.0295, Train: 0.3108, Test: 0.3032\n",
            "Early stopping:  33.734630700731316\n",
            "Epoch: 020, Loss: 39.0688, Train: 0.2921, Test: 0.2855\n",
            "Early stopping:  30.591605086355006\n",
            "Epoch: 021, Loss: 25.0703, Train: 0.2961, Test: 0.2940\n",
            "Early stopping:  28.46432991392755\n",
            "Epoch: 022, Loss: 11.2459, Train: 0.2927, Test: 0.2848\n",
            "Early stopping:  27.95275709787083\n",
            "Epoch: 023, Loss: 4.5532, Train: 0.2819, Test: 0.2699\n",
            "Early stopping:  20.880333587620672\n",
            "Epoch: 024, Loss: 3.0464, Train: 0.2552, Test: 0.2509\n",
            "Early stopping:  15.281374921971933\n",
            "Epoch: 025, Loss: 2.7499, Train: 0.2490, Test: 0.2406\n",
            "Early stopping:  9.447556250298685\n",
            "Epoch: 026, Loss: 2.6331, Train: 0.2467, Test: 0.2370\n",
            "Early stopping:  3.659706741402609\n",
            "Epoch: 027, Loss: 2.5081, Train: 0.2530, Test: 0.2396\n",
            "Early stopping:  0.8374980227473453\n",
            "Epoch: 028, Loss: 2.3566, Train: 0.2655, Test: 0.2464\n",
            "Early stopping:  0.26136950398398573\n",
            "Epoch: 029, Loss: 2.2370, Train: 0.2700, Test: 0.2523\n",
            "Early stopping:  0.20610374835795614\n",
            "Epoch: 030, Loss: 2.1634, Train: 0.2689, Test: 0.2528\n",
            "Early stopping:  0.19259139560486802\n",
            "Epoch: 031, Loss: 2.1309, Train: 0.2638, Test: 0.2471\n",
            "Early stopping:  0.15454846093619776\n",
            "Epoch: 032, Loss: 2.1163, Train: 0.2587, Test: 0.2443\n",
            "Early stopping:  0.09874609657313412\n",
            "Epoch: 033, Loss: 2.1110, Train: 0.2677, Test: 0.2453\n",
            "Early stopping:  0.05185145088752398\n",
            "Epoch: 034, Loss: 2.1055, Train: 0.2723, Test: 0.2492\n",
            "Early stopping:  0.0232428234150831\n",
            "Epoch: 035, Loss: 2.0957, Train: 0.2808, Test: 0.2572\n",
            "Early stopping:  0.013088389087451918\n",
            "Epoch: 036, Loss: 2.0823, Train: 0.2893, Test: 0.2661\n",
            "Early stopping:  0.013489376182608113\n",
            "Epoch: 037, Loss: 2.0660, Train: 0.3029, Test: 0.2753\n",
            "Early stopping:  0.01821940404695898\n",
            "Epoch: 038, Loss: 2.0478, Train: 0.3154, Test: 0.2876\n",
            "Early stopping:  0.02310375478479497\n",
            "Epoch: 039, Loss: 2.0298, Train: 0.3301, Test: 0.3052\n",
            "Early stopping:  0.026335569738977382\n",
            "Epoch: 040, Loss: 2.0115, Train: 0.3534, Test: 0.3268\n",
            "Early stopping:  0.028103462547425805\n",
            "Epoch: 041, Loss: 1.9913, Train: 0.3710, Test: 0.3443\n",
            "Early stopping:  0.029365005202435533\n",
            "Epoch: 042, Loss: 1.9723, Train: 0.3800, Test: 0.3602\n",
            "Early stopping:  0.029965546923847574\n",
            "Epoch: 043, Loss: 1.9530, Train: 0.3976, Test: 0.3701\n",
            "Early stopping:  0.03050519160025413\n",
            "Epoch: 044, Loss: 1.9321, Train: 0.4056, Test: 0.3799\n",
            "Early stopping:  0.031194197549283924\n",
            "Epoch: 045, Loss: 1.9123, Train: 0.4124, Test: 0.3825\n",
            "Early stopping:  0.03133502738784009\n",
            "Epoch: 046, Loss: 1.8921, Train: 0.4124, Test: 0.3850\n",
            "Early stopping:  0.03178376836583577\n",
            "Epoch: 047, Loss: 1.8739, Train: 0.4135, Test: 0.3863\n",
            "Early stopping:  0.031322932855295846\n",
            "Epoch: 048, Loss: 1.8581, Train: 0.4152, Test: 0.3908\n",
            "Early stopping:  0.029488868754241237\n",
            "Epoch: 049, Loss: 1.8428, Train: 0.4175, Test: 0.3978\n",
            "Early stopping:  0.027418869387329466\n",
            "Epoch: 050, Loss: 1.8297, Train: 0.4186, Test: 0.4015\n",
            "Early stopping:  0.02468654786538585\n",
            "Epoch: 051, Loss: 1.8182, Train: 0.4214, Test: 0.4018\n",
            "Early stopping:  0.022179916451105026\n",
            "Epoch: 052, Loss: 1.8068, Train: 0.4220, Test: 0.4009\n",
            "Early stopping:  0.020177066027356597\n",
            "Epoch: 053, Loss: 1.7965, Train: 0.4237, Test: 0.4032\n",
            "Early stopping:  0.018262471821625367\n",
            "Epoch: 054, Loss: 1.7866, Train: 0.4305, Test: 0.4062\n",
            "Early stopping:  0.017076843164067164\n",
            "Epoch: 055, Loss: 1.7749, Train: 0.4282, Test: 0.4094\n",
            "Early stopping:  0.016863134293675173\n",
            "Epoch: 056, Loss: 1.7625, Train: 0.4334, Test: 0.4130\n",
            "Early stopping:  0.01742188987370752\n",
            "Epoch: 057, Loss: 1.7497, Train: 0.4356, Test: 0.4141\n",
            "Early stopping:  0.01865053419760451\n",
            "Epoch: 058, Loss: 1.7356, Train: 0.4379, Test: 0.4131\n",
            "Early stopping:  0.020117465336583852\n",
            "Epoch: 059, Loss: 1.7223, Train: 0.4424, Test: 0.4137\n",
            "Early stopping:  0.020913102185856312\n",
            "Epoch: 060, Loss: 1.7086, Train: 0.4436, Test: 0.4166\n",
            "Early stopping:  0.02137381768299424\n",
            "Epoch: 061, Loss: 1.6951, Train: 0.4470, Test: 0.4186\n",
            "Early stopping:  0.02152847053847977\n",
            "Epoch: 062, Loss: 1.6836, Train: 0.4481, Test: 0.4162\n",
            "Early stopping:  0.020779139890164208\n",
            "Epoch: 063, Loss: 1.6720, Train: 0.4504, Test: 0.4158\n",
            "Early stopping:  0.019875901388158236\n",
            "Epoch: 064, Loss: 1.6621, Train: 0.4532, Test: 0.4181\n",
            "Early stopping:  0.01838668019633867\n",
            "Epoch: 065, Loss: 1.6511, Train: 0.4487, Test: 0.4196\n",
            "Early stopping:  0.017294143700563196\n",
            "Epoch: 066, Loss: 1.6404, Train: 0.4526, Test: 0.4236\n",
            "Early stopping:  0.01694073116545395\n",
            "Epoch: 067, Loss: 1.6303, Train: 0.4589, Test: 0.4288\n",
            "Early stopping:  0.016621462024471585\n",
            "Epoch: 068, Loss: 1.6207, Train: 0.4691, Test: 0.4303\n",
            "Early stopping:  0.016410240222656598\n",
            "Epoch: 069, Loss: 1.6108, Train: 0.4702, Test: 0.4345\n",
            "Early stopping:  0.01589742724986788\n",
            "Epoch: 070, Loss: 1.6005, Train: 0.4725, Test: 0.4359\n",
            "Early stopping:  0.01570791423036867\n",
            "Epoch: 071, Loss: 1.5926, Train: 0.4799, Test: 0.4378\n",
            "Early stopping:  0.01512236156224501\n",
            "Epoch: 072, Loss: 1.5817, Train: 0.4867, Test: 0.4419\n",
            "Early stopping:  0.015198797583098481\n",
            "Epoch: 073, Loss: 1.5724, Train: 0.4912, Test: 0.4456\n",
            "Early stopping:  0.015110284096737125\n",
            "Epoch: 074, Loss: 1.5614, Train: 0.4935, Test: 0.4485\n",
            "Early stopping:  0.015595785813630038\n",
            "Epoch: 075, Loss: 1.5516, Train: 0.4901, Test: 0.4511\n",
            "Early stopping:  0.016179898879634804\n",
            "Epoch: 076, Loss: 1.5426, Train: 0.5003, Test: 0.4535\n",
            "Early stopping:  0.015675994583975103\n",
            "Epoch: 077, Loss: 1.5334, Train: 0.5031, Test: 0.4544\n",
            "Early stopping:  0.015307247312355627\n",
            "Epoch: 078, Loss: 1.5237, Train: 0.5009, Test: 0.4527\n",
            "Early stopping:  0.01478016957119531\n",
            "Epoch: 079, Loss: 1.5156, Train: 0.5094, Test: 0.4582\n",
            "Early stopping:  0.014371096893526423\n",
            "Epoch: 080, Loss: 1.5062, Train: 0.5122, Test: 0.4590\n",
            "Early stopping:  0.014342068159493952\n",
            "Epoch: 081, Loss: 1.4965, Train: 0.5111, Test: 0.4585\n",
            "Early stopping:  0.014476804887903781\n",
            "Epoch: 082, Loss: 1.4860, Train: 0.5190, Test: 0.4607\n",
            "Early stopping:  0.014961613938098734\n",
            "Epoch: 083, Loss: 1.4747, Train: 0.5230, Test: 0.4626\n",
            "Early stopping:  0.016131033415254473\n",
            "Epoch: 084, Loss: 1.4659, Train: 0.5264, Test: 0.4675\n",
            "Early stopping:  0.016178737257237674\n",
            "Epoch: 085, Loss: 1.4553, Train: 0.5286, Test: 0.4711\n",
            "Early stopping:  0.01620949285357522\n",
            "Epoch: 086, Loss: 1.4469, Train: 0.5326, Test: 0.4729\n",
            "Early stopping:  0.01545696507212017\n",
            "Epoch: 087, Loss: 1.4372, Train: 0.5355, Test: 0.4758\n",
            "Early stopping:  0.014865170696280355\n",
            "Epoch: 088, Loss: 1.4285, Train: 0.5366, Test: 0.4769\n",
            "Early stopping:  0.014700492983853179\n",
            "Epoch: 089, Loss: 1.4203, Train: 0.5394, Test: 0.4770\n",
            "Early stopping:  0.0139738746064758\n",
            "Epoch: 090, Loss: 1.4107, Train: 0.5394, Test: 0.4796\n",
            "Early stopping:  0.014143533032022242\n",
            "Epoch: 091, Loss: 1.4022, Train: 0.5400, Test: 0.4823\n",
            "Early stopping:  0.013908894574384989\n",
            "Epoch: 092, Loss: 1.3939, Train: 0.5474, Test: 0.4841\n",
            "Early stopping:  0.013806007589565529\n",
            "Epoch: 093, Loss: 1.3862, Train: 0.5485, Test: 0.4844\n",
            "Early stopping:  0.013456648349090156\n",
            "Epoch: 094, Loss: 1.3763, Train: 0.5479, Test: 0.4855\n",
            "Early stopping:  0.013410596036652036\n",
            "Epoch: 095, Loss: 1.3699, Train: 0.5536, Test: 0.4874\n",
            "Early stopping:  0.013006810037148904\n",
            "Epoch: 096, Loss: 1.3663, Train: 0.5564, Test: 0.4887\n",
            "Early stopping:  0.01144684459346872\n",
            "Epoch: 097, Loss: 1.3555, Train: 0.5542, Test: 0.4895\n",
            "Early stopping:  0.011421209534985745\n",
            "Epoch: 098, Loss: 1.3498, Train: 0.5610, Test: 0.4906\n",
            "Early stopping:  0.010766085239891544\n",
            "Epoch: 099, Loss: 1.3387, Train: 0.5661, Test: 0.4918\n",
            "Early stopping:  0.012617091393407904\n",
            "Epoch: 100, Loss: 1.3329, Train: 0.5649, Test: 0.4914\n",
            "Early stopping:  0.013284779387550587\n",
            "Epoch: 101, Loss: 1.3222, Train: 0.5638, Test: 0.4940\n",
            "Early stopping:  0.013266040895554072\n",
            "Epoch: 102, Loss: 1.3177, Train: 0.5752, Test: 0.4983\n",
            "Early stopping:  0.012879582064004509\n",
            "Epoch: 103, Loss: 1.3075, Train: 0.5769, Test: 0.5017\n",
            "Early stopping:  0.012367384214896785\n",
            "Epoch: 104, Loss: 1.3019, Train: 0.5757, Test: 0.5010\n",
            "Early stopping:  0.012240320208680629\n",
            "Epoch: 105, Loss: 1.2924, Train: 0.5723, Test: 0.5026\n",
            "Early stopping:  0.012017821914522534\n",
            "Epoch: 106, Loss: 1.2857, Train: 0.5786, Test: 0.5064\n",
            "Early stopping:  0.012546400582133998\n",
            "Epoch: 107, Loss: 1.2795, Train: 0.5735, Test: 0.5060\n",
            "Early stopping:  0.01143406727274794\n",
            "Epoch: 108, Loss: 1.2706, Train: 0.5706, Test: 0.5037\n",
            "Early stopping:  0.01194527558079422\n",
            "Epoch: 109, Loss: 1.2672, Train: 0.5803, Test: 0.5065\n",
            "Early stopping:  0.010424393285179004\n",
            "Epoch: 110, Loss: 1.2559, Train: 0.5831, Test: 0.5074\n",
            "Early stopping:  0.011500608740947245\n",
            "Epoch: 111, Loss: 1.2493, Train: 0.5797, Test: 0.5060\n",
            "Early stopping:  0.011994860645617528\n",
            "Epoch: 112, Loss: 1.2410, Train: 0.5854, Test: 0.5086\n",
            "Early stopping:  0.012289151055876576\n",
            "Epoch: 113, Loss: 1.2328, Train: 0.5905, Test: 0.5111\n",
            "Early stopping:  0.01324392797851773\n",
            "Epoch: 114, Loss: 1.2270, Train: 0.5927, Test: 0.5113\n",
            "Early stopping:  0.011769168411659191\n",
            "Epoch: 115, Loss: 1.2178, Train: 0.5933, Test: 0.5106\n",
            "Early stopping:  0.012205672198806139\n",
            "Epoch: 116, Loss: 1.2115, Train: 0.5956, Test: 0.5105\n",
            "Early stopping:  0.011740516222325971\n",
            "Epoch: 117, Loss: 1.2029, Train: 0.5967, Test: 0.5103\n",
            "Early stopping:  0.011954869073460942\n",
            "Epoch: 118, Loss: 1.1955, Train: 0.5978, Test: 0.5101\n",
            "Early stopping:  0.012333592890812267\n",
            "Epoch: 119, Loss: 1.1873, Train: 0.6035, Test: 0.5108\n",
            "Early stopping:  0.012201070294766865\n",
            "Epoch: 120, Loss: 1.1785, Train: 0.6092, Test: 0.5119\n",
            "Early stopping:  0.012907600709044252\n",
            "Epoch: 121, Loss: 1.1698, Train: 0.6115, Test: 0.5102\n",
            "Early stopping:  0.013131789779313547\n",
            "Epoch: 122, Loss: 1.1612, Train: 0.6109, Test: 0.5092\n",
            "Early stopping:  0.013603730454547797\n",
            "Epoch: 123, Loss: 1.1550, Train: 0.6200, Test: 0.5128\n",
            "Early stopping:  0.012963624002085325\n",
            "Epoch: 124, Loss: 1.1491, Train: 0.6262, Test: 0.5128\n",
            "Early stopping:  0.011700650619528368\n",
            "Epoch: 125, Loss: 1.1379, Train: 0.6256, Test: 0.5126\n",
            "Early stopping:  0.012089245041148037\n",
            "Epoch: 126, Loss: 1.1333, Train: 0.6313, Test: 0.5147\n",
            "Early stopping:  0.011613089558565715\n",
            "Epoch: 127, Loss: 1.1233, Train: 0.6358, Test: 0.5157\n",
            "Early stopping:  0.01261433580495244\n",
            "Epoch: 128, Loss: 1.1182, Train: 0.6358, Test: 0.5186\n",
            "Early stopping:  0.012177732324849925\n",
            "Epoch: 129, Loss: 1.1023, Train: 0.6296, Test: 0.5177\n",
            "Early stopping:  0.013944609160483644\n",
            "Epoch: 130, Loss: 1.1100, Train: 0.6478, Test: 0.5223\n",
            "Early stopping:  0.011918544173538728\n",
            "Epoch: 131, Loss: 1.0932, Train: 0.6506, Test: 0.5264\n",
            "Early stopping:  0.012075534367513945\n",
            "Epoch: 132, Loss: 1.1011, Train: 0.6557, Test: 0.5262\n",
            "Early stopping:  0.009514974805165017\n",
            "PREDICTIONS -> tensor([9, 0, 1,  ..., 5, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.51      0.50      0.51       758\n",
            "         capital_goods       0.36      0.31      0.33       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.46      0.42      0.44       793\n",
            " consumer_non-cyclical       0.72      0.39      0.51       446\n",
            "                energy       0.55      0.42      0.48       283\n",
            "             financial       0.63      0.54      0.58       767\n",
            "            healthcare       0.60      0.29      0.39       318\n",
            "              services       0.51      0.76      0.61      2076\n",
            "            technology       0.44      0.36      0.40       396\n",
            "        transportation       0.71      0.52      0.60       404\n",
            "             utilities       0.59      0.50      0.54       225\n",
            "\n",
            "              accuracy                           0.53      7054\n",
            "             macro avg       0.51      0.42      0.45      7054\n",
            "          weighted avg       0.53      0.53      0.51      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 49.2478, Train: 0.1140, Test: 0.1150\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 415.3795, Train: 0.2944, Test: 0.2952\n",
            "Early stopping:  258.8942581439155\n",
            "Epoch: 003, Loss: 373.3789, Train: 0.0839, Test: 0.0829\n",
            "Early stopping:  200.36530572506877\n",
            "Epoch: 004, Loss: 300.5334, Train: 0.0964, Test: 0.0930\n",
            "Early stopping:  163.94056525328367\n",
            "Epoch: 005, Loss: 306.6446, Train: 0.0613, Test: 0.0631\n",
            "Early stopping:  142.31748537100984\n",
            "Epoch: 006, Loss: 362.9936, Train: 0.1242, Test: 0.1284\n",
            "Early stopping:  48.21976981576436\n",
            "Epoch: 007, Loss: 338.4504, Train: 0.2694, Test: 0.2686\n",
            "Early stopping:  32.59856879766933\n",
            "Epoch: 008, Loss: 264.8783, Train: 0.2830, Test: 0.2882\n",
            "Early stopping:  37.563749360503834\n",
            "Epoch: 009, Loss: 256.5168, Train: 0.2711, Test: 0.2661\n",
            "Early stopping:  45.93811388737137\n",
            "Epoch: 010, Loss: 217.9408, Train: 0.2632, Test: 0.2591\n",
            "Early stopping:  60.42197827251344\n",
            "Epoch: 011, Loss: 164.4391, Train: 0.1940, Test: 0.1902\n",
            "Early stopping:  64.0774377653944\n",
            "Epoch: 012, Loss: 119.7805, Train: 0.2768, Test: 0.2682\n",
            "Early stopping:  61.87485421440577\n",
            "Epoch: 013, Loss: 69.2598, Train: 0.1486, Test: 0.1503\n",
            "Early stopping:  74.81250316740125\n",
            "Epoch: 014, Loss: 42.0161, Train: 0.1083, Test: 0.0974\n",
            "Early stopping:  71.04090065385957\n",
            "Epoch: 015, Loss: 21.2977, Train: 0.1645, Test: 0.1572\n",
            "Early stopping:  58.43831229354965\n",
            "Epoch: 016, Loss: 9.6716, Train: 0.2014, Test: 0.1922\n",
            "Early stopping:  43.946788125464764\n",
            "Epoch: 017, Loss: 6.1693, Train: 0.2082, Test: 0.2088\n",
            "Early stopping:  26.182492481163525\n",
            "Epoch: 018, Loss: 4.8828, Train: 0.1968, Test: 0.1985\n",
            "Early stopping:  15.507460184899776\n",
            "Epoch: 019, Loss: 3.9976, Train: 0.1917, Test: 0.1925\n",
            "Early stopping:  7.096923064528638\n",
            "Epoch: 020, Loss: 3.1871, Train: 0.1781, Test: 0.1779\n",
            "Early stopping:  2.5398825702738645\n",
            "Epoch: 021, Loss: 2.6497, Train: 0.1560, Test: 0.1524\n",
            "Early stopping:  1.3974925782094039\n",
            "Epoch: 022, Loss: 2.4174, Train: 0.1384, Test: 0.1347\n",
            "Early stopping:  1.0156231918800567\n",
            "Epoch: 023, Loss: 2.3332, Train: 0.1367, Test: 0.1350\n",
            "Early stopping:  0.689646314355929\n",
            "Epoch: 024, Loss: 2.3243, Train: 0.1418, Test: 0.1389\n",
            "Early stopping:  0.36270068478504464\n",
            "Epoch: 025, Loss: 2.3100, Train: 0.1531, Test: 0.1443\n",
            "Early stopping:  0.14205517170215828\n",
            "Epoch: 026, Loss: 2.3063, Train: 0.1696, Test: 0.1547\n",
            "Early stopping:  0.045539320565073184\n",
            "Epoch: 027, Loss: 2.2982, Train: 0.1781, Test: 0.1698\n",
            "Early stopping:  0.014139474615450918\n",
            "Epoch: 028, Loss: 2.2802, Train: 0.1963, Test: 0.1803\n",
            "Early stopping:  0.0162407563230901\n",
            "Epoch: 029, Loss: 2.2593, Train: 0.2133, Test: 0.1980\n",
            "Early stopping:  0.021048165894939085\n",
            "Epoch: 030, Loss: 2.2446, Train: 0.2433, Test: 0.2318\n",
            "Early stopping:  0.025885145682849237\n",
            "Epoch: 031, Loss: 2.2139, Train: 0.2734, Test: 0.2589\n",
            "Early stopping:  0.03247798116919968\n",
            "Epoch: 032, Loss: 2.1946, Train: 0.2910, Test: 0.2798\n",
            "Early stopping:  0.034410169813688725\n",
            "Epoch: 033, Loss: 2.1676, Train: 0.2950, Test: 0.2889\n",
            "Early stopping:  0.0370618383556878\n",
            "Epoch: 034, Loss: 2.1429, Train: 0.3046, Test: 0.2920\n",
            "Early stopping:  0.03956165581347753\n",
            "Epoch: 035, Loss: 2.1319, Train: 0.3086, Test: 0.3007\n",
            "Early stopping:  0.034371053825030404\n",
            "Epoch: 036, Loss: 2.1072, Train: 0.3210, Test: 0.3116\n",
            "Early stopping:  0.03354149681097519\n",
            "Epoch: 037, Loss: 2.0805, Train: 0.3239, Test: 0.3204\n",
            "Early stopping:  0.03346243502158492\n",
            "Epoch: 038, Loss: 2.0509, Train: 0.3267, Test: 0.3251\n",
            "Early stopping:  0.03764747598618714\n",
            "Epoch: 039, Loss: 2.0268, Train: 0.3347, Test: 0.3316\n",
            "Early stopping:  0.042168241056112166\n",
            "Epoch: 040, Loss: 2.0181, Train: 0.3381, Test: 0.3341\n",
            "Early stopping:  0.03717482175200713\n",
            "Epoch: 041, Loss: 2.0005, Train: 0.3307, Test: 0.3322\n",
            "Early stopping:  0.031052347500351164\n",
            "Epoch: 042, Loss: 1.9826, Train: 0.3301, Test: 0.3292\n",
            "Early stopping:  0.02594047618730629\n",
            "Epoch: 043, Loss: 1.9686, Train: 0.3279, Test: 0.3231\n",
            "Early stopping:  0.02415988076596741\n",
            "Epoch: 044, Loss: 1.9529, Train: 0.3296, Test: 0.3236\n",
            "Early stopping:  0.025698698013547495\n",
            "Epoch: 045, Loss: 1.9339, Train: 0.3324, Test: 0.3269\n",
            "Early stopping:  0.02578748586338725\n",
            "Epoch: 046, Loss: 1.9168, Train: 0.3483, Test: 0.3384\n",
            "Early stopping:  0.02634192576418832\n",
            "Epoch: 047, Loss: 1.8899, Train: 0.3500, Test: 0.3451\n",
            "Early stopping:  0.03075917635104017\n",
            "Epoch: 048, Loss: 1.9075, Train: 0.3681, Test: 0.3561\n",
            "Early stopping:  0.02424335876773313\n",
            "Epoch: 049, Loss: 1.8540, Train: 0.3596, Test: 0.3548\n",
            "Early stopping:  0.030442447644111123\n",
            "Epoch: 050, Loss: 1.8430, Train: 0.3693, Test: 0.3516\n",
            "Early stopping:  0.03251336260238264\n",
            "Epoch: 051, Loss: 1.8226, Train: 0.3647, Test: 0.3516\n",
            "Early stopping:  0.034692434769262336\n",
            "Epoch: 052, Loss: 1.8057, Train: 0.3704, Test: 0.3578\n",
            "Early stopping:  0.03879200379159363\n",
            "Epoch: 053, Loss: 1.7832, Train: 0.3823, Test: 0.3662\n",
            "Early stopping:  0.028420068012054184\n",
            "Epoch: 054, Loss: 1.7719, Train: 0.3800, Test: 0.3665\n",
            "Early stopping:  0.028822965984974653\n",
            "Epoch: 055, Loss: 1.7468, Train: 0.3868, Test: 0.3724\n",
            "Early stopping:  0.029481436079103494\n",
            "Epoch: 056, Loss: 1.7328, Train: 0.3925, Test: 0.3775\n",
            "Early stopping:  0.028993456440760446\n",
            "Epoch: 057, Loss: 1.7143, Train: 0.3942, Test: 0.3845\n",
            "Early stopping:  0.028115047458696526\n",
            "Epoch: 058, Loss: 1.6951, Train: 0.4084, Test: 0.3923\n",
            "Early stopping:  0.029497180209002348\n",
            "Epoch: 059, Loss: 1.6778, Train: 0.4124, Test: 0.3911\n",
            "Early stopping:  0.027807012093464818\n",
            "Epoch: 060, Loss: 1.6630, Train: 0.4243, Test: 0.4008\n",
            "Early stopping:  0.027866868108001992\n",
            "Epoch: 061, Loss: 1.6455, Train: 0.4305, Test: 0.4093\n",
            "Early stopping:  0.026863665559923453\n",
            "Epoch: 062, Loss: 1.6290, Train: 0.4362, Test: 0.4097\n",
            "Early stopping:  0.026020675825545726\n",
            "Epoch: 063, Loss: 1.6079, Train: 0.4351, Test: 0.4067\n",
            "Early stopping:  0.027537039446802886\n",
            "Epoch: 064, Loss: 1.5873, Train: 0.4402, Test: 0.4032\n",
            "Early stopping:  0.029934069789034092\n",
            "Epoch: 065, Loss: 1.5709, Train: 0.4407, Test: 0.4063\n",
            "Early stopping:  0.030208370365335514\n",
            "Epoch: 066, Loss: 1.5523, Train: 0.4572, Test: 0.4185\n",
            "Early stopping:  0.030130436455729964\n",
            "Epoch: 067, Loss: 1.5309, Train: 0.4691, Test: 0.4305\n",
            "Early stopping:  0.029916976153102886\n",
            "Epoch: 068, Loss: 1.5150, Train: 0.4816, Test: 0.4341\n",
            "Early stopping:  0.029239950460808385\n",
            "Epoch: 069, Loss: 1.4981, Train: 0.4878, Test: 0.4385\n",
            "Early stopping:  0.02896524701756582\n",
            "Epoch: 070, Loss: 1.4822, Train: 0.4969, Test: 0.4457\n",
            "Early stopping:  0.027383696844312366\n",
            "Epoch: 071, Loss: 1.4648, Train: 0.5014, Test: 0.4437\n",
            "Early stopping:  0.026062353074237748\n",
            "Epoch: 072, Loss: 1.4478, Train: 0.5088, Test: 0.4514\n",
            "Early stopping:  0.026508614746577994\n",
            "Epoch: 073, Loss: 1.4269, Train: 0.5218, Test: 0.4542\n",
            "Early stopping:  0.02799486154358758\n",
            "Epoch: 074, Loss: 1.4081, Train: 0.5303, Test: 0.4636\n",
            "Early stopping:  0.02945563082280077\n",
            "Epoch: 075, Loss: 1.3866, Train: 0.5440, Test: 0.4698\n",
            "Early stopping:  0.031032597858417728\n",
            "Epoch: 076, Loss: 1.3674, Train: 0.5615, Test: 0.4789\n",
            "Early stopping:  0.03180000831032779\n",
            "Epoch: 077, Loss: 1.3428, Train: 0.5701, Test: 0.4827\n",
            "Early stopping:  0.03305718378434958\n",
            "Epoch: 078, Loss: 1.3227, Train: 0.5774, Test: 0.4901\n",
            "Early stopping:  0.033968751829703575\n",
            "Epoch: 079, Loss: 1.3037, Train: 0.5769, Test: 0.4909\n",
            "Early stopping:  0.03332543326339561\n",
            "Epoch: 080, Loss: 1.2909, Train: 0.5876, Test: 0.4983\n",
            "Early stopping:  0.03056987104190277\n",
            "Epoch: 081, Loss: 1.2677, Train: 0.5933, Test: 0.4979\n",
            "Early stopping:  0.02886518844901303\n",
            "Epoch: 082, Loss: 1.2484, Train: 0.5973, Test: 0.5027\n",
            "Early stopping:  0.029267202603043524\n",
            "Epoch: 083, Loss: 1.2344, Train: 0.6069, Test: 0.5094\n",
            "Early stopping:  0.028737792204275265\n",
            "Epoch: 084, Loss: 1.2198, Train: 0.6137, Test: 0.5055\n",
            "Early stopping:  0.02790089087418215\n",
            "Epoch: 085, Loss: 1.2063, Train: 0.6103, Test: 0.5120\n",
            "Early stopping:  0.023981072264211454\n",
            "Epoch: 086, Loss: 1.1867, Train: 0.6171, Test: 0.5129\n",
            "Early stopping:  0.024015697436517314\n",
            "Epoch: 087, Loss: 1.1721, Train: 0.6234, Test: 0.5139\n",
            "Early stopping:  0.025007598531292422\n",
            "Epoch: 088, Loss: 1.1595, Train: 0.6279, Test: 0.5186\n",
            "Early stopping:  0.024558315137467687\n",
            "Epoch: 089, Loss: 1.1496, Train: 0.6262, Test: 0.5106\n",
            "Early stopping:  0.02244250622119595\n",
            "Epoch: 090, Loss: 1.1375, Train: 0.6347, Test: 0.5201\n",
            "Early stopping:  0.01917515178503502\n",
            "Epoch: 091, Loss: 1.1166, Train: 0.6404, Test: 0.5149\n",
            "Early stopping:  0.021275158783188537\n",
            "Epoch: 092, Loss: 1.1040, Train: 0.6455, Test: 0.5198\n",
            "Early stopping:  0.022951633690463148\n",
            "Epoch: 093, Loss: 1.0858, Train: 0.6478, Test: 0.5232\n",
            "Early stopping:  0.02552671067511922\n",
            "Epoch: 094, Loss: 1.0776, Train: 0.6500, Test: 0.5190\n",
            "Early stopping:  0.02401448557267149\n",
            "Epoch: 095, Loss: 1.0668, Train: 0.6500, Test: 0.5232\n",
            "Early stopping:  0.020085393759521526\n",
            "Epoch: 096, Loss: 1.0508, Train: 0.6534, Test: 0.5251\n",
            "Early stopping:  0.020008950040307557\n",
            "Epoch: 097, Loss: 1.0432, Train: 0.6585, Test: 0.5234\n",
            "Early stopping:  0.01783309078194365\n",
            "Epoch: 098, Loss: 1.0313, Train: 0.6642, Test: 0.5274\n",
            "Early stopping:  0.018447177121782427\n",
            "Epoch: 099, Loss: 1.0182, Train: 0.6676, Test: 0.5261\n",
            "Early stopping:  0.018538849195721276\n",
            "Epoch: 100, Loss: 1.0122, Train: 0.6716, Test: 0.5310\n",
            "Early stopping:  0.016270759631572825\n",
            "Epoch: 101, Loss: 0.9979, Train: 0.6807, Test: 0.5299\n",
            "Early stopping:  0.017441366987823448\n",
            "Epoch: 102, Loss: 0.9882, Train: 0.6858, Test: 0.5327\n",
            "Early stopping:  0.016912752089687264\n",
            "Epoch: 103, Loss: 0.9809, Train: 0.6892, Test: 0.5361\n",
            "Early stopping:  0.015676540897133655\n",
            "Epoch: 104, Loss: 0.9676, Train: 0.6863, Test: 0.5308\n",
            "Early stopping:  0.016877960586909036\n",
            "Epoch: 105, Loss: 0.9616, Train: 0.6948, Test: 0.5359\n",
            "Early stopping:  0.01482486728164219\n",
            "Epoch: 106, Loss: 0.9483, Train: 0.7045, Test: 0.5315\n",
            "Early stopping:  0.015771978905203723\n",
            "Epoch: 107, Loss: 0.9459, Train: 0.6982, Test: 0.5394\n",
            "Early stopping:  0.014395796301784789\n",
            "Epoch: 108, Loss: 0.9416, Train: 0.7050, Test: 0.5333\n",
            "Early stopping:  0.011062706871291536\n",
            "Epoch: 109, Loss: 0.9264, Train: 0.7062, Test: 0.5350\n",
            "Early stopping:  0.012690878639414887\n",
            "Epoch: 110, Loss: 0.9228, Train: 0.7141, Test: 0.5379\n",
            "Early stopping:  0.011655456597283585\n",
            "Epoch: 111, Loss: 0.9085, Train: 0.7136, Test: 0.5397\n",
            "Early stopping:  0.015073593083069204\n",
            "Epoch: 112, Loss: 0.9046, Train: 0.7175, Test: 0.5422\n",
            "Early stopping:  0.014837939858039162\n",
            "Epoch: 113, Loss: 0.8904, Train: 0.7238, Test: 0.5364\n",
            "Early stopping:  0.014541707299424762\n",
            "Epoch: 114, Loss: 0.8895, Train: 0.7204, Test: 0.5438\n",
            "Early stopping:  0.013821598264954852\n",
            "Epoch: 115, Loss: 0.8769, Train: 0.7204, Test: 0.5427\n",
            "Early stopping:  0.01274374382710304\n",
            "Epoch: 116, Loss: 0.8756, Train: 0.7272, Test: 0.5447\n",
            "Early stopping:  0.011833332920571309\n",
            "Epoch: 117, Loss: 0.8622, Train: 0.7170, Test: 0.5400\n",
            "Early stopping:  0.011593330020516185\n",
            "Epoch: 118, Loss: 0.8680, Train: 0.7266, Test: 0.5400\n",
            "Early stopping:  0.01029307773259067\n",
            "Epoch: 119, Loss: 0.8459, Train: 0.7277, Test: 0.5431\n",
            "Early stopping:  0.012547311636170198\n",
            "Epoch: 120, Loss: 0.8510, Train: 0.7334, Test: 0.5427\n",
            "Early stopping:  0.012137851731272634\n",
            "Epoch: 121, Loss: 0.8374, Train: 0.7311, Test: 0.5422\n",
            "Early stopping:  0.012316834716035539\n",
            "Epoch: 122, Loss: 0.8347, Train: 0.7323, Test: 0.5444\n",
            "Early stopping:  0.013215736342517265\n",
            "Epoch: 123, Loss: 0.8286, Train: 0.7396, Test: 0.5420\n",
            "Early stopping:  0.008909567164133\n",
            "PREDICTIONS -> tensor([ 9,  8,  5,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.57      0.52       758\n",
            "         capital_goods       0.42      0.33      0.37       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.45      0.53      0.49       793\n",
            " consumer_non-cyclical       0.60      0.43      0.50       446\n",
            "                energy       0.46      0.45      0.46       283\n",
            "             financial       0.60      0.53      0.56       767\n",
            "            healthcare       0.84      0.42      0.56       318\n",
            "              services       0.57      0.71      0.63      2076\n",
            "            technology       0.43      0.29      0.35       396\n",
            "        transportation       0.70      0.61      0.65       404\n",
            "             utilities       0.58      0.45      0.51       225\n",
            "\n",
            "              accuracy                           0.54      7054\n",
            "             macro avg       0.51      0.44      0.47      7054\n",
            "          weighted avg       0.54      0.54      0.53      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 62.1969, Train: 0.2944, Test: 0.2963\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 249.8075, Train: 0.1066, Test: 0.1093\n",
            "Early stopping:  132.6607055623393\n",
            "Epoch: 003, Loss: 484.2702, Train: 0.0573, Test: 0.0539\n",
            "Early stopping:  211.46959496584978\n",
            "Epoch: 004, Loss: 606.5807, Train: 0.0403, Test: 0.0396\n",
            "Early stopping:  242.71332707403397\n",
            "Epoch: 005, Loss: 473.2188, Train: 0.0811, Test: 0.0764\n",
            "Early stopping:  217.21835620327803\n",
            "Epoch: 006, Loss: 295.1411, Train: 0.0987, Test: 0.0958\n",
            "Early stopping:  146.89644276574788\n",
            "Epoch: 007, Loss: 317.1659, Train: 0.1531, Test: 0.1486\n",
            "Early stopping:  129.202569977916\n",
            "Epoch: 008, Loss: 243.3797, Train: 0.1117, Test: 0.0977\n",
            "Early stopping:  149.6587836995345\n",
            "Epoch: 009, Loss: 199.1652, Train: 0.3188, Test: 0.3147\n",
            "Early stopping:  104.32996857021092\n",
            "Epoch: 010, Loss: 134.2852, Train: 0.3120, Test: 0.3102\n",
            "Early stopping:  73.86816937635652\n",
            "Epoch: 011, Loss: 139.8613, Train: 0.3074, Test: 0.3039\n",
            "Early stopping:  76.34815851469303\n",
            "Epoch: 012, Loss: 111.0250, Train: 0.2995, Test: 0.2983\n",
            "Early stopping:  54.308881661171945\n",
            "Epoch: 013, Loss: 69.1264, Train: 0.1645, Test: 0.1579\n",
            "Early stopping:  47.3356998963251\n",
            "Epoch: 014, Loss: 46.3589, Train: 0.1344, Test: 0.1296\n",
            "Early stopping:  40.97685677678696\n",
            "Epoch: 015, Loss: 37.5596, Train: 0.1151, Test: 0.1126\n",
            "Early stopping:  43.57173269138311\n",
            "Epoch: 016, Loss: 26.3037, Train: 0.1265, Test: 0.1150\n",
            "Early stopping:  33.5164154669203\n",
            "Epoch: 017, Loss: 16.0554, Train: 0.1254, Test: 0.1073\n",
            "Early stopping:  20.319002921583493\n",
            "Epoch: 018, Loss: 8.4921, Train: 0.1367, Test: 0.1148\n",
            "Early stopping:  15.403498630533493\n",
            "Epoch: 019, Loss: 4.3632, Train: 0.2184, Test: 0.2043\n",
            "Early stopping:  13.510494213143748\n",
            "Epoch: 020, Loss: 2.7023, Train: 0.2405, Test: 0.2292\n",
            "Early stopping:  9.711414840280945\n",
            "Epoch: 021, Loss: 2.7373, Train: 0.2343, Test: 0.2308\n",
            "Early stopping:  5.6510205677149505\n",
            "Epoch: 022, Loss: 2.4483, Train: 0.2382, Test: 0.2316\n",
            "Early stopping:  2.5439368731380636\n",
            "Epoch: 023, Loss: 2.3132, Train: 0.2411, Test: 0.2203\n",
            "Early stopping:  0.8297556249763013\n",
            "Epoch: 024, Loss: 2.2771, Train: 0.2269, Test: 0.2105\n",
            "Early stopping:  0.21473208393691046\n",
            "Epoch: 025, Loss: 2.2704, Train: 0.2167, Test: 0.1997\n",
            "Early stopping:  0.19693196873424\n",
            "Epoch: 026, Loss: 2.2791, Train: 0.2053, Test: 0.1965\n",
            "Early stopping:  0.07490944625182513\n",
            "Epoch: 027, Loss: 2.2918, Train: 0.2036, Test: 0.1928\n",
            "Early stopping:  0.016886222446502296\n",
            "Epoch: 028, Loss: 2.2936, Train: 0.2008, Test: 0.1931\n",
            "Early stopping:  0.00995763346428509\n",
            "PREDICTIONS -> tensor([ 1,  6, 10,  ..., 10,  7, 10], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.27      0.03      0.06       758\n",
            "         capital_goods       0.17      0.05      0.07       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.33      0.26      0.29       793\n",
            " consumer_non-cyclical       0.67      0.11      0.19       446\n",
            "                energy       0.00      0.00      0.00       283\n",
            "             financial       0.21      0.44      0.28       767\n",
            "            healthcare       0.18      0.59      0.28       318\n",
            "              services       0.65      0.12      0.20      2076\n",
            "            technology       0.00      0.00      0.00       396\n",
            "        transportation       0.09      0.70      0.16       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.19      7054\n",
            "             macro avg       0.21      0.19      0.13      7054\n",
            "          weighted avg       0.35      0.19      0.17      7054\n",
            "\n",
            "time: 2min 49s (started: 2024-10-16 21:12:53 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft__SQKegSl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b78c9f-ae91-414e-965c-3f68f1682d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 461 ms (started: 2024-10-16 21:15:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApKs3OENgSl9"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Daj92_RRgSl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6de4930-aad3-4338-9ff1-7d5757a63a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5265, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2934, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  0.16486346709654756\n",
            "Epoch: 003, Loss: 2.1768, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  0.1780637922302343\n",
            "Epoch: 004, Loss: 2.1381, Train: 0.3040, Test: 0.3018\n",
            "Early stopping:  0.174826645004053\n",
            "Epoch: 005, Loss: 2.0749, Train: 0.3647, Test: 0.3517\n",
            "Early stopping:  0.17788485092203946\n",
            "Epoch: 006, Loss: 2.0104, Train: 0.3766, Test: 0.3640\n",
            "Early stopping:  0.10713488602355686\n",
            "Epoch: 007, Loss: 1.9462, Train: 0.3755, Test: 0.3597\n",
            "Early stopping:  0.09346602699669965\n",
            "Epoch: 008, Loss: 1.8919, Train: 0.3693, Test: 0.3589\n",
            "Early stopping:  0.09824546245335916\n",
            "Epoch: 009, Loss: 1.8446, Train: 0.3857, Test: 0.3706\n",
            "Early stopping:  0.09176138103861087\n",
            "Epoch: 010, Loss: 1.7883, Train: 0.4226, Test: 0.4016\n",
            "Early stopping:  0.08640051866912205\n",
            "Epoch: 011, Loss: 1.7222, Train: 0.4668, Test: 0.4369\n",
            "Early stopping:  0.08735051334781835\n",
            "Epoch: 012, Loss: 1.6600, Train: 0.4901, Test: 0.4570\n",
            "Early stopping:  0.0928447784358154\n",
            "Epoch: 013, Loss: 1.6043, Train: 0.5003, Test: 0.4636\n",
            "Early stopping:  0.09631553576805278\n",
            "Epoch: 014, Loss: 1.5468, Train: 0.5071, Test: 0.4724\n",
            "Early stopping:  0.09507838628207134\n",
            "Epoch: 015, Loss: 1.4873, Train: 0.5201, Test: 0.4860\n",
            "Early stopping:  0.09222652794498462\n",
            "Epoch: 016, Loss: 1.4296, Train: 0.5479, Test: 0.5094\n",
            "Early stopping:  0.0913802697228124\n",
            "Epoch: 017, Loss: 1.3728, Train: 0.5820, Test: 0.5337\n",
            "Early stopping:  0.09172880826028781\n",
            "Epoch: 018, Loss: 1.3209, Train: 0.6024, Test: 0.5498\n",
            "Early stopping:  0.08954411887721343\n",
            "Epoch: 019, Loss: 1.2739, Train: 0.6211, Test: 0.5637\n",
            "Early stopping:  0.08473285075083341\n",
            "Epoch: 020, Loss: 1.2254, Train: 0.6302, Test: 0.5697\n",
            "Early stopping:  0.08027538540184098\n",
            "Epoch: 021, Loss: 1.1793, Train: 0.6370, Test: 0.5754\n",
            "Early stopping:  0.0763193543031864\n",
            "Epoch: 022, Loss: 1.1395, Train: 0.6489, Test: 0.5819\n",
            "Early stopping:  0.07237699471110946\n",
            "Epoch: 023, Loss: 1.1014, Train: 0.6619, Test: 0.5893\n",
            "Early stopping:  0.06824232691077249\n",
            "Epoch: 024, Loss: 1.0648, Train: 0.6784, Test: 0.5992\n",
            "Early stopping:  0.06319042257319182\n",
            "Epoch: 025, Loss: 1.0292, Train: 0.6892, Test: 0.6048\n",
            "Early stopping:  0.05928469590285882\n",
            "Epoch: 026, Loss: 0.9963, Train: 0.6937, Test: 0.6100\n",
            "Early stopping:  0.05669043675504259\n",
            "Epoch: 027, Loss: 0.9659, Train: 0.7062, Test: 0.6148\n",
            "Early stopping:  0.05368147150568089\n",
            "Epoch: 028, Loss: 0.9341, Train: 0.7147, Test: 0.6164\n",
            "Early stopping:  0.05134569522324638\n",
            "Epoch: 029, Loss: 0.9036, Train: 0.7255, Test: 0.6198\n",
            "Early stopping:  0.04956176373234872\n",
            "Epoch: 030, Loss: 0.8751, Train: 0.7385, Test: 0.6235\n",
            "Early stopping:  0.04819846622443608\n",
            "Epoch: 031, Loss: 0.8476, Train: 0.7527, Test: 0.6266\n",
            "Early stopping:  0.04677074315058661\n",
            "Epoch: 032, Loss: 0.8197, Train: 0.7646, Test: 0.6301\n",
            "Early stopping:  0.04505062411116376\n",
            "Epoch: 033, Loss: 0.7931, Train: 0.7731, Test: 0.6325\n",
            "Early stopping:  0.04372601525708365\n",
            "Epoch: 034, Loss: 0.7670, Train: 0.7748, Test: 0.6377\n",
            "Early stopping:  0.042831335684058294\n",
            "Epoch: 035, Loss: 0.7419, Train: 0.7811, Test: 0.6378\n",
            "Early stopping:  0.041780919390170854\n",
            "Epoch: 036, Loss: 0.7172, Train: 0.7981, Test: 0.6440\n",
            "Early stopping:  0.0404975662108003\n",
            "Epoch: 037, Loss: 0.6934, Train: 0.8071, Test: 0.6460\n",
            "Early stopping:  0.03939176813695304\n",
            "Epoch: 038, Loss: 0.6702, Train: 0.8123, Test: 0.6490\n",
            "Early stopping:  0.03828303097547423\n",
            "Epoch: 039, Loss: 0.6479, Train: 0.8247, Test: 0.6497\n",
            "Early stopping:  0.03716690536202763\n",
            "Epoch: 040, Loss: 0.6255, Train: 0.8355, Test: 0.6517\n",
            "Early stopping:  0.03619187842394985\n",
            "Epoch: 041, Loss: 0.6037, Train: 0.8423, Test: 0.6542\n",
            "Early stopping:  0.03541883565901893\n",
            "Epoch: 042, Loss: 0.5825, Train: 0.8503, Test: 0.6551\n",
            "Early stopping:  0.03473087477010104\n",
            "Epoch: 043, Loss: 0.5613, Train: 0.8605, Test: 0.6557\n",
            "Early stopping:  0.03420736020235651\n",
            "Epoch: 044, Loss: 0.5404, Train: 0.8616, Test: 0.6575\n",
            "Early stopping:  0.03362663705872182\n",
            "Epoch: 045, Loss: 0.5199, Train: 0.8707, Test: 0.6615\n",
            "Early stopping:  0.03315629127412504\n",
            "Epoch: 046, Loss: 0.4994, Train: 0.8752, Test: 0.6623\n",
            "Early stopping:  0.03282138506686996\n",
            "Epoch: 047, Loss: 0.4795, Train: 0.8888, Test: 0.6612\n",
            "Early stopping:  0.032347860781698407\n",
            "Epoch: 048, Loss: 0.4599, Train: 0.9002, Test: 0.6620\n",
            "Early stopping:  0.03185707779264139\n",
            "Epoch: 049, Loss: 0.4406, Train: 0.9115, Test: 0.6626\n",
            "Early stopping:  0.031336019452127536\n",
            "Epoch: 050, Loss: 0.4217, Train: 0.9178, Test: 0.6629\n",
            "Early stopping:  0.03070928232031237\n",
            "Epoch: 051, Loss: 0.4031, Train: 0.9251, Test: 0.6643\n",
            "Early stopping:  0.030195483196357918\n",
            "Epoch: 052, Loss: 0.3847, Train: 0.9314, Test: 0.6642\n",
            "Early stopping:  0.029694150255833947\n",
            "Epoch: 053, Loss: 0.3667, Train: 0.9382, Test: 0.6647\n",
            "Early stopping:  0.029205251506854525\n",
            "Epoch: 054, Loss: 0.3491, Train: 0.9484, Test: 0.6629\n",
            "Early stopping:  0.028710144156557635\n",
            "Epoch: 055, Loss: 0.3320, Train: 0.9546, Test: 0.6627\n",
            "Early stopping:  0.028113366881254726\n",
            "Epoch: 056, Loss: 0.3154, Train: 0.9575, Test: 0.6627\n",
            "Early stopping:  0.027426914746014042\n",
            "Epoch: 057, Loss: 0.2992, Train: 0.9620, Test: 0.6635\n",
            "Early stopping:  0.026689613796023214\n",
            "Epoch: 058, Loss: 0.2838, Train: 0.9654, Test: 0.6620\n",
            "Early stopping:  0.02584466651247097\n",
            "Epoch: 059, Loss: 0.2693, Train: 0.9671, Test: 0.6625\n",
            "Early stopping:  0.024809028586639203\n",
            "Epoch: 060, Loss: 0.2574, Train: 0.9739, Test: 0.6569\n",
            "Early stopping:  0.023105613140488042\n",
            "Epoch: 061, Loss: 0.2497, Train: 0.9688, Test: 0.6613\n",
            "Early stopping:  0.019999164713691134\n",
            "Epoch: 062, Loss: 0.2421, Train: 0.9847, Test: 0.6578\n",
            "Early stopping:  0.016478762183269983\n",
            "Epoch: 063, Loss: 0.2245, Train: 0.9858, Test: 0.6644\n",
            "Early stopping:  0.01680075286938096\n",
            "Epoch: 064, Loss: 0.2081, Train: 0.9813, Test: 0.6630\n",
            "Early stopping:  0.01992757312541268\n",
            "Epoch: 065, Loss: 0.2064, Train: 0.9915, Test: 0.6582\n",
            "Early stopping:  0.01952423180504541\n",
            "Epoch: 066, Loss: 0.1953, Train: 0.9926, Test: 0.6627\n",
            "Early stopping:  0.018254137505221358\n",
            "Epoch: 067, Loss: 0.1814, Train: 0.9887, Test: 0.6625\n",
            "Early stopping:  0.016037536623740096\n",
            "Epoch: 068, Loss: 0.1808, Train: 0.9955, Test: 0.6585\n",
            "Early stopping:  0.013118145692129294\n",
            "Epoch: 069, Loss: 0.1702, Train: 0.9960, Test: 0.6601\n",
            "Early stopping:  0.014132845147344331\n",
            "Epoch: 070, Loss: 0.1609, Train: 0.9938, Test: 0.6615\n",
            "Early stopping:  0.012963319710241745\n",
            "Epoch: 071, Loss: 0.1599, Train: 0.9972, Test: 0.6586\n",
            "Early stopping:  0.01035551006627528\n",
            "Epoch: 072, Loss: 0.1505, Train: 0.9983, Test: 0.6574\n",
            "Early stopping:  0.011491500722861099\n",
            "Epoch: 073, Loss: 0.1449, Train: 0.9960, Test: 0.6576\n",
            "Early stopping:  0.009809382120354572\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.64      0.66       758\n",
            "         capital_goods       0.56      0.53      0.55       508\n",
            "conglomerates_industry       0.86      0.23      0.36        80\n",
            "     consumer_cyclical       0.57      0.60      0.58       793\n",
            " consumer_non-cyclical       0.71      0.55      0.62       446\n",
            "                energy       0.79      0.68      0.73       283\n",
            "             financial       0.71      0.66      0.69       767\n",
            "            healthcare       0.78      0.65      0.71       318\n",
            "              services       0.63      0.77      0.69      2076\n",
            "            technology       0.55      0.46      0.50       396\n",
            "        transportation       0.82      0.71      0.76       404\n",
            "             utilities       0.80      0.74      0.77       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.70      0.60      0.63      7054\n",
            "          weighted avg       0.66      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4899, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2617, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16138989242967372\n",
            "Epoch: 003, Loss: 2.1808, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16031586371117282\n",
            "Epoch: 004, Loss: 2.1400, Train: 0.2955, Test: 0.2956\n",
            "Early stopping:  0.15628532643082446\n",
            "Epoch: 005, Loss: 2.0757, Train: 0.3188, Test: 0.3124\n",
            "Early stopping:  0.1603897118510999\n",
            "Epoch: 006, Loss: 2.0152, Train: 0.3556, Test: 0.3466\n",
            "Early stopping:  0.09494164050895267\n",
            "Epoch: 007, Loss: 1.9557, Train: 0.3795, Test: 0.3670\n",
            "Early stopping:  0.0911393928599425\n",
            "Epoch: 008, Loss: 1.8924, Train: 0.3902, Test: 0.3747\n",
            "Early stopping:  0.09729719506688612\n",
            "Epoch: 009, Loss: 1.8276, Train: 0.4016, Test: 0.3840\n",
            "Early stopping:  0.09789018344146286\n",
            "Epoch: 010, Loss: 1.7656, Train: 0.4294, Test: 0.4053\n",
            "Early stopping:  0.09917082521280277\n",
            "Epoch: 011, Loss: 1.7003, Train: 0.4697, Test: 0.4430\n",
            "Early stopping:  0.10079610269157187\n",
            "Epoch: 012, Loss: 1.6329, Train: 0.4940, Test: 0.4619\n",
            "Early stopping:  0.1021677476186537\n",
            "Epoch: 013, Loss: 1.5737, Train: 0.5162, Test: 0.4783\n",
            "Early stopping:  0.10129070561344643\n",
            "Epoch: 014, Loss: 1.5144, Train: 0.5383, Test: 0.5000\n",
            "Early stopping:  0.09950789794741904\n",
            "Epoch: 015, Loss: 1.4493, Train: 0.5536, Test: 0.5123\n",
            "Early stopping:  0.09815152898100159\n",
            "Epoch: 016, Loss: 1.3912, Train: 0.5644, Test: 0.5291\n",
            "Early stopping:  0.09611631344278751\n",
            "Epoch: 017, Loss: 1.3397, Train: 0.5888, Test: 0.5458\n",
            "Early stopping:  0.09355155525291456\n",
            "Epoch: 018, Loss: 1.2856, Train: 0.6166, Test: 0.5590\n",
            "Early stopping:  0.08977905797564929\n",
            "Epoch: 019, Loss: 1.2346, Train: 0.6262, Test: 0.5697\n",
            "Early stopping:  0.08459598789918235\n",
            "Epoch: 020, Loss: 1.1876, Train: 0.6370, Test: 0.5758\n",
            "Early stopping:  0.08101561883991214\n",
            "Epoch: 021, Loss: 1.1426, Train: 0.6415, Test: 0.5824\n",
            "Early stopping:  0.07788156577273435\n",
            "Epoch: 022, Loss: 1.1028, Train: 0.6551, Test: 0.5899\n",
            "Early stopping:  0.07244398080779076\n",
            "Epoch: 023, Loss: 1.0632, Train: 0.6716, Test: 0.5990\n",
            "Early stopping:  0.06766976243036313\n",
            "Epoch: 024, Loss: 1.0252, Train: 0.6801, Test: 0.5981\n",
            "Early stopping:  0.06394775391439796\n",
            "Epoch: 025, Loss: 0.9916, Train: 0.6920, Test: 0.6048\n",
            "Early stopping:  0.06005772089168949\n",
            "Epoch: 026, Loss: 0.9581, Train: 0.7028, Test: 0.6111\n",
            "Early stopping:  0.05715647583180754\n",
            "Epoch: 027, Loss: 0.9262, Train: 0.7187, Test: 0.6165\n",
            "Early stopping:  0.0539840026176317\n",
            "Epoch: 028, Loss: 0.8936, Train: 0.7294, Test: 0.6182\n",
            "Early stopping:  0.05195700914669001\n",
            "Epoch: 029, Loss: 0.8641, Train: 0.7396, Test: 0.6239\n",
            "Early stopping:  0.05050078786425057\n",
            "Epoch: 030, Loss: 0.8348, Train: 0.7442, Test: 0.6294\n",
            "Early stopping:  0.048793355841487265\n",
            "Epoch: 031, Loss: 0.8051, Train: 0.7578, Test: 0.6324\n",
            "Early stopping:  0.04757809423120146\n",
            "Epoch: 032, Loss: 0.7781, Train: 0.7720, Test: 0.6348\n",
            "Early stopping:  0.0458679141688161\n",
            "Epoch: 033, Loss: 0.7507, Train: 0.7839, Test: 0.6379\n",
            "Early stopping:  0.04485180034220808\n",
            "Epoch: 034, Loss: 0.7240, Train: 0.7913, Test: 0.6409\n",
            "Early stopping:  0.04366004119886871\n",
            "Epoch: 035, Loss: 0.6982, Train: 0.7986, Test: 0.6429\n",
            "Early stopping:  0.042365260132225385\n",
            "Epoch: 036, Loss: 0.6733, Train: 0.8151, Test: 0.6449\n",
            "Early stopping:  0.041437259808656285\n",
            "Epoch: 037, Loss: 0.6481, Train: 0.8230, Test: 0.6486\n",
            "Early stopping:  0.040453840299532536\n",
            "Epoch: 038, Loss: 0.6240, Train: 0.8281, Test: 0.6523\n",
            "Early stopping:  0.03953165498323714\n",
            "Epoch: 039, Loss: 0.6008, Train: 0.8412, Test: 0.6514\n",
            "Early stopping:  0.038595720453629884\n",
            "Epoch: 040, Loss: 0.5774, Train: 0.8537, Test: 0.6532\n",
            "Early stopping:  0.03781015668648641\n",
            "Epoch: 041, Loss: 0.5554, Train: 0.8593, Test: 0.6558\n",
            "Early stopping:  0.036693664688556844\n",
            "Epoch: 042, Loss: 0.5333, Train: 0.8673, Test: 0.6547\n",
            "Early stopping:  0.035869615169411034\n",
            "Epoch: 043, Loss: 0.5118, Train: 0.8815, Test: 0.6581\n",
            "Early stopping:  0.03512306758962463\n",
            "Epoch: 044, Loss: 0.4908, Train: 0.8888, Test: 0.6579\n",
            "Early stopping:  0.03428050714215885\n",
            "Epoch: 045, Loss: 0.4699, Train: 0.8911, Test: 0.6602\n",
            "Early stopping:  0.033766015700567524\n",
            "Epoch: 046, Loss: 0.4497, Train: 0.9053, Test: 0.6596\n",
            "Early stopping:  0.03305762698553451\n",
            "Epoch: 047, Loss: 0.4300, Train: 0.9132, Test: 0.6596\n",
            "Early stopping:  0.03237459453004193\n",
            "Epoch: 048, Loss: 0.4102, Train: 0.9195, Test: 0.6623\n",
            "Early stopping:  0.03181025218773336\n",
            "Epoch: 049, Loss: 0.3913, Train: 0.9274, Test: 0.6625\n",
            "Early stopping:  0.031110259475152818\n",
            "Epoch: 050, Loss: 0.3728, Train: 0.9319, Test: 0.6612\n",
            "Early stopping:  0.030436276393457164\n",
            "Epoch: 051, Loss: 0.3550, Train: 0.9427, Test: 0.6629\n",
            "Early stopping:  0.029606855719794635\n",
            "Epoch: 052, Loss: 0.3378, Train: 0.9512, Test: 0.6632\n",
            "Early stopping:  0.028640138041782316\n",
            "Epoch: 053, Loss: 0.3211, Train: 0.9575, Test: 0.6633\n",
            "Early stopping:  0.027744245917734056\n",
            "Epoch: 054, Loss: 0.3051, Train: 0.9620, Test: 0.6642\n",
            "Early stopping:  0.026792477822810256\n",
            "Epoch: 055, Loss: 0.2898, Train: 0.9643, Test: 0.6670\n",
            "Early stopping:  0.025803643675888274\n",
            "Epoch: 056, Loss: 0.2752, Train: 0.9722, Test: 0.6647\n",
            "Early stopping:  0.024732119657724552\n",
            "Epoch: 057, Loss: 0.2617, Train: 0.9739, Test: 0.6653\n",
            "Early stopping:  0.02353322516801033\n",
            "Epoch: 058, Loss: 0.2506, Train: 0.9773, Test: 0.6605\n",
            "Early stopping:  0.02171971681611411\n",
            "Epoch: 059, Loss: 0.2446, Train: 0.9671, Test: 0.6610\n",
            "Early stopping:  0.018384857336667285\n",
            "Epoch: 060, Loss: 0.2451, Train: 0.9818, Test: 0.6562\n",
            "Early stopping:  0.013030379674354265\n",
            "Epoch: 061, Loss: 0.2302, Train: 0.9836, Test: 0.6629\n",
            "Early stopping:  0.011386015999763795\n",
            "Epoch: 062, Loss: 0.2066, Train: 0.9796, Test: 0.6642\n",
            "Early stopping:  0.017775532316885237\n",
            "Epoch: 063, Loss: 0.2089, Train: 0.9898, Test: 0.6589\n",
            "Early stopping:  0.018631554650187888\n",
            "Epoch: 064, Loss: 0.1978, Train: 0.9921, Test: 0.6595\n",
            "Early stopping:  0.019377914828654984\n",
            "Epoch: 065, Loss: 0.1850, Train: 0.9853, Test: 0.6647\n",
            "Early stopping:  0.016608096994693445\n",
            "Epoch: 066, Loss: 0.1864, Train: 0.9943, Test: 0.6619\n",
            "Early stopping:  0.011082140909721455\n",
            "Epoch: 067, Loss: 0.1723, Train: 0.9943, Test: 0.6571\n",
            "Early stopping:  0.013858246206072839\n",
            "Epoch: 068, Loss: 0.1709, Train: 0.9892, Test: 0.6627\n",
            "Early stopping:  0.011101613779722708\n",
            "Epoch: 069, Loss: 0.1657, Train: 0.9943, Test: 0.6618\n",
            "Early stopping:  0.009164547872824185\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.67      0.68       758\n",
            "         capital_goods       0.54      0.53      0.53       508\n",
            "conglomerates_industry       0.82      0.17      0.29        80\n",
            "     consumer_cyclical       0.58      0.61      0.59       793\n",
            " consumer_non-cyclical       0.70      0.54      0.61       446\n",
            "                energy       0.78      0.67      0.72       283\n",
            "             financial       0.73      0.68      0.70       767\n",
            "            healthcare       0.72      0.72      0.72       318\n",
            "              services       0.65      0.75      0.70      2076\n",
            "            technology       0.56      0.48      0.52       396\n",
            "        transportation       0.80      0.76      0.78       404\n",
            "             utilities       0.76      0.68      0.72       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.69      0.60      0.63      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4745, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2477, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16035780040333897\n",
            "Epoch: 003, Loss: 2.1676, Train: 0.3001, Test: 0.2969\n",
            "Early stopping:  0.1591764098341141\n",
            "Epoch: 004, Loss: 2.0757, Train: 0.3590, Test: 0.3496\n",
            "Early stopping:  0.17057936043164512\n",
            "Epoch: 005, Loss: 2.0258, Train: 0.3897, Test: 0.3791\n",
            "Early stopping:  0.1764035205119161\n",
            "Epoch: 006, Loss: 1.9700, Train: 0.3863, Test: 0.3764\n",
            "Early stopping:  0.11108432957057036\n",
            "Epoch: 007, Loss: 1.8953, Train: 0.3783, Test: 0.3628\n",
            "Early stopping:  0.10336686508684531\n",
            "Epoch: 008, Loss: 1.8247, Train: 0.3817, Test: 0.3673\n",
            "Early stopping:  0.10034364150847247\n",
            "Epoch: 009, Loss: 1.7609, Train: 0.4005, Test: 0.3897\n",
            "Early stopping:  0.10684479923731764\n",
            "Epoch: 010, Loss: 1.6886, Train: 0.4470, Test: 0.4257\n",
            "Early stopping:  0.11028510065656359\n",
            "Epoch: 011, Loss: 1.6147, Train: 0.4889, Test: 0.4578\n",
            "Early stopping:  0.11028308454182273\n",
            "Epoch: 012, Loss: 1.5519, Train: 0.5167, Test: 0.4827\n",
            "Early stopping:  0.10943459177857078\n",
            "Epoch: 013, Loss: 1.4898, Train: 0.5394, Test: 0.5020\n",
            "Early stopping:  0.10741252943072505\n",
            "Epoch: 014, Loss: 1.4245, Train: 0.5564, Test: 0.5174\n",
            "Early stopping:  0.10332170451372173\n",
            "Epoch: 015, Loss: 1.3651, Train: 0.5695, Test: 0.5330\n",
            "Early stopping:  0.09910591020313368\n",
            "Epoch: 016, Loss: 1.3121, Train: 0.5990, Test: 0.5500\n",
            "Early stopping:  0.0956128194315398\n",
            "Epoch: 017, Loss: 1.2607, Train: 0.6200, Test: 0.5675\n",
            "Early stopping:  0.09034497061008631\n",
            "Epoch: 018, Loss: 1.2096, Train: 0.6268, Test: 0.5734\n",
            "Early stopping:  0.08449209797122113\n",
            "Epoch: 019, Loss: 1.1623, Train: 0.6398, Test: 0.5826\n",
            "Early stopping:  0.08036467668629857\n",
            "Epoch: 020, Loss: 1.1201, Train: 0.6517, Test: 0.5885\n",
            "Early stopping:  0.07635832847692578\n",
            "Epoch: 021, Loss: 1.0784, Train: 0.6597, Test: 0.5947\n",
            "Early stopping:  0.07186852089318947\n",
            "Epoch: 022, Loss: 1.0413, Train: 0.6733, Test: 0.5974\n",
            "Early stopping:  0.06652284701498606\n",
            "Epoch: 023, Loss: 1.0072, Train: 0.6858, Test: 0.6094\n",
            "Early stopping:  0.06154751976640083\n",
            "Epoch: 024, Loss: 0.9721, Train: 0.6999, Test: 0.6155\n",
            "Early stopping:  0.058088568645409386\n",
            "Epoch: 025, Loss: 0.9397, Train: 0.7164, Test: 0.6212\n",
            "Early stopping:  0.05483350039247264\n",
            "Epoch: 026, Loss: 0.9075, Train: 0.7232, Test: 0.6232\n",
            "Early stopping:  0.05300941999162734\n",
            "Epoch: 027, Loss: 0.8750, Train: 0.7345, Test: 0.6289\n",
            "Early stopping:  0.05203067073161691\n",
            "Epoch: 028, Loss: 0.8440, Train: 0.7493, Test: 0.6324\n",
            "Early stopping:  0.05075730975230621\n",
            "Epoch: 029, Loss: 0.8143, Train: 0.7544, Test: 0.6348\n",
            "Early stopping:  0.04969204572665621\n",
            "Epoch: 030, Loss: 0.7859, Train: 0.7691, Test: 0.6411\n",
            "Early stopping:  0.0480756423057092\n",
            "Epoch: 031, Loss: 0.7586, Train: 0.7822, Test: 0.6452\n",
            "Early stopping:  0.04600670976752314\n",
            "Epoch: 032, Loss: 0.7309, Train: 0.7884, Test: 0.6483\n",
            "Early stopping:  0.044579644717413866\n",
            "Epoch: 033, Loss: 0.7050, Train: 0.8037, Test: 0.6511\n",
            "Early stopping:  0.043277312853206885\n",
            "Epoch: 034, Loss: 0.6790, Train: 0.8145, Test: 0.6523\n",
            "Early stopping:  0.042289456629164016\n",
            "Epoch: 035, Loss: 0.6529, Train: 0.8242, Test: 0.6527\n",
            "Early stopping:  0.041624273012583764\n",
            "Epoch: 036, Loss: 0.6294, Train: 0.8293, Test: 0.6534\n",
            "Early stopping:  0.04032060592412483\n",
            "Epoch: 037, Loss: 0.6063, Train: 0.8338, Test: 0.6540\n",
            "Early stopping:  0.039042765955900775\n",
            "Epoch: 038, Loss: 0.5839, Train: 0.8457, Test: 0.6566\n",
            "Early stopping:  0.03744623092054367\n",
            "Epoch: 039, Loss: 0.5622, Train: 0.8565, Test: 0.6579\n",
            "Early stopping:  0.03590580984124252\n",
            "Epoch: 040, Loss: 0.5403, Train: 0.8661, Test: 0.6586\n",
            "Early stopping:  0.03517100389518279\n",
            "Epoch: 041, Loss: 0.5184, Train: 0.8763, Test: 0.6633\n",
            "Early stopping:  0.03469756132247202\n",
            "Epoch: 042, Loss: 0.4974, Train: 0.8883, Test: 0.6642\n",
            "Early stopping:  0.034254910522653066\n",
            "Epoch: 043, Loss: 0.4769, Train: 0.8996, Test: 0.6661\n",
            "Early stopping:  0.033727743987818616\n",
            "Epoch: 044, Loss: 0.4564, Train: 0.9098, Test: 0.6661\n",
            "Early stopping:  0.03307939556035247\n",
            "Epoch: 045, Loss: 0.4367, Train: 0.9195, Test: 0.6683\n",
            "Early stopping:  0.03233806121647752\n",
            "Epoch: 046, Loss: 0.4167, Train: 0.9234, Test: 0.6674\n",
            "Early stopping:  0.031891806500145004\n",
            "Epoch: 047, Loss: 0.3975, Train: 0.9319, Test: 0.6688\n",
            "Early stopping:  0.03141640087967659\n",
            "Epoch: 048, Loss: 0.3788, Train: 0.9342, Test: 0.6690\n",
            "Early stopping:  0.030751525969084377\n",
            "Epoch: 049, Loss: 0.3619, Train: 0.9433, Test: 0.6674\n",
            "Early stopping:  0.029669466321955683\n",
            "Epoch: 050, Loss: 0.3495, Train: 0.9206, Test: 0.6630\n",
            "Early stopping:  0.026975241702504746\n",
            "Epoch: 051, Loss: 0.3469, Train: 0.9490, Test: 0.6540\n",
            "Early stopping:  0.021240029113500494\n",
            "Epoch: 052, Loss: 0.3400, Train: 0.9546, Test: 0.6676\n",
            "Early stopping:  0.015281423643983494\n",
            "Epoch: 053, Loss: 0.3022, Train: 0.9512, Test: 0.6656\n",
            "Early stopping:  0.022612228052793885\n",
            "Epoch: 054, Loss: 0.2990, Train: 0.9609, Test: 0.6602\n",
            "Early stopping:  0.02483205808648895\n",
            "Epoch: 055, Loss: 0.2913, Train: 0.9682, Test: 0.6701\n",
            "Early stopping:  0.025587628487905422\n",
            "Epoch: 056, Loss: 0.2642, Train: 0.9586, Test: 0.6640\n",
            "Early stopping:  0.02722373005631998\n",
            "Epoch: 057, Loss: 0.2698, Train: 0.9750, Test: 0.6674\n",
            "Early stopping:  0.017292138338433266\n",
            "Epoch: 058, Loss: 0.2460, Train: 0.9779, Test: 0.6659\n",
            "Early stopping:  0.02135589146932864\n",
            "Epoch: 059, Loss: 0.2420, Train: 0.9750, Test: 0.6644\n",
            "Early stopping:  0.019860273579721616\n",
            "Epoch: 060, Loss: 0.2313, Train: 0.9796, Test: 0.6660\n",
            "Early stopping:  0.01598332526553753\n",
            "Epoch: 061, Loss: 0.2183, Train: 0.9836, Test: 0.6659\n",
            "Early stopping:  0.01915499185907735\n",
            "Epoch: 062, Loss: 0.2163, Train: 0.9858, Test: 0.6686\n",
            "Early stopping:  0.013466360611836873\n",
            "Epoch: 063, Loss: 0.2007, Train: 0.9813, Test: 0.6632\n",
            "Early stopping:  0.01569265440295668\n",
            "Epoch: 064, Loss: 0.2017, Train: 0.9887, Test: 0.6688\n",
            "Early stopping:  0.012728762552429869\n",
            "Epoch: 065, Loss: 0.1880, Train: 0.9898, Test: 0.6674\n",
            "Early stopping:  0.012470135628317624\n",
            "Epoch: 066, Loss: 0.1856, Train: 0.9921, Test: 0.6656\n",
            "Early stopping:  0.01232918039833199\n",
            "Epoch: 067, Loss: 0.1781, Train: 0.9932, Test: 0.6680\n",
            "Early stopping:  0.010183623846642197\n",
            "Epoch: 068, Loss: 0.1710, Train: 0.9909, Test: 0.6663\n",
            "Early stopping:  0.011526504702457733\n",
            "Epoch: 069, Loss: 0.1691, Train: 0.9943, Test: 0.6667\n",
            "Early stopping:  0.008445418081893098\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.69      0.66       758\n",
            "         capital_goods       0.58      0.55      0.56       508\n",
            "conglomerates_industry       0.90      0.24      0.38        80\n",
            "     consumer_cyclical       0.61      0.63      0.62       793\n",
            " consumer_non-cyclical       0.71      0.56      0.63       446\n",
            "                energy       0.77      0.69      0.73       283\n",
            "             financial       0.70      0.69      0.70       767\n",
            "            healthcare       0.77      0.72      0.74       318\n",
            "              services       0.66      0.73      0.70      2076\n",
            "            technology       0.59      0.50      0.54       396\n",
            "        transportation       0.77      0.76      0.77       404\n",
            "             utilities       0.74      0.66      0.70       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.70      0.62      0.64      7054\n",
            "          weighted avg       0.67      0.67      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4646, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2564, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14718100968489306\n",
            "Epoch: 003, Loss: 2.1577, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15666176261773185\n",
            "Epoch: 004, Loss: 2.1354, Train: 0.2984, Test: 0.2964\n",
            "Early stopping:  0.15021188277058975\n",
            "Epoch: 005, Loss: 2.0695, Train: 0.3284, Test: 0.3221\n",
            "Early stopping:  0.1539429934545146\n",
            "Epoch: 006, Loss: 2.0011, Train: 0.3568, Test: 0.3524\n",
            "Early stopping:  0.09600593072825893\n",
            "Epoch: 007, Loss: 1.9446, Train: 0.3732, Test: 0.3676\n",
            "Early stopping:  0.08956135986531184\n",
            "Epoch: 008, Loss: 1.8886, Train: 0.3863, Test: 0.3769\n",
            "Early stopping:  0.09787826727916585\n",
            "Epoch: 009, Loss: 1.8299, Train: 0.4016, Test: 0.3860\n",
            "Early stopping:  0.09361692123454828\n",
            "Epoch: 010, Loss: 1.7699, Train: 0.4209, Test: 0.4046\n",
            "Early stopping:  0.09128402190817102\n",
            "Epoch: 011, Loss: 1.7081, Train: 0.4458, Test: 0.4232\n",
            "Early stopping:  0.09356901979860928\n",
            "Epoch: 012, Loss: 1.6463, Train: 0.4725, Test: 0.4426\n",
            "Early stopping:  0.09589301684502391\n",
            "Epoch: 013, Loss: 1.5859, Train: 0.4901, Test: 0.4585\n",
            "Early stopping:  0.0966864269162901\n",
            "Epoch: 014, Loss: 1.5259, Train: 0.5139, Test: 0.4763\n",
            "Early stopping:  0.09646197946645897\n",
            "Epoch: 015, Loss: 1.4664, Train: 0.5360, Test: 0.4932\n",
            "Early stopping:  0.0954632489137642\n",
            "Epoch: 016, Loss: 1.4083, Train: 0.5610, Test: 0.5122\n",
            "Early stopping:  0.09416132569362079\n",
            "Epoch: 017, Loss: 1.3518, Train: 0.5961, Test: 0.5343\n",
            "Early stopping:  0.09263585365998608\n",
            "Epoch: 018, Loss: 1.2979, Train: 0.6109, Test: 0.5544\n",
            "Early stopping:  0.09023960580272843\n",
            "Epoch: 019, Loss: 1.2485, Train: 0.6268, Test: 0.5685\n",
            "Early stopping:  0.08642007539253325\n",
            "Epoch: 020, Loss: 1.2015, Train: 0.6341, Test: 0.5808\n",
            "Early stopping:  0.08180479757536946\n",
            "Epoch: 021, Loss: 1.1557, Train: 0.6444, Test: 0.5893\n",
            "Early stopping:  0.07731775011087239\n",
            "Epoch: 022, Loss: 1.1130, Train: 0.6563, Test: 0.5987\n",
            "Early stopping:  0.07318070131189605\n",
            "Epoch: 023, Loss: 1.0746, Train: 0.6687, Test: 0.6048\n",
            "Early stopping:  0.06902885940115804\n",
            "Epoch: 024, Loss: 1.0384, Train: 0.6773, Test: 0.6087\n",
            "Early stopping:  0.0644738435180608\n",
            "Epoch: 025, Loss: 1.0028, Train: 0.6892, Test: 0.6109\n",
            "Early stopping:  0.06018037277064725\n",
            "Epoch: 026, Loss: 0.9696, Train: 0.7016, Test: 0.6116\n",
            "Early stopping:  0.05669412041890405\n",
            "Epoch: 027, Loss: 0.9388, Train: 0.7090, Test: 0.6172\n",
            "Early stopping:  0.05385251934474485\n",
            "Epoch: 028, Loss: 0.9074, Train: 0.7221, Test: 0.6219\n",
            "Early stopping:  0.05157793989498341\n",
            "Epoch: 029, Loss: 0.8765, Train: 0.7368, Test: 0.6236\n",
            "Early stopping:  0.04980923047797416\n",
            "Epoch: 030, Loss: 0.8468, Train: 0.7470, Test: 0.6256\n",
            "Early stopping:  0.04870482369695835\n",
            "Epoch: 031, Loss: 0.8171, Train: 0.7601, Test: 0.6307\n",
            "Early stopping:  0.0480643433492302\n",
            "Epoch: 032, Loss: 0.7889, Train: 0.7686, Test: 0.6352\n",
            "Early stopping:  0.046871527886221336\n",
            "Epoch: 033, Loss: 0.7614, Train: 0.7845, Test: 0.6395\n",
            "Early stopping:  0.04553500031381437\n",
            "Epoch: 034, Loss: 0.7344, Train: 0.8026, Test: 0.6422\n",
            "Early stopping:  0.044332360665484835\n",
            "Epoch: 035, Loss: 0.7086, Train: 0.8037, Test: 0.6456\n",
            "Early stopping:  0.04290379433699682\n",
            "Epoch: 036, Loss: 0.6832, Train: 0.8179, Test: 0.6463\n",
            "Early stopping:  0.04175472922170961\n",
            "Epoch: 037, Loss: 0.6582, Train: 0.8270, Test: 0.6486\n",
            "Early stopping:  0.040738983098804686\n",
            "Epoch: 038, Loss: 0.6333, Train: 0.8321, Test: 0.6506\n",
            "Early stopping:  0.03995557807612815\n",
            "Epoch: 039, Loss: 0.6091, Train: 0.8434, Test: 0.6513\n",
            "Early stopping:  0.039367299929214165\n",
            "Epoch: 040, Loss: 0.5855, Train: 0.8548, Test: 0.6518\n",
            "Early stopping:  0.03866823316790519\n",
            "Epoch: 041, Loss: 0.5625, Train: 0.8639, Test: 0.6531\n",
            "Early stopping:  0.03782182165070357\n",
            "Epoch: 042, Loss: 0.5396, Train: 0.8695, Test: 0.6535\n",
            "Early stopping:  0.03700213642084575\n",
            "Epoch: 043, Loss: 0.5174, Train: 0.8798, Test: 0.6540\n",
            "Early stopping:  0.03625716508403422\n",
            "Epoch: 044, Loss: 0.4952, Train: 0.8917, Test: 0.6549\n",
            "Early stopping:  0.03568554857886566\n",
            "Epoch: 045, Loss: 0.4733, Train: 0.9013, Test: 0.6578\n",
            "Early stopping:  0.03523958101707556\n",
            "Epoch: 046, Loss: 0.4515, Train: 0.9081, Test: 0.6578\n",
            "Early stopping:  0.034842452609609074\n",
            "Epoch: 047, Loss: 0.4303, Train: 0.9161, Test: 0.6609\n",
            "Early stopping:  0.034466001787882644\n",
            "Epoch: 048, Loss: 0.4094, Train: 0.9251, Test: 0.6601\n",
            "Early stopping:  0.03392362285391717\n",
            "Epoch: 049, Loss: 0.3892, Train: 0.9246, Test: 0.6663\n",
            "Early stopping:  0.03324843327836168\n",
            "Epoch: 050, Loss: 0.3714, Train: 0.9365, Test: 0.6514\n",
            "Early stopping:  0.03184272081072053\n",
            "Epoch: 051, Loss: 0.3672, Train: 0.8945, Test: 0.6542\n",
            "Early stopping:  0.026495181210603107\n",
            "Epoch: 052, Loss: 0.3912, Train: 0.9461, Test: 0.6511\n",
            "Early stopping:  0.016975333114553\n",
            "Epoch: 053, Loss: 0.3405, Train: 0.9507, Test: 0.6521\n",
            "Early stopping:  0.02050748802183076\n",
            "Epoch: 054, Loss: 0.3275, Train: 0.9331, Test: 0.6660\n",
            "Early stopping:  0.0254459583005824\n",
            "Epoch: 055, Loss: 0.3205, Train: 0.9444, Test: 0.6663\n",
            "Early stopping:  0.029389860031427704\n",
            "Epoch: 056, Loss: 0.2963, Train: 0.9614, Test: 0.6520\n",
            "Early stopping:  0.035185156369989444\n",
            "Epoch: 057, Loss: 0.2907, Train: 0.9694, Test: 0.6551\n",
            "Early stopping:  0.021069433596802686\n",
            "Epoch: 058, Loss: 0.2737, Train: 0.9586, Test: 0.6684\n",
            "Early stopping:  0.02209609191878718\n",
            "Epoch: 059, Loss: 0.2650, Train: 0.9637, Test: 0.6667\n",
            "Early stopping:  0.02154458023265143\n",
            "Epoch: 060, Loss: 0.2559, Train: 0.9796, Test: 0.6595\n",
            "Early stopping:  0.01700670649537189\n",
            "Epoch: 061, Loss: 0.2412, Train: 0.9813, Test: 0.6575\n",
            "Early stopping:  0.018609114541115853\n",
            "Epoch: 062, Loss: 0.2395, Train: 0.9762, Test: 0.6673\n",
            "Early stopping:  0.014866017680293086\n",
            "Epoch: 063, Loss: 0.2228, Train: 0.9762, Test: 0.6663\n",
            "Early stopping:  0.01627621430705005\n",
            "Epoch: 064, Loss: 0.2234, Train: 0.9870, Test: 0.6627\n",
            "Early stopping:  0.01384794304026162\n",
            "Epoch: 065, Loss: 0.2068, Train: 0.9887, Test: 0.6569\n",
            "Early stopping:  0.014107209748400637\n",
            "Epoch: 066, Loss: 0.2084, Train: 0.9898, Test: 0.6639\n",
            "Early stopping:  0.013290712063445337\n",
            "Epoch: 067, Loss: 0.1935, Train: 0.9858, Test: 0.6670\n",
            "Early stopping:  0.012493203178259308\n",
            "Epoch: 068, Loss: 0.1944, Train: 0.9909, Test: 0.6629\n",
            "Early stopping:  0.012246865913321935\n",
            "Epoch: 069, Loss: 0.1818, Train: 0.9938, Test: 0.6555\n",
            "Early stopping:  0.01090459144744623\n",
            "Epoch: 070, Loss: 0.1816, Train: 0.9955, Test: 0.6612\n",
            "Early stopping:  0.011045313308370695\n",
            "Epoch: 071, Loss: 0.1716, Train: 0.9932, Test: 0.6632\n",
            "Early stopping:  0.00949840020944986\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.66      0.66       758\n",
            "         capital_goods       0.56      0.52      0.54       508\n",
            "conglomerates_industry       0.94      0.20      0.33        80\n",
            "     consumer_cyclical       0.58      0.61      0.60       793\n",
            " consumer_non-cyclical       0.72      0.54      0.62       446\n",
            "                energy       0.79      0.67      0.72       283\n",
            "             financial       0.74      0.66      0.70       767\n",
            "            healthcare       0.72      0.70      0.71       318\n",
            "              services       0.63      0.78      0.70      2076\n",
            "            technology       0.61      0.49      0.55       396\n",
            "        transportation       0.81      0.73      0.76       404\n",
            "             utilities       0.76      0.68      0.72       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.60      0.63      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4836, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2705, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15067245461553552\n",
            "Epoch: 003, Loss: 2.1662, Train: 0.2950, Test: 0.2952\n",
            "Early stopping:  0.16180329780670807\n",
            "Epoch: 004, Loss: 2.1202, Train: 0.3165, Test: 0.3139\n",
            "Early stopping:  0.16174073098834035\n",
            "Epoch: 005, Loss: 2.0600, Train: 0.3778, Test: 0.3667\n",
            "Early stopping:  0.1662327660751678\n",
            "Epoch: 006, Loss: 2.0028, Train: 0.3971, Test: 0.3811\n",
            "Early stopping:  0.1025496691374368\n",
            "Epoch: 007, Loss: 1.9342, Train: 0.3857, Test: 0.3748\n",
            "Early stopping:  0.09211847623757877\n",
            "Epoch: 008, Loss: 1.8649, Train: 0.3812, Test: 0.3709\n",
            "Early stopping:  0.10070781095817523\n",
            "Epoch: 009, Loss: 1.8052, Train: 0.4050, Test: 0.3900\n",
            "Early stopping:  0.10242235651818818\n",
            "Epoch: 010, Loss: 1.7441, Train: 0.4447, Test: 0.4237\n",
            "Early stopping:  0.10225608332960132\n",
            "Epoch: 011, Loss: 1.6768, Train: 0.4850, Test: 0.4541\n",
            "Early stopping:  0.10051321156284625\n",
            "Epoch: 012, Loss: 1.6123, Train: 0.5071, Test: 0.4733\n",
            "Early stopping:  0.10019423740418562\n",
            "Epoch: 013, Loss: 1.5469, Train: 0.5230, Test: 0.4877\n",
            "Early stopping:  0.10252006725438592\n",
            "Epoch: 014, Loss: 1.4791, Train: 0.5417, Test: 0.5037\n",
            "Early stopping:  0.1043469901873845\n",
            "Epoch: 015, Loss: 1.4177, Train: 0.5735, Test: 0.5244\n",
            "Early stopping:  0.10301655344352749\n",
            "Epoch: 016, Loss: 1.3580, Train: 0.5933, Test: 0.5455\n",
            "Early stopping:  0.10087657010576512\n",
            "Epoch: 017, Loss: 1.2986, Train: 0.6086, Test: 0.5598\n",
            "Early stopping:  0.09769488168637909\n",
            "Epoch: 018, Loss: 1.2464, Train: 0.6183, Test: 0.5702\n",
            "Early stopping:  0.09245013811264564\n",
            "Epoch: 019, Loss: 1.1975, Train: 0.6330, Test: 0.5800\n",
            "Early stopping:  0.08737836554437438\n",
            "Epoch: 020, Loss: 1.1523, Train: 0.6410, Test: 0.5861\n",
            "Early stopping:  0.08115431170283596\n",
            "Epoch: 021, Loss: 1.1104, Train: 0.6591, Test: 0.5975\n",
            "Early stopping:  0.07445995586312848\n",
            "Epoch: 022, Loss: 1.0711, Train: 0.6733, Test: 0.6022\n",
            "Early stopping:  0.06925963765032987\n",
            "Epoch: 023, Loss: 1.0364, Train: 0.6829, Test: 0.6070\n",
            "Early stopping:  0.06387046052959167\n",
            "Epoch: 024, Loss: 0.9997, Train: 0.6982, Test: 0.6114\n",
            "Early stopping:  0.06001585269585927\n",
            "Epoch: 025, Loss: 0.9657, Train: 0.7067, Test: 0.6150\n",
            "Early stopping:  0.05707584219809156\n",
            "Epoch: 026, Loss: 0.9326, Train: 0.7158, Test: 0.6177\n",
            "Early stopping:  0.05498834734453111\n",
            "Epoch: 027, Loss: 0.9007, Train: 0.7289, Test: 0.6249\n",
            "Early stopping:  0.05353326296584805\n",
            "Epoch: 028, Loss: 0.8687, Train: 0.7425, Test: 0.6284\n",
            "Early stopping:  0.05170521063905869\n",
            "Epoch: 029, Loss: 0.8389, Train: 0.7499, Test: 0.6325\n",
            "Early stopping:  0.05021145684154516\n",
            "Epoch: 030, Loss: 0.8094, Train: 0.7629, Test: 0.6355\n",
            "Early stopping:  0.048741268902374145\n",
            "Epoch: 031, Loss: 0.7807, Train: 0.7754, Test: 0.6394\n",
            "Early stopping:  0.047329096863715406\n",
            "Epoch: 032, Loss: 0.7521, Train: 0.7811, Test: 0.6426\n",
            "Early stopping:  0.04606425749328617\n",
            "Epoch: 033, Loss: 0.7253, Train: 0.7969, Test: 0.6442\n",
            "Early stopping:  0.04498564463021639\n",
            "Epoch: 034, Loss: 0.6992, Train: 0.8128, Test: 0.6460\n",
            "Early stopping:  0.043608297849109114\n",
            "Epoch: 035, Loss: 0.6741, Train: 0.8174, Test: 0.6479\n",
            "Early stopping:  0.042067449740795175\n",
            "Epoch: 036, Loss: 0.6493, Train: 0.8253, Test: 0.6517\n",
            "Early stopping:  0.04060275537401066\n",
            "Epoch: 037, Loss: 0.6260, Train: 0.8378, Test: 0.6545\n",
            "Early stopping:  0.03929777076625352\n",
            "Epoch: 038, Loss: 0.6024, Train: 0.8508, Test: 0.6588\n",
            "Early stopping:  0.03825253276011356\n",
            "Epoch: 039, Loss: 0.5795, Train: 0.8576, Test: 0.6583\n",
            "Early stopping:  0.03735619362239361\n",
            "Epoch: 040, Loss: 0.5578, Train: 0.8661, Test: 0.6579\n",
            "Early stopping:  0.03629945300025867\n",
            "Epoch: 041, Loss: 0.5361, Train: 0.8724, Test: 0.6581\n",
            "Early stopping:  0.035490708708452716\n",
            "Epoch: 042, Loss: 0.5152, Train: 0.8792, Test: 0.6593\n",
            "Early stopping:  0.03444306827426627\n",
            "Epoch: 043, Loss: 0.4947, Train: 0.8894, Test: 0.6592\n",
            "Early stopping:  0.033540303360967985\n",
            "Epoch: 044, Loss: 0.4745, Train: 0.8996, Test: 0.6605\n",
            "Early stopping:  0.03287771647002637\n",
            "Epoch: 045, Loss: 0.4544, Train: 0.9064, Test: 0.6616\n",
            "Early stopping:  0.03227481698147814\n",
            "Epoch: 046, Loss: 0.4349, Train: 0.9132, Test: 0.6623\n",
            "Early stopping:  0.031780614930048835\n",
            "Epoch: 047, Loss: 0.4155, Train: 0.9189, Test: 0.6659\n",
            "Early stopping:  0.03133406758677347\n",
            "Epoch: 048, Loss: 0.3966, Train: 0.9319, Test: 0.6670\n",
            "Early stopping:  0.030782570533866398\n",
            "Epoch: 049, Loss: 0.3780, Train: 0.9382, Test: 0.6686\n",
            "Early stopping:  0.03020079292588925\n",
            "Epoch: 050, Loss: 0.3600, Train: 0.9404, Test: 0.6694\n",
            "Early stopping:  0.02960887900231647\n",
            "Epoch: 051, Loss: 0.3424, Train: 0.9484, Test: 0.6693\n",
            "Early stopping:  0.028895079215621764\n",
            "Epoch: 052, Loss: 0.3255, Train: 0.9507, Test: 0.6686\n",
            "Early stopping:  0.028125045199251984\n",
            "Epoch: 053, Loss: 0.3092, Train: 0.9592, Test: 0.6677\n",
            "Early stopping:  0.027212713854479542\n",
            "Epoch: 054, Loss: 0.2939, Train: 0.9597, Test: 0.6650\n",
            "Early stopping:  0.026142501565310568\n",
            "Epoch: 055, Loss: 0.2814, Train: 0.9728, Test: 0.6596\n",
            "Early stopping:  0.024311631154827082\n",
            "Epoch: 056, Loss: 0.2769, Train: 0.9421, Test: 0.6598\n",
            "Early stopping:  0.020087403413439602\n",
            "Epoch: 057, Loss: 0.2820, Train: 0.9762, Test: 0.6540\n",
            "Early stopping:  0.013080132154161775\n",
            "Epoch: 058, Loss: 0.2601, Train: 0.9824, Test: 0.6650\n",
            "Early stopping:  0.012244716447541493\n",
            "Epoch: 059, Loss: 0.2319, Train: 0.9620, Test: 0.6606\n",
            "Early stopping:  0.021294190988064066\n",
            "Epoch: 060, Loss: 0.2437, Train: 0.9853, Test: 0.6613\n",
            "Early stopping:  0.021362223428396842\n",
            "Epoch: 061, Loss: 0.2161, Train: 0.9887, Test: 0.6565\n",
            "Early stopping:  0.025457153107827944\n",
            "Epoch: 062, Loss: 0.2151, Train: 0.9818, Test: 0.6640\n",
            "Early stopping:  0.019056918357386987\n",
            "Epoch: 063, Loss: 0.2051, Train: 0.9875, Test: 0.6637\n",
            "Early stopping:  0.015282517635435696\n",
            "Epoch: 064, Loss: 0.1930, Train: 0.9915, Test: 0.6528\n",
            "Early stopping:  0.01872904267693795\n",
            "Epoch: 065, Loss: 0.1922, Train: 0.9949, Test: 0.6592\n",
            "Early stopping:  0.011512176943711283\n",
            "Epoch: 066, Loss: 0.1769, Train: 0.9864, Test: 0.6598\n",
            "Early stopping:  0.014437372508423614\n",
            "Epoch: 067, Loss: 0.1799, Train: 0.9966, Test: 0.6591\n",
            "Early stopping:  0.011327908783397905\n",
            "Epoch: 068, Loss: 0.1651, Train: 0.9955, Test: 0.6538\n",
            "Early stopping:  0.011628569037988323\n",
            "Epoch: 069, Loss: 0.1671, Train: 0.9966, Test: 0.6581\n",
            "Early stopping:  0.010917580116787706\n",
            "Epoch: 070, Loss: 0.1564, Train: 0.9949, Test: 0.6571\n",
            "Early stopping:  0.009460364522858528\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.65      0.65       758\n",
            "         capital_goods       0.58      0.53      0.55       508\n",
            "conglomerates_industry       0.95      0.25      0.40        80\n",
            "     consumer_cyclical       0.59      0.60      0.59       793\n",
            " consumer_non-cyclical       0.73      0.51      0.60       446\n",
            "                energy       0.82      0.68      0.75       283\n",
            "             financial       0.71      0.67      0.69       767\n",
            "            healthcare       0.74      0.68      0.71       318\n",
            "              services       0.62      0.77      0.69      2076\n",
            "            technology       0.58      0.47      0.52       396\n",
            "        transportation       0.80      0.74      0.77       404\n",
            "             utilities       0.77      0.67      0.71       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.60      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.65      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4390, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2415, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.13968493979613947\n",
            "Epoch: 003, Loss: 2.1428, Train: 0.2944, Test: 0.2946\n",
            "Early stopping:  0.15086038305669444\n",
            "Epoch: 004, Loss: 2.1010, Train: 0.3137, Test: 0.3107\n",
            "Early stopping:  0.1506429974965365\n",
            "Epoch: 005, Loss: 2.0363, Train: 0.3539, Test: 0.3472\n",
            "Early stopping:  0.15685448560685492\n",
            "Epoch: 006, Loss: 1.9640, Train: 0.3738, Test: 0.3605\n",
            "Early stopping:  0.10531606416516223\n",
            "Epoch: 007, Loss: 1.8970, Train: 0.3817, Test: 0.3703\n",
            "Early stopping:  0.09979835252615843\n",
            "Epoch: 008, Loss: 1.8340, Train: 0.4124, Test: 0.3925\n",
            "Early stopping:  0.10648082738423627\n",
            "Epoch: 009, Loss: 1.7703, Train: 0.4379, Test: 0.4203\n",
            "Early stopping:  0.104715220678086\n",
            "Epoch: 010, Loss: 1.7032, Train: 0.4594, Test: 0.4437\n",
            "Early stopping:  0.10250644362417856\n",
            "Epoch: 011, Loss: 1.6334, Train: 0.4867, Test: 0.4553\n",
            "Early stopping:  0.10405740642751152\n",
            "Epoch: 012, Loss: 1.5669, Train: 0.4997, Test: 0.4636\n",
            "Early stopping:  0.10611769748764455\n",
            "Epoch: 013, Loss: 1.5024, Train: 0.5303, Test: 0.4831\n",
            "Early stopping:  0.10627050998729037\n",
            "Epoch: 014, Loss: 1.4382, Train: 0.5627, Test: 0.5106\n",
            "Early stopping:  0.10451553726802872\n",
            "Epoch: 015, Loss: 1.3778, Train: 0.5893, Test: 0.5380\n",
            "Early stopping:  0.10121594466204398\n",
            "Epoch: 016, Loss: 1.3187, Train: 0.6137, Test: 0.5615\n",
            "Early stopping:  0.09823078079832825\n",
            "Epoch: 017, Loss: 1.2617, Train: 0.6211, Test: 0.5702\n",
            "Early stopping:  0.09504175693334375\n",
            "Epoch: 018, Loss: 1.2110, Train: 0.6381, Test: 0.5804\n",
            "Early stopping:  0.0902410535769397\n",
            "Epoch: 019, Loss: 1.1642, Train: 0.6500, Test: 0.5899\n",
            "Early stopping:  0.08466587429125923\n",
            "Epoch: 020, Loss: 1.1180, Train: 0.6557, Test: 0.5972\n",
            "Early stopping:  0.07894628579569668\n",
            "Epoch: 021, Loss: 1.0754, Train: 0.6687, Test: 0.6038\n",
            "Early stopping:  0.07365623447311269\n",
            "Epoch: 022, Loss: 1.0384, Train: 0.6795, Test: 0.6073\n",
            "Early stopping:  0.06870347900584503\n",
            "Epoch: 023, Loss: 1.0035, Train: 0.6937, Test: 0.6138\n",
            "Early stopping:  0.06349358064033041\n",
            "Epoch: 024, Loss: 0.9688, Train: 0.7028, Test: 0.6201\n",
            "Early stopping:  0.058596539532072875\n",
            "Epoch: 025, Loss: 0.9369, Train: 0.7136, Test: 0.6228\n",
            "Early stopping:  0.05481602587557493\n",
            "Epoch: 026, Loss: 0.9065, Train: 0.7255, Test: 0.6284\n",
            "Early stopping:  0.05227868860088781\n",
            "Epoch: 027, Loss: 0.8764, Train: 0.7379, Test: 0.6287\n",
            "Early stopping:  0.05008614447552669\n",
            "Epoch: 028, Loss: 0.8460, Train: 0.7476, Test: 0.6317\n",
            "Early stopping:  0.048420820975882385\n",
            "Epoch: 029, Loss: 0.8176, Train: 0.7578, Test: 0.6352\n",
            "Early stopping:  0.047297051491013334\n",
            "Epoch: 030, Loss: 0.7904, Train: 0.7674, Test: 0.6384\n",
            "Early stopping:  0.04602912793214847\n",
            "Epoch: 031, Loss: 0.7632, Train: 0.7777, Test: 0.6412\n",
            "Early stopping:  0.04459764081994063\n",
            "Epoch: 032, Loss: 0.7368, Train: 0.7901, Test: 0.6480\n",
            "Early stopping:  0.04313613187864601\n",
            "Epoch: 033, Loss: 0.7115, Train: 0.8060, Test: 0.6501\n",
            "Early stopping:  0.04204396287397041\n",
            "Epoch: 034, Loss: 0.6858, Train: 0.8088, Test: 0.6534\n",
            "Early stopping:  0.04127108307321715\n",
            "Epoch: 035, Loss: 0.6610, Train: 0.8151, Test: 0.6568\n",
            "Early stopping:  0.04039520282920716\n",
            "Epoch: 036, Loss: 0.6375, Train: 0.8293, Test: 0.6596\n",
            "Early stopping:  0.03938093569644385\n",
            "Epoch: 037, Loss: 0.6142, Train: 0.8389, Test: 0.6603\n",
            "Early stopping:  0.03840859492869888\n",
            "Epoch: 038, Loss: 0.5915, Train: 0.8452, Test: 0.6635\n",
            "Early stopping:  0.03721253788317028\n",
            "Epoch: 039, Loss: 0.5699, Train: 0.8571, Test: 0.6639\n",
            "Early stopping:  0.03607861621632852\n",
            "Epoch: 040, Loss: 0.5481, Train: 0.8639, Test: 0.6666\n",
            "Early stopping:  0.035279075411255835\n",
            "Epoch: 041, Loss: 0.5266, Train: 0.8735, Test: 0.6673\n",
            "Early stopping:  0.03456436404031715\n",
            "Epoch: 042, Loss: 0.5057, Train: 0.8820, Test: 0.6670\n",
            "Early stopping:  0.03397141250160514\n",
            "Epoch: 043, Loss: 0.4850, Train: 0.8911, Test: 0.6669\n",
            "Early stopping:  0.033553931036792155\n",
            "Epoch: 044, Loss: 0.4643, Train: 0.9064, Test: 0.6657\n",
            "Early stopping:  0.033082455340874034\n",
            "Epoch: 045, Loss: 0.4442, Train: 0.9070, Test: 0.6693\n",
            "Early stopping:  0.03260438066093148\n",
            "Epoch: 046, Loss: 0.4242, Train: 0.9172, Test: 0.6708\n",
            "Early stopping:  0.03224628862962954\n",
            "Epoch: 047, Loss: 0.4044, Train: 0.9178, Test: 0.6731\n",
            "Early stopping:  0.03182633307978611\n",
            "Epoch: 048, Loss: 0.3852, Train: 0.9257, Test: 0.6724\n",
            "Early stopping:  0.03129254913894789\n",
            "Epoch: 049, Loss: 0.3662, Train: 0.9291, Test: 0.6725\n",
            "Early stopping:  0.03080934900028354\n",
            "Epoch: 050, Loss: 0.3480, Train: 0.9416, Test: 0.6714\n",
            "Early stopping:  0.030129997021906958\n",
            "Epoch: 051, Loss: 0.3304, Train: 0.9393, Test: 0.6730\n",
            "Early stopping:  0.02930303805504332\n",
            "Epoch: 052, Loss: 0.3143, Train: 0.9541, Test: 0.6698\n",
            "Early stopping:  0.028129598251817083\n",
            "Epoch: 053, Loss: 0.3008, Train: 0.9490, Test: 0.6724\n",
            "Early stopping:  0.026081710449080323\n",
            "Epoch: 054, Loss: 0.2879, Train: 0.9682, Test: 0.6724\n",
            "Early stopping:  0.023735540448140722\n",
            "Epoch: 055, Loss: 0.2708, Train: 0.9739, Test: 0.6756\n",
            "Early stopping:  0.023028723543279336\n",
            "Epoch: 056, Loss: 0.2522, Train: 0.9750, Test: 0.6754\n",
            "Early stopping:  0.024450147490518232\n",
            "Epoch: 057, Loss: 0.2422, Train: 0.9830, Test: 0.6724\n",
            "Early stopping:  0.0242657152644324\n",
            "Epoch: 058, Loss: 0.2325, Train: 0.9841, Test: 0.6756\n",
            "Early stopping:  0.022291061558689183\n",
            "Epoch: 059, Loss: 0.2162, Train: 0.9853, Test: 0.6745\n",
            "Early stopping:  0.020539734904511244\n",
            "Epoch: 060, Loss: 0.2059, Train: 0.9943, Test: 0.6751\n",
            "Early stopping:  0.01884001803890805\n",
            "Epoch: 061, Loss: 0.1990, Train: 0.9909, Test: 0.6718\n",
            "Early stopping:  0.01804097933559788\n",
            "Epoch: 062, Loss: 0.1863, Train: 0.9926, Test: 0.6721\n",
            "Early stopping:  0.017503391594558014\n",
            "Epoch: 063, Loss: 0.1780, Train: 0.9966, Test: 0.6718\n",
            "Early stopping:  0.015224911002716683\n",
            "Epoch: 064, Loss: 0.1727, Train: 0.9972, Test: 0.6730\n",
            "Early stopping:  0.013945580408843276\n",
            "Epoch: 065, Loss: 0.1635, Train: 0.9972, Test: 0.6694\n",
            "Early stopping:  0.013489205506402096\n",
            "Epoch: 066, Loss: 0.1577, Train: 0.9977, Test: 0.6686\n",
            "Early stopping:  0.011343014198178264\n",
            "Epoch: 067, Loss: 0.1571, Train: 0.9960, Test: 0.6686\n",
            "Early stopping:  0.00923146826921829\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.66      0.67       758\n",
            "         capital_goods       0.57      0.52      0.54       508\n",
            "conglomerates_industry       0.88      0.17      0.29        80\n",
            "     consumer_cyclical       0.61      0.66      0.63       793\n",
            " consumer_non-cyclical       0.74      0.56      0.64       446\n",
            "                energy       0.78      0.65      0.71       283\n",
            "             financial       0.74      0.65      0.69       767\n",
            "            healthcare       0.76      0.68      0.72       318\n",
            "              services       0.63      0.78      0.70      2076\n",
            "            technology       0.64      0.53      0.58       396\n",
            "        transportation       0.81      0.75      0.77       404\n",
            "             utilities       0.76      0.66      0.71       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.72      0.60      0.64      7054\n",
            "          weighted avg       0.68      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5014, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2706, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16325851510525272\n",
            "Epoch: 003, Loss: 2.1814, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16518459280729184\n",
            "Epoch: 004, Loss: 2.1427, Train: 0.2967, Test: 0.2960\n",
            "Early stopping:  0.16078697094985692\n",
            "Epoch: 005, Loss: 2.0669, Train: 0.3290, Test: 0.3263\n",
            "Early stopping:  0.1672360497560707\n",
            "Epoch: 006, Loss: 2.0044, Train: 0.3795, Test: 0.3693\n",
            "Early stopping:  0.10277782286786544\n",
            "Epoch: 007, Loss: 1.9531, Train: 0.4033, Test: 0.3856\n",
            "Early stopping:  0.09441345744187722\n",
            "Epoch: 008, Loss: 1.8991, Train: 0.4027, Test: 0.3856\n",
            "Early stopping:  0.0953450447868191\n",
            "Epoch: 009, Loss: 1.8385, Train: 0.4061, Test: 0.3869\n",
            "Early stopping:  0.08892195066569311\n",
            "Epoch: 010, Loss: 1.7762, Train: 0.4158, Test: 0.4010\n",
            "Early stopping:  0.09037637278367396\n",
            "Epoch: 011, Loss: 1.7123, Train: 0.4436, Test: 0.4269\n",
            "Early stopping:  0.09563774001306118\n",
            "Epoch: 012, Loss: 1.6459, Train: 0.4816, Test: 0.4501\n",
            "Early stopping:  0.10004416661001683\n",
            "Epoch: 013, Loss: 1.5850, Train: 0.5009, Test: 0.4658\n",
            "Early stopping:  0.10078545175522641\n",
            "Epoch: 014, Loss: 1.5312, Train: 0.5207, Test: 0.4837\n",
            "Early stopping:  0.09767790092637468\n",
            "Epoch: 015, Loss: 1.4739, Train: 0.5406, Test: 0.4994\n",
            "Early stopping:  0.09360211425323592\n",
            "Epoch: 016, Loss: 1.4146, Train: 0.5621, Test: 0.5164\n",
            "Early stopping:  0.0907264535916832\n",
            "Epoch: 017, Loss: 1.3621, Train: 0.5757, Test: 0.5303\n",
            "Early stopping:  0.08892532833585921\n",
            "Epoch: 018, Loss: 1.3140, Train: 0.6069, Test: 0.5498\n",
            "Early stopping:  0.08643034528579907\n",
            "Epoch: 019, Loss: 1.2652, Train: 0.6239, Test: 0.5654\n",
            "Early stopping:  0.08196589139010425\n",
            "Epoch: 020, Loss: 1.2184, Train: 0.6341, Test: 0.5798\n",
            "Early stopping:  0.07738399392513652\n",
            "Epoch: 021, Loss: 1.1743, Train: 0.6483, Test: 0.5882\n",
            "Early stopping:  0.07452812179929044\n",
            "Epoch: 022, Loss: 1.1319, Train: 0.6619, Test: 0.5944\n",
            "Early stopping:  0.07199749110101993\n",
            "Epoch: 023, Loss: 1.0913, Train: 0.6710, Test: 0.5974\n",
            "Early stopping:  0.06869725236486202\n",
            "Epoch: 024, Loss: 1.0533, Train: 0.6824, Test: 0.6029\n",
            "Early stopping:  0.06536070578381237\n",
            "Epoch: 025, Loss: 1.0184, Train: 0.6999, Test: 0.6100\n",
            "Early stopping:  0.061775734889264224\n",
            "Epoch: 026, Loss: 0.9847, Train: 0.7085, Test: 0.6172\n",
            "Early stopping:  0.058104513570049234\n",
            "Epoch: 027, Loss: 0.9514, Train: 0.7198, Test: 0.6215\n",
            "Early stopping:  0.0551033885152341\n",
            "Epoch: 028, Loss: 0.9198, Train: 0.7317, Test: 0.6260\n",
            "Early stopping:  0.05279657025511905\n",
            "Epoch: 029, Loss: 0.8895, Train: 0.7362, Test: 0.6286\n",
            "Early stopping:  0.05105904095799924\n",
            "Epoch: 030, Loss: 0.8605, Train: 0.7470, Test: 0.6299\n",
            "Early stopping:  0.04911152890193264\n",
            "Epoch: 031, Loss: 0.8312, Train: 0.7567, Test: 0.6321\n",
            "Early stopping:  0.047416430468657345\n",
            "Epoch: 032, Loss: 0.8023, Train: 0.7714, Test: 0.6337\n",
            "Early stopping:  0.046371812310919094\n",
            "Epoch: 033, Loss: 0.7753, Train: 0.7799, Test: 0.6374\n",
            "Early stopping:  0.045286711900215146\n",
            "Epoch: 034, Loss: 0.7476, Train: 0.7907, Test: 0.6388\n",
            "Early stopping:  0.04453316569761689\n",
            "Epoch: 035, Loss: 0.7206, Train: 0.7952, Test: 0.6433\n",
            "Early stopping:  0.04363847235888454\n",
            "Epoch: 036, Loss: 0.6950, Train: 0.8043, Test: 0.6463\n",
            "Early stopping:  0.04260321016011829\n",
            "Epoch: 037, Loss: 0.6702, Train: 0.8128, Test: 0.6497\n",
            "Early stopping:  0.041562179590489835\n",
            "Epoch: 038, Loss: 0.6460, Train: 0.8242, Test: 0.6507\n",
            "Early stopping:  0.04009869472571345\n",
            "Epoch: 039, Loss: 0.6228, Train: 0.8344, Test: 0.6501\n",
            "Early stopping:  0.03866427042243978\n",
            "Epoch: 040, Loss: 0.5997, Train: 0.8452, Test: 0.6520\n",
            "Early stopping:  0.03763089370419653\n",
            "Epoch: 041, Loss: 0.5769, Train: 0.8531, Test: 0.6535\n",
            "Early stopping:  0.03681776537925994\n",
            "Epoch: 042, Loss: 0.5548, Train: 0.8627, Test: 0.6562\n",
            "Early stopping:  0.036102728144612115\n",
            "Epoch: 043, Loss: 0.5326, Train: 0.8729, Test: 0.6593\n",
            "Early stopping:  0.03564747500824293\n",
            "Epoch: 044, Loss: 0.5111, Train: 0.8843, Test: 0.6608\n",
            "Early stopping:  0.035051018567562466\n",
            "Epoch: 045, Loss: 0.4900, Train: 0.8945, Test: 0.6618\n",
            "Early stopping:  0.034406129293912596\n",
            "Epoch: 046, Loss: 0.4691, Train: 0.8990, Test: 0.6626\n",
            "Early stopping:  0.033826567815579175\n",
            "Epoch: 047, Loss: 0.4487, Train: 0.9053, Test: 0.6613\n",
            "Early stopping:  0.033149471242873464\n",
            "Epoch: 048, Loss: 0.4287, Train: 0.9081, Test: 0.6625\n",
            "Early stopping:  0.032575902071224246\n",
            "Epoch: 049, Loss: 0.4090, Train: 0.9183, Test: 0.6650\n",
            "Early stopping:  0.03199701295114944\n",
            "Epoch: 050, Loss: 0.3895, Train: 0.9251, Test: 0.6664\n",
            "Early stopping:  0.031451546164013515\n",
            "Epoch: 051, Loss: 0.3703, Train: 0.9319, Test: 0.6644\n",
            "Early stopping:  0.030990284731565867\n",
            "Epoch: 052, Loss: 0.3519, Train: 0.9427, Test: 0.6644\n",
            "Early stopping:  0.030416875079497648\n",
            "Epoch: 053, Loss: 0.3338, Train: 0.9461, Test: 0.6643\n",
            "Early stopping:  0.029745657379093297\n",
            "Epoch: 054, Loss: 0.3166, Train: 0.9558, Test: 0.6637\n",
            "Early stopping:  0.02884224693717749\n",
            "Epoch: 055, Loss: 0.3003, Train: 0.9614, Test: 0.6622\n",
            "Early stopping:  0.02773573857193828\n",
            "Epoch: 056, Loss: 0.2854, Train: 0.9665, Test: 0.6626\n",
            "Early stopping:  0.026337921934795772\n",
            "Epoch: 057, Loss: 0.2720, Train: 0.9660, Test: 0.6637\n",
            "Early stopping:  0.024511657838691844\n",
            "Epoch: 058, Loss: 0.2601, Train: 0.9745, Test: 0.6630\n",
            "Early stopping:  0.02238707088054476\n",
            "Epoch: 059, Loss: 0.2462, Train: 0.9779, Test: 0.6674\n",
            "Early stopping:  0.02111038761465338\n",
            "Epoch: 060, Loss: 0.2306, Train: 0.9796, Test: 0.6687\n",
            "Early stopping:  0.021434932189004885\n",
            "Epoch: 061, Loss: 0.2185, Train: 0.9841, Test: 0.6632\n",
            "Early stopping:  0.021589760205069745\n",
            "Epoch: 062, Loss: 0.2105, Train: 0.9813, Test: 0.6649\n",
            "Early stopping:  0.020189757708665535\n",
            "Epoch: 063, Loss: 0.2016, Train: 0.9898, Test: 0.6649\n",
            "Early stopping:  0.01745959048210964\n",
            "Epoch: 064, Loss: 0.1898, Train: 0.9909, Test: 0.6670\n",
            "Early stopping:  0.015593104031791422\n",
            "Epoch: 065, Loss: 0.1796, Train: 0.9881, Test: 0.6653\n",
            "Early stopping:  0.01560364711919698\n",
            "Epoch: 066, Loss: 0.1735, Train: 0.9955, Test: 0.6649\n",
            "Early stopping:  0.015237362198689194\n",
            "Epoch: 067, Loss: 0.1680, Train: 0.9909, Test: 0.6649\n",
            "Early stopping:  0.013408613371917085\n",
            "Epoch: 068, Loss: 0.1603, Train: 0.9960, Test: 0.6652\n",
            "Early stopping:  0.011256491151318644\n",
            "Epoch: 069, Loss: 0.1518, Train: 0.9960, Test: 0.6650\n",
            "Early stopping:  0.01093683877187362\n",
            "Epoch: 070, Loss: 0.1457, Train: 0.9949, Test: 0.6640\n",
            "Early stopping:  0.011380360782655979\n",
            "Epoch: 071, Loss: 0.1421, Train: 0.9977, Test: 0.6630\n",
            "Early stopping:  0.010607389270703281\n",
            "Epoch: 072, Loss: 0.1389, Train: 0.9960, Test: 0.6642\n",
            "Early stopping:  0.008489987607507266\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.66      0.66       758\n",
            "         capital_goods       0.55      0.55      0.55       508\n",
            "conglomerates_industry       0.92      0.14      0.24        80\n",
            "     consumer_cyclical       0.65      0.64      0.64       793\n",
            " consumer_non-cyclical       0.70      0.53      0.60       446\n",
            "                energy       0.81      0.67      0.74       283\n",
            "             financial       0.70      0.65      0.67       767\n",
            "            healthcare       0.76      0.68      0.72       318\n",
            "              services       0.63      0.78      0.69      2076\n",
            "            technology       0.63      0.51      0.56       396\n",
            "        transportation       0.81      0.72      0.76       404\n",
            "             utilities       0.76      0.65      0.70       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.60      0.63      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4877, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2868, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14203622818086523\n",
            "Epoch: 003, Loss: 2.1673, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16190364582268793\n",
            "Epoch: 004, Loss: 2.1376, Train: 0.2995, Test: 0.2973\n",
            "Early stopping:  0.15890424845784268\n",
            "Epoch: 005, Loss: 2.0664, Train: 0.3443, Test: 0.3380\n",
            "Early stopping:  0.1649691730966319\n",
            "Epoch: 006, Loss: 1.9975, Train: 0.3959, Test: 0.3869\n",
            "Early stopping:  0.10911714400788036\n",
            "Epoch: 007, Loss: 1.9347, Train: 0.4180, Test: 0.4043\n",
            "Early stopping:  0.09640453308866963\n",
            "Epoch: 008, Loss: 1.8726, Train: 0.4203, Test: 0.4053\n",
            "Early stopping:  0.10467448081394812\n",
            "Epoch: 009, Loss: 1.8135, Train: 0.4237, Test: 0.4063\n",
            "Early stopping:  0.0997623316237983\n",
            "Epoch: 010, Loss: 1.7536, Train: 0.4356, Test: 0.4193\n",
            "Early stopping:  0.09630382703506027\n",
            "Epoch: 011, Loss: 1.6867, Train: 0.4657, Test: 0.4392\n",
            "Early stopping:  0.09726861588446603\n",
            "Epoch: 012, Loss: 1.6172, Train: 0.4878, Test: 0.4614\n",
            "Early stopping:  0.10088822938335575\n",
            "Epoch: 013, Loss: 1.5536, Train: 0.5099, Test: 0.4732\n",
            "Early stopping:  0.10380373667255448\n",
            "Epoch: 014, Loss: 1.4961, Train: 0.5400, Test: 0.4894\n",
            "Early stopping:  0.10253871341832738\n",
            "Epoch: 015, Loss: 1.4397, Train: 0.5621, Test: 0.5129\n",
            "Early stopping:  0.09734961761967469\n",
            "Epoch: 016, Loss: 1.3823, Train: 0.5820, Test: 0.5308\n",
            "Early stopping:  0.09232914210299491\n",
            "Epoch: 017, Loss: 1.3263, Train: 0.6081, Test: 0.5432\n",
            "Early stopping:  0.08988552405272825\n",
            "Epoch: 018, Loss: 1.2749, Train: 0.6166, Test: 0.5614\n",
            "Early stopping:  0.08790544182196121\n",
            "Epoch: 019, Loss: 1.2260, Train: 0.6302, Test: 0.5758\n",
            "Early stopping:  0.08461605991183736\n",
            "Epoch: 020, Loss: 1.1786, Train: 0.6449, Test: 0.5843\n",
            "Early stopping:  0.08030274866896071\n",
            "Epoch: 021, Loss: 1.1341, Train: 0.6619, Test: 0.5900\n",
            "Early stopping:  0.07603114050898212\n",
            "Epoch: 022, Loss: 1.0919, Train: 0.6676, Test: 0.5964\n",
            "Early stopping:  0.07243165787768845\n",
            "Epoch: 023, Loss: 1.0538, Train: 0.6812, Test: 0.6039\n",
            "Early stopping:  0.06820278411044729\n",
            "Epoch: 024, Loss: 1.0183, Train: 0.6852, Test: 0.6090\n",
            "Early stopping:  0.06345771111109343\n",
            "Epoch: 025, Loss: 0.9831, Train: 0.6994, Test: 0.6181\n",
            "Early stopping:  0.05943178263623677\n",
            "Epoch: 026, Loss: 0.9481, Train: 0.7130, Test: 0.6238\n",
            "Early stopping:  0.05665581200237574\n",
            "Epoch: 027, Loss: 0.9156, Train: 0.7215, Test: 0.6270\n",
            "Early stopping:  0.054800652515154195\n",
            "Epoch: 028, Loss: 0.8846, Train: 0.7328, Test: 0.6289\n",
            "Early stopping:  0.052971193871531826\n",
            "Epoch: 029, Loss: 0.8541, Train: 0.7453, Test: 0.6314\n",
            "Early stopping:  0.05085427988001944\n",
            "Epoch: 030, Loss: 0.8248, Train: 0.7533, Test: 0.6343\n",
            "Early stopping:  0.0487441911629041\n",
            "Epoch: 031, Loss: 0.7956, Train: 0.7669, Test: 0.6389\n",
            "Early stopping:  0.047391978352715296\n",
            "Epoch: 032, Loss: 0.7680, Train: 0.7742, Test: 0.6412\n",
            "Early stopping:  0.046104595674794926\n",
            "Epoch: 033, Loss: 0.7415, Train: 0.7884, Test: 0.6450\n",
            "Early stopping:  0.04460491256214864\n",
            "Epoch: 034, Loss: 0.7148, Train: 0.8037, Test: 0.6481\n",
            "Early stopping:  0.04336428150886807\n",
            "Epoch: 035, Loss: 0.6893, Train: 0.8162, Test: 0.6490\n",
            "Early stopping:  0.042053199537567344\n",
            "Epoch: 036, Loss: 0.6642, Train: 0.8253, Test: 0.6517\n",
            "Early stopping:  0.04109689710196345\n",
            "Epoch: 037, Loss: 0.6394, Train: 0.8355, Test: 0.6521\n",
            "Early stopping:  0.04027813674317338\n",
            "Epoch: 038, Loss: 0.6156, Train: 0.8383, Test: 0.6534\n",
            "Early stopping:  0.03924515364530383\n",
            "Epoch: 039, Loss: 0.5924, Train: 0.8463, Test: 0.6531\n",
            "Early stopping:  0.03833399884552038\n",
            "Epoch: 040, Loss: 0.5692, Train: 0.8548, Test: 0.6566\n",
            "Early stopping:  0.03745684872863744\n",
            "Epoch: 041, Loss: 0.5466, Train: 0.8673, Test: 0.6601\n",
            "Early stopping:  0.036689900543940286\n",
            "Epoch: 042, Loss: 0.5239, Train: 0.8763, Test: 0.6615\n",
            "Early stopping:  0.03625100599914562\n",
            "Epoch: 043, Loss: 0.5014, Train: 0.8820, Test: 0.6635\n",
            "Early stopping:  0.03594930902395178\n",
            "Epoch: 044, Loss: 0.4799, Train: 0.8928, Test: 0.6657\n",
            "Early stopping:  0.03539682348251619\n",
            "Epoch: 045, Loss: 0.4585, Train: 0.8996, Test: 0.6666\n",
            "Early stopping:  0.03480592440367319\n",
            "Epoch: 046, Loss: 0.4378, Train: 0.9087, Test: 0.6698\n",
            "Early stopping:  0.034015961032398984\n",
            "Epoch: 047, Loss: 0.4176, Train: 0.9149, Test: 0.6688\n",
            "Early stopping:  0.033154342961492146\n",
            "Epoch: 048, Loss: 0.3983, Train: 0.9246, Test: 0.6659\n",
            "Early stopping:  0.0322833499398172\n",
            "Epoch: 049, Loss: 0.3808, Train: 0.9206, Test: 0.6686\n",
            "Early stopping:  0.030843241700972246\n",
            "Epoch: 050, Loss: 0.3676, Train: 0.9325, Test: 0.6622\n",
            "Early stopping:  0.028094081569240262\n",
            "Epoch: 051, Loss: 0.3547, Train: 0.9348, Test: 0.6678\n",
            "Early stopping:  0.024851091972270686\n",
            "Epoch: 052, Loss: 0.3330, Train: 0.9495, Test: 0.6669\n",
            "Early stopping:  0.02487021376120308\n",
            "Epoch: 053, Loss: 0.3119, Train: 0.9558, Test: 0.6664\n",
            "Early stopping:  0.02748303052649549\n",
            "Epoch: 054, Loss: 0.3028, Train: 0.9478, Test: 0.6670\n",
            "Early stopping:  0.027486291893947204\n",
            "Epoch: 055, Loss: 0.2898, Train: 0.9631, Test: 0.6669\n",
            "Early stopping:  0.025666048696694376\n",
            "Epoch: 056, Loss: 0.2705, Train: 0.9705, Test: 0.6680\n",
            "Early stopping:  0.023436798502680677\n",
            "Epoch: 057, Loss: 0.2605, Train: 0.9637, Test: 0.6725\n",
            "Early stopping:  0.021503554397158436\n",
            "Epoch: 058, Loss: 0.2514, Train: 0.9773, Test: 0.6721\n",
            "Early stopping:  0.02111271147094286\n",
            "Epoch: 059, Loss: 0.2354, Train: 0.9824, Test: 0.6720\n",
            "Early stopping:  0.02041356605168677\n",
            "Epoch: 060, Loss: 0.2250, Train: 0.9796, Test: 0.6725\n",
            "Early stopping:  0.01845273894460639\n",
            "Epoch: 061, Loss: 0.2184, Train: 0.9870, Test: 0.6690\n",
            "Early stopping:  0.017655579544624128\n",
            "Epoch: 062, Loss: 0.2066, Train: 0.9892, Test: 0.6720\n",
            "Early stopping:  0.017013085966251843\n",
            "Epoch: 063, Loss: 0.1958, Train: 0.9898, Test: 0.6727\n",
            "Early stopping:  0.015498915670314726\n",
            "Epoch: 064, Loss: 0.1905, Train: 0.9909, Test: 0.6673\n",
            "Early stopping:  0.014592434074597288\n",
            "Epoch: 065, Loss: 0.1828, Train: 0.9926, Test: 0.6724\n",
            "Early stopping:  0.01393921993420209\n",
            "Epoch: 066, Loss: 0.1729, Train: 0.9926, Test: 0.6734\n",
            "Early stopping:  0.012780139832248733\n",
            "Epoch: 067, Loss: 0.1675, Train: 0.9955, Test: 0.6669\n",
            "Early stopping:  0.011781415770306685\n",
            "Epoch: 068, Loss: 0.1629, Train: 0.9949, Test: 0.6714\n",
            "Early stopping:  0.011260735494949586\n",
            "Epoch: 069, Loss: 0.1554, Train: 0.9949, Test: 0.6688\n",
            "Early stopping:  0.010343036442506143\n",
            "Epoch: 070, Loss: 0.1493, Train: 0.9977, Test: 0.6653\n",
            "Early stopping:  0.009408312693449425\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.68      0.66       758\n",
            "         capital_goods       0.51      0.57      0.54       508\n",
            "conglomerates_industry       0.77      0.21      0.33        80\n",
            "     consumer_cyclical       0.63      0.64      0.63       793\n",
            " consumer_non-cyclical       0.74      0.55      0.63       446\n",
            "                energy       0.74      0.70      0.72       283\n",
            "             financial       0.71      0.70      0.70       767\n",
            "            healthcare       0.80      0.78      0.79       318\n",
            "              services       0.67      0.73      0.70      2076\n",
            "            technology       0.58      0.49      0.53       396\n",
            "        transportation       0.77      0.71      0.74       404\n",
            "             utilities       0.74      0.62      0.67       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.69      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.67      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5395, Train: 0.2950, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2874, Train: 0.2944, Test: 0.2939\n",
            "Early stopping:  0.17824745172227305\n",
            "Epoch: 003, Loss: 2.1609, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.19271057915875117\n",
            "Epoch: 004, Loss: 2.1517, Train: 0.2955, Test: 0.2943\n",
            "Early stopping:  0.1806602630395662\n",
            "Epoch: 005, Loss: 2.0792, Train: 0.3120, Test: 0.3096\n",
            "Early stopping:  0.1814915339513801\n",
            "Epoch: 006, Loss: 2.0019, Train: 0.3693, Test: 0.3575\n",
            "Early stopping:  0.10605503062659537\n",
            "Epoch: 007, Loss: 1.9445, Train: 0.4129, Test: 0.3931\n",
            "Early stopping:  0.09404606842322102\n",
            "Epoch: 008, Loss: 1.8891, Train: 0.4243, Test: 0.4016\n",
            "Early stopping:  0.1046346343346363\n",
            "Epoch: 009, Loss: 1.8273, Train: 0.4288, Test: 0.4045\n",
            "Early stopping:  0.09769175483393609\n",
            "Epoch: 010, Loss: 1.7615, Train: 0.4351, Test: 0.4124\n",
            "Early stopping:  0.0946179700253523\n",
            "Epoch: 011, Loss: 1.6959, Train: 0.4532, Test: 0.4286\n",
            "Early stopping:  0.09885582221736484\n",
            "Epoch: 012, Loss: 1.6295, Train: 0.4872, Test: 0.4549\n",
            "Early stopping:  0.10285471315854426\n",
            "Epoch: 013, Loss: 1.5610, Train: 0.5065, Test: 0.4738\n",
            "Early stopping:  0.10508477007825526\n",
            "Epoch: 014, Loss: 1.4967, Train: 0.5377, Test: 0.4956\n",
            "Early stopping:  0.10507642550720625\n",
            "Epoch: 015, Loss: 1.4398, Train: 0.5672, Test: 0.5153\n",
            "Early stopping:  0.10205408287487971\n",
            "Epoch: 016, Loss: 1.3815, Train: 0.5922, Test: 0.5397\n",
            "Early stopping:  0.09768913290355034\n",
            "Epoch: 017, Loss: 1.3216, Train: 0.6047, Test: 0.5557\n",
            "Early stopping:  0.09393932512271677\n",
            "Epoch: 018, Loss: 1.2699, Train: 0.6154, Test: 0.5654\n",
            "Early stopping:  0.0904376952035459\n",
            "Epoch: 019, Loss: 1.2262, Train: 0.6313, Test: 0.5746\n",
            "Early stopping:  0.08533205834678917\n",
            "Epoch: 020, Loss: 1.1822, Train: 0.6398, Test: 0.5814\n",
            "Early stopping:  0.07828903220502625\n",
            "Epoch: 021, Loss: 1.1390, Train: 0.6506, Test: 0.5868\n",
            "Early stopping:  0.07164962768625002\n",
            "Epoch: 022, Loss: 1.1009, Train: 0.6687, Test: 0.5931\n",
            "Early stopping:  0.0672684875348452\n",
            "Epoch: 023, Loss: 1.0653, Train: 0.6727, Test: 0.5992\n",
            "Early stopping:  0.06382575053916555\n",
            "Epoch: 024, Loss: 1.0316, Train: 0.6824, Test: 0.6031\n",
            "Early stopping:  0.0593471871078317\n",
            "Epoch: 025, Loss: 0.9988, Train: 0.6931, Test: 0.6104\n",
            "Early stopping:  0.05530786951925854\n",
            "Epoch: 026, Loss: 0.9646, Train: 0.7062, Test: 0.6177\n",
            "Early stopping:  0.05361397462213237\n",
            "Epoch: 027, Loss: 0.9322, Train: 0.7175, Test: 0.6218\n",
            "Early stopping:  0.05270046046055883\n",
            "Epoch: 028, Loss: 0.9032, Train: 0.7277, Test: 0.6262\n",
            "Early stopping:  0.051156332860410395\n",
            "Epoch: 029, Loss: 0.8730, Train: 0.7368, Test: 0.6300\n",
            "Early stopping:  0.04951571045096927\n",
            "Epoch: 030, Loss: 0.8441, Train: 0.7419, Test: 0.6360\n",
            "Early stopping:  0.04745125407107781\n",
            "Epoch: 031, Loss: 0.8179, Train: 0.7555, Test: 0.6379\n",
            "Early stopping:  0.04548272149848548\n",
            "Epoch: 032, Loss: 0.7913, Train: 0.7731, Test: 0.6385\n",
            "Early stopping:  0.04412110634747919\n",
            "Epoch: 033, Loss: 0.7657, Train: 0.7805, Test: 0.6429\n",
            "Early stopping:  0.042285332431618174\n",
            "Epoch: 034, Loss: 0.7401, Train: 0.7924, Test: 0.6494\n",
            "Early stopping:  0.041138025511395\n",
            "Epoch: 035, Loss: 0.7152, Train: 0.8026, Test: 0.6527\n",
            "Early stopping:  0.04057075766546887\n",
            "Epoch: 036, Loss: 0.6921, Train: 0.8157, Test: 0.6541\n",
            "Early stopping:  0.0393579566957069\n",
            "Epoch: 037, Loss: 0.6685, Train: 0.8208, Test: 0.6542\n",
            "Early stopping:  0.03834304044429172\n",
            "Epoch: 038, Loss: 0.6458, Train: 0.8321, Test: 0.6558\n",
            "Early stopping:  0.037226117056200776\n",
            "Epoch: 039, Loss: 0.6239, Train: 0.8332, Test: 0.6579\n",
            "Early stopping:  0.03620503739479904\n",
            "Epoch: 040, Loss: 0.6028, Train: 0.8423, Test: 0.6596\n",
            "Early stopping:  0.03529583464009359\n",
            "Epoch: 041, Loss: 0.5816, Train: 0.8537, Test: 0.6601\n",
            "Early stopping:  0.034273849798047214\n",
            "Epoch: 042, Loss: 0.5604, Train: 0.8667, Test: 0.6613\n",
            "Early stopping:  0.03369167556239451\n",
            "Epoch: 043, Loss: 0.5401, Train: 0.8735, Test: 0.6660\n",
            "Early stopping:  0.033217271854120496\n",
            "Epoch: 044, Loss: 0.5198, Train: 0.8746, Test: 0.6652\n",
            "Early stopping:  0.03281825287018324\n",
            "Epoch: 045, Loss: 0.5001, Train: 0.8854, Test: 0.6632\n",
            "Early stopping:  0.03220627330772822\n",
            "Epoch: 046, Loss: 0.4804, Train: 0.8956, Test: 0.6650\n",
            "Early stopping:  0.03162099788345243\n",
            "Epoch: 047, Loss: 0.4616, Train: 0.9053, Test: 0.6660\n",
            "Early stopping:  0.03105098215427397\n",
            "Epoch: 048, Loss: 0.4427, Train: 0.9121, Test: 0.6666\n",
            "Early stopping:  0.030474301230775068\n",
            "Epoch: 049, Loss: 0.4244, Train: 0.9234, Test: 0.6684\n",
            "Early stopping:  0.0298991741927084\n",
            "Epoch: 050, Loss: 0.4064, Train: 0.9325, Test: 0.6684\n",
            "Early stopping:  0.029278010562982914\n",
            "Epoch: 051, Loss: 0.3889, Train: 0.9359, Test: 0.6687\n",
            "Early stopping:  0.02872541963159728\n",
            "Epoch: 052, Loss: 0.3720, Train: 0.9404, Test: 0.6693\n",
            "Early stopping:  0.027973163967803437\n",
            "Epoch: 053, Loss: 0.3554, Train: 0.9444, Test: 0.6698\n",
            "Early stopping:  0.02725521962982934\n",
            "Epoch: 054, Loss: 0.3394, Train: 0.9507, Test: 0.6697\n",
            "Early stopping:  0.026484359287072805\n",
            "Epoch: 055, Loss: 0.3239, Train: 0.9569, Test: 0.6688\n",
            "Early stopping:  0.025706456971423704\n",
            "Epoch: 056, Loss: 0.3090, Train: 0.9620, Test: 0.6677\n",
            "Early stopping:  0.02491146129033659\n",
            "Epoch: 057, Loss: 0.2945, Train: 0.9643, Test: 0.6684\n",
            "Early stopping:  0.024082123313289593\n",
            "Epoch: 058, Loss: 0.2806, Train: 0.9705, Test: 0.6686\n",
            "Early stopping:  0.023250521332571918\n",
            "Epoch: 059, Loss: 0.2673, Train: 0.9750, Test: 0.6686\n",
            "Early stopping:  0.022385481496268157\n",
            "Epoch: 060, Loss: 0.2546, Train: 0.9767, Test: 0.6676\n",
            "Early stopping:  0.021511680977044152\n",
            "Epoch: 061, Loss: 0.2426, Train: 0.9818, Test: 0.6669\n",
            "Early stopping:  0.02055376999266738\n",
            "Epoch: 062, Loss: 0.2312, Train: 0.9870, Test: 0.6697\n",
            "Early stopping:  0.019542086325257956\n",
            "Epoch: 063, Loss: 0.2206, Train: 0.9898, Test: 0.6660\n",
            "Early stopping:  0.01849063806151574\n",
            "Epoch: 064, Loss: 0.2105, Train: 0.9904, Test: 0.6688\n",
            "Early stopping:  0.017419787484628024\n",
            "Epoch: 065, Loss: 0.2014, Train: 0.9921, Test: 0.6676\n",
            "Early stopping:  0.016317563684601005\n",
            "Epoch: 066, Loss: 0.1927, Train: 0.9932, Test: 0.6694\n",
            "Early stopping:  0.015222458480566166\n",
            "Epoch: 067, Loss: 0.1850, Train: 0.9938, Test: 0.6683\n",
            "Early stopping:  0.014086064063518143\n",
            "Epoch: 068, Loss: 0.1774, Train: 0.9943, Test: 0.6691\n",
            "Early stopping:  0.013086777679791337\n",
            "Epoch: 069, Loss: 0.1703, Train: 0.9949, Test: 0.6686\n",
            "Early stopping:  0.012261910424310718\n",
            "Epoch: 070, Loss: 0.1634, Train: 0.9955, Test: 0.6677\n",
            "Early stopping:  0.01160066255593286\n",
            "Epoch: 071, Loss: 0.1572, Train: 0.9960, Test: 0.6677\n",
            "Early stopping:  0.011000486600104747\n",
            "Epoch: 072, Loss: 0.1517, Train: 0.9966, Test: 0.6676\n",
            "Early stopping:  0.010192518175804462\n",
            "Epoch: 073, Loss: 0.1469, Train: 0.9966, Test: 0.6678\n",
            "Early stopping:  0.009269669244581068\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.65      0.69      0.67       758\n",
            "         capital_goods       0.53      0.53      0.53       508\n",
            "conglomerates_industry       0.86      0.23      0.36        80\n",
            "     consumer_cyclical       0.64      0.60      0.62       793\n",
            " consumer_non-cyclical       0.74      0.54      0.63       446\n",
            "                energy       0.79      0.71      0.75       283\n",
            "             financial       0.72      0.69      0.70       767\n",
            "            healthcare       0.74      0.73      0.74       318\n",
            "              services       0.64      0.76      0.70      2076\n",
            "            technology       0.59      0.45      0.51       396\n",
            "        transportation       0.79      0.74      0.76       404\n",
            "             utilities       0.79      0.68      0.73       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.71      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.67      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4619, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2496, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15011662197737383\n",
            "Epoch: 003, Loss: 2.1610, Train: 0.2989, Test: 0.2971\n",
            "Early stopping:  0.15462988570091535\n",
            "Epoch: 004, Loss: 2.1030, Train: 0.3517, Test: 0.3459\n",
            "Early stopping:  0.15735970305276076\n",
            "Epoch: 005, Loss: 2.0300, Train: 0.3795, Test: 0.3693\n",
            "Early stopping:  0.1664924428377002\n",
            "Epoch: 006, Loss: 1.9590, Train: 0.3919, Test: 0.3792\n",
            "Early stopping:  0.11280254892134418\n",
            "Epoch: 007, Loss: 1.8907, Train: 0.4039, Test: 0.3870\n",
            "Early stopping:  0.10832236096278373\n",
            "Epoch: 008, Loss: 1.8259, Train: 0.4146, Test: 0.4002\n",
            "Early stopping:  0.10969610271365396\n",
            "Epoch: 009, Loss: 1.7609, Train: 0.4362, Test: 0.4195\n",
            "Early stopping:  0.10618766561634961\n",
            "Epoch: 010, Loss: 1.6921, Train: 0.4617, Test: 0.4434\n",
            "Early stopping:  0.10492921202289703\n",
            "Epoch: 011, Loss: 1.6216, Train: 0.4878, Test: 0.4617\n",
            "Early stopping:  0.10626066621131888\n",
            "Epoch: 012, Loss: 1.5546, Train: 0.5088, Test: 0.4789\n",
            "Early stopping:  0.10782610746082705\n",
            "Epoch: 013, Loss: 1.4899, Train: 0.5411, Test: 0.5021\n",
            "Early stopping:  0.107456647325798\n",
            "Epoch: 014, Loss: 1.4250, Train: 0.5655, Test: 0.5250\n",
            "Early stopping:  0.10532714379695035\n",
            "Epoch: 015, Loss: 1.3605, Train: 0.5910, Test: 0.5462\n",
            "Early stopping:  0.10306614044452105\n",
            "Epoch: 016, Loss: 1.3003, Train: 0.6092, Test: 0.5602\n",
            "Early stopping:  0.1008896620207685\n",
            "Epoch: 017, Loss: 1.2462, Train: 0.6268, Test: 0.5744\n",
            "Early stopping:  0.09686044566782089\n",
            "Epoch: 018, Loss: 1.1955, Train: 0.6370, Test: 0.5832\n",
            "Early stopping:  0.09075516394204342\n",
            "Epoch: 019, Loss: 1.1474, Train: 0.6517, Test: 0.5913\n",
            "Early stopping:  0.08403977633336551\n",
            "Epoch: 020, Loss: 1.1036, Train: 0.6625, Test: 0.5987\n",
            "Early stopping:  0.07786416288626498\n",
            "Epoch: 021, Loss: 1.0632, Train: 0.6682, Test: 0.6018\n",
            "Early stopping:  0.07245733045551392\n",
            "Epoch: 022, Loss: 1.0262, Train: 0.6824, Test: 0.6046\n",
            "Early stopping:  0.0669477924146001\n",
            "Epoch: 023, Loss: 0.9911, Train: 0.6914, Test: 0.6107\n",
            "Early stopping:  0.061732911160644194\n",
            "Epoch: 024, Loss: 0.9562, Train: 0.7039, Test: 0.6153\n",
            "Early stopping:  0.058060256065695676\n",
            "Epoch: 025, Loss: 0.9213, Train: 0.7124, Test: 0.6225\n",
            "Early stopping:  0.05594243819572724\n",
            "Epoch: 026, Loss: 0.8883, Train: 0.7209, Test: 0.6242\n",
            "Early stopping:  0.05463526116877822\n",
            "Epoch: 027, Loss: 0.8553, Train: 0.7374, Test: 0.6280\n",
            "Early stopping:  0.05368658078813365\n",
            "Epoch: 028, Loss: 0.8237, Train: 0.7499, Test: 0.6304\n",
            "Early stopping:  0.05235504968439693\n",
            "Epoch: 029, Loss: 0.7937, Train: 0.7623, Test: 0.6327\n",
            "Early stopping:  0.050591289354735454\n",
            "Epoch: 030, Loss: 0.7641, Train: 0.7725, Test: 0.6364\n",
            "Early stopping:  0.049011449147775325\n",
            "Epoch: 031, Loss: 0.7361, Train: 0.7884, Test: 0.6406\n",
            "Early stopping:  0.04710477458591357\n",
            "Epoch: 032, Loss: 0.7094, Train: 0.7992, Test: 0.6436\n",
            "Early stopping:  0.04523381438958651\n",
            "Epoch: 033, Loss: 0.6829, Train: 0.8066, Test: 0.6463\n",
            "Early stopping:  0.04369984112187693\n",
            "Epoch: 034, Loss: 0.6574, Train: 0.8174, Test: 0.6490\n",
            "Early stopping:  0.04219278411015569\n",
            "Epoch: 035, Loss: 0.6327, Train: 0.8276, Test: 0.6511\n",
            "Early stopping:  0.04095048490722113\n",
            "Epoch: 036, Loss: 0.6089, Train: 0.8389, Test: 0.6520\n",
            "Early stopping:  0.03973230282605902\n",
            "Epoch: 037, Loss: 0.5857, Train: 0.8457, Test: 0.6524\n",
            "Early stopping:  0.03839299917835646\n",
            "Epoch: 038, Loss: 0.5634, Train: 0.8576, Test: 0.6538\n",
            "Early stopping:  0.03715345618283197\n",
            "Epoch: 039, Loss: 0.5413, Train: 0.8701, Test: 0.6585\n",
            "Early stopping:  0.036119905652250976\n",
            "Epoch: 040, Loss: 0.5192, Train: 0.8832, Test: 0.6592\n",
            "Early stopping:  0.03541371049149868\n",
            "Epoch: 041, Loss: 0.4978, Train: 0.8962, Test: 0.6616\n",
            "Early stopping:  0.0347860388561546\n",
            "Epoch: 042, Loss: 0.4771, Train: 0.9030, Test: 0.6613\n",
            "Early stopping:  0.03417012828166247\n",
            "Epoch: 043, Loss: 0.4569, Train: 0.9098, Test: 0.6636\n",
            "Early stopping:  0.0333378857016847\n",
            "Epoch: 044, Loss: 0.4372, Train: 0.9155, Test: 0.6653\n",
            "Early stopping:  0.03239146267304489\n",
            "Epoch: 045, Loss: 0.4181, Train: 0.9257, Test: 0.6659\n",
            "Early stopping:  0.03153211141469766\n",
            "Epoch: 046, Loss: 0.3991, Train: 0.9308, Test: 0.6674\n",
            "Early stopping:  0.030797441739573127\n",
            "Epoch: 047, Loss: 0.3807, Train: 0.9370, Test: 0.6676\n",
            "Early stopping:  0.030133458976915595\n",
            "Epoch: 048, Loss: 0.3631, Train: 0.9382, Test: 0.6691\n",
            "Early stopping:  0.02933112232192005\n",
            "Epoch: 049, Loss: 0.3478, Train: 0.9455, Test: 0.6620\n",
            "Early stopping:  0.027926124672164158\n",
            "Epoch: 050, Loss: 0.3383, Train: 0.9291, Test: 0.6678\n",
            "Early stopping:  0.024576808453860597\n",
            "Epoch: 051, Loss: 0.3329, Train: 0.9529, Test: 0.6593\n",
            "Early stopping:  0.01944584977253254\n",
            "Epoch: 052, Loss: 0.3123, Train: 0.9580, Test: 0.6690\n",
            "Early stopping:  0.01876513164143786\n",
            "Epoch: 053, Loss: 0.2877, Train: 0.9507, Test: 0.6677\n",
            "Early stopping:  0.02400217080344944\n",
            "Epoch: 054, Loss: 0.2880, Train: 0.9716, Test: 0.6622\n",
            "Early stopping:  0.023975833302735592\n",
            "Epoch: 055, Loss: 0.2706, Train: 0.9739, Test: 0.6644\n",
            "Early stopping:  0.024411244763495436\n",
            "Epoch: 056, Loss: 0.2543, Train: 0.9626, Test: 0.6666\n",
            "Early stopping:  0.02168851448122901\n",
            "Epoch: 057, Loss: 0.2531, Train: 0.9779, Test: 0.6629\n",
            "Early stopping:  0.017074425116550412\n",
            "Epoch: 058, Loss: 0.2340, Train: 0.9807, Test: 0.6608\n",
            "Early stopping:  0.020324358956545243\n",
            "Epoch: 059, Loss: 0.2292, Train: 0.9801, Test: 0.6697\n",
            "Early stopping:  0.016771720821098436\n",
            "Epoch: 060, Loss: 0.2206, Train: 0.9875, Test: 0.6670\n",
            "Early stopping:  0.014907887778426688\n",
            "Epoch: 061, Loss: 0.2080, Train: 0.9915, Test: 0.6596\n",
            "Early stopping:  0.0166916068260092\n",
            "Epoch: 062, Loss: 0.2059, Train: 0.9909, Test: 0.6642\n",
            "Early stopping:  0.012463841022755507\n",
            "Epoch: 063, Loss: 0.1935, Train: 0.9909, Test: 0.6674\n",
            "Early stopping:  0.013820242928518005\n",
            "Epoch: 064, Loss: 0.1898, Train: 0.9943, Test: 0.6596\n",
            "Early stopping:  0.012322632431297933\n",
            "Epoch: 065, Loss: 0.1833, Train: 0.9960, Test: 0.6626\n",
            "Early stopping:  0.010590493731334754\n",
            "Epoch: 066, Loss: 0.1746, Train: 0.9943, Test: 0.6627\n",
            "Early stopping:  0.011693757901725254\n",
            "Epoch: 067, Loss: 0.1729, Train: 0.9972, Test: 0.6608\n",
            "Early stopping:  0.009076323477348103\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.65      0.68      0.66       758\n",
            "         capital_goods       0.53      0.54      0.54       508\n",
            "conglomerates_industry       1.00      0.20      0.33        80\n",
            "     consumer_cyclical       0.56      0.61      0.58       793\n",
            " consumer_non-cyclical       0.71      0.55      0.62       446\n",
            "                energy       0.74      0.67      0.70       283\n",
            "             financial       0.75      0.69      0.72       767\n",
            "            healthcare       0.73      0.72      0.73       318\n",
            "              services       0.66      0.73      0.70      2076\n",
            "            technology       0.56      0.52      0.54       396\n",
            "        transportation       0.75      0.75      0.75       404\n",
            "             utilities       0.81      0.66      0.73       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.71      0.61      0.63      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "time: 2min 11s (started: 2024-10-16 21:15:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJo0jT6FgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e7e0df-c444-4d84-8dc1-5a7ad2109e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 480 ms (started: 2024-10-16 21:17:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMXRTt74gSl-"
      },
      "source": [
        "### Training rotulated base = 40% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7K2nquSgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e3c5f3-1ced-4e9d-8393-0d67954db515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 470 µs (started: 2024-10-16 21:17:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYMlDhagSl-"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1th-kiFgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7daea39-df3b-4403-abbf-6c6758b18847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 79.6370, Train: 0.0227, Test: 0.0202\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 322.6305, Train: 0.0635, Test: 0.0639\n",
            "Early stopping:  171.82235146161983\n",
            "Epoch: 003, Loss: 383.2128, Train: 0.0712, Test: 0.0652\n",
            "Early stopping:  160.66234079446485\n",
            "Epoch: 004, Loss: 405.4357, Train: 0.2944, Test: 0.2947\n",
            "Early stopping:  149.54644842997007\n",
            "Epoch: 005, Loss: 338.2818, Train: 0.2893, Test: 0.2854\n",
            "Early stopping:  130.77465788671057\n",
            "Epoch: 006, Loss: 272.8007, Train: 0.1069, Test: 0.0977\n",
            "Early stopping:  52.12861232183127\n",
            "Epoch: 007, Loss: 278.4528, Train: 0.1262, Test: 0.1225\n",
            "Early stopping:  59.91815756107001\n",
            "Epoch: 008, Loss: 220.3753, Train: 0.0763, Test: 0.0737\n",
            "Early stopping:  70.8506510495985\n",
            "Epoch: 009, Loss: 230.4311, Train: 0.1466, Test: 0.1427\n",
            "Early stopping:  46.77432733544935\n",
            "Epoch: 010, Loss: 154.4404, Train: 0.1339, Test: 0.1213\n",
            "Early stopping:  49.93295046524715\n",
            "Epoch: 011, Loss: 131.1138, Train: 0.2896, Test: 0.2920\n",
            "Early stopping:  59.73513569798399\n",
            "Epoch: 012, Loss: 110.4703, Train: 0.1860, Test: 0.1778\n",
            "Early stopping:  53.58545397939438\n",
            "Epoch: 013, Loss: 101.1846, Train: 0.2791, Test: 0.2844\n",
            "Early stopping:  51.69892058807401\n",
            "Epoch: 014, Loss: 76.4316, Train: 0.1852, Test: 0.1831\n",
            "Early stopping:  29.638364997458815\n",
            "Epoch: 015, Loss: 56.8298, Train: 0.1912, Test: 0.1898\n",
            "Early stopping:  29.083157359323344\n",
            "Epoch: 016, Loss: 37.2992, Train: 0.2745, Test: 0.2625\n",
            "Early stopping:  30.360005059503653\n",
            "Epoch: 017, Loss: 21.4751, Train: 0.2569, Test: 0.2415\n",
            "Early stopping:  31.490183384613676\n",
            "Epoch: 018, Loss: 12.6546, Train: 0.2595, Test: 0.2444\n",
            "Early stopping:  26.00164843729656\n",
            "Epoch: 019, Loss: 5.5052, Train: 0.2672, Test: 0.2504\n",
            "Early stopping:  20.578650067810724\n",
            "Epoch: 020, Loss: 3.4420, Train: 0.2408, Test: 0.2306\n",
            "Early stopping:  13.807929239701327\n",
            "Epoch: 021, Loss: 3.0464, Train: 0.2385, Test: 0.2285\n",
            "Early stopping:  7.862570774295166\n",
            "Epoch: 022, Loss: 2.8282, Train: 0.2516, Test: 0.2400\n",
            "Early stopping:  4.140716566918062\n",
            "Epoch: 023, Loss: 2.6311, Train: 0.2845, Test: 0.2627\n",
            "Early stopping:  1.1657570529592953\n",
            "Epoch: 024, Loss: 2.3805, Train: 0.3063, Test: 0.2933\n",
            "Early stopping:  0.4051817166543458\n",
            "Epoch: 025, Loss: 2.2211, Train: 0.3219, Test: 0.3147\n",
            "Early stopping:  0.3323298227795644\n",
            "Epoch: 026, Loss: 2.1724, Train: 0.3131, Test: 0.3047\n",
            "Early stopping:  0.2784000592453848\n",
            "Epoch: 027, Loss: 2.1556, Train: 0.3023, Test: 0.2905\n",
            "Early stopping:  0.19919190225544198\n",
            "Epoch: 028, Loss: 2.1385, Train: 0.3035, Test: 0.2818\n",
            "Early stopping:  0.09824524491987635\n",
            "Epoch: 029, Loss: 2.1343, Train: 0.3003, Test: 0.2769\n",
            "Early stopping:  0.03512582945705725\n",
            "Epoch: 030, Loss: 2.1206, Train: 0.2995, Test: 0.2763\n",
            "Early stopping:  0.020113914305581584\n",
            "Epoch: 031, Loss: 2.1010, Train: 0.3009, Test: 0.2835\n",
            "Early stopping:  0.020474176433643215\n",
            "Epoch: 032, Loss: 2.0951, Train: 0.3046, Test: 0.2945\n",
            "Early stopping:  0.01940282628013043\n",
            "Epoch: 033, Loss: 2.0876, Train: 0.3174, Test: 0.3032\n",
            "Early stopping:  0.01922026992941544\n",
            "Epoch: 034, Loss: 2.0754, Train: 0.3253, Test: 0.3130\n",
            "Early stopping:  0.016750640850988076\n",
            "Epoch: 035, Loss: 2.0617, Train: 0.3454, Test: 0.3279\n",
            "Early stopping:  0.015767843716777617\n",
            "Epoch: 036, Loss: 2.0633, Train: 0.3571, Test: 0.3368\n",
            "Early stopping:  0.014685206363513073\n",
            "Epoch: 037, Loss: 2.0346, Train: 0.3656, Test: 0.3438\n",
            "Early stopping:  0.01973289200086276\n",
            "Epoch: 038, Loss: 2.0192, Train: 0.3664, Test: 0.3472\n",
            "Early stopping:  0.023136358300484435\n",
            "Epoch: 039, Loss: 2.0095, Train: 0.3673, Test: 0.3570\n",
            "Early stopping:  0.02437973066959241\n",
            "Epoch: 040, Loss: 1.9957, Train: 0.3707, Test: 0.3570\n",
            "Early stopping:  0.025933682026715296\n",
            "Epoch: 041, Loss: 1.9767, Train: 0.3755, Test: 0.3561\n",
            "Early stopping:  0.022157371013678293\n",
            "Epoch: 042, Loss: 1.9551, Train: 0.3729, Test: 0.3542\n",
            "Early stopping:  0.025751793189507117\n",
            "Epoch: 043, Loss: 1.9423, Train: 0.3727, Test: 0.3561\n",
            "Early stopping:  0.027769069159688874\n",
            "Epoch: 044, Loss: 1.9310, Train: 0.3798, Test: 0.3659\n",
            "Early stopping:  0.026116974279516474\n",
            "Epoch: 045, Loss: 1.9122, Train: 0.3922, Test: 0.3767\n",
            "Early stopping:  0.024367756876973467\n",
            "Epoch: 046, Loss: 1.8931, Train: 0.4005, Test: 0.3835\n",
            "Early stopping:  0.024533744257958404\n",
            "Epoch: 047, Loss: 1.8800, Train: 0.4053, Test: 0.3899\n",
            "Early stopping:  0.02577025057094607\n",
            "Epoch: 048, Loss: 1.8637, Train: 0.4146, Test: 0.3967\n",
            "Early stopping:  0.02643143872196766\n",
            "Epoch: 049, Loss: 1.8433, Train: 0.4229, Test: 0.4007\n",
            "Early stopping:  0.026502452924247126\n",
            "Epoch: 050, Loss: 1.8236, Train: 0.4288, Test: 0.3977\n",
            "Early stopping:  0.027912449155560754\n",
            "Epoch: 051, Loss: 1.8020, Train: 0.4336, Test: 0.4050\n",
            "Early stopping:  0.03106136306144548\n",
            "Epoch: 052, Loss: 1.7793, Train: 0.4396, Test: 0.4101\n",
            "Early stopping:  0.03324742330688029\n",
            "Epoch: 053, Loss: 1.7618, Train: 0.4413, Test: 0.4120\n",
            "Early stopping:  0.032799004730419946\n",
            "Epoch: 054, Loss: 1.7473, Train: 0.4427, Test: 0.4154\n",
            "Early stopping:  0.030598621005544707\n",
            "Epoch: 055, Loss: 1.7335, Train: 0.4413, Test: 0.4143\n",
            "Early stopping:  0.026865846368293915\n",
            "Epoch: 056, Loss: 1.7202, Train: 0.4424, Test: 0.4143\n",
            "Early stopping:  0.02317992469786234\n",
            "Epoch: 057, Loss: 1.7064, Train: 0.4410, Test: 0.4166\n",
            "Early stopping:  0.021784845382687826\n",
            "Epoch: 058, Loss: 1.6929, Train: 0.4436, Test: 0.4211\n",
            "Early stopping:  0.021488768253348344\n",
            "Epoch: 059, Loss: 1.6809, Train: 0.4498, Test: 0.4239\n",
            "Early stopping:  0.020968941137744695\n",
            "Epoch: 060, Loss: 1.6692, Train: 0.4526, Test: 0.4262\n",
            "Early stopping:  0.020194825167608704\n",
            "Epoch: 061, Loss: 1.6581, Train: 0.4597, Test: 0.4275\n",
            "Early stopping:  0.019031665988464508\n",
            "Epoch: 062, Loss: 1.6459, Train: 0.4674, Test: 0.4322\n",
            "Early stopping:  0.018440640915783898\n",
            "Epoch: 063, Loss: 1.6341, Train: 0.4733, Test: 0.4377\n",
            "Early stopping:  0.018483916199531932\n",
            "Epoch: 064, Loss: 1.6226, Train: 0.4762, Test: 0.4421\n",
            "Early stopping:  0.01855100264726182\n",
            "Epoch: 065, Loss: 1.6103, Train: 0.4784, Test: 0.4460\n",
            "Early stopping:  0.018839887002363855\n",
            "Epoch: 066, Loss: 1.5996, Train: 0.4821, Test: 0.4511\n",
            "Early stopping:  0.018431737590818783\n",
            "Epoch: 067, Loss: 1.5864, Train: 0.4875, Test: 0.4530\n",
            "Early stopping:  0.018716601204712545\n",
            "Epoch: 068, Loss: 1.5746, Train: 0.4881, Test: 0.4534\n",
            "Early stopping:  0.01894418323313176\n",
            "Epoch: 069, Loss: 1.5616, Train: 0.4901, Test: 0.4544\n",
            "Early stopping:  0.019352313562577404\n",
            "Epoch: 070, Loss: 1.5491, Train: 0.4955, Test: 0.4568\n",
            "Early stopping:  0.019872618338772366\n",
            "Epoch: 071, Loss: 1.5367, Train: 0.5020, Test: 0.4619\n",
            "Early stopping:  0.019751540260597795\n",
            "Epoch: 072, Loss: 1.5260, Train: 0.5037, Test: 0.4661\n",
            "Early stopping:  0.01930618753634238\n",
            "Epoch: 073, Loss: 1.5158, Train: 0.5074, Test: 0.4665\n",
            "Early stopping:  0.018142724067271906\n",
            "Epoch: 074, Loss: 1.5073, Train: 0.5099, Test: 0.4734\n",
            "Early stopping:  0.01657212891108006\n",
            "Epoch: 075, Loss: 1.4975, Train: 0.5145, Test: 0.4753\n",
            "Early stopping:  0.015352745735936561\n",
            "Epoch: 076, Loss: 1.4869, Train: 0.5128, Test: 0.4763\n",
            "Early stopping:  0.015281406186136883\n",
            "Epoch: 077, Loss: 1.4798, Train: 0.5199, Test: 0.4782\n",
            "Early stopping:  0.014633363305967928\n",
            "Epoch: 078, Loss: 1.4782, Train: 0.5252, Test: 0.4820\n",
            "Early stopping:  0.012319531035768692\n",
            "Epoch: 079, Loss: 1.4664, Train: 0.5247, Test: 0.4837\n",
            "Early stopping:  0.011471419930104995\n",
            "Epoch: 080, Loss: 1.4547, Train: 0.5235, Test: 0.4838\n",
            "Early stopping:  0.01269183421399212\n",
            "Epoch: 081, Loss: 1.4464, Train: 0.5332, Test: 0.4855\n",
            "Early stopping:  0.014565564183288528\n",
            "Epoch: 082, Loss: 1.4400, Train: 0.5360, Test: 0.4874\n",
            "Early stopping:  0.015380224320235867\n",
            "Epoch: 083, Loss: 1.4341, Train: 0.5323, Test: 0.4880\n",
            "Early stopping:  0.012675790222762912\n",
            "Epoch: 084, Loss: 1.4216, Train: 0.5352, Test: 0.4884\n",
            "Early stopping:  0.01254308520577769\n",
            "Epoch: 085, Loss: 1.4195, Train: 0.5355, Test: 0.4889\n",
            "Early stopping:  0.011626300530264989\n",
            "Epoch: 086, Loss: 1.4107, Train: 0.5352, Test: 0.4880\n",
            "Early stopping:  0.011782885107760384\n",
            "Epoch: 087, Loss: 1.4050, Train: 0.5403, Test: 0.4897\n",
            "Early stopping:  0.011155257048837295\n",
            "Epoch: 088, Loss: 1.3981, Train: 0.5386, Test: 0.4927\n",
            "Early stopping:  0.009844219049291328\n",
            "PREDICTIONS -> tensor([10, 11,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.51      0.52      0.51       568\n",
            "         capital_goods       0.36      0.06      0.10       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.45      0.24      0.31       595\n",
            " consumer_non-cyclical       0.57      0.35      0.44       334\n",
            "                energy       0.46      0.42      0.44       213\n",
            "             financial       0.68      0.51      0.58       576\n",
            "            healthcare       0.54      0.52      0.53       238\n",
            "              services       0.46      0.79      0.58      1557\n",
            "            technology       0.44      0.11      0.17       297\n",
            "        transportation       0.45      0.54      0.49       303\n",
            "             utilities       0.62      0.57      0.60       169\n",
            "\n",
            "              accuracy                           0.49      5291\n",
            "             macro avg       0.46      0.39      0.40      5291\n",
            "          weighted avg       0.49      0.49      0.46      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 68.2805, Train: 0.1149, Test: 0.1125\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 542.1357, Train: 0.0763, Test: 0.0765\n",
            "Early stopping:  335.06625880635346\n",
            "Epoch: 003, Loss: 651.7943, Train: 0.0499, Test: 0.0474\n",
            "Early stopping:  310.12150501311834\n",
            "Epoch: 004, Loss: 607.6354, Train: 0.1095, Test: 0.1040\n",
            "Early stopping:  269.9067820356782\n",
            "Epoch: 005, Loss: 403.8134, Train: 0.1086, Test: 0.1092\n",
            "Early stopping:  235.47286312583893\n",
            "Epoch: 006, Loss: 278.1821, Train: 0.1171, Test: 0.1164\n",
            "Early stopping:  153.9507228096973\n",
            "Epoch: 007, Loss: 127.2587, Train: 0.1146, Test: 0.1119\n",
            "Early stopping:  220.68716972500516\n",
            "Epoch: 008, Loss: 56.8997, Train: 0.1438, Test: 0.1283\n",
            "Early stopping:  220.6200038253517\n",
            "Epoch: 009, Loss: 24.0395, Train: 0.1049, Test: 0.1015\n",
            "Early stopping:  159.6652219139419\n",
            "Epoch: 010, Loss: 6.0467, Train: 0.0893, Test: 0.0843\n",
            "Early stopping:  110.60660571924369\n",
            "Epoch: 011, Loss: 4.1722, Train: 0.1200, Test: 0.1096\n",
            "Early stopping:  51.29092092576878\n",
            "Epoch: 012, Loss: 3.4187, Train: 0.1747, Test: 0.1597\n",
            "Early stopping:  22.8701507363151\n",
            "Epoch: 013, Loss: 3.0441, Train: 0.1736, Test: 0.1631\n",
            "Early stopping:  8.96073858399661\n",
            "Epoch: 014, Loss: 2.8925, Train: 0.1707, Test: 0.1610\n",
            "Early stopping:  1.2903964790365432\n",
            "Epoch: 015, Loss: 2.7942, Train: 0.1727, Test: 0.1659\n",
            "Early stopping:  0.5603320706889101\n",
            "Epoch: 016, Loss: 2.7155, Train: 0.1730, Test: 0.1684\n",
            "Early stopping:  0.27769571632045364\n",
            "Epoch: 017, Loss: 2.6681, Train: 0.1792, Test: 0.1701\n",
            "Early stopping:  0.15006351429794895\n",
            "Epoch: 018, Loss: 2.6146, Train: 0.1784, Test: 0.1705\n",
            "Early stopping:  0.10912455476585141\n",
            "Epoch: 019, Loss: 2.5581, Train: 0.1821, Test: 0.1756\n",
            "Early stopping:  0.09093514239523458\n",
            "Epoch: 020, Loss: 2.5028, Train: 0.1835, Test: 0.1763\n",
            "Early stopping:  0.08470811381207619\n",
            "Epoch: 021, Loss: 2.4445, Train: 0.1883, Test: 0.1782\n",
            "Early stopping:  0.08841012267654585\n",
            "Epoch: 022, Loss: 2.3865, Train: 0.1926, Test: 0.1796\n",
            "Early stopping:  0.09010243084471463\n",
            "Epoch: 023, Loss: 2.3284, Train: 0.1982, Test: 0.1858\n",
            "Early stopping:  0.09104570610762527\n",
            "Epoch: 024, Loss: 2.2696, Train: 0.2124, Test: 0.1994\n",
            "Early stopping:  0.09210797779843903\n",
            "Epoch: 025, Loss: 2.2167, Train: 0.2314, Test: 0.2181\n",
            "Early stopping:  0.09052722161230549\n",
            "Epoch: 026, Loss: 2.1723, Train: 0.2487, Test: 0.2340\n",
            "Early stopping:  0.08552767726074288\n",
            "Epoch: 027, Loss: 2.1482, Train: 0.2674, Test: 0.2470\n",
            "Early stopping:  0.07313224601219691\n",
            "Epoch: 028, Loss: 2.0948, Train: 0.2847, Test: 0.2657\n",
            "Early stopping:  0.06653157110821196\n",
            "Epoch: 029, Loss: 2.0555, Train: 0.3154, Test: 0.3030\n",
            "Early stopping:  0.06355701255207402\n",
            "Epoch: 030, Loss: 2.0153, Train: 0.3494, Test: 0.3338\n",
            "Early stopping:  0.064596636163681\n",
            "Epoch: 031, Loss: 1.9706, Train: 0.3551, Test: 0.3372\n",
            "Early stopping:  0.0688311939119969\n",
            "Epoch: 032, Loss: 1.9603, Train: 0.3568, Test: 0.3445\n",
            "Early stopping:  0.05671193537803504\n",
            "Epoch: 033, Loss: 1.9289, Train: 0.3582, Test: 0.3432\n",
            "Early stopping:  0.04959395496515198\n",
            "Epoch: 034, Loss: 1.9011, Train: 0.3647, Test: 0.3519\n",
            "Early stopping:  0.04330193035931818\n",
            "Epoch: 035, Loss: 1.8770, Train: 0.3644, Test: 0.3555\n",
            "Early stopping:  0.039330942733334345\n",
            "Epoch: 036, Loss: 1.8505, Train: 0.3710, Test: 0.3617\n",
            "Early stopping:  0.04300732775217787\n",
            "Epoch: 037, Loss: 1.8325, Train: 0.3749, Test: 0.3674\n",
            "Early stopping:  0.03858486230365048\n",
            "Epoch: 038, Loss: 1.8104, Train: 0.3843, Test: 0.3714\n",
            "Early stopping:  0.03578722828511558\n",
            "Epoch: 039, Loss: 1.7914, Train: 0.3877, Test: 0.3712\n",
            "Early stopping:  0.03347082503489314\n",
            "Epoch: 040, Loss: 1.7775, Train: 0.4124, Test: 0.3963\n",
            "Early stopping:  0.02965161427197623\n",
            "Epoch: 041, Loss: 1.7556, Train: 0.4231, Test: 0.4031\n",
            "Early stopping:  0.029604483161631705\n",
            "Epoch: 042, Loss: 1.7357, Train: 0.4226, Test: 0.4037\n",
            "Early stopping:  0.02934004958580872\n",
            "Epoch: 043, Loss: 1.7237, Train: 0.4334, Test: 0.4113\n",
            "Early stopping:  0.028138448748436847\n",
            "Epoch: 044, Loss: 1.7027, Train: 0.4470, Test: 0.4167\n",
            "Early stopping:  0.028804471213321595\n",
            "Epoch: 045, Loss: 1.6821, Train: 0.4470, Test: 0.4224\n",
            "Early stopping:  0.028543999620658723\n",
            "Epoch: 046, Loss: 1.6699, Train: 0.4472, Test: 0.4200\n",
            "Early stopping:  0.02751535224953957\n",
            "Epoch: 047, Loss: 1.6491, Train: 0.4543, Test: 0.4254\n",
            "Early stopping:  0.028886073648939276\n",
            "Epoch: 048, Loss: 1.6348, Train: 0.4538, Test: 0.4275\n",
            "Early stopping:  0.026784797425843988\n",
            "Epoch: 049, Loss: 1.6169, Train: 0.4560, Test: 0.4330\n",
            "Early stopping:  0.02626002523913978\n",
            "Epoch: 050, Loss: 1.6131, Train: 0.4640, Test: 0.4341\n",
            "Early stopping:  0.023505599346744697\n",
            "Epoch: 051, Loss: 1.5938, Train: 0.4694, Test: 0.4370\n",
            "Early stopping:  0.021173313342668823\n",
            "Epoch: 052, Loss: 1.5801, Train: 0.4745, Test: 0.4402\n",
            "Early stopping:  0.021214920265174655\n",
            "Epoch: 053, Loss: 1.5707, Train: 0.4753, Test: 0.4443\n",
            "Early stopping:  0.02011945883947787\n",
            "Epoch: 054, Loss: 1.5586, Train: 0.4816, Test: 0.4487\n",
            "Early stopping:  0.021091143169152955\n",
            "Epoch: 055, Loss: 1.5448, Train: 0.4847, Test: 0.4508\n",
            "Early stopping:  0.018931792797497942\n",
            "Epoch: 056, Loss: 1.5360, Train: 0.4875, Test: 0.4504\n",
            "Early stopping:  0.018084406974443683\n",
            "Epoch: 057, Loss: 1.5279, Train: 0.4929, Test: 0.4530\n",
            "Early stopping:  0.017218343976877153\n",
            "Epoch: 058, Loss: 1.5159, Train: 0.4949, Test: 0.4555\n",
            "Early stopping:  0.016236436056099857\n",
            "Epoch: 059, Loss: 1.5056, Train: 0.4952, Test: 0.4568\n",
            "Early stopping:  0.015630981176443747\n",
            "Epoch: 060, Loss: 1.4975, Train: 0.4989, Test: 0.4598\n",
            "Early stopping:  0.015724405461275286\n",
            "Epoch: 061, Loss: 1.4858, Train: 0.5051, Test: 0.4631\n",
            "Early stopping:  0.016253323470349322\n",
            "Epoch: 062, Loss: 1.4774, Train: 0.5085, Test: 0.4653\n",
            "Early stopping:  0.015316716721504716\n",
            "Epoch: 063, Loss: 1.4677, Train: 0.5136, Test: 0.4699\n",
            "Early stopping:  0.015153685969391702\n",
            "Epoch: 064, Loss: 1.4550, Train: 0.5162, Test: 0.4729\n",
            "Early stopping:  0.016334514794507937\n",
            "Epoch: 065, Loss: 1.4453, Train: 0.5170, Test: 0.4733\n",
            "Early stopping:  0.01637369866619829\n",
            "Epoch: 066, Loss: 1.4364, Train: 0.5221, Test: 0.4736\n",
            "Early stopping:  0.016562064053013168\n",
            "Epoch: 067, Loss: 1.4282, Train: 0.5286, Test: 0.4757\n",
            "Early stopping:  0.015514854178270476\n",
            "Epoch: 068, Loss: 1.4197, Train: 0.5272, Test: 0.4761\n",
            "Early stopping:  0.013888430763890812\n",
            "Epoch: 069, Loss: 1.4112, Train: 0.5289, Test: 0.4782\n",
            "Early stopping:  0.013421442998597204\n",
            "Epoch: 070, Loss: 1.4047, Train: 0.5301, Test: 0.4810\n",
            "Early stopping:  0.012712324925531284\n",
            "Epoch: 071, Loss: 1.3956, Train: 0.5292, Test: 0.4812\n",
            "Early stopping:  0.01269792147018781\n",
            "Epoch: 072, Loss: 1.3896, Train: 0.5366, Test: 0.4820\n",
            "Early stopping:  0.012020585419409422\n",
            "Epoch: 073, Loss: 1.3827, Train: 0.5349, Test: 0.4816\n",
            "Early stopping:  0.011433725195889215\n",
            "Epoch: 074, Loss: 1.3736, Train: 0.5349, Test: 0.4831\n",
            "Early stopping:  0.011905675852244382\n",
            "Epoch: 075, Loss: 1.3671, Train: 0.5400, Test: 0.4840\n",
            "Early stopping:  0.011572915694618154\n",
            "Epoch: 076, Loss: 1.3582, Train: 0.5437, Test: 0.4827\n",
            "Early stopping:  0.012397984557393741\n",
            "Epoch: 077, Loss: 1.3537, Train: 0.5408, Test: 0.4876\n",
            "Early stopping:  0.01165494944541892\n",
            "Epoch: 078, Loss: 1.3417, Train: 0.5428, Test: 0.4816\n",
            "Early stopping:  0.01228761847214681\n",
            "Epoch: 079, Loss: 1.3513, Train: 0.5431, Test: 0.4884\n",
            "Early stopping:  0.009292013019400202\n",
            "PREDICTIONS -> tensor([8, 0, 8,  ..., 6, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.46      0.62      0.52       568\n",
            "         capital_goods       0.21      0.01      0.02       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.39      0.50      0.44       595\n",
            " consumer_non-cyclical       0.69      0.34      0.46       334\n",
            "                energy       0.00      0.00      0.00       213\n",
            "             financial       0.59      0.60      0.59       576\n",
            "            healthcare       0.65      0.42      0.51       238\n",
            "              services       0.47      0.77      0.58      1557\n",
            "            technology       0.00      0.00      0.00       297\n",
            "        transportation       0.67      0.58      0.62       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.49      5291\n",
            "             macro avg       0.34      0.32      0.31      5291\n",
            "          weighted avg       0.42      0.49      0.43      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 60.7672, Train: 0.0513, Test: 0.0505\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 288.2207, Train: 0.1358, Test: 0.1327\n",
            "Early stopping:  160.83386018403752\n",
            "Epoch: 003, Loss: 334.5786, Train: 0.2093, Test: 0.2049\n",
            "Early stopping:  146.54734471680848\n",
            "Epoch: 004, Loss: 440.8289, Train: 0.2941, Test: 0.2939\n",
            "Early stopping:  160.17750522715096\n",
            "Epoch: 005, Loss: 451.9881, Train: 0.3029, Test: 0.2958\n",
            "Early stopping:  158.37692548411215\n",
            "Epoch: 006, Loss: 298.4916, Train: 0.0686, Test: 0.0637\n",
            "Early stopping:  78.32163166820658\n",
            "Epoch: 007, Loss: 308.2779, Train: 0.0760, Test: 0.0739\n",
            "Early stopping:  73.93641326937744\n",
            "Epoch: 008, Loss: 199.1404, Train: 0.1151, Test: 0.1164\n",
            "Early stopping:  106.3933018945527\n",
            "Epoch: 009, Loss: 181.5539, Train: 0.1202, Test: 0.1196\n",
            "Early stopping:  107.98159755237779\n",
            "Epoch: 010, Loss: 127.1104, Train: 0.2405, Test: 0.2281\n",
            "Early stopping:  78.18749587683266\n",
            "Epoch: 011, Loss: 49.7233, Train: 0.2544, Test: 0.2493\n",
            "Early stopping:  95.34077664334059\n",
            "Epoch: 012, Loss: 43.0617, Train: 0.2345, Test: 0.2370\n",
            "Early stopping:  72.38936850203459\n",
            "Epoch: 013, Loss: 34.8982, Train: 0.1282, Test: 0.1278\n",
            "Early stopping:  64.38873687065184\n",
            "Epoch: 014, Loss: 24.7149, Train: 0.1330, Test: 0.1263\n",
            "Early stopping:  40.888520310155606\n",
            "Epoch: 015, Loss: 14.9763, Train: 0.1585, Test: 0.1472\n",
            "Early stopping:  13.936301518841109\n",
            "Epoch: 016, Loss: 10.3083, Train: 0.1716, Test: 0.1550\n",
            "Early stopping:  13.590894827750382\n",
            "Epoch: 017, Loss: 7.2779, Train: 0.1631, Test: 0.1555\n",
            "Early stopping:  11.324327218449687\n",
            "Epoch: 018, Loss: 5.3263, Train: 0.1693, Test: 0.1569\n",
            "Early stopping:  7.727675486981634\n",
            "Epoch: 019, Loss: 4.1920, Train: 0.1815, Test: 0.1743\n",
            "Early stopping:  4.338824585650949\n",
            "Epoch: 020, Loss: 3.4262, Train: 0.2155, Test: 0.2115\n",
            "Early stopping:  2.7606884201189503\n",
            "Epoch: 021, Loss: 2.9548, Train: 0.2326, Test: 0.2306\n",
            "Early stopping:  1.727955622800893\n",
            "Epoch: 022, Loss: 2.7145, Train: 0.2445, Test: 0.2400\n",
            "Early stopping:  1.0589927881646748\n",
            "Epoch: 023, Loss: 2.5740, Train: 0.2507, Test: 0.2423\n",
            "Early stopping:  0.6555642522208092\n",
            "Epoch: 024, Loss: 2.4790, Train: 0.2445, Test: 0.2368\n",
            "Early stopping:  0.37852701316085036\n",
            "Epoch: 025, Loss: 2.4120, Train: 0.2422, Test: 0.2327\n",
            "Early stopping:  0.21568309582979098\n",
            "Epoch: 026, Loss: 2.3618, Train: 0.2382, Test: 0.2310\n",
            "Early stopping:  0.140029533401382\n",
            "Epoch: 027, Loss: 2.3230, Train: 0.2493, Test: 0.2380\n",
            "Early stopping:  0.09945550194684669\n",
            "Epoch: 028, Loss: 2.3041, Train: 0.2635, Test: 0.2506\n",
            "Early stopping:  0.07087337294361666\n",
            "Epoch: 029, Loss: 2.2875, Train: 0.3256, Test: 0.3107\n",
            "Early stopping:  0.04990234537858426\n",
            "Epoch: 030, Loss: 2.2549, Train: 0.3366, Test: 0.3262\n",
            "Early stopping:  0.039858078239673536\n",
            "Epoch: 031, Loss: 2.2195, Train: 0.3420, Test: 0.3319\n",
            "Early stopping:  0.04106017385440371\n",
            "Epoch: 032, Loss: 2.1877, Train: 0.3043, Test: 0.3011\n",
            "Early stopping:  0.04786225111079786\n",
            "Epoch: 033, Loss: 2.1621, Train: 0.3117, Test: 0.3041\n",
            "Early stopping:  0.05035660318993164\n",
            "Epoch: 034, Loss: 2.1450, Train: 0.3208, Test: 0.3151\n",
            "Early stopping:  0.04421670052983814\n",
            "Epoch: 035, Loss: 2.1360, Train: 0.3259, Test: 0.3166\n",
            "Early stopping:  0.03394539990629149\n",
            "Epoch: 036, Loss: 2.1297, Train: 0.3607, Test: 0.3487\n",
            "Early stopping:  0.023335742740173\n",
            "Epoch: 037, Loss: 2.1235, Train: 0.3647, Test: 0.3519\n",
            "Early stopping:  0.015044395963678726\n",
            "Epoch: 038, Loss: 2.1158, Train: 0.3710, Test: 0.3549\n",
            "Early stopping:  0.01125248922017261\n",
            "Epoch: 039, Loss: 2.1058, Train: 0.3729, Test: 0.3559\n",
            "Early stopping:  0.011802915639677298\n",
            "Epoch: 040, Loss: 2.0940, Train: 0.3744, Test: 0.3593\n",
            "Early stopping:  0.01418575238537949\n",
            "Epoch: 041, Loss: 2.0818, Train: 0.3778, Test: 0.3619\n",
            "Early stopping:  0.0166843838392461\n",
            "Epoch: 042, Loss: 2.0706, Train: 0.3832, Test: 0.3655\n",
            "Early stopping:  0.018089785009651135\n",
            "Epoch: 043, Loss: 2.0611, Train: 0.3860, Test: 0.3661\n",
            "Early stopping:  0.017869929659157277\n",
            "Epoch: 044, Loss: 2.0529, Train: 0.3880, Test: 0.3678\n",
            "Early stopping:  0.016345203735098873\n",
            "Epoch: 045, Loss: 2.0456, Train: 0.3925, Test: 0.3676\n",
            "Early stopping:  0.014319834224628101\n",
            "Epoch: 046, Loss: 2.0385, Train: 0.3900, Test: 0.3670\n",
            "Early stopping:  0.012648119656341992\n",
            "Epoch: 047, Loss: 2.0313, Train: 0.3894, Test: 0.3650\n",
            "Early stopping:  0.01171782764100139\n",
            "Epoch: 048, Loss: 2.0233, Train: 0.3888, Test: 0.3629\n",
            "Early stopping:  0.011613357547512774\n",
            "Epoch: 049, Loss: 2.0133, Train: 0.3866, Test: 0.3582\n",
            "Early stopping:  0.012642574694861878\n",
            "Epoch: 050, Loss: 2.0019, Train: 0.3860, Test: 0.3578\n",
            "Early stopping:  0.014489373742630473\n",
            "Epoch: 051, Loss: 1.9888, Train: 0.3857, Test: 0.3529\n",
            "Early stopping:  0.01687652736684241\n",
            "Epoch: 052, Loss: 1.9750, Train: 0.3846, Test: 0.3521\n",
            "Early stopping:  0.01917496472484328\n",
            "Epoch: 053, Loss: 1.9600, Train: 0.3936, Test: 0.3627\n",
            "Early stopping:  0.02113661235613373\n",
            "Epoch: 054, Loss: 1.9403, Train: 0.3971, Test: 0.3665\n",
            "Early stopping:  0.02410556592047053\n",
            "Epoch: 055, Loss: 1.9244, Train: 0.3988, Test: 0.3670\n",
            "Early stopping:  0.025889039048119983\n",
            "Epoch: 056, Loss: 1.9018, Train: 0.4033, Test: 0.3772\n",
            "Early stopping:  0.028830273711757594\n",
            "Epoch: 057, Loss: 1.8965, Train: 0.4328, Test: 0.3980\n",
            "Early stopping:  0.026479427327560395\n",
            "Epoch: 058, Loss: 1.8703, Train: 0.4328, Test: 0.3975\n",
            "Early stopping:  0.02692961061162016\n",
            "Epoch: 059, Loss: 1.8637, Train: 0.4328, Test: 0.3984\n",
            "Early stopping:  0.024700187269429803\n",
            "Epoch: 060, Loss: 1.8390, Train: 0.4212, Test: 0.3956\n",
            "Early stopping:  0.025611454635000874\n",
            "Epoch: 061, Loss: 1.8414, Train: 0.4291, Test: 0.3984\n",
            "Early stopping:  0.023542875089525465\n",
            "Epoch: 062, Loss: 1.8220, Train: 0.4351, Test: 0.4069\n",
            "Early stopping:  0.019611904935204955\n",
            "Epoch: 063, Loss: 1.8103, Train: 0.4413, Test: 0.4081\n",
            "Early stopping:  0.020365120852719538\n",
            "Epoch: 064, Loss: 1.8050, Train: 0.4416, Test: 0.4079\n",
            "Early stopping:  0.01641781927768414\n",
            "Epoch: 065, Loss: 1.7858, Train: 0.4376, Test: 0.4039\n",
            "Early stopping:  0.020598342621737892\n",
            "Epoch: 066, Loss: 1.7733, Train: 0.4390, Test: 0.4046\n",
            "Early stopping:  0.019546405220156056\n",
            "Epoch: 067, Loss: 1.7678, Train: 0.4450, Test: 0.4094\n",
            "Early stopping:  0.018801788766086056\n",
            "Epoch: 068, Loss: 1.7512, Train: 0.4524, Test: 0.4113\n",
            "Early stopping:  0.020154850830053472\n",
            "Epoch: 069, Loss: 1.7461, Train: 0.4560, Test: 0.4150\n",
            "Early stopping:  0.016252377169230257\n",
            "Epoch: 070, Loss: 1.7401, Train: 0.4560, Test: 0.4145\n",
            "Early stopping:  0.014264130467593882\n",
            "Epoch: 071, Loss: 1.7274, Train: 0.4572, Test: 0.4111\n",
            "Early stopping:  0.014872209498726302\n",
            "Epoch: 072, Loss: 1.7220, Train: 0.4600, Test: 0.4133\n",
            "Early stopping:  0.012364993851506382\n",
            "Epoch: 073, Loss: 1.7113, Train: 0.4620, Test: 0.4205\n",
            "Early stopping:  0.01393901062446087\n",
            "Epoch: 074, Loss: 1.7014, Train: 0.4609, Test: 0.4217\n",
            "Early stopping:  0.014852693053348374\n",
            "Epoch: 075, Loss: 1.6942, Train: 0.4606, Test: 0.4232\n",
            "Early stopping:  0.013820387536616657\n",
            "Epoch: 076, Loss: 1.6837, Train: 0.4645, Test: 0.4247\n",
            "Early stopping:  0.014846864903502397\n",
            "Epoch: 077, Loss: 1.6761, Train: 0.4657, Test: 0.4220\n",
            "Early stopping:  0.0139644099440241\n",
            "Epoch: 078, Loss: 1.6695, Train: 0.4660, Test: 0.4253\n",
            "Early stopping:  0.012996318936484301\n",
            "Epoch: 079, Loss: 1.6612, Train: 0.4657, Test: 0.4279\n",
            "Early stopping:  0.012730878053547784\n",
            "Epoch: 080, Loss: 1.6552, Train: 0.4671, Test: 0.4296\n",
            "Early stopping:  0.011389425231701542\n",
            "Epoch: 081, Loss: 1.6484, Train: 0.4697, Test: 0.4292\n",
            "Early stopping:  0.011032859132195117\n",
            "Epoch: 082, Loss: 1.6391, Train: 0.4705, Test: 0.4290\n",
            "Early stopping:  0.011670446745201702\n",
            "Epoch: 083, Loss: 1.6310, Train: 0.4725, Test: 0.4324\n",
            "Early stopping:  0.012124176926858615\n",
            "Epoch: 084, Loss: 1.6218, Train: 0.4767, Test: 0.4328\n",
            "Early stopping:  0.013289352276388644\n",
            "Epoch: 085, Loss: 1.6138, Train: 0.4787, Test: 0.4355\n",
            "Early stopping:  0.013670337012158888\n",
            "Epoch: 086, Loss: 1.6056, Train: 0.4816, Test: 0.4368\n",
            "Early stopping:  0.013321864937871563\n",
            "Epoch: 087, Loss: 1.5975, Train: 0.4855, Test: 0.4370\n",
            "Early stopping:  0.0131899533469271\n",
            "Epoch: 088, Loss: 1.5902, Train: 0.4864, Test: 0.4402\n",
            "Early stopping:  0.012597786849975912\n",
            "Epoch: 089, Loss: 1.5818, Train: 0.4870, Test: 0.4398\n",
            "Early stopping:  0.012562958636715362\n",
            "Epoch: 090, Loss: 1.5745, Train: 0.4892, Test: 0.4411\n",
            "Early stopping:  0.012318932655266374\n",
            "Epoch: 091, Loss: 1.5645, Train: 0.4909, Test: 0.4426\n",
            "Early stopping:  0.012906500618090355\n",
            "Epoch: 092, Loss: 1.5569, Train: 0.4963, Test: 0.4442\n",
            "Early stopping:  0.013246128969150405\n",
            "Epoch: 093, Loss: 1.5469, Train: 0.4980, Test: 0.4440\n",
            "Early stopping:  0.013832557691425511\n",
            "Epoch: 094, Loss: 1.5379, Train: 0.5034, Test: 0.4479\n",
            "Early stopping:  0.014392533705886367\n",
            "Epoch: 095, Loss: 1.5273, Train: 0.5034, Test: 0.4494\n",
            "Early stopping:  0.014798794131825924\n",
            "Epoch: 096, Loss: 1.5188, Train: 0.5071, Test: 0.4493\n",
            "Early stopping:  0.01514434891037397\n",
            "Epoch: 097, Loss: 1.5082, Train: 0.5096, Test: 0.4496\n",
            "Early stopping:  0.015256202402280655\n",
            "Epoch: 098, Loss: 1.4990, Train: 0.5102, Test: 0.4519\n",
            "Early stopping:  0.015337620269668088\n",
            "Epoch: 099, Loss: 1.4893, Train: 0.5145, Test: 0.4508\n",
            "Early stopping:  0.015181025763886519\n",
            "Epoch: 100, Loss: 1.4792, Train: 0.5119, Test: 0.4530\n",
            "Early stopping:  0.015505444886741899\n",
            "Epoch: 101, Loss: 1.4707, Train: 0.5136, Test: 0.4545\n",
            "Early stopping:  0.014975963307670925\n",
            "Epoch: 102, Loss: 1.4597, Train: 0.5122, Test: 0.4572\n",
            "Early stopping:  0.01535729379997417\n",
            "Epoch: 103, Loss: 1.4504, Train: 0.5167, Test: 0.4574\n",
            "Early stopping:  0.015391882561725809\n",
            "Epoch: 104, Loss: 1.4412, Train: 0.5164, Test: 0.4572\n",
            "Early stopping:  0.0152580839668007\n",
            "Epoch: 105, Loss: 1.4322, Train: 0.5170, Test: 0.4572\n",
            "Early stopping:  0.015124974545510452\n",
            "Epoch: 106, Loss: 1.4238, Train: 0.5173, Test: 0.4617\n",
            "Early stopping:  0.014239690794959663\n",
            "Epoch: 107, Loss: 1.4182, Train: 0.5230, Test: 0.4600\n",
            "Early stopping:  0.01298601455850232\n",
            "Epoch: 108, Loss: 1.4139, Train: 0.5255, Test: 0.4638\n",
            "Early stopping:  0.010952194504784317\n",
            "Epoch: 109, Loss: 1.4049, Train: 0.5244, Test: 0.4646\n",
            "Early stopping:  0.010258009781691406\n",
            "Epoch: 110, Loss: 1.3945, Train: 0.5233, Test: 0.4631\n",
            "Early stopping:  0.011546248929496959\n",
            "Epoch: 111, Loss: 1.3932, Train: 0.5298, Test: 0.4625\n",
            "Early stopping:  0.011200588827815525\n",
            "Epoch: 112, Loss: 1.3785, Train: 0.5329, Test: 0.4638\n",
            "Early stopping:  0.013329111488876118\n",
            "Epoch: 113, Loss: 1.3760, Train: 0.5360, Test: 0.4642\n",
            "Early stopping:  0.012027884501357961\n",
            "Epoch: 114, Loss: 1.3629, Train: 0.5360, Test: 0.4646\n",
            "Early stopping:  0.013149166032855113\n",
            "Epoch: 115, Loss: 1.3610, Train: 0.5406, Test: 0.4651\n",
            "Early stopping:  0.013087044180036247\n",
            "Epoch: 116, Loss: 1.3497, Train: 0.5411, Test: 0.4663\n",
            "Early stopping:  0.011802129766474583\n",
            "Epoch: 117, Loss: 1.3452, Train: 0.5414, Test: 0.4668\n",
            "Early stopping:  0.012086353179672715\n",
            "Epoch: 118, Loss: 1.3353, Train: 0.5471, Test: 0.4661\n",
            "Early stopping:  0.011449752354343094\n",
            "Epoch: 119, Loss: 1.3290, Train: 0.5510, Test: 0.4719\n",
            "Early stopping:  0.012495269033401885\n",
            "Epoch: 120, Loss: 1.3196, Train: 0.5530, Test: 0.4740\n",
            "Early stopping:  0.012153175333976996\n",
            "Epoch: 121, Loss: 1.3168, Train: 0.5618, Test: 0.4774\n",
            "Early stopping:  0.011632277064680323\n",
            "Epoch: 122, Loss: 1.3061, Train: 0.5607, Test: 0.4804\n",
            "Early stopping:  0.011292712432410337\n",
            "Epoch: 123, Loss: 1.3067, Train: 0.5627, Test: 0.4844\n",
            "Early stopping:  0.009593489884254274\n",
            "PREDICTIONS -> tensor([8, 0, 0,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.42      0.60      0.49       568\n",
            "         capital_goods       0.34      0.06      0.10       381\n",
            "conglomerates_industry       0.41      0.12      0.18        60\n",
            "     consumer_cyclical       0.48      0.39      0.43       595\n",
            " consumer_non-cyclical       0.66      0.34      0.45       334\n",
            "                energy       0.50      0.01      0.03       213\n",
            "             financial       0.64      0.57      0.61       576\n",
            "            healthcare       0.62      0.56      0.59       238\n",
            "              services       0.45      0.76      0.57      1557\n",
            "            technology       0.44      0.33      0.38       297\n",
            "        transportation       0.61      0.31      0.41       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.48      5291\n",
            "             macro avg       0.46      0.34      0.35      5291\n",
            "          weighted avg       0.48      0.48      0.44      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 50.9853, Train: 0.2918, Test: 0.2922\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 406.9365, Train: 0.1052, Test: 0.1079\n",
            "Early stopping:  251.69552247028875\n",
            "Epoch: 003, Loss: 698.6190, Train: 0.0624, Test: 0.0639\n",
            "Early stopping:  324.34791973222235\n",
            "Epoch: 004, Loss: 443.0388, Train: 0.0939, Test: 0.0954\n",
            "Early stopping:  266.3863153313482\n",
            "Epoch: 005, Loss: 342.2607, Train: 0.0644, Test: 0.0650\n",
            "Early stopping:  232.13270085825363\n",
            "Epoch: 006, Loss: 254.4146, Train: 0.1120, Test: 0.1094\n",
            "Early stopping:  166.86973798150197\n",
            "Epoch: 007, Loss: 204.3433, Train: 0.1211, Test: 0.1196\n",
            "Early stopping:  195.69038758131495\n",
            "Epoch: 008, Loss: 137.8034, Train: 0.0771, Test: 0.0737\n",
            "Early stopping:  119.36356829183558\n",
            "Epoch: 009, Loss: 83.8390, Train: 0.2059, Test: 0.1964\n",
            "Early stopping:  100.59449496416059\n",
            "Epoch: 010, Loss: 28.0093, Train: 0.1923, Test: 0.1843\n",
            "Early stopping:  90.7158067403808\n",
            "Epoch: 011, Loss: 16.6722, Train: 0.1384, Test: 0.1334\n",
            "Early stopping:  78.31332847690709\n",
            "Epoch: 012, Loss: 8.4126, Train: 0.1551, Test: 0.1457\n",
            "Early stopping:  54.8998925303259\n",
            "Epoch: 013, Loss: 5.0427, Train: 0.2362, Test: 0.2274\n",
            "Early stopping:  32.23446564827963\n",
            "Epoch: 014, Loss: 3.7643, Train: 0.3264, Test: 0.3154\n",
            "Early stopping:  10.080976184523166\n",
            "Epoch: 015, Loss: 3.0969, Train: 0.3403, Test: 0.3224\n",
            "Early stopping:  5.574311217123276\n",
            "Epoch: 016, Loss: 2.7100, Train: 0.3400, Test: 0.3177\n",
            "Early stopping:  2.305507688131251\n",
            "Epoch: 017, Loss: 2.4971, Train: 0.3437, Test: 0.3217\n",
            "Early stopping:  1.0261566350309914\n",
            "Epoch: 018, Loss: 2.3291, Train: 0.3330, Test: 0.3103\n",
            "Early stopping:  0.571861748556154\n",
            "Epoch: 019, Loss: 2.2299, Train: 0.3171, Test: 0.3015\n",
            "Early stopping:  0.3450338706128455\n",
            "Epoch: 020, Loss: 2.2027, Train: 0.3142, Test: 0.2954\n",
            "Early stopping:  0.21106099221558766\n",
            "Epoch: 021, Loss: 2.2032, Train: 0.3114, Test: 0.2994\n",
            "Early stopping:  0.12563540240672222\n",
            "Epoch: 022, Loss: 2.2089, Train: 0.3083, Test: 0.2977\n",
            "Early stopping:  0.05387914168862934\n",
            "Epoch: 023, Loss: 2.2035, Train: 0.3040, Test: 0.2875\n",
            "Early stopping:  0.011592113287598863\n",
            "Epoch: 024, Loss: 2.1819, Train: 0.3001, Test: 0.2786\n",
            "Early stopping:  0.010456213078385382\n",
            "Epoch: 025, Loss: 2.1540, Train: 0.3193, Test: 0.2975\n",
            "Early stopping:  0.022778382717091095\n",
            "Epoch: 026, Loss: 2.1251, Train: 0.3409, Test: 0.3177\n",
            "Early stopping:  0.035130566878145565\n",
            "Epoch: 027, Loss: 2.0868, Train: 0.3349, Test: 0.3119\n",
            "Early stopping:  0.04612189616793584\n",
            "Epoch: 028, Loss: 2.0613, Train: 0.3389, Test: 0.3173\n",
            "Early stopping:  0.04883297281913685\n",
            "Epoch: 029, Loss: 2.0416, Train: 0.3389, Test: 0.3200\n",
            "Early stopping:  0.04590119425990345\n",
            "Epoch: 030, Loss: 2.0366, Train: 0.3375, Test: 0.3207\n",
            "Early stopping:  0.03643678493512749\n",
            "Epoch: 031, Loss: 2.0129, Train: 0.3332, Test: 0.3202\n",
            "Early stopping:  0.027756790167365573\n",
            "Epoch: 032, Loss: 1.9944, Train: 0.3392, Test: 0.3238\n",
            "Early stopping:  0.026064635219237058\n",
            "Epoch: 033, Loss: 1.9759, Train: 0.3423, Test: 0.3260\n",
            "Early stopping:  0.02781501471777767\n",
            "Epoch: 034, Loss: 1.9599, Train: 0.3452, Test: 0.3334\n",
            "Early stopping:  0.030175254979036968\n",
            "Epoch: 035, Loss: 1.9477, Train: 0.3514, Test: 0.3376\n",
            "Early stopping:  0.026147281379255247\n",
            "Epoch: 036, Loss: 1.9269, Train: 0.3599, Test: 0.3432\n",
            "Early stopping:  0.025876757495416406\n",
            "Epoch: 037, Loss: 1.9067, Train: 0.3633, Test: 0.3485\n",
            "Early stopping:  0.02725079537094078\n",
            "Epoch: 038, Loss: 1.8903, Train: 0.3687, Test: 0.3525\n",
            "Early stopping:  0.028603462529925027\n",
            "Epoch: 039, Loss: 1.8792, Train: 0.3738, Test: 0.3576\n",
            "Early stopping:  0.027638654172503524\n",
            "Epoch: 040, Loss: 1.8587, Train: 0.3817, Test: 0.3629\n",
            "Early stopping:  0.026027781518270055\n",
            "Epoch: 041, Loss: 1.8410, Train: 0.3891, Test: 0.3659\n",
            "Early stopping:  0.02587222517486346\n",
            "Epoch: 042, Loss: 1.8268, Train: 0.3917, Test: 0.3725\n",
            "Early stopping:  0.02620545026167871\n",
            "Epoch: 043, Loss: 1.8127, Train: 0.3985, Test: 0.3784\n",
            "Early stopping:  0.026169583840190056\n",
            "Epoch: 044, Loss: 1.7960, Train: 0.4050, Test: 0.3814\n",
            "Early stopping:  0.024322289533802995\n",
            "Epoch: 045, Loss: 1.7798, Train: 0.4132, Test: 0.3907\n",
            "Early stopping:  0.024235914000706214\n",
            "Epoch: 046, Loss: 1.7687, Train: 0.4175, Test: 0.3931\n",
            "Early stopping:  0.02361260614045541\n",
            "Epoch: 047, Loss: 1.7539, Train: 0.4254, Test: 0.3967\n",
            "Early stopping:  0.02293993530804891\n",
            "Epoch: 048, Loss: 1.7368, Train: 0.4271, Test: 0.3986\n",
            "Early stopping:  0.022859272045188946\n",
            "Epoch: 049, Loss: 1.7241, Train: 0.4319, Test: 0.4012\n",
            "Early stopping:  0.022713033974747745\n",
            "Epoch: 050, Loss: 1.7112, Train: 0.4325, Test: 0.4056\n",
            "Early stopping:  0.0229482905132267\n",
            "Epoch: 051, Loss: 1.6979, Train: 0.4362, Test: 0.4107\n",
            "Early stopping:  0.02181971121339226\n",
            "Epoch: 052, Loss: 1.6862, Train: 0.4387, Test: 0.4107\n",
            "Early stopping:  0.020161118971242977\n",
            "Epoch: 053, Loss: 1.6754, Train: 0.4407, Test: 0.4103\n",
            "Early stopping:  0.01938714472900686\n",
            "Epoch: 054, Loss: 1.6631, Train: 0.4421, Test: 0.4103\n",
            "Early stopping:  0.018783957332111887\n",
            "Epoch: 055, Loss: 1.6516, Train: 0.4413, Test: 0.4120\n",
            "Early stopping:  0.018281828407645383\n",
            "Epoch: 056, Loss: 1.6421, Train: 0.4444, Test: 0.4149\n",
            "Early stopping:  0.017709439498717793\n",
            "Epoch: 057, Loss: 1.6305, Train: 0.4472, Test: 0.4160\n",
            "Early stopping:  0.017540142013199853\n",
            "Epoch: 058, Loss: 1.6203, Train: 0.4492, Test: 0.4184\n",
            "Early stopping:  0.01689502832317482\n",
            "Epoch: 059, Loss: 1.6102, Train: 0.4515, Test: 0.4198\n",
            "Early stopping:  0.016580287215230794\n",
            "Epoch: 060, Loss: 1.6001, Train: 0.4566, Test: 0.4211\n",
            "Early stopping:  0.01650581640229041\n",
            "Epoch: 061, Loss: 1.5903, Train: 0.4577, Test: 0.4235\n",
            "Early stopping:  0.015881311603717183\n",
            "Epoch: 062, Loss: 1.5811, Train: 0.4603, Test: 0.4262\n",
            "Early stopping:  0.01551214315211829\n",
            "Epoch: 063, Loss: 1.5725, Train: 0.4654, Test: 0.4271\n",
            "Early stopping:  0.014906358058082261\n",
            "Epoch: 064, Loss: 1.5636, Train: 0.4691, Test: 0.4277\n",
            "Early stopping:  0.014349443269001965\n",
            "Epoch: 065, Loss: 1.5544, Train: 0.4748, Test: 0.4296\n",
            "Early stopping:  0.014111607685841646\n",
            "Epoch: 066, Loss: 1.5452, Train: 0.4787, Test: 0.4317\n",
            "Early stopping:  0.014211672210237352\n",
            "Epoch: 067, Loss: 1.5366, Train: 0.4858, Test: 0.4404\n",
            "Early stopping:  0.01426237908137646\n",
            "Epoch: 068, Loss: 1.5289, Train: 0.4895, Test: 0.4479\n",
            "Early stopping:  0.013798599524432839\n",
            "Epoch: 069, Loss: 1.5193, Train: 0.4940, Test: 0.4521\n",
            "Early stopping:  0.013686760185158664\n",
            "Epoch: 070, Loss: 1.5092, Train: 0.4972, Test: 0.4572\n",
            "Early stopping:  0.014137617535379492\n",
            "Epoch: 071, Loss: 1.4988, Train: 0.4997, Test: 0.4634\n",
            "Early stopping:  0.01509601620049163\n",
            "Epoch: 072, Loss: 1.4892, Train: 0.5048, Test: 0.4672\n",
            "Early stopping:  0.015819743790753526\n",
            "Epoch: 073, Loss: 1.4804, Train: 0.5159, Test: 0.4750\n",
            "Early stopping:  0.015485290492791867\n",
            "Epoch: 074, Loss: 1.4684, Train: 0.5176, Test: 0.4748\n",
            "Early stopping:  0.015825727748445696\n",
            "Epoch: 075, Loss: 1.4587, Train: 0.5216, Test: 0.4751\n",
            "Early stopping:  0.015984652589836373\n",
            "Epoch: 076, Loss: 1.4459, Train: 0.5269, Test: 0.4814\n",
            "Early stopping:  0.01715922736496764\n",
            "Epoch: 077, Loss: 1.4369, Train: 0.5295, Test: 0.4799\n",
            "Early stopping:  0.017347932579385876\n",
            "Epoch: 078, Loss: 1.4259, Train: 0.5343, Test: 0.4825\n",
            "Early stopping:  0.016926959391456442\n",
            "Epoch: 079, Loss: 1.4136, Train: 0.5377, Test: 0.4872\n",
            "Early stopping:  0.017441259191735718\n",
            "Epoch: 080, Loss: 1.4013, Train: 0.5397, Test: 0.4888\n",
            "Early stopping:  0.017831088839980302\n",
            "Epoch: 081, Loss: 1.3951, Train: 0.5471, Test: 0.4925\n",
            "Early stopping:  0.017190935195871077\n",
            "Epoch: 082, Loss: 1.3807, Train: 0.5522, Test: 0.4939\n",
            "Early stopping:  0.017308414076625922\n",
            "Epoch: 083, Loss: 1.3727, Train: 0.5570, Test: 0.4863\n",
            "Early stopping:  0.016277615368252882\n",
            "Epoch: 084, Loss: 1.3646, Train: 0.5598, Test: 0.4956\n",
            "Early stopping:  0.015263315608199433\n",
            "Epoch: 085, Loss: 1.3524, Train: 0.5627, Test: 0.4957\n",
            "Early stopping:  0.01615427807360137\n",
            "Epoch: 086, Loss: 1.3399, Train: 0.5618, Test: 0.4965\n",
            "Early stopping:  0.01620959265678594\n",
            "Epoch: 087, Loss: 1.3322, Train: 0.5615, Test: 0.4974\n",
            "Early stopping:  0.016770969454476307\n",
            "Epoch: 088, Loss: 1.3257, Train: 0.5624, Test: 0.4957\n",
            "Early stopping:  0.015645677529311966\n",
            "Epoch: 089, Loss: 1.3222, Train: 0.5701, Test: 0.5046\n",
            "Early stopping:  0.012081148323020829\n",
            "Epoch: 090, Loss: 1.3076, Train: 0.5749, Test: 0.5061\n",
            "Early stopping:  0.012073935698602876\n",
            "Epoch: 091, Loss: 1.2951, Train: 0.5746, Test: 0.5044\n",
            "Early stopping:  0.015002461841122399\n",
            "Epoch: 092, Loss: 1.2928, Train: 0.5805, Test: 0.5131\n",
            "Early stopping:  0.015086669825680658\n",
            "Epoch: 093, Loss: 1.2853, Train: 0.5837, Test: 0.5152\n",
            "Early stopping:  0.014501821882720934\n",
            "Epoch: 094, Loss: 1.2737, Train: 0.5814, Test: 0.5148\n",
            "Early stopping:  0.012537780124541069\n",
            "Epoch: 095, Loss: 1.2645, Train: 0.5885, Test: 0.5177\n",
            "Early stopping:  0.012989970723246876\n",
            "Epoch: 096, Loss: 1.2602, Train: 0.5839, Test: 0.5124\n",
            "Early stopping:  0.013733636082676675\n",
            "Epoch: 097, Loss: 1.2576, Train: 0.5916, Test: 0.5222\n",
            "Early stopping:  0.011331344416244367\n",
            "Epoch: 098, Loss: 1.2455, Train: 0.5930, Test: 0.5179\n",
            "Early stopping:  0.010261694053800218\n",
            "Epoch: 099, Loss: 1.2329, Train: 0.5961, Test: 0.5198\n",
            "Early stopping:  0.012862438995675164\n",
            "Epoch: 100, Loss: 1.2261, Train: 0.5976, Test: 0.5235\n",
            "Early stopping:  0.014933350964318068\n",
            "Epoch: 101, Loss: 1.2216, Train: 0.6032, Test: 0.5179\n",
            "Early stopping:  0.014755324437884819\n",
            "Epoch: 102, Loss: 1.2156, Train: 0.6018, Test: 0.5271\n",
            "Early stopping:  0.011502862740730186\n",
            "Epoch: 103, Loss: 1.2062, Train: 0.6047, Test: 0.5258\n",
            "Early stopping:  0.010187397825293897\n",
            "Epoch: 104, Loss: 1.1979, Train: 0.6018, Test: 0.5209\n",
            "Early stopping:  0.011456773616619884\n",
            "Epoch: 105, Loss: 1.1997, Train: 0.6055, Test: 0.5277\n",
            "Early stopping:  0.010204530583527568\n",
            "Epoch: 106, Loss: 1.1995, Train: 0.6049, Test: 0.5266\n",
            "Early stopping:  0.007329852857290675\n",
            "PREDICTIONS -> tensor([ 9,  8,  0,  ..., 11,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.39      0.52      0.45       568\n",
            "         capital_goods       0.46      0.08      0.14       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.52      0.44      0.48       595\n",
            " consumer_non-cyclical       0.65      0.28      0.40       334\n",
            "                energy       0.51      0.50      0.50       213\n",
            "             financial       0.73      0.57      0.64       576\n",
            "            healthcare       0.64      0.55      0.59       238\n",
            "              services       0.52      0.77      0.62      1557\n",
            "            technology       0.40      0.40      0.40       297\n",
            "        transportation       0.62      0.59      0.60       303\n",
            "             utilities       0.68      0.20      0.31       169\n",
            "\n",
            "              accuracy                           0.53      5291\n",
            "             macro avg       0.51      0.41      0.43      5291\n",
            "          weighted avg       0.54      0.53      0.50      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 60.2699, Train: 0.0947, Test: 0.0873\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 187.4028, Train: 0.2952, Test: 0.2948\n",
            "Early stopping:  89.89649810665261\n",
            "Epoch: 003, Loss: 374.5662, Train: 0.1180, Test: 0.1196\n",
            "Early stopping:  158.10070925420655\n",
            "Epoch: 004, Loss: 417.2597, Train: 0.1115, Test: 0.1168\n",
            "Early stopping:  166.35144511701571\n",
            "Epoch: 005, Loss: 497.0523, Train: 0.1066, Test: 0.1047\n",
            "Early stopping:  178.9001222852118\n",
            "Epoch: 006, Loss: 443.3454, Train: 0.1029, Test: 0.0954\n",
            "Early stopping:  118.47821816033668\n",
            "Epoch: 007, Loss: 276.9371, Train: 0.0871, Test: 0.0782\n",
            "Early stopping:  82.72008924391629\n",
            "Epoch: 008, Loss: 271.1854, Train: 0.3066, Test: 0.3045\n",
            "Early stopping:  101.92899838398269\n",
            "Epoch: 009, Loss: 237.6630, Train: 0.3057, Test: 0.3047\n",
            "Early stopping:  116.61244698663376\n",
            "Epoch: 010, Loss: 231.3471, Train: 0.3069, Test: 0.3067\n",
            "Early stopping:  86.88616662084966\n",
            "Epoch: 011, Loss: 185.6687, Train: 0.3106, Test: 0.3149\n",
            "Early stopping:  36.6317162172867\n",
            "Epoch: 012, Loss: 126.9541, Train: 0.2102, Test: 0.2041\n",
            "Early stopping:  55.79344811272582\n",
            "Epoch: 013, Loss: 96.6786, Train: 0.1985, Test: 0.1918\n",
            "Early stopping:  62.561801780484444\n",
            "Epoch: 014, Loss: 81.7931, Train: 0.1685, Test: 0.1580\n",
            "Early stopping:  62.77773399284511\n",
            "Epoch: 015, Loss: 62.8123, Train: 0.1699, Test: 0.1506\n",
            "Early stopping:  47.98413625703677\n",
            "Epoch: 016, Loss: 28.6760, Train: 0.1653, Test: 0.1574\n",
            "Early stopping:  36.78906029135909\n",
            "Epoch: 017, Loss: 17.5725, Train: 0.1588, Test: 0.1486\n",
            "Early stopping:  33.83180299316756\n",
            "Epoch: 018, Loss: 11.8604, Train: 0.1605, Test: 0.1465\n",
            "Early stopping:  30.367118047347983\n",
            "Epoch: 019, Loss: 7.9456, Train: 0.2153, Test: 0.2020\n",
            "Early stopping:  22.13007026996462\n",
            "Epoch: 020, Loss: 5.7709, Train: 0.2555, Test: 0.2442\n",
            "Early stopping:  9.171054126769253\n",
            "Epoch: 021, Loss: 4.5619, Train: 0.2748, Test: 0.2672\n",
            "Early stopping:  5.278164162660401\n",
            "Epoch: 022, Loss: 3.9387, Train: 0.2918, Test: 0.2843\n",
            "Early stopping:  3.208369991118418\n",
            "Epoch: 023, Loss: 3.5284, Train: 0.2944, Test: 0.2912\n",
            "Early stopping:  1.7778947568809105\n",
            "Epoch: 024, Loss: 3.2101, Train: 0.3083, Test: 0.3033\n",
            "Early stopping:  1.0121602354494652\n",
            "Epoch: 025, Loss: 2.9090, Train: 0.3247, Test: 0.3185\n",
            "Early stopping:  0.6461807001436947\n",
            "Epoch: 026, Loss: 2.6064, Train: 0.3324, Test: 0.3292\n",
            "Early stopping:  0.5204178280382077\n",
            "Epoch: 027, Loss: 2.3820, Train: 0.3358, Test: 0.3319\n",
            "Early stopping:  0.4588164883823282\n",
            "Epoch: 028, Loss: 2.2562, Train: 0.3406, Test: 0.3290\n",
            "Early stopping:  0.3895542394004746\n",
            "Epoch: 029, Loss: 2.1861, Train: 0.3432, Test: 0.3292\n",
            "Early stopping:  0.2938306859494309\n",
            "Epoch: 030, Loss: 2.1495, Train: 0.3375, Test: 0.3239\n",
            "Early stopping:  0.1849604214248324\n",
            "Epoch: 031, Loss: 2.1377, Train: 0.3281, Test: 0.3164\n",
            "Early stopping:  0.10049243978161893\n",
            "Epoch: 032, Loss: 2.1289, Train: 0.3259, Test: 0.3166\n",
            "Early stopping:  0.052025737529837544\n",
            "Epoch: 033, Loss: 2.1096, Train: 0.3284, Test: 0.3215\n",
            "Early stopping:  0.02847236099572252\n",
            "Epoch: 034, Loss: 2.0916, Train: 0.3301, Test: 0.3228\n",
            "Early stopping:  0.023027883070935336\n",
            "Epoch: 035, Loss: 2.0747, Train: 0.3349, Test: 0.3325\n",
            "Early stopping:  0.025971141611102682\n",
            "Epoch: 036, Loss: 2.0569, Train: 0.3392, Test: 0.3387\n",
            "Early stopping:  0.028268996557936046\n",
            "Epoch: 037, Loss: 2.0394, Train: 0.3537, Test: 0.3487\n",
            "Early stopping:  0.027671098740639218\n",
            "Epoch: 038, Loss: 2.0191, Train: 0.3653, Test: 0.3548\n",
            "Early stopping:  0.02853313660804008\n",
            "Epoch: 039, Loss: 1.9962, Train: 0.3746, Test: 0.3608\n",
            "Early stopping:  0.03087849686636371\n",
            "Epoch: 040, Loss: 1.9731, Train: 0.3846, Test: 0.3659\n",
            "Early stopping:  0.03340528993614458\n",
            "Epoch: 041, Loss: 1.9513, Train: 0.3948, Test: 0.3729\n",
            "Early stopping:  0.03514860923275886\n",
            "Epoch: 042, Loss: 1.9295, Train: 0.4044, Test: 0.3840\n",
            "Early stopping:  0.035414069641409196\n",
            "Epoch: 043, Loss: 1.9071, Train: 0.4169, Test: 0.3927\n",
            "Early stopping:  0.03506695397636101\n",
            "Epoch: 044, Loss: 1.8835, Train: 0.4274, Test: 0.4001\n",
            "Early stopping:  0.035310241970885656\n",
            "Epoch: 045, Loss: 1.8611, Train: 0.4396, Test: 0.4067\n",
            "Early stopping:  0.035791527262227556\n",
            "Epoch: 046, Loss: 1.8402, Train: 0.4450, Test: 0.4147\n",
            "Early stopping:  0.035509047759129414\n",
            "Epoch: 047, Loss: 1.8218, Train: 0.4453, Test: 0.4237\n",
            "Early stopping:  0.033836592616756145\n",
            "Epoch: 048, Loss: 1.8075, Train: 0.4549, Test: 0.4256\n",
            "Early stopping:  0.030348163034205726\n",
            "Epoch: 049, Loss: 1.7931, Train: 0.4589, Test: 0.4298\n",
            "Early stopping:  0.026770656204823145\n",
            "Epoch: 050, Loss: 1.7739, Train: 0.4614, Test: 0.4287\n",
            "Early stopping:  0.02555704116134574\n",
            "Epoch: 051, Loss: 1.7542, Train: 0.4640, Test: 0.4298\n",
            "Early stopping:  0.026782330847487153\n",
            "Epoch: 052, Loss: 1.7363, Train: 0.4691, Test: 0.4326\n",
            "Early stopping:  0.02869939854318772\n",
            "Epoch: 053, Loss: 1.7176, Train: 0.4719, Test: 0.4360\n",
            "Early stopping:  0.029830358474328075\n",
            "Epoch: 054, Loss: 1.6996, Train: 0.4745, Test: 0.4353\n",
            "Early stopping:  0.029298286688951715\n",
            "Epoch: 055, Loss: 1.6875, Train: 0.4765, Test: 0.4409\n",
            "Early stopping:  0.026979403661990354\n",
            "Epoch: 056, Loss: 1.6737, Train: 0.4801, Test: 0.4419\n",
            "Early stopping:  0.024675370743942333\n",
            "Epoch: 057, Loss: 1.6619, Train: 0.4836, Test: 0.4455\n",
            "Early stopping:  0.02177542219840859\n",
            "Epoch: 058, Loss: 1.6507, Train: 0.4850, Test: 0.4483\n",
            "Early stopping:  0.01953375784986896\n",
            "Epoch: 059, Loss: 1.6383, Train: 0.4932, Test: 0.4479\n",
            "Early stopping:  0.01920870704260881\n",
            "Epoch: 060, Loss: 1.6265, Train: 0.4949, Test: 0.4487\n",
            "Early stopping:  0.018661314398294517\n",
            "Epoch: 061, Loss: 1.6165, Train: 0.4974, Test: 0.4527\n",
            "Early stopping:  0.01819435798275831\n",
            "Epoch: 062, Loss: 1.6058, Train: 0.5006, Test: 0.4547\n",
            "Early stopping:  0.017656420051047655\n",
            "Epoch: 063, Loss: 1.5941, Train: 0.5031, Test: 0.4585\n",
            "Early stopping:  0.017236332998967503\n",
            "Epoch: 064, Loss: 1.5829, Train: 0.5057, Test: 0.4604\n",
            "Early stopping:  0.017320647551149423\n",
            "Epoch: 065, Loss: 1.5703, Train: 0.5077, Test: 0.4606\n",
            "Early stopping:  0.018231308475790027\n",
            "Epoch: 066, Loss: 1.5591, Train: 0.5082, Test: 0.4606\n",
            "Early stopping:  0.018526092440023303\n",
            "Epoch: 067, Loss: 1.5479, Train: 0.5116, Test: 0.4642\n",
            "Early stopping:  0.018386180464152926\n",
            "Epoch: 068, Loss: 1.5386, Train: 0.5133, Test: 0.4636\n",
            "Early stopping:  0.017578321880853506\n",
            "Epoch: 069, Loss: 1.5285, Train: 0.5153, Test: 0.4668\n",
            "Early stopping:  0.01648806512301194\n",
            "Epoch: 070, Loss: 1.5190, Train: 0.5173, Test: 0.4689\n",
            "Early stopping:  0.015769183926364373\n",
            "Epoch: 071, Loss: 1.5084, Train: 0.5196, Test: 0.4727\n",
            "Early stopping:  0.01559735518616949\n",
            "Epoch: 072, Loss: 1.4987, Train: 0.5230, Test: 0.4753\n",
            "Early stopping:  0.0158017577817965\n",
            "Epoch: 073, Loss: 1.4885, Train: 0.5233, Test: 0.4753\n",
            "Early stopping:  0.01585383018496332\n",
            "Epoch: 074, Loss: 1.4788, Train: 0.5247, Test: 0.4757\n",
            "Early stopping:  0.01583722961364365\n",
            "Epoch: 075, Loss: 1.4684, Train: 0.5238, Test: 0.4748\n",
            "Early stopping:  0.015770701681070295\n",
            "Epoch: 076, Loss: 1.4591, Train: 0.5244, Test: 0.4751\n",
            "Early stopping:  0.01570176109234608\n",
            "Epoch: 077, Loss: 1.4496, Train: 0.5303, Test: 0.4787\n",
            "Early stopping:  0.015443635554920178\n",
            "Epoch: 078, Loss: 1.4402, Train: 0.5372, Test: 0.4838\n",
            "Early stopping:  0.015197504095523997\n",
            "Epoch: 079, Loss: 1.4301, Train: 0.5389, Test: 0.4872\n",
            "Early stopping:  0.015098777671078175\n",
            "Epoch: 080, Loss: 1.4208, Train: 0.5394, Test: 0.4888\n",
            "Early stopping:  0.015196473598757214\n",
            "Epoch: 081, Loss: 1.4112, Train: 0.5440, Test: 0.4918\n",
            "Early stopping:  0.015204428657837929\n",
            "Epoch: 082, Loss: 1.4012, Train: 0.5451, Test: 0.4925\n",
            "Early stopping:  0.015322184367246788\n",
            "Epoch: 083, Loss: 1.3918, Train: 0.5493, Test: 0.4946\n",
            "Early stopping:  0.015191322788090917\n",
            "Epoch: 084, Loss: 1.3829, Train: 0.5516, Test: 0.4956\n",
            "Early stopping:  0.01504714644923274\n",
            "Epoch: 085, Loss: 1.3735, Train: 0.5539, Test: 0.5010\n",
            "Early stopping:  0.014832140942584221\n",
            "Epoch: 086, Loss: 1.3647, Train: 0.5562, Test: 0.5016\n",
            "Early stopping:  0.014440944154724269\n",
            "Epoch: 087, Loss: 1.3552, Train: 0.5641, Test: 0.5029\n",
            "Early stopping:  0.014475370759352244\n",
            "Epoch: 088, Loss: 1.3461, Train: 0.5661, Test: 0.5027\n",
            "Early stopping:  0.01453018600111791\n",
            "Epoch: 089, Loss: 1.3378, Train: 0.5661, Test: 0.5035\n",
            "Early stopping:  0.01424710473668226\n",
            "Epoch: 090, Loss: 1.3301, Train: 0.5669, Test: 0.5048\n",
            "Early stopping:  0.013713623041408083\n",
            "Epoch: 091, Loss: 1.3220, Train: 0.5715, Test: 0.5069\n",
            "Early stopping:  0.013018610171248186\n",
            "Epoch: 092, Loss: 1.3129, Train: 0.5740, Test: 0.5111\n",
            "Early stopping:  0.013012217931981209\n",
            "Epoch: 093, Loss: 1.3033, Train: 0.5766, Test: 0.5120\n",
            "Early stopping:  0.013628247617273397\n",
            "Epoch: 094, Loss: 1.2942, Train: 0.5760, Test: 0.5137\n",
            "Early stopping:  0.014320643081883659\n",
            "Epoch: 095, Loss: 1.2865, Train: 0.5797, Test: 0.5171\n",
            "Early stopping:  0.014182331826272382\n",
            "Epoch: 096, Loss: 1.2788, Train: 0.5828, Test: 0.5184\n",
            "Early stopping:  0.01345659465891391\n",
            "Epoch: 097, Loss: 1.2694, Train: 0.5871, Test: 0.5201\n",
            "Early stopping:  0.013169849259574574\n",
            "Epoch: 098, Loss: 1.2607, Train: 0.5893, Test: 0.5235\n",
            "Early stopping:  0.013295753550218488\n",
            "Epoch: 099, Loss: 1.2537, Train: 0.5893, Test: 0.5232\n",
            "Early stopping:  0.013256976969592197\n",
            "Epoch: 100, Loss: 1.2461, Train: 0.5913, Test: 0.5256\n",
            "Early stopping:  0.012824581939144564\n",
            "Epoch: 101, Loss: 1.2381, Train: 0.5936, Test: 0.5273\n",
            "Early stopping:  0.012219821280472368\n",
            "Epoch: 102, Loss: 1.2300, Train: 0.5995, Test: 0.5281\n",
            "Early stopping:  0.012192506536839396\n",
            "Epoch: 103, Loss: 1.2251, Train: 0.5956, Test: 0.5301\n",
            "Early stopping:  0.011643068703081842\n",
            "Epoch: 104, Loss: 1.2175, Train: 0.6024, Test: 0.5303\n",
            "Early stopping:  0.011147988081887031\n",
            "Epoch: 105, Loss: 1.2113, Train: 0.6018, Test: 0.5334\n",
            "Early stopping:  0.010475206997090758\n",
            "Epoch: 106, Loss: 1.2028, Train: 0.6001, Test: 0.5341\n",
            "Early stopping:  0.01080547212414348\n",
            "Epoch: 107, Loss: 1.1984, Train: 0.6052, Test: 0.5318\n",
            "Early stopping:  0.010804024535478707\n",
            "Epoch: 108, Loss: 1.1901, Train: 0.6075, Test: 0.5339\n",
            "Early stopping:  0.010745092543378252\n",
            "Epoch: 109, Loss: 1.1849, Train: 0.6066, Test: 0.5362\n",
            "Early stopping:  0.010409102810954635\n",
            "Epoch: 110, Loss: 1.1770, Train: 0.6129, Test: 0.5358\n",
            "Early stopping:  0.010327813825361484\n",
            "Epoch: 111, Loss: 1.1689, Train: 0.6137, Test: 0.5368\n",
            "Early stopping:  0.011410934345259454\n",
            "Epoch: 112, Loss: 1.1625, Train: 0.6146, Test: 0.5381\n",
            "Early stopping:  0.01126953523881141\n",
            "Epoch: 113, Loss: 1.1555, Train: 0.6163, Test: 0.5404\n",
            "Early stopping:  0.011601141579060427\n",
            "Epoch: 114, Loss: 1.1491, Train: 0.6222, Test: 0.5388\n",
            "Early stopping:  0.010953902753711194\n",
            "Epoch: 115, Loss: 1.1439, Train: 0.6239, Test: 0.5411\n",
            "Early stopping:  0.010012398756301352\n",
            "Epoch: 116, Loss: 1.1397, Train: 0.6251, Test: 0.5349\n",
            "Early stopping:  0.009083516830803514\n",
            "PREDICTIONS -> tensor([ 9,  5,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.42      0.51      0.46       568\n",
            "         capital_goods       0.36      0.23      0.28       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.39      0.46      0.42       595\n",
            " consumer_non-cyclical       0.64      0.32      0.43       334\n",
            "                energy       0.71      0.44      0.54       213\n",
            "             financial       0.66      0.60      0.63       576\n",
            "            healthcare       0.63      0.55      0.59       238\n",
            "              services       0.56      0.75      0.64      1557\n",
            "            technology       0.48      0.13      0.21       297\n",
            "        transportation       0.70      0.63      0.66       303\n",
            "             utilities       0.58      0.54      0.56       169\n",
            "\n",
            "              accuracy                           0.53      5291\n",
            "             macro avg       0.51      0.43      0.45      5291\n",
            "          weighted avg       0.53      0.53      0.52      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 61.4425, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 477.4500, Train: 0.0519, Test: 0.0520\n",
            "Early stopping:  294.16169978208603\n",
            "Epoch: 003, Loss: 382.2858, Train: 0.0570, Test: 0.0573\n",
            "Early stopping:  217.96745088284447\n",
            "Epoch: 004, Loss: 545.1262, Train: 0.1302, Test: 0.1315\n",
            "Early stopping:  214.10780133206762\n",
            "Epoch: 005, Loss: 341.0731, Train: 0.1222, Test: 0.1202\n",
            "Early stopping:  185.7732317516248\n",
            "Epoch: 006, Loss: 262.5145, Train: 0.1225, Test: 0.1230\n",
            "Early stopping:  111.50589219101218\n",
            "Epoch: 007, Loss: 224.9292, Train: 0.2774, Test: 0.2710\n",
            "Early stopping:  124.98565687911636\n",
            "Epoch: 008, Loss: 138.7693, Train: 0.3029, Test: 0.3005\n",
            "Early stopping:  153.93111606133218\n",
            "Epoch: 009, Loss: 137.0224, Train: 0.2904, Test: 0.2871\n",
            "Early stopping:  86.55991569346558\n",
            "Epoch: 010, Loss: 106.3335, Train: 0.2198, Test: 0.2181\n",
            "Early stopping:  66.36240827614948\n",
            "Epoch: 011, Loss: 70.3961, Train: 0.1659, Test: 0.1561\n",
            "Early stopping:  57.22719002616434\n",
            "Epoch: 012, Loss: 46.4610, Train: 0.1191, Test: 0.1123\n",
            "Early stopping:  40.79348091016196\n",
            "Epoch: 013, Loss: 24.2331, Train: 0.1778, Test: 0.1656\n",
            "Early stopping:  45.34220744399117\n",
            "Epoch: 014, Loss: 9.5508, Train: 0.2785, Test: 0.2552\n",
            "Early stopping:  38.38072411525406\n",
            "Epoch: 015, Loss: 4.6424, Train: 0.2334, Test: 0.2141\n",
            "Early stopping:  27.34803323662554\n",
            "Epoch: 016, Loss: 3.9760, Train: 0.2212, Test: 0.1975\n",
            "Early stopping:  17.993833790278302\n",
            "Epoch: 017, Loss: 3.5310, Train: 0.2215, Test: 0.2020\n",
            "Early stopping:  8.750952129300288\n",
            "Epoch: 018, Loss: 3.1619, Train: 0.2371, Test: 0.2230\n",
            "Early stopping:  2.6181757755668005\n",
            "Epoch: 019, Loss: 2.8226, Train: 0.2504, Test: 0.2402\n",
            "Early stopping:  0.7115751113876393\n",
            "Epoch: 020, Loss: 2.5797, Train: 0.2584, Test: 0.2476\n",
            "Early stopping:  0.556619006663935\n",
            "Epoch: 021, Loss: 2.4309, Train: 0.2677, Test: 0.2523\n",
            "Early stopping:  0.44589118508584435\n",
            "Epoch: 022, Loss: 2.3338, Train: 0.2779, Test: 0.2582\n",
            "Early stopping:  0.33298888929005876\n",
            "Epoch: 023, Loss: 2.2583, Train: 0.2930, Test: 0.2763\n",
            "Early stopping:  0.22368185947179217\n",
            "Epoch: 024, Loss: 2.1940, Train: 0.3168, Test: 0.3035\n",
            "Early stopping:  0.15157606807532942\n",
            "Epoch: 025, Loss: 2.1508, Train: 0.3483, Test: 0.3260\n",
            "Early stopping:  0.11180483320980245\n",
            "Epoch: 026, Loss: 2.1301, Train: 0.3721, Test: 0.3440\n",
            "Early stopping:  0.08327293092174043\n",
            "Epoch: 027, Loss: 2.1165, Train: 0.3834, Test: 0.3576\n",
            "Early stopping:  0.057413857016119854\n",
            "Epoch: 028, Loss: 2.0988, Train: 0.3902, Test: 0.3627\n",
            "Early stopping:  0.036612385461424375\n",
            "Epoch: 029, Loss: 2.0771, Train: 0.3897, Test: 0.3631\n",
            "Early stopping:  0.02834021402638297\n",
            "Epoch: 030, Loss: 2.0579, Train: 0.3942, Test: 0.3744\n",
            "Early stopping:  0.02915775512554616\n",
            "Epoch: 031, Loss: 2.0404, Train: 0.4027, Test: 0.3854\n",
            "Early stopping:  0.03056642148223439\n",
            "Epoch: 032, Loss: 2.0240, Train: 0.4047, Test: 0.3816\n",
            "Early stopping:  0.029486729962897966\n",
            "Epoch: 033, Loss: 2.0071, Train: 0.4050, Test: 0.3831\n",
            "Early stopping:  0.02752363882128422\n",
            "Epoch: 034, Loss: 1.9891, Train: 0.4087, Test: 0.3837\n",
            "Early stopping:  0.027030539237107968\n",
            "Epoch: 035, Loss: 1.9679, Train: 0.4112, Test: 0.3850\n",
            "Early stopping:  0.028492768898074325\n",
            "Epoch: 036, Loss: 1.9467, Train: 0.4149, Test: 0.3878\n",
            "Early stopping:  0.030695078649407615\n",
            "Epoch: 037, Loss: 1.9253, Train: 0.4183, Test: 0.3903\n",
            "Early stopping:  0.03257948822121886\n",
            "Epoch: 038, Loss: 1.9064, Train: 0.4243, Test: 0.3978\n",
            "Early stopping:  0.03288839335345333\n",
            "Epoch: 039, Loss: 1.8849, Train: 0.4387, Test: 0.4062\n",
            "Early stopping:  0.032631203975700435\n",
            "Epoch: 040, Loss: 1.8552, Train: 0.4509, Test: 0.4156\n",
            "Early stopping:  0.035450049616767496\n",
            "Epoch: 041, Loss: 1.8275, Train: 0.4609, Test: 0.4254\n",
            "Early stopping:  0.03917734761223825\n",
            "Epoch: 042, Loss: 1.8057, Train: 0.4671, Test: 0.4324\n",
            "Early stopping:  0.040971048011975875\n",
            "Epoch: 043, Loss: 1.7832, Train: 0.4776, Test: 0.4353\n",
            "Early stopping:  0.04006388538950416\n",
            "Epoch: 044, Loss: 1.7599, Train: 0.4844, Test: 0.4360\n",
            "Early stopping:  0.03716715093800099\n",
            "Epoch: 045, Loss: 1.7383, Train: 0.4833, Test: 0.4445\n",
            "Early stopping:  0.035466001242332956\n",
            "Epoch: 046, Loss: 1.7171, Train: 0.4841, Test: 0.4508\n",
            "Early stopping:  0.03513773219302836\n",
            "Epoch: 047, Loss: 1.6950, Train: 0.4872, Test: 0.4544\n",
            "Early stopping:  0.034677599823713434\n",
            "Epoch: 048, Loss: 1.6697, Train: 0.4929, Test: 0.4568\n",
            "Early stopping:  0.03538426264356989\n",
            "Epoch: 049, Loss: 1.6428, Train: 0.4994, Test: 0.4606\n",
            "Early stopping:  0.03772029305586673\n",
            "Epoch: 050, Loss: 1.6211, Train: 0.5014, Test: 0.4629\n",
            "Early stopping:  0.03864599441125745\n",
            "Epoch: 051, Loss: 1.6014, Train: 0.5034, Test: 0.4699\n",
            "Early stopping:  0.03737392253561992\n",
            "Epoch: 052, Loss: 1.5849, Train: 0.5065, Test: 0.4759\n",
            "Early stopping:  0.03351703753642962\n",
            "Epoch: 053, Loss: 1.5656, Train: 0.5094, Test: 0.4821\n",
            "Early stopping:  0.030162651437041107\n",
            "Epoch: 054, Loss: 1.5504, Train: 0.5082, Test: 0.4820\n",
            "Early stopping:  0.02800942770826968\n",
            "Epoch: 055, Loss: 1.5348, Train: 0.5122, Test: 0.4837\n",
            "Early stopping:  0.026525993440132248\n",
            "Epoch: 056, Loss: 1.5196, Train: 0.5153, Test: 0.4829\n",
            "Early stopping:  0.025546862408585554\n",
            "Epoch: 057, Loss: 1.5047, Train: 0.5196, Test: 0.4857\n",
            "Early stopping:  0.02412164546666571\n",
            "Epoch: 058, Loss: 1.4893, Train: 0.5250, Test: 0.4888\n",
            "Early stopping:  0.024075103759421596\n",
            "Epoch: 059, Loss: 1.4767, Train: 0.5278, Test: 0.4906\n",
            "Early stopping:  0.02317434770029242\n",
            "Epoch: 060, Loss: 1.4604, Train: 0.5312, Test: 0.4914\n",
            "Early stopping:  0.023188455145122355\n",
            "Epoch: 061, Loss: 1.4472, Train: 0.5349, Test: 0.4927\n",
            "Early stopping:  0.022781412827085276\n",
            "Epoch: 062, Loss: 1.4330, Train: 0.5434, Test: 0.4935\n",
            "Early stopping:  0.022490301203987678\n",
            "Epoch: 063, Loss: 1.4200, Train: 0.5474, Test: 0.4967\n",
            "Early stopping:  0.022287594032988148\n",
            "Epoch: 064, Loss: 1.4057, Train: 0.5522, Test: 0.5003\n",
            "Early stopping:  0.02158850881986811\n",
            "Epoch: 065, Loss: 1.3905, Train: 0.5525, Test: 0.5073\n",
            "Early stopping:  0.022249949369459784\n",
            "Epoch: 066, Loss: 1.3798, Train: 0.5562, Test: 0.5107\n",
            "Early stopping:  0.02150882056129827\n",
            "Epoch: 067, Loss: 1.3658, Train: 0.5649, Test: 0.5116\n",
            "Early stopping:  0.02126086432318541\n",
            "Epoch: 068, Loss: 1.3553, Train: 0.5644, Test: 0.5156\n",
            "Early stopping:  0.019897647374443182\n",
            "Epoch: 069, Loss: 1.3437, Train: 0.5703, Test: 0.5171\n",
            "Early stopping:  0.018700601170785057\n",
            "Epoch: 070, Loss: 1.3330, Train: 0.5726, Test: 0.5209\n",
            "Early stopping:  0.018311803826566515\n",
            "Epoch: 071, Loss: 1.3206, Train: 0.5771, Test: 0.5209\n",
            "Early stopping:  0.017838486678033904\n",
            "Epoch: 072, Loss: 1.3110, Train: 0.5805, Test: 0.5250\n",
            "Early stopping:  0.01766735695463845\n",
            "Epoch: 073, Loss: 1.2983, Train: 0.5825, Test: 0.5264\n",
            "Early stopping:  0.01784441503681964\n",
            "Epoch: 074, Loss: 1.2899, Train: 0.5851, Test: 0.5279\n",
            "Early stopping:  0.017194887731509255\n",
            "Epoch: 075, Loss: 1.2757, Train: 0.5856, Test: 0.5300\n",
            "Early stopping:  0.017571188097569364\n",
            "Epoch: 076, Loss: 1.2669, Train: 0.5916, Test: 0.5313\n",
            "Early stopping:  0.017557413451077017\n",
            "Epoch: 077, Loss: 1.2579, Train: 0.5936, Test: 0.5320\n",
            "Early stopping:  0.016449589216018516\n",
            "Epoch: 078, Loss: 1.2451, Train: 0.5939, Test: 0.5337\n",
            "Early stopping:  0.01702115942177298\n",
            "Epoch: 079, Loss: 1.2383, Train: 0.6012, Test: 0.5381\n",
            "Early stopping:  0.01532853522687808\n",
            "Epoch: 080, Loss: 1.2272, Train: 0.6049, Test: 0.5381\n",
            "Early stopping:  0.015721103733844454\n",
            "Epoch: 081, Loss: 1.2175, Train: 0.6078, Test: 0.5373\n",
            "Early stopping:  0.015681818574800496\n",
            "Epoch: 082, Loss: 1.2104, Train: 0.6078, Test: 0.5398\n",
            "Early stopping:  0.014336851953842936\n",
            "Epoch: 083, Loss: 1.1990, Train: 0.6089, Test: 0.5405\n",
            "Early stopping:  0.015128139131771662\n",
            "Epoch: 084, Loss: 1.1901, Train: 0.6112, Test: 0.5407\n",
            "Early stopping:  0.014666959619581936\n",
            "Epoch: 085, Loss: 1.1833, Train: 0.6143, Test: 0.5445\n",
            "Early stopping:  0.014060446966818036\n",
            "Epoch: 086, Loss: 1.1717, Train: 0.6200, Test: 0.5458\n",
            "Early stopping:  0.014770393685982031\n",
            "Epoch: 087, Loss: 1.1629, Train: 0.6217, Test: 0.5470\n",
            "Early stopping:  0.014355082562436176\n",
            "Epoch: 088, Loss: 1.1555, Train: 0.6279, Test: 0.5494\n",
            "Early stopping:  0.014207583169422253\n",
            "Epoch: 089, Loss: 1.1452, Train: 0.6288, Test: 0.5507\n",
            "Early stopping:  0.014651002004396818\n",
            "Epoch: 090, Loss: 1.1364, Train: 0.6273, Test: 0.5517\n",
            "Early stopping:  0.013987619269746742\n",
            "Epoch: 091, Loss: 1.1284, Train: 0.6384, Test: 0.5558\n",
            "Early stopping:  0.013975891004865704\n",
            "Epoch: 092, Loss: 1.1197, Train: 0.6441, Test: 0.5560\n",
            "Early stopping:  0.014003071047419846\n",
            "Epoch: 093, Loss: 1.1115, Train: 0.6461, Test: 0.5594\n",
            "Early stopping:  0.013275893990732063\n",
            "Epoch: 094, Loss: 1.1007, Train: 0.6461, Test: 0.5615\n",
            "Early stopping:  0.013976367113112082\n",
            "Epoch: 095, Loss: 1.0914, Train: 0.6517, Test: 0.5602\n",
            "Early stopping:  0.014699495951892927\n",
            "Epoch: 096, Loss: 1.0834, Train: 0.6537, Test: 0.5628\n",
            "Early stopping:  0.014682424845074624\n",
            "Epoch: 097, Loss: 1.0750, Train: 0.6531, Test: 0.5634\n",
            "Early stopping:  0.014325335366173059\n",
            "Epoch: 098, Loss: 1.0674, Train: 0.6560, Test: 0.5642\n",
            "Early stopping:  0.013140710392013174\n",
            "Epoch: 099, Loss: 1.0532, Train: 0.6662, Test: 0.5659\n",
            "Early stopping:  0.014744004469561624\n",
            "Epoch: 100, Loss: 1.0416, Train: 0.6685, Test: 0.5695\n",
            "Early stopping:  0.016775296973500522\n",
            "Epoch: 101, Loss: 1.0315, Train: 0.6696, Test: 0.5700\n",
            "Early stopping:  0.017893225227468466\n",
            "Epoch: 102, Loss: 1.0256, Train: 0.6744, Test: 0.5696\n",
            "Early stopping:  0.016806953120642967\n",
            "Epoch: 103, Loss: 1.0157, Train: 0.6753, Test: 0.5734\n",
            "Early stopping:  0.014460024751141203\n",
            "Epoch: 104, Loss: 1.0048, Train: 0.6815, Test: 0.5742\n",
            "Early stopping:  0.01420378045214214\n",
            "Epoch: 105, Loss: 0.9930, Train: 0.6838, Test: 0.5780\n",
            "Early stopping:  0.015586084842933015\n",
            "Epoch: 106, Loss: 0.9849, Train: 0.6883, Test: 0.5738\n",
            "Early stopping:  0.016499341244490637\n",
            "Epoch: 107, Loss: 0.9771, Train: 0.6937, Test: 0.5763\n",
            "Early stopping:  0.015418505917047739\n",
            "Epoch: 108, Loss: 0.9682, Train: 0.6934, Test: 0.5804\n",
            "Early stopping:  0.014114409483824902\n",
            "Epoch: 109, Loss: 0.9595, Train: 0.6903, Test: 0.5732\n",
            "Early stopping:  0.013212620620519493\n",
            "Epoch: 110, Loss: 0.9570, Train: 0.6980, Test: 0.5821\n",
            "Early stopping:  0.011750545594109476\n",
            "Epoch: 111, Loss: 0.9512, Train: 0.6991, Test: 0.5776\n",
            "Early stopping:  0.01013660804816317\n",
            "Epoch: 112, Loss: 0.9402, Train: 0.7056, Test: 0.5787\n",
            "Early stopping:  0.010374758815707638\n",
            "Epoch: 113, Loss: 0.9258, Train: 0.7039, Test: 0.5819\n",
            "Early stopping:  0.013867589692369783\n",
            "Epoch: 114, Loss: 0.9278, Train: 0.7062, Test: 0.5782\n",
            "Early stopping:  0.01383282492230036\n",
            "Epoch: 115, Loss: 0.9205, Train: 0.7079, Test: 0.5804\n",
            "Early stopping:  0.012462025049637948\n",
            "Epoch: 116, Loss: 0.9023, Train: 0.7067, Test: 0.5804\n",
            "Early stopping:  0.013816474427389022\n",
            "Epoch: 117, Loss: 0.9098, Train: 0.7116, Test: 0.5785\n",
            "Early stopping:  0.010879004940468742\n",
            "Epoch: 118, Loss: 0.9025, Train: 0.7184, Test: 0.5889\n",
            "Early stopping:  0.011279416692968927\n",
            "Epoch: 119, Loss: 0.8940, Train: 0.7127, Test: 0.5812\n",
            "Early stopping:  0.009928936482383292\n",
            "PREDICTIONS -> tensor([ 9,  1,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.59      0.59      0.59       568\n",
            "         capital_goods       0.38      0.52      0.44       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.58      0.49      0.53       595\n",
            " consumer_non-cyclical       0.63      0.40      0.49       334\n",
            "                energy       0.57      0.52      0.54       213\n",
            "             financial       0.69      0.58      0.63       576\n",
            "            healthcare       0.75      0.50      0.60       238\n",
            "              services       0.58      0.75      0.66      1557\n",
            "            technology       0.45      0.28      0.35       297\n",
            "        transportation       0.69      0.68      0.69       303\n",
            "             utilities       0.58      0.58      0.58       169\n",
            "\n",
            "              accuracy                           0.58      5291\n",
            "             macro avg       0.54      0.49      0.51      5291\n",
            "          weighted avg       0.58      0.58      0.57      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 45.4250, Train: 0.2941, Test: 0.2945\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 481.3965, Train: 0.0927, Test: 0.0883\n",
            "Early stopping:  308.2784097293969\n",
            "Epoch: 003, Loss: 491.0963, Train: 0.0910, Test: 0.0828\n",
            "Early stopping:  254.55457281047828\n",
            "Epoch: 004, Loss: 444.9410, Train: 0.0686, Test: 0.0692\n",
            "Early stopping:  214.4490107193216\n",
            "Epoch: 005, Loss: 322.6753, Train: 0.1035, Test: 0.1055\n",
            "Early stopping:  186.7130460331747\n",
            "Epoch: 006, Loss: 348.7273, Train: 0.0519, Test: 0.0514\n",
            "Early stopping:  77.41646149371435\n",
            "Epoch: 007, Loss: 275.1554, Train: 0.0397, Test: 0.0359\n",
            "Early stopping:  89.1005388100253\n",
            "Epoch: 008, Loss: 223.4855, Train: 0.1225, Test: 0.1200\n",
            "Early stopping:  83.25237998219072\n",
            "Epoch: 009, Loss: 226.2468, Train: 0.1931, Test: 0.1852\n",
            "Early stopping:  56.233079481979594\n",
            "Epoch: 010, Loss: 157.8737, Train: 0.2723, Test: 0.2771\n",
            "Early stopping:  70.82696648621263\n",
            "Epoch: 011, Loss: 149.3544, Train: 0.2924, Test: 0.2994\n",
            "Early stopping:  52.493145365110294\n",
            "Epoch: 012, Loss: 130.7567, Train: 0.2896, Test: 0.2916\n",
            "Early stopping:  44.309184737032346\n",
            "Epoch: 013, Loss: 94.6859, Train: 0.3023, Test: 0.3088\n",
            "Early stopping:  48.192409162181626\n",
            "Epoch: 014, Loss: 52.7654, Train: 0.1824, Test: 0.1811\n",
            "Early stopping:  43.38974169562308\n",
            "Epoch: 015, Loss: 50.4556, Train: 0.1858, Test: 0.1811\n",
            "Early stopping:  44.719071403284815\n",
            "Epoch: 016, Loss: 34.7712, Train: 0.1795, Test: 0.1741\n",
            "Early stopping:  39.32160504787766\n",
            "Epoch: 017, Loss: 25.2329, Train: 0.1980, Test: 0.1882\n",
            "Early stopping:  26.63407138886242\n",
            "Epoch: 018, Loss: 11.5545, Train: 0.1696, Test: 0.1627\n",
            "Early stopping:  17.317738230234585\n",
            "Epoch: 019, Loss: 6.5617, Train: 0.1546, Test: 0.1376\n",
            "Early stopping:  17.760306553470546\n",
            "Epoch: 020, Loss: 4.7563, Train: 0.1656, Test: 0.1565\n",
            "Early stopping:  12.953892965450274\n",
            "Epoch: 021, Loss: 3.8550, Train: 0.2178, Test: 0.2124\n",
            "Early stopping:  8.814314862528828\n",
            "Epoch: 022, Loss: 3.2171, Train: 0.2652, Test: 0.2572\n",
            "Early stopping:  3.3563185554041484\n",
            "Epoch: 023, Loss: 2.9420, Train: 0.3052, Test: 0.2914\n",
            "Early stopping:  1.460559104820576\n",
            "Epoch: 024, Loss: 2.7248, Train: 0.3210, Test: 0.3022\n",
            "Early stopping:  0.821071536493594\n",
            "Epoch: 025, Loss: 2.5662, Train: 0.3259, Test: 0.3086\n",
            "Early stopping:  0.5066497115643376\n",
            "Epoch: 026, Loss: 2.4584, Train: 0.3242, Test: 0.3013\n",
            "Early stopping:  0.30388877381011825\n",
            "Epoch: 027, Loss: 2.3909, Train: 0.3148, Test: 0.2943\n",
            "Early stopping:  0.22139122607879394\n",
            "Epoch: 028, Loss: 2.3532, Train: 0.3001, Test: 0.2854\n",
            "Early stopping:  0.15007862035049432\n",
            "Epoch: 029, Loss: 2.3219, Train: 0.2944, Test: 0.2769\n",
            "Early stopping:  0.0971771496174876\n",
            "Epoch: 030, Loss: 2.2876, Train: 0.2978, Test: 0.2782\n",
            "Early stopping:  0.06587895021530085\n",
            "Epoch: 031, Loss: 2.2401, Train: 0.2975, Test: 0.2769\n",
            "Early stopping:  0.05824673118523561\n",
            "Epoch: 032, Loss: 2.2284, Train: 0.2995, Test: 0.2795\n",
            "Early stopping:  0.052993992935568966\n",
            "Epoch: 033, Loss: 2.1958, Train: 0.3074, Test: 0.2884\n",
            "Early stopping:  0.04993361033213655\n",
            "Epoch: 034, Loss: 2.1507, Train: 0.3171, Test: 0.2986\n",
            "Early stopping:  0.05107510219144793\n",
            "Epoch: 035, Loss: 2.1345, Train: 0.3341, Test: 0.3188\n",
            "Early stopping:  0.046492007232997214\n",
            "Epoch: 036, Loss: 2.1177, Train: 0.3573, Test: 0.3332\n",
            "Early stopping:  0.04568552382096731\n",
            "Epoch: 037, Loss: 2.0892, Train: 0.3596, Test: 0.3495\n",
            "Early stopping:  0.03968635622276576\n",
            "Epoch: 038, Loss: 2.0599, Train: 0.3690, Test: 0.3565\n",
            "Early stopping:  0.0362543765602797\n",
            "Epoch: 039, Loss: 2.0391, Train: 0.3732, Test: 0.3661\n",
            "Early stopping:  0.03945286236244255\n",
            "Epoch: 040, Loss: 2.0128, Train: 0.3783, Test: 0.3663\n",
            "Early stopping:  0.041162273049258936\n",
            "Epoch: 041, Loss: 1.9775, Train: 0.3857, Test: 0.3670\n",
            "Early stopping:  0.04292996995027195\n",
            "Epoch: 042, Loss: 1.9548, Train: 0.3919, Test: 0.3678\n",
            "Early stopping:  0.043122091248505974\n",
            "Epoch: 043, Loss: 1.9395, Train: 0.3942, Test: 0.3735\n",
            "Early stopping:  0.04104269279382416\n",
            "Epoch: 044, Loss: 1.9230, Train: 0.3973, Test: 0.3789\n",
            "Early stopping:  0.03498713278249956\n",
            "Epoch: 045, Loss: 1.9044, Train: 0.4056, Test: 0.3835\n",
            "Early stopping:  0.028210073031347666\n",
            "Epoch: 046, Loss: 1.8891, Train: 0.4107, Test: 0.3878\n",
            "Early stopping:  0.02631923431820956\n",
            "Epoch: 047, Loss: 1.8730, Train: 0.4107, Test: 0.3948\n",
            "Early stopping:  0.026379094766837752\n",
            "Epoch: 048, Loss: 1.8450, Train: 0.4146, Test: 0.3977\n",
            "Early stopping:  0.029839679199554554\n",
            "Epoch: 049, Loss: 1.8292, Train: 0.4189, Test: 0.4012\n",
            "Early stopping:  0.03092902782395231\n",
            "Epoch: 050, Loss: 1.8182, Train: 0.4197, Test: 0.3995\n",
            "Early stopping:  0.02968949180904638\n",
            "Epoch: 051, Loss: 1.8005, Train: 0.4146, Test: 0.4014\n",
            "Early stopping:  0.027564130303640376\n",
            "Epoch: 052, Loss: 1.7836, Train: 0.4180, Test: 0.4029\n",
            "Early stopping:  0.02402198185637867\n",
            "Epoch: 053, Loss: 1.7727, Train: 0.4212, Test: 0.4039\n",
            "Early stopping:  0.02342066677607777\n",
            "Epoch: 054, Loss: 1.7552, Train: 0.4257, Test: 0.4018\n",
            "Early stopping:  0.024368221182890254\n",
            "Epoch: 055, Loss: 1.7402, Train: 0.4265, Test: 0.4075\n",
            "Early stopping:  0.023595285641123172\n",
            "Epoch: 056, Loss: 1.7261, Train: 0.4277, Test: 0.4101\n",
            "Early stopping:  0.023390795480282424\n",
            "Epoch: 057, Loss: 1.7080, Train: 0.4285, Test: 0.4111\n",
            "Early stopping:  0.025115270292735758\n",
            "Epoch: 058, Loss: 1.6939, Train: 0.4351, Test: 0.4158\n",
            "Early stopping:  0.0245068622964877\n",
            "Epoch: 059, Loss: 1.6788, Train: 0.4370, Test: 0.4164\n",
            "Early stopping:  0.024518104642400396\n",
            "Epoch: 060, Loss: 1.6658, Train: 0.4416, Test: 0.4156\n",
            "Early stopping:  0.0236991591603947\n",
            "Epoch: 061, Loss: 1.6481, Train: 0.4453, Test: 0.4192\n",
            "Early stopping:  0.02338782752648014\n",
            "Epoch: 062, Loss: 1.6345, Train: 0.4481, Test: 0.4211\n",
            "Early stopping:  0.02366057999631849\n",
            "Epoch: 063, Loss: 1.6222, Train: 0.4461, Test: 0.4192\n",
            "Early stopping:  0.022893746547815893\n",
            "Epoch: 064, Loss: 1.6092, Train: 0.4484, Test: 0.4211\n",
            "Early stopping:  0.022062210816604203\n",
            "Epoch: 065, Loss: 1.5986, Train: 0.4566, Test: 0.4218\n",
            "Early stopping:  0.01966096385528696\n",
            "Epoch: 066, Loss: 1.5852, Train: 0.4614, Test: 0.4247\n",
            "Early stopping:  0.019336867649991003\n",
            "Epoch: 067, Loss: 1.5712, Train: 0.4654, Test: 0.4281\n",
            "Early stopping:  0.019959985462642035\n",
            "Epoch: 068, Loss: 1.5576, Train: 0.4722, Test: 0.4315\n",
            "Early stopping:  0.020682705607128208\n",
            "Epoch: 069, Loss: 1.5453, Train: 0.4722, Test: 0.4349\n",
            "Early stopping:  0.021237976901328098\n",
            "Epoch: 070, Loss: 1.5332, Train: 0.4773, Test: 0.4409\n",
            "Early stopping:  0.02053143985523527\n",
            "Epoch: 071, Loss: 1.5192, Train: 0.4816, Test: 0.4426\n",
            "Early stopping:  0.0203138138701663\n",
            "Epoch: 072, Loss: 1.5079, Train: 0.4904, Test: 0.4476\n",
            "Early stopping:  0.019855686749996933\n",
            "Epoch: 073, Loss: 1.4951, Train: 0.4915, Test: 0.4494\n",
            "Early stopping:  0.019869169741317423\n",
            "Epoch: 074, Loss: 1.4822, Train: 0.4932, Test: 0.4527\n",
            "Early stopping:  0.019958340077206402\n",
            "Epoch: 075, Loss: 1.4711, Train: 0.5000, Test: 0.4561\n",
            "Early stopping:  0.01926844099122114\n",
            "Epoch: 076, Loss: 1.4621, Train: 0.4986, Test: 0.4545\n",
            "Early stopping:  0.018336988138061557\n",
            "Epoch: 077, Loss: 1.4496, Train: 0.5037, Test: 0.4555\n",
            "Early stopping:  0.017593676376397988\n",
            "Epoch: 078, Loss: 1.4385, Train: 0.5065, Test: 0.4578\n",
            "Early stopping:  0.017232509807941634\n",
            "Epoch: 079, Loss: 1.4243, Train: 0.5088, Test: 0.4619\n",
            "Early stopping:  0.018568471812585967\n",
            "Epoch: 080, Loss: 1.4175, Train: 0.5045, Test: 0.4613\n",
            "Early stopping:  0.01815709150092848\n",
            "Epoch: 081, Loss: 1.4036, Train: 0.5094, Test: 0.4619\n",
            "Early stopping:  0.01792253611350296\n",
            "Epoch: 082, Loss: 1.3954, Train: 0.5147, Test: 0.4634\n",
            "Early stopping:  0.016977349781320095\n",
            "Epoch: 083, Loss: 1.3830, Train: 0.5199, Test: 0.4670\n",
            "Early stopping:  0.016637710602190894\n",
            "Epoch: 084, Loss: 1.3722, Train: 0.5241, Test: 0.4717\n",
            "Early stopping:  0.017644193234009134\n",
            "Epoch: 085, Loss: 1.3624, Train: 0.5267, Test: 0.4714\n",
            "Early stopping:  0.016750639355511843\n",
            "Epoch: 086, Loss: 1.3507, Train: 0.5303, Test: 0.4704\n",
            "Early stopping:  0.01741001109949114\n",
            "Epoch: 087, Loss: 1.3421, Train: 0.5377, Test: 0.4742\n",
            "Early stopping:  0.0163322131750975\n",
            "Epoch: 088, Loss: 1.3308, Train: 0.5437, Test: 0.4770\n",
            "Early stopping:  0.01630817218508265\n",
            "Epoch: 089, Loss: 1.3220, Train: 0.5468, Test: 0.4808\n",
            "Early stopping:  0.01595283949537204\n",
            "Epoch: 090, Loss: 1.3113, Train: 0.5516, Test: 0.4806\n",
            "Early stopping:  0.015687661414740678\n",
            "Epoch: 091, Loss: 1.3029, Train: 0.5545, Test: 0.4848\n",
            "Early stopping:  0.015498941580805958\n",
            "Epoch: 092, Loss: 1.2929, Train: 0.5581, Test: 0.4850\n",
            "Early stopping:  0.015004506759303298\n",
            "Epoch: 093, Loss: 1.2842, Train: 0.5638, Test: 0.4895\n",
            "Early stopping:  0.014866367322850805\n",
            "Epoch: 094, Loss: 1.2752, Train: 0.5672, Test: 0.4925\n",
            "Early stopping:  0.014358360829640291\n",
            "Epoch: 095, Loss: 1.2644, Train: 0.5712, Test: 0.4927\n",
            "Early stopping:  0.01499450785388248\n",
            "Epoch: 096, Loss: 1.2560, Train: 0.5763, Test: 0.4991\n",
            "Early stopping:  0.014806922589298888\n",
            "Epoch: 097, Loss: 1.2447, Train: 0.5786, Test: 0.5043\n",
            "Early stopping:  0.015566468497322307\n",
            "Epoch: 098, Loss: 1.2362, Train: 0.5788, Test: 0.5084\n",
            "Early stopping:  0.01546530257629413\n",
            "Epoch: 099, Loss: 1.2266, Train: 0.5834, Test: 0.5116\n",
            "Early stopping:  0.015094594236517893\n",
            "Epoch: 100, Loss: 1.2136, Train: 0.5874, Test: 0.5137\n",
            "Early stopping:  0.01629105476478318\n",
            "Epoch: 101, Loss: 1.2091, Train: 0.5919, Test: 0.5194\n",
            "Early stopping:  0.01493707305080506\n",
            "Epoch: 102, Loss: 1.2015, Train: 0.5902, Test: 0.5180\n",
            "Early stopping:  0.013924538366364349\n",
            "Epoch: 103, Loss: 1.1930, Train: 0.5942, Test: 0.5163\n",
            "Early stopping:  0.01268371187625288\n",
            "Epoch: 104, Loss: 1.1958, Train: 0.5993, Test: 0.5201\n",
            "Early stopping:  0.008721738414395771\n",
            "PREDICTIONS -> tensor([ 9,  6,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.39      0.48      0.43       568\n",
            "         capital_goods       0.30      0.07      0.11       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.45      0.36      0.40       595\n",
            " consumer_non-cyclical       0.65      0.40      0.50       334\n",
            "                energy       0.54      0.54      0.54       213\n",
            "             financial       0.62      0.66      0.64       576\n",
            "            healthcare       0.67      0.48      0.56       238\n",
            "              services       0.50      0.76      0.61      1557\n",
            "            technology       0.56      0.28      0.38       297\n",
            "        transportation       0.70      0.52      0.60       303\n",
            "             utilities       0.66      0.42      0.51       169\n",
            "\n",
            "              accuracy                           0.52      5291\n",
            "             macro avg       0.50      0.41      0.44      5291\n",
            "          weighted avg       0.52      0.52      0.50      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 44.8691, Train: 0.2989, Test: 0.2982\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 281.2397, Train: 0.1134, Test: 0.1117\n",
            "Early stopping:  167.13923911833504\n",
            "Epoch: 003, Loss: 512.8523, Train: 0.1220, Test: 0.1215\n",
            "Early stopping:  233.9956406621144\n",
            "Epoch: 004, Loss: 598.3341, Train: 0.1095, Test: 0.1091\n",
            "Early stopping:  248.7809161353591\n",
            "Epoch: 005, Loss: 527.0493, Train: 0.0581, Test: 0.0575\n",
            "Early stopping:  228.13444500248917\n",
            "Epoch: 006, Loss: 384.3198, Train: 0.0732, Test: 0.0711\n",
            "Early stopping:  126.59498630924989\n",
            "Epoch: 007, Loss: 200.1475, Train: 0.0874, Test: 0.0841\n",
            "Early stopping:  156.9085317129225\n",
            "Epoch: 008, Loss: 138.1883, Train: 0.1489, Test: 0.1440\n",
            "Early stopping:  199.74469213119625\n",
            "Epoch: 009, Loss: 97.8501, Train: 0.2558, Test: 0.2470\n",
            "Early stopping:  180.99133035668945\n",
            "Epoch: 010, Loss: 80.0957, Train: 0.2691, Test: 0.2550\n",
            "Early stopping:  123.11593135544287\n",
            "Epoch: 011, Loss: 69.8511, Train: 0.3157, Test: 0.3100\n",
            "Early stopping:  53.183302636935196\n",
            "Epoch: 012, Loss: 64.0660, Train: 0.2365, Test: 0.2298\n",
            "Early stopping:  29.83446392030407\n",
            "Epoch: 013, Loss: 60.3474, Train: 0.2303, Test: 0.2281\n",
            "Early stopping:  15.06070002463068\n",
            "Epoch: 014, Loss: 49.1676, Train: 0.1900, Test: 0.1869\n",
            "Early stopping:  11.447332089643506\n",
            "Epoch: 015, Loss: 39.9162, Train: 0.2067, Test: 0.1943\n",
            "Early stopping:  12.030606990226312\n",
            "Epoch: 016, Loss: 32.4039, Train: 0.1906, Test: 0.1807\n",
            "Early stopping:  13.356252181658178\n",
            "Epoch: 017, Loss: 25.2893, Train: 0.2904, Test: 0.2731\n",
            "Early stopping:  13.802351065348502\n",
            "Epoch: 018, Loss: 17.9700, Train: 0.3454, Test: 0.3287\n",
            "Early stopping:  12.195491816842942\n",
            "Epoch: 019, Loss: 13.6097, Train: 0.3633, Test: 0.3425\n",
            "Early stopping:  10.642411631287713\n",
            "Epoch: 020, Loss: 8.5452, Train: 0.3599, Test: 0.3342\n",
            "Early stopping:  9.447924739890297\n",
            "Epoch: 021, Loss: 4.3211, Train: 0.3417, Test: 0.3236\n",
            "Early stopping:  8.166774586145465\n",
            "Epoch: 022, Loss: 2.7457, Train: 0.3417, Test: 0.3268\n",
            "Early stopping:  6.363271454687047\n",
            "Epoch: 023, Loss: 2.4911, Train: 0.3383, Test: 0.3281\n",
            "Early stopping:  4.730745549160454\n",
            "Epoch: 024, Loss: 2.4576, Train: 0.3341, Test: 0.3281\n",
            "Early stopping:  2.594598431438696\n",
            "Epoch: 025, Loss: 2.3571, Train: 0.3296, Test: 0.3215\n",
            "Early stopping:  0.8212574599578095\n",
            "Epoch: 026, Loss: 2.2897, Train: 0.3426, Test: 0.3349\n",
            "Early stopping:  0.17455936418719137\n",
            "Epoch: 027, Loss: 2.1951, Train: 0.3800, Test: 0.3542\n",
            "Early stopping:  0.12128813709895227\n",
            "Epoch: 028, Loss: 2.0800, Train: 0.3922, Test: 0.3644\n",
            "Early stopping:  0.14550560282933248\n",
            "Epoch: 029, Loss: 2.0427, Train: 0.3917, Test: 0.3651\n",
            "Early stopping:  0.1337970578488802\n",
            "Epoch: 030, Loss: 2.0108, Train: 0.3849, Test: 0.3693\n",
            "Early stopping:  0.11602103337061499\n",
            "Epoch: 031, Loss: 1.9866, Train: 0.3857, Test: 0.3633\n",
            "Early stopping:  0.08171569448335399\n",
            "Epoch: 032, Loss: 1.9853, Train: 0.3857, Test: 0.3648\n",
            "Early stopping:  0.04037794755709073\n",
            "Epoch: 033, Loss: 1.9798, Train: 0.3832, Test: 0.3610\n",
            "Early stopping:  0.026186519506933136\n",
            "Epoch: 034, Loss: 1.9684, Train: 0.3817, Test: 0.3587\n",
            "Early stopping:  0.015539386355699623\n",
            "Epoch: 035, Loss: 1.9558, Train: 0.3800, Test: 0.3636\n",
            "Early stopping:  0.013008317060038051\n",
            "Epoch: 036, Loss: 1.9384, Train: 0.3894, Test: 0.3729\n",
            "Early stopping:  0.018936385197459476\n",
            "Epoch: 037, Loss: 1.9171, Train: 0.3982, Test: 0.3833\n",
            "Early stopping:  0.024782049771486937\n",
            "Epoch: 038, Loss: 1.8944, Train: 0.4138, Test: 0.3871\n",
            "Early stopping:  0.029662618560482856\n",
            "Epoch: 039, Loss: 1.8721, Train: 0.4212, Test: 0.3988\n",
            "Early stopping:  0.03344944478580614\n",
            "Epoch: 040, Loss: 1.8466, Train: 0.4257, Test: 0.4109\n",
            "Early stopping:  0.03616192167906142\n",
            "Epoch: 041, Loss: 1.8253, Train: 0.4336, Test: 0.4171\n",
            "Early stopping:  0.03662459468077565\n",
            "Epoch: 042, Loss: 1.8005, Train: 0.4453, Test: 0.4218\n",
            "Early stopping:  0.03711459574128777\n",
            "Epoch: 043, Loss: 1.7775, Train: 0.4543, Test: 0.4309\n",
            "Early stopping:  0.037211091426928355\n",
            "Epoch: 044, Loss: 1.7514, Train: 0.4634, Test: 0.4336\n",
            "Early stopping:  0.037682557314294034\n",
            "Epoch: 045, Loss: 1.7283, Train: 0.4722, Test: 0.4345\n",
            "Early stopping:  0.03842691484506384\n",
            "Epoch: 046, Loss: 1.7068, Train: 0.4782, Test: 0.4356\n",
            "Early stopping:  0.03741950076088678\n",
            "Epoch: 047, Loss: 1.6890, Train: 0.4801, Test: 0.4355\n",
            "Early stopping:  0.03512296471966543\n",
            "Epoch: 048, Loss: 1.6706, Train: 0.4867, Test: 0.4407\n",
            "Early stopping:  0.03181198414101889\n",
            "Epoch: 049, Loss: 1.6539, Train: 0.4912, Test: 0.4440\n",
            "Early stopping:  0.029294915955381515\n",
            "Epoch: 050, Loss: 1.6361, Train: 0.4926, Test: 0.4483\n",
            "Early stopping:  0.027913071898294527\n",
            "Epoch: 051, Loss: 1.6191, Train: 0.4974, Test: 0.4510\n",
            "Early stopping:  0.027546275120203172\n",
            "Epoch: 052, Loss: 1.6030, Train: 0.5037, Test: 0.4589\n",
            "Early stopping:  0.02688959667912348\n",
            "Epoch: 053, Loss: 1.5867, Train: 0.5108, Test: 0.4674\n",
            "Early stopping:  0.026476431677153255\n",
            "Epoch: 054, Loss: 1.5714, Train: 0.5111, Test: 0.4723\n",
            "Early stopping:  0.025593254798826664\n",
            "Epoch: 055, Loss: 1.5566, Train: 0.5170, Test: 0.4774\n",
            "Early stopping:  0.024761461269122126\n",
            "Epoch: 056, Loss: 1.5424, Train: 0.5221, Test: 0.4778\n",
            "Early stopping:  0.023923618019154807\n",
            "Epoch: 057, Loss: 1.5277, Train: 0.5286, Test: 0.4840\n",
            "Early stopping:  0.023235867123433722\n",
            "Epoch: 058, Loss: 1.5132, Train: 0.5312, Test: 0.4891\n",
            "Early stopping:  0.022973662555748026\n",
            "Epoch: 059, Loss: 1.4998, Train: 0.5366, Test: 0.4912\n",
            "Early stopping:  0.022599689221975614\n",
            "Epoch: 060, Loss: 1.4875, Train: 0.5408, Test: 0.4954\n",
            "Early stopping:  0.021802218872442797\n",
            "Epoch: 061, Loss: 1.4752, Train: 0.5482, Test: 0.4997\n",
            "Early stopping:  0.020685606369959177\n",
            "Epoch: 062, Loss: 1.4621, Train: 0.5519, Test: 0.4986\n",
            "Early stopping:  0.020032917832445887\n",
            "Epoch: 063, Loss: 1.4494, Train: 0.5539, Test: 0.5014\n",
            "Early stopping:  0.0199310879456364\n",
            "Epoch: 064, Loss: 1.4378, Train: 0.5562, Test: 0.5050\n",
            "Early stopping:  0.019792086881517266\n",
            "Epoch: 065, Loss: 1.4265, Train: 0.5573, Test: 0.5058\n",
            "Early stopping:  0.01927626961589676\n",
            "Epoch: 066, Loss: 1.4153, Train: 0.5576, Test: 0.5056\n",
            "Early stopping:  0.01845766317375581\n",
            "Epoch: 067, Loss: 1.4044, Train: 0.5562, Test: 0.5056\n",
            "Early stopping:  0.01782486852904471\n",
            "Epoch: 068, Loss: 1.3942, Train: 0.5590, Test: 0.5073\n",
            "Early stopping:  0.017304351203852878\n",
            "Epoch: 069, Loss: 1.3843, Train: 0.5598, Test: 0.5078\n",
            "Early stopping:  0.016679722438911825\n",
            "Epoch: 070, Loss: 1.3748, Train: 0.5638, Test: 0.5084\n",
            "Early stopping:  0.015968710483438148\n",
            "Epoch: 071, Loss: 1.3654, Train: 0.5664, Test: 0.5099\n",
            "Early stopping:  0.015364834429395553\n",
            "Epoch: 072, Loss: 1.3562, Train: 0.5706, Test: 0.5118\n",
            "Early stopping:  0.014975190967993475\n",
            "Epoch: 073, Loss: 1.3471, Train: 0.5686, Test: 0.5105\n",
            "Early stopping:  0.014718423119489243\n",
            "Epoch: 074, Loss: 1.3390, Train: 0.5732, Test: 0.5162\n",
            "Early stopping:  0.014228830867400953\n",
            "Epoch: 075, Loss: 1.3360, Train: 0.5715, Test: 0.5114\n",
            "Early stopping:  0.01220474025988957\n",
            "Epoch: 076, Loss: 1.3249, Train: 0.5788, Test: 0.5173\n",
            "Early stopping:  0.011794077055922744\n",
            "Epoch: 077, Loss: 1.3137, Train: 0.5800, Test: 0.5188\n",
            "Early stopping:  0.01301183624551281\n",
            "Epoch: 078, Loss: 1.3038, Train: 0.5763, Test: 0.5169\n",
            "Early stopping:  0.01485646286398053\n",
            "Epoch: 079, Loss: 1.3020, Train: 0.5822, Test: 0.5209\n",
            "Early stopping:  0.014384800329998785\n",
            "Epoch: 080, Loss: 1.2901, Train: 0.5856, Test: 0.5228\n",
            "Early stopping:  0.013091317634628716\n",
            "Epoch: 081, Loss: 1.2797, Train: 0.5854, Test: 0.5207\n",
            "Early stopping:  0.013156957748948594\n",
            "Epoch: 082, Loss: 1.2751, Train: 0.5871, Test: 0.5230\n",
            "Early stopping:  0.012855395949647162\n",
            "Epoch: 083, Loss: 1.2613, Train: 0.5922, Test: 0.5264\n",
            "Early stopping:  0.015368065505350338\n",
            "Epoch: 084, Loss: 1.2561, Train: 0.5927, Test: 0.5224\n",
            "Early stopping:  0.013809436067523868\n",
            "Epoch: 085, Loss: 1.2476, Train: 0.5933, Test: 0.5252\n",
            "Early stopping:  0.013308185281139091\n",
            "Epoch: 086, Loss: 1.2384, Train: 0.5959, Test: 0.5288\n",
            "Early stopping:  0.013916654047655717\n",
            "Epoch: 087, Loss: 1.2333, Train: 0.5976, Test: 0.5286\n",
            "Early stopping:  0.011718412615052042\n",
            "Epoch: 088, Loss: 1.2226, Train: 0.6018, Test: 0.5288\n",
            "Early stopping:  0.012895270093599597\n",
            "Epoch: 089, Loss: 1.2157, Train: 0.6012, Test: 0.5328\n",
            "Early stopping:  0.012646054930206024\n",
            "Epoch: 090, Loss: 1.2088, Train: 0.6055, Test: 0.5330\n",
            "Early stopping:  0.01221274546528103\n",
            "Epoch: 091, Loss: 1.1998, Train: 0.6041, Test: 0.5303\n",
            "Early stopping:  0.012824612136691463\n",
            "Epoch: 092, Loss: 1.1938, Train: 0.6078, Test: 0.5318\n",
            "Early stopping:  0.011632721898964365\n",
            "Epoch: 093, Loss: 1.1856, Train: 0.6126, Test: 0.5298\n",
            "Early stopping:  0.01189111635361851\n",
            "Epoch: 094, Loss: 1.1779, Train: 0.6137, Test: 0.5335\n",
            "Early stopping:  0.012022567918286991\n",
            "Epoch: 095, Loss: 1.1726, Train: 0.6151, Test: 0.5318\n",
            "Early stopping:  0.011145193999448839\n",
            "Epoch: 096, Loss: 1.1659, Train: 0.6200, Test: 0.5334\n",
            "Early stopping:  0.010913774174145123\n",
            "Epoch: 097, Loss: 1.1560, Train: 0.6205, Test: 0.5375\n",
            "Early stopping:  0.01132045563852909\n",
            "Epoch: 098, Loss: 1.1500, Train: 0.6154, Test: 0.5362\n",
            "Early stopping:  0.011515879301225279\n",
            "Epoch: 099, Loss: 1.1471, Train: 0.6254, Test: 0.5337\n",
            "Early stopping:  0.010757333011437128\n",
            "Epoch: 100, Loss: 1.1390, Train: 0.6290, Test: 0.5360\n",
            "Early stopping:  0.010080987406755329\n",
            "Epoch: 101, Loss: 1.1296, Train: 0.6245, Test: 0.5398\n",
            "Early stopping:  0.010281676405895408\n",
            "Epoch: 102, Loss: 1.1295, Train: 0.6350, Test: 0.5377\n",
            "Early stopping:  0.009557669438567826\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.53      0.40      0.46       568\n",
            "         capital_goods       0.31      0.33      0.32       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.43      0.51      0.47       595\n",
            " consumer_non-cyclical       0.64      0.31      0.42       334\n",
            "                energy       0.58      0.46      0.51       213\n",
            "             financial       0.68      0.58      0.62       576\n",
            "            healthcare       0.72      0.61      0.66       238\n",
            "              services       0.55      0.77      0.64      1557\n",
            "            technology       0.54      0.25      0.34       297\n",
            "        transportation       0.57      0.53      0.55       303\n",
            "             utilities       0.58      0.46      0.51       169\n",
            "\n",
            "              accuracy                           0.54      5291\n",
            "             macro avg       0.51      0.43      0.46      5291\n",
            "          weighted avg       0.54      0.54      0.52      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 75.1803, Train: 0.3154, Test: 0.3062\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 326.7638, Train: 0.0323, Test: 0.0340\n",
            "Early stopping:  177.8963907277133\n",
            "Epoch: 003, Loss: 405.3710, Train: 0.0797, Test: 0.0756\n",
            "Early stopping:  172.48150437270058\n",
            "Epoch: 004, Loss: 268.2447, Train: 0.1809, Test: 0.1765\n",
            "Early stopping:  140.83121556432312\n",
            "Epoch: 005, Loss: 239.6844, Train: 0.2748, Test: 0.2765\n",
            "Early stopping:  122.66077672012595\n",
            "Epoch: 006, Loss: 189.9344, Train: 0.0788, Test: 0.0760\n",
            "Early stopping:  83.06639856034695\n",
            "Epoch: 007, Loss: 185.8397, Train: 0.0610, Test: 0.0575\n",
            "Early stopping:  89.44060085876671\n",
            "Epoch: 008, Loss: 176.4785, Train: 0.1140, Test: 0.1115\n",
            "Early stopping:  39.88393950987732\n",
            "Epoch: 009, Loss: 153.8968, Train: 0.1687, Test: 0.1658\n",
            "Early stopping:  31.49863763328566\n",
            "Epoch: 010, Loss: 96.2621, Train: 0.1739, Test: 0.1784\n",
            "Early stopping:  38.51591768116857\n",
            "Epoch: 011, Loss: 101.0500, Train: 0.2975, Test: 0.3007\n",
            "Early stopping:  41.88851352637057\n",
            "Epoch: 012, Loss: 110.9364, Train: 0.2989, Test: 0.3015\n",
            "Early stopping:  35.5147531573735\n",
            "Epoch: 013, Loss: 99.3974, Train: 0.3052, Test: 0.3045\n",
            "Early stopping:  23.8872533361381\n",
            "Epoch: 014, Loss: 71.4325, Train: 0.1458, Test: 0.1391\n",
            "Early stopping:  14.693561755358544\n",
            "Epoch: 015, Loss: 59.3373, Train: 0.1656, Test: 0.1682\n",
            "Early stopping:  21.916813060733922\n",
            "Epoch: 016, Loss: 51.6577, Train: 0.1557, Test: 0.1452\n",
            "Early stopping:  25.624580436850092\n",
            "Epoch: 017, Loss: 37.8892, Train: 0.1554, Test: 0.1446\n",
            "Early stopping:  23.25964604788819\n",
            "Epoch: 018, Loss: 28.4916, Train: 0.1838, Test: 0.1865\n",
            "Early stopping:  17.016785604349412\n",
            "Epoch: 019, Loss: 19.8990, Train: 0.1744, Test: 0.1778\n",
            "Early stopping:  16.196445358765203\n",
            "Epoch: 020, Loss: 17.9264, Train: 0.1923, Test: 0.1945\n",
            "Early stopping:  13.91489565057016\n",
            "Epoch: 021, Loss: 11.1576, Train: 0.2357, Test: 0.2355\n",
            "Early stopping:  10.337077390322957\n",
            "Epoch: 022, Loss: 5.9103, Train: 0.2337, Test: 0.2198\n",
            "Early stopping:  8.630067452555998\n",
            "Epoch: 023, Loss: 4.5674, Train: 0.2689, Test: 0.2546\n",
            "Early stopping:  6.900997967301444\n",
            "Epoch: 024, Loss: 3.8405, Train: 0.3185, Test: 0.3011\n",
            "Early stopping:  5.908461087103556\n",
            "Epoch: 025, Loss: 3.1798, Train: 0.3602, Test: 0.3527\n",
            "Early stopping:  3.19831756821744\n",
            "Epoch: 026, Loss: 2.6715, Train: 0.3718, Test: 0.3676\n",
            "Early stopping:  1.2677206535246621\n",
            "Epoch: 027, Loss: 2.5717, Train: 0.3732, Test: 0.3629\n",
            "Early stopping:  0.8391120832421283\n",
            "Epoch: 028, Loss: 2.6069, Train: 0.3792, Test: 0.3650\n",
            "Early stopping:  0.543431079850277\n",
            "Epoch: 029, Loss: 2.4837, Train: 0.3786, Test: 0.3670\n",
            "Early stopping:  0.27516217268298476\n",
            "Epoch: 030, Loss: 2.3047, Train: 0.3667, Test: 0.3525\n",
            "Early stopping:  0.14192555431645212\n",
            "Epoch: 031, Loss: 2.1751, Train: 0.3650, Test: 0.3504\n",
            "Early stopping:  0.18367411486549706\n",
            "Epoch: 032, Loss: 2.0696, Train: 0.3633, Test: 0.3479\n",
            "Early stopping:  0.2194952529105174\n",
            "Epoch: 033, Loss: 2.0541, Train: 0.3622, Test: 0.3404\n",
            "Early stopping:  0.1794653842843052\n",
            "Epoch: 034, Loss: 2.0908, Train: 0.3551, Test: 0.3370\n",
            "Early stopping:  0.10380114670156858\n",
            "Epoch: 035, Loss: 2.1204, Train: 0.3480, Test: 0.3345\n",
            "Early stopping:  0.047814668215021776\n",
            "Epoch: 036, Loss: 2.1306, Train: 0.3474, Test: 0.3362\n",
            "Early stopping:  0.03250583518445306\n",
            "Epoch: 037, Loss: 2.1302, Train: 0.3531, Test: 0.3368\n",
            "Early stopping:  0.03287518216824992\n",
            "Epoch: 038, Loss: 2.1230, Train: 0.3573, Test: 0.3379\n",
            "Early stopping:  0.016399462281373293\n",
            "Epoch: 039, Loss: 2.1107, Train: 0.3562, Test: 0.3323\n",
            "Early stopping:  0.008185376807977133\n",
            "PREDICTIONS -> tensor([9, 8, 0,  ..., 8, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.27      0.21      0.24       568\n",
            "         capital_goods       0.16      0.16      0.16       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.28      0.38      0.33       595\n",
            " consumer_non-cyclical       0.24      0.07      0.11       334\n",
            "                energy       0.15      0.22      0.18       213\n",
            "             financial       0.59      0.15      0.23       576\n",
            "            healthcare       0.25      0.49      0.33       238\n",
            "              services       0.39      0.56      0.46      1557\n",
            "            technology       0.45      0.31      0.37       297\n",
            "        transportation       0.61      0.38      0.47       303\n",
            "             utilities       0.20      0.02      0.03       169\n",
            "\n",
            "              accuracy                           0.33      5291\n",
            "             macro avg       0.30      0.25      0.24      5291\n",
            "          weighted avg       0.35      0.33      0.31      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 182.7376, Train: 0.0865, Test: 0.0883\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 200.2208, Train: 0.0961, Test: 0.0960\n",
            "Early stopping:  12.362521697410351\n",
            "Epoch: 003, Loss: 297.0530, Train: 0.1024, Test: 0.1047\n",
            "Early stopping:  61.57672669199641\n",
            "Epoch: 004, Loss: 504.0166, Train: 0.2941, Test: 0.2945\n",
            "Early stopping:  147.5059860887377\n",
            "Epoch: 005, Loss: 565.9239, Train: 0.2706, Test: 0.2727\n",
            "Early stopping:  175.7541876600924\n",
            "Epoch: 006, Loss: 442.3615, Train: 0.1937, Test: 0.1898\n",
            "Early stopping:  150.52918244845483\n",
            "Epoch: 007, Loss: 351.1262, Train: 0.1047, Test: 0.1092\n",
            "Early stopping:  109.52234015437544\n",
            "Epoch: 008, Loss: 272.1595, Train: 0.0851, Test: 0.0900\n",
            "Early stopping:  117.4730978890026\n",
            "Epoch: 009, Loss: 263.9707, Train: 0.3088, Test: 0.3083\n",
            "Early stopping:  126.86480897309768\n",
            "Epoch: 010, Loss: 206.4316, Train: 0.2130, Test: 0.2070\n",
            "Early stopping:  91.44534116477585\n",
            "Epoch: 011, Loss: 194.3599, Train: 0.2292, Test: 0.2259\n",
            "Early stopping:  62.48446033336068\n",
            "Epoch: 012, Loss: 178.7003, Train: 0.3120, Test: 0.3084\n",
            "Early stopping:  42.285973391060374\n",
            "Epoch: 013, Loss: 158.9716, Train: 0.2998, Test: 0.2943\n",
            "Early stopping:  39.69020419554039\n",
            "Epoch: 014, Loss: 131.2304, Train: 0.0757, Test: 0.0673\n",
            "Early stopping:  29.763648014266263\n",
            "Epoch: 015, Loss: 110.2925, Train: 0.1220, Test: 0.1132\n",
            "Early stopping:  34.24185399289063\n",
            "Epoch: 016, Loss: 77.7311, Train: 0.1588, Test: 0.1503\n",
            "Early stopping:  39.75997359254319\n",
            "Epoch: 017, Loss: 48.8665, Train: 0.2073, Test: 0.1966\n",
            "Early stopping:  43.366648620028954\n",
            "Epoch: 018, Loss: 32.3360, Train: 0.2368, Test: 0.2219\n",
            "Early stopping:  41.196000540518504\n",
            "Epoch: 019, Loss: 15.2506, Train: 0.1756, Test: 0.1633\n",
            "Early stopping:  37.70706940996281\n",
            "Epoch: 020, Loss: 5.0463, Train: 0.1716, Test: 0.1597\n",
            "Early stopping:  28.761982057341314\n",
            "Epoch: 021, Loss: 5.1970, Train: 0.1651, Test: 0.1559\n",
            "Early stopping:  18.98541668259696\n",
            "Epoch: 022, Loss: 5.0842, Train: 0.1812, Test: 0.1676\n",
            "Early stopping:  11.883643562786117\n",
            "Epoch: 023, Loss: 4.2559, Train: 0.1988, Test: 0.1879\n",
            "Early stopping:  4.64581231094826\n",
            "Epoch: 024, Loss: 3.3994, Train: 0.2059, Test: 0.1935\n",
            "Early stopping:  0.7664589025820675\n",
            "Epoch: 025, Loss: 2.7966, Train: 0.2065, Test: 0.1930\n",
            "Early stopping:  1.0458414469023796\n",
            "Epoch: 026, Loss: 2.4853, Train: 0.2076, Test: 0.1983\n",
            "Early stopping:  1.0678446542583122\n",
            "Epoch: 027, Loss: 2.3650, Train: 0.2099, Test: 0.2009\n",
            "Early stopping:  0.7791033847880422\n",
            "Epoch: 028, Loss: 2.2967, Train: 0.2127, Test: 0.2068\n",
            "Early stopping:  0.4512595667003783\n",
            "Epoch: 029, Loss: 2.2488, Train: 0.2170, Test: 0.2105\n",
            "Early stopping:  0.2190656806906634\n",
            "Epoch: 030, Loss: 2.2146, Train: 0.2258, Test: 0.2155\n",
            "Early stopping:  0.1073149662151636\n",
            "Epoch: 031, Loss: 2.1895, Train: 0.2286, Test: 0.2262\n",
            "Early stopping:  0.06981553613354091\n",
            "Epoch: 032, Loss: 2.1726, Train: 0.2374, Test: 0.2374\n",
            "Early stopping:  0.04952825976482574\n",
            "Epoch: 033, Loss: 2.1634, Train: 0.2419, Test: 0.2474\n",
            "Early stopping:  0.03451962190415782\n",
            "Epoch: 034, Loss: 2.1572, Train: 0.2496, Test: 0.2550\n",
            "Early stopping:  0.023095157695720597\n",
            "Epoch: 035, Loss: 2.1509, Train: 0.2587, Test: 0.2620\n",
            "Early stopping:  0.0150174623476483\n",
            "Epoch: 036, Loss: 2.1436, Train: 0.2725, Test: 0.2712\n",
            "Early stopping:  0.011175576037776993\n",
            "Epoch: 037, Loss: 2.1356, Train: 0.2819, Test: 0.2809\n",
            "Early stopping:  0.010980881214712005\n",
            "Epoch: 038, Loss: 2.1271, Train: 0.2918, Test: 0.2905\n",
            "Early stopping:  0.011969825563962637\n",
            "Epoch: 039, Loss: 2.1182, Train: 0.2995, Test: 0.2973\n",
            "Early stopping:  0.01297172527353148\n",
            "Epoch: 040, Loss: 2.1086, Train: 0.3094, Test: 0.3067\n",
            "Early stopping:  0.013816939247238834\n",
            "Epoch: 041, Loss: 2.0983, Train: 0.3233, Test: 0.3162\n",
            "Early stopping:  0.014707951479927785\n",
            "Epoch: 042, Loss: 2.0873, Train: 0.3327, Test: 0.3258\n",
            "Early stopping:  0.01573346091186989\n",
            "Epoch: 043, Loss: 2.0762, Train: 0.3452, Test: 0.3362\n",
            "Early stopping:  0.016659399579426637\n",
            "Epoch: 044, Loss: 2.0647, Train: 0.3551, Test: 0.3368\n",
            "Early stopping:  0.01739814219826036\n",
            "Epoch: 045, Loss: 2.0534, Train: 0.3616, Test: 0.3442\n",
            "Early stopping:  0.017804724990447978\n",
            "Epoch: 046, Loss: 2.0416, Train: 0.3721, Test: 0.3487\n",
            "Early stopping:  0.018060963892907494\n",
            "Epoch: 047, Loss: 2.0297, Train: 0.3763, Test: 0.3572\n",
            "Early stopping:  0.018344087981600476\n",
            "Epoch: 048, Loss: 2.0173, Train: 0.3815, Test: 0.3631\n",
            "Early stopping:  0.01874813847216205\n",
            "Epoch: 049, Loss: 2.0040, Train: 0.3874, Test: 0.3697\n",
            "Early stopping:  0.01947677803962565\n",
            "Epoch: 050, Loss: 1.9899, Train: 0.3965, Test: 0.3746\n",
            "Early stopping:  0.020445496633210946\n",
            "Epoch: 051, Loss: 1.9749, Train: 0.4019, Test: 0.3820\n",
            "Early stopping:  0.021663188404679116\n",
            "Epoch: 052, Loss: 1.9590, Train: 0.4058, Test: 0.3863\n",
            "Early stopping:  0.023032370621252442\n",
            "Epoch: 053, Loss: 1.9414, Train: 0.4146, Test: 0.3926\n",
            "Early stopping:  0.02469472064260674\n",
            "Epoch: 054, Loss: 1.9226, Train: 0.4214, Test: 0.3990\n",
            "Early stopping:  0.02660001547725963\n",
            "Epoch: 055, Loss: 1.9034, Train: 0.4231, Test: 0.4045\n",
            "Early stopping:  0.028392209599352455\n",
            "Epoch: 056, Loss: 1.8851, Train: 0.4271, Test: 0.4056\n",
            "Early stopping:  0.02938358231596016\n",
            "Epoch: 057, Loss: 1.8717, Train: 0.4285, Test: 0.4079\n",
            "Early stopping:  0.02804110411899062\n",
            "Epoch: 058, Loss: 1.8642, Train: 0.4342, Test: 0.4105\n",
            "Early stopping:  0.023804607593867558\n",
            "Epoch: 059, Loss: 1.8522, Train: 0.4441, Test: 0.4149\n",
            "Early stopping:  0.019717877680883737\n",
            "Epoch: 060, Loss: 1.8381, Train: 0.4524, Test: 0.4205\n",
            "Early stopping:  0.01803049606879564\n",
            "Epoch: 061, Loss: 1.8233, Train: 0.4555, Test: 0.4254\n",
            "Early stopping:  0.019573656030188485\n",
            "Epoch: 062, Loss: 1.8112, Train: 0.4614, Test: 0.4287\n",
            "Early stopping:  0.021336287794266683\n",
            "Epoch: 063, Loss: 1.8022, Train: 0.4631, Test: 0.4321\n",
            "Early stopping:  0.02014156364835758\n",
            "Epoch: 064, Loss: 1.7941, Train: 0.4648, Test: 0.4338\n",
            "Early stopping:  0.01739996619498605\n",
            "Epoch: 065, Loss: 1.7848, Train: 0.4651, Test: 0.4332\n",
            "Early stopping:  0.014918761939254479\n",
            "Epoch: 066, Loss: 1.7740, Train: 0.4691, Test: 0.4355\n",
            "Early stopping:  0.014525562031097509\n",
            "Epoch: 067, Loss: 1.7617, Train: 0.4736, Test: 0.4398\n",
            "Early stopping:  0.016004574892342974\n",
            "Epoch: 068, Loss: 1.7494, Train: 0.4793, Test: 0.4434\n",
            "Early stopping:  0.017798835199499423\n",
            "Epoch: 069, Loss: 1.7382, Train: 0.4836, Test: 0.4489\n",
            "Early stopping:  0.018633044042073776\n",
            "Epoch: 070, Loss: 1.7277, Train: 0.4838, Test: 0.4523\n",
            "Early stopping:  0.018367398228203484\n",
            "Epoch: 071, Loss: 1.7173, Train: 0.4818, Test: 0.4549\n",
            "Early stopping:  0.01748795820755795\n",
            "Epoch: 072, Loss: 1.7072, Train: 0.4824, Test: 0.4572\n",
            "Early stopping:  0.016643338766621458\n",
            "Epoch: 073, Loss: 1.6983, Train: 0.4833, Test: 0.4578\n",
            "Early stopping:  0.01589128928088902\n",
            "Epoch: 074, Loss: 1.6901, Train: 0.4867, Test: 0.4608\n",
            "Early stopping:  0.014933998851897131\n",
            "Epoch: 075, Loss: 1.6819, Train: 0.4884, Test: 0.4648\n",
            "Early stopping:  0.013920125707240834\n",
            "Epoch: 076, Loss: 1.6731, Train: 0.4901, Test: 0.4687\n",
            "Early stopping:  0.013372385367966491\n",
            "Epoch: 077, Loss: 1.6642, Train: 0.4952, Test: 0.4680\n",
            "Early stopping:  0.013441619629783822\n",
            "Epoch: 078, Loss: 1.6558, Train: 0.4989, Test: 0.4687\n",
            "Early stopping:  0.013647580547710833\n",
            "Epoch: 079, Loss: 1.6476, Train: 0.5034, Test: 0.4682\n",
            "Early stopping:  0.013608473819673646\n",
            "Epoch: 080, Loss: 1.6388, Train: 0.5040, Test: 0.4702\n",
            "Early stopping:  0.013490850971798252\n",
            "Epoch: 081, Loss: 1.6302, Train: 0.5071, Test: 0.4700\n",
            "Early stopping:  0.013462316183518673\n",
            "Epoch: 082, Loss: 1.6219, Train: 0.5077, Test: 0.4712\n",
            "Early stopping:  0.013460217825059606\n",
            "Epoch: 083, Loss: 1.6136, Train: 0.5102, Test: 0.4725\n",
            "Early stopping:  0.013397745086614602\n",
            "Epoch: 084, Loss: 1.6052, Train: 0.5125, Test: 0.4725\n",
            "Early stopping:  0.013246678956763265\n",
            "Epoch: 085, Loss: 1.5968, Train: 0.5125, Test: 0.4733\n",
            "Early stopping:  0.013185955062410609\n",
            "Epoch: 086, Loss: 1.5888, Train: 0.5130, Test: 0.4751\n",
            "Early stopping:  0.013147819292633957\n",
            "Epoch: 087, Loss: 1.5806, Train: 0.5159, Test: 0.4767\n",
            "Early stopping:  0.013048112314308984\n",
            "Epoch: 088, Loss: 1.5724, Train: 0.5167, Test: 0.4778\n",
            "Early stopping:  0.012939961262411108\n",
            "Epoch: 089, Loss: 1.5645, Train: 0.5170, Test: 0.4810\n",
            "Early stopping:  0.012796863379934487\n",
            "Epoch: 090, Loss: 1.5568, Train: 0.5213, Test: 0.4829\n",
            "Early stopping:  0.012632476486897875\n",
            "Epoch: 091, Loss: 1.5492, Train: 0.5241, Test: 0.4833\n",
            "Early stopping:  0.012396890576675437\n",
            "Epoch: 092, Loss: 1.5421, Train: 0.5221, Test: 0.4823\n",
            "Early stopping:  0.01201084852201472\n",
            "Epoch: 093, Loss: 1.5354, Train: 0.5241, Test: 0.4850\n",
            "Early stopping:  0.011548962822516304\n",
            "Epoch: 094, Loss: 1.5287, Train: 0.5250, Test: 0.4861\n",
            "Early stopping:  0.011069095211257894\n",
            "Epoch: 095, Loss: 1.5221, Train: 0.5281, Test: 0.4884\n",
            "Early stopping:  0.010670149748455515\n",
            "Epoch: 096, Loss: 1.5156, Train: 0.5289, Test: 0.4888\n",
            "Early stopping:  0.010471764753629995\n",
            "Epoch: 097, Loss: 1.5092, Train: 0.5301, Test: 0.4882\n",
            "Early stopping:  0.010374975993935685\n",
            "Epoch: 098, Loss: 1.5027, Train: 0.5306, Test: 0.4889\n",
            "Early stopping:  0.010284060543517052\n",
            "Epoch: 099, Loss: 1.4962, Train: 0.5318, Test: 0.4901\n",
            "Early stopping:  0.010230068343598648\n",
            "Epoch: 100, Loss: 1.4899, Train: 0.5329, Test: 0.4914\n",
            "Early stopping:  0.01018356616706202\n",
            "Epoch: 101, Loss: 1.4837, Train: 0.5343, Test: 0.4925\n",
            "Early stopping:  0.010084008028098295\n",
            "Epoch: 102, Loss: 1.4776, Train: 0.5349, Test: 0.4927\n",
            "Early stopping:  0.009924451458459558\n",
            "PREDICTIONS -> tensor([ 9,  0,  9,  ..., 10,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.61      0.49       568\n",
            "         capital_goods       0.35      0.15      0.21       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.44      0.37      0.40       595\n",
            " consumer_non-cyclical       0.75      0.16      0.27       334\n",
            "                energy       0.00      0.00      0.00       213\n",
            "             financial       0.61      0.55      0.58       576\n",
            "            healthcare       0.77      0.55      0.64       238\n",
            "              services       0.49      0.78      0.60      1557\n",
            "            technology       0.43      0.35      0.39       297\n",
            "        transportation       0.62      0.51      0.56       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.49      5291\n",
            "             macro avg       0.41      0.34      0.34      5291\n",
            "          weighted avg       0.47      0.49      0.45      5291\n",
            "\n",
            "time: 2min 47s (started: 2024-10-16 21:17:55 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSlXhl4egSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e22f4a-33e1-4924-9a2b-c9e3a4e6f438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 457 ms (started: 2024-10-16 21:20:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPHNFqI6gSl-"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duldbVZWgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ea98ed-acc9-491a-85c8-035485c30f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5349, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2907, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17268676511713973\n",
            "Epoch: 003, Loss: 2.1603, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.19016417184888382\n",
            "Epoch: 004, Loss: 2.1378, Train: 0.2986, Test: 0.2984\n",
            "Early stopping:  0.18223976877522258\n",
            "Epoch: 005, Loss: 2.0677, Train: 0.3599, Test: 0.3525\n",
            "Early stopping:  0.1844055469448322\n",
            "Epoch: 006, Loss: 1.9993, Train: 0.4124, Test: 0.3982\n",
            "Early stopping:  0.10928324167733866\n",
            "Epoch: 007, Loss: 1.9346, Train: 0.4214, Test: 0.4046\n",
            "Early stopping:  0.09431104831616102\n",
            "Epoch: 008, Loss: 1.8744, Train: 0.4132, Test: 0.3975\n",
            "Early stopping:  0.10438391248713279\n",
            "Epoch: 009, Loss: 1.8191, Train: 0.4070, Test: 0.3978\n",
            "Early stopping:  0.09843184336140279\n",
            "Epoch: 010, Loss: 1.7646, Train: 0.4229, Test: 0.4082\n",
            "Early stopping:  0.09254425453936702\n",
            "Epoch: 011, Loss: 1.7057, Train: 0.4478, Test: 0.4304\n",
            "Early stopping:  0.08977309729015655\n",
            "Epoch: 012, Loss: 1.6429, Train: 0.4816, Test: 0.4534\n",
            "Early stopping:  0.09116767726441914\n",
            "Epoch: 013, Loss: 1.5845, Train: 0.5031, Test: 0.4659\n",
            "Early stopping:  0.09344842688572329\n",
            "Epoch: 014, Loss: 1.5335, Train: 0.5179, Test: 0.4821\n",
            "Early stopping:  0.09229367223216406\n",
            "Epoch: 015, Loss: 1.4821, Train: 0.5366, Test: 0.4969\n",
            "Early stopping:  0.08811590339141612\n",
            "Epoch: 016, Loss: 1.4258, Train: 0.5485, Test: 0.5092\n",
            "Early stopping:  0.08487662721342953\n",
            "Epoch: 017, Loss: 1.3740, Train: 0.5712, Test: 0.5298\n",
            "Early stopping:  0.08360221732759979\n",
            "Epoch: 018, Loss: 1.3287, Train: 0.5947, Test: 0.5507\n",
            "Early stopping:  0.0819149891490534\n",
            "Epoch: 019, Loss: 1.2837, Train: 0.6126, Test: 0.5668\n",
            "Early stopping:  0.078197510181789\n",
            "Epoch: 020, Loss: 1.2419, Train: 0.6271, Test: 0.5787\n",
            "Early stopping:  0.07249066051797239\n",
            "Epoch: 021, Loss: 1.2052, Train: 0.6347, Test: 0.5851\n",
            "Early stopping:  0.06717026396507018\n",
            "Epoch: 022, Loss: 1.1684, Train: 0.6410, Test: 0.5912\n",
            "Early stopping:  0.06314692824188298\n",
            "Epoch: 023, Loss: 1.1336, Train: 0.6370, Test: 0.5921\n",
            "Early stopping:  0.059110867736767006\n",
            "Epoch: 024, Loss: 1.1059, Train: 0.6452, Test: 0.5967\n",
            "Early stopping:  0.0544126113796475\n",
            "Epoch: 025, Loss: 1.0791, Train: 0.6605, Test: 0.6067\n",
            "Early stopping:  0.04988978815175193\n",
            "Epoch: 026, Loss: 1.0516, Train: 0.6750, Test: 0.6144\n",
            "Early stopping:  0.04563376384657268\n",
            "Epoch: 027, Loss: 1.0269, Train: 0.6826, Test: 0.6201\n",
            "Early stopping:  0.042330040470661016\n",
            "Epoch: 028, Loss: 1.0023, Train: 0.6872, Test: 0.6262\n",
            "Early stopping:  0.0410200964890953\n",
            "Epoch: 029, Loss: 0.9790, Train: 0.6946, Test: 0.6314\n",
            "Early stopping:  0.03947370811962734\n",
            "Epoch: 030, Loss: 0.9562, Train: 0.7062, Test: 0.6386\n",
            "Early stopping:  0.03773925721121433\n",
            "Epoch: 031, Loss: 0.9328, Train: 0.7153, Test: 0.6415\n",
            "Early stopping:  0.037043597262176944\n",
            "Epoch: 032, Loss: 0.9122, Train: 0.7249, Test: 0.6464\n",
            "Early stopping:  0.03579966520733244\n",
            "Epoch: 033, Loss: 0.8917, Train: 0.7331, Test: 0.6485\n",
            "Early stopping:  0.034591206904250146\n",
            "Epoch: 034, Loss: 0.8711, Train: 0.7436, Test: 0.6530\n",
            "Early stopping:  0.03344392307414004\n",
            "Epoch: 035, Loss: 0.8516, Train: 0.7507, Test: 0.6581\n",
            "Early stopping:  0.032189292196993236\n",
            "Epoch: 036, Loss: 0.8327, Train: 0.7587, Test: 0.6623\n",
            "Early stopping:  0.0314763413340864\n",
            "Epoch: 037, Loss: 0.8153, Train: 0.7663, Test: 0.6657\n",
            "Early stopping:  0.03023351819824959\n",
            "Epoch: 038, Loss: 0.7972, Train: 0.7723, Test: 0.6641\n",
            "Early stopping:  0.029125174422878753\n",
            "Epoch: 039, Loss: 0.7810, Train: 0.7791, Test: 0.6664\n",
            "Early stopping:  0.02793851211949012\n",
            "Epoch: 040, Loss: 0.7648, Train: 0.7825, Test: 0.6687\n",
            "Early stopping:  0.026905982512262387\n",
            "Epoch: 041, Loss: 0.7487, Train: 0.7901, Test: 0.6715\n",
            "Early stopping:  0.026177353833369537\n",
            "Epoch: 042, Loss: 0.7322, Train: 0.7955, Test: 0.6764\n",
            "Early stopping:  0.025653251554084936\n",
            "Epoch: 043, Loss: 0.7168, Train: 0.8006, Test: 0.6804\n",
            "Early stopping:  0.025478452251387982\n",
            "Epoch: 044, Loss: 0.7009, Train: 0.8077, Test: 0.6847\n",
            "Early stopping:  0.025261523950946673\n",
            "Epoch: 045, Loss: 0.6851, Train: 0.8086, Test: 0.6857\n",
            "Early stopping:  0.025075961903412596\n",
            "Epoch: 046, Loss: 0.6694, Train: 0.8091, Test: 0.6863\n",
            "Early stopping:  0.024863583365622752\n",
            "Epoch: 047, Loss: 0.6544, Train: 0.8145, Test: 0.6912\n",
            "Early stopping:  0.024680960791796196\n",
            "Epoch: 048, Loss: 0.6389, Train: 0.8253, Test: 0.6906\n",
            "Early stopping:  0.02443521790750075\n",
            "Epoch: 049, Loss: 0.6237, Train: 0.8281, Test: 0.6933\n",
            "Early stopping:  0.02422958214628029\n",
            "Epoch: 050, Loss: 0.6090, Train: 0.8355, Test: 0.6972\n",
            "Early stopping:  0.023977127002768345\n",
            "Epoch: 051, Loss: 0.5939, Train: 0.8415, Test: 0.6978\n",
            "Early stopping:  0.023891567171226348\n",
            "Epoch: 052, Loss: 0.5791, Train: 0.8471, Test: 0.6987\n",
            "Early stopping:  0.023620635153457713\n",
            "Epoch: 053, Loss: 0.5646, Train: 0.8537, Test: 0.7006\n",
            "Early stopping:  0.02339160126535564\n",
            "Epoch: 054, Loss: 0.5501, Train: 0.8588, Test: 0.7021\n",
            "Early stopping:  0.02324544572248278\n",
            "Epoch: 055, Loss: 0.5358, Train: 0.8630, Test: 0.7040\n",
            "Early stopping:  0.0229624094083999\n",
            "Epoch: 056, Loss: 0.5221, Train: 0.8673, Test: 0.7044\n",
            "Early stopping:  0.022613853866284846\n",
            "Epoch: 057, Loss: 0.5100, Train: 0.8627, Test: 0.7016\n",
            "Early stopping:  0.021733370139871165\n",
            "Epoch: 058, Loss: 0.5061, Train: 0.8539, Test: 0.6967\n",
            "Early stopping:  0.018279362412687163\n",
            "Epoch: 059, Loss: 0.5205, Train: 0.8752, Test: 0.7031\n",
            "Early stopping:  0.01163107135046718\n",
            "Epoch: 060, Loss: 0.4821, Train: 0.8786, Test: 0.7036\n",
            "Early stopping:  0.016045912017024086\n",
            "Epoch: 061, Loss: 0.4685, Train: 0.8724, Test: 0.7046\n",
            "Early stopping:  0.021413122397262615\n",
            "Epoch: 062, Loss: 0.4732, Train: 0.8928, Test: 0.7099\n",
            "Early stopping:  0.022331391088973602\n",
            "Epoch: 063, Loss: 0.4422, Train: 0.8775, Test: 0.7042\n",
            "Early stopping:  0.028339149795388728\n",
            "Epoch: 064, Loss: 0.4536, Train: 0.8917, Test: 0.7116\n",
            "Early stopping:  0.015935037795740282\n",
            "Epoch: 065, Loss: 0.4248, Train: 0.8971, Test: 0.7074\n",
            "Early stopping:  0.019728160803331476\n",
            "Epoch: 066, Loss: 0.4266, Train: 0.9061, Test: 0.7129\n",
            "Early stopping:  0.020111124057189452\n",
            "Epoch: 067, Loss: 0.4056, Train: 0.9030, Test: 0.7088\n",
            "Early stopping:  0.018293223606151945\n",
            "Epoch: 068, Loss: 0.4038, Train: 0.9107, Test: 0.7137\n",
            "Early stopping:  0.020157133190333888\n",
            "Epoch: 069, Loss: 0.3907, Train: 0.9138, Test: 0.7133\n",
            "Early stopping:  0.015208453134352184\n",
            "Epoch: 070, Loss: 0.3833, Train: 0.9175, Test: 0.7118\n",
            "Early stopping:  0.0165789777001906\n",
            "Epoch: 071, Loss: 0.3731, Train: 0.9220, Test: 0.7148\n",
            "Early stopping:  0.013770276972369619\n",
            "Epoch: 072, Loss: 0.3631, Train: 0.9254, Test: 0.7133\n",
            "Early stopping:  0.01569719292588396\n",
            "Epoch: 073, Loss: 0.3571, Train: 0.9282, Test: 0.7142\n",
            "Early stopping:  0.013863504371083826\n",
            "Epoch: 074, Loss: 0.3456, Train: 0.9311, Test: 0.7139\n",
            "Early stopping:  0.014497150765295185\n",
            "Epoch: 075, Loss: 0.3411, Train: 0.9370, Test: 0.7148\n",
            "Early stopping:  0.012973175477464427\n",
            "Epoch: 076, Loss: 0.3280, Train: 0.9396, Test: 0.7146\n",
            "Early stopping:  0.01379165667066855\n",
            "Epoch: 077, Loss: 0.3253, Train: 0.9424, Test: 0.7140\n",
            "Early stopping:  0.013077234949429131\n",
            "Epoch: 078, Loss: 0.3123, Train: 0.9436, Test: 0.7167\n",
            "Early stopping:  0.013272260695556598\n",
            "Epoch: 079, Loss: 0.3103, Train: 0.9481, Test: 0.7135\n",
            "Early stopping:  0.012587187372842933\n",
            "Epoch: 080, Loss: 0.2977, Train: 0.9515, Test: 0.7142\n",
            "Early stopping:  0.01228377992605744\n",
            "Epoch: 081, Loss: 0.2943, Train: 0.9526, Test: 0.7152\n",
            "Early stopping:  0.012420781665607008\n",
            "Epoch: 082, Loss: 0.2847, Train: 0.9560, Test: 0.7154\n",
            "Early stopping:  0.011502617477620535\n",
            "Epoch: 083, Loss: 0.2798, Train: 0.9492, Test: 0.7129\n",
            "Early stopping:  0.011883262475744685\n",
            "Epoch: 084, Loss: 0.2763, Train: 0.9572, Test: 0.7099\n",
            "Early stopping:  0.009201166988680537\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.77      0.69       568\n",
            "         capital_goods       0.56      0.65      0.60       381\n",
            "conglomerates_industry       0.81      0.35      0.49        60\n",
            "     consumer_cyclical       0.69      0.65      0.67       595\n",
            " consumer_non-cyclical       0.77      0.61      0.68       334\n",
            "                energy       0.77      0.69      0.73       213\n",
            "             financial       0.77      0.74      0.76       576\n",
            "            healthcare       0.84      0.73      0.78       238\n",
            "              services       0.72      0.76      0.74      1557\n",
            "            technology       0.63      0.56      0.59       297\n",
            "        transportation       0.82      0.77      0.80       303\n",
            "             utilities       0.81      0.75      0.78       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.74      0.67      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4914, Train: 0.2941, Test: 0.2941\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2669, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15874458762973598\n",
            "Epoch: 003, Loss: 2.1514, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1728754883891727\n",
            "Epoch: 004, Loss: 2.1420, Train: 0.2950, Test: 0.2954\n",
            "Early stopping:  0.16256417715560792\n",
            "Epoch: 005, Loss: 2.0776, Train: 0.3191, Test: 0.3154\n",
            "Early stopping:  0.16335989601131154\n",
            "Epoch: 006, Loss: 2.0040, Train: 0.3503, Test: 0.3497\n",
            "Early stopping:  0.09738871423536462\n",
            "Epoch: 007, Loss: 1.9467, Train: 0.3820, Test: 0.3778\n",
            "Early stopping:  0.08839204110670756\n",
            "Epoch: 008, Loss: 1.8951, Train: 0.3976, Test: 0.3899\n",
            "Early stopping:  0.09897846279220318\n",
            "Epoch: 009, Loss: 1.8375, Train: 0.4112, Test: 0.3986\n",
            "Early stopping:  0.09336796597878022\n",
            "Epoch: 010, Loss: 1.7754, Train: 0.4299, Test: 0.4081\n",
            "Early stopping:  0.08962953119034883\n",
            "Epoch: 011, Loss: 1.7133, Train: 0.4495, Test: 0.4317\n",
            "Early stopping:  0.09280559721358593\n",
            "Epoch: 012, Loss: 1.6517, Train: 0.4725, Test: 0.4534\n",
            "Early stopping:  0.09660928046001921\n",
            "Epoch: 013, Loss: 1.5915, Train: 0.4889, Test: 0.4682\n",
            "Early stopping:  0.09736246987571828\n",
            "Epoch: 014, Loss: 1.5355, Train: 0.5111, Test: 0.4801\n",
            "Early stopping:  0.09512360581947896\n",
            "Epoch: 015, Loss: 1.4806, Train: 0.5255, Test: 0.4957\n",
            "Early stopping:  0.09199241027511171\n",
            "Epoch: 016, Loss: 1.4262, Train: 0.5488, Test: 0.5143\n",
            "Early stopping:  0.08885490519516438\n",
            "Epoch: 017, Loss: 1.3775, Train: 0.5749, Test: 0.5339\n",
            "Early stopping:  0.08496725502998993\n",
            "Epoch: 018, Loss: 1.3321, Train: 0.5953, Test: 0.5506\n",
            "Early stopping:  0.08069439046025091\n",
            "Epoch: 019, Loss: 1.2877, Train: 0.6078, Test: 0.5611\n",
            "Early stopping:  0.07594253190469448\n",
            "Epoch: 020, Loss: 1.2475, Train: 0.6185, Test: 0.5732\n",
            "Early stopping:  0.07074718793575561\n",
            "Epoch: 021, Loss: 1.2097, Train: 0.6242, Test: 0.5806\n",
            "Early stopping:  0.06651239666478176\n",
            "Epoch: 022, Loss: 1.1736, Train: 0.6336, Test: 0.5880\n",
            "Early stopping:  0.06251691794147828\n",
            "Epoch: 023, Loss: 1.1417, Train: 0.6401, Test: 0.5906\n",
            "Early stopping:  0.057915292703246066\n",
            "Epoch: 024, Loss: 1.1102, Train: 0.6531, Test: 0.5976\n",
            "Early stopping:  0.05421617671365716\n",
            "Epoch: 025, Loss: 1.0813, Train: 0.6645, Test: 0.6057\n",
            "Early stopping:  0.05065138596488099\n",
            "Epoch: 026, Loss: 1.0556, Train: 0.6687, Test: 0.6127\n",
            "Early stopping:  0.04692188191931666\n",
            "Epoch: 027, Loss: 1.0300, Train: 0.6778, Test: 0.6169\n",
            "Early stopping:  0.044008437163855135\n",
            "Epoch: 028, Loss: 1.0053, Train: 0.6852, Test: 0.6241\n",
            "Early stopping:  0.04131334847143615\n",
            "Epoch: 029, Loss: 0.9818, Train: 0.6954, Test: 0.6305\n",
            "Early stopping:  0.03942495259709582\n",
            "Epoch: 030, Loss: 0.9597, Train: 0.7065, Test: 0.6326\n",
            "Early stopping:  0.03795159731429547\n",
            "Epoch: 031, Loss: 0.9376, Train: 0.7167, Test: 0.6405\n",
            "Early stopping:  0.03644916023345842\n",
            "Epoch: 032, Loss: 0.9156, Train: 0.7263, Test: 0.6424\n",
            "Early stopping:  0.035350773698959725\n",
            "Epoch: 033, Loss: 0.8945, Train: 0.7343, Test: 0.6471\n",
            "Early stopping:  0.03455835706222722\n",
            "Epoch: 034, Loss: 0.8746, Train: 0.7431, Test: 0.6498\n",
            "Early stopping:  0.03372812802019259\n",
            "Epoch: 035, Loss: 0.8540, Train: 0.7513, Test: 0.6551\n",
            "Early stopping:  0.032916034592693384\n",
            "Epoch: 036, Loss: 0.8345, Train: 0.7567, Test: 0.6572\n",
            "Early stopping:  0.03205135935109952\n",
            "Epoch: 037, Loss: 0.8152, Train: 0.7618, Test: 0.6632\n",
            "Early stopping:  0.03141910416772874\n",
            "Epoch: 038, Loss: 0.7972, Train: 0.7691, Test: 0.6657\n",
            "Early stopping:  0.030626854529168364\n",
            "Epoch: 039, Loss: 0.7789, Train: 0.7785, Test: 0.6679\n",
            "Early stopping:  0.029659422797940562\n",
            "Epoch: 040, Loss: 0.7615, Train: 0.7828, Test: 0.6715\n",
            "Early stopping:  0.02882253306846304\n",
            "Epoch: 041, Loss: 0.7446, Train: 0.7870, Test: 0.6745\n",
            "Early stopping:  0.027959132177888372\n",
            "Epoch: 042, Loss: 0.7281, Train: 0.7947, Test: 0.6768\n",
            "Early stopping:  0.02728298146265108\n",
            "Epoch: 043, Loss: 0.7114, Train: 0.8020, Test: 0.6840\n",
            "Early stopping:  0.026660419483722674\n",
            "Epoch: 044, Loss: 0.6949, Train: 0.8074, Test: 0.6853\n",
            "Early stopping:  0.02633897818299481\n",
            "Epoch: 045, Loss: 0.6787, Train: 0.8125, Test: 0.6866\n",
            "Early stopping:  0.02609907046169266\n",
            "Epoch: 046, Loss: 0.6627, Train: 0.8196, Test: 0.6889\n",
            "Early stopping:  0.025846517542895367\n",
            "Epoch: 047, Loss: 0.6468, Train: 0.8264, Test: 0.6902\n",
            "Early stopping:  0.025517317742998185\n",
            "Epoch: 048, Loss: 0.6314, Train: 0.8327, Test: 0.6934\n",
            "Early stopping:  0.025120884026372133\n",
            "Epoch: 049, Loss: 0.6158, Train: 0.8378, Test: 0.6953\n",
            "Early stopping:  0.024833214192706746\n",
            "Epoch: 050, Loss: 0.6004, Train: 0.8409, Test: 0.6972\n",
            "Early stopping:  0.02457989495454325\n",
            "Epoch: 051, Loss: 0.5855, Train: 0.8477, Test: 0.6993\n",
            "Early stopping:  0.024287641860229973\n",
            "Epoch: 052, Loss: 0.5708, Train: 0.8508, Test: 0.7010\n",
            "Early stopping:  0.02394635305321992\n",
            "Epoch: 053, Loss: 0.5565, Train: 0.8556, Test: 0.7063\n",
            "Early stopping:  0.02343073068763458\n",
            "Epoch: 054, Loss: 0.5425, Train: 0.8588, Test: 0.7033\n",
            "Early stopping:  0.02290449848976482\n",
            "Epoch: 055, Loss: 0.5296, Train: 0.8678, Test: 0.7070\n",
            "Early stopping:  0.02215257771670322\n",
            "Epoch: 056, Loss: 0.5184, Train: 0.8667, Test: 0.7044\n",
            "Early stopping:  0.020865492511115722\n",
            "Epoch: 057, Loss: 0.5104, Train: 0.8738, Test: 0.7089\n",
            "Early stopping:  0.018476795441758547\n",
            "Epoch: 058, Loss: 0.4988, Train: 0.8766, Test: 0.7088\n",
            "Early stopping:  0.016906423455255476\n",
            "Epoch: 059, Loss: 0.4810, Train: 0.8837, Test: 0.7127\n",
            "Early stopping:  0.018659545090177945\n",
            "Epoch: 060, Loss: 0.4664, Train: 0.8885, Test: 0.7135\n",
            "Early stopping:  0.02130357592978868\n",
            "Epoch: 061, Loss: 0.4601, Train: 0.8843, Test: 0.7106\n",
            "Early stopping:  0.021237117959363148\n",
            "Epoch: 062, Loss: 0.4515, Train: 0.9016, Test: 0.7163\n",
            "Early stopping:  0.01863158333954244\n",
            "Epoch: 063, Loss: 0.4353, Train: 0.9044, Test: 0.7163\n",
            "Early stopping:  0.017008575537338592\n",
            "Epoch: 064, Loss: 0.4239, Train: 0.8985, Test: 0.7154\n",
            "Early stopping:  0.017572461843205822\n",
            "Epoch: 065, Loss: 0.4183, Train: 0.9118, Test: 0.7150\n",
            "Early stopping:  0.017784758598003273\n",
            "Epoch: 066, Loss: 0.4083, Train: 0.9109, Test: 0.7152\n",
            "Early stopping:  0.016608452086981123\n",
            "Epoch: 067, Loss: 0.3947, Train: 0.9158, Test: 0.7150\n",
            "Early stopping:  0.015456303891763265\n",
            "Epoch: 068, Loss: 0.3851, Train: 0.9237, Test: 0.7157\n",
            "Early stopping:  0.01615181429127705\n",
            "Epoch: 069, Loss: 0.3791, Train: 0.9158, Test: 0.7123\n",
            "Early stopping:  0.01620694146396463\n",
            "Epoch: 070, Loss: 0.3711, Train: 0.9325, Test: 0.7169\n",
            "Early stopping:  0.014407652006397927\n",
            "Epoch: 071, Loss: 0.3591, Train: 0.9325, Test: 0.7171\n",
            "Early stopping:  0.013559295552682043\n",
            "Epoch: 072, Loss: 0.3488, Train: 0.9314, Test: 0.7152\n",
            "Early stopping:  0.014760168142028792\n",
            "Epoch: 073, Loss: 0.3422, Train: 0.9416, Test: 0.7184\n",
            "Early stopping:  0.01525295891779536\n",
            "Epoch: 074, Loss: 0.3367, Train: 0.9322, Test: 0.7148\n",
            "Early stopping:  0.01372867510238553\n",
            "Epoch: 075, Loss: 0.3298, Train: 0.9436, Test: 0.7163\n",
            "Early stopping:  0.011232905148522175\n",
            "Epoch: 076, Loss: 0.3217, Train: 0.9433, Test: 0.7167\n",
            "Early stopping:  0.010543171442145362\n",
            "Epoch: 077, Loss: 0.3120, Train: 0.9504, Test: 0.7182\n",
            "Early stopping:  0.011981079813604049\n",
            "Epoch: 078, Loss: 0.3024, Train: 0.9538, Test: 0.7193\n",
            "Early stopping:  0.013694411402104818\n",
            "Epoch: 079, Loss: 0.2946, Train: 0.9541, Test: 0.7169\n",
            "Early stopping:  0.014213559622048804\n",
            "Epoch: 080, Loss: 0.2892, Train: 0.9575, Test: 0.7165\n",
            "Early stopping:  0.01310829997576167\n",
            "Epoch: 081, Loss: 0.2856, Train: 0.9507, Test: 0.7135\n",
            "Early stopping:  0.010622806804334457\n",
            "Epoch: 082, Loss: 0.2820, Train: 0.9586, Test: 0.7178\n",
            "Early stopping:  0.00799888897684276\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.64      0.78      0.70       568\n",
            "         capital_goods       0.63      0.59      0.61       381\n",
            "conglomerates_industry       0.93      0.42      0.57        60\n",
            "     consumer_cyclical       0.66      0.65      0.65       595\n",
            " consumer_non-cyclical       0.72      0.65      0.68       334\n",
            "                energy       0.82      0.77      0.79       213\n",
            "             financial       0.80      0.73      0.76       576\n",
            "            healthcare       0.81      0.76      0.78       238\n",
            "              services       0.71      0.77      0.74      1557\n",
            "            technology       0.66      0.59      0.62       297\n",
            "        transportation       0.85      0.78      0.81       303\n",
            "             utilities       0.87      0.74      0.80       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.76      0.69      0.71      5291\n",
            "          weighted avg       0.72      0.72      0.72      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5561, Train: 0.2955, Test: 0.2950\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3004, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  0.18082178122931844\n",
            "Epoch: 003, Loss: 2.1794, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  0.1923231867790469\n",
            "Epoch: 004, Loss: 2.1670, Train: 0.2972, Test: 0.2969\n",
            "Early stopping:  0.18056996799067507\n",
            "Epoch: 005, Loss: 2.0957, Train: 0.3230, Test: 0.3234\n",
            "Early stopping:  0.181280713426358\n",
            "Epoch: 006, Loss: 2.0358, Train: 0.3829, Test: 0.3754\n",
            "Early stopping:  0.09951918001480864\n",
            "Epoch: 007, Loss: 1.9907, Train: 0.4070, Test: 0.3971\n",
            "Early stopping:  0.08171045901730882\n",
            "Epoch: 008, Loss: 1.9376, Train: 0.4090, Test: 0.3941\n",
            "Early stopping:  0.08950063426092057\n",
            "Epoch: 009, Loss: 1.8764, Train: 0.3996, Test: 0.3882\n",
            "Early stopping:  0.08497305539147956\n",
            "Epoch: 010, Loss: 1.8174, Train: 0.4024, Test: 0.3897\n",
            "Early stopping:  0.0872645219194472\n",
            "Epoch: 011, Loss: 1.7629, Train: 0.4200, Test: 0.4043\n",
            "Early stopping:  0.09103999225695389\n",
            "Epoch: 012, Loss: 1.7049, Train: 0.4464, Test: 0.4358\n",
            "Early stopping:  0.09155655627180287\n",
            "Epoch: 013, Loss: 1.6436, Train: 0.4833, Test: 0.4538\n",
            "Early stopping:  0.09142017034971475\n",
            "Epoch: 014, Loss: 1.5865, Train: 0.5068, Test: 0.4767\n",
            "Early stopping:  0.09189085624349533\n",
            "Epoch: 015, Loss: 1.5314, Train: 0.5315, Test: 0.4935\n",
            "Early stopping:  0.09194302881257857\n",
            "Epoch: 016, Loss: 1.4770, Train: 0.5519, Test: 0.5109\n",
            "Early stopping:  0.08983782377628502\n",
            "Epoch: 017, Loss: 1.4257, Train: 0.5672, Test: 0.5226\n",
            "Early stopping:  0.08626603676742142\n",
            "Epoch: 018, Loss: 1.3751, Train: 0.5834, Test: 0.5364\n",
            "Early stopping:  0.08359292938029365\n",
            "Epoch: 019, Loss: 1.3247, Train: 0.5944, Test: 0.5460\n",
            "Early stopping:  0.08149250034790385\n",
            "Epoch: 020, Loss: 1.2799, Train: 0.6064, Test: 0.5627\n",
            "Early stopping:  0.0783238036641415\n",
            "Epoch: 021, Loss: 1.2388, Train: 0.6183, Test: 0.5740\n",
            "Early stopping:  0.07422737696570889\n",
            "Epoch: 022, Loss: 1.1986, Train: 0.6347, Test: 0.5834\n",
            "Early stopping:  0.06945776287639972\n",
            "Epoch: 023, Loss: 1.1625, Train: 0.6441, Test: 0.5916\n",
            "Early stopping:  0.06418612245108865\n",
            "Epoch: 024, Loss: 1.1303, Train: 0.6563, Test: 0.5974\n",
            "Early stopping:  0.05944623932677926\n",
            "Epoch: 025, Loss: 1.0989, Train: 0.6648, Test: 0.6025\n",
            "Early stopping:  0.055108212835105135\n",
            "Epoch: 026, Loss: 1.0702, Train: 0.6687, Test: 0.6093\n",
            "Early stopping:  0.05069998874266928\n",
            "Epoch: 027, Loss: 1.0449, Train: 0.6758, Test: 0.6160\n",
            "Early stopping:  0.046743541729627754\n",
            "Epoch: 028, Loss: 1.0183, Train: 0.6835, Test: 0.6188\n",
            "Early stopping:  0.04400786224110627\n",
            "Epoch: 029, Loss: 0.9931, Train: 0.6960, Test: 0.6250\n",
            "Early stopping:  0.041689716034219886\n",
            "Epoch: 030, Loss: 0.9693, Train: 0.7065, Test: 0.6339\n",
            "Early stopping:  0.04010156812892593\n",
            "Epoch: 031, Loss: 0.9453, Train: 0.7147, Test: 0.6407\n",
            "Early stopping:  0.03924923275179903\n",
            "Epoch: 032, Loss: 0.9209, Train: 0.7232, Test: 0.6447\n",
            "Early stopping:  0.038345403774816454\n",
            "Epoch: 033, Loss: 0.8987, Train: 0.7351, Test: 0.6477\n",
            "Early stopping:  0.03749363332814016\n",
            "Epoch: 034, Loss: 0.8773, Train: 0.7422, Test: 0.6507\n",
            "Early stopping:  0.036466422215867704\n",
            "Epoch: 035, Loss: 0.8561, Train: 0.7516, Test: 0.6543\n",
            "Early stopping:  0.03511995204371489\n",
            "Epoch: 036, Loss: 0.8362, Train: 0.7598, Test: 0.6570\n",
            "Early stopping:  0.03355677315598268\n",
            "Epoch: 037, Loss: 0.8167, Train: 0.7646, Test: 0.6636\n",
            "Early stopping:  0.032430328736244664\n",
            "Epoch: 038, Loss: 0.7982, Train: 0.7751, Test: 0.6653\n",
            "Early stopping:  0.03125622437358196\n",
            "Epoch: 039, Loss: 0.7798, Train: 0.7825, Test: 0.6685\n",
            "Early stopping:  0.03014414355121315\n",
            "Epoch: 040, Loss: 0.7621, Train: 0.7890, Test: 0.6685\n",
            "Early stopping:  0.02926913329270998\n",
            "Epoch: 041, Loss: 0.7451, Train: 0.7950, Test: 0.6740\n",
            "Early stopping:  0.028352502617887326\n",
            "Epoch: 042, Loss: 0.7283, Train: 0.7992, Test: 0.6749\n",
            "Early stopping:  0.02757932071502643\n",
            "Epoch: 043, Loss: 0.7117, Train: 0.8049, Test: 0.6772\n",
            "Early stopping:  0.026881595276577797\n",
            "Epoch: 044, Loss: 0.6955, Train: 0.8114, Test: 0.6804\n",
            "Early stopping:  0.026356358945811113\n",
            "Epoch: 045, Loss: 0.6790, Train: 0.8142, Test: 0.6859\n",
            "Early stopping:  0.026084831638904283\n",
            "Epoch: 046, Loss: 0.6630, Train: 0.8210, Test: 0.6899\n",
            "Early stopping:  0.02579828421396676\n",
            "Epoch: 047, Loss: 0.6470, Train: 0.8261, Test: 0.6933\n",
            "Early stopping:  0.02557785457774869\n",
            "Epoch: 048, Loss: 0.6316, Train: 0.8287, Test: 0.6944\n",
            "Early stopping:  0.025275958351700222\n",
            "Epoch: 049, Loss: 0.6163, Train: 0.8378, Test: 0.6991\n",
            "Early stopping:  0.0248240565743688\n",
            "Epoch: 050, Loss: 0.6016, Train: 0.8378, Test: 0.6991\n",
            "Early stopping:  0.024291014688360153\n",
            "Epoch: 051, Loss: 0.5878, Train: 0.8486, Test: 0.7004\n",
            "Early stopping:  0.023463764934558002\n",
            "Epoch: 052, Loss: 0.5764, Train: 0.8423, Test: 0.6995\n",
            "Early stopping:  0.021972689682498568\n",
            "Epoch: 053, Loss: 0.5664, Train: 0.8613, Test: 0.7021\n",
            "Early stopping:  0.019829788132818688\n",
            "Epoch: 054, Loss: 0.5501, Train: 0.8650, Test: 0.7048\n",
            "Early stopping:  0.01972132009687462\n",
            "Epoch: 055, Loss: 0.5325, Train: 0.8670, Test: 0.7038\n",
            "Early stopping:  0.02182002070175581\n",
            "Epoch: 056, Loss: 0.5217, Train: 0.8724, Test: 0.7053\n",
            "Early stopping:  0.02275651345833638\n",
            "Epoch: 057, Loss: 0.5131, Train: 0.8724, Test: 0.7055\n",
            "Early stopping:  0.021584563499663485\n",
            "Epoch: 058, Loss: 0.4994, Train: 0.8806, Test: 0.7095\n",
            "Early stopping:  0.019260286266532325\n",
            "Epoch: 059, Loss: 0.4835, Train: 0.8820, Test: 0.7127\n",
            "Early stopping:  0.019159075173938955\n",
            "Epoch: 060, Loss: 0.4740, Train: 0.8809, Test: 0.7084\n",
            "Early stopping:  0.01986974127500175\n",
            "Epoch: 061, Loss: 0.4661, Train: 0.8894, Test: 0.7142\n",
            "Early stopping:  0.01905117145095083\n",
            "Epoch: 062, Loss: 0.4528, Train: 0.8945, Test: 0.7150\n",
            "Early stopping:  0.017615721455067984\n",
            "Epoch: 063, Loss: 0.4386, Train: 0.8942, Test: 0.7125\n",
            "Early stopping:  0.0176669347221527\n",
            "Epoch: 064, Loss: 0.4291, Train: 0.9039, Test: 0.7157\n",
            "Early stopping:  0.018620157824397486\n",
            "Epoch: 065, Loss: 0.4213, Train: 0.8971, Test: 0.7114\n",
            "Early stopping:  0.018053362883627938\n",
            "Epoch: 066, Loss: 0.4111, Train: 0.9118, Test: 0.7191\n",
            "Early stopping:  0.01601091707445017\n",
            "Epoch: 067, Loss: 0.3978, Train: 0.9124, Test: 0.7184\n",
            "Early stopping:  0.015830565159202836\n",
            "Epoch: 068, Loss: 0.3857, Train: 0.9115, Test: 0.7154\n",
            "Early stopping:  0.01750128448902335\n",
            "Epoch: 069, Loss: 0.3768, Train: 0.9234, Test: 0.7193\n",
            "Early stopping:  0.018100833839152754\n",
            "Epoch: 070, Loss: 0.3696, Train: 0.9101, Test: 0.7135\n",
            "Early stopping:  0.01654363350302135\n",
            "Epoch: 071, Loss: 0.3636, Train: 0.9285, Test: 0.7174\n",
            "Early stopping:  0.013490810278847624\n",
            "Epoch: 072, Loss: 0.3560, Train: 0.9180, Test: 0.7133\n",
            "Early stopping:  0.011522223640294073\n",
            "Epoch: 073, Loss: 0.3457, Train: 0.9356, Test: 0.7193\n",
            "Early stopping:  0.012045368971639115\n",
            "Epoch: 074, Loss: 0.3321, Train: 0.9348, Test: 0.7224\n",
            "Early stopping:  0.014898434483653075\n",
            "Epoch: 075, Loss: 0.3208, Train: 0.9359, Test: 0.7218\n",
            "Early stopping:  0.017375715856002356\n",
            "Epoch: 076, Loss: 0.3146, Train: 0.9450, Test: 0.7201\n",
            "Early stopping:  0.017143659117689326\n",
            "Epoch: 077, Loss: 0.3106, Train: 0.9339, Test: 0.7176\n",
            "Early stopping:  0.014258031741881488\n",
            "Epoch: 078, Loss: 0.3065, Train: 0.9507, Test: 0.7180\n",
            "Early stopping:  0.009985952658162331\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.73      0.71       568\n",
            "         capital_goods       0.60      0.66      0.63       381\n",
            "conglomerates_industry       1.00      0.35      0.52        60\n",
            "     consumer_cyclical       0.63      0.70      0.67       595\n",
            " consumer_non-cyclical       0.75      0.62      0.68       334\n",
            "                energy       0.85      0.72      0.78       213\n",
            "             financial       0.78      0.73      0.75       576\n",
            "            healthcare       0.84      0.76      0.80       238\n",
            "              services       0.72      0.77      0.74      1557\n",
            "            technology       0.66      0.57      0.61       297\n",
            "        transportation       0.81      0.77      0.79       303\n",
            "             utilities       0.85      0.74      0.79       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.76      0.68      0.71      5291\n",
            "          weighted avg       0.72      0.72      0.72      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4912, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2713, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15545409887279346\n",
            "Epoch: 003, Loss: 2.1599, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  0.16855390679786159\n",
            "Epoch: 004, Loss: 2.1356, Train: 0.2995, Test: 0.2996\n",
            "Early stopping:  0.16224280209390426\n",
            "Epoch: 005, Loss: 2.0632, Train: 0.3412, Test: 0.3396\n",
            "Early stopping:  0.16687436070601294\n",
            "Epoch: 006, Loss: 1.9962, Train: 0.3704, Test: 0.3655\n",
            "Early stopping:  0.10386644131568315\n",
            "Epoch: 007, Loss: 1.9380, Train: 0.3786, Test: 0.3655\n",
            "Early stopping:  0.0930817597610056\n",
            "Epoch: 008, Loss: 1.8803, Train: 0.3795, Test: 0.3670\n",
            "Early stopping:  0.10068458226218006\n",
            "Epoch: 009, Loss: 1.8217, Train: 0.3965, Test: 0.3829\n",
            "Early stopping:  0.0947381959366534\n",
            "Epoch: 010, Loss: 1.7592, Train: 0.4297, Test: 0.4130\n",
            "Early stopping:  0.09336098230831458\n",
            "Epoch: 011, Loss: 1.6918, Train: 0.4609, Test: 0.4445\n",
            "Early stopping:  0.09707430926532297\n",
            "Epoch: 012, Loss: 1.6281, Train: 0.4847, Test: 0.4600\n",
            "Early stopping:  0.10031457544888034\n",
            "Epoch: 013, Loss: 1.5707, Train: 0.4980, Test: 0.4704\n",
            "Early stopping:  0.10014507927950686\n",
            "Epoch: 014, Loss: 1.5113, Train: 0.5153, Test: 0.4852\n",
            "Early stopping:  0.09758412866262883\n",
            "Epoch: 015, Loss: 1.4527, Train: 0.5323, Test: 0.5027\n",
            "Early stopping:  0.09407102039473211\n",
            "Epoch: 016, Loss: 1.3986, Train: 0.5641, Test: 0.5239\n",
            "Early stopping:  0.09124025711191898\n",
            "Epoch: 017, Loss: 1.3462, Train: 0.5964, Test: 0.5443\n",
            "Early stopping:  0.08885200895236199\n",
            "Epoch: 018, Loss: 1.2989, Train: 0.6100, Test: 0.5642\n",
            "Early stopping:  0.08408748080687223\n",
            "Epoch: 019, Loss: 1.2541, Train: 0.6208, Test: 0.5755\n",
            "Early stopping:  0.0786552506384397\n",
            "Epoch: 020, Loss: 1.2110, Train: 0.6251, Test: 0.5791\n",
            "Early stopping:  0.07393435914681303\n",
            "Epoch: 021, Loss: 1.1732, Train: 0.6373, Test: 0.5850\n",
            "Early stopping:  0.06863854202793639\n",
            "Epoch: 022, Loss: 1.1375, Train: 0.6475, Test: 0.5921\n",
            "Early stopping:  0.06390710105905137\n",
            "Epoch: 023, Loss: 1.1050, Train: 0.6557, Test: 0.6005\n",
            "Early stopping:  0.058842192502085354\n",
            "Epoch: 024, Loss: 1.0749, Train: 0.6625, Test: 0.6091\n",
            "Early stopping:  0.05387456167650421\n",
            "Epoch: 025, Loss: 1.0483, Train: 0.6679, Test: 0.6154\n",
            "Early stopping:  0.04947765241538734\n",
            "Epoch: 026, Loss: 1.0252, Train: 0.6770, Test: 0.6192\n",
            "Early stopping:  0.04456401418538439\n",
            "Epoch: 027, Loss: 1.0012, Train: 0.6863, Test: 0.6282\n",
            "Early stopping:  0.04075892255515348\n",
            "Epoch: 028, Loss: 0.9769, Train: 0.6988, Test: 0.6326\n",
            "Early stopping:  0.03843520706474377\n",
            "Epoch: 029, Loss: 0.9539, Train: 0.7079, Test: 0.6371\n",
            "Early stopping:  0.03748783280213797\n",
            "Epoch: 030, Loss: 0.9318, Train: 0.7175, Test: 0.6420\n",
            "Early stopping:  0.037023294346214694\n",
            "Epoch: 031, Loss: 0.9089, Train: 0.7286, Test: 0.6511\n",
            "Early stopping:  0.036324113153931865\n",
            "Epoch: 032, Loss: 0.8871, Train: 0.7343, Test: 0.6536\n",
            "Early stopping:  0.03553295117549484\n",
            "Epoch: 033, Loss: 0.8666, Train: 0.7408, Test: 0.6575\n",
            "Early stopping:  0.03468493843545053\n",
            "Epoch: 034, Loss: 0.8466, Train: 0.7490, Test: 0.6609\n",
            "Early stopping:  0.03366046923343268\n",
            "Epoch: 035, Loss: 0.8262, Train: 0.7598, Test: 0.6640\n",
            "Early stopping:  0.03254145796396273\n",
            "Epoch: 036, Loss: 0.8073, Train: 0.7683, Test: 0.6679\n",
            "Early stopping:  0.03160146641228181\n",
            "Epoch: 037, Loss: 0.7892, Train: 0.7742, Test: 0.6715\n",
            "Early stopping:  0.03068751472552922\n",
            "Epoch: 038, Loss: 0.7713, Train: 0.7850, Test: 0.6751\n",
            "Early stopping:  0.029677938297762312\n",
            "Epoch: 039, Loss: 0.7543, Train: 0.7898, Test: 0.6779\n",
            "Early stopping:  0.02845300797695761\n",
            "Epoch: 040, Loss: 0.7374, Train: 0.7961, Test: 0.6806\n",
            "Early stopping:  0.02761688435837301\n",
            "Epoch: 041, Loss: 0.7204, Train: 0.8046, Test: 0.6853\n",
            "Early stopping:  0.02711182951968853\n",
            "Epoch: 042, Loss: 0.7037, Train: 0.8106, Test: 0.6870\n",
            "Early stopping:  0.02673147093882019\n",
            "Epoch: 043, Loss: 0.6871, Train: 0.8131, Test: 0.6893\n",
            "Early stopping:  0.026577381167749395\n",
            "Epoch: 044, Loss: 0.6704, Train: 0.8191, Test: 0.6917\n",
            "Early stopping:  0.026465828849901618\n",
            "Epoch: 045, Loss: 0.6540, Train: 0.8247, Test: 0.6933\n",
            "Early stopping:  0.026251143748413596\n",
            "Epoch: 046, Loss: 0.6381, Train: 0.8310, Test: 0.6946\n",
            "Early stopping:  0.025990761781939837\n",
            "Epoch: 047, Loss: 0.6220, Train: 0.8375, Test: 0.6950\n",
            "Early stopping:  0.02571861299857118\n",
            "Epoch: 048, Loss: 0.6060, Train: 0.8423, Test: 0.6982\n",
            "Early stopping:  0.0254442227335604\n",
            "Epoch: 049, Loss: 0.5902, Train: 0.8471, Test: 0.6997\n",
            "Early stopping:  0.025265082886332352\n",
            "Epoch: 050, Loss: 0.5746, Train: 0.8525, Test: 0.7023\n",
            "Early stopping:  0.025101067849840623\n",
            "Epoch: 051, Loss: 0.5590, Train: 0.8576, Test: 0.7042\n",
            "Early stopping:  0.02488591689986636\n",
            "Epoch: 052, Loss: 0.5436, Train: 0.8636, Test: 0.7065\n",
            "Early stopping:  0.0246500572144294\n",
            "Epoch: 053, Loss: 0.5285, Train: 0.8695, Test: 0.7088\n",
            "Early stopping:  0.024406157720709257\n",
            "Epoch: 054, Loss: 0.5136, Train: 0.8712, Test: 0.7106\n",
            "Early stopping:  0.024104100680906338\n",
            "Epoch: 055, Loss: 0.4990, Train: 0.8792, Test: 0.7114\n",
            "Early stopping:  0.02369534430973401\n",
            "Epoch: 056, Loss: 0.4858, Train: 0.8718, Test: 0.7106\n",
            "Early stopping:  0.022961954062136674\n",
            "Epoch: 057, Loss: 0.4818, Train: 0.8798, Test: 0.7031\n",
            "Early stopping:  0.019477265254780562\n",
            "Epoch: 058, Loss: 0.4964, Train: 0.8724, Test: 0.7091\n",
            "Early stopping:  0.012484317714250008\n",
            "Epoch: 059, Loss: 0.4689, Train: 0.8925, Test: 0.7165\n",
            "Early stopping:  0.012147765339770345\n",
            "Epoch: 060, Loss: 0.4376, Train: 0.8951, Test: 0.7110\n",
            "Early stopping:  0.02267644122934897\n",
            "Epoch: 061, Loss: 0.4500, Train: 0.8973, Test: 0.7178\n",
            "Early stopping:  0.02368088433739081\n",
            "Epoch: 062, Loss: 0.4161, Train: 0.8919, Test: 0.7118\n",
            "Early stopping:  0.030602992015990156\n",
            "Epoch: 063, Loss: 0.4223, Train: 0.9152, Test: 0.7167\n",
            "Early stopping:  0.021329142223890013\n",
            "Epoch: 064, Loss: 0.4026, Train: 0.9166, Test: 0.7176\n",
            "Early stopping:  0.01850946156236946\n",
            "Epoch: 065, Loss: 0.3947, Train: 0.9033, Test: 0.7156\n",
            "Early stopping:  0.02136429910487344\n",
            "Epoch: 066, Loss: 0.3875, Train: 0.9146, Test: 0.7148\n",
            "Early stopping:  0.014493877434886882\n",
            "Epoch: 067, Loss: 0.3713, Train: 0.9246, Test: 0.7203\n",
            "Early stopping:  0.018831125613368655\n",
            "Epoch: 068, Loss: 0.3694, Train: 0.9254, Test: 0.7208\n",
            "Early stopping:  0.014485235188687705\n",
            "Epoch: 069, Loss: 0.3523, Train: 0.9203, Test: 0.7154\n",
            "Early stopping:  0.01662263116485153\n",
            "Epoch: 070, Loss: 0.3526, Train: 0.9342, Test: 0.7216\n",
            "Early stopping:  0.014736159858241492\n",
            "Epoch: 071, Loss: 0.3360, Train: 0.9368, Test: 0.7225\n",
            "Early stopping:  0.014497718655793947\n",
            "Epoch: 072, Loss: 0.3342, Train: 0.9379, Test: 0.7214\n",
            "Early stopping:  0.01439691827353954\n",
            "Epoch: 073, Loss: 0.3212, Train: 0.9379, Test: 0.7195\n",
            "Early stopping:  0.013327536846996233\n",
            "Epoch: 074, Loss: 0.3164, Train: 0.9472, Test: 0.7267\n",
            "Early stopping:  0.01418178944683583\n",
            "Epoch: 075, Loss: 0.3072, Train: 0.9475, Test: 0.7246\n",
            "Early stopping:  0.012148172117409177\n",
            "Epoch: 076, Loss: 0.3002, Train: 0.9492, Test: 0.7229\n",
            "Early stopping:  0.013100470037562265\n",
            "Epoch: 077, Loss: 0.2937, Train: 0.9538, Test: 0.7248\n",
            "Early stopping:  0.011292570696338978\n",
            "Epoch: 078, Loss: 0.2852, Train: 0.9560, Test: 0.7222\n",
            "Early stopping:  0.012050193781289954\n",
            "Epoch: 079, Loss: 0.2797, Train: 0.9586, Test: 0.7254\n",
            "Early stopping:  0.011083445552900273\n",
            "Epoch: 080, Loss: 0.2708, Train: 0.9606, Test: 0.7263\n",
            "Early stopping:  0.011510705360763531\n",
            "Epoch: 081, Loss: 0.2661, Train: 0.9634, Test: 0.7254\n",
            "Early stopping:  0.011037537704863877\n",
            "Epoch: 082, Loss: 0.2575, Train: 0.9640, Test: 0.7239\n",
            "Early stopping:  0.010936490008558824\n",
            "Epoch: 083, Loss: 0.2530, Train: 0.9674, Test: 0.7261\n",
            "Early stopping:  0.01062549781309015\n",
            "Epoch: 084, Loss: 0.2460, Train: 0.9691, Test: 0.7273\n",
            "Early stopping:  0.009963421339600143\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.70      0.77      0.74       568\n",
            "         capital_goods       0.68      0.61      0.64       381\n",
            "conglomerates_industry       0.92      0.37      0.52        60\n",
            "     consumer_cyclical       0.70      0.64      0.67       595\n",
            " consumer_non-cyclical       0.74      0.63      0.68       334\n",
            "                energy       0.83      0.73      0.78       213\n",
            "             financial       0.77      0.76      0.76       576\n",
            "            healthcare       0.84      0.80      0.82       238\n",
            "              services       0.69      0.80      0.74      1557\n",
            "            technology       0.74      0.57      0.64       297\n",
            "        transportation       0.80      0.78      0.79       303\n",
            "             utilities       0.83      0.72      0.77       169\n",
            "\n",
            "              accuracy                           0.73      5291\n",
            "             macro avg       0.77      0.68      0.71      5291\n",
            "          weighted avg       0.73      0.73      0.73      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4973, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2825, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1518872953770135\n",
            "Epoch: 003, Loss: 2.1667, Train: 0.2944, Test: 0.2945\n",
            "Early stopping:  0.16771881000854055\n",
            "Epoch: 004, Loss: 2.1198, Train: 0.3040, Test: 0.3071\n",
            "Early stopping:  0.16830365066464897\n",
            "Epoch: 005, Loss: 2.0504, Train: 0.3576, Test: 0.3532\n",
            "Early stopping:  0.17490293163636744\n",
            "Epoch: 006, Loss: 1.9820, Train: 0.3849, Test: 0.3771\n",
            "Early stopping:  0.11452814544062638\n",
            "Epoch: 007, Loss: 1.9188, Train: 0.3999, Test: 0.3861\n",
            "Early stopping:  0.10039681653483822\n",
            "Epoch: 008, Loss: 1.8562, Train: 0.4075, Test: 0.3924\n",
            "Early stopping:  0.10420410012668946\n",
            "Epoch: 009, Loss: 1.7917, Train: 0.4282, Test: 0.4120\n",
            "Early stopping:  0.10171169132711397\n",
            "Epoch: 010, Loss: 1.7251, Train: 0.4580, Test: 0.4407\n",
            "Early stopping:  0.10133351111003265\n",
            "Epoch: 011, Loss: 1.6601, Train: 0.4782, Test: 0.4579\n",
            "Early stopping:  0.10252879617978429\n",
            "Epoch: 012, Loss: 1.6008, Train: 0.4918, Test: 0.4702\n",
            "Early stopping:  0.10158147362038693\n",
            "Epoch: 013, Loss: 1.5406, Train: 0.5094, Test: 0.4831\n",
            "Early stopping:  0.09907837392170239\n",
            "Epoch: 014, Loss: 1.4798, Train: 0.5320, Test: 0.5037\n",
            "Early stopping:  0.09648788088756864\n",
            "Epoch: 015, Loss: 1.4202, Train: 0.5689, Test: 0.5258\n",
            "Early stopping:  0.0950167001684414\n",
            "Epoch: 016, Loss: 1.3645, Train: 0.5910, Test: 0.5445\n",
            "Early stopping:  0.09378431318205989\n",
            "Epoch: 017, Loss: 1.3173, Train: 0.6049, Test: 0.5564\n",
            "Early stopping:  0.08895161196072715\n",
            "Epoch: 018, Loss: 1.2714, Train: 0.6137, Test: 0.5636\n",
            "Early stopping:  0.08229816803729537\n",
            "Epoch: 019, Loss: 1.2262, Train: 0.6177, Test: 0.5744\n",
            "Early stopping:  0.0761283625335088\n",
            "Epoch: 020, Loss: 1.1874, Train: 0.6299, Test: 0.5808\n",
            "Early stopping:  0.07046603834878598\n",
            "Epoch: 021, Loss: 1.1542, Train: 0.6390, Test: 0.5870\n",
            "Early stopping:  0.06501426110737268\n",
            "Epoch: 022, Loss: 1.1235, Train: 0.6455, Test: 0.5944\n",
            "Early stopping:  0.05836207357894371\n",
            "Epoch: 023, Loss: 1.0922, Train: 0.6503, Test: 0.6050\n",
            "Early stopping:  0.05253839993373843\n",
            "Epoch: 024, Loss: 1.0661, Train: 0.6602, Test: 0.6107\n",
            "Early stopping:  0.048193076655914116\n",
            "Epoch: 025, Loss: 1.0413, Train: 0.6690, Test: 0.6148\n",
            "Early stopping:  0.044815366426923825\n",
            "Epoch: 026, Loss: 1.0157, Train: 0.6801, Test: 0.6184\n",
            "Early stopping:  0.04216323021441843\n",
            "Epoch: 027, Loss: 0.9901, Train: 0.6866, Test: 0.6231\n",
            "Early stopping:  0.040254691790944125\n",
            "Epoch: 028, Loss: 0.9666, Train: 0.6980, Test: 0.6286\n",
            "Early stopping:  0.039560475519896575\n",
            "Epoch: 029, Loss: 0.9423, Train: 0.7107, Test: 0.6349\n",
            "Early stopping:  0.03910361770657278\n",
            "Epoch: 030, Loss: 0.9183, Train: 0.7198, Test: 0.6401\n",
            "Early stopping:  0.038365777357218335\n",
            "Epoch: 031, Loss: 0.8969, Train: 0.7314, Test: 0.6454\n",
            "Early stopping:  0.03710520330098919\n",
            "Epoch: 032, Loss: 0.8759, Train: 0.7377, Test: 0.6494\n",
            "Early stopping:  0.03586750174895678\n",
            "Epoch: 033, Loss: 0.8551, Train: 0.7436, Test: 0.6519\n",
            "Early stopping:  0.034297687317150674\n",
            "Epoch: 034, Loss: 0.8356, Train: 0.7501, Test: 0.6572\n",
            "Early stopping:  0.032783950468154466\n",
            "Epoch: 035, Loss: 0.8165, Train: 0.7618, Test: 0.6592\n",
            "Early stopping:  0.03181454034335454\n",
            "Epoch: 036, Loss: 0.7971, Train: 0.7706, Test: 0.6624\n",
            "Early stopping:  0.031018464995597624\n",
            "Epoch: 037, Loss: 0.7793, Train: 0.7779, Test: 0.6649\n",
            "Early stopping:  0.03004391347261013\n",
            "Epoch: 038, Loss: 0.7623, Train: 0.7813, Test: 0.6687\n",
            "Early stopping:  0.029092148547932052\n",
            "Epoch: 039, Loss: 0.7447, Train: 0.7862, Test: 0.6725\n",
            "Early stopping:  0.028221911136562892\n",
            "Epoch: 040, Loss: 0.7282, Train: 0.7955, Test: 0.6761\n",
            "Early stopping:  0.027267491579904032\n",
            "Epoch: 041, Loss: 0.7116, Train: 0.7992, Test: 0.6791\n",
            "Early stopping:  0.026800297485845476\n",
            "Epoch: 042, Loss: 0.6948, Train: 0.8088, Test: 0.6821\n",
            "Early stopping:  0.026573657338898035\n",
            "Epoch: 043, Loss: 0.6788, Train: 0.8137, Test: 0.6861\n",
            "Early stopping:  0.02614043509867056\n",
            "Epoch: 044, Loss: 0.6625, Train: 0.8236, Test: 0.6912\n",
            "Early stopping:  0.02596147956091583\n",
            "Epoch: 045, Loss: 0.6466, Train: 0.8256, Test: 0.6908\n",
            "Early stopping:  0.025665275988163292\n",
            "Epoch: 046, Loss: 0.6327, Train: 0.8332, Test: 0.6931\n",
            "Early stopping:  0.024734304039798826\n",
            "Epoch: 047, Loss: 0.6227, Train: 0.8242, Test: 0.6914\n",
            "Early stopping:  0.022546481181119352\n",
            "Epoch: 048, Loss: 0.6164, Train: 0.8434, Test: 0.6968\n",
            "Early stopping:  0.018629681805756543\n",
            "Epoch: 049, Loss: 0.5973, Train: 0.8463, Test: 0.7008\n",
            "Early stopping:  0.018393362088835113\n",
            "Epoch: 050, Loss: 0.5733, Train: 0.8432, Test: 0.6989\n",
            "Early stopping:  0.023525356685577037\n",
            "Epoch: 051, Loss: 0.5693, Train: 0.8525, Test: 0.6987\n",
            "Early stopping:  0.024282269842998166\n",
            "Epoch: 052, Loss: 0.5578, Train: 0.8588, Test: 0.7048\n",
            "Early stopping:  0.02364096583913779\n",
            "Epoch: 053, Loss: 0.5356, Train: 0.8573, Test: 0.7050\n",
            "Early stopping:  0.022525345811623812\n",
            "Epoch: 054, Loss: 0.5298, Train: 0.8707, Test: 0.7048\n",
            "Early stopping:  0.01962272255558436\n",
            "Epoch: 055, Loss: 0.5192, Train: 0.8701, Test: 0.7065\n",
            "Early stopping:  0.020629980659845365\n",
            "Epoch: 056, Loss: 0.5000, Train: 0.8687, Test: 0.7070\n",
            "Early stopping:  0.021290691584260533\n",
            "Epoch: 057, Loss: 0.4935, Train: 0.8826, Test: 0.7076\n",
            "Early stopping:  0.01834966911874042\n",
            "Epoch: 058, Loss: 0.4841, Train: 0.8812, Test: 0.7112\n",
            "Early stopping:  0.0187802480092211\n",
            "Epoch: 059, Loss: 0.4664, Train: 0.8806, Test: 0.7110\n",
            "Early stopping:  0.019517636525470113\n",
            "Epoch: 060, Loss: 0.4583, Train: 0.8928, Test: 0.7114\n",
            "Early stopping:  0.017703606402693417\n",
            "Epoch: 061, Loss: 0.4510, Train: 0.8934, Test: 0.7139\n",
            "Early stopping:  0.017765631118280893\n",
            "Epoch: 062, Loss: 0.4353, Train: 0.9005, Test: 0.7156\n",
            "Early stopping:  0.01809652760388867\n",
            "Epoch: 063, Loss: 0.4236, Train: 0.9070, Test: 0.7120\n",
            "Early stopping:  0.017341120920657285\n",
            "Epoch: 064, Loss: 0.4177, Train: 0.9027, Test: 0.7152\n",
            "Early stopping:  0.017345388635731387\n",
            "Epoch: 065, Loss: 0.4069, Train: 0.9129, Test: 0.7156\n",
            "Early stopping:  0.016919156643979156\n",
            "Epoch: 066, Loss: 0.3930, Train: 0.9197, Test: 0.7159\n",
            "Early stopping:  0.016137685977005035\n",
            "Epoch: 067, Loss: 0.3827, Train: 0.9166, Test: 0.7154\n",
            "Early stopping:  0.016983180553267675\n",
            "Epoch: 068, Loss: 0.3760, Train: 0.9254, Test: 0.7144\n",
            "Early stopping:  0.017131002894494037\n",
            "Epoch: 069, Loss: 0.3686, Train: 0.9240, Test: 0.7163\n",
            "Early stopping:  0.015001375274489998\n",
            "Epoch: 070, Loss: 0.3576, Train: 0.9351, Test: 0.7191\n",
            "Early stopping:  0.013466253768632253\n",
            "Epoch: 071, Loss: 0.3454, Train: 0.9385, Test: 0.7178\n",
            "Early stopping:  0.014794373721281296\n",
            "Epoch: 072, Loss: 0.3348, Train: 0.9399, Test: 0.7184\n",
            "Early stopping:  0.016730878723577765\n",
            "Epoch: 073, Loss: 0.3264, Train: 0.9458, Test: 0.7184\n",
            "Early stopping:  0.01698477365559995\n",
            "Epoch: 074, Loss: 0.3201, Train: 0.9379, Test: 0.7154\n",
            "Early stopping:  0.01499430661501514\n",
            "Epoch: 075, Loss: 0.3161, Train: 0.9481, Test: 0.7129\n",
            "Early stopping:  0.011770119044429017\n",
            "Epoch: 076, Loss: 0.3162, Train: 0.9277, Test: 0.7103\n",
            "Early stopping:  0.007937035214352005\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.71      0.73       568\n",
            "         capital_goods       0.70      0.56      0.62       381\n",
            "conglomerates_industry       0.94      0.27      0.42        60\n",
            "     consumer_cyclical       0.72      0.61      0.66       595\n",
            " consumer_non-cyclical       0.75      0.57      0.65       334\n",
            "                energy       0.82      0.68      0.74       213\n",
            "             financial       0.74      0.77      0.75       576\n",
            "            healthcare       0.83      0.75      0.79       238\n",
            "              services       0.63      0.84      0.72      1557\n",
            "            technology       0.70      0.52      0.60       297\n",
            "        transportation       0.84      0.77      0.80       303\n",
            "             utilities       0.83      0.69      0.75       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.77      0.64      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4931, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2777, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15234315569050735\n",
            "Epoch: 003, Loss: 2.1597, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16904402952584832\n",
            "Epoch: 004, Loss: 2.1267, Train: 0.2967, Test: 0.2981\n",
            "Early stopping:  0.16571737499430234\n",
            "Epoch: 005, Loss: 2.0575, Train: 0.3440, Test: 0.3408\n",
            "Early stopping:  0.17072955823002814\n",
            "Epoch: 006, Loss: 1.9918, Train: 0.4010, Test: 0.3905\n",
            "Early stopping:  0.10815835064718705\n",
            "Epoch: 007, Loss: 1.9345, Train: 0.4203, Test: 0.4026\n",
            "Early stopping:  0.09304073989996421\n",
            "Epoch: 008, Loss: 1.8741, Train: 0.4189, Test: 0.4018\n",
            "Early stopping:  0.09941503727894448\n",
            "Epoch: 009, Loss: 1.8082, Train: 0.4212, Test: 0.4035\n",
            "Early stopping:  0.09747858230084225\n",
            "Epoch: 010, Loss: 1.7427, Train: 0.4370, Test: 0.4164\n",
            "Early stopping:  0.0987777872188512\n",
            "Epoch: 011, Loss: 1.6788, Train: 0.4699, Test: 0.4494\n",
            "Early stopping:  0.10165270701842744\n",
            "Epoch: 012, Loss: 1.6124, Train: 0.5006, Test: 0.4695\n",
            "Early stopping:  0.10320384269122233\n",
            "Epoch: 013, Loss: 1.5464, Train: 0.5241, Test: 0.4854\n",
            "Early stopping:  0.10338947139971254\n",
            "Epoch: 014, Loss: 1.4857, Train: 0.5431, Test: 0.5065\n",
            "Early stopping:  0.10221672600575178\n",
            "Epoch: 015, Loss: 1.4254, Train: 0.5621, Test: 0.5292\n",
            "Early stopping:  0.10019785361317474\n",
            "Epoch: 016, Loss: 1.3674, Train: 0.5854, Test: 0.5449\n",
            "Early stopping:  0.0966383410600184\n",
            "Epoch: 017, Loss: 1.3189, Train: 0.5981, Test: 0.5579\n",
            "Early stopping:  0.0907425529225126\n",
            "Epoch: 018, Loss: 1.2730, Train: 0.6143, Test: 0.5696\n",
            "Early stopping:  0.08428365807286822\n",
            "Epoch: 019, Loss: 1.2276, Train: 0.6251, Test: 0.5791\n",
            "Early stopping:  0.07760113520568217\n",
            "Epoch: 020, Loss: 1.1892, Train: 0.6293, Test: 0.5846\n",
            "Early stopping:  0.07084957162253204\n",
            "Epoch: 021, Loss: 1.1545, Train: 0.6395, Test: 0.5929\n",
            "Early stopping:  0.06534004754633917\n",
            "Epoch: 022, Loss: 1.1211, Train: 0.6438, Test: 0.6006\n",
            "Early stopping:  0.05968345740101032\n",
            "Epoch: 023, Loss: 1.0900, Train: 0.6580, Test: 0.6063\n",
            "Early stopping:  0.054312549318055466\n",
            "Epoch: 024, Loss: 1.0585, Train: 0.6719, Test: 0.6158\n",
            "Early stopping:  0.05156454247779777\n",
            "Epoch: 025, Loss: 1.0306, Train: 0.6809, Test: 0.6194\n",
            "Early stopping:  0.049122776942388105\n",
            "Epoch: 026, Loss: 1.0035, Train: 0.6900, Test: 0.6267\n",
            "Early stopping:  0.046632693383748924\n",
            "Epoch: 027, Loss: 0.9767, Train: 0.6965, Test: 0.6320\n",
            "Early stopping:  0.0445532656462134\n",
            "Epoch: 028, Loss: 0.9527, Train: 0.7073, Test: 0.6367\n",
            "Early stopping:  0.04199006781655114\n",
            "Epoch: 029, Loss: 0.9291, Train: 0.7215, Test: 0.6398\n",
            "Early stopping:  0.04013640589196109\n",
            "Epoch: 030, Loss: 0.9067, Train: 0.7286, Test: 0.6435\n",
            "Early stopping:  0.038151268577328735\n",
            "Epoch: 031, Loss: 0.8841, Train: 0.7343, Test: 0.6498\n",
            "Early stopping:  0.036552561369160595\n",
            "Epoch: 032, Loss: 0.8635, Train: 0.7431, Test: 0.6570\n",
            "Early stopping:  0.03533064052865832\n",
            "Epoch: 033, Loss: 0.8429, Train: 0.7564, Test: 0.6621\n",
            "Early stopping:  0.03409802943947373\n",
            "Epoch: 034, Loss: 0.8233, Train: 0.7643, Test: 0.6638\n",
            "Early stopping:  0.0328896883072427\n",
            "Epoch: 035, Loss: 0.8048, Train: 0.7703, Test: 0.6672\n",
            "Early stopping:  0.03143032657902482\n",
            "Epoch: 036, Loss: 0.7867, Train: 0.7785, Test: 0.6694\n",
            "Early stopping:  0.03031067462112154\n",
            "Epoch: 037, Loss: 0.7686, Train: 0.7839, Test: 0.6700\n",
            "Early stopping:  0.02928675222697523\n",
            "Epoch: 038, Loss: 0.7512, Train: 0.7893, Test: 0.6755\n",
            "Early stopping:  0.028522433848820035\n",
            "Epoch: 039, Loss: 0.7343, Train: 0.7938, Test: 0.6779\n",
            "Early stopping:  0.02791979419290219\n",
            "Epoch: 040, Loss: 0.7173, Train: 0.8009, Test: 0.6813\n",
            "Early stopping:  0.027381689366210386\n",
            "Epoch: 041, Loss: 0.7008, Train: 0.8111, Test: 0.6849\n",
            "Early stopping:  0.02679730696937665\n",
            "Epoch: 042, Loss: 0.6848, Train: 0.8168, Test: 0.6855\n",
            "Early stopping:  0.02628643042073082\n",
            "Epoch: 043, Loss: 0.6690, Train: 0.8202, Test: 0.6887\n",
            "Early stopping:  0.025769198469223886\n",
            "Epoch: 044, Loss: 0.6533, Train: 0.8273, Test: 0.6953\n",
            "Early stopping:  0.025247770734032088\n",
            "Epoch: 045, Loss: 0.6378, Train: 0.8313, Test: 0.6936\n",
            "Early stopping:  0.024897199975749993\n",
            "Epoch: 046, Loss: 0.6221, Train: 0.8335, Test: 0.6950\n",
            "Early stopping:  0.024765534129269837\n",
            "Epoch: 047, Loss: 0.6068, Train: 0.8381, Test: 0.6987\n",
            "Early stopping:  0.02460899637370384\n",
            "Epoch: 048, Loss: 0.5916, Train: 0.8452, Test: 0.7002\n",
            "Early stopping:  0.02444019916421135\n",
            "Epoch: 049, Loss: 0.5761, Train: 0.8488, Test: 0.7025\n",
            "Early stopping:  0.024369770204925367\n",
            "Epoch: 050, Loss: 0.5608, Train: 0.8559, Test: 0.7065\n",
            "Early stopping:  0.024242147003389388\n",
            "Epoch: 051, Loss: 0.5457, Train: 0.8588, Test: 0.7057\n",
            "Early stopping:  0.024199373961242304\n",
            "Epoch: 052, Loss: 0.5305, Train: 0.8678, Test: 0.7074\n",
            "Early stopping:  0.024116437768279333\n",
            "Epoch: 053, Loss: 0.5155, Train: 0.8724, Test: 0.7093\n",
            "Early stopping:  0.023937138759563725\n",
            "Epoch: 054, Loss: 0.5008, Train: 0.8786, Test: 0.7088\n",
            "Early stopping:  0.02373752840867909\n",
            "Epoch: 055, Loss: 0.4861, Train: 0.8823, Test: 0.7108\n",
            "Early stopping:  0.023509041487716105\n",
            "Epoch: 056, Loss: 0.4717, Train: 0.8866, Test: 0.7116\n",
            "Early stopping:  0.023246065662916784\n",
            "Epoch: 057, Loss: 0.4573, Train: 0.8953, Test: 0.7131\n",
            "Early stopping:  0.02303767618717249\n",
            "Epoch: 058, Loss: 0.4430, Train: 0.8982, Test: 0.7144\n",
            "Early stopping:  0.022852271306487062\n",
            "Epoch: 059, Loss: 0.4290, Train: 0.9081, Test: 0.7139\n",
            "Early stopping:  0.022613100026508917\n",
            "Epoch: 060, Loss: 0.4153, Train: 0.9039, Test: 0.7123\n",
            "Early stopping:  0.022294422846194255\n",
            "Epoch: 061, Loss: 0.4050, Train: 0.9152, Test: 0.7118\n",
            "Early stopping:  0.020945746658542597\n",
            "Epoch: 062, Loss: 0.4026, Train: 0.8902, Test: 0.7046\n",
            "Early stopping:  0.016985013348194543\n",
            "Epoch: 063, Loss: 0.3990, Train: 0.9231, Test: 0.7157\n",
            "Early stopping:  0.012137065461870168\n",
            "Epoch: 064, Loss: 0.3714, Train: 0.9229, Test: 0.7150\n",
            "Early stopping:  0.016397130588833006\n",
            "Epoch: 065, Loss: 0.3601, Train: 0.9098, Test: 0.7127\n",
            "Early stopping:  0.020472549002254484\n",
            "Epoch: 066, Loss: 0.3596, Train: 0.9339, Test: 0.7154\n",
            "Early stopping:  0.020905610636079142\n",
            "Epoch: 067, Loss: 0.3381, Train: 0.9359, Test: 0.7165\n",
            "Early stopping:  0.022201452372705047\n",
            "Epoch: 068, Loss: 0.3335, Train: 0.9285, Test: 0.7154\n",
            "Early stopping:  0.016086870836297475\n",
            "Epoch: 069, Loss: 0.3245, Train: 0.9385, Test: 0.7152\n",
            "Early stopping:  0.01599793280090106\n",
            "Epoch: 070, Loss: 0.3101, Train: 0.9467, Test: 0.7159\n",
            "Early stopping:  0.01822909173702482\n",
            "Epoch: 071, Loss: 0.3069, Train: 0.9430, Test: 0.7208\n",
            "Early stopping:  0.013809337566437046\n",
            "Epoch: 072, Loss: 0.2935, Train: 0.9453, Test: 0.7178\n",
            "Early stopping:  0.015614186193754452\n",
            "Epoch: 073, Loss: 0.2876, Train: 0.9546, Test: 0.7159\n",
            "Early stopping:  0.014522744636977645\n",
            "Epoch: 074, Loss: 0.2803, Train: 0.9583, Test: 0.7207\n",
            "Early stopping:  0.012665023253613504\n",
            "Epoch: 075, Loss: 0.2696, Train: 0.9526, Test: 0.7161\n",
            "Early stopping:  0.014023683714252005\n",
            "Epoch: 076, Loss: 0.2663, Train: 0.9643, Test: 0.7193\n",
            "Early stopping:  0.011561789348806915\n",
            "Epoch: 077, Loss: 0.2566, Train: 0.9657, Test: 0.7216\n",
            "Early stopping:  0.01211262906026345\n",
            "Epoch: 078, Loss: 0.2492, Train: 0.9617, Test: 0.7199\n",
            "Early stopping:  0.01197132842555227\n",
            "Epoch: 079, Loss: 0.2455, Train: 0.9682, Test: 0.7205\n",
            "Early stopping:  0.010459506310431601\n",
            "Epoch: 080, Loss: 0.2354, Train: 0.9716, Test: 0.7224\n",
            "Early stopping:  0.011636796517018228\n",
            "Epoch: 081, Loss: 0.2310, Train: 0.9694, Test: 0.7216\n",
            "Early stopping:  0.010381050067811363\n",
            "Epoch: 082, Loss: 0.2256, Train: 0.9728, Test: 0.7227\n",
            "Early stopping:  0.009873421350287457\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.69      0.72      0.70       568\n",
            "         capital_goods       0.66      0.60      0.63       381\n",
            "conglomerates_industry       0.95      0.32      0.47        60\n",
            "     consumer_cyclical       0.69      0.65      0.67       595\n",
            " consumer_non-cyclical       0.75      0.59      0.66       334\n",
            "                energy       0.84      0.74      0.79       213\n",
            "             financial       0.80      0.77      0.78       576\n",
            "            healthcare       0.84      0.75      0.79       238\n",
            "              services       0.68      0.82      0.74      1557\n",
            "            technology       0.72      0.61      0.66       297\n",
            "        transportation       0.82      0.75      0.78       303\n",
            "             utilities       0.83      0.72      0.77       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.67      0.70      5291\n",
            "          weighted avg       0.73      0.72      0.72      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5008, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2809, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15548849070117834\n",
            "Epoch: 003, Loss: 2.1602, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1726812924009112\n",
            "Epoch: 004, Loss: 2.1420, Train: 0.3001, Test: 0.3009\n",
            "Early stopping:  0.16515221544498038\n",
            "Epoch: 005, Loss: 2.0678, Train: 0.3352, Test: 0.3340\n",
            "Early stopping:  0.16946550477627206\n",
            "Epoch: 006, Loss: 1.9970, Train: 0.3812, Test: 0.3748\n",
            "Early stopping:  0.10653281708511478\n",
            "Epoch: 007, Loss: 1.9421, Train: 0.3953, Test: 0.3873\n",
            "Early stopping:  0.09311250415544903\n",
            "Epoch: 008, Loss: 1.8877, Train: 0.4016, Test: 0.3905\n",
            "Early stopping:  0.10057599893726081\n",
            "Epoch: 009, Loss: 1.8286, Train: 0.4044, Test: 0.3941\n",
            "Early stopping:  0.093022246446072\n",
            "Epoch: 010, Loss: 1.7664, Train: 0.4169, Test: 0.4022\n",
            "Early stopping:  0.0909004371350452\n",
            "Epoch: 011, Loss: 1.7034, Train: 0.4365, Test: 0.4217\n",
            "Early stopping:  0.09471184774527558\n",
            "Epoch: 012, Loss: 1.6411, Train: 0.4634, Test: 0.4387\n",
            "Early stopping:  0.09777402977239531\n",
            "Epoch: 013, Loss: 1.5847, Train: 0.4833, Test: 0.4583\n",
            "Early stopping:  0.09695053520280227\n",
            "Epoch: 014, Loss: 1.5320, Train: 0.5077, Test: 0.4748\n",
            "Early stopping:  0.09295126708140693\n",
            "Epoch: 015, Loss: 1.4755, Train: 0.5374, Test: 0.5003\n",
            "Early stopping:  0.08934513102610758\n",
            "Epoch: 016, Loss: 1.4183, Train: 0.5720, Test: 0.5228\n",
            "Early stopping:  0.08775604391227516\n",
            "Epoch: 017, Loss: 1.3659, Train: 0.5944, Test: 0.5451\n",
            "Early stopping:  0.08719338047121208\n",
            "Epoch: 018, Loss: 1.3185, Train: 0.6024, Test: 0.5549\n",
            "Early stopping:  0.0849199873295236\n",
            "Epoch: 019, Loss: 1.2746, Train: 0.6163, Test: 0.5676\n",
            "Early stopping:  0.07941367311171177\n",
            "Epoch: 020, Loss: 1.2320, Train: 0.6228, Test: 0.5738\n",
            "Early stopping:  0.07341889057063758\n",
            "Epoch: 021, Loss: 1.1910, Train: 0.6288, Test: 0.5800\n",
            "Early stopping:  0.06900536451306641\n",
            "Epoch: 022, Loss: 1.1558, Train: 0.6387, Test: 0.5846\n",
            "Early stopping:  0.06471045009923376\n",
            "Epoch: 023, Loss: 1.1247, Train: 0.6469, Test: 0.5925\n",
            "Early stopping:  0.05957435895864103\n",
            "Epoch: 024, Loss: 1.0963, Train: 0.6600, Test: 0.6057\n",
            "Early stopping:  0.05353732418720032\n",
            "Epoch: 025, Loss: 1.0684, Train: 0.6676, Test: 0.6107\n",
            "Early stopping:  0.04824351443345443\n",
            "Epoch: 026, Loss: 1.0404, Train: 0.6724, Test: 0.6161\n",
            "Early stopping:  0.04540519124998588\n",
            "Epoch: 027, Loss: 1.0159, Train: 0.6804, Test: 0.6207\n",
            "Early stopping:  0.04325766382612225\n",
            "Epoch: 028, Loss: 0.9925, Train: 0.6877, Test: 0.6241\n",
            "Early stopping:  0.04114755842172898\n",
            "Epoch: 029, Loss: 0.9683, Train: 0.7036, Test: 0.6330\n",
            "Early stopping:  0.03925090017097396\n",
            "Epoch: 030, Loss: 0.9446, Train: 0.7116, Test: 0.6379\n",
            "Early stopping:  0.03782009272265893\n",
            "Epoch: 031, Loss: 0.9228, Train: 0.7201, Test: 0.6422\n",
            "Early stopping:  0.03704361419822022\n",
            "Epoch: 032, Loss: 0.9006, Train: 0.7309, Test: 0.6456\n",
            "Early stopping:  0.036288339248522206\n",
            "Epoch: 033, Loss: 0.8779, Train: 0.7419, Test: 0.6498\n",
            "Early stopping:  0.03554684338980997\n",
            "Epoch: 034, Loss: 0.8569, Train: 0.7482, Test: 0.6573\n",
            "Early stopping:  0.034824849490517555\n",
            "Epoch: 035, Loss: 0.8376, Train: 0.7589, Test: 0.6628\n",
            "Early stopping:  0.03384456198883267\n",
            "Epoch: 036, Loss: 0.8182, Train: 0.7649, Test: 0.6649\n",
            "Early stopping:  0.03243332177380795\n",
            "Epoch: 037, Loss: 0.7994, Train: 0.7666, Test: 0.6645\n",
            "Early stopping:  0.030965431789953528\n",
            "Epoch: 038, Loss: 0.7818, Train: 0.7720, Test: 0.6692\n",
            "Early stopping:  0.0297922030986404\n",
            "Epoch: 039, Loss: 0.7642, Train: 0.7805, Test: 0.6725\n",
            "Early stopping:  0.02897072648733056\n",
            "Epoch: 040, Loss: 0.7469, Train: 0.7856, Test: 0.6736\n",
            "Early stopping:  0.028125209106490297\n",
            "Epoch: 041, Loss: 0.7305, Train: 0.7918, Test: 0.6787\n",
            "Early stopping:  0.027295937083525505\n",
            "Epoch: 042, Loss: 0.7139, Train: 0.7986, Test: 0.6812\n",
            "Early stopping:  0.026804321859322633\n",
            "Epoch: 043, Loss: 0.6972, Train: 0.8069, Test: 0.6840\n",
            "Early stopping:  0.02639630796063972\n",
            "Epoch: 044, Loss: 0.6814, Train: 0.8114, Test: 0.6864\n",
            "Early stopping:  0.025974181138274016\n",
            "Epoch: 045, Loss: 0.6654, Train: 0.8179, Test: 0.6870\n",
            "Early stopping:  0.02573349059364146\n",
            "Epoch: 046, Loss: 0.6494, Train: 0.8219, Test: 0.6908\n",
            "Early stopping:  0.02542152715010125\n",
            "Epoch: 047, Loss: 0.6343, Train: 0.8273, Test: 0.6916\n",
            "Early stopping:  0.024950296455419532\n",
            "Epoch: 048, Loss: 0.6192, Train: 0.8332, Test: 0.6940\n",
            "Early stopping:  0.024593936395350294\n",
            "Epoch: 049, Loss: 0.6044, Train: 0.8386, Test: 0.6950\n",
            "Early stopping:  0.02409073770873847\n",
            "Epoch: 050, Loss: 0.5898, Train: 0.8463, Test: 0.6970\n",
            "Early stopping:  0.02359117086483973\n",
            "Epoch: 051, Loss: 0.5751, Train: 0.8542, Test: 0.7023\n",
            "Early stopping:  0.023366355921953044\n",
            "Epoch: 052, Loss: 0.5610, Train: 0.8582, Test: 0.7038\n",
            "Early stopping:  0.02303801996183074\n",
            "Epoch: 053, Loss: 0.5469, Train: 0.8610, Test: 0.7042\n",
            "Early stopping:  0.022742572930532008\n",
            "Epoch: 054, Loss: 0.5331, Train: 0.8659, Test: 0.7033\n",
            "Early stopping:  0.022393399409601743\n",
            "Epoch: 055, Loss: 0.5196, Train: 0.8693, Test: 0.7048\n",
            "Early stopping:  0.02196162311017478\n",
            "Epoch: 056, Loss: 0.5064, Train: 0.8721, Test: 0.7042\n",
            "Early stopping:  0.021587322189188934\n",
            "Epoch: 057, Loss: 0.4945, Train: 0.8763, Test: 0.7023\n",
            "Early stopping:  0.020788884244124034\n",
            "Epoch: 058, Loss: 0.4843, Train: 0.8724, Test: 0.7082\n",
            "Early stopping:  0.01942171542983165\n",
            "Epoch: 059, Loss: 0.4774, Train: 0.8866, Test: 0.7055\n",
            "Early stopping:  0.01696013270715261\n",
            "Epoch: 060, Loss: 0.4656, Train: 0.8894, Test: 0.7116\n",
            "Early stopping:  0.01565210034323751\n",
            "Epoch: 061, Loss: 0.4478, Train: 0.8942, Test: 0.7084\n",
            "Early stopping:  0.017996465808286014\n",
            "Epoch: 062, Loss: 0.4351, Train: 0.9044, Test: 0.7078\n",
            "Early stopping:  0.020453786530391725\n",
            "Epoch: 063, Loss: 0.4302, Train: 0.8945, Test: 0.7095\n",
            "Early stopping:  0.02002936583510379\n",
            "Epoch: 064, Loss: 0.4199, Train: 0.9084, Test: 0.7144\n",
            "Early stopping:  0.01763230137353834\n",
            "Epoch: 065, Loss: 0.4035, Train: 0.9141, Test: 0.7157\n",
            "Early stopping:  0.016670329121603454\n",
            "Epoch: 066, Loss: 0.3963, Train: 0.9019, Test: 0.7118\n",
            "Early stopping:  0.01671931384786935\n",
            "Epoch: 067, Loss: 0.3908, Train: 0.9226, Test: 0.7161\n",
            "Early stopping:  0.016479850295040566\n",
            "Epoch: 068, Loss: 0.3768, Train: 0.9246, Test: 0.7169\n",
            "Early stopping:  0.015882480190215206\n",
            "Epoch: 069, Loss: 0.3662, Train: 0.9161, Test: 0.7142\n",
            "Early stopping:  0.015075864768306713\n",
            "Epoch: 070, Loss: 0.3612, Train: 0.9328, Test: 0.7150\n",
            "Early stopping:  0.015193779435179525\n",
            "Epoch: 071, Loss: 0.3521, Train: 0.9311, Test: 0.7144\n",
            "Early stopping:  0.014925078259549859\n",
            "Epoch: 072, Loss: 0.3407, Train: 0.9336, Test: 0.7156\n",
            "Early stopping:  0.013756245215041445\n",
            "Epoch: 073, Loss: 0.3327, Train: 0.9413, Test: 0.7137\n",
            "Early stopping:  0.013924456111311873\n",
            "Epoch: 074, Loss: 0.3264, Train: 0.9399, Test: 0.7129\n",
            "Early stopping:  0.014126002685292823\n",
            "Epoch: 075, Loss: 0.3185, Train: 0.9453, Test: 0.7188\n",
            "Early stopping:  0.012942894115429078\n",
            "Epoch: 076, Loss: 0.3088, Train: 0.9467, Test: 0.7165\n",
            "Early stopping:  0.01237399352373106\n",
            "Epoch: 077, Loss: 0.3013, Train: 0.9495, Test: 0.7157\n",
            "Early stopping:  0.012756368956535468\n",
            "Epoch: 078, Loss: 0.2951, Train: 0.9524, Test: 0.7140\n",
            "Early stopping:  0.012672962984065474\n",
            "Epoch: 079, Loss: 0.2875, Train: 0.9526, Test: 0.7152\n",
            "Early stopping:  0.012027915934317977\n",
            "Epoch: 080, Loss: 0.2804, Train: 0.9594, Test: 0.7140\n",
            "Early stopping:  0.01116941490224853\n",
            "Epoch: 081, Loss: 0.2741, Train: 0.9575, Test: 0.7156\n",
            "Early stopping:  0.010928719424159536\n",
            "Epoch: 082, Loss: 0.2676, Train: 0.9643, Test: 0.7123\n",
            "Early stopping:  0.010811927030878866\n",
            "Epoch: 083, Loss: 0.2593, Train: 0.9677, Test: 0.7127\n",
            "Early stopping:  0.01094682224573033\n",
            "Epoch: 084, Loss: 0.2528, Train: 0.9660, Test: 0.7174\n",
            "Early stopping:  0.0110726680776142\n",
            "Epoch: 085, Loss: 0.2478, Train: 0.9722, Test: 0.7127\n",
            "Early stopping:  0.010695656194588343\n",
            "Epoch: 086, Loss: 0.2419, Train: 0.9719, Test: 0.7142\n",
            "Early stopping:  0.009989566833258018\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.71      0.73      0.72       568\n",
            "         capital_goods       0.63      0.59      0.61       381\n",
            "conglomerates_industry       0.96      0.38      0.55        60\n",
            "     consumer_cyclical       0.70      0.62      0.65       595\n",
            " consumer_non-cyclical       0.73      0.61      0.66       334\n",
            "                energy       0.82      0.72      0.77       213\n",
            "             financial       0.77      0.72      0.74       576\n",
            "            healthcare       0.86      0.76      0.81       238\n",
            "              services       0.66      0.82      0.73      1557\n",
            "            technology       0.70      0.61      0.65       297\n",
            "        transportation       0.87      0.74      0.80       303\n",
            "             utilities       0.82      0.72      0.77       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.77      0.67      0.70      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4721, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2433, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16177798061076204\n",
            "Epoch: 003, Loss: 2.1804, Train: 0.2941, Test: 0.2945\n",
            "Early stopping:  0.15353064195735516\n",
            "Epoch: 004, Loss: 2.1147, Train: 0.3077, Test: 0.3069\n",
            "Early stopping:  0.15547678423357889\n",
            "Epoch: 005, Loss: 2.0428, Train: 0.3761, Test: 0.3735\n",
            "Early stopping:  0.16410953918972365\n",
            "Epoch: 006, Loss: 1.9926, Train: 0.4081, Test: 0.4005\n",
            "Early stopping:  0.10116066854473806\n",
            "Epoch: 007, Loss: 1.9383, Train: 0.4030, Test: 0.3926\n",
            "Early stopping:  0.09607778191359227\n",
            "Epoch: 008, Loss: 1.8761, Train: 0.3936, Test: 0.3827\n",
            "Early stopping:  0.09211286133249072\n",
            "Epoch: 009, Loss: 1.8134, Train: 0.3956, Test: 0.3852\n",
            "Early stopping:  0.09109463979039216\n",
            "Epoch: 010, Loss: 1.7537, Train: 0.4155, Test: 0.4043\n",
            "Early stopping:  0.09534193513534703\n",
            "Epoch: 011, Loss: 1.6923, Train: 0.4498, Test: 0.4334\n",
            "Early stopping:  0.09716710547446675\n",
            "Epoch: 012, Loss: 1.6274, Train: 0.4816, Test: 0.4604\n",
            "Early stopping:  0.09779961522396212\n",
            "Epoch: 013, Loss: 1.5642, Train: 0.5108, Test: 0.4784\n",
            "Early stopping:  0.09876592498131022\n",
            "Epoch: 014, Loss: 1.5057, Train: 0.5318, Test: 0.4984\n",
            "Early stopping:  0.09868326351548719\n",
            "Epoch: 015, Loss: 1.4499, Train: 0.5513, Test: 0.5158\n",
            "Early stopping:  0.09594128928403962\n",
            "Epoch: 016, Loss: 1.3927, Train: 0.5712, Test: 0.5343\n",
            "Early stopping:  0.09233385829670791\n",
            "Epoch: 017, Loss: 1.3369, Train: 0.5905, Test: 0.5481\n",
            "Early stopping:  0.08976586226481335\n",
            "Epoch: 018, Loss: 1.2885, Train: 0.6012, Test: 0.5576\n",
            "Early stopping:  0.08659404563235246\n",
            "Epoch: 019, Loss: 1.2453, Train: 0.6143, Test: 0.5706\n",
            "Early stopping:  0.08131610890201191\n",
            "Epoch: 020, Loss: 1.2012, Train: 0.6299, Test: 0.5840\n",
            "Early stopping:  0.07514060528903435\n",
            "Epoch: 021, Loss: 1.1593, Train: 0.6398, Test: 0.5929\n",
            "Early stopping:  0.06998417623828732\n",
            "Epoch: 022, Loss: 1.1258, Train: 0.6489, Test: 0.5982\n",
            "Early stopping:  0.06513884040828596\n",
            "Epoch: 023, Loss: 1.0945, Train: 0.6546, Test: 0.6039\n",
            "Early stopping:  0.059793723183375884\n",
            "Epoch: 024, Loss: 1.0629, Train: 0.6634, Test: 0.6091\n",
            "Early stopping:  0.05407449913779534\n",
            "Epoch: 025, Loss: 1.0352, Train: 0.6719, Test: 0.6146\n",
            "Early stopping:  0.04920636686298921\n",
            "Epoch: 026, Loss: 1.0101, Train: 0.6829, Test: 0.6224\n",
            "Early stopping:  0.04601164425042042\n",
            "Epoch: 027, Loss: 0.9840, Train: 0.6931, Test: 0.6273\n",
            "Early stopping:  0.043343212933340196\n",
            "Epoch: 028, Loss: 0.9590, Train: 0.7050, Test: 0.6313\n",
            "Early stopping:  0.040982625460167406\n",
            "Epoch: 029, Loss: 0.9360, Train: 0.7133, Test: 0.6390\n",
            "Early stopping:  0.03947820455451196\n",
            "Epoch: 030, Loss: 0.9116, Train: 0.7232, Test: 0.6420\n",
            "Early stopping:  0.03874197304435412\n",
            "Epoch: 031, Loss: 0.8886, Train: 0.7345, Test: 0.6485\n",
            "Early stopping:  0.03764409503876242\n",
            "Epoch: 032, Loss: 0.8662, Train: 0.7459, Test: 0.6538\n",
            "Early stopping:  0.03681521951616805\n",
            "Epoch: 033, Loss: 0.8436, Train: 0.7575, Test: 0.6587\n",
            "Early stopping:  0.03637213090121977\n",
            "Epoch: 034, Loss: 0.8230, Train: 0.7643, Test: 0.6607\n",
            "Early stopping:  0.03515004167189216\n",
            "Epoch: 035, Loss: 0.8024, Train: 0.7689, Test: 0.6666\n",
            "Early stopping:  0.03411955399178676\n",
            "Epoch: 036, Loss: 0.7822, Train: 0.7717, Test: 0.6679\n",
            "Early stopping:  0.0330880864338011\n",
            "Epoch: 037, Loss: 0.7638, Train: 0.7811, Test: 0.6713\n",
            "Early stopping:  0.03168702743013215\n",
            "Epoch: 038, Loss: 0.7449, Train: 0.7904, Test: 0.6757\n",
            "Early stopping:  0.030798118220475523\n",
            "Epoch: 039, Loss: 0.7267, Train: 0.7947, Test: 0.6785\n",
            "Early stopping:  0.029832389790815393\n",
            "Epoch: 040, Loss: 0.7090, Train: 0.8026, Test: 0.6821\n",
            "Early stopping:  0.029018151312363544\n",
            "Epoch: 041, Loss: 0.6909, Train: 0.8114, Test: 0.6846\n",
            "Early stopping:  0.02873789302247349\n",
            "Epoch: 042, Loss: 0.6732, Train: 0.8171, Test: 0.6895\n",
            "Early stopping:  0.02834637010196928\n",
            "Epoch: 043, Loss: 0.6553, Train: 0.8239, Test: 0.6940\n",
            "Early stopping:  0.028256943116466066\n",
            "Epoch: 044, Loss: 0.6379, Train: 0.8281, Test: 0.6967\n",
            "Early stopping:  0.028117285482147012\n",
            "Epoch: 045, Loss: 0.6202, Train: 0.8310, Test: 0.6995\n",
            "Early stopping:  0.02792448378682089\n",
            "Epoch: 046, Loss: 0.6033, Train: 0.8392, Test: 0.6999\n",
            "Early stopping:  0.02763056557444325\n",
            "Epoch: 047, Loss: 0.5864, Train: 0.8423, Test: 0.6991\n",
            "Early stopping:  0.027253237248792226\n",
            "Epoch: 048, Loss: 0.5716, Train: 0.8525, Test: 0.7038\n",
            "Early stopping:  0.026316813160357513\n",
            "Epoch: 049, Loss: 0.5592, Train: 0.8432, Test: 0.6963\n",
            "Early stopping:  0.02437579286059308\n",
            "Epoch: 050, Loss: 0.5487, Train: 0.8625, Test: 0.7061\n",
            "Early stopping:  0.021675483494212732\n",
            "Epoch: 051, Loss: 0.5287, Train: 0.8678, Test: 0.7070\n",
            "Early stopping:  0.02198773875055317\n",
            "Epoch: 052, Loss: 0.5086, Train: 0.8616, Test: 0.7040\n",
            "Early stopping:  0.024989090662550766\n",
            "Epoch: 053, Loss: 0.5004, Train: 0.8769, Test: 0.7084\n",
            "Early stopping:  0.02516914328443668\n",
            "Epoch: 054, Loss: 0.4885, Train: 0.8775, Test: 0.7086\n",
            "Early stopping:  0.02388505122726047\n",
            "Epoch: 055, Loss: 0.4696, Train: 0.8800, Test: 0.7086\n",
            "Early stopping:  0.022089130652066708\n",
            "Epoch: 056, Loss: 0.4572, Train: 0.8919, Test: 0.7108\n",
            "Early stopping:  0.02129478665378331\n",
            "Epoch: 057, Loss: 0.4487, Train: 0.8897, Test: 0.7086\n",
            "Early stopping:  0.021451955346951882\n",
            "Epoch: 058, Loss: 0.4344, Train: 0.9002, Test: 0.7131\n",
            "Early stopping:  0.02060012383773182\n",
            "Epoch: 059, Loss: 0.4185, Train: 0.9067, Test: 0.7146\n",
            "Early stopping:  0.01987022068745247\n",
            "Epoch: 060, Loss: 0.4093, Train: 0.9022, Test: 0.7123\n",
            "Early stopping:  0.020036181465309322\n",
            "Epoch: 061, Loss: 0.4008, Train: 0.9149, Test: 0.7154\n",
            "Early stopping:  0.01927065998801821\n",
            "Epoch: 062, Loss: 0.3860, Train: 0.9138, Test: 0.7127\n",
            "Early stopping:  0.018210142483365038\n",
            "Epoch: 063, Loss: 0.3726, Train: 0.9200, Test: 0.7127\n",
            "Early stopping:  0.018319271878446022\n",
            "Epoch: 064, Loss: 0.3633, Train: 0.9271, Test: 0.7125\n",
            "Early stopping:  0.01908001898007495\n",
            "Epoch: 065, Loss: 0.3553, Train: 0.9229, Test: 0.7152\n",
            "Early stopping:  0.01816348409809398\n",
            "Epoch: 066, Loss: 0.3461, Train: 0.9373, Test: 0.7159\n",
            "Early stopping:  0.015444899037318204\n",
            "Epoch: 067, Loss: 0.3334, Train: 0.9382, Test: 0.7146\n",
            "Early stopping:  0.015183872938324072\n",
            "Epoch: 068, Loss: 0.3212, Train: 0.9433, Test: 0.7176\n",
            "Early stopping:  0.01683163504420349\n",
            "Epoch: 069, Loss: 0.3122, Train: 0.9444, Test: 0.7157\n",
            "Early stopping:  0.01758067259142888\n",
            "Epoch: 070, Loss: 0.3052, Train: 0.9436, Test: 0.7152\n",
            "Early stopping:  0.016413467810322973\n",
            "Epoch: 071, Loss: 0.3015, Train: 0.9433, Test: 0.7095\n",
            "Early stopping:  0.012892452388182974\n",
            "Epoch: 072, Loss: 0.3018, Train: 0.9472, Test: 0.7080\n",
            "Early stopping:  0.008396685795065768\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.75      0.70       568\n",
            "         capital_goods       0.62      0.63      0.62       381\n",
            "conglomerates_industry       0.95      0.32      0.47        60\n",
            "     consumer_cyclical       0.61      0.74      0.66       595\n",
            " consumer_non-cyclical       0.79      0.57      0.66       334\n",
            "                energy       0.84      0.71      0.77       213\n",
            "             financial       0.83      0.67      0.74       576\n",
            "            healthcare       0.83      0.76      0.79       238\n",
            "              services       0.70      0.76      0.73      1557\n",
            "            technology       0.60      0.60      0.60       297\n",
            "        transportation       0.81      0.78      0.80       303\n",
            "             utilities       0.84      0.71      0.77       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.76      0.67      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5084, Train: 0.2941, Test: 0.2939\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2611, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17490857838324\n",
            "Epoch: 003, Loss: 2.1839, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1695321906266481\n",
            "Epoch: 004, Loss: 2.1142, Train: 0.3029, Test: 0.3050\n",
            "Early stopping:  0.17183684250820455\n",
            "Epoch: 005, Loss: 2.0382, Train: 0.3715, Test: 0.3653\n",
            "Early stopping:  0.18056155321196096\n",
            "Epoch: 006, Loss: 1.9957, Train: 0.4005, Test: 0.3912\n",
            "Early stopping:  0.10739078868337443\n",
            "Epoch: 007, Loss: 1.9477, Train: 0.4067, Test: 0.3929\n",
            "Early stopping:  0.09409899386349305\n",
            "Epoch: 008, Loss: 1.8831, Train: 0.3985, Test: 0.3856\n",
            "Early stopping:  0.08779056376901644\n",
            "Epoch: 009, Loss: 1.8195, Train: 0.3973, Test: 0.3831\n",
            "Early stopping:  0.08734798058393198\n",
            "Epoch: 010, Loss: 1.7682, Train: 0.4095, Test: 0.3948\n",
            "Early stopping:  0.09232283965826303\n",
            "Epoch: 011, Loss: 1.7160, Train: 0.4390, Test: 0.4228\n",
            "Early stopping:  0.09157600648411061\n",
            "Epoch: 012, Loss: 1.6545, Train: 0.4759, Test: 0.4585\n",
            "Early stopping:  0.0887326789548586\n",
            "Epoch: 013, Loss: 1.5965, Train: 0.5034, Test: 0.4767\n",
            "Early stopping:  0.088581416060605\n",
            "Epoch: 014, Loss: 1.5497, Train: 0.5210, Test: 0.4840\n",
            "Early stopping:  0.08809561927986025\n",
            "Epoch: 015, Loss: 1.5005, Train: 0.5306, Test: 0.4959\n",
            "Early stopping:  0.08485845244081788\n",
            "Epoch: 016, Loss: 1.4447, Train: 0.5394, Test: 0.5060\n",
            "Early stopping:  0.0815752242213582\n",
            "Epoch: 017, Loss: 1.3962, Train: 0.5505, Test: 0.5215\n",
            "Early stopping:  0.07998570668816837\n",
            "Epoch: 018, Loss: 1.3558, Train: 0.5791, Test: 0.5379\n",
            "Early stopping:  0.07790264763084\n",
            "Epoch: 019, Loss: 1.3120, Train: 0.5967, Test: 0.5553\n",
            "Early stopping:  0.07379596842509072\n",
            "Epoch: 020, Loss: 1.2689, Train: 0.6115, Test: 0.5657\n",
            "Early stopping:  0.06891511525587098\n",
            "Epoch: 021, Loss: 1.2344, Train: 0.6214, Test: 0.5753\n",
            "Early stopping:  0.06494789580966008\n",
            "Epoch: 022, Loss: 1.2002, Train: 0.6273, Test: 0.5829\n",
            "Early stopping:  0.06159708351930457\n",
            "Epoch: 023, Loss: 1.1639, Train: 0.6327, Test: 0.5908\n",
            "Early stopping:  0.05774757747492141\n",
            "Epoch: 024, Loss: 1.1341, Train: 0.6418, Test: 0.5944\n",
            "Early stopping:  0.053799133656887016\n",
            "Epoch: 025, Loss: 1.1045, Train: 0.6566, Test: 0.6031\n",
            "Early stopping:  0.05160003291615974\n",
            "Epoch: 026, Loss: 1.0735, Train: 0.6699, Test: 0.6101\n",
            "Early stopping:  0.04951855787357619\n",
            "Epoch: 027, Loss: 1.0478, Train: 0.6781, Test: 0.6135\n",
            "Early stopping:  0.04633335207303463\n",
            "Epoch: 028, Loss: 1.0221, Train: 0.6821, Test: 0.6203\n",
            "Early stopping:  0.044409568073916716\n",
            "Epoch: 029, Loss: 0.9966, Train: 0.6866, Test: 0.6252\n",
            "Early stopping:  0.04228134280500771\n",
            "Epoch: 030, Loss: 0.9753, Train: 0.6999, Test: 0.6330\n",
            "Early stopping:  0.03915082054211474\n",
            "Epoch: 031, Loss: 0.9523, Train: 0.7104, Test: 0.6354\n",
            "Early stopping:  0.03760017610760667\n",
            "Epoch: 032, Loss: 0.9297, Train: 0.7184, Test: 0.6371\n",
            "Early stopping:  0.03622097445862362\n",
            "Epoch: 033, Loss: 0.9100, Train: 0.7249, Test: 0.6428\n",
            "Early stopping:  0.03459530882481456\n",
            "Epoch: 034, Loss: 0.8884, Train: 0.7326, Test: 0.6468\n",
            "Early stopping:  0.034194881353786274\n",
            "Epoch: 035, Loss: 0.8680, Train: 0.7399, Test: 0.6519\n",
            "Early stopping:  0.03322443051447334\n",
            "Epoch: 036, Loss: 0.8487, Train: 0.7504, Test: 0.6539\n",
            "Early stopping:  0.03228388423333264\n",
            "Epoch: 037, Loss: 0.8289, Train: 0.7572, Test: 0.6562\n",
            "Early stopping:  0.03192901420358033\n",
            "Epoch: 038, Loss: 0.8111, Train: 0.7640, Test: 0.6594\n",
            "Early stopping:  0.03063966813103537\n",
            "Epoch: 039, Loss: 0.7923, Train: 0.7706, Test: 0.6624\n",
            "Early stopping:  0.029882727845726566\n",
            "Epoch: 040, Loss: 0.7750, Train: 0.7788, Test: 0.6687\n",
            "Early stopping:  0.02910670817836236\n",
            "Epoch: 041, Loss: 0.7574, Train: 0.7862, Test: 0.6702\n",
            "Early stopping:  0.02832426822574362\n",
            "Epoch: 042, Loss: 0.7404, Train: 0.7913, Test: 0.6730\n",
            "Early stopping:  0.02787523123598557\n",
            "Epoch: 043, Loss: 0.7238, Train: 0.7978, Test: 0.6762\n",
            "Early stopping:  0.027129875138982594\n",
            "Epoch: 044, Loss: 0.7073, Train: 0.8052, Test: 0.6808\n",
            "Early stopping:  0.026709357163604824\n",
            "Epoch: 045, Loss: 0.6913, Train: 0.8100, Test: 0.6855\n",
            "Early stopping:  0.026121436824146988\n",
            "Epoch: 046, Loss: 0.6754, Train: 0.8162, Test: 0.6872\n",
            "Early stopping:  0.02567091594848511\n",
            "Epoch: 047, Loss: 0.6597, Train: 0.8191, Test: 0.6908\n",
            "Early stopping:  0.025317787435777263\n",
            "Epoch: 048, Loss: 0.6443, Train: 0.8261, Test: 0.6938\n",
            "Early stopping:  0.024938925244776176\n",
            "Epoch: 049, Loss: 0.6290, Train: 0.8347, Test: 0.6972\n",
            "Early stopping:  0.02462408650782685\n",
            "Epoch: 050, Loss: 0.6139, Train: 0.8412, Test: 0.6982\n",
            "Early stopping:  0.024323675796577033\n",
            "Epoch: 051, Loss: 0.5990, Train: 0.8443, Test: 0.6991\n",
            "Early stopping:  0.0240065758558195\n",
            "Epoch: 052, Loss: 0.5843, Train: 0.8474, Test: 0.7012\n",
            "Early stopping:  0.023732387899401408\n",
            "Epoch: 053, Loss: 0.5696, Train: 0.8517, Test: 0.7029\n",
            "Early stopping:  0.02347928189318482\n",
            "Epoch: 054, Loss: 0.5555, Train: 0.8568, Test: 0.7055\n",
            "Early stopping:  0.023112464162464774\n",
            "Epoch: 055, Loss: 0.5413, Train: 0.8605, Test: 0.7072\n",
            "Early stopping:  0.022806405492005973\n",
            "Epoch: 056, Loss: 0.5275, Train: 0.8644, Test: 0.7110\n",
            "Early stopping:  0.022426903726137627\n",
            "Epoch: 057, Loss: 0.5135, Train: 0.8690, Test: 0.7099\n",
            "Early stopping:  0.02214594789000224\n",
            "Epoch: 058, Loss: 0.5001, Train: 0.8755, Test: 0.7118\n",
            "Early stopping:  0.021884601395607016\n",
            "Epoch: 059, Loss: 0.4871, Train: 0.8741, Test: 0.7089\n",
            "Early stopping:  0.02146662174757758\n",
            "Epoch: 060, Loss: 0.4763, Train: 0.8834, Test: 0.7188\n",
            "Early stopping:  0.020408419595771943\n",
            "Epoch: 061, Loss: 0.4686, Train: 0.8772, Test: 0.7112\n",
            "Early stopping:  0.018097772828766706\n",
            "Epoch: 062, Loss: 0.4576, Train: 0.8936, Test: 0.7210\n",
            "Early stopping:  0.016443152741841067\n",
            "Epoch: 063, Loss: 0.4397, Train: 0.8976, Test: 0.7195\n",
            "Early stopping:  0.01816220638889218\n",
            "Epoch: 064, Loss: 0.4263, Train: 0.8928, Test: 0.7123\n",
            "Early stopping:  0.02058061793732033\n",
            "Epoch: 065, Loss: 0.4205, Train: 0.9070, Test: 0.7224\n",
            "Early stopping:  0.020375408913964967\n",
            "Epoch: 066, Loss: 0.4097, Train: 0.9092, Test: 0.7195\n",
            "Early stopping:  0.018480009984120322\n",
            "Epoch: 067, Loss: 0.3941, Train: 0.9090, Test: 0.7180\n",
            "Early stopping:  0.01722329209273458\n",
            "Epoch: 068, Loss: 0.3853, Train: 0.9206, Test: 0.7242\n",
            "Early stopping:  0.01731671499673264\n",
            "Epoch: 069, Loss: 0.3786, Train: 0.9169, Test: 0.7205\n",
            "Early stopping:  0.017297426377357712\n",
            "Epoch: 070, Loss: 0.3654, Train: 0.9254, Test: 0.7229\n",
            "Early stopping:  0.016617047460212735\n",
            "Epoch: 071, Loss: 0.3542, Train: 0.9322, Test: 0.7254\n",
            "Early stopping:  0.01586856555671528\n",
            "Epoch: 072, Loss: 0.3472, Train: 0.9246, Test: 0.7250\n",
            "Early stopping:  0.016017572376668576\n",
            "Epoch: 073, Loss: 0.3381, Train: 0.9393, Test: 0.7265\n",
            "Early stopping:  0.015800675350200143\n",
            "Epoch: 074, Loss: 0.3275, Train: 0.9407, Test: 0.7271\n",
            "Early stopping:  0.014554807370870544\n",
            "Epoch: 075, Loss: 0.3184, Train: 0.9421, Test: 0.7263\n",
            "Early stopping:  0.014460761079245014\n",
            "Epoch: 076, Loss: 0.3102, Train: 0.9487, Test: 0.7271\n",
            "Early stopping:  0.014815956987177028\n",
            "Epoch: 077, Loss: 0.3024, Train: 0.9450, Test: 0.7242\n",
            "Early stopping:  0.01404998367289106\n",
            "Epoch: 078, Loss: 0.2951, Train: 0.9521, Test: 0.7286\n",
            "Early stopping:  0.012778458396438007\n",
            "Epoch: 079, Loss: 0.2866, Train: 0.9541, Test: 0.7248\n",
            "Early stopping:  0.012428333763086078\n",
            "Epoch: 080, Loss: 0.2773, Train: 0.9555, Test: 0.7259\n",
            "Early stopping:  0.012930731188026172\n",
            "Epoch: 081, Loss: 0.2698, Train: 0.9594, Test: 0.7282\n",
            "Early stopping:  0.013137886794656333\n",
            "Epoch: 082, Loss: 0.2641, Train: 0.9583, Test: 0.7252\n",
            "Early stopping:  0.012505620020356979\n",
            "Epoch: 083, Loss: 0.2580, Train: 0.9665, Test: 0.7282\n",
            "Early stopping:  0.011192383594153322\n",
            "Epoch: 084, Loss: 0.2514, Train: 0.9592, Test: 0.7267\n",
            "Early stopping:  0.01007837315346974\n",
            "Epoch: 085, Loss: 0.2464, Train: 0.9725, Test: 0.7280\n",
            "Early stopping:  0.00942267078021792\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.74      0.73       568\n",
            "         capital_goods       0.61      0.63      0.62       381\n",
            "conglomerates_industry       0.96      0.37      0.53        60\n",
            "     consumer_cyclical       0.69      0.70      0.70       595\n",
            " consumer_non-cyclical       0.71      0.64      0.68       334\n",
            "                energy       0.81      0.72      0.76       213\n",
            "             financial       0.75      0.79      0.77       576\n",
            "            healthcare       0.84      0.77      0.80       238\n",
            "              services       0.72      0.78      0.75      1557\n",
            "            technology       0.69      0.58      0.63       297\n",
            "        transportation       0.79      0.79      0.79       303\n",
            "             utilities       0.86      0.73      0.79       169\n",
            "\n",
            "              accuracy                           0.73      5291\n",
            "             macro avg       0.76      0.69      0.71      5291\n",
            "          weighted avg       0.73      0.73      0.73      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5312, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3207, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14882052209196683\n",
            "Epoch: 003, Loss: 2.1756, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17878848389998256\n",
            "Epoch: 004, Loss: 2.1358, Train: 0.2992, Test: 0.2990\n",
            "Early stopping:  0.17885992049846028\n",
            "Epoch: 005, Loss: 2.0750, Train: 0.3505, Test: 0.3476\n",
            "Early stopping:  0.18250678554965513\n",
            "Epoch: 006, Loss: 2.0061, Train: 0.3962, Test: 0.3852\n",
            "Early stopping:  0.11839386640812101\n",
            "Epoch: 007, Loss: 1.9411, Train: 0.4075, Test: 0.3952\n",
            "Early stopping:  0.09509145996665735\n",
            "Epoch: 008, Loss: 1.8772, Train: 0.4104, Test: 0.3950\n",
            "Early stopping:  0.10296993219055348\n",
            "Epoch: 009, Loss: 1.8147, Train: 0.4189, Test: 0.4048\n",
            "Early stopping:  0.10271415581205405\n",
            "Epoch: 010, Loss: 1.7534, Train: 0.4404, Test: 0.4277\n",
            "Early stopping:  0.0998849238170922\n",
            "Epoch: 011, Loss: 1.6890, Train: 0.4654, Test: 0.4470\n",
            "Early stopping:  0.09930350643116452\n",
            "Epoch: 012, Loss: 1.6236, Train: 0.4912, Test: 0.4595\n",
            "Early stopping:  0.10011012624098908\n",
            "Epoch: 013, Loss: 1.5647, Train: 0.5060, Test: 0.4734\n",
            "Early stopping:  0.09961486451429338\n",
            "Epoch: 014, Loss: 1.5080, Train: 0.5235, Test: 0.4850\n",
            "Early stopping:  0.0973017631670181\n",
            "Epoch: 015, Loss: 1.4505, Train: 0.5451, Test: 0.5046\n",
            "Early stopping:  0.09371451308530099\n",
            "Epoch: 016, Loss: 1.3972, Train: 0.5706, Test: 0.5320\n",
            "Early stopping:  0.08966818306728205\n",
            "Epoch: 017, Loss: 1.3475, Train: 0.5950, Test: 0.5589\n",
            "Early stopping:  0.08624814813831296\n",
            "Epoch: 018, Loss: 1.3007, Train: 0.6106, Test: 0.5668\n",
            "Early stopping:  0.08191424899610436\n",
            "Epoch: 019, Loss: 1.2583, Train: 0.6185, Test: 0.5723\n",
            "Early stopping:  0.07610664486368632\n",
            "Epoch: 020, Loss: 1.2171, Train: 0.6234, Test: 0.5774\n",
            "Early stopping:  0.07109218390590756\n",
            "Epoch: 021, Loss: 1.1795, Train: 0.6313, Test: 0.5853\n",
            "Early stopping:  0.06640174678735014\n",
            "Epoch: 022, Loss: 1.1472, Train: 0.6427, Test: 0.5910\n",
            "Early stopping:  0.06109543680788266\n",
            "Epoch: 023, Loss: 1.1157, Train: 0.6514, Test: 0.6023\n",
            "Early stopping:  0.05623978266724624\n",
            "Epoch: 024, Loss: 1.0853, Train: 0.6594, Test: 0.6086\n",
            "Early stopping:  0.05181452275861701\n",
            "Epoch: 025, Loss: 1.0593, Train: 0.6668, Test: 0.6135\n",
            "Early stopping:  0.04780786212503224\n",
            "Epoch: 026, Loss: 1.0350, Train: 0.6790, Test: 0.6195\n",
            "Early stopping:  0.04444564759376015\n",
            "Epoch: 027, Loss: 1.0084, Train: 0.6880, Test: 0.6256\n",
            "Early stopping:  0.041930175391221985\n",
            "Epoch: 028, Loss: 0.9835, Train: 0.6954, Test: 0.6307\n",
            "Early stopping:  0.04027464829575005\n",
            "Epoch: 029, Loss: 0.9594, Train: 0.7036, Test: 0.6349\n",
            "Early stopping:  0.03975273638472788\n",
            "Epoch: 030, Loss: 0.9353, Train: 0.7170, Test: 0.6422\n",
            "Early stopping:  0.0392720953448028\n",
            "Epoch: 031, Loss: 0.9117, Train: 0.7306, Test: 0.6460\n",
            "Early stopping:  0.038185108829563545\n",
            "Epoch: 032, Loss: 0.8891, Train: 0.7337, Test: 0.6513\n",
            "Early stopping:  0.0374082752309937\n",
            "Epoch: 033, Loss: 0.8677, Train: 0.7448, Test: 0.6530\n",
            "Early stopping:  0.03635355192051503\n",
            "Epoch: 034, Loss: 0.8469, Train: 0.7530, Test: 0.6545\n",
            "Early stopping:  0.0349391376293093\n",
            "Epoch: 035, Loss: 0.8268, Train: 0.7626, Test: 0.6583\n",
            "Early stopping:  0.033521489979182105\n",
            "Epoch: 036, Loss: 0.8074, Train: 0.7723, Test: 0.6621\n",
            "Early stopping:  0.03227951466829449\n",
            "Epoch: 037, Loss: 0.7879, Train: 0.7799, Test: 0.6674\n",
            "Early stopping:  0.03147252307048006\n",
            "Epoch: 038, Loss: 0.7693, Train: 0.7893, Test: 0.6740\n",
            "Early stopping:  0.030706401054265576\n",
            "Epoch: 039, Loss: 0.7513, Train: 0.7930, Test: 0.6770\n",
            "Early stopping:  0.029901260845474068\n",
            "Epoch: 040, Loss: 0.7332, Train: 0.7992, Test: 0.6762\n",
            "Early stopping:  0.029262518925447133\n",
            "Epoch: 041, Loss: 0.7160, Train: 0.8060, Test: 0.6796\n",
            "Early stopping:  0.02842226464742268\n",
            "Epoch: 042, Loss: 0.6995, Train: 0.8094, Test: 0.6829\n",
            "Early stopping:  0.027655541031570415\n",
            "Epoch: 043, Loss: 0.6827, Train: 0.8154, Test: 0.6872\n",
            "Early stopping:  0.027035117830358037\n",
            "Epoch: 044, Loss: 0.6662, Train: 0.8182, Test: 0.6902\n",
            "Early stopping:  0.026448629066519434\n",
            "Epoch: 045, Loss: 0.6500, Train: 0.8225, Test: 0.6934\n",
            "Early stopping:  0.02614093724153462\n",
            "Epoch: 046, Loss: 0.6338, Train: 0.8287, Test: 0.6933\n",
            "Early stopping:  0.025947518127241102\n",
            "Epoch: 047, Loss: 0.6178, Train: 0.8315, Test: 0.6927\n",
            "Early stopping:  0.02566580487776886\n",
            "Epoch: 048, Loss: 0.6022, Train: 0.8432, Test: 0.6984\n",
            "Early stopping:  0.025348225911021323\n",
            "Epoch: 049, Loss: 0.5865, Train: 0.8429, Test: 0.7012\n",
            "Early stopping:  0.025089754327147565\n",
            "Epoch: 050, Loss: 0.5714, Train: 0.8548, Test: 0.7002\n",
            "Early stopping:  0.0246895545166294\n",
            "Epoch: 051, Loss: 0.5569, Train: 0.8511, Test: 0.7033\n",
            "Early stopping:  0.02411336456799841\n",
            "Epoch: 052, Loss: 0.5435, Train: 0.8630, Test: 0.7033\n",
            "Early stopping:  0.023222317672470308\n",
            "Epoch: 053, Loss: 0.5309, Train: 0.8599, Test: 0.7059\n",
            "Early stopping:  0.021998823928801772\n",
            "Epoch: 054, Loss: 0.5176, Train: 0.8701, Test: 0.7076\n",
            "Early stopping:  0.021127548218599617\n",
            "Epoch: 055, Loss: 0.5016, Train: 0.8752, Test: 0.7097\n",
            "Early stopping:  0.02162853063633335\n",
            "Epoch: 056, Loss: 0.4864, Train: 0.8772, Test: 0.7097\n",
            "Early stopping:  0.02272379846972705\n",
            "Epoch: 057, Loss: 0.4743, Train: 0.8846, Test: 0.7065\n",
            "Early stopping:  0.022850047420099072\n",
            "Epoch: 058, Loss: 0.4635, Train: 0.8866, Test: 0.7123\n",
            "Early stopping:  0.02151725271689159\n",
            "Epoch: 059, Loss: 0.4523, Train: 0.8917, Test: 0.7135\n",
            "Early stopping:  0.019268595089733064\n",
            "Epoch: 060, Loss: 0.4393, Train: 0.8996, Test: 0.7118\n",
            "Early stopping:  0.018393033190491334\n",
            "Epoch: 061, Loss: 0.4272, Train: 0.9007, Test: 0.7150\n",
            "Early stopping:  0.0187219568272965\n",
            "Epoch: 062, Loss: 0.4169, Train: 0.9098, Test: 0.7114\n",
            "Early stopping:  0.018704784198887946\n",
            "Epoch: 063, Loss: 0.4050, Train: 0.9087, Test: 0.7114\n",
            "Early stopping:  0.018497159179990952\n",
            "Epoch: 064, Loss: 0.3932, Train: 0.9155, Test: 0.7139\n",
            "Early stopping:  0.018069723783990015\n",
            "Epoch: 065, Loss: 0.3829, Train: 0.9197, Test: 0.7159\n",
            "Early stopping:  0.017749340718564825\n",
            "Epoch: 066, Loss: 0.3750, Train: 0.9166, Test: 0.7137\n",
            "Early stopping:  0.01678875288504919\n",
            "Epoch: 067, Loss: 0.3679, Train: 0.9271, Test: 0.7184\n",
            "Early stopping:  0.01469117568360141\n",
            "Epoch: 068, Loss: 0.3572, Train: 0.9328, Test: 0.7157\n",
            "Early stopping:  0.013790036615589429\n",
            "Epoch: 069, Loss: 0.3455, Train: 0.9305, Test: 0.7174\n",
            "Early stopping:  0.014712044724830587\n",
            "Epoch: 070, Loss: 0.3377, Train: 0.9427, Test: 0.7171\n",
            "Early stopping:  0.015382053345980544\n",
            "Epoch: 071, Loss: 0.3307, Train: 0.9365, Test: 0.7178\n",
            "Early stopping:  0.014954900463720664\n",
            "Epoch: 072, Loss: 0.3208, Train: 0.9484, Test: 0.7169\n",
            "Early stopping:  0.013920227398631501\n",
            "Epoch: 073, Loss: 0.3092, Train: 0.9526, Test: 0.7205\n",
            "Early stopping:  0.01424860411754998\n",
            "Epoch: 074, Loss: 0.3014, Train: 0.9450, Test: 0.7159\n",
            "Early stopping:  0.014941771783265062\n",
            "Epoch: 075, Loss: 0.2966, Train: 0.9563, Test: 0.7167\n",
            "Early stopping:  0.014010620473556957\n",
            "Epoch: 076, Loss: 0.2887, Train: 0.9526, Test: 0.7190\n",
            "Early stopping:  0.012282966769082623\n",
            "Epoch: 077, Loss: 0.2789, Train: 0.9589, Test: 0.7188\n",
            "Early stopping:  0.011644100491547673\n",
            "Epoch: 078, Loss: 0.2716, Train: 0.9586, Test: 0.7190\n",
            "Early stopping:  0.012269033297785746\n",
            "Epoch: 079, Loss: 0.2686, Train: 0.9552, Test: 0.7116\n",
            "Early stopping:  0.011706093885898031\n",
            "Epoch: 080, Loss: 0.2701, Train: 0.9490, Test: 0.7156\n",
            "Early stopping:  0.00834275209162903\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.73      0.74       568\n",
            "         capital_goods       0.66      0.64      0.65       381\n",
            "conglomerates_industry       1.00      0.38      0.55        60\n",
            "     consumer_cyclical       0.75      0.58      0.65       595\n",
            " consumer_non-cyclical       0.77      0.60      0.68       334\n",
            "                energy       0.82      0.71      0.76       213\n",
            "             financial       0.71      0.78      0.74       576\n",
            "            healthcare       0.78      0.76      0.77       238\n",
            "              services       0.66      0.81      0.73      1557\n",
            "            technology       0.70      0.55      0.61       297\n",
            "        transportation       0.84      0.77      0.80       303\n",
            "             utilities       0.82      0.72      0.77       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.67      0.70      5291\n",
            "          weighted avg       0.72      0.72      0.71      5291\n",
            "\n",
            "time: 2min 33s (started: 2024-10-16 21:20:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWkRxTZ-gSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86879012-ca72-4abd-85fe-cad40acc3484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 443 ms (started: 2024-10-16 21:23:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQn7_2oIgSl-"
      },
      "source": [
        "### Training rotulated base = 60% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLJWdMmggSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e8636d-41aa-4e46-bcd2-11eba7d0446b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 425 µs (started: 2024-10-16 21:23:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpQ-xPd7gSl-"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVQBJuCOgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3896b24a-da04-42e3-9c7b-8f6ae045896c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 85.7005, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 431.2759, Train: 0.0588, Test: 0.0536\n",
            "Early stopping:  244.35874743841907\n",
            "Epoch: 003, Loss: 291.2050, Train: 0.0690, Test: 0.0692\n",
            "Early stopping:  173.81713328319566\n",
            "Epoch: 004, Loss: 365.4099, Train: 0.1681, Test: 0.1636\n",
            "Early stopping:  149.82112500142875\n",
            "Epoch: 005, Loss: 390.6188, Train: 0.1236, Test: 0.1211\n",
            "Early stopping:  136.8399067706771\n",
            "Epoch: 006, Loss: 367.7355, Train: 0.0491, Test: 0.0499\n",
            "Early stopping:  51.0193527671637\n",
            "Epoch: 007, Loss: 334.8643, Train: 0.1191, Test: 0.1154\n",
            "Early stopping:  38.366107158476595\n",
            "Epoch: 008, Loss: 205.0591, Train: 0.0853, Test: 0.0831\n",
            "Early stopping:  74.0758029213539\n",
            "Epoch: 009, Loss: 138.5168, Train: 0.1909, Test: 0.1945\n",
            "Early stopping:  109.89425505131518\n",
            "Epoch: 010, Loss: 80.4778, Train: 0.2921, Test: 0.2952\n",
            "Early stopping:  123.69991790804804\n",
            "Epoch: 011, Loss: 76.4372, Train: 0.2524, Test: 0.2470\n",
            "Early stopping:  107.38467183297531\n",
            "Epoch: 012, Loss: 62.6877, Train: 0.1000, Test: 0.0921\n",
            "Early stopping:  59.26880588798081\n",
            "Epoch: 013, Loss: 55.1807, Train: 0.0922, Test: 0.0828\n",
            "Early stopping:  32.85375672887635\n",
            "Epoch: 014, Loss: 37.7215, Train: 0.1038, Test: 0.1009\n",
            "Early stopping:  17.212146275411452\n",
            "Epoch: 015, Loss: 19.4621, Train: 0.0928, Test: 0.0916\n",
            "Early stopping:  22.190243847649768\n",
            "Epoch: 016, Loss: 8.4155, Train: 0.1166, Test: 0.1162\n",
            "Early stopping:  22.995108770217268\n",
            "Epoch: 017, Loss: 4.7374, Train: 0.1495, Test: 0.1483\n",
            "Early stopping:  21.14795915440252\n",
            "Epoch: 018, Loss: 3.2023, Train: 0.2155, Test: 0.2095\n",
            "Early stopping:  14.349322360419485\n",
            "Epoch: 019, Loss: 2.5532, Train: 0.2253, Test: 0.2226\n",
            "Early stopping:  6.970499846827659\n",
            "Epoch: 020, Loss: 2.4181, Train: 0.2359, Test: 0.2297\n",
            "Early stopping:  2.4960026670049444\n",
            "Epoch: 021, Loss: 2.3858, Train: 0.2393, Test: 0.2305\n",
            "Early stopping:  0.9946346863714861\n",
            "Epoch: 022, Loss: 2.3813, Train: 0.2389, Test: 0.2314\n",
            "Early stopping:  0.35037553163502433\n",
            "Epoch: 023, Loss: 2.3757, Train: 0.2350, Test: 0.2308\n",
            "Early stopping:  0.07470261143830965\n",
            "Epoch: 024, Loss: 2.3614, Train: 0.2348, Test: 0.2297\n",
            "Early stopping:  0.020928900617737594\n",
            "Epoch: 025, Loss: 2.3408, Train: 0.2378, Test: 0.2294\n",
            "Early stopping:  0.018269720195531733\n",
            "Epoch: 026, Loss: 2.3163, Train: 0.2439, Test: 0.2353\n",
            "Early stopping:  0.02674221607789875\n",
            "Epoch: 027, Loss: 2.2937, Train: 0.2520, Test: 0.2336\n",
            "Early stopping:  0.03319987020473602\n",
            "Epoch: 028, Loss: 2.2737, Train: 0.2595, Test: 0.2399\n",
            "Early stopping:  0.03520827068106769\n",
            "Epoch: 029, Loss: 2.2548, Train: 0.2590, Test: 0.2396\n",
            "Early stopping:  0.03397910798849981\n",
            "Epoch: 030, Loss: 2.2333, Train: 0.2660, Test: 0.2518\n",
            "Early stopping:  0.032437113413743966\n",
            "Epoch: 031, Loss: 2.2128, Train: 0.2815, Test: 0.2611\n",
            "Early stopping:  0.0319767461213274\n",
            "Epoch: 032, Loss: 2.2022, Train: 0.2820, Test: 0.2659\n",
            "Early stopping:  0.0294129492981862\n",
            "Epoch: 033, Loss: 2.1943, Train: 0.2881, Test: 0.2719\n",
            "Early stopping:  0.024571860729969777\n",
            "Epoch: 034, Loss: 2.1760, Train: 0.2949, Test: 0.2773\n",
            "Early stopping:  0.02131035126560698\n",
            "Epoch: 035, Loss: 2.1469, Train: 0.3034, Test: 0.2898\n",
            "Early stopping:  0.025886661410597145\n",
            "Epoch: 036, Loss: 2.1164, Train: 0.3187, Test: 0.3034\n",
            "Early stopping:  0.035440454459341145\n",
            "Epoch: 037, Loss: 2.0911, Train: 0.3348, Test: 0.3150\n",
            "Early stopping:  0.042172711280605894\n",
            "Epoch: 038, Loss: 2.0684, Train: 0.3452, Test: 0.3275\n",
            "Early stopping:  0.04291388749345642\n",
            "Epoch: 039, Loss: 2.0502, Train: 0.3612, Test: 0.3459\n",
            "Early stopping:  0.03833107198063816\n",
            "Epoch: 040, Loss: 2.0299, Train: 0.3735, Test: 0.3544\n",
            "Early stopping:  0.033885242429698\n",
            "Epoch: 041, Loss: 2.0043, Train: 0.3817, Test: 0.3623\n",
            "Early stopping:  0.03358194735256928\n",
            "Epoch: 042, Loss: 1.9781, Train: 0.3900, Test: 0.3677\n",
            "Early stopping:  0.03594554215562318\n",
            "Epoch: 043, Loss: 1.9558, Train: 0.3947, Test: 0.3734\n",
            "Early stopping:  0.038099872633800914\n",
            "Epoch: 044, Loss: 1.9374, Train: 0.4006, Test: 0.3774\n",
            "Early stopping:  0.037022555572153\n",
            "Epoch: 045, Loss: 1.9227, Train: 0.4038, Test: 0.3805\n",
            "Early stopping:  0.03244415019143726\n",
            "Epoch: 046, Loss: 1.9090, Train: 0.4095, Test: 0.3870\n",
            "Early stopping:  0.02723193433106088\n",
            "Epoch: 047, Loss: 1.8931, Train: 0.4136, Test: 0.3930\n",
            "Early stopping:  0.02432970530540525\n",
            "Epoch: 048, Loss: 1.8772, Train: 0.4181, Test: 0.3978\n",
            "Early stopping:  0.02370948061724981\n",
            "Epoch: 049, Loss: 1.8625, Train: 0.4274, Test: 0.4035\n",
            "Early stopping:  0.024051463852813648\n",
            "Epoch: 050, Loss: 1.8476, Train: 0.4301, Test: 0.4100\n",
            "Early stopping:  0.024267437215016893\n",
            "Epoch: 051, Loss: 1.8335, Train: 0.4331, Test: 0.4137\n",
            "Early stopping:  0.023555858866632046\n",
            "Epoch: 052, Loss: 1.8206, Train: 0.4393, Test: 0.4188\n",
            "Early stopping:  0.022511675064935052\n",
            "Epoch: 053, Loss: 1.8069, Train: 0.4429, Test: 0.4208\n",
            "Early stopping:  0.02187290581908437\n",
            "Epoch: 054, Loss: 1.7919, Train: 0.4480, Test: 0.4227\n",
            "Early stopping:  0.021826535132888892\n",
            "Epoch: 055, Loss: 1.7776, Train: 0.4507, Test: 0.4216\n",
            "Early stopping:  0.02220977173944961\n",
            "Epoch: 056, Loss: 1.7655, Train: 0.4524, Test: 0.4239\n",
            "Early stopping:  0.022059180438214677\n",
            "Epoch: 057, Loss: 1.7526, Train: 0.4548, Test: 0.4267\n",
            "Early stopping:  0.021364581340281425\n",
            "Epoch: 058, Loss: 1.7401, Train: 0.4573, Test: 0.4259\n",
            "Early stopping:  0.020340378060238754\n",
            "Epoch: 059, Loss: 1.7299, Train: 0.4573, Test: 0.4259\n",
            "Early stopping:  0.01911272863568366\n",
            "Epoch: 060, Loss: 1.7202, Train: 0.4618, Test: 0.4293\n",
            "Early stopping:  0.017945584252282314\n",
            "Epoch: 061, Loss: 1.7099, Train: 0.4618, Test: 0.4312\n",
            "Early stopping:  0.016666958279643217\n",
            "Epoch: 062, Loss: 1.6993, Train: 0.4645, Test: 0.4318\n",
            "Early stopping:  0.016054448884130466\n",
            "Epoch: 063, Loss: 1.6881, Train: 0.4694, Test: 0.4366\n",
            "Early stopping:  0.01652788401252597\n",
            "Epoch: 064, Loss: 1.6765, Train: 0.4749, Test: 0.4363\n",
            "Early stopping:  0.017262948736075465\n",
            "Epoch: 065, Loss: 1.6665, Train: 0.4758, Test: 0.4380\n",
            "Early stopping:  0.017327870276591627\n",
            "Epoch: 066, Loss: 1.6566, Train: 0.4764, Test: 0.4432\n",
            "Early stopping:  0.016918483813544577\n",
            "Epoch: 067, Loss: 1.6466, Train: 0.4743, Test: 0.4491\n",
            "Early stopping:  0.016260601875214793\n",
            "Epoch: 068, Loss: 1.6370, Train: 0.4803, Test: 0.4488\n",
            "Early stopping:  0.01565382505030801\n",
            "Epoch: 069, Loss: 1.6263, Train: 0.4819, Test: 0.4517\n",
            "Early stopping:  0.015825394035973876\n",
            "Epoch: 070, Loss: 1.6165, Train: 0.4849, Test: 0.4542\n",
            "Early stopping:  0.015903213903840356\n",
            "Epoch: 071, Loss: 1.6079, Train: 0.4866, Test: 0.4585\n",
            "Early stopping:  0.015494363842980014\n",
            "Epoch: 072, Loss: 1.5996, Train: 0.4896, Test: 0.4593\n",
            "Early stopping:  0.014749513871718209\n",
            "Epoch: 073, Loss: 1.5910, Train: 0.4887, Test: 0.4607\n",
            "Early stopping:  0.013838201064698828\n",
            "Epoch: 074, Loss: 1.5823, Train: 0.4934, Test: 0.4636\n",
            "Early stopping:  0.013485661654096424\n",
            "Epoch: 075, Loss: 1.5731, Train: 0.4958, Test: 0.4661\n",
            "Early stopping:  0.0137556038265059\n",
            "Epoch: 076, Loss: 1.5652, Train: 0.4964, Test: 0.4661\n",
            "Early stopping:  0.013722333469743287\n",
            "Epoch: 077, Loss: 1.5565, Train: 0.4992, Test: 0.4681\n",
            "Early stopping:  0.013610104248963133\n",
            "Epoch: 078, Loss: 1.5478, Train: 0.5000, Test: 0.4701\n",
            "Early stopping:  0.01353352878892804\n",
            "Epoch: 079, Loss: 1.5393, Train: 0.5032, Test: 0.4741\n",
            "Early stopping:  0.013441326049678829\n",
            "Epoch: 080, Loss: 1.5310, Train: 0.5076, Test: 0.4786\n",
            "Early stopping:  0.013559646170711119\n",
            "Epoch: 081, Loss: 1.5222, Train: 0.5125, Test: 0.4826\n",
            "Early stopping:  0.013504567546632767\n",
            "Epoch: 082, Loss: 1.5141, Train: 0.5117, Test: 0.4814\n",
            "Early stopping:  0.01335005924955035\n",
            "Epoch: 083, Loss: 1.5073, Train: 0.5140, Test: 0.4826\n",
            "Early stopping:  0.01280365639788142\n",
            "Epoch: 084, Loss: 1.5003, Train: 0.5157, Test: 0.4828\n",
            "Early stopping:  0.012087562954919221\n",
            "Epoch: 085, Loss: 1.4937, Train: 0.5170, Test: 0.4840\n",
            "Early stopping:  0.01120826734295206\n",
            "Epoch: 086, Loss: 1.4870, Train: 0.5191, Test: 0.4882\n",
            "Early stopping:  0.010719263270383687\n",
            "Epoch: 087, Loss: 1.4801, Train: 0.5210, Test: 0.4894\n",
            "Early stopping:  0.010714776838882339\n",
            "Epoch: 088, Loss: 1.4734, Train: 0.5238, Test: 0.4911\n",
            "Early stopping:  0.010639706850299708\n",
            "Epoch: 089, Loss: 1.4663, Train: 0.5251, Test: 0.4939\n",
            "Early stopping:  0.010812273991669704\n",
            "Epoch: 090, Loss: 1.4590, Train: 0.5261, Test: 0.4942\n",
            "Early stopping:  0.011017039265405926\n",
            "Epoch: 091, Loss: 1.4520, Train: 0.5276, Test: 0.4933\n",
            "Early stopping:  0.011162420539184563\n",
            "Epoch: 092, Loss: 1.4450, Train: 0.5295, Test: 0.4933\n",
            "Early stopping:  0.01124736521488612\n",
            "Epoch: 093, Loss: 1.4381, Train: 0.5321, Test: 0.4962\n",
            "Early stopping:  0.011120163371716988\n",
            "Epoch: 094, Loss: 1.4315, Train: 0.5335, Test: 0.4965\n",
            "Early stopping:  0.010890158142180965\n",
            "Epoch: 095, Loss: 1.4255, Train: 0.5346, Test: 0.4936\n",
            "Early stopping:  0.010519256236171845\n",
            "Epoch: 096, Loss: 1.4196, Train: 0.5367, Test: 0.4933\n",
            "Early stopping:  0.010051645006527474\n",
            "Epoch: 097, Loss: 1.4140, Train: 0.5382, Test: 0.4925\n",
            "Early stopping:  0.009526394580145757\n",
            "PREDICTIONS -> tensor([ 8,  0,  5,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.40      0.58      0.48       379\n",
            "         capital_goods       0.33      0.06      0.10       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.42      0.34      0.38       396\n",
            " consumer_non-cyclical       0.67      0.37      0.48       223\n",
            "                energy       0.54      0.38      0.45       141\n",
            "             financial       0.60      0.61      0.61       384\n",
            "            healthcare       0.75      0.55      0.64       159\n",
            "              services       0.47      0.81      0.60      1038\n",
            "            technology       0.42      0.05      0.09       198\n",
            "        transportation       0.00      0.00      0.00       202\n",
            "             utilities       0.66      0.51      0.58       113\n",
            "\n",
            "              accuracy                           0.49      3527\n",
            "             macro avg       0.44      0.36      0.37      3527\n",
            "          weighted avg       0.46      0.49      0.44      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 77.4738, Train: 0.2898, Test: 0.2889\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 395.6100, Train: 0.1102, Test: 0.1094\n",
            "Early stopping:  224.95623195761337\n",
            "Epoch: 003, Loss: 454.6859, Train: 0.0777, Test: 0.0757\n",
            "Early stopping:  202.89139374723155\n",
            "Epoch: 004, Loss: 287.9645, Train: 0.1219, Test: 0.1225\n",
            "Early stopping:  166.0018565394788\n",
            "Epoch: 005, Loss: 329.0551, Train: 0.0567, Test: 0.0525\n",
            "Early stopping:  144.20014063565935\n",
            "Epoch: 006, Loss: 253.0538, Train: 0.0794, Test: 0.0766\n",
            "Early stopping:  81.47003805216606\n",
            "Epoch: 007, Loss: 156.0074, Train: 0.0750, Test: 0.0766\n",
            "Early stopping:  109.29387516998877\n",
            "Epoch: 008, Loss: 166.0291, Train: 0.0410, Test: 0.0400\n",
            "Early stopping:  75.68982632275497\n",
            "Epoch: 009, Loss: 143.5769, Train: 0.1673, Test: 0.1693\n",
            "Early stopping:  79.51000881469261\n",
            "Epoch: 010, Loss: 90.4975, Train: 0.2631, Test: 0.2538\n",
            "Early stopping:  58.72566276223511\n",
            "Epoch: 011, Loss: 74.3631, Train: 0.2590, Test: 0.2455\n",
            "Early stopping:  41.04410474755769\n",
            "Epoch: 012, Loss: 50.9966, Train: 0.2558, Test: 0.2492\n",
            "Early stopping:  48.16119046965675\n",
            "Epoch: 013, Loss: 27.8670, Train: 0.2611, Test: 0.2518\n",
            "Early stopping:  43.90557472645626\n",
            "Epoch: 014, Loss: 13.1137, Train: 0.2219, Test: 0.2163\n",
            "Early stopping:  31.920693865414616\n",
            "Epoch: 015, Loss: 8.4363, Train: 0.2106, Test: 0.2005\n",
            "Early stopping:  27.57198928181528\n",
            "Epoch: 016, Loss: 5.7701, Train: 0.2127, Test: 0.1982\n",
            "Early stopping:  18.69934111196385\n",
            "Epoch: 017, Loss: 4.2843, Train: 0.2240, Test: 0.2047\n",
            "Early stopping:  9.539247607799313\n",
            "Epoch: 018, Loss: 3.5341, Train: 0.2255, Test: 0.2129\n",
            "Early stopping:  3.883819029796952\n",
            "Epoch: 019, Loss: 3.1483, Train: 0.2431, Test: 0.2280\n",
            "Early stopping:  2.149705427428322\n",
            "Epoch: 020, Loss: 2.8533, Train: 0.2595, Test: 0.2521\n",
            "Early stopping:  1.1659573965820136\n",
            "Epoch: 021, Loss: 2.6462, Train: 0.2558, Test: 0.2504\n",
            "Early stopping:  0.6468109121544321\n",
            "Epoch: 022, Loss: 2.5167, Train: 0.2584, Test: 0.2523\n",
            "Early stopping:  0.40908995483140254\n",
            "Epoch: 023, Loss: 2.4395, Train: 0.2699, Test: 0.2640\n",
            "Early stopping:  0.28576916627844706\n",
            "Epoch: 024, Loss: 2.3911, Train: 0.2767, Test: 0.2767\n",
            "Early stopping:  0.18571200896957787\n",
            "Epoch: 025, Loss: 2.3659, Train: 0.2928, Test: 0.2909\n",
            "Early stopping:  0.11312050156119345\n",
            "Epoch: 026, Loss: 2.3400, Train: 0.3161, Test: 0.3065\n",
            "Early stopping:  0.06974577403075452\n",
            "Epoch: 027, Loss: 2.3207, Train: 0.3187, Test: 0.3130\n",
            "Early stopping:  0.046398254014810604\n",
            "Epoch: 028, Loss: 2.2742, Train: 0.3242, Test: 0.3193\n",
            "Early stopping:  0.044620960978549194\n",
            "Epoch: 029, Loss: 2.2487, Train: 0.3310, Test: 0.3229\n",
            "Early stopping:  0.04790568464066265\n",
            "Epoch: 030, Loss: 2.2185, Train: 0.3410, Test: 0.3297\n",
            "Early stopping:  0.05011898812688456\n",
            "Epoch: 031, Loss: 2.1802, Train: 0.3403, Test: 0.3385\n",
            "Early stopping:  0.053475639728816665\n",
            "Epoch: 032, Loss: 2.1510, Train: 0.3529, Test: 0.3462\n",
            "Early stopping:  0.049884277545288125\n",
            "Epoch: 033, Loss: 2.1202, Train: 0.3665, Test: 0.3575\n",
            "Early stopping:  0.05134972994749691\n",
            "Epoch: 034, Loss: 2.0847, Train: 0.3701, Test: 0.3635\n",
            "Early stopping:  0.051847137069760345\n",
            "Epoch: 035, Loss: 2.0643, Train: 0.3777, Test: 0.3697\n",
            "Early stopping:  0.047271685914181606\n",
            "Epoch: 036, Loss: 2.0458, Train: 0.3836, Test: 0.3720\n",
            "Early stopping:  0.0425031931017511\n",
            "Epoch: 037, Loss: 2.0233, Train: 0.3870, Test: 0.3760\n",
            "Early stopping:  0.03709276995018665\n",
            "Epoch: 038, Loss: 2.0018, Train: 0.3904, Test: 0.3771\n",
            "Early stopping:  0.03271511163054964\n",
            "Epoch: 039, Loss: 1.9783, Train: 0.4057, Test: 0.3884\n",
            "Early stopping:  0.03419218017930868\n",
            "Epoch: 040, Loss: 1.9577, Train: 0.4153, Test: 0.3961\n",
            "Early stopping:  0.03499817189981301\n",
            "Epoch: 041, Loss: 1.9457, Train: 0.4206, Test: 0.4040\n",
            "Early stopping:  0.031695243611133184\n",
            "Epoch: 042, Loss: 1.9277, Train: 0.4204, Test: 0.4066\n",
            "Early stopping:  0.028747351596725333\n",
            "Epoch: 043, Loss: 1.9043, Train: 0.4212, Test: 0.4088\n",
            "Early stopping:  0.0282517082770517\n",
            "Epoch: 044, Loss: 1.8858, Train: 0.4240, Test: 0.4128\n",
            "Early stopping:  0.029430236021962018\n",
            "Epoch: 045, Loss: 1.8686, Train: 0.4251, Test: 0.4185\n",
            "Early stopping:  0.031045540323304865\n",
            "Epoch: 046, Loss: 1.8480, Train: 0.4255, Test: 0.4193\n",
            "Early stopping:  0.030892888522050865\n",
            "Epoch: 047, Loss: 1.8259, Train: 0.4274, Test: 0.4222\n",
            "Early stopping:  0.030830712420332976\n",
            "Epoch: 048, Loss: 1.8016, Train: 0.4297, Test: 0.4264\n",
            "Early stopping:  0.03346529763098357\n",
            "Epoch: 049, Loss: 1.7822, Train: 0.4316, Test: 0.4253\n",
            "Early stopping:  0.034684830448711956\n",
            "Epoch: 050, Loss: 1.7660, Train: 0.4321, Test: 0.4213\n",
            "Early stopping:  0.032940884864406696\n",
            "Epoch: 051, Loss: 1.7514, Train: 0.4399, Test: 0.4264\n",
            "Early stopping:  0.029332446606560553\n",
            "Epoch: 052, Loss: 1.7340, Train: 0.4450, Test: 0.4281\n",
            "Early stopping:  0.026287598712380046\n",
            "Epoch: 053, Loss: 1.7206, Train: 0.4448, Test: 0.4298\n",
            "Early stopping:  0.024560489030594396\n",
            "Epoch: 054, Loss: 1.7049, Train: 0.4491, Test: 0.4341\n",
            "Early stopping:  0.02418995744738234\n",
            "Epoch: 055, Loss: 1.6908, Train: 0.4565, Test: 0.4417\n",
            "Early stopping:  0.023770890029440973\n",
            "Epoch: 056, Loss: 1.6730, Train: 0.4677, Test: 0.4525\n",
            "Early stopping:  0.024003470443324126\n",
            "Epoch: 057, Loss: 1.6590, Train: 0.4745, Test: 0.4619\n",
            "Early stopping:  0.02453694834668637\n",
            "Epoch: 058, Loss: 1.6469, Train: 0.4764, Test: 0.4670\n",
            "Early stopping:  0.023421345434559624\n",
            "Epoch: 059, Loss: 1.6350, Train: 0.4803, Test: 0.4673\n",
            "Early stopping:  0.021859896153074116\n",
            "Epoch: 060, Loss: 1.6226, Train: 0.4847, Test: 0.4661\n",
            "Early stopping:  0.019747053317751735\n",
            "Epoch: 061, Loss: 1.6103, Train: 0.4885, Test: 0.4636\n",
            "Early stopping:  0.01925644404083781\n",
            "Epoch: 062, Loss: 1.5972, Train: 0.4934, Test: 0.4661\n",
            "Early stopping:  0.01964047611739239\n",
            "Epoch: 063, Loss: 1.5836, Train: 0.4938, Test: 0.4673\n",
            "Early stopping:  0.02028370210734048\n",
            "Epoch: 064, Loss: 1.5707, Train: 0.4957, Test: 0.4661\n",
            "Early stopping:  0.020634175378431402\n",
            "Epoch: 065, Loss: 1.5592, Train: 0.4989, Test: 0.4724\n",
            "Early stopping:  0.020352094016657597\n",
            "Epoch: 066, Loss: 1.5472, Train: 0.5025, Test: 0.4749\n",
            "Early stopping:  0.019683431342175324\n",
            "Epoch: 067, Loss: 1.5375, Train: 0.5074, Test: 0.4786\n",
            "Early stopping:  0.01833306734460827\n",
            "Epoch: 068, Loss: 1.5244, Train: 0.5085, Test: 0.4800\n",
            "Early stopping:  0.018097627370427253\n",
            "Epoch: 069, Loss: 1.5131, Train: 0.5083, Test: 0.4837\n",
            "Early stopping:  0.018196334650033112\n",
            "Epoch: 070, Loss: 1.5000, Train: 0.5140, Test: 0.4897\n",
            "Early stopping:  0.01878575459647091\n",
            "Epoch: 071, Loss: 1.4893, Train: 0.5223, Test: 0.4979\n",
            "Early stopping:  0.019101201839923008\n",
            "Epoch: 072, Loss: 1.4767, Train: 0.5302, Test: 0.4979\n",
            "Early stopping:  0.01885717113522365\n",
            "Epoch: 073, Loss: 1.4638, Train: 0.5378, Test: 0.5033\n",
            "Early stopping:  0.019282495212942657\n",
            "Epoch: 074, Loss: 1.4489, Train: 0.5408, Test: 0.5061\n",
            "Early stopping:  0.020231590359108268\n",
            "Epoch: 075, Loss: 1.4359, Train: 0.5457, Test: 0.5115\n",
            "Early stopping:  0.021289564239398557\n",
            "Epoch: 076, Loss: 1.4244, Train: 0.5516, Test: 0.5143\n",
            "Early stopping:  0.020963472013954085\n",
            "Epoch: 077, Loss: 1.4104, Train: 0.5556, Test: 0.5143\n",
            "Early stopping:  0.02077047814358744\n",
            "Epoch: 078, Loss: 1.3980, Train: 0.5582, Test: 0.5183\n",
            "Early stopping:  0.02010784940305338\n",
            "Epoch: 079, Loss: 1.3885, Train: 0.5648, Test: 0.5206\n",
            "Early stopping:  0.0191722003147785\n",
            "Epoch: 080, Loss: 1.3747, Train: 0.5671, Test: 0.5200\n",
            "Early stopping:  0.019209511244763472\n",
            "Epoch: 081, Loss: 1.3644, Train: 0.5669, Test: 0.5208\n",
            "Early stopping:  0.01825993002663196\n",
            "Epoch: 082, Loss: 1.3562, Train: 0.5720, Test: 0.5257\n",
            "Early stopping:  0.017106784486708857\n",
            "Epoch: 083, Loss: 1.3484, Train: 0.5741, Test: 0.5299\n",
            "Early stopping:  0.015739044928307317\n",
            "Epoch: 084, Loss: 1.3373, Train: 0.5754, Test: 0.5279\n",
            "Early stopping:  0.01440379528725176\n",
            "Epoch: 085, Loss: 1.3341, Train: 0.5796, Test: 0.5336\n",
            "Early stopping:  0.01269623222356519\n",
            "Epoch: 086, Loss: 1.3207, Train: 0.5794, Test: 0.5398\n",
            "Early stopping:  0.013646151187942532\n",
            "Epoch: 087, Loss: 1.3155, Train: 0.5841, Test: 0.5407\n",
            "Early stopping:  0.01320452226328149\n",
            "Epoch: 088, Loss: 1.3006, Train: 0.5877, Test: 0.5339\n",
            "Early stopping:  0.014839271689483012\n",
            "Epoch: 089, Loss: 1.2962, Train: 0.5905, Test: 0.5449\n",
            "Early stopping:  0.01538222419969327\n",
            "Epoch: 090, Loss: 1.2885, Train: 0.5991, Test: 0.5481\n",
            "Early stopping:  0.013464225524063643\n",
            "Epoch: 091, Loss: 1.2728, Train: 0.5989, Test: 0.5483\n",
            "Early stopping:  0.01570547281344263\n",
            "Epoch: 092, Loss: 1.2655, Train: 0.6045, Test: 0.5554\n",
            "Early stopping:  0.015064426443246801\n",
            "Epoch: 093, Loss: 1.2554, Train: 0.6049, Test: 0.5534\n",
            "Early stopping:  0.01664845002555519\n",
            "Epoch: 094, Loss: 1.2496, Train: 0.6102, Test: 0.5602\n",
            "Early stopping:  0.015277884386268915\n",
            "Epoch: 095, Loss: 1.2340, Train: 0.6053, Test: 0.5563\n",
            "Early stopping:  0.01498809835036429\n",
            "Epoch: 096, Loss: 1.2336, Train: 0.6127, Test: 0.5583\n",
            "Early stopping:  0.013839272333637489\n",
            "Epoch: 097, Loss: 1.2198, Train: 0.6125, Test: 0.5600\n",
            "Early stopping:  0.014148698679104064\n",
            "Epoch: 098, Loss: 1.2176, Train: 0.6176, Test: 0.5625\n",
            "Early stopping:  0.012911943884170286\n",
            "Epoch: 099, Loss: 1.2067, Train: 0.6159, Test: 0.5580\n",
            "Early stopping:  0.011578359342795618\n",
            "Epoch: 100, Loss: 1.2080, Train: 0.6246, Test: 0.5631\n",
            "Early stopping:  0.010855861843837057\n",
            "Epoch: 101, Loss: 1.1911, Train: 0.6206, Test: 0.5642\n",
            "Early stopping:  0.011369337611153914\n",
            "Epoch: 102, Loss: 1.1941, Train: 0.6263, Test: 0.5602\n",
            "Early stopping:  0.010846010194272029\n",
            "Epoch: 103, Loss: 1.1888, Train: 0.6280, Test: 0.5690\n",
            "Early stopping:  0.008994013140690345\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.56      0.48       379\n",
            "         capital_goods       0.45      0.23      0.30       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.55      0.60      0.58       396\n",
            " consumer_non-cyclical       0.63      0.51      0.56       223\n",
            "                energy       0.43      0.04      0.08       141\n",
            "             financial       0.74      0.60      0.66       384\n",
            "            healthcare       0.74      0.53      0.62       159\n",
            "              services       0.57      0.79      0.67      1038\n",
            "            technology       0.51      0.31      0.39       198\n",
            "        transportation       0.73      0.64      0.68       202\n",
            "             utilities       0.50      0.47      0.48       113\n",
            "\n",
            "              accuracy                           0.57      3527\n",
            "             macro avg       0.52      0.44      0.46      3527\n",
            "          weighted avg       0.57      0.57      0.55      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 113.9397, Train: 0.0588, Test: 0.0567\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 387.1398, Train: 0.2904, Test: 0.2898\n",
            "Early stopping:  193.181613620619\n",
            "Epoch: 003, Loss: 266.7357, Train: 0.1051, Test: 0.1052\n",
            "Early stopping:  136.9197008085725\n",
            "Epoch: 004, Loss: 311.3484, Train: 0.0813, Test: 0.0780\n",
            "Early stopping:  115.17624751835325\n",
            "Epoch: 005, Loss: 197.4960, Train: 0.0675, Test: 0.0663\n",
            "Early stopping:  104.85459420856719\n",
            "Epoch: 006, Loss: 271.9218, Train: 0.0667, Test: 0.0663\n",
            "Early stopping:  69.4077596119756\n",
            "Epoch: 007, Loss: 250.9160, Train: 0.1236, Test: 0.1228\n",
            "Early stopping:  41.270203461533754\n",
            "Epoch: 008, Loss: 246.2799, Train: 0.1758, Test: 0.1815\n",
            "Early stopping:  41.41001185305531\n",
            "Epoch: 009, Loss: 245.2961, Train: 0.3023, Test: 0.3071\n",
            "Early stopping:  27.312380730808307\n",
            "Epoch: 010, Loss: 264.1449, Train: 0.3011, Test: 0.3017\n",
            "Early stopping:  11.772116450428918\n",
            "Epoch: 011, Loss: 264.4868, Train: 0.2560, Test: 0.2583\n",
            "Early stopping:  9.453980973909891\n",
            "Epoch: 012, Loss: 238.4995, Train: 0.2448, Test: 0.2523\n",
            "Early stopping:  11.863942497616701\n",
            "Epoch: 013, Loss: 189.6248, Train: 0.0932, Test: 0.0981\n",
            "Early stopping:  30.61739580891344\n",
            "Epoch: 014, Loss: 169.0808, Train: 0.1021, Test: 0.1018\n",
            "Early stopping:  43.73784272021531\n",
            "Epoch: 015, Loss: 134.8778, Train: 0.2350, Test: 0.2407\n",
            "Early stopping:  52.304282545423256\n",
            "Epoch: 016, Loss: 78.8647, Train: 0.2227, Test: 0.2328\n",
            "Early stopping:  59.81710772337403\n",
            "Epoch: 017, Loss: 48.0611, Train: 0.2592, Test: 0.2623\n",
            "Early stopping:  59.709864400879134\n",
            "Epoch: 018, Loss: 28.0659, Train: 0.2248, Test: 0.2263\n",
            "Early stopping:  59.09836280188193\n",
            "Epoch: 019, Loss: 17.8701, Train: 0.2433, Test: 0.2336\n",
            "Early stopping:  47.13766502702179\n",
            "Epoch: 020, Loss: 9.4419, Train: 0.2255, Test: 0.2209\n",
            "Early stopping:  27.743339232643983\n",
            "Epoch: 021, Loss: 5.8895, Train: 0.2244, Test: 0.2194\n",
            "Early stopping:  16.95727397094737\n",
            "Epoch: 022, Loss: 4.0845, Train: 0.2399, Test: 0.2348\n",
            "Early stopping:  9.91689020635581\n",
            "Epoch: 023, Loss: 3.0637, Train: 0.2870, Test: 0.2711\n",
            "Early stopping:  5.992200706621123\n",
            "Epoch: 024, Loss: 2.5372, Train: 0.3006, Test: 0.2986\n",
            "Early stopping:  2.791684626294756\n",
            "Epoch: 025, Loss: 2.3125, Train: 0.3180, Test: 0.3079\n",
            "Early stopping:  1.4619137672867009\n",
            "Epoch: 026, Loss: 2.2366, Train: 0.3217, Test: 0.3105\n",
            "Early stopping:  0.7636270331251009\n",
            "Epoch: 027, Loss: 2.2281, Train: 0.3246, Test: 0.3122\n",
            "Early stopping:  0.35165739447591005\n",
            "Epoch: 028, Loss: 2.2415, Train: 0.3193, Test: 0.3045\n",
            "Early stopping:  0.13078742357046672\n",
            "Epoch: 029, Loss: 2.2576, Train: 0.3208, Test: 0.3082\n",
            "Early stopping:  0.03376284562998055\n",
            "Epoch: 030, Loss: 2.2683, Train: 0.3240, Test: 0.3110\n",
            "Early stopping:  0.016287077874678684\n",
            "Epoch: 031, Loss: 2.2718, Train: 0.3299, Test: 0.3167\n",
            "Early stopping:  0.018444286365895988\n",
            "Epoch: 032, Loss: 2.2697, Train: 0.3323, Test: 0.3227\n",
            "Early stopping:  0.01258860405830964\n",
            "Epoch: 033, Loss: 2.2639, Train: 0.3233, Test: 0.3164\n",
            "Early stopping:  0.005640413867399589\n",
            "PREDICTIONS -> tensor([6, 6, 6,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.59      0.05      0.10       379\n",
            "         capital_goods       0.13      0.09      0.11       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.40      0.20      0.26       396\n",
            " consumer_non-cyclical       0.45      0.35      0.40       223\n",
            "                energy       0.08      0.11      0.09       141\n",
            "             financial       0.20      0.66      0.30       384\n",
            "            healthcare       1.00      0.09      0.16       159\n",
            "              services       0.45      0.60      0.52      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.17      0.03      0.05       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.32      3527\n",
            "             macro avg       0.29      0.18      0.17      3527\n",
            "          weighted avg       0.36      0.32      0.27      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 76.7492, Train: 0.2134, Test: 0.2121\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 284.3421, Train: 0.2904, Test: 0.2937\n",
            "Early stopping:  146.79031716574156\n",
            "Epoch: 003, Loss: 302.7186, Train: 0.1151, Test: 0.1157\n",
            "Early stopping:  125.49544372783583\n",
            "Epoch: 004, Loss: 537.9555, Train: 0.0565, Test: 0.0556\n",
            "Early stopping:  188.60496955276318\n",
            "Epoch: 005, Loss: 611.4575, Train: 0.1066, Test: 0.1066\n",
            "Early stopping:  214.53457974579098\n",
            "Epoch: 006, Loss: 513.6990, Train: 0.0741, Test: 0.0729\n",
            "Early stopping:  147.47594160770566\n",
            "Epoch: 007, Loss: 329.8387, Train: 0.0210, Test: 0.0198\n",
            "Early stopping:  135.62419060699872\n",
            "Epoch: 008, Loss: 200.6943, Train: 0.0688, Test: 0.0706\n",
            "Early stopping:  168.68549013266608\n",
            "Epoch: 009, Loss: 152.7201, Train: 0.2129, Test: 0.2101\n",
            "Early stopping:  197.5430922765544\n",
            "Epoch: 010, Loss: 103.9707, Train: 0.1361, Test: 0.1341\n",
            "Early stopping:  164.78147426177816\n",
            "Epoch: 011, Loss: 100.2796, Train: 0.1873, Test: 0.1823\n",
            "Early stopping:  94.51008586688528\n",
            "Epoch: 012, Loss: 67.2503, Train: 0.2909, Test: 0.2869\n",
            "Early stopping:  52.168905906253464\n",
            "Epoch: 013, Loss: 47.5531, Train: 0.2913, Test: 0.2915\n",
            "Early stopping:  40.18435810241198\n",
            "Epoch: 014, Loss: 30.9089, Train: 0.2187, Test: 0.2058\n",
            "Early stopping:  32.056229984737335\n",
            "Epoch: 015, Loss: 15.9146, Train: 0.2567, Test: 0.2339\n",
            "Early stopping:  32.899607556271135\n",
            "Epoch: 016, Loss: 4.1669, Train: 0.1268, Test: 0.1208\n",
            "Early stopping:  25.061849376213672\n",
            "Epoch: 017, Loss: 6.1539, Train: 0.1225, Test: 0.1179\n",
            "Early stopping:  18.254162072390198\n",
            "Epoch: 018, Loss: 6.0704, Train: 0.1308, Test: 0.1231\n",
            "Early stopping:  11.197065648777073\n",
            "Epoch: 019, Loss: 5.0847, Train: 0.1537, Test: 0.1432\n",
            "Early stopping:  4.785443812573543\n",
            "Epoch: 020, Loss: 3.9221, Train: 0.1934, Test: 0.1868\n",
            "Early stopping:  1.0378634592900042\n",
            "Epoch: 021, Loss: 2.9698, Train: 0.2482, Test: 0.2365\n",
            "Early stopping:  1.3820378500311534\n",
            "Epoch: 022, Loss: 2.4694, Train: 0.2732, Test: 0.2628\n",
            "Early stopping:  1.4849031011760645\n",
            "Epoch: 023, Loss: 2.3372, Train: 0.2563, Test: 0.2543\n",
            "Early stopping:  1.1491384688006845\n",
            "Epoch: 024, Loss: 2.3301, Train: 0.2522, Test: 0.2523\n",
            "Early stopping:  0.6767720885440014\n",
            "Epoch: 025, Loss: 2.3487, Train: 0.2429, Test: 0.2362\n",
            "Early stopping:  0.2736518182748664\n",
            "Epoch: 026, Loss: 2.3537, Train: 0.2467, Test: 0.2319\n",
            "Early stopping:  0.05753286260422524\n",
            "Epoch: 027, Loss: 2.3365, Train: 0.2584, Test: 0.2441\n",
            "Early stopping:  0.009672336610285446\n",
            "PREDICTIONS -> tensor([ 1,  6,  1,  ..., 11,  6,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.02      0.03       379\n",
            "         capital_goods       0.14      0.36      0.20       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.18      0.20      0.19       396\n",
            " consumer_non-cyclical       0.00      0.00      0.00       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.23      0.74      0.35       384\n",
            "            healthcare       0.46      0.38      0.42       159\n",
            "              services       0.34      0.29      0.32      1038\n",
            "            technology       1.00      0.01      0.01       198\n",
            "        transportation       0.00      0.00      0.00       202\n",
            "             utilities       0.24      0.36      0.29       113\n",
            "\n",
            "              accuracy                           0.24      3527\n",
            "             macro avg       0.28      0.20      0.15      3527\n",
            "          weighted avg       0.32      0.24      0.20      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 98.5399, Train: 0.1389, Test: 0.1401\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 141.3221, Train: 0.2911, Test: 0.2909\n",
            "Early stopping:  30.251580937416552\n",
            "Epoch: 003, Loss: 276.8069, Train: 0.0639, Test: 0.0629\n",
            "Early stopping:  93.06408355979512\n",
            "Epoch: 004, Loss: 299.4301, Train: 0.0422, Test: 0.0369\n",
            "Early stopping:  99.09268806595253\n",
            "Epoch: 005, Loss: 295.9991, Train: 0.0599, Test: 0.0584\n",
            "Early stopping:  95.16499940877368\n",
            "Epoch: 006, Loss: 307.8432, Train: 0.1164, Test: 0.1165\n",
            "Early stopping:  69.66875947361336\n",
            "Epoch: 007, Loss: 208.2483, Train: 0.1800, Test: 0.1684\n",
            "Early stopping:  40.4351149071214\n",
            "Epoch: 008, Loss: 142.1701, Train: 0.1217, Test: 0.1196\n",
            "Early stopping:  72.92609630053772\n",
            "Epoch: 009, Loss: 142.3519, Train: 0.3045, Test: 0.3048\n",
            "Early stopping:  80.17922872337515\n",
            "Epoch: 010, Loss: 116.3018, Train: 0.2951, Test: 0.2949\n",
            "Early stopping:  77.44445302688567\n",
            "Epoch: 011, Loss: 103.5653, Train: 0.2584, Test: 0.2577\n",
            "Early stopping:  40.38944092654925\n",
            "Epoch: 012, Loss: 78.9359, Train: 0.1756, Test: 0.1713\n",
            "Early stopping:  26.951534750799834\n",
            "Epoch: 013, Loss: 50.6781, Train: 0.1276, Test: 0.1321\n",
            "Early stopping:  35.12528954587041\n",
            "Epoch: 014, Loss: 23.2502, Train: 0.0854, Test: 0.0870\n",
            "Early stopping:  38.09812058316431\n",
            "Epoch: 015, Loss: 12.0196, Train: 0.1189, Test: 0.1171\n",
            "Early stopping:  38.06892513850682\n",
            "Epoch: 016, Loss: 6.4817, Train: 0.1612, Test: 0.1642\n",
            "Early stopping:  30.21993824987183\n",
            "Epoch: 017, Loss: 3.9859, Train: 0.2146, Test: 0.2143\n",
            "Early stopping:  19.052717489551767\n",
            "Epoch: 018, Loss: 2.9355, Train: 0.2352, Test: 0.2212\n",
            "Early stopping:  8.332922550726193\n",
            "Epoch: 019, Loss: 2.5075, Train: 0.2174, Test: 0.2056\n",
            "Early stopping:  3.9132943887254457\n",
            "Epoch: 020, Loss: 2.3647, Train: 0.2015, Test: 0.1945\n",
            "Early stopping:  1.702842135638291\n",
            "Epoch: 021, Loss: 2.3337, Train: 0.1989, Test: 0.1880\n",
            "Early stopping:  0.6917305634294033\n",
            "Epoch: 022, Loss: 2.3300, Train: 0.1979, Test: 0.1905\n",
            "Early stopping:  0.25711554702829187\n",
            "Epoch: 023, Loss: 2.3230, Train: 0.2068, Test: 0.2027\n",
            "Early stopping:  0.0775393088952543\n",
            "Epoch: 024, Loss: 2.3077, Train: 0.2197, Test: 0.2260\n",
            "Early stopping:  0.020893987703301614\n",
            "Epoch: 025, Loss: 2.2888, Train: 0.2539, Test: 0.2640\n",
            "Early stopping:  0.018458649931514616\n",
            "Epoch: 026, Loss: 2.2658, Train: 0.2909, Test: 0.2960\n",
            "Early stopping:  0.02613434662643414\n",
            "Epoch: 027, Loss: 2.2582, Train: 0.3040, Test: 0.3110\n",
            "Early stopping:  0.02736467252822443\n",
            "Epoch: 028, Loss: 2.2494, Train: 0.3064, Test: 0.3153\n",
            "Early stopping:  0.02386923266948761\n",
            "Epoch: 029, Loss: 2.2131, Train: 0.3028, Test: 0.3164\n",
            "Early stopping:  0.02766227330276546\n",
            "Epoch: 030, Loss: 2.1789, Train: 0.3093, Test: 0.3116\n",
            "Early stopping:  0.036405751090527\n",
            "Epoch: 031, Loss: 2.1556, Train: 0.3153, Test: 0.3107\n",
            "Early stopping:  0.044167378154238206\n",
            "Epoch: 032, Loss: 2.1360, Train: 0.3312, Test: 0.3261\n",
            "Early stopping:  0.04534459333985313\n",
            "Epoch: 033, Loss: 2.1156, Train: 0.3473, Test: 0.3411\n",
            "Early stopping:  0.037891948897562346\n",
            "Epoch: 034, Loss: 2.0940, Train: 0.3565, Test: 0.3561\n",
            "Early stopping:  0.03320663132165221\n",
            "Epoch: 035, Loss: 2.0739, Train: 0.3631, Test: 0.3635\n",
            "Early stopping:  0.032486912990638134\n",
            "Epoch: 036, Loss: 2.0599, Train: 0.3679, Test: 0.3689\n",
            "Early stopping:  0.030750198779477912\n",
            "Epoch: 037, Loss: 2.0516, Train: 0.3737, Test: 0.3751\n",
            "Early stopping:  0.026000902462457072\n",
            "Epoch: 038, Loss: 2.0423, Train: 0.3769, Test: 0.3762\n",
            "Early stopping:  0.02023684454659089\n",
            "Epoch: 039, Loss: 2.0260, Train: 0.3796, Test: 0.3782\n",
            "Early stopping:  0.01806160413310618\n",
            "Epoch: 040, Loss: 2.0047, Train: 0.3817, Test: 0.3811\n",
            "Early stopping:  0.021959617868012597\n",
            "Epoch: 041, Loss: 1.9892, Train: 0.3892, Test: 0.3879\n",
            "Early stopping:  0.025877554055981994\n",
            "Epoch: 042, Loss: 1.9698, Train: 0.3938, Test: 0.3896\n",
            "Early stopping:  0.028754367446841084\n",
            "Epoch: 043, Loss: 1.9533, Train: 0.3928, Test: 0.3961\n",
            "Early stopping:  0.02853885265351818\n",
            "Epoch: 044, Loss: 1.9363, Train: 0.3951, Test: 0.3967\n",
            "Early stopping:  0.027330219373741478\n",
            "Epoch: 045, Loss: 1.9188, Train: 0.3966, Test: 0.3967\n",
            "Early stopping:  0.027564674497577647\n",
            "Epoch: 046, Loss: 1.9054, Train: 0.3981, Test: 0.4006\n",
            "Early stopping:  0.025836435259809182\n",
            "Epoch: 047, Loss: 1.8920, Train: 0.3975, Test: 0.4026\n",
            "Early stopping:  0.024286021039819528\n",
            "Epoch: 048, Loss: 1.8787, Train: 0.4025, Test: 0.4049\n",
            "Early stopping:  0.022467468857264722\n",
            "Epoch: 049, Loss: 1.8660, Train: 0.4070, Test: 0.4054\n",
            "Early stopping:  0.020917467950145204\n",
            "Epoch: 050, Loss: 1.8496, Train: 0.4068, Test: 0.4086\n",
            "Early stopping:  0.021793859068282784\n",
            "Epoch: 051, Loss: 1.8365, Train: 0.4121, Test: 0.4105\n",
            "Early stopping:  0.022178630308743393\n",
            "Epoch: 052, Loss: 1.8260, Train: 0.4168, Test: 0.4154\n",
            "Early stopping:  0.021406244436597083\n",
            "Epoch: 053, Loss: 1.8115, Train: 0.4210, Test: 0.4176\n",
            "Early stopping:  0.021021833288328278\n",
            "Epoch: 054, Loss: 1.8016, Train: 0.4225, Test: 0.4142\n",
            "Early stopping:  0.01915780043781156\n",
            "Epoch: 055, Loss: 1.7901, Train: 0.4278, Test: 0.4185\n",
            "Early stopping:  0.01856331457786123\n",
            "Epoch: 056, Loss: 1.7763, Train: 0.4263, Test: 0.4151\n",
            "Early stopping:  0.019122281037119947\n",
            "Epoch: 057, Loss: 1.7739, Train: 0.4329, Test: 0.4230\n",
            "Early stopping:  0.01615147351610459\n",
            "Epoch: 058, Loss: 1.7527, Train: 0.4376, Test: 0.4233\n",
            "Early stopping:  0.018429687919882056\n",
            "Epoch: 059, Loss: 1.7446, Train: 0.4372, Test: 0.4236\n",
            "Early stopping:  0.01851392100314412\n",
            "Epoch: 060, Loss: 1.7344, Train: 0.4391, Test: 0.4264\n",
            "Early stopping:  0.018285611968628972\n",
            "Epoch: 061, Loss: 1.7197, Train: 0.4389, Test: 0.4290\n",
            "Early stopping:  0.020270080152309897\n",
            "Epoch: 062, Loss: 1.7077, Train: 0.4414, Test: 0.4267\n",
            "Early stopping:  0.018276822508394994\n",
            "Epoch: 063, Loss: 1.6982, Train: 0.4493, Test: 0.4312\n",
            "Early stopping:  0.018964292589007413\n",
            "Epoch: 064, Loss: 1.6826, Train: 0.4565, Test: 0.4409\n",
            "Early stopping:  0.019846124087215786\n",
            "Epoch: 065, Loss: 1.6727, Train: 0.4629, Test: 0.4451\n",
            "Early stopping:  0.0188782048071238\n",
            "Epoch: 066, Loss: 1.6581, Train: 0.4616, Test: 0.4497\n",
            "Early stopping:  0.019755332484194824\n",
            "Epoch: 067, Loss: 1.6508, Train: 0.4679, Test: 0.4483\n",
            "Early stopping:  0.018966907814357666\n",
            "Epoch: 068, Loss: 1.6371, Train: 0.4692, Test: 0.4517\n",
            "Early stopping:  0.017923597075616703\n",
            "Epoch: 069, Loss: 1.6311, Train: 0.4711, Test: 0.4531\n",
            "Early stopping:  0.016635495293347367\n",
            "Epoch: 070, Loss: 1.6174, Train: 0.4735, Test: 0.4553\n",
            "Early stopping:  0.01611452656743872\n",
            "Epoch: 071, Loss: 1.6151, Train: 0.4735, Test: 0.4573\n",
            "Early stopping:  0.014718794983282884\n",
            "Epoch: 072, Loss: 1.6023, Train: 0.4749, Test: 0.4576\n",
            "Early stopping:  0.013748216401697504\n",
            "Epoch: 073, Loss: 1.5975, Train: 0.4762, Test: 0.4607\n",
            "Early stopping:  0.013244228110773248\n",
            "Epoch: 074, Loss: 1.5830, Train: 0.4786, Test: 0.4610\n",
            "Early stopping:  0.013969235876965922\n",
            "Epoch: 075, Loss: 1.5800, Train: 0.4796, Test: 0.4675\n",
            "Early stopping:  0.014404010933193693\n",
            "Epoch: 076, Loss: 1.5668, Train: 0.4830, Test: 0.4670\n",
            "Early stopping:  0.014267059750707897\n",
            "Epoch: 077, Loss: 1.5613, Train: 0.4851, Test: 0.4692\n",
            "Early stopping:  0.014286040136806911\n",
            "Epoch: 078, Loss: 1.5532, Train: 0.4862, Test: 0.4721\n",
            "Early stopping:  0.01256991343947676\n",
            "Epoch: 079, Loss: 1.5429, Train: 0.4887, Test: 0.4738\n",
            "Early stopping:  0.013997244227743443\n",
            "Epoch: 080, Loss: 1.5405, Train: 0.4890, Test: 0.4755\n",
            "Early stopping:  0.011379136402573403\n",
            "Epoch: 081, Loss: 1.5286, Train: 0.4896, Test: 0.4763\n",
            "Early stopping:  0.012500641287556888\n",
            "Epoch: 082, Loss: 1.5243, Train: 0.4919, Test: 0.4775\n",
            "Early stopping:  0.011596531429372914\n",
            "Epoch: 083, Loss: 1.5136, Train: 0.4932, Test: 0.4760\n",
            "Early stopping:  0.012051738451344132\n",
            "Epoch: 084, Loss: 1.5095, Train: 0.4975, Test: 0.4758\n",
            "Early stopping:  0.012349966100254264\n",
            "Epoch: 085, Loss: 1.4995, Train: 0.4983, Test: 0.4803\n",
            "Early stopping:  0.011654884675223067\n",
            "Epoch: 086, Loss: 1.4938, Train: 0.5015, Test: 0.4786\n",
            "Early stopping:  0.011953389944814792\n",
            "Epoch: 087, Loss: 1.4850, Train: 0.5032, Test: 0.4794\n",
            "Early stopping:  0.011588317353750446\n",
            "Epoch: 088, Loss: 1.4795, Train: 0.5066, Test: 0.4831\n",
            "Early stopping:  0.011840461254119786\n",
            "Epoch: 089, Loss: 1.4727, Train: 0.5076, Test: 0.4823\n",
            "Early stopping:  0.01077108263799746\n",
            "Epoch: 090, Loss: 1.4662, Train: 0.5081, Test: 0.4837\n",
            "Early stopping:  0.010709680973283599\n",
            "Epoch: 091, Loss: 1.4589, Train: 0.5093, Test: 0.4860\n",
            "Early stopping:  0.010382793625638043\n",
            "Epoch: 092, Loss: 1.4524, Train: 0.5112, Test: 0.4905\n",
            "Early stopping:  0.010734982389446\n",
            "Epoch: 093, Loss: 1.4460, Train: 0.5147, Test: 0.4916\n",
            "Early stopping:  0.010601311964725788\n",
            "Epoch: 094, Loss: 1.4393, Train: 0.5172, Test: 0.4908\n",
            "Early stopping:  0.01051348769772917\n",
            "Epoch: 095, Loss: 1.4350, Train: 0.5200, Test: 0.4925\n",
            "Early stopping:  0.009639990599874242\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 0, 0, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.42      0.54      0.47       379\n",
            "         capital_goods       0.23      0.04      0.06       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.41      0.44      0.42       396\n",
            " consumer_non-cyclical       0.57      0.31      0.41       223\n",
            "                energy       0.45      0.07      0.12       141\n",
            "             financial       0.66      0.58      0.62       384\n",
            "            healthcare       0.81      0.21      0.34       159\n",
            "              services       0.47      0.79      0.59      1038\n",
            "            technology       0.39      0.11      0.17       198\n",
            "        transportation       0.73      0.68      0.70       202\n",
            "             utilities       0.69      0.27      0.39       113\n",
            "\n",
            "              accuracy                           0.49      3527\n",
            "             macro avg       0.49      0.34      0.36      3527\n",
            "          weighted avg       0.49      0.49      0.45      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 85.0025, Train: 0.0722, Test: 0.0751\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 271.6493, Train: 0.1132, Test: 0.1126\n",
            "Early stopping:  131.97924834163211\n",
            "Epoch: 003, Loss: 374.3616, Train: 0.2032, Test: 0.1993\n",
            "Early stopping:  146.6944436013379\n",
            "Epoch: 004, Loss: 318.3354, Train: 0.2934, Test: 0.2932\n",
            "Early stopping:  125.4586022032129\n",
            "Epoch: 005, Loss: 453.1882, Train: 0.0775, Test: 0.0726\n",
            "Early stopping:  138.16553274592786\n",
            "Epoch: 006, Loss: 490.8804, Train: 0.1178, Test: 0.1168\n",
            "Early stopping:  91.11942943283674\n",
            "Epoch: 007, Loss: 496.3184, Train: 0.0762, Test: 0.0797\n",
            "Early stopping:  77.70076875535449\n",
            "Epoch: 008, Loss: 435.3859, Train: 0.0378, Test: 0.0391\n",
            "Early stopping:  72.02627342028718\n",
            "Epoch: 009, Loss: 409.4298, Train: 0.2769, Test: 0.2810\n",
            "Early stopping:  36.87288151979309\n",
            "Epoch: 010, Loss: 370.8759, Train: 0.2945, Test: 0.2929\n",
            "Early stopping:  53.602439950427666\n",
            "Epoch: 011, Loss: 349.6661, Train: 0.1749, Test: 0.1800\n",
            "Early stopping:  57.53097443303214\n",
            "Epoch: 012, Loss: 314.8792, Train: 0.3019, Test: 0.3039\n",
            "Early stopping:  47.6962065408754\n",
            "Epoch: 013, Loss: 239.4267, Train: 0.3070, Test: 0.3088\n",
            "Early stopping:  64.35034258489816\n",
            "Epoch: 014, Loss: 171.5873, Train: 0.3013, Test: 0.3020\n",
            "Early stopping:  82.60843518555345\n",
            "Epoch: 015, Loss: 105.9630, Train: 0.1233, Test: 0.1273\n",
            "Early stopping:  100.26440338685939\n",
            "Epoch: 016, Loss: 82.2207, Train: 0.1030, Test: 0.1072\n",
            "Early stopping:  95.91315572124964\n",
            "Epoch: 017, Loss: 56.0301, Train: 0.1431, Test: 0.1452\n",
            "Early stopping:  74.21322621752186\n",
            "Epoch: 018, Loss: 26.7484, Train: 0.2539, Test: 0.2435\n",
            "Early stopping:  55.035066542065124\n",
            "Epoch: 019, Loss: 7.1419, Train: 0.2531, Test: 0.2498\n",
            "Early stopping:  40.080772079395075\n",
            "Epoch: 020, Loss: 4.0163, Train: 0.1958, Test: 0.1965\n",
            "Early stopping:  33.45122534002307\n",
            "Epoch: 021, Loss: 3.5210, Train: 0.1620, Test: 0.1608\n",
            "Early stopping:  22.554291056412595\n",
            "Epoch: 022, Loss: 3.5676, Train: 0.1592, Test: 0.1562\n",
            "Early stopping:  10.035256172100178\n",
            "Epoch: 023, Loss: 3.3628, Train: 0.1788, Test: 0.1778\n",
            "Early stopping:  1.5950119880068456\n",
            "Epoch: 024, Loss: 2.9309, Train: 0.2104, Test: 0.2007\n",
            "Early stopping:  0.39122186317897795\n",
            "Epoch: 025, Loss: 2.5352, Train: 0.2217, Test: 0.2078\n",
            "Early stopping:  0.44092716032457263\n",
            "Epoch: 026, Loss: 2.3932, Train: 0.2130, Test: 0.2084\n",
            "Early stopping:  0.5084300780256918\n",
            "Epoch: 027, Loss: 2.4187, Train: 0.2121, Test: 0.2033\n",
            "Early stopping:  0.41508601856460287\n",
            "Epoch: 028, Loss: 2.4153, Train: 0.2155, Test: 0.2129\n",
            "Early stopping:  0.22619428163078034\n",
            "Epoch: 029, Loss: 2.3693, Train: 0.2282, Test: 0.2226\n",
            "Early stopping:  0.06398814023356872\n",
            "Epoch: 030, Loss: 2.3043, Train: 0.2490, Test: 0.2475\n",
            "Early stopping:  0.046792500655031655\n",
            "Epoch: 031, Loss: 2.2301, Train: 0.2722, Test: 0.2767\n",
            "Early stopping:  0.08027560951952763\n",
            "Epoch: 032, Loss: 2.1600, Train: 0.3047, Test: 0.3014\n",
            "Early stopping:  0.10310810123643104\n",
            "Epoch: 033, Loss: 2.1063, Train: 0.3270, Test: 0.3238\n",
            "Early stopping:  0.10613162293717954\n",
            "Epoch: 034, Loss: 2.0695, Train: 0.3518, Test: 0.3391\n",
            "Early stopping:  0.09465841695270613\n",
            "Epoch: 035, Loss: 2.0384, Train: 0.3561, Test: 0.3482\n",
            "Early stopping:  0.07603819793249343\n",
            "Epoch: 036, Loss: 2.0105, Train: 0.3599, Test: 0.3601\n",
            "Early stopping:  0.05855801428005855\n",
            "Epoch: 037, Loss: 1.9909, Train: 0.3824, Test: 0.3879\n",
            "Early stopping:  0.04609563969104876\n",
            "Epoch: 038, Loss: 1.9766, Train: 0.3941, Test: 0.3947\n",
            "Early stopping:  0.03729570841307048\n",
            "Epoch: 039, Loss: 1.9665, Train: 0.4008, Test: 0.3967\n",
            "Early stopping:  0.02860648950448052\n",
            "Epoch: 040, Loss: 1.9570, Train: 0.4043, Test: 0.4035\n",
            "Early stopping:  0.02103916633014842\n",
            "Epoch: 041, Loss: 1.9449, Train: 0.4098, Test: 0.4040\n",
            "Early stopping:  0.01771639691853034\n",
            "Epoch: 042, Loss: 1.9299, Train: 0.4149, Test: 0.4097\n",
            "Early stopping:  0.01830055832161492\n",
            "Epoch: 043, Loss: 1.9150, Train: 0.4183, Test: 0.4125\n",
            "Early stopping:  0.020684568461333097\n",
            "Epoch: 044, Loss: 1.8910, Train: 0.4219, Test: 0.4128\n",
            "Early stopping:  0.02580861802125206\n",
            "Epoch: 045, Loss: 1.8671, Train: 0.4280, Test: 0.4171\n",
            "Early stopping:  0.030972726450254708\n",
            "Epoch: 046, Loss: 1.8459, Train: 0.4321, Test: 0.4213\n",
            "Early stopping:  0.03421966943216294\n",
            "Epoch: 047, Loss: 1.8277, Train: 0.4350, Test: 0.4225\n",
            "Early stopping:  0.03478763945537732\n",
            "Epoch: 048, Loss: 1.8083, Train: 0.4372, Test: 0.4247\n",
            "Early stopping:  0.03243479114288628\n",
            "Epoch: 049, Loss: 1.7888, Train: 0.4414, Test: 0.4310\n",
            "Early stopping:  0.030712049664121763\n",
            "Epoch: 050, Loss: 1.7684, Train: 0.4433, Test: 0.4324\n",
            "Early stopping:  0.03066819359593211\n",
            "Epoch: 051, Loss: 1.7470, Train: 0.4450, Test: 0.4329\n",
            "Early stopping:  0.03184799946603697\n",
            "Epoch: 052, Loss: 1.7261, Train: 0.4501, Test: 0.4355\n",
            "Early stopping:  0.03260049581261186\n",
            "Epoch: 053, Loss: 1.7058, Train: 0.4533, Test: 0.4355\n",
            "Early stopping:  0.03292414646843235\n",
            "Epoch: 054, Loss: 1.6873, Train: 0.4578, Test: 0.4403\n",
            "Early stopping:  0.032172994808992175\n",
            "Epoch: 055, Loss: 1.6707, Train: 0.4620, Test: 0.4409\n",
            "Early stopping:  0.03029075379525763\n",
            "Epoch: 056, Loss: 1.6525, Train: 0.4660, Test: 0.4420\n",
            "Early stopping:  0.0288509662857307\n",
            "Epoch: 057, Loss: 1.6377, Train: 0.4664, Test: 0.4494\n",
            "Early stopping:  0.027050886738082997\n",
            "Epoch: 058, Loss: 1.6214, Train: 0.4749, Test: 0.4500\n",
            "Early stopping:  0.0260662693534394\n",
            "Epoch: 059, Loss: 1.5999, Train: 0.4775, Test: 0.4519\n",
            "Early stopping:  0.027375000543966924\n",
            "Epoch: 060, Loss: 1.5853, Train: 0.4783, Test: 0.4545\n",
            "Early stopping:  0.02727211391908738\n",
            "Epoch: 061, Loss: 1.5689, Train: 0.4807, Test: 0.4570\n",
            "Early stopping:  0.027512604980478528\n",
            "Epoch: 062, Loss: 1.5562, Train: 0.4847, Test: 0.4636\n",
            "Early stopping:  0.025623808867889907\n",
            "Epoch: 063, Loss: 1.5448, Train: 0.4909, Test: 0.4661\n",
            "Early stopping:  0.022069242347348056\n",
            "Epoch: 064, Loss: 1.5320, Train: 0.4940, Test: 0.4670\n",
            "Early stopping:  0.020730459299770295\n",
            "Epoch: 065, Loss: 1.5176, Train: 0.4987, Test: 0.4712\n",
            "Early stopping:  0.020061991315035596\n",
            "Epoch: 066, Loss: 1.5086, Train: 0.5015, Test: 0.4673\n",
            "Early stopping:  0.01939478380840881\n",
            "Epoch: 067, Loss: 1.4951, Train: 0.5068, Test: 0.4735\n",
            "Early stopping:  0.019464915606407392\n",
            "Epoch: 068, Loss: 1.4803, Train: 0.5146, Test: 0.4828\n",
            "Early stopping:  0.019954264931673817\n",
            "Epoch: 069, Loss: 1.4718, Train: 0.5263, Test: 0.4891\n",
            "Early stopping:  0.019046175758712626\n",
            "Epoch: 070, Loss: 1.4559, Train: 0.5267, Test: 0.4922\n",
            "Early stopping:  0.020399194843485086\n",
            "Epoch: 071, Loss: 1.4478, Train: 0.5289, Test: 0.4931\n",
            "Early stopping:  0.018880888471838265\n",
            "Epoch: 072, Loss: 1.4339, Train: 0.5321, Test: 0.4959\n",
            "Early stopping:  0.01852646930865427\n",
            "Epoch: 073, Loss: 1.4246, Train: 0.5344, Test: 0.4953\n",
            "Early stopping:  0.018485290409218025\n",
            "Epoch: 074, Loss: 1.4138, Train: 0.5391, Test: 0.5007\n",
            "Early stopping:  0.01705674640833638\n",
            "Epoch: 075, Loss: 1.4037, Train: 0.5389, Test: 0.5007\n",
            "Early stopping:  0.017184314295713647\n",
            "Epoch: 076, Loss: 1.3940, Train: 0.5435, Test: 0.5021\n",
            "Early stopping:  0.015945536495800862\n",
            "Epoch: 077, Loss: 1.3826, Train: 0.5435, Test: 0.5024\n",
            "Early stopping:  0.016423001610976862\n",
            "Epoch: 078, Loss: 1.3748, Train: 0.5446, Test: 0.5081\n",
            "Early stopping:  0.015686130860767627\n",
            "Epoch: 079, Loss: 1.3623, Train: 0.5467, Test: 0.5092\n",
            "Early stopping:  0.016134827335873698\n",
            "Epoch: 080, Loss: 1.3527, Train: 0.5486, Test: 0.5064\n",
            "Early stopping:  0.016286740686527412\n",
            "Epoch: 081, Loss: 1.3423, Train: 0.5501, Test: 0.5103\n",
            "Early stopping:  0.016257602863557888\n",
            "Epoch: 082, Loss: 1.3302, Train: 0.5620, Test: 0.5169\n",
            "Early stopping:  0.017298121175491444\n",
            "Epoch: 083, Loss: 1.3217, Train: 0.5624, Test: 0.5166\n",
            "Early stopping:  0.016430390110720814\n",
            "Epoch: 084, Loss: 1.3122, Train: 0.5705, Test: 0.5177\n",
            "Early stopping:  0.01606748135770202\n",
            "Epoch: 085, Loss: 1.3060, Train: 0.5726, Test: 0.5191\n",
            "Early stopping:  0.014384602125209476\n",
            "Epoch: 086, Loss: 1.2955, Train: 0.5737, Test: 0.5186\n",
            "Early stopping:  0.013476158643049662\n",
            "Epoch: 087, Loss: 1.2860, Train: 0.5788, Test: 0.5183\n",
            "Early stopping:  0.013964625626454015\n",
            "Epoch: 088, Loss: 1.2753, Train: 0.5796, Test: 0.5189\n",
            "Early stopping:  0.014891691230113996\n",
            "Epoch: 089, Loss: 1.2684, Train: 0.5769, Test: 0.5223\n",
            "Early stopping:  0.015139447379518951\n",
            "Epoch: 090, Loss: 1.2586, Train: 0.5747, Test: 0.5228\n",
            "Early stopping:  0.014484514534742091\n",
            "Epoch: 091, Loss: 1.2530, Train: 0.5845, Test: 0.5194\n",
            "Early stopping:  0.013150058011357182\n",
            "Epoch: 092, Loss: 1.2487, Train: 0.5843, Test: 0.5237\n",
            "Early stopping:  0.010947879519091399\n",
            "Epoch: 093, Loss: 1.2455, Train: 0.5879, Test: 0.5214\n",
            "Early stopping:  0.009021143701215784\n",
            "PREDICTIONS -> tensor([ 9,  5,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.47      0.58      0.52       379\n",
            "         capital_goods       0.37      0.13      0.19       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.40      0.40      0.40       396\n",
            " consumer_non-cyclical       0.49      0.43      0.46       223\n",
            "                energy       0.52      0.43      0.47       141\n",
            "             financial       0.65      0.61      0.63       384\n",
            "            healthcare       0.81      0.27      0.41       159\n",
            "              services       0.54      0.71      0.62      1038\n",
            "            technology       0.44      0.30      0.36       198\n",
            "        transportation       0.60      0.63      0.61       202\n",
            "             utilities       0.50      0.62      0.55       113\n",
            "\n",
            "              accuracy                           0.52      3527\n",
            "             macro avg       0.48      0.42      0.43      3527\n",
            "          weighted avg       0.52      0.52      0.50      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 65.2301, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 302.8269, Train: 0.0652, Test: 0.0695\n",
            "Early stopping:  168.00629892159182\n",
            "Epoch: 003, Loss: 339.3593, Train: 0.0539, Test: 0.0482\n",
            "Early stopping:  148.84761004797946\n",
            "Epoch: 004, Loss: 292.4200, Train: 0.0819, Test: 0.0777\n",
            "Early stopping:  124.78665663535652\n",
            "Epoch: 005, Loss: 223.5701, Train: 0.1556, Test: 0.1542\n",
            "Early stopping:  108.7108918113971\n",
            "Epoch: 006, Loss: 252.2322, Train: 0.1093, Test: 0.1103\n",
            "Early stopping:  45.09037843862528\n",
            "Epoch: 007, Loss: 238.1584, Train: 0.3004, Test: 0.2881\n",
            "Early stopping:  46.89098679124072\n",
            "Epoch: 008, Loss: 164.7992, Train: 0.3096, Test: 0.3122\n",
            "Early stopping:  46.529160841826766\n",
            "Epoch: 009, Loss: 190.4897, Train: 0.2974, Test: 0.2952\n",
            "Early stopping:  35.74300510440405\n",
            "Epoch: 010, Loss: 173.9310, Train: 0.3089, Test: 0.3059\n",
            "Early stopping:  39.103961020631495\n",
            "Epoch: 011, Loss: 130.1017, Train: 0.2076, Test: 0.2002\n",
            "Early stopping:  39.52497829939618\n",
            "Epoch: 012, Loss: 99.5379, Train: 0.1282, Test: 0.1262\n",
            "Early stopping:  36.59849780726045\n",
            "Epoch: 013, Loss: 90.8210, Train: 0.1289, Test: 0.1236\n",
            "Early stopping:  44.18298417638529\n",
            "Epoch: 014, Loss: 89.5305, Train: 0.1210, Test: 0.1194\n",
            "Early stopping:  35.907527737384086\n",
            "Epoch: 015, Loss: 61.0917, Train: 0.2418, Test: 0.2308\n",
            "Early stopping:  24.73261893098762\n",
            "Epoch: 016, Loss: 35.1878, Train: 0.3214, Test: 0.3158\n",
            "Early stopping:  26.653938674105415\n",
            "Epoch: 017, Loss: 21.9431, Train: 0.2250, Test: 0.2121\n",
            "Early stopping:  31.171898588602957\n",
            "Epoch: 018, Loss: 18.3052, Train: 0.2408, Test: 0.2359\n",
            "Early stopping:  29.925734095513064\n",
            "Epoch: 019, Loss: 11.7372, Train: 0.2936, Test: 0.2898\n",
            "Early stopping:  19.546178077358697\n",
            "Epoch: 020, Loss: 6.7354, Train: 0.2465, Test: 0.2492\n",
            "Early stopping:  10.887134071600622\n",
            "Epoch: 021, Loss: 4.6811, Train: 0.2291, Test: 0.2277\n",
            "Early stopping:  7.376593021942093\n",
            "Epoch: 022, Loss: 3.8603, Train: 0.2495, Test: 0.2543\n",
            "Early stopping:  6.0053310064360295\n",
            "Epoch: 023, Loss: 3.2428, Train: 0.2900, Test: 0.2702\n",
            "Early stopping:  3.441056953786091\n",
            "Epoch: 024, Loss: 2.8727, Train: 0.3108, Test: 0.3011\n",
            "Early stopping:  1.5347952674260257\n",
            "Epoch: 025, Loss: 2.6491, Train: 0.3353, Test: 0.3272\n",
            "Early stopping:  0.8217640726347631\n",
            "Epoch: 026, Loss: 2.4949, Train: 0.3473, Test: 0.3445\n",
            "Early stopping:  0.5456148409586927\n",
            "Epoch: 027, Loss: 2.3798, Train: 0.3497, Test: 0.3482\n",
            "Early stopping:  0.34194569235247785\n",
            "Epoch: 028, Loss: 2.2866, Train: 0.3541, Test: 0.3527\n",
            "Early stopping:  0.23151463467999026\n",
            "Epoch: 029, Loss: 2.2155, Train: 0.3535, Test: 0.3561\n",
            "Early stopping:  0.1718987847439833\n",
            "Epoch: 030, Loss: 2.1691, Train: 0.3505, Test: 0.3519\n",
            "Early stopping:  0.13073532439929003\n",
            "Epoch: 031, Loss: 2.1455, Train: 0.3469, Test: 0.3442\n",
            "Early stopping:  0.09523199059870441\n",
            "Epoch: 032, Loss: 2.1377, Train: 0.3450, Test: 0.3468\n",
            "Early stopping:  0.06151503782778146\n",
            "Epoch: 033, Loss: 2.1322, Train: 0.3423, Test: 0.3408\n",
            "Early stopping:  0.03407850245314956\n",
            "Epoch: 034, Loss: 2.1240, Train: 0.3433, Test: 0.3436\n",
            "Early stopping:  0.017212279872377417\n",
            "Epoch: 035, Loss: 2.1109, Train: 0.3476, Test: 0.3504\n",
            "Early stopping:  0.013290995338837163\n",
            "Epoch: 036, Loss: 2.0934, Train: 0.3558, Test: 0.3581\n",
            "Early stopping:  0.017823878438945163\n",
            "Epoch: 037, Loss: 2.0769, Train: 0.3656, Test: 0.3572\n",
            "Early stopping:  0.022517775483721605\n",
            "Epoch: 038, Loss: 2.0781, Train: 0.3732, Test: 0.3672\n",
            "Early stopping:  0.02055967870581685\n",
            "Epoch: 039, Loss: 2.0467, Train: 0.3777, Test: 0.3692\n",
            "Early stopping:  0.023716506676651844\n",
            "Epoch: 040, Loss: 2.0245, Train: 0.3788, Test: 0.3740\n",
            "Early stopping:  0.027808699713160925\n",
            "Epoch: 041, Loss: 2.0067, Train: 0.3853, Test: 0.3850\n",
            "Early stopping:  0.03160240151979638\n",
            "Epoch: 042, Loss: 1.9872, Train: 0.3894, Test: 0.3901\n",
            "Early stopping:  0.035307887962065\n",
            "Epoch: 043, Loss: 1.9652, Train: 0.3922, Test: 0.3989\n",
            "Early stopping:  0.031664699559143666\n",
            "Epoch: 044, Loss: 1.9430, Train: 0.3947, Test: 0.4054\n",
            "Early stopping:  0.03236578554476756\n",
            "Epoch: 045, Loss: 1.9255, Train: 0.3974, Test: 0.4012\n",
            "Early stopping:  0.03270396673433124\n",
            "Epoch: 046, Loss: 1.9167, Train: 0.4038, Test: 0.4035\n",
            "Early stopping:  0.02892109031265728\n",
            "Epoch: 047, Loss: 1.9015, Train: 0.4115, Test: 0.4080\n",
            "Early stopping:  0.02459097974320981\n",
            "Epoch: 048, Loss: 1.8787, Train: 0.4174, Test: 0.4154\n",
            "Early stopping:  0.02440257657820037\n",
            "Epoch: 049, Loss: 1.8596, Train: 0.4223, Test: 0.4191\n",
            "Early stopping:  0.02718404052862034\n",
            "Epoch: 050, Loss: 1.8474, Train: 0.4267, Test: 0.4199\n",
            "Early stopping:  0.028649193559084705\n",
            "Epoch: 051, Loss: 1.8358, Train: 0.4295, Test: 0.4168\n",
            "Early stopping:  0.0260096313640881\n",
            "Epoch: 052, Loss: 1.8220, Train: 0.4336, Test: 0.4174\n",
            "Early stopping:  0.02178780855836196\n",
            "Epoch: 053, Loss: 1.8067, Train: 0.4325, Test: 0.4225\n",
            "Early stopping:  0.02078754788491321\n",
            "Epoch: 054, Loss: 1.7930, Train: 0.4389, Test: 0.4208\n",
            "Early stopping:  0.021845517317360882\n",
            "Epoch: 055, Loss: 1.7777, Train: 0.4399, Test: 0.4244\n",
            "Early stopping:  0.022948223783490283\n",
            "Epoch: 056, Loss: 1.7600, Train: 0.4444, Test: 0.4276\n",
            "Early stopping:  0.02418892148006079\n",
            "Epoch: 057, Loss: 1.7455, Train: 0.4484, Test: 0.4267\n",
            "Early stopping:  0.024588992707168987\n",
            "Epoch: 058, Loss: 1.7321, Train: 0.4501, Test: 0.4293\n",
            "Early stopping:  0.024396027290944897\n",
            "Epoch: 059, Loss: 1.7190, Train: 0.4518, Test: 0.4369\n",
            "Early stopping:  0.023044457459427948\n",
            "Epoch: 060, Loss: 1.7076, Train: 0.4537, Test: 0.4372\n",
            "Early stopping:  0.020784629940878343\n",
            "Epoch: 061, Loss: 1.6974, Train: 0.4575, Test: 0.4392\n",
            "Early stopping:  0.019110458590564323\n",
            "Epoch: 062, Loss: 1.6851, Train: 0.4580, Test: 0.4440\n",
            "Early stopping:  0.018271586954560378\n",
            "Epoch: 063, Loss: 1.6735, Train: 0.4633, Test: 0.4451\n",
            "Early stopping:  0.017948379877492312\n",
            "Epoch: 064, Loss: 1.6627, Train: 0.4673, Test: 0.4488\n",
            "Early stopping:  0.01798639920038394\n",
            "Epoch: 065, Loss: 1.6507, Train: 0.4701, Test: 0.4514\n",
            "Early stopping:  0.018298347552454127\n",
            "Epoch: 066, Loss: 1.6396, Train: 0.4707, Test: 0.4553\n",
            "Early stopping:  0.017998648113310933\n",
            "Epoch: 067, Loss: 1.6281, Train: 0.4735, Test: 0.4570\n",
            "Early stopping:  0.018006013064936918\n",
            "Epoch: 068, Loss: 1.6178, Train: 0.4773, Test: 0.4604\n",
            "Early stopping:  0.01778237654044605\n",
            "Epoch: 069, Loss: 1.6067, Train: 0.4813, Test: 0.4656\n",
            "Early stopping:  0.017373481171088053\n",
            "Epoch: 070, Loss: 1.5954, Train: 0.4826, Test: 0.4684\n",
            "Early stopping:  0.017360961805905382\n",
            "Epoch: 071, Loss: 1.5858, Train: 0.4860, Test: 0.4704\n",
            "Early stopping:  0.016930034996922198\n",
            "Epoch: 072, Loss: 1.5759, Train: 0.4896, Test: 0.4718\n",
            "Early stopping:  0.016584881083268208\n",
            "Epoch: 073, Loss: 1.5656, Train: 0.4887, Test: 0.4741\n",
            "Early stopping:  0.016081082824820382\n",
            "Epoch: 074, Loss: 1.5553, Train: 0.4909, Test: 0.4760\n",
            "Early stopping:  0.015870273329521532\n",
            "Epoch: 075, Loss: 1.5466, Train: 0.4951, Test: 0.4786\n",
            "Early stopping:  0.015663676794741163\n",
            "Epoch: 076, Loss: 1.5358, Train: 0.4998, Test: 0.4772\n",
            "Early stopping:  0.015694372038363626\n",
            "Epoch: 077, Loss: 1.5254, Train: 0.5042, Test: 0.4769\n",
            "Early stopping:  0.01581570130629608\n",
            "Epoch: 078, Loss: 1.5162, Train: 0.5079, Test: 0.4800\n",
            "Early stopping:  0.015699799431126926\n",
            "Epoch: 079, Loss: 1.5048, Train: 0.5117, Test: 0.4811\n",
            "Early stopping:  0.016305486313478678\n",
            "Epoch: 080, Loss: 1.4965, Train: 0.5130, Test: 0.4862\n",
            "Early stopping:  0.015687863537950535\n",
            "Epoch: 081, Loss: 1.4880, Train: 0.5155, Test: 0.4897\n",
            "Early stopping:  0.014963131573555064\n",
            "Epoch: 082, Loss: 1.4793, Train: 0.5189, Test: 0.4925\n",
            "Early stopping:  0.014354457234513786\n",
            "Epoch: 083, Loss: 1.4713, Train: 0.5200, Test: 0.4928\n",
            "Early stopping:  0.013326666753972321\n",
            "Epoch: 084, Loss: 1.4625, Train: 0.5233, Test: 0.4962\n",
            "Early stopping:  0.013404716095665566\n",
            "Epoch: 085, Loss: 1.4527, Train: 0.5257, Test: 0.5004\n",
            "Early stopping:  0.013842080179443971\n",
            "Epoch: 086, Loss: 1.4430, Train: 0.5282, Test: 0.5024\n",
            "Early stopping:  0.01443808465218587\n",
            "Epoch: 087, Loss: 1.4345, Train: 0.5293, Test: 0.5027\n",
            "Early stopping:  0.014722260766508883\n",
            "Epoch: 088, Loss: 1.4251, Train: 0.5329, Test: 0.5041\n",
            "Early stopping:  0.014714691989030328\n",
            "Epoch: 089, Loss: 1.4178, Train: 0.5342, Test: 0.5081\n",
            "Early stopping:  0.013857552265407552\n",
            "Epoch: 090, Loss: 1.4085, Train: 0.5393, Test: 0.5067\n",
            "Early stopping:  0.013562875653523378\n",
            "Epoch: 091, Loss: 1.4002, Train: 0.5414, Test: 0.5118\n",
            "Early stopping:  0.013486798659550783\n",
            "Epoch: 092, Loss: 1.3938, Train: 0.5427, Test: 0.5152\n",
            "Early stopping:  0.012725013832300541\n",
            "Epoch: 093, Loss: 1.3844, Train: 0.5457, Test: 0.5160\n",
            "Early stopping:  0.012907260646281313\n",
            "Epoch: 094, Loss: 1.3756, Train: 0.5473, Test: 0.5140\n",
            "Early stopping:  0.012914737282269486\n",
            "Epoch: 095, Loss: 1.3694, Train: 0.5516, Test: 0.5189\n",
            "Early stopping:  0.012634435531498317\n",
            "Epoch: 096, Loss: 1.3598, Train: 0.5527, Test: 0.5194\n",
            "Early stopping:  0.013128038565097899\n",
            "Epoch: 097, Loss: 1.3512, Train: 0.5558, Test: 0.5194\n",
            "Early stopping:  0.01300863416839692\n",
            "Epoch: 098, Loss: 1.3431, Train: 0.5554, Test: 0.5237\n",
            "Early stopping:  0.013154150341616733\n",
            "Epoch: 099, Loss: 1.3375, Train: 0.5616, Test: 0.5254\n",
            "Early stopping:  0.012800939699758795\n",
            "Epoch: 100, Loss: 1.3308, Train: 0.5643, Test: 0.5299\n",
            "Early stopping:  0.011403056059677816\n",
            "Epoch: 101, Loss: 1.3206, Train: 0.5658, Test: 0.5367\n",
            "Early stopping:  0.011684354808750382\n",
            "Epoch: 102, Loss: 1.3142, Train: 0.5686, Test: 0.5302\n",
            "Early stopping:  0.011872285087500985\n",
            "Epoch: 103, Loss: 1.3109, Train: 0.5715, Test: 0.5393\n",
            "Early stopping:  0.0111849983776819\n",
            "Epoch: 104, Loss: 1.3054, Train: 0.5784, Test: 0.5430\n",
            "Early stopping:  0.00976843475710296\n",
            "PREDICTIONS -> tensor([ 9, 10,  1,  ...,  0,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.43      0.56      0.48       379\n",
            "         capital_goods       0.50      0.26      0.35       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.50      0.60      0.55       396\n",
            " consumer_non-cyclical       0.77      0.35      0.48       223\n",
            "                energy       0.70      0.23      0.34       141\n",
            "             financial       0.70      0.61      0.65       384\n",
            "            healthcare       0.75      0.52      0.61       159\n",
            "              services       0.54      0.77      0.63      1038\n",
            "            technology       0.48      0.21      0.29       198\n",
            "        transportation       0.49      0.64      0.56       202\n",
            "             utilities       1.00      0.04      0.07       113\n",
            "\n",
            "              accuracy                           0.54      3527\n",
            "             macro avg       0.57      0.40      0.42      3527\n",
            "          weighted avg       0.57      0.54      0.52      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 43.7581, Train: 0.1796, Test: 0.1914\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 186.8997, Train: 0.0839, Test: 0.0836\n",
            "Early stopping:  101.21639173995004\n",
            "Epoch: 003, Loss: 513.6397, Train: 0.1389, Test: 0.1330\n",
            "Early stopping:  240.84476880377045\n",
            "Epoch: 004, Loss: 633.9034, Train: 0.1070, Test: 0.1038\n",
            "Early stopping:  275.4669115415788\n",
            "Epoch: 005, Loss: 744.9307, Train: 0.0457, Test: 0.0451\n",
            "Early stopping:  298.28243507573126\n",
            "Epoch: 006, Loss: 634.0963, Train: 0.0603, Test: 0.0601\n",
            "Early stopping:  215.06214360686332\n",
            "Epoch: 007, Loss: 620.6777, Train: 0.1953, Test: 0.2070\n",
            "Early stopping:  81.95453182325748\n",
            "Epoch: 008, Loss: 350.1459, Train: 0.3064, Test: 0.3048\n",
            "Early stopping:  146.72995937522177\n",
            "Epoch: 009, Loss: 386.7044, Train: 0.3053, Test: 0.3051\n",
            "Early stopping:  170.76038098574043\n",
            "Epoch: 010, Loss: 358.1747, Train: 0.2682, Test: 0.2603\n",
            "Early stopping:  144.42939406169438\n",
            "Epoch: 011, Loss: 287.2710, Train: 0.2361, Test: 0.2308\n",
            "Early stopping:  128.27340271909938\n",
            "Epoch: 012, Loss: 213.0922, Train: 0.1401, Test: 0.1352\n",
            "Early stopping:  69.48338593102145\n",
            "Epoch: 013, Loss: 170.9301, Train: 0.1342, Test: 0.1273\n",
            "Early stopping:  92.0197556158455\n",
            "Epoch: 014, Loss: 126.2978, Train: 0.1898, Test: 0.1846\n",
            "Early stopping:  92.50853605758356\n",
            "Epoch: 015, Loss: 80.9907, Train: 0.1896, Test: 0.1783\n",
            "Early stopping:  79.49754615162043\n",
            "Epoch: 016, Loss: 60.5795, Train: 0.2280, Test: 0.2243\n",
            "Early stopping:  62.876264914672035\n",
            "Epoch: 017, Loss: 58.1438, Train: 0.2064, Test: 0.1942\n",
            "Early stopping:  48.44598857345686\n",
            "Epoch: 018, Loss: 49.9439, Train: 0.2618, Test: 0.2470\n",
            "Early stopping:  30.769541322617453\n",
            "Epoch: 019, Loss: 27.3030, Train: 0.2639, Test: 0.2662\n",
            "Early stopping:  19.41938069922154\n",
            "Epoch: 020, Loss: 14.8620, Train: 0.2645, Test: 0.2676\n",
            "Early stopping:  20.13244963392755\n",
            "Epoch: 021, Loss: 9.2377, Train: 0.1979, Test: 0.1999\n",
            "Early stopping:  21.443680461374992\n",
            "Epoch: 022, Loss: 7.3496, Train: 0.1563, Test: 0.1525\n",
            "Early stopping:  17.586253300315818\n",
            "Epoch: 023, Loss: 6.2352, Train: 0.1694, Test: 0.1687\n",
            "Early stopping:  8.658625291786734\n",
            "Epoch: 024, Loss: 5.0409, Train: 0.2414, Test: 0.2390\n",
            "Early stopping:  3.8544648619793085\n",
            "Epoch: 025, Loss: 4.0632, Train: 0.3055, Test: 0.2903\n",
            "Early stopping:  2.017935951101335\n",
            "Epoch: 026, Loss: 3.5024, Train: 0.2992, Test: 0.2866\n",
            "Early stopping:  1.5719780683122582\n",
            "Epoch: 027, Loss: 3.1403, Train: 0.2989, Test: 0.2858\n",
            "Early stopping:  1.253439236786089\n",
            "Epoch: 028, Loss: 2.8438, Train: 0.3042, Test: 0.2932\n",
            "Early stopping:  0.8680110477755848\n",
            "Epoch: 029, Loss: 2.5916, Train: 0.3034, Test: 0.2971\n",
            "Early stopping:  0.5773116054267252\n",
            "Epoch: 030, Loss: 2.4012, Train: 0.3057, Test: 0.3000\n",
            "Early stopping:  0.43808222919249507\n",
            "Epoch: 031, Loss: 2.2806, Train: 0.3053, Test: 0.2929\n",
            "Early stopping:  0.3463090191350663\n",
            "Epoch: 032, Loss: 2.2138, Train: 0.3321, Test: 0.3215\n",
            "Early stopping:  0.2552599514528167\n",
            "Epoch: 033, Loss: 2.1854, Train: 0.3350, Test: 0.3297\n",
            "Early stopping:  0.16602257074086205\n",
            "Epoch: 034, Loss: 2.1738, Train: 0.3397, Test: 0.3340\n",
            "Early stopping:  0.09363285739547966\n",
            "Epoch: 035, Loss: 2.1665, Train: 0.3433, Test: 0.3357\n",
            "Early stopping:  0.04645073379097844\n",
            "Epoch: 036, Loss: 2.1566, Train: 0.3510, Test: 0.3405\n",
            "Early stopping:  0.022017847075088148\n",
            "Epoch: 037, Loss: 2.1446, Train: 0.3558, Test: 0.3422\n",
            "Early stopping:  0.015671930932860147\n",
            "Epoch: 038, Loss: 2.1314, Train: 0.3586, Test: 0.3473\n",
            "Early stopping:  0.016991161866854804\n",
            "Epoch: 039, Loss: 2.1181, Train: 0.3616, Test: 0.3476\n",
            "Early stopping:  0.019340093116533962\n",
            "Epoch: 040, Loss: 2.1049, Train: 0.3609, Test: 0.3482\n",
            "Early stopping:  0.02053251725024837\n",
            "Epoch: 041, Loss: 2.0922, Train: 0.3605, Test: 0.3485\n",
            "Early stopping:  0.020753750550128233\n",
            "Epoch: 042, Loss: 2.0803, Train: 0.3584, Test: 0.3504\n",
            "Early stopping:  0.020252419093112765\n",
            "Epoch: 043, Loss: 2.0698, Train: 0.3595, Test: 0.3502\n",
            "Early stopping:  0.019184105532332567\n",
            "Epoch: 044, Loss: 2.0608, Train: 0.3594, Test: 0.3538\n",
            "Early stopping:  0.017531341262193387\n",
            "Epoch: 045, Loss: 2.0519, Train: 0.3605, Test: 0.3533\n",
            "Early stopping:  0.01584073044354476\n",
            "Epoch: 046, Loss: 2.0422, Train: 0.3601, Test: 0.3550\n",
            "Early stopping:  0.014862585730475416\n",
            "Epoch: 047, Loss: 2.0314, Train: 0.3616, Test: 0.3561\n",
            "Early stopping:  0.015103677768938804\n",
            "Epoch: 048, Loss: 2.0184, Train: 0.3629, Test: 0.3578\n",
            "Early stopping:  0.016700473685953614\n",
            "Epoch: 049, Loss: 2.0039, Train: 0.3637, Test: 0.3570\n",
            "Early stopping:  0.019019287598002265\n",
            "Epoch: 050, Loss: 1.9880, Train: 0.3629, Test: 0.3575\n",
            "Early stopping:  0.02155617575830962\n",
            "Epoch: 051, Loss: 1.9702, Train: 0.3643, Test: 0.3570\n",
            "Early stopping:  0.024188743036424157\n",
            "Epoch: 052, Loss: 1.9548, Train: 0.3660, Test: 0.3581\n",
            "Early stopping:  0.025457604547251555\n",
            "Epoch: 053, Loss: 1.9404, Train: 0.3677, Test: 0.3606\n",
            "Early stopping:  0.025349839769552983\n",
            "Epoch: 054, Loss: 1.9253, Train: 0.3701, Test: 0.3626\n",
            "Early stopping:  0.0245669337266752\n",
            "Epoch: 055, Loss: 1.9138, Train: 0.3726, Test: 0.3666\n",
            "Early stopping:  0.02253451492094288\n",
            "Epoch: 056, Loss: 1.9008, Train: 0.3741, Test: 0.3663\n",
            "Early stopping:  0.021294715834271838\n",
            "Epoch: 057, Loss: 1.8874, Train: 0.3739, Test: 0.3666\n",
            "Early stopping:  0.020626930175934942\n",
            "Epoch: 058, Loss: 1.8754, Train: 0.3800, Test: 0.3709\n",
            "Early stopping:  0.01994634266396778\n",
            "Epoch: 059, Loss: 1.8608, Train: 0.3853, Test: 0.3743\n",
            "Early stopping:  0.020807267747015506\n",
            "Epoch: 060, Loss: 1.8460, Train: 0.3862, Test: 0.3774\n",
            "Early stopping:  0.0215659896545101\n",
            "Epoch: 061, Loss: 1.8324, Train: 0.3877, Test: 0.3785\n",
            "Early stopping:  0.022063245140198914\n",
            "Epoch: 062, Loss: 1.8198, Train: 0.3919, Test: 0.3833\n",
            "Early stopping:  0.02207386950941783\n",
            "Epoch: 063, Loss: 1.8073, Train: 0.3960, Test: 0.3850\n",
            "Early stopping:  0.021056961744049776\n",
            "Epoch: 064, Loss: 1.7956, Train: 0.3977, Test: 0.3870\n",
            "Early stopping:  0.019934496479623972\n",
            "Epoch: 065, Loss: 1.7833, Train: 0.4002, Test: 0.3884\n",
            "Early stopping:  0.019360342396751065\n",
            "Epoch: 066, Loss: 1.7718, Train: 0.4051, Test: 0.3890\n",
            "Early stopping:  0.018973836916948882\n",
            "Epoch: 067, Loss: 1.7604, Train: 0.4078, Test: 0.3907\n",
            "Early stopping:  0.018583703206074782\n",
            "Epoch: 068, Loss: 1.7496, Train: 0.4115, Test: 0.3947\n",
            "Early stopping:  0.01816599970759112\n",
            "Epoch: 069, Loss: 1.7399, Train: 0.4144, Test: 0.3938\n",
            "Early stopping:  0.01727173833272842\n",
            "Epoch: 070, Loss: 1.7302, Train: 0.4168, Test: 0.3944\n",
            "Early stopping:  0.016418103215611324\n",
            "Epoch: 071, Loss: 1.7204, Train: 0.4161, Test: 0.3967\n",
            "Early stopping:  0.01570109297235309\n",
            "Epoch: 072, Loss: 1.7105, Train: 0.4183, Test: 0.3989\n",
            "Early stopping:  0.015430852661632506\n",
            "Epoch: 073, Loss: 1.7007, Train: 0.4193, Test: 0.3986\n",
            "Early stopping:  0.015501205873762221\n",
            "Epoch: 074, Loss: 1.6914, Train: 0.4208, Test: 0.4003\n",
            "Early stopping:  0.015404681264298788\n",
            "Epoch: 075, Loss: 1.6827, Train: 0.4259, Test: 0.4037\n",
            "Early stopping:  0.01496461168594482\n",
            "Epoch: 076, Loss: 1.6740, Train: 0.4289, Test: 0.4063\n",
            "Early stopping:  0.014411077981766034\n",
            "Epoch: 077, Loss: 1.6647, Train: 0.4308, Test: 0.4063\n",
            "Early stopping:  0.014131521904650288\n",
            "Epoch: 078, Loss: 1.6569, Train: 0.4304, Test: 0.4074\n",
            "Early stopping:  0.013738328422751755\n",
            "Epoch: 079, Loss: 1.6472, Train: 0.4325, Test: 0.4088\n",
            "Early stopping:  0.013910296749250201\n",
            "Epoch: 080, Loss: 1.6376, Train: 0.4342, Test: 0.4097\n",
            "Early stopping:  0.014286551573792262\n",
            "Epoch: 081, Loss: 1.6291, Train: 0.4372, Test: 0.4108\n",
            "Early stopping:  0.014345228639559596\n",
            "Epoch: 082, Loss: 1.6200, Train: 0.4389, Test: 0.4148\n",
            "Early stopping:  0.01455977094908556\n",
            "Epoch: 083, Loss: 1.6111, Train: 0.4431, Test: 0.4185\n",
            "Early stopping:  0.014194685971261101\n",
            "Epoch: 084, Loss: 1.6022, Train: 0.4423, Test: 0.4202\n",
            "Early stopping:  0.01401776253977622\n",
            "Epoch: 085, Loss: 1.5938, Train: 0.4465, Test: 0.4225\n",
            "Early stopping:  0.013963319297876605\n",
            "Epoch: 086, Loss: 1.5841, Train: 0.4484, Test: 0.4267\n",
            "Early stopping:  0.014074852943095411\n",
            "Epoch: 087, Loss: 1.5750, Train: 0.4509, Test: 0.4273\n",
            "Early stopping:  0.014294457254592824\n",
            "Epoch: 088, Loss: 1.5655, Train: 0.4565, Test: 0.4293\n",
            "Early stopping:  0.01459801332715997\n",
            "Epoch: 089, Loss: 1.5559, Train: 0.4626, Test: 0.4327\n",
            "Early stopping:  0.014935103240540643\n",
            "Epoch: 090, Loss: 1.5468, Train: 0.4684, Test: 0.4369\n",
            "Early stopping:  0.014811278851362278\n",
            "Epoch: 091, Loss: 1.5376, Train: 0.4758, Test: 0.4417\n",
            "Early stopping:  0.014766411155559269\n",
            "Epoch: 092, Loss: 1.5285, Train: 0.4839, Test: 0.4480\n",
            "Early stopping:  0.01460548878320363\n",
            "Epoch: 093, Loss: 1.5181, Train: 0.4888, Test: 0.4519\n",
            "Early stopping:  0.014881595158219931\n",
            "Epoch: 094, Loss: 1.5075, Train: 0.4955, Test: 0.4585\n",
            "Early stopping:  0.015549041285935379\n",
            "Epoch: 095, Loss: 1.4974, Train: 0.5006, Test: 0.4675\n",
            "Early stopping:  0.016027421159411148\n",
            "Epoch: 096, Loss: 1.4858, Train: 0.5028, Test: 0.4698\n",
            "Early stopping:  0.01677005199997789\n",
            "Epoch: 097, Loss: 1.4752, Train: 0.5134, Test: 0.4794\n",
            "Early stopping:  0.016983377333589532\n",
            "Epoch: 098, Loss: 1.4657, Train: 0.5161, Test: 0.4777\n",
            "Early stopping:  0.016734251479003193\n",
            "Epoch: 099, Loss: 1.4516, Train: 0.5178, Test: 0.4758\n",
            "Early stopping:  0.01771521302374471\n",
            "Epoch: 100, Loss: 1.4492, Train: 0.5274, Test: 0.4877\n",
            "Early stopping:  0.015525842815873835\n",
            "Epoch: 101, Loss: 1.4376, Train: 0.5314, Test: 0.4911\n",
            "Early stopping:  0.014729142768009613\n",
            "Epoch: 102, Loss: 1.4316, Train: 0.5352, Test: 0.4902\n",
            "Early stopping:  0.013221155150051817\n",
            "Epoch: 103, Loss: 1.4099, Train: 0.5333, Test: 0.4885\n",
            "Early stopping:  0.016732086077971334\n",
            "Epoch: 104, Loss: 1.4100, Train: 0.5442, Test: 0.5018\n",
            "Early stopping:  0.017366260540602706\n",
            "Epoch: 105, Loss: 1.3914, Train: 0.5480, Test: 0.5055\n",
            "Early stopping:  0.01863491633115136\n",
            "Epoch: 106, Loss: 1.3822, Train: 0.5493, Test: 0.5075\n",
            "Early stopping:  0.019126108672777347\n",
            "Epoch: 107, Loss: 1.3746, Train: 0.5554, Test: 0.5118\n",
            "Early stopping:  0.01605258529630605\n",
            "Epoch: 108, Loss: 1.3622, Train: 0.5575, Test: 0.5064\n",
            "Early stopping:  0.018015861988334235\n",
            "Epoch: 109, Loss: 1.3531, Train: 0.5614, Test: 0.5109\n",
            "Early stopping:  0.015307806816644983\n",
            "Epoch: 110, Loss: 1.3471, Train: 0.5648, Test: 0.5129\n",
            "Early stopping:  0.014578306037641422\n",
            "Epoch: 111, Loss: 1.3307, Train: 0.5688, Test: 0.5157\n",
            "Early stopping:  0.016452632405483853\n",
            "Epoch: 112, Loss: 1.3248, Train: 0.5715, Test: 0.5169\n",
            "Early stopping:  0.015568943612510263\n",
            "Epoch: 113, Loss: 1.3038, Train: 0.5709, Test: 0.5200\n",
            "Early stopping:  0.019507314919896705\n",
            "Epoch: 114, Loss: 1.3062, Train: 0.5760, Test: 0.5265\n",
            "Early stopping:  0.01796778828657153\n",
            "Epoch: 115, Loss: 1.2848, Train: 0.5750, Test: 0.5282\n",
            "Early stopping:  0.01823912951505105\n",
            "Epoch: 116, Loss: 1.2865, Train: 0.5798, Test: 0.5316\n",
            "Early stopping:  0.016360366718612025\n",
            "Epoch: 117, Loss: 1.2702, Train: 0.5807, Test: 0.5291\n",
            "Early stopping:  0.01485741944117632\n",
            "Epoch: 118, Loss: 1.2698, Train: 0.5817, Test: 0.5305\n",
            "Early stopping:  0.014931618977672086\n",
            "Epoch: 119, Loss: 1.2540, Train: 0.5849, Test: 0.5359\n",
            "Early stopping:  0.013264346955689873\n",
            "Epoch: 120, Loss: 1.2509, Train: 0.5851, Test: 0.5336\n",
            "Early stopping:  0.01436137471926012\n",
            "Epoch: 121, Loss: 1.2420, Train: 0.5870, Test: 0.5327\n",
            "Early stopping:  0.012336861347434986\n",
            "Epoch: 122, Loss: 1.2330, Train: 0.5885, Test: 0.5353\n",
            "Early stopping:  0.013785612022529198\n",
            "Epoch: 123, Loss: 1.2285, Train: 0.5873, Test: 0.5373\n",
            "Early stopping:  0.011010955913153979\n",
            "Epoch: 124, Loss: 1.2183, Train: 0.5881, Test: 0.5387\n",
            "Early stopping:  0.01250409255274846\n",
            "Epoch: 125, Loss: 1.2132, Train: 0.5924, Test: 0.5379\n",
            "Early stopping:  0.011504078199483276\n",
            "Epoch: 126, Loss: 1.2061, Train: 0.5957, Test: 0.5415\n",
            "Early stopping:  0.010994997757440039\n",
            "Epoch: 127, Loss: 1.2006, Train: 0.5996, Test: 0.5435\n",
            "Early stopping:  0.010825778034711927\n",
            "Epoch: 128, Loss: 1.1904, Train: 0.6004, Test: 0.5455\n",
            "Early stopping:  0.01088704821154858\n",
            "Epoch: 129, Loss: 1.1893, Train: 0.6051, Test: 0.5475\n",
            "Early stopping:  0.010200275841468417\n",
            "Epoch: 130, Loss: 1.1794, Train: 0.6078, Test: 0.5506\n",
            "Early stopping:  0.01043932540757844\n",
            "Epoch: 131, Loss: 1.1737, Train: 0.6098, Test: 0.5566\n",
            "Early stopping:  0.010463595575364148\n",
            "Epoch: 132, Loss: 1.1673, Train: 0.6132, Test: 0.5560\n",
            "Early stopping:  0.00996550320567229\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.42      0.55      0.48       379\n",
            "         capital_goods       0.39      0.25      0.31       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.57      0.56      0.56       396\n",
            " consumer_non-cyclical       0.73      0.43      0.54       223\n",
            "                energy       0.61      0.22      0.32       141\n",
            "             financial       0.74      0.58      0.65       384\n",
            "            healthcare       0.62      0.53      0.58       159\n",
            "              services       0.54      0.77      0.64      1038\n",
            "            technology       0.50      0.16      0.24       198\n",
            "        transportation       0.67      0.61      0.64       202\n",
            "             utilities       0.56      0.65      0.60       113\n",
            "\n",
            "              accuracy                           0.56      3527\n",
            "             macro avg       0.53      0.44      0.46      3527\n",
            "          weighted avg       0.56      0.56      0.54      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 141.9694, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 195.8751, Train: 0.1104, Test: 0.1120\n",
            "Early stopping:  38.11709728757467\n",
            "Epoch: 003, Loss: 462.2955, Train: 0.1221, Test: 0.1256\n",
            "Early stopping:  171.5101719540258\n",
            "Epoch: 004, Loss: 373.9397, Train: 0.0904, Test: 0.0970\n",
            "Early stopping:  149.94954671252108\n",
            "Epoch: 005, Loss: 315.6813, Train: 0.0749, Test: 0.0825\n",
            "Early stopping:  130.23776570999408\n",
            "Epoch: 006, Loss: 291.5570, Train: 0.0516, Test: 0.0536\n",
            "Early stopping:  98.84643791596356\n",
            "Epoch: 007, Loss: 299.0990, Train: 0.0856, Test: 0.0842\n",
            "Early stopping:  71.33830408688782\n",
            "Epoch: 008, Loss: 197.4769, Train: 0.0745, Test: 0.0763\n",
            "Early stopping:  63.63387855883763\n",
            "Epoch: 009, Loss: 105.1909, Train: 0.3076, Test: 0.3085\n",
            "Early stopping:  89.22439843616016\n",
            "Epoch: 010, Loss: 89.3109, Train: 0.2858, Test: 0.2949\n",
            "Early stopping:  99.23482120933127\n",
            "Epoch: 011, Loss: 82.7275, Train: 0.2268, Test: 0.2248\n",
            "Early stopping:  92.98904734257182\n",
            "Epoch: 012, Loss: 76.4215, Train: 0.2467, Test: 0.2526\n",
            "Early stopping:  49.93604601063481\n",
            "Epoch: 013, Loss: 66.5339, Train: 0.2314, Test: 0.2331\n",
            "Early stopping:  14.503182292130987\n",
            "Epoch: 014, Loss: 57.9261, Train: 0.2595, Test: 0.2589\n",
            "Early stopping:  12.538557991009844\n",
            "Epoch: 015, Loss: 44.4782, Train: 0.2824, Test: 0.2781\n",
            "Early stopping:  15.135582440786306\n",
            "Epoch: 016, Loss: 31.1973, Train: 0.3102, Test: 0.3107\n",
            "Early stopping:  17.869405935184627\n",
            "Epoch: 017, Loss: 19.0583, Train: 0.3172, Test: 0.3150\n",
            "Early stopping:  19.286089543370736\n",
            "Epoch: 018, Loss: 8.2435, Train: 0.2974, Test: 0.2855\n",
            "Early stopping:  19.749760644764155\n",
            "Epoch: 019, Loss: 3.4007, Train: 0.2509, Test: 0.2461\n",
            "Early stopping:  16.815662664780884\n",
            "Epoch: 020, Loss: 2.9789, Train: 0.2242, Test: 0.2180\n",
            "Early stopping:  12.073068041477475\n",
            "Epoch: 021, Loss: 2.7853, Train: 0.2083, Test: 0.2027\n",
            "Early stopping:  6.953516022665475\n",
            "Epoch: 022, Loss: 2.6299, Train: 0.2019, Test: 0.1976\n",
            "Early stopping:  2.3854493676677446\n",
            "Epoch: 023, Loss: 2.5191, Train: 0.2025, Test: 0.2016\n",
            "Early stopping:  0.3468672465928591\n",
            "Epoch: 024, Loss: 2.4296, Train: 0.2113, Test: 0.2070\n",
            "Early stopping:  0.21845368505723045\n",
            "Epoch: 025, Loss: 2.3690, Train: 0.2216, Test: 0.2138\n",
            "Early stopping:  0.16575046843856778\n",
            "Epoch: 026, Loss: 2.3360, Train: 0.2227, Test: 0.2177\n",
            "Early stopping:  0.11922994320419822\n",
            "Epoch: 027, Loss: 2.3191, Train: 0.2265, Test: 0.2226\n",
            "Early stopping:  0.08139194774780104\n",
            "Epoch: 028, Loss: 2.3116, Train: 0.2278, Test: 0.2263\n",
            "Early stopping:  0.048134386353720825\n",
            "Epoch: 029, Loss: 2.3089, Train: 0.2304, Test: 0.2305\n",
            "Early stopping:  0.024735010108478558\n",
            "Epoch: 030, Loss: 2.3059, Train: 0.2877, Test: 0.2855\n",
            "Early stopping:  0.01205466239313725\n",
            "Epoch: 031, Loss: 2.3011, Train: 0.2919, Test: 0.2858\n",
            "Early stopping:  0.006723663117945581\n",
            "PREDICTIONS -> tensor([3, 8, 8,  ..., 8, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       379\n",
            "         capital_goods       0.14      0.07      0.09       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.24      0.58      0.34       396\n",
            " consumer_non-cyclical       0.64      0.21      0.31       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.47      0.49      0.48       384\n",
            "            healthcare       0.69      0.31      0.43       159\n",
            "              services       0.28      0.35      0.31      1038\n",
            "            technology       1.00      0.01      0.01       198\n",
            "        transportation       0.19      0.53      0.28       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.29      3527\n",
            "             macro avg       0.30      0.21      0.19      3527\n",
            "          weighted avg       0.31      0.29      0.24      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 63.0405, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 453.5551, Train: 0.0335, Test: 0.0357\n",
            "Early stopping:  276.13554797064194\n",
            "Epoch: 003, Loss: 664.6469, Train: 0.1163, Test: 0.1148\n",
            "Early stopping:  305.2298670618461\n",
            "Epoch: 004, Loss: 489.1987, Train: 0.1138, Test: 0.1128\n",
            "Early stopping:  253.74773596308887\n",
            "Epoch: 005, Loss: 406.7010, Train: 0.1113, Test: 0.1117\n",
            "Early stopping:  219.8061368156803\n",
            "Epoch: 006, Loss: 459.8578, Train: 0.1227, Test: 0.1216\n",
            "Early stopping:  99.45207747159454\n",
            "Epoch: 007, Loss: 315.9105, Train: 0.0739, Test: 0.0737\n",
            "Early stopping:  128.5221157241936\n",
            "Epoch: 008, Loss: 289.1974, Train: 0.2890, Test: 0.2864\n",
            "Early stopping:  87.50173597545209\n",
            "Epoch: 009, Loss: 231.7673, Train: 0.2941, Test: 0.2932\n",
            "Early stopping:  91.77155540604575\n",
            "Epoch: 010, Loss: 237.8791, Train: 0.2905, Test: 0.2841\n",
            "Early stopping:  92.46549300162432\n",
            "Epoch: 011, Loss: 202.7789, Train: 0.2794, Test: 0.2753\n",
            "Early stopping:  45.9298313197084\n",
            "Epoch: 012, Loss: 152.0713, Train: 0.1416, Test: 0.1435\n",
            "Early stopping:  50.29859241432676\n",
            "Epoch: 013, Loss: 109.5911, Train: 0.0862, Test: 0.0834\n",
            "Early stopping:  54.89479954120479\n",
            "Epoch: 014, Loss: 80.3836, Train: 0.1242, Test: 0.1171\n",
            "Early stopping:  64.76504024999394\n",
            "Epoch: 015, Loss: 39.8885, Train: 0.1786, Test: 0.1718\n",
            "Early stopping:  63.10669655745715\n",
            "Epoch: 016, Loss: 11.5385, Train: 0.1595, Test: 0.1559\n",
            "Early stopping:  55.58201159256656\n",
            "Epoch: 017, Loss: 5.8774, Train: 0.2045, Test: 0.1956\n",
            "Early stopping:  44.72448431848581\n",
            "Epoch: 018, Loss: 4.0656, Train: 0.2499, Test: 0.2418\n",
            "Early stopping:  32.473107400641055\n",
            "Epoch: 019, Loss: 3.3657, Train: 0.2265, Test: 0.2240\n",
            "Early stopping:  15.398765953272527\n",
            "Epoch: 020, Loss: 3.2735, Train: 0.2034, Test: 0.1993\n",
            "Early stopping:  3.467652734679981\n",
            "Epoch: 021, Loss: 3.2054, Train: 0.1934, Test: 0.1874\n",
            "Early stopping:  1.127123034848254\n",
            "Epoch: 022, Loss: 3.0775, Train: 0.1991, Test: 0.1973\n",
            "Early stopping:  0.38798121581939743\n",
            "Epoch: 023, Loss: 2.8900, Train: 0.2066, Test: 0.2067\n",
            "Early stopping:  0.18501522257999808\n",
            "Epoch: 024, Loss: 2.6967, Train: 0.2134, Test: 0.2070\n",
            "Early stopping:  0.2361069821832193\n",
            "Epoch: 025, Loss: 2.5315, Train: 0.2157, Test: 0.2030\n",
            "Early stopping:  0.27387804156281775\n",
            "Epoch: 026, Loss: 2.4174, Train: 0.2091, Test: 0.1999\n",
            "Early stopping:  0.2666018158639153\n",
            "Epoch: 027, Loss: 2.3490, Train: 0.2074, Test: 0.1988\n",
            "Early stopping:  0.218988859947734\n",
            "Epoch: 028, Loss: 2.3088, Train: 0.2098, Test: 0.2047\n",
            "Early stopping:  0.15661873337311247\n",
            "Epoch: 029, Loss: 2.2758, Train: 0.2225, Test: 0.2177\n",
            "Early stopping:  0.10146023856526194\n",
            "Epoch: 030, Loss: 2.2551, Train: 0.2289, Test: 0.2277\n",
            "Early stopping:  0.06444788401333432\n",
            "Epoch: 031, Loss: 2.2378, Train: 0.2371, Test: 0.2333\n",
            "Early stopping:  0.04433433986884173\n",
            "Epoch: 032, Loss: 2.2199, Train: 0.2478, Test: 0.2455\n",
            "Early stopping:  0.034474077374978845\n",
            "Epoch: 033, Loss: 2.2008, Train: 0.2629, Test: 0.2569\n",
            "Early stopping:  0.02930364563095771\n",
            "Epoch: 034, Loss: 2.1809, Train: 0.2764, Test: 0.2679\n",
            "Early stopping:  0.029328038663978945\n",
            "Epoch: 035, Loss: 2.1607, Train: 0.2947, Test: 0.2889\n",
            "Early stopping:  0.030548353617634787\n",
            "Epoch: 036, Loss: 2.1412, Train: 0.3117, Test: 0.3071\n",
            "Early stopping:  0.03123078212887618\n",
            "Epoch: 037, Loss: 2.1233, Train: 0.3250, Test: 0.3212\n",
            "Early stopping:  0.030801448114917442\n",
            "Epoch: 038, Loss: 2.1082, Train: 0.3350, Test: 0.3272\n",
            "Early stopping:  0.02897015424862714\n",
            "Epoch: 039, Loss: 2.0967, Train: 0.3440, Test: 0.3368\n",
            "Early stopping:  0.025596509674878875\n",
            "Epoch: 040, Loss: 2.0873, Train: 0.3522, Test: 0.3439\n",
            "Early stopping:  0.021422187012097055\n",
            "Epoch: 041, Loss: 2.0776, Train: 0.3594, Test: 0.3490\n",
            "Early stopping:  0.017848691921731673\n",
            "Epoch: 042, Loss: 2.0645, Train: 0.3628, Test: 0.3538\n",
            "Early stopping:  0.01685457073760514\n",
            "Epoch: 043, Loss: 2.0473, Train: 0.3635, Test: 0.3595\n",
            "Early stopping:  0.019415225602509616\n",
            "Epoch: 044, Loss: 2.0277, Train: 0.3648, Test: 0.3626\n",
            "Early stopping:  0.023873264387192426\n",
            "Epoch: 045, Loss: 2.0090, Train: 0.3722, Test: 0.3657\n",
            "Early stopping:  0.027583724272513797\n",
            "Epoch: 046, Loss: 1.9926, Train: 0.3754, Test: 0.3657\n",
            "Early stopping:  0.02879042181936646\n",
            "Epoch: 047, Loss: 1.9791, Train: 0.3762, Test: 0.3632\n",
            "Early stopping:  0.02717590808286878\n",
            "Epoch: 048, Loss: 1.9666, Train: 0.3766, Test: 0.3638\n",
            "Early stopping:  0.024115234737651856\n",
            "Epoch: 049, Loss: 1.9544, Train: 0.3733, Test: 0.3621\n",
            "Early stopping:  0.021419309981592662\n",
            "Epoch: 050, Loss: 1.9401, Train: 0.3792, Test: 0.3663\n",
            "Early stopping:  0.020511044915908026\n",
            "Epoch: 051, Loss: 1.9216, Train: 0.3854, Test: 0.3714\n",
            "Early stopping:  0.02246787708404964\n",
            "Epoch: 052, Loss: 1.8993, Train: 0.3951, Test: 0.3796\n",
            "Early stopping:  0.026687025719940083\n",
            "Epoch: 053, Loss: 1.8759, Train: 0.4042, Test: 0.3876\n",
            "Early stopping:  0.031430364238008955\n",
            "Epoch: 054, Loss: 1.8543, Train: 0.4140, Test: 0.4006\n",
            "Early stopping:  0.03440354104843931\n",
            "Epoch: 055, Loss: 1.8328, Train: 0.4183, Test: 0.4086\n",
            "Early stopping:  0.03518989805109051\n",
            "Epoch: 056, Loss: 1.8120, Train: 0.4219, Test: 0.4134\n",
            "Early stopping:  0.0344205755824138\n",
            "Epoch: 057, Loss: 1.7922, Train: 0.4263, Test: 0.4157\n",
            "Early stopping:  0.033127234564422875\n",
            "Epoch: 058, Loss: 1.7739, Train: 0.4316, Test: 0.4176\n",
            "Early stopping:  0.03184583440492537\n",
            "Epoch: 059, Loss: 1.7551, Train: 0.4384, Test: 0.4213\n",
            "Early stopping:  0.03062034590587434\n",
            "Epoch: 060, Loss: 1.7359, Train: 0.4474, Test: 0.4270\n",
            "Early stopping:  0.029931487842618263\n",
            "Epoch: 061, Loss: 1.7166, Train: 0.4820, Test: 0.4568\n",
            "Early stopping:  0.0299392992414917\n",
            "Epoch: 062, Loss: 1.6988, Train: 0.4875, Test: 0.4531\n",
            "Early stopping:  0.029835696890295003\n",
            "Epoch: 063, Loss: 1.6853, Train: 0.4917, Test: 0.4570\n",
            "Early stopping:  0.027991793130272898\n",
            "Epoch: 064, Loss: 1.6714, Train: 0.4945, Test: 0.4590\n",
            "Early stopping:  0.025430750506924373\n",
            "Epoch: 065, Loss: 1.6561, Train: 0.4966, Test: 0.4630\n",
            "Early stopping:  0.023469724749995736\n",
            "Epoch: 066, Loss: 1.6416, Train: 0.4979, Test: 0.4692\n",
            "Early stopping:  0.022732395153991704\n",
            "Epoch: 067, Loss: 1.6262, Train: 0.5023, Test: 0.4743\n",
            "Early stopping:  0.023414542460629465\n",
            "Epoch: 068, Loss: 1.6096, Train: 0.5051, Test: 0.4814\n",
            "Early stopping:  0.024270981415882832\n",
            "Epoch: 069, Loss: 1.5939, Train: 0.5072, Test: 0.4860\n",
            "Early stopping:  0.024723873731329195\n",
            "Epoch: 070, Loss: 1.5809, Train: 0.5115, Test: 0.4914\n",
            "Early stopping:  0.024292185987466696\n",
            "Epoch: 071, Loss: 1.5689, Train: 0.5130, Test: 0.4925\n",
            "Early stopping:  0.02271870479447123\n",
            "Epoch: 072, Loss: 1.5553, Train: 0.5163, Test: 0.4959\n",
            "Early stopping:  0.02116926508944419\n",
            "Epoch: 073, Loss: 1.5425, Train: 0.5155, Test: 0.4976\n",
            "Early stopping:  0.020320451386336497\n",
            "Epoch: 074, Loss: 1.5300, Train: 0.5170, Test: 0.4993\n",
            "Early stopping:  0.020284570894819786\n",
            "Epoch: 075, Loss: 1.5171, Train: 0.5202, Test: 0.5030\n",
            "Early stopping:  0.020376996193710393\n",
            "Epoch: 076, Loss: 1.5050, Train: 0.5214, Test: 0.5027\n",
            "Early stopping:  0.0199048475849095\n",
            "Epoch: 077, Loss: 1.4932, Train: 0.5236, Test: 0.5052\n",
            "Early stopping:  0.019548792854074168\n",
            "Epoch: 078, Loss: 1.4807, Train: 0.5289, Test: 0.5078\n",
            "Early stopping:  0.019368471464449876\n",
            "Epoch: 079, Loss: 1.4699, Train: 0.5342, Test: 0.5123\n",
            "Early stopping:  0.018772087008853047\n",
            "Epoch: 080, Loss: 1.4593, Train: 0.5384, Test: 0.5123\n",
            "Early stopping:  0.018139128659041683\n",
            "Epoch: 081, Loss: 1.4485, Train: 0.5406, Test: 0.5152\n",
            "Early stopping:  0.01751073741869548\n",
            "Epoch: 082, Loss: 1.4379, Train: 0.5433, Test: 0.5157\n",
            "Early stopping:  0.01692055278547294\n",
            "Epoch: 083, Loss: 1.4267, Train: 0.5476, Test: 0.5180\n",
            "Early stopping:  0.0170506488956991\n",
            "Epoch: 084, Loss: 1.4154, Train: 0.5499, Test: 0.5197\n",
            "Early stopping:  0.01733717756119164\n",
            "Epoch: 085, Loss: 1.4052, Train: 0.5537, Test: 0.5225\n",
            "Early stopping:  0.017240109355972262\n",
            "Epoch: 086, Loss: 1.3951, Train: 0.5588, Test: 0.5234\n",
            "Early stopping:  0.016925064138240038\n",
            "Epoch: 087, Loss: 1.3851, Train: 0.5614, Test: 0.5262\n",
            "Early stopping:  0.016385154233424255\n",
            "Epoch: 088, Loss: 1.3741, Train: 0.5647, Test: 0.5282\n",
            "Early stopping:  0.016256135164937232\n",
            "Epoch: 089, Loss: 1.3639, Train: 0.5660, Test: 0.5305\n",
            "Early stopping:  0.01639931459584066\n",
            "Epoch: 090, Loss: 1.3537, Train: 0.5696, Test: 0.5302\n",
            "Early stopping:  0.016449860018582313\n",
            "Epoch: 091, Loss: 1.3442, Train: 0.5750, Test: 0.5308\n",
            "Early stopping:  0.01615882798284878\n",
            "Epoch: 092, Loss: 1.3347, Train: 0.5773, Test: 0.5305\n",
            "Early stopping:  0.015568184936414665\n",
            "Epoch: 093, Loss: 1.3247, Train: 0.5786, Test: 0.5342\n",
            "Early stopping:  0.015401947608529384\n",
            "Epoch: 094, Loss: 1.3144, Train: 0.5805, Test: 0.5319\n",
            "Early stopping:  0.015532622042135529\n",
            "Epoch: 095, Loss: 1.3043, Train: 0.5853, Test: 0.5367\n",
            "Early stopping:  0.015834690030905117\n",
            "Epoch: 096, Loss: 1.2946, Train: 0.5856, Test: 0.5361\n",
            "Early stopping:  0.015914215183498476\n",
            "Epoch: 097, Loss: 1.2852, Train: 0.5900, Test: 0.5398\n",
            "Early stopping:  0.015592785117365275\n",
            "Epoch: 098, Loss: 1.2758, Train: 0.5932, Test: 0.5407\n",
            "Early stopping:  0.015199383013400208\n",
            "Epoch: 099, Loss: 1.2658, Train: 0.5924, Test: 0.5438\n",
            "Early stopping:  0.015143193900373648\n",
            "Epoch: 100, Loss: 1.2587, Train: 0.5987, Test: 0.5464\n",
            "Early stopping:  0.01443400501374726\n",
            "Epoch: 101, Loss: 1.2520, Train: 0.5991, Test: 0.5500\n",
            "Early stopping:  0.01326754924790735\n",
            "Epoch: 102, Loss: 1.2418, Train: 0.5962, Test: 0.5481\n",
            "Early stopping:  0.012993213680064819\n",
            "Epoch: 103, Loss: 1.2369, Train: 0.6047, Test: 0.5500\n",
            "Early stopping:  0.011881245314127037\n",
            "Epoch: 104, Loss: 1.2255, Train: 0.6085, Test: 0.5543\n",
            "Early stopping:  0.012975976433760436\n",
            "Epoch: 105, Loss: 1.2198, Train: 0.6098, Test: 0.5551\n",
            "Early stopping:  0.012837707884703618\n",
            "Epoch: 106, Loss: 1.2076, Train: 0.6110, Test: 0.5580\n",
            "Early stopping:  0.013645473180773502\n",
            "Epoch: 107, Loss: 1.2026, Train: 0.6195, Test: 0.5594\n",
            "Early stopping:  0.01378743012113653\n",
            "Epoch: 108, Loss: 1.1892, Train: 0.6229, Test: 0.5617\n",
            "Early stopping:  0.014349596767410053\n",
            "Epoch: 109, Loss: 1.2030, Train: 0.6246, Test: 0.5724\n",
            "Early stopping:  0.010996769468948384\n",
            "Epoch: 110, Loss: 1.1771, Train: 0.6227, Test: 0.5656\n",
            "Early stopping:  0.012545709294644778\n",
            "Epoch: 111, Loss: 1.1707, Train: 0.6272, Test: 0.5648\n",
            "Early stopping:  0.014645768858006548\n",
            "Epoch: 112, Loss: 1.1731, Train: 0.6319, Test: 0.5736\n",
            "Early stopping:  0.01343333017549049\n",
            "Epoch: 113, Loss: 1.1600, Train: 0.6346, Test: 0.5753\n",
            "Early stopping:  0.015965811768534118\n",
            "Epoch: 114, Loss: 1.1558, Train: 0.6352, Test: 0.5668\n",
            "Early stopping:  0.009031515879335036\n",
            "PREDICTIONS -> tensor([9, 0, 9,  ..., 5, 5, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.51      0.61      0.56       379\n",
            "         capital_goods       0.40      0.26      0.32       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.51      0.54      0.53       396\n",
            " consumer_non-cyclical       0.67      0.38      0.49       223\n",
            "                energy       0.50      0.52      0.51       141\n",
            "             financial       0.68      0.63      0.65       384\n",
            "            healthcare       0.74      0.52      0.61       159\n",
            "              services       0.55      0.75      0.64      1038\n",
            "            technology       0.55      0.39      0.46       198\n",
            "        transportation       0.71      0.70      0.70       202\n",
            "             utilities       0.92      0.10      0.18       113\n",
            "\n",
            "              accuracy                           0.57      3527\n",
            "             macro avg       0.56      0.45      0.47      3527\n",
            "          weighted avg       0.57      0.57      0.55      3527\n",
            "\n",
            "time: 2min 22s (started: 2024-10-16 21:23:17 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAXvNOjagSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ae8a74-2333-40e6-a62d-e33bfc2891d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 462 ms (started: 2024-10-16 21:25:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LrmRjvUgSl-"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOseVsLRgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562b5429-78e2-4226-84ec-a20e13336a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4947, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2598, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16610831641416535\n",
            "Epoch: 003, Loss: 2.1692, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16798152112172923\n",
            "Epoch: 004, Loss: 2.1185, Train: 0.3079, Test: 0.3099\n",
            "Early stopping:  0.16667945080533791\n",
            "Epoch: 005, Loss: 2.0690, Train: 0.3484, Test: 0.3419\n",
            "Early stopping:  0.16785197664113266\n",
            "Epoch: 006, Loss: 2.0245, Train: 0.3558, Test: 0.3519\n",
            "Early stopping:  0.09129730094661055\n",
            "Epoch: 007, Loss: 1.9698, Train: 0.3493, Test: 0.3470\n",
            "Early stopping:  0.07795430496476206\n",
            "Epoch: 008, Loss: 1.9120, Train: 0.3495, Test: 0.3419\n",
            "Early stopping:  0.08108033664837107\n",
            "Epoch: 009, Loss: 1.8587, Train: 0.3578, Test: 0.3502\n",
            "Early stopping:  0.08436433456421047\n",
            "Epoch: 010, Loss: 1.8075, Train: 0.3811, Test: 0.3748\n",
            "Early stopping:  0.08622075347826913\n",
            "Epoch: 011, Loss: 1.7516, Train: 0.4208, Test: 0.4088\n",
            "Early stopping:  0.08554514377951433\n",
            "Epoch: 012, Loss: 1.6927, Train: 0.4533, Test: 0.4412\n",
            "Early stopping:  0.0863279254005525\n",
            "Epoch: 013, Loss: 1.6372, Train: 0.4792, Test: 0.4647\n",
            "Early stopping:  0.08822075458356508\n",
            "Epoch: 014, Loss: 1.5839, Train: 0.4945, Test: 0.4831\n",
            "Early stopping:  0.08881175486340116\n",
            "Epoch: 015, Loss: 1.5281, Train: 0.5113, Test: 0.4979\n",
            "Early stopping:  0.08788606520273369\n",
            "Epoch: 016, Loss: 1.4739, Train: 0.5301, Test: 0.5177\n",
            "Early stopping:  0.08644115200281186\n",
            "Epoch: 017, Loss: 1.4257, Train: 0.5505, Test: 0.5404\n",
            "Early stopping:  0.08430878168380268\n",
            "Epoch: 018, Loss: 1.3778, Train: 0.5682, Test: 0.5529\n",
            "Early stopping:  0.0814201344319064\n",
            "Epoch: 019, Loss: 1.3301, Train: 0.5802, Test: 0.5600\n",
            "Early stopping:  0.07785408289738095\n",
            "Epoch: 020, Loss: 1.2896, Train: 0.5888, Test: 0.5736\n",
            "Early stopping:  0.0734332565690829\n",
            "Epoch: 021, Loss: 1.2530, Train: 0.6006, Test: 0.5863\n",
            "Early stopping:  0.06867889305342766\n",
            "Epoch: 022, Loss: 1.2181, Train: 0.6130, Test: 0.5960\n",
            "Early stopping:  0.06281064708282298\n",
            "Epoch: 023, Loss: 1.1886, Train: 0.6236, Test: 0.6036\n",
            "Early stopping:  0.056139077316471525\n",
            "Epoch: 024, Loss: 1.1599, Train: 0.6308, Test: 0.6121\n",
            "Early stopping:  0.051272261036630726\n",
            "Epoch: 025, Loss: 1.1333, Train: 0.6406, Test: 0.6130\n",
            "Early stopping:  0.04709431856102245\n",
            "Epoch: 026, Loss: 1.1097, Train: 0.6461, Test: 0.6170\n",
            "Early stopping:  0.04305618993978019\n",
            "Epoch: 027, Loss: 1.0856, Train: 0.6541, Test: 0.6255\n",
            "Early stopping:  0.04052785922549152\n",
            "Epoch: 028, Loss: 1.0624, Train: 0.6648, Test: 0.6317\n",
            "Early stopping:  0.03838636345574326\n",
            "Epoch: 029, Loss: 1.0397, Train: 0.6747, Test: 0.6391\n",
            "Early stopping:  0.037089180271758296\n",
            "Epoch: 030, Loss: 1.0177, Train: 0.6849, Test: 0.6467\n",
            "Early stopping:  0.03635485003193722\n",
            "Epoch: 031, Loss: 0.9960, Train: 0.6928, Test: 0.6521\n",
            "Early stopping:  0.03541738874505381\n",
            "Epoch: 032, Loss: 0.9751, Train: 0.6992, Test: 0.6535\n",
            "Early stopping:  0.0345388266318059\n",
            "Epoch: 033, Loss: 0.9543, Train: 0.7091, Test: 0.6581\n",
            "Early stopping:  0.033743957546244395\n",
            "Epoch: 034, Loss: 0.9347, Train: 0.7180, Test: 0.6620\n",
            "Early stopping:  0.032840621514815684\n",
            "Epoch: 035, Loss: 0.9155, Train: 0.7259, Test: 0.6683\n",
            "Early stopping:  0.03182307420358079\n",
            "Epoch: 036, Loss: 0.8970, Train: 0.7333, Test: 0.6731\n",
            "Early stopping:  0.030829137399685444\n",
            "Epoch: 037, Loss: 0.8789, Train: 0.7420, Test: 0.6742\n",
            "Early stopping:  0.029833681394295933\n",
            "Epoch: 038, Loss: 0.8619, Train: 0.7503, Test: 0.6739\n",
            "Early stopping:  0.02884737398754249\n",
            "Epoch: 039, Loss: 0.8453, Train: 0.7611, Test: 0.6782\n",
            "Early stopping:  0.027785001724069405\n",
            "Epoch: 040, Loss: 0.8292, Train: 0.7658, Test: 0.6842\n",
            "Early stopping:  0.026765442997562585\n",
            "Epoch: 041, Loss: 0.8137, Train: 0.7715, Test: 0.6887\n",
            "Early stopping:  0.02576445381477166\n",
            "Epoch: 042, Loss: 0.7980, Train: 0.7769, Test: 0.6927\n",
            "Early stopping:  0.025193732327959573\n",
            "Epoch: 043, Loss: 0.7828, Train: 0.7849, Test: 0.6983\n",
            "Early stopping:  0.024700644563330557\n",
            "Epoch: 044, Loss: 0.7671, Train: 0.7879, Test: 0.7009\n",
            "Early stopping:  0.02451298898086705\n",
            "Epoch: 045, Loss: 0.7517, Train: 0.7928, Test: 0.7034\n",
            "Early stopping:  0.024488321174863024\n",
            "Epoch: 046, Loss: 0.7363, Train: 0.7989, Test: 0.7063\n",
            "Early stopping:  0.02440381302102044\n",
            "Epoch: 047, Loss: 0.7211, Train: 0.8028, Test: 0.7074\n",
            "Early stopping:  0.024375048857049577\n",
            "Epoch: 048, Loss: 0.7061, Train: 0.8074, Test: 0.7108\n",
            "Early stopping:  0.02414890881521195\n",
            "Epoch: 049, Loss: 0.6913, Train: 0.8115, Test: 0.7134\n",
            "Early stopping:  0.0238817498937894\n",
            "Epoch: 050, Loss: 0.6767, Train: 0.8157, Test: 0.7170\n",
            "Early stopping:  0.023575365839882494\n",
            "Epoch: 051, Loss: 0.6623, Train: 0.8212, Test: 0.7204\n",
            "Early stopping:  0.02325543761169716\n",
            "Epoch: 052, Loss: 0.6480, Train: 0.8225, Test: 0.7210\n",
            "Early stopping:  0.02296168234843712\n",
            "Epoch: 053, Loss: 0.6341, Train: 0.8304, Test: 0.7278\n",
            "Early stopping:  0.02262956742484763\n",
            "Epoch: 054, Loss: 0.6203, Train: 0.8301, Test: 0.7292\n",
            "Early stopping:  0.022275294173231205\n",
            "Epoch: 055, Loss: 0.6069, Train: 0.8427, Test: 0.7363\n",
            "Early stopping:  0.02187455706247975\n",
            "Epoch: 056, Loss: 0.5944, Train: 0.8395, Test: 0.7312\n",
            "Early stopping:  0.02122911373101889\n",
            "Epoch: 057, Loss: 0.5834, Train: 0.8509, Test: 0.7369\n",
            "Early stopping:  0.02014447411124508\n",
            "Epoch: 058, Loss: 0.5726, Train: 0.8463, Test: 0.7321\n",
            "Early stopping:  0.018846985567235977\n",
            "Epoch: 059, Loss: 0.5599, Train: 0.8594, Test: 0.7414\n",
            "Early stopping:  0.018357479594410756\n",
            "Epoch: 060, Loss: 0.5442, Train: 0.8628, Test: 0.7414\n",
            "Early stopping:  0.019660029875479827\n",
            "Epoch: 061, Loss: 0.5324, Train: 0.8607, Test: 0.7409\n",
            "Early stopping:  0.02064220053322547\n",
            "Epoch: 062, Loss: 0.5235, Train: 0.8718, Test: 0.7468\n",
            "Early stopping:  0.019931871078943505\n",
            "Epoch: 063, Loss: 0.5121, Train: 0.8705, Test: 0.7443\n",
            "Early stopping:  0.018458614539678182\n",
            "Epoch: 064, Loss: 0.4990, Train: 0.8794, Test: 0.7451\n",
            "Early stopping:  0.017544224795826523\n",
            "Epoch: 065, Loss: 0.4879, Train: 0.8777, Test: 0.7491\n",
            "Early stopping:  0.017970924827831313\n",
            "Epoch: 066, Loss: 0.4794, Train: 0.8803, Test: 0.7457\n",
            "Early stopping:  0.017820387935883863\n",
            "Epoch: 067, Loss: 0.4700, Train: 0.8851, Test: 0.7528\n",
            "Early stopping:  0.016484755981800686\n",
            "Epoch: 068, Loss: 0.4571, Train: 0.8947, Test: 0.7547\n",
            "Early stopping:  0.016127422317690918\n",
            "Epoch: 069, Loss: 0.4462, Train: 0.8911, Test: 0.7522\n",
            "Early stopping:  0.01678017279321284\n",
            "Epoch: 070, Loss: 0.4386, Train: 0.9013, Test: 0.7550\n",
            "Early stopping:  0.016735326682014266\n",
            "Epoch: 071, Loss: 0.4297, Train: 0.8977, Test: 0.7513\n",
            "Early stopping:  0.01575827413104783\n",
            "Epoch: 072, Loss: 0.4191, Train: 0.9093, Test: 0.7545\n",
            "Early stopping:  0.014639417280852952\n",
            "Epoch: 073, Loss: 0.4098, Train: 0.9026, Test: 0.7511\n",
            "Early stopping:  0.014612186589499913\n",
            "Epoch: 074, Loss: 0.4034, Train: 0.9095, Test: 0.7516\n",
            "Early stopping:  0.014294668024556395\n",
            "Epoch: 075, Loss: 0.3971, Train: 0.9089, Test: 0.7525\n",
            "Early stopping:  0.012872611005064857\n",
            "Epoch: 076, Loss: 0.3867, Train: 0.9172, Test: 0.7547\n",
            "Early stopping:  0.012297191259526882\n",
            "Epoch: 077, Loss: 0.3734, Train: 0.9163, Test: 0.7562\n",
            "Early stopping:  0.0143456864560424\n",
            "Epoch: 078, Loss: 0.3660, Train: 0.9214, Test: 0.7530\n",
            "Early stopping:  0.015688578401200408\n",
            "Epoch: 079, Loss: 0.3609, Train: 0.9261, Test: 0.7536\n",
            "Early stopping:  0.01492925604580611\n",
            "Epoch: 080, Loss: 0.3507, Train: 0.9246, Test: 0.7553\n",
            "Early stopping:  0.01355333276850045\n",
            "Epoch: 081, Loss: 0.3415, Train: 0.9323, Test: 0.7570\n",
            "Early stopping:  0.012615470153139165\n",
            "Epoch: 082, Loss: 0.3367, Train: 0.9274, Test: 0.7528\n",
            "Early stopping:  0.012437403348497668\n",
            "Epoch: 083, Loss: 0.3311, Train: 0.9382, Test: 0.7559\n",
            "Early stopping:  0.011807210356976038\n",
            "Epoch: 084, Loss: 0.3210, Train: 0.9365, Test: 0.7567\n",
            "Early stopping:  0.011104914812703993\n",
            "Epoch: 085, Loss: 0.3127, Train: 0.9395, Test: 0.7553\n",
            "Early stopping:  0.011711758125564993\n",
            "Epoch: 086, Loss: 0.3082, Train: 0.9397, Test: 0.7565\n",
            "Early stopping:  0.01201770573013235\n",
            "Epoch: 087, Loss: 0.3035, Train: 0.9448, Test: 0.7559\n",
            "Early stopping:  0.010930068564019716\n",
            "Epoch: 088, Loss: 0.2960, Train: 0.9425, Test: 0.7565\n",
            "Early stopping:  0.009415103677627042\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.71      0.75       379\n",
            "         capital_goods       0.75      0.67      0.71       254\n",
            "conglomerates_industry       1.00      0.78      0.87        40\n",
            "     consumer_cyclical       0.67      0.74      0.71       396\n",
            " consumer_non-cyclical       0.75      0.65      0.70       223\n",
            "                energy       0.86      0.76      0.81       141\n",
            "             financial       0.82      0.79      0.80       384\n",
            "            healthcare       0.86      0.72      0.78       159\n",
            "              services       0.71      0.83      0.77      1038\n",
            "            technology       0.73      0.58      0.65       198\n",
            "        transportation       0.83      0.82      0.82       202\n",
            "             utilities       0.86      0.79      0.82       113\n",
            "\n",
            "              accuracy                           0.76      3527\n",
            "             macro avg       0.80      0.74      0.77      3527\n",
            "          weighted avg       0.76      0.76      0.76      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4520, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2743, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.12564026117315724\n",
            "Epoch: 003, Loss: 2.2004, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1292989327374633\n",
            "Epoch: 004, Loss: 2.1477, Train: 0.2983, Test: 0.2971\n",
            "Early stopping:  0.13283013527226023\n",
            "Epoch: 005, Loss: 2.0822, Train: 0.3284, Test: 0.3249\n",
            "Early stopping:  0.14206877753793162\n",
            "Epoch: 006, Loss: 2.0347, Train: 0.3675, Test: 0.3640\n",
            "Early stopping:  0.09467707475005802\n",
            "Epoch: 007, Loss: 1.9833, Train: 0.3762, Test: 0.3728\n",
            "Early stopping:  0.0866099055810648\n",
            "Epoch: 008, Loss: 1.9168, Train: 0.3730, Test: 0.3680\n",
            "Early stopping:  0.08878139622066343\n",
            "Epoch: 009, Loss: 1.8494, Train: 0.3853, Test: 0.3760\n",
            "Early stopping:  0.09256478090087576\n",
            "Epoch: 010, Loss: 1.7906, Train: 0.4064, Test: 0.3969\n",
            "Early stopping:  0.09848342714455197\n",
            "Epoch: 011, Loss: 1.7313, Train: 0.4416, Test: 0.4281\n",
            "Early stopping:  0.09972013098309233\n",
            "Epoch: 012, Loss: 1.6703, Train: 0.4796, Test: 0.4647\n",
            "Early stopping:  0.0966530803775943\n",
            "Epoch: 013, Loss: 1.6125, Train: 0.5025, Test: 0.4857\n",
            "Early stopping:  0.0939112481594497\n",
            "Epoch: 014, Loss: 1.5572, Train: 0.5121, Test: 0.4993\n",
            "Early stopping:  0.09258147014739442\n",
            "Epoch: 015, Loss: 1.5037, Train: 0.5214, Test: 0.5086\n",
            "Early stopping:  0.08988058837323541\n",
            "Epoch: 016, Loss: 1.4512, Train: 0.5372, Test: 0.5214\n",
            "Early stopping:  0.08651738627839278\n",
            "Epoch: 017, Loss: 1.4013, Train: 0.5499, Test: 0.5347\n",
            "Early stopping:  0.08357967527873804\n",
            "Epoch: 018, Loss: 1.3566, Train: 0.5722, Test: 0.5503\n",
            "Early stopping:  0.07967493470110788\n",
            "Epoch: 019, Loss: 1.3129, Train: 0.5943, Test: 0.5679\n",
            "Early stopping:  0.07535713070055824\n",
            "Epoch: 020, Loss: 1.2713, Train: 0.6078, Test: 0.5872\n",
            "Early stopping:  0.07091193768976599\n",
            "Epoch: 021, Loss: 1.2368, Train: 0.6172, Test: 0.5943\n",
            "Early stopping:  0.06558141726242953\n",
            "Epoch: 022, Loss: 1.2030, Train: 0.6257, Test: 0.5985\n",
            "Early stopping:  0.060723891978160346\n",
            "Epoch: 023, Loss: 1.1703, Train: 0.6297, Test: 0.6033\n",
            "Early stopping:  0.05594798665065862\n",
            "Epoch: 024, Loss: 1.1427, Train: 0.6389, Test: 0.6124\n",
            "Early stopping:  0.05124184532536229\n",
            "Epoch: 025, Loss: 1.1147, Train: 0.6510, Test: 0.6178\n",
            "Early stopping:  0.04822089188422518\n",
            "Epoch: 026, Loss: 1.0885, Train: 0.6603, Test: 0.6204\n",
            "Early stopping:  0.04504059247048936\n",
            "Epoch: 027, Loss: 1.0643, Train: 0.6647, Test: 0.6252\n",
            "Early stopping:  0.04209470728983414\n",
            "Epoch: 028, Loss: 1.0402, Train: 0.6709, Test: 0.6337\n",
            "Early stopping:  0.04038852360675796\n",
            "Epoch: 029, Loss: 1.0175, Train: 0.6817, Test: 0.6419\n",
            "Early stopping:  0.03837649562284555\n",
            "Epoch: 030, Loss: 0.9962, Train: 0.6921, Test: 0.6456\n",
            "Early stopping:  0.0365897310635928\n",
            "Epoch: 031, Loss: 0.9748, Train: 0.7026, Test: 0.6493\n",
            "Early stopping:  0.03527962007041831\n",
            "Epoch: 032, Loss: 0.9545, Train: 0.7136, Test: 0.6530\n",
            "Early stopping:  0.03386598117341781\n",
            "Epoch: 033, Loss: 0.9343, Train: 0.7227, Test: 0.6609\n",
            "Early stopping:  0.03292447132605094\n",
            "Epoch: 034, Loss: 0.9158, Train: 0.7295, Test: 0.6649\n",
            "Early stopping:  0.03185649372291085\n",
            "Epoch: 035, Loss: 0.8970, Train: 0.7380, Test: 0.6714\n",
            "Early stopping:  0.03072581385826411\n",
            "Epoch: 036, Loss: 0.8796, Train: 0.7450, Test: 0.6748\n",
            "Early stopping:  0.029593706445703572\n",
            "Epoch: 037, Loss: 0.8628, Train: 0.7512, Test: 0.6802\n",
            "Early stopping:  0.028337924915740817\n",
            "Epoch: 038, Loss: 0.8461, Train: 0.7588, Test: 0.6833\n",
            "Early stopping:  0.027448960684373434\n",
            "Epoch: 039, Loss: 0.8301, Train: 0.7643, Test: 0.6876\n",
            "Early stopping:  0.02644462744107497\n",
            "Epoch: 040, Loss: 0.8140, Train: 0.7690, Test: 0.6904\n",
            "Early stopping:  0.025920241035230787\n",
            "Epoch: 041, Loss: 0.7987, Train: 0.7726, Test: 0.6941\n",
            "Early stopping:  0.025366778904783605\n",
            "Epoch: 042, Loss: 0.7832, Train: 0.7790, Test: 0.6949\n",
            "Early stopping:  0.024844787451344438\n",
            "Epoch: 043, Loss: 0.7681, Train: 0.7817, Test: 0.7006\n",
            "Early stopping:  0.02447402126273491\n",
            "Epoch: 044, Loss: 0.7532, Train: 0.7873, Test: 0.7048\n",
            "Early stopping:  0.024042347190320397\n",
            "Epoch: 045, Loss: 0.7383, Train: 0.7913, Test: 0.7085\n",
            "Early stopping:  0.023847246626634518\n",
            "Epoch: 046, Loss: 0.7239, Train: 0.7968, Test: 0.7111\n",
            "Early stopping:  0.023468900102755437\n",
            "Epoch: 047, Loss: 0.7094, Train: 0.8013, Test: 0.7136\n",
            "Early stopping:  0.023190338508648944\n",
            "Epoch: 048, Loss: 0.6954, Train: 0.8051, Test: 0.7156\n",
            "Early stopping:  0.022851451847920173\n",
            "Epoch: 049, Loss: 0.6813, Train: 0.8104, Test: 0.7190\n",
            "Early stopping:  0.022522078689441682\n",
            "Epoch: 050, Loss: 0.6676, Train: 0.8144, Test: 0.7210\n",
            "Early stopping:  0.02225186706512249\n",
            "Epoch: 051, Loss: 0.6539, Train: 0.8204, Test: 0.7233\n",
            "Early stopping:  0.021942811166589055\n",
            "Epoch: 052, Loss: 0.6403, Train: 0.8234, Test: 0.7247\n",
            "Early stopping:  0.021772982974091012\n",
            "Epoch: 053, Loss: 0.6270, Train: 0.8287, Test: 0.7250\n",
            "Early stopping:  0.02150322383705629\n",
            "Epoch: 054, Loss: 0.6142, Train: 0.8263, Test: 0.7264\n",
            "Early stopping:  0.021138348034367992\n",
            "Epoch: 055, Loss: 0.6049, Train: 0.8369, Test: 0.7216\n",
            "Early stopping:  0.019652919786321475\n",
            "Epoch: 056, Loss: 0.6048, Train: 0.8168, Test: 0.7193\n",
            "Early stopping:  0.015286255392336953\n",
            "Epoch: 057, Loss: 0.6015, Train: 0.8491, Test: 0.7289\n",
            "Early stopping:  0.01035916483547683\n",
            "Epoch: 058, Loss: 0.5709, Train: 0.8499, Test: 0.7315\n",
            "Early stopping:  0.016553508048346877\n",
            "Epoch: 059, Loss: 0.5629, Train: 0.8355, Test: 0.7250\n",
            "Early stopping:  0.02042244423101418\n",
            "Epoch: 060, Loss: 0.5617, Train: 0.8560, Test: 0.7377\n",
            "Early stopping:  0.021142482422099828\n",
            "Epoch: 061, Loss: 0.5374, Train: 0.8597, Test: 0.7369\n",
            "Early stopping:  0.023070832260331305\n",
            "Epoch: 062, Loss: 0.5368, Train: 0.8505, Test: 0.7341\n",
            "Early stopping:  0.01577989598025793\n",
            "Epoch: 063, Loss: 0.5240, Train: 0.8618, Test: 0.7341\n",
            "Early stopping:  0.017075588881346252\n",
            "Epoch: 064, Loss: 0.5109, Train: 0.8699, Test: 0.7417\n",
            "Early stopping:  0.01884549493912929\n",
            "Epoch: 065, Loss: 0.5079, Train: 0.8679, Test: 0.7400\n",
            "Early stopping:  0.013889111379767036\n",
            "Epoch: 066, Loss: 0.4922, Train: 0.8665, Test: 0.7397\n",
            "Early stopping:  0.016907459105752993\n",
            "Epoch: 067, Loss: 0.4872, Train: 0.8817, Test: 0.7451\n",
            "Early stopping:  0.014863121295024873\n",
            "Epoch: 068, Loss: 0.4786, Train: 0.8832, Test: 0.7445\n",
            "Early stopping:  0.013754710410102877\n",
            "Epoch: 069, Loss: 0.4659, Train: 0.8760, Test: 0.7411\n",
            "Early stopping:  0.015632632139233858\n",
            "Epoch: 070, Loss: 0.4625, Train: 0.8924, Test: 0.7479\n",
            "Early stopping:  0.012923605717395726\n",
            "Epoch: 071, Loss: 0.4510, Train: 0.8949, Test: 0.7474\n",
            "Early stopping:  0.014130840866423066\n",
            "Epoch: 072, Loss: 0.4413, Train: 0.8866, Test: 0.7457\n",
            "Early stopping:  0.014304431412693958\n",
            "Epoch: 073, Loss: 0.4372, Train: 0.9013, Test: 0.7516\n",
            "Early stopping:  0.01262785084790619\n",
            "Epoch: 074, Loss: 0.4250, Train: 0.9047, Test: 0.7516\n",
            "Early stopping:  0.014188366709582609\n",
            "Epoch: 075, Loss: 0.4172, Train: 0.8966, Test: 0.7522\n",
            "Early stopping:  0.013374640373090378\n",
            "Epoch: 076, Loss: 0.4124, Train: 0.9102, Test: 0.7547\n",
            "Early stopping:  0.01247234126106835\n",
            "Epoch: 077, Loss: 0.4009, Train: 0.9134, Test: 0.7556\n",
            "Early stopping:  0.013621508234791337\n",
            "Epoch: 078, Loss: 0.3935, Train: 0.9064, Test: 0.7562\n",
            "Early stopping:  0.012636621243293336\n",
            "Epoch: 079, Loss: 0.3886, Train: 0.9187, Test: 0.7567\n",
            "Early stopping:  0.012159237084273501\n",
            "Epoch: 080, Loss: 0.3782, Train: 0.9189, Test: 0.7582\n",
            "Early stopping:  0.012851407242517024\n",
            "Epoch: 081, Loss: 0.3703, Train: 0.9161, Test: 0.7604\n",
            "Early stopping:  0.012143366983731658\n",
            "Epoch: 082, Loss: 0.3652, Train: 0.9255, Test: 0.7616\n",
            "Early stopping:  0.011924788541445018\n",
            "Epoch: 083, Loss: 0.3570, Train: 0.9253, Test: 0.7613\n",
            "Early stopping:  0.01213333483431653\n",
            "Epoch: 084, Loss: 0.3488, Train: 0.9250, Test: 0.7616\n",
            "Early stopping:  0.011438074625449244\n",
            "Epoch: 085, Loss: 0.3426, Train: 0.9331, Test: 0.7621\n",
            "Early stopping:  0.011368679925248119\n",
            "Epoch: 086, Loss: 0.3357, Train: 0.9318, Test: 0.7621\n",
            "Early stopping:  0.011611983019686887\n",
            "Epoch: 087, Loss: 0.3290, Train: 0.9365, Test: 0.7610\n",
            "Early stopping:  0.01093784433247102\n",
            "Epoch: 088, Loss: 0.3234, Train: 0.9338, Test: 0.7565\n",
            "Early stopping:  0.01019065079730671\n",
            "Epoch: 089, Loss: 0.3206, Train: 0.9327, Test: 0.7559\n",
            "Early stopping:  0.008991498838839925\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11, 11], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.73      0.81      0.77       379\n",
            "         capital_goods       0.65      0.68      0.67       254\n",
            "conglomerates_industry       0.96      0.65      0.78        40\n",
            "     consumer_cyclical       0.79      0.64      0.71       396\n",
            " consumer_non-cyclical       0.72      0.67      0.69       223\n",
            "                energy       0.83      0.74      0.78       141\n",
            "             financial       0.79      0.81      0.80       384\n",
            "            healthcare       0.83      0.82      0.83       159\n",
            "              services       0.75      0.80      0.77      1038\n",
            "            technology       0.68      0.63      0.65       198\n",
            "        transportation       0.80      0.84      0.82       202\n",
            "             utilities       0.78      0.81      0.79       113\n",
            "\n",
            "              accuracy                           0.76      3527\n",
            "             macro avg       0.78      0.74      0.75      3527\n",
            "          weighted avg       0.76      0.76      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5427, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2995, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.17193840567498028\n",
            "Epoch: 003, Loss: 2.1687, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.18978216358579325\n",
            "Epoch: 004, Loss: 2.1654, Train: 0.2979, Test: 0.2971\n",
            "Early stopping:  0.17711983767383466\n",
            "Epoch: 005, Loss: 2.1007, Train: 0.3274, Test: 0.3235\n",
            "Early stopping:  0.17607756169968808\n",
            "Epoch: 006, Loss: 2.0244, Train: 0.3715, Test: 0.3657\n",
            "Early stopping:  0.1013797811802655\n",
            "Epoch: 007, Loss: 1.9715, Train: 0.3924, Test: 0.3845\n",
            "Early stopping:  0.08695374722437683\n",
            "Epoch: 008, Loss: 1.9266, Train: 0.4021, Test: 0.3916\n",
            "Early stopping:  0.09639802443766182\n",
            "Epoch: 009, Loss: 1.8733, Train: 0.4042, Test: 0.3921\n",
            "Early stopping:  0.08784442477513348\n",
            "Epoch: 010, Loss: 1.8129, Train: 0.4104, Test: 0.3961\n",
            "Early stopping:  0.08252194075300302\n",
            "Epoch: 011, Loss: 1.7532, Train: 0.4242, Test: 0.4122\n",
            "Early stopping:  0.08715797288007839\n",
            "Epoch: 012, Loss: 1.6942, Train: 0.4512, Test: 0.4403\n",
            "Early stopping:  0.09248228438812717\n",
            "Epoch: 013, Loss: 1.6338, Train: 0.4747, Test: 0.4675\n",
            "Early stopping:  0.09448235361073774\n",
            "Epoch: 014, Loss: 1.5770, Train: 0.4958, Test: 0.4820\n",
            "Early stopping:  0.09348344415695616\n",
            "Epoch: 015, Loss: 1.5242, Train: 0.5146, Test: 0.5013\n",
            "Early stopping:  0.09098890631181958\n",
            "Epoch: 016, Loss: 1.4683, Train: 0.5401, Test: 0.5214\n",
            "Early stopping:  0.0888094692296267\n",
            "Epoch: 017, Loss: 1.4140, Train: 0.5652, Test: 0.5438\n",
            "Early stopping:  0.08669817093942299\n",
            "Epoch: 018, Loss: 1.3673, Train: 0.5836, Test: 0.5654\n",
            "Early stopping:  0.08375369086690439\n",
            "Epoch: 019, Loss: 1.3242, Train: 0.5921, Test: 0.5753\n",
            "Early stopping:  0.07933027579335017\n",
            "Epoch: 020, Loss: 1.2833, Train: 0.6008, Test: 0.5855\n",
            "Early stopping:  0.0728339064061924\n",
            "Epoch: 021, Loss: 1.2467, Train: 0.6081, Test: 0.5900\n",
            "Early stopping:  0.06627394476352287\n",
            "Epoch: 022, Loss: 1.2130, Train: 0.6117, Test: 0.5897\n",
            "Early stopping:  0.06113819644443254\n",
            "Epoch: 023, Loss: 1.1828, Train: 0.6161, Test: 0.5957\n",
            "Early stopping:  0.05592900407225966\n",
            "Epoch: 024, Loss: 1.1564, Train: 0.6282, Test: 0.6031\n",
            "Early stopping:  0.050342613956706986\n",
            "Epoch: 025, Loss: 1.1302, Train: 0.6391, Test: 0.6110\n",
            "Early stopping:  0.045883730127816484\n",
            "Epoch: 026, Loss: 1.1041, Train: 0.6526, Test: 0.6209\n",
            "Early stopping:  0.04277334198506543\n",
            "Epoch: 027, Loss: 1.0794, Train: 0.6609, Test: 0.6303\n",
            "Early stopping:  0.040951391589557545\n",
            "Epoch: 028, Loss: 1.0561, Train: 0.6688, Test: 0.6354\n",
            "Early stopping:  0.03974885370457358\n",
            "Epoch: 029, Loss: 1.0329, Train: 0.6764, Test: 0.6396\n",
            "Early stopping:  0.038351395366965725\n",
            "Epoch: 030, Loss: 1.0099, Train: 0.6822, Test: 0.6430\n",
            "Early stopping:  0.0371387180498671\n",
            "Epoch: 031, Loss: 0.9885, Train: 0.6921, Test: 0.6515\n",
            "Early stopping:  0.03607232023320086\n",
            "Epoch: 032, Loss: 0.9680, Train: 0.7045, Test: 0.6566\n",
            "Early stopping:  0.03489579781132639\n",
            "Epoch: 033, Loss: 0.9477, Train: 0.7130, Test: 0.6618\n",
            "Early stopping:  0.03358883536830271\n",
            "Epoch: 034, Loss: 0.9276, Train: 0.7234, Test: 0.6657\n",
            "Early stopping:  0.03248155262096394\n",
            "Epoch: 035, Loss: 0.9088, Train: 0.7306, Test: 0.6748\n",
            "Early stopping:  0.03157214027563823\n",
            "Epoch: 036, Loss: 0.8899, Train: 0.7401, Test: 0.6742\n",
            "Early stopping:  0.03083482714949699\n",
            "Epoch: 037, Loss: 0.8719, Train: 0.7473, Test: 0.6765\n",
            "Early stopping:  0.029912310249334703\n",
            "Epoch: 038, Loss: 0.8544, Train: 0.7533, Test: 0.6765\n",
            "Early stopping:  0.02899942751261145\n",
            "Epoch: 039, Loss: 0.8376, Train: 0.7609, Test: 0.6796\n",
            "Early stopping:  0.028175815834916042\n",
            "Epoch: 040, Loss: 0.8209, Train: 0.7665, Test: 0.6853\n",
            "Early stopping:  0.027268523585333576\n",
            "Epoch: 041, Loss: 0.8048, Train: 0.7749, Test: 0.6881\n",
            "Early stopping:  0.026520480914442753\n",
            "Epoch: 042, Loss: 0.7885, Train: 0.7775, Test: 0.6912\n",
            "Early stopping:  0.02602321808828426\n",
            "Epoch: 043, Loss: 0.7726, Train: 0.7839, Test: 0.6961\n",
            "Early stopping:  0.025663839740357457\n",
            "Epoch: 044, Loss: 0.7568, Train: 0.7888, Test: 0.6997\n",
            "Early stopping:  0.025356983659259515\n",
            "Epoch: 045, Loss: 0.7412, Train: 0.7922, Test: 0.7034\n",
            "Early stopping:  0.02510599373820588\n",
            "Epoch: 046, Loss: 0.7260, Train: 0.7981, Test: 0.7071\n",
            "Early stopping:  0.024727388473617523\n",
            "Epoch: 047, Loss: 0.7109, Train: 0.8043, Test: 0.7077\n",
            "Early stopping:  0.024388198070121977\n",
            "Epoch: 048, Loss: 0.6960, Train: 0.8095, Test: 0.7105\n",
            "Early stopping:  0.02401587034365974\n",
            "Epoch: 049, Loss: 0.6815, Train: 0.8127, Test: 0.7085\n",
            "Early stopping:  0.02363433372927829\n",
            "Epoch: 050, Loss: 0.6670, Train: 0.8170, Test: 0.7122\n",
            "Early stopping:  0.023307592773887654\n",
            "Epoch: 051, Loss: 0.6528, Train: 0.8210, Test: 0.7151\n",
            "Early stopping:  0.022988105175192026\n",
            "Epoch: 052, Loss: 0.6388, Train: 0.8236, Test: 0.7210\n",
            "Early stopping:  0.02262814235857434\n",
            "Epoch: 053, Loss: 0.6254, Train: 0.8295, Test: 0.7216\n",
            "Early stopping:  0.022201479138741008\n",
            "Epoch: 054, Loss: 0.6133, Train: 0.8276, Test: 0.7224\n",
            "Early stopping:  0.021325198778517223\n",
            "Epoch: 055, Loss: 0.6048, Train: 0.8391, Test: 0.7233\n",
            "Early stopping:  0.01926749710772918\n",
            "Epoch: 056, Loss: 0.5946, Train: 0.8386, Test: 0.7278\n",
            "Early stopping:  0.017290461921906485\n",
            "Epoch: 057, Loss: 0.5779, Train: 0.8469, Test: 0.7309\n",
            "Early stopping:  0.018095482450076732\n",
            "Epoch: 058, Loss: 0.5629, Train: 0.8514, Test: 0.7304\n",
            "Early stopping:  0.02039239660682376\n",
            "Epoch: 059, Loss: 0.5560, Train: 0.8478, Test: 0.7306\n",
            "Early stopping:  0.020625460254062918\n",
            "Epoch: 060, Loss: 0.5467, Train: 0.8595, Test: 0.7360\n",
            "Early stopping:  0.018915984497268296\n",
            "Epoch: 061, Loss: 0.5313, Train: 0.8648, Test: 0.7346\n",
            "Early stopping:  0.01745709863273408\n",
            "Epoch: 062, Loss: 0.5206, Train: 0.8584, Test: 0.7358\n",
            "Early stopping:  0.01743533889473066\n",
            "Epoch: 063, Loss: 0.5143, Train: 0.8741, Test: 0.7380\n",
            "Early stopping:  0.01746615586925475\n",
            "Epoch: 064, Loss: 0.5040, Train: 0.8722, Test: 0.7394\n",
            "Early stopping:  0.016350575344476533\n",
            "Epoch: 065, Loss: 0.4910, Train: 0.8752, Test: 0.7440\n",
            "Early stopping:  0.01543459034913872\n",
            "Epoch: 066, Loss: 0.4815, Train: 0.8871, Test: 0.7457\n",
            "Early stopping:  0.016124477962014074\n",
            "Epoch: 067, Loss: 0.4750, Train: 0.8783, Test: 0.7420\n",
            "Early stopping:  0.016082116287784283\n",
            "Epoch: 068, Loss: 0.4674, Train: 0.8930, Test: 0.7479\n",
            "Early stopping:  0.014251436263365683\n",
            "Epoch: 069, Loss: 0.4567, Train: 0.8902, Test: 0.7479\n",
            "Early stopping:  0.013131148377178733\n",
            "Epoch: 070, Loss: 0.4467, Train: 0.8970, Test: 0.7479\n",
            "Early stopping:  0.013972938014937077\n",
            "Epoch: 071, Loss: 0.4376, Train: 0.9021, Test: 0.7499\n",
            "Early stopping:  0.015121259030433959\n",
            "Epoch: 072, Loss: 0.4302, Train: 0.8962, Test: 0.7471\n",
            "Early stopping:  0.014829510928173398\n",
            "Epoch: 073, Loss: 0.4251, Train: 0.9066, Test: 0.7502\n",
            "Early stopping:  0.012702172015028107\n",
            "Epoch: 074, Loss: 0.4198, Train: 0.8992, Test: 0.7465\n",
            "Early stopping:  0.010574687465412227\n",
            "Epoch: 075, Loss: 0.4144, Train: 0.9134, Test: 0.7505\n",
            "Early stopping:  0.009009120960947643\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.80      0.76       379\n",
            "         capital_goods       0.68      0.64      0.66       254\n",
            "conglomerates_industry       1.00      0.57      0.73        40\n",
            "     consumer_cyclical       0.68      0.73      0.70       396\n",
            " consumer_non-cyclical       0.74      0.68      0.71       223\n",
            "                energy       0.81      0.79      0.80       141\n",
            "             financial       0.81      0.79      0.80       384\n",
            "            healthcare       0.80      0.77      0.78       159\n",
            "              services       0.75      0.77      0.76      1038\n",
            "            technology       0.68      0.59      0.63       198\n",
            "        transportation       0.81      0.84      0.82       202\n",
            "             utilities       0.85      0.81      0.83       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.78      0.73      0.75      3527\n",
            "          weighted avg       0.75      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4719, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2550, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15336597541016972\n",
            "Epoch: 003, Loss: 2.1988, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.144187766029051\n",
            "Epoch: 004, Loss: 2.1430, Train: 0.2974, Test: 0.2954\n",
            "Early stopping:  0.14393201993873297\n",
            "Epoch: 005, Loss: 2.0702, Train: 0.3314, Test: 0.3280\n",
            "Early stopping:  0.15263169891315936\n",
            "Epoch: 006, Loss: 2.0217, Train: 0.3692, Test: 0.3598\n",
            "Early stopping:  0.09425666915790706\n",
            "Epoch: 007, Loss: 1.9767, Train: 0.3837, Test: 0.3697\n",
            "Early stopping:  0.08975888491368669\n",
            "Epoch: 008, Loss: 1.9175, Train: 0.3837, Test: 0.3703\n",
            "Early stopping:  0.08639310018769263\n",
            "Epoch: 009, Loss: 1.8535, Train: 0.3875, Test: 0.3791\n",
            "Early stopping:  0.08522714399207973\n",
            "Epoch: 010, Loss: 1.7950, Train: 0.4074, Test: 0.3958\n",
            "Early stopping:  0.09132253730748774\n",
            "Epoch: 011, Loss: 1.7372, Train: 0.4346, Test: 0.4225\n",
            "Early stopping:  0.0951200225725712\n",
            "Epoch: 012, Loss: 1.6755, Train: 0.4679, Test: 0.4497\n",
            "Early stopping:  0.0949216065394312\n",
            "Epoch: 013, Loss: 1.6157, Train: 0.4900, Test: 0.4780\n",
            "Early stopping:  0.09411437864885028\n",
            "Epoch: 014, Loss: 1.5605, Train: 0.5181, Test: 0.5004\n",
            "Early stopping:  0.09338704829533391\n",
            "Epoch: 015, Loss: 1.5066, Train: 0.5405, Test: 0.5177\n",
            "Early stopping:  0.09117281816784202\n",
            "Epoch: 016, Loss: 1.4543, Train: 0.5503, Test: 0.5259\n",
            "Early stopping:  0.0872217800947196\n",
            "Epoch: 017, Loss: 1.4064, Train: 0.5620, Test: 0.5364\n",
            "Early stopping:  0.08298213335277474\n",
            "Epoch: 018, Loss: 1.3614, Train: 0.5733, Test: 0.5483\n",
            "Early stopping:  0.0788397996671563\n",
            "Epoch: 019, Loss: 1.3195, Train: 0.5845, Test: 0.5566\n",
            "Early stopping:  0.07391469254530207\n",
            "Epoch: 020, Loss: 1.2812, Train: 0.5983, Test: 0.5710\n",
            "Early stopping:  0.0685484471362665\n",
            "Epoch: 021, Loss: 1.2435, Train: 0.6106, Test: 0.5872\n",
            "Early stopping:  0.06424194121066423\n",
            "Epoch: 022, Loss: 1.2100, Train: 0.6233, Test: 0.5985\n",
            "Early stopping:  0.05993037784284442\n",
            "Epoch: 023, Loss: 1.1797, Train: 0.6316, Test: 0.6093\n",
            "Early stopping:  0.05552213390646815\n",
            "Epoch: 024, Loss: 1.1486, Train: 0.6408, Test: 0.6150\n",
            "Early stopping:  0.05208271526758333\n",
            "Epoch: 025, Loss: 1.1203, Train: 0.6497, Test: 0.6223\n",
            "Early stopping:  0.04869535245155589\n",
            "Epoch: 026, Loss: 1.0946, Train: 0.6560, Test: 0.6266\n",
            "Early stopping:  0.04594090562260448\n",
            "Epoch: 027, Loss: 1.0689, Train: 0.6645, Test: 0.6348\n",
            "Early stopping:  0.04364326059066066\n",
            "Epoch: 028, Loss: 1.0452, Train: 0.6766, Test: 0.6425\n",
            "Early stopping:  0.04083555802370625\n",
            "Epoch: 029, Loss: 1.0227, Train: 0.6864, Test: 0.6490\n",
            "Early stopping:  0.038682423420365084\n",
            "Epoch: 030, Loss: 0.9998, Train: 0.6972, Test: 0.6538\n",
            "Early stopping:  0.0372693477512085\n",
            "Epoch: 031, Loss: 0.9776, Train: 0.7076, Test: 0.6583\n",
            "Early stopping:  0.036046522338580016\n",
            "Epoch: 032, Loss: 0.9567, Train: 0.7174, Test: 0.6643\n",
            "Early stopping:  0.03514899976195112\n",
            "Epoch: 033, Loss: 0.9350, Train: 0.7244, Test: 0.6683\n",
            "Early stopping:  0.03457270567281416\n",
            "Epoch: 034, Loss: 0.9151, Train: 0.7308, Test: 0.6728\n",
            "Early stopping:  0.03352845656081066\n",
            "Epoch: 035, Loss: 0.8958, Train: 0.7395, Test: 0.6756\n",
            "Early stopping:  0.032426489632893095\n",
            "Epoch: 036, Loss: 0.8766, Train: 0.7437, Test: 0.6759\n",
            "Early stopping:  0.031516138056444105\n",
            "Epoch: 037, Loss: 0.8587, Train: 0.7524, Test: 0.6802\n",
            "Early stopping:  0.030221649712213756\n",
            "Epoch: 038, Loss: 0.8409, Train: 0.7618, Test: 0.6847\n",
            "Early stopping:  0.02933934647817233\n",
            "Epoch: 039, Loss: 0.8234, Train: 0.7664, Test: 0.6859\n",
            "Early stopping:  0.028554452172496442\n",
            "Epoch: 040, Loss: 0.8072, Train: 0.7703, Test: 0.6893\n",
            "Early stopping:  0.027546190137277996\n",
            "Epoch: 041, Loss: 0.7901, Train: 0.7756, Test: 0.6952\n",
            "Early stopping:  0.027034270467362172\n",
            "Epoch: 042, Loss: 0.7739, Train: 0.7817, Test: 0.6986\n",
            "Early stopping:  0.0264765663181567\n",
            "Epoch: 043, Loss: 0.7578, Train: 0.7877, Test: 0.7029\n",
            "Early stopping:  0.02600305554908822\n",
            "Epoch: 044, Loss: 0.7414, Train: 0.7926, Test: 0.7063\n",
            "Early stopping:  0.025899056295158516\n",
            "Epoch: 045, Loss: 0.7258, Train: 0.7966, Test: 0.7091\n",
            "Early stopping:  0.02546956491915824\n",
            "Epoch: 046, Loss: 0.7102, Train: 0.8030, Test: 0.7156\n",
            "Early stopping:  0.025196170527287415\n",
            "Epoch: 047, Loss: 0.6946, Train: 0.8079, Test: 0.7176\n",
            "Early stopping:  0.024921040315392502\n",
            "Epoch: 048, Loss: 0.6794, Train: 0.8127, Test: 0.7210\n",
            "Early stopping:  0.02453766011525804\n",
            "Epoch: 049, Loss: 0.6640, Train: 0.8189, Test: 0.7230\n",
            "Early stopping:  0.02439723214118733\n",
            "Epoch: 050, Loss: 0.6489, Train: 0.8219, Test: 0.7275\n",
            "Early stopping:  0.024211267030750376\n",
            "Epoch: 051, Loss: 0.6342, Train: 0.8287, Test: 0.7289\n",
            "Early stopping:  0.023927269424079287\n",
            "Epoch: 052, Loss: 0.6208, Train: 0.8210, Test: 0.7233\n",
            "Early stopping:  0.023271173469772068\n",
            "Epoch: 053, Loss: 0.6141, Train: 0.8308, Test: 0.7190\n",
            "Early stopping:  0.0204177333839238\n",
            "Epoch: 054, Loss: 0.6099, Train: 0.8302, Test: 0.7264\n",
            "Early stopping:  0.015965406311165043\n",
            "Epoch: 055, Loss: 0.5874, Train: 0.8416, Test: 0.7338\n",
            "Early stopping:  0.017137212978913487\n",
            "Epoch: 056, Loss: 0.5706, Train: 0.8482, Test: 0.7318\n",
            "Early stopping:  0.020912033819778714\n",
            "Epoch: 057, Loss: 0.5690, Train: 0.8533, Test: 0.7360\n",
            "Early stopping:  0.02122153069721238\n",
            "Epoch: 058, Loss: 0.5470, Train: 0.8512, Test: 0.7338\n",
            "Early stopping:  0.02342050705281161\n",
            "Epoch: 059, Loss: 0.5409, Train: 0.8611, Test: 0.7363\n",
            "Early stopping:  0.01890558647083027\n",
            "Epoch: 060, Loss: 0.5298, Train: 0.8647, Test: 0.7409\n",
            "Early stopping:  0.017845641442301913\n",
            "Epoch: 061, Loss: 0.5150, Train: 0.8609, Test: 0.7363\n",
            "Early stopping:  0.020130184420225412\n",
            "Epoch: 062, Loss: 0.5103, Train: 0.8720, Test: 0.7437\n",
            "Early stopping:  0.01591641648676182\n",
            "Epoch: 063, Loss: 0.4943, Train: 0.8766, Test: 0.7403\n",
            "Early stopping:  0.017984686451844676\n",
            "Epoch: 064, Loss: 0.4893, Train: 0.8766, Test: 0.7406\n",
            "Early stopping:  0.016319717101144552\n",
            "Epoch: 065, Loss: 0.4777, Train: 0.8849, Test: 0.7414\n",
            "Early stopping:  0.01534216713973439\n",
            "Epoch: 066, Loss: 0.4658, Train: 0.8870, Test: 0.7411\n",
            "Early stopping:  0.01688759504507027\n",
            "Epoch: 067, Loss: 0.4606, Train: 0.8940, Test: 0.7460\n",
            "Early stopping:  0.014524446393962956\n",
            "Epoch: 068, Loss: 0.4465, Train: 0.8892, Test: 0.7417\n",
            "Early stopping:  0.01634058464264489\n",
            "Epoch: 069, Loss: 0.4405, Train: 0.8987, Test: 0.7468\n",
            "Early stopping:  0.01493265347299586\n",
            "Epoch: 070, Loss: 0.4302, Train: 0.9015, Test: 0.7477\n",
            "Early stopping:  0.014558984041191611\n",
            "Epoch: 071, Loss: 0.4193, Train: 0.9021, Test: 0.7440\n",
            "Early stopping:  0.01572416344510304\n",
            "Epoch: 072, Loss: 0.4136, Train: 0.9096, Test: 0.7496\n",
            "Early stopping:  0.013811740818998396\n",
            "Epoch: 073, Loss: 0.4025, Train: 0.9140, Test: 0.7505\n",
            "Early stopping:  0.01467989398309973\n",
            "Epoch: 074, Loss: 0.3956, Train: 0.9138, Test: 0.7491\n",
            "Early stopping:  0.013653849016977668\n",
            "Epoch: 075, Loss: 0.3886, Train: 0.9172, Test: 0.7519\n",
            "Early stopping:  0.012622213725339372\n",
            "Epoch: 076, Loss: 0.3783, Train: 0.9217, Test: 0.7528\n",
            "Early stopping:  0.01344163986613396\n",
            "Epoch: 077, Loss: 0.3721, Train: 0.9195, Test: 0.7494\n",
            "Early stopping:  0.012396707093324407\n",
            "Epoch: 078, Loss: 0.3640, Train: 0.9272, Test: 0.7533\n",
            "Early stopping:  0.012626219752184975\n",
            "Epoch: 079, Loss: 0.3545, Train: 0.9280, Test: 0.7545\n",
            "Early stopping:  0.013087448456221363\n",
            "Epoch: 080, Loss: 0.3487, Train: 0.9302, Test: 0.7542\n",
            "Early stopping:  0.01218314203974171\n",
            "Epoch: 081, Loss: 0.3414, Train: 0.9310, Test: 0.7536\n",
            "Early stopping:  0.012179759833728841\n",
            "Epoch: 082, Loss: 0.3332, Train: 0.9384, Test: 0.7508\n",
            "Early stopping:  0.011849147669685376\n",
            "Epoch: 083, Loss: 0.3282, Train: 0.9302, Test: 0.7525\n",
            "Early stopping:  0.010780733411470357\n",
            "Epoch: 084, Loss: 0.3238, Train: 0.9403, Test: 0.7496\n",
            "Early stopping:  0.010049601946643354\n",
            "Epoch: 085, Loss: 0.3188, Train: 0.9312, Test: 0.7482\n",
            "Early stopping:  0.008709835986878174\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.73      0.75       379\n",
            "         capital_goods       0.76      0.59      0.67       254\n",
            "conglomerates_industry       0.96      0.62      0.76        40\n",
            "     consumer_cyclical       0.66      0.76      0.71       396\n",
            " consumer_non-cyclical       0.79      0.67      0.72       223\n",
            "                energy       0.88      0.76      0.81       141\n",
            "             financial       0.85      0.71      0.77       384\n",
            "            healthcare       0.83      0.77      0.80       159\n",
            "              services       0.69      0.84      0.76      1038\n",
            "            technology       0.77      0.58      0.66       198\n",
            "        transportation       0.88      0.80      0.83       202\n",
            "             utilities       0.85      0.76      0.80       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.81      0.72      0.75      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4684, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2552, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15070886949264892\n",
            "Epoch: 003, Loss: 2.1784, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1502163155840749\n",
            "Epoch: 004, Loss: 2.1325, Train: 0.2974, Test: 0.2983\n",
            "Early stopping:  0.148705184757802\n",
            "Epoch: 005, Loss: 2.0725, Train: 0.3405, Test: 0.3351\n",
            "Early stopping:  0.15333817479674258\n",
            "Epoch: 006, Loss: 2.0247, Train: 0.3830, Test: 0.3745\n",
            "Early stopping:  0.08999138321973643\n",
            "Epoch: 007, Loss: 1.9698, Train: 0.3960, Test: 0.3910\n",
            "Early stopping:  0.08308518757454837\n",
            "Epoch: 008, Loss: 1.9063, Train: 0.3947, Test: 0.3893\n",
            "Early stopping:  0.08787067822453268\n",
            "Epoch: 009, Loss: 1.8432, Train: 0.3975, Test: 0.3898\n",
            "Early stopping:  0.0913896921365484\n",
            "Epoch: 010, Loss: 1.7854, Train: 0.4112, Test: 0.4029\n",
            "Early stopping:  0.09572116317472172\n",
            "Epoch: 011, Loss: 1.7292, Train: 0.4361, Test: 0.4273\n",
            "Early stopping:  0.09524001866317576\n",
            "Epoch: 012, Loss: 1.6706, Train: 0.4660, Test: 0.4485\n",
            "Early stopping:  0.09259338915803676\n",
            "Epoch: 013, Loss: 1.6131, Train: 0.4892, Test: 0.4687\n",
            "Early stopping:  0.09093291499249381\n",
            "Epoch: 014, Loss: 1.5599, Train: 0.5049, Test: 0.4902\n",
            "Early stopping:  0.08967668324841542\n",
            "Epoch: 015, Loss: 1.5086, Train: 0.5295, Test: 0.5078\n",
            "Early stopping:  0.08728161538622027\n",
            "Epoch: 016, Loss: 1.4580, Train: 0.5488, Test: 0.5223\n",
            "Early stopping:  0.08375837903763418\n",
            "Epoch: 017, Loss: 1.4097, Train: 0.5624, Test: 0.5376\n",
            "Early stopping:  0.08042367647436399\n",
            "Epoch: 018, Loss: 1.3642, Train: 0.5803, Test: 0.5588\n",
            "Early stopping:  0.07752530944968021\n",
            "Epoch: 019, Loss: 1.3205, Train: 0.5966, Test: 0.5758\n",
            "Early stopping:  0.07434446435434423\n",
            "Epoch: 020, Loss: 1.2798, Train: 0.6112, Test: 0.5931\n",
            "Early stopping:  0.0704920927425184\n",
            "Epoch: 021, Loss: 1.2426, Train: 0.6189, Test: 0.5982\n",
            "Early stopping:  0.06625076127486934\n",
            "Epoch: 022, Loss: 1.2071, Train: 0.6255, Test: 0.6042\n",
            "Early stopping:  0.06206290286443965\n",
            "Epoch: 023, Loss: 1.1728, Train: 0.6335, Test: 0.6099\n",
            "Early stopping:  0.05824942079091781\n",
            "Epoch: 024, Loss: 1.1409, Train: 0.6452, Test: 0.6153\n",
            "Early stopping:  0.05497481765895928\n",
            "Epoch: 025, Loss: 1.1126, Train: 0.6582, Test: 0.6238\n",
            "Early stopping:  0.05162706129075046\n",
            "Epoch: 026, Loss: 1.0866, Train: 0.6667, Test: 0.6291\n",
            "Early stopping:  0.04768843812576907\n",
            "Epoch: 027, Loss: 1.0603, Train: 0.6756, Test: 0.6360\n",
            "Early stopping:  0.04418595410311715\n",
            "Epoch: 028, Loss: 1.0352, Train: 0.6817, Test: 0.6425\n",
            "Early stopping:  0.041705586732207874\n",
            "Epoch: 029, Loss: 1.0121, Train: 0.6890, Test: 0.6490\n",
            "Early stopping:  0.03991867782294886\n",
            "Epoch: 030, Loss: 0.9886, Train: 0.7023, Test: 0.6538\n",
            "Early stopping:  0.038646076637262654\n",
            "Epoch: 031, Loss: 0.9658, Train: 0.7095, Test: 0.6581\n",
            "Early stopping:  0.03725478378707434\n",
            "Epoch: 032, Loss: 0.9439, Train: 0.7178, Test: 0.6643\n",
            "Early stopping:  0.03620467469060591\n",
            "Epoch: 033, Loss: 0.9224, Train: 0.7246, Test: 0.6663\n",
            "Early stopping:  0.03544438673233808\n",
            "Epoch: 034, Loss: 0.9022, Train: 0.7319, Test: 0.6666\n",
            "Early stopping:  0.03419775768746042\n",
            "Epoch: 035, Loss: 0.8826, Train: 0.7403, Test: 0.6717\n",
            "Early stopping:  0.03291681309929433\n",
            "Epoch: 036, Loss: 0.8640, Train: 0.7457, Test: 0.6754\n",
            "Early stopping:  0.03154960369739956\n",
            "Epoch: 037, Loss: 0.8467, Train: 0.7505, Test: 0.6799\n",
            "Early stopping:  0.029978079241188758\n",
            "Epoch: 038, Loss: 0.8303, Train: 0.7575, Test: 0.6819\n",
            "Early stopping:  0.028421577087725133\n",
            "Epoch: 039, Loss: 0.8140, Train: 0.7622, Test: 0.6867\n",
            "Early stopping:  0.027057026693765748\n",
            "Epoch: 040, Loss: 0.7983, Train: 0.7681, Test: 0.6910\n",
            "Early stopping:  0.02597570393952879\n",
            "Epoch: 041, Loss: 0.7826, Train: 0.7720, Test: 0.6949\n",
            "Early stopping:  0.02535195843079981\n",
            "Epoch: 042, Loss: 0.7668, Train: 0.7786, Test: 0.6983\n",
            "Early stopping:  0.025039174108863402\n",
            "Epoch: 043, Loss: 0.7512, Train: 0.7866, Test: 0.6997\n",
            "Early stopping:  0.024802002325692938\n",
            "Epoch: 044, Loss: 0.7361, Train: 0.7917, Test: 0.7051\n",
            "Early stopping:  0.02462354026695434\n",
            "Epoch: 045, Loss: 0.7210, Train: 0.7968, Test: 0.7080\n",
            "Early stopping:  0.024336536812773348\n",
            "Epoch: 046, Loss: 0.7061, Train: 0.8013, Test: 0.7091\n",
            "Early stopping:  0.023984739969612444\n",
            "Epoch: 047, Loss: 0.6915, Train: 0.8059, Test: 0.7128\n",
            "Early stopping:  0.023617934506541625\n",
            "Epoch: 048, Loss: 0.6770, Train: 0.8085, Test: 0.7170\n",
            "Early stopping:  0.023342839335659706\n",
            "Epoch: 049, Loss: 0.6627, Train: 0.8146, Test: 0.7179\n",
            "Early stopping:  0.023050939145698477\n",
            "Epoch: 050, Loss: 0.6483, Train: 0.8180, Test: 0.7213\n",
            "Early stopping:  0.02283929699082459\n",
            "Epoch: 051, Loss: 0.6344, Train: 0.8223, Test: 0.7255\n",
            "Early stopping:  0.02261466963958541\n",
            "Epoch: 052, Loss: 0.6210, Train: 0.8297, Test: 0.7304\n",
            "Early stopping:  0.02218778090821636\n",
            "Epoch: 053, Loss: 0.6103, Train: 0.8223, Test: 0.7216\n",
            "Early stopping:  0.02091984837238163\n",
            "Epoch: 054, Loss: 0.6085, Train: 0.8333, Test: 0.7278\n",
            "Early stopping:  0.016852201737857165\n",
            "Epoch: 055, Loss: 0.6043, Train: 0.8350, Test: 0.7324\n",
            "Early stopping:  0.012119335838236284\n",
            "Epoch: 056, Loss: 0.5761, Train: 0.8403, Test: 0.7346\n",
            "Early stopping:  0.016763979226842144\n",
            "Epoch: 057, Loss: 0.5651, Train: 0.8484, Test: 0.7346\n",
            "Early stopping:  0.020771335492938973\n",
            "Epoch: 058, Loss: 0.5638, Train: 0.8518, Test: 0.7375\n",
            "Early stopping:  0.021414191271296495\n",
            "Epoch: 059, Loss: 0.5400, Train: 0.8503, Test: 0.7355\n",
            "Early stopping:  0.023311578519348647\n",
            "Epoch: 060, Loss: 0.5378, Train: 0.8645, Test: 0.7445\n",
            "Early stopping:  0.016837200442344463\n",
            "Epoch: 061, Loss: 0.5256, Train: 0.8650, Test: 0.7420\n",
            "Early stopping:  0.017333158022060466\n",
            "Epoch: 062, Loss: 0.5117, Train: 0.8616, Test: 0.7409\n",
            "Early stopping:  0.019309503337058574\n",
            "Epoch: 063, Loss: 0.5090, Train: 0.8733, Test: 0.7440\n",
            "Early stopping:  0.01434340714840452\n",
            "Epoch: 064, Loss: 0.4926, Train: 0.8769, Test: 0.7431\n",
            "Early stopping:  0.017183938483926558\n",
            "Epoch: 065, Loss: 0.4879, Train: 0.8715, Test: 0.7440\n",
            "Early stopping:  0.015265182572807496\n",
            "Epoch: 066, Loss: 0.4781, Train: 0.8817, Test: 0.7491\n",
            "Early stopping:  0.014270563767124024\n",
            "Epoch: 067, Loss: 0.4665, Train: 0.8853, Test: 0.7479\n",
            "Early stopping:  0.015922323023215827\n",
            "Epoch: 068, Loss: 0.4621, Train: 0.8856, Test: 0.7457\n",
            "Early stopping:  0.013154094190687353\n",
            "Epoch: 069, Loss: 0.4500, Train: 0.8904, Test: 0.7465\n",
            "Early stopping:  0.01460734734653354\n",
            "Epoch: 070, Loss: 0.4429, Train: 0.8955, Test: 0.7496\n",
            "Early stopping:  0.013866233398538848\n",
            "Epoch: 071, Loss: 0.4353, Train: 0.8957, Test: 0.7494\n",
            "Early stopping:  0.013013865266489703\n",
            "Epoch: 072, Loss: 0.4252, Train: 0.9019, Test: 0.7477\n",
            "Early stopping:  0.01406111876794101\n",
            "Epoch: 073, Loss: 0.4192, Train: 0.9051, Test: 0.7539\n",
            "Early stopping:  0.012573064303831243\n",
            "Epoch: 074, Loss: 0.4097, Train: 0.9051, Test: 0.7522\n",
            "Early stopping:  0.013079151237538543\n",
            "Epoch: 075, Loss: 0.4024, Train: 0.9130, Test: 0.7547\n",
            "Early stopping:  0.012911357525462026\n",
            "Epoch: 076, Loss: 0.3958, Train: 0.9121, Test: 0.7562\n",
            "Early stopping:  0.011966544584228142\n",
            "Epoch: 077, Loss: 0.3857, Train: 0.9125, Test: 0.7545\n",
            "Early stopping:  0.012796695732769491\n",
            "Epoch: 078, Loss: 0.3798, Train: 0.9204, Test: 0.7579\n",
            "Early stopping:  0.012102603670424669\n",
            "Epoch: 079, Loss: 0.3735, Train: 0.9183, Test: 0.7562\n",
            "Early stopping:  0.011698990957595176\n",
            "Epoch: 080, Loss: 0.3645, Train: 0.9217, Test: 0.7582\n",
            "Early stopping:  0.011874218523137895\n",
            "Epoch: 081, Loss: 0.3587, Train: 0.9251, Test: 0.7590\n",
            "Early stopping:  0.010980891425027582\n",
            "Epoch: 082, Loss: 0.3524, Train: 0.9293, Test: 0.7590\n",
            "Early stopping:  0.011047285588631032\n",
            "Epoch: 083, Loss: 0.3446, Train: 0.9259, Test: 0.7579\n",
            "Early stopping:  0.011100623128285544\n",
            "Epoch: 084, Loss: 0.3400, Train: 0.9367, Test: 0.7613\n",
            "Early stopping:  0.010014564516750549\n",
            "Epoch: 085, Loss: 0.3373, Train: 0.9189, Test: 0.7511\n",
            "Early stopping:  0.008863738835600985\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.83      0.74      0.79       379\n",
            "         capital_goods       0.70      0.61      0.65       254\n",
            "conglomerates_industry       1.00      0.57      0.73        40\n",
            "     consumer_cyclical       0.71      0.67      0.69       396\n",
            " consumer_non-cyclical       0.77      0.65      0.70       223\n",
            "                energy       0.87      0.73      0.80       141\n",
            "             financial       0.83      0.78      0.80       384\n",
            "            healthcare       0.87      0.75      0.80       159\n",
            "              services       0.67      0.86      0.75      1038\n",
            "            technology       0.82      0.58      0.68       198\n",
            "        transportation       0.86      0.80      0.83       202\n",
            "             utilities       0.85      0.77      0.81       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.82      0.71      0.75      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4782, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2691, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.1478567079602197\n",
            "Epoch: 003, Loss: 2.1678, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.1582840587327188\n",
            "Epoch: 004, Loss: 2.1542, Train: 0.2975, Test: 0.2957\n",
            "Early stopping:  0.14962476928635746\n",
            "Epoch: 005, Loss: 2.0845, Train: 0.3191, Test: 0.3147\n",
            "Early stopping:  0.1532085125145482\n",
            "Epoch: 006, Loss: 2.0142, Train: 0.3730, Test: 0.3675\n",
            "Early stopping:  0.09555161472344258\n",
            "Epoch: 007, Loss: 1.9656, Train: 0.3996, Test: 0.3918\n",
            "Early stopping:  0.08748698868547473\n",
            "Epoch: 008, Loss: 1.9182, Train: 0.4134, Test: 0.3984\n",
            "Early stopping:  0.09389482476680962\n",
            "Epoch: 009, Loss: 1.8586, Train: 0.4170, Test: 0.4020\n",
            "Early stopping:  0.08685353891714459\n",
            "Epoch: 010, Loss: 1.7932, Train: 0.4216, Test: 0.4083\n",
            "Early stopping:  0.0870369402380947\n",
            "Epoch: 011, Loss: 1.7328, Train: 0.4319, Test: 0.4247\n",
            "Early stopping:  0.09353053615635715\n",
            "Epoch: 012, Loss: 1.6784, Train: 0.4546, Test: 0.4443\n",
            "Early stopping:  0.09574874672979482\n",
            "Epoch: 013, Loss: 1.6240, Train: 0.4771, Test: 0.4729\n",
            "Early stopping:  0.09239181368583523\n",
            "Epoch: 014, Loss: 1.5675, Train: 0.4985, Test: 0.4902\n",
            "Early stopping:  0.08857838094566006\n",
            "Epoch: 015, Loss: 1.5110, Train: 0.5302, Test: 0.5146\n",
            "Early stopping:  0.08767049199569453\n",
            "Epoch: 016, Loss: 1.4601, Train: 0.5550, Test: 0.5379\n",
            "Early stopping:  0.08691856683460031\n",
            "Epoch: 017, Loss: 1.4160, Train: 0.5709, Test: 0.5520\n",
            "Early stopping:  0.08286300506738571\n",
            "Epoch: 018, Loss: 1.3706, Train: 0.5784, Test: 0.5634\n",
            "Early stopping:  0.07737657976746616\n",
            "Epoch: 019, Loss: 1.3238, Train: 0.5841, Test: 0.5600\n",
            "Early stopping:  0.07336116767028476\n",
            "Epoch: 020, Loss: 1.2832, Train: 0.5887, Test: 0.5605\n",
            "Early stopping:  0.07055328428325469\n",
            "Epoch: 021, Loss: 1.2504, Train: 0.5989, Test: 0.5690\n",
            "Early stopping:  0.06635509550142009\n",
            "Epoch: 022, Loss: 1.2183, Train: 0.6127, Test: 0.5861\n",
            "Early stopping:  0.06001208128232973\n",
            "Epoch: 023, Loss: 1.1860, Train: 0.6257, Test: 0.5994\n",
            "Early stopping:  0.05391155812703211\n",
            "Epoch: 024, Loss: 1.1588, Train: 0.6384, Test: 0.6096\n",
            "Early stopping:  0.04954078999910225\n",
            "Epoch: 025, Loss: 1.1349, Train: 0.6459, Test: 0.6153\n",
            "Early stopping:  0.04602134204814343\n",
            "Epoch: 026, Loss: 1.1095, Train: 0.6518, Test: 0.6187\n",
            "Early stopping:  0.042536773941597654\n",
            "Epoch: 027, Loss: 1.0835, Train: 0.6552, Test: 0.6229\n",
            "Early stopping:  0.04018636155167767\n",
            "Epoch: 028, Loss: 1.0612, Train: 0.6633, Test: 0.6283\n",
            "Early stopping:  0.038979396275979905\n",
            "Epoch: 029, Loss: 1.0401, Train: 0.6737, Test: 0.6331\n",
            "Early stopping:  0.03764901706202132\n",
            "Epoch: 030, Loss: 1.0173, Train: 0.6820, Test: 0.6405\n",
            "Early stopping:  0.03604939843533867\n",
            "Epoch: 031, Loss: 0.9961, Train: 0.6896, Test: 0.6436\n",
            "Early stopping:  0.03458751809408856\n",
            "Epoch: 032, Loss: 0.9766, Train: 0.7008, Test: 0.6544\n",
            "Early stopping:  0.033717716133370074\n",
            "Epoch: 033, Loss: 0.9568, Train: 0.7096, Test: 0.6586\n",
            "Early stopping:  0.03278646649005085\n",
            "Epoch: 034, Loss: 0.9376, Train: 0.7155, Test: 0.6623\n",
            "Early stopping:  0.03141981752043347\n",
            "Epoch: 035, Loss: 0.9196, Train: 0.7257, Test: 0.6666\n",
            "Early stopping:  0.030362893624205453\n",
            "Epoch: 036, Loss: 0.9019, Train: 0.7352, Test: 0.6737\n",
            "Early stopping:  0.029537608953700084\n",
            "Epoch: 037, Loss: 0.8849, Train: 0.7393, Test: 0.6742\n",
            "Early stopping:  0.028397782971526408\n",
            "Epoch: 038, Loss: 0.8687, Train: 0.7471, Test: 0.6790\n",
            "Early stopping:  0.02729640413289217\n",
            "Epoch: 039, Loss: 0.8518, Train: 0.7522, Test: 0.6819\n",
            "Early stopping:  0.026711697907688584\n",
            "Epoch: 040, Loss: 0.8357, Train: 0.7567, Test: 0.6856\n",
            "Early stopping:  0.026168657296724702\n",
            "Epoch: 041, Loss: 0.8202, Train: 0.7637, Test: 0.6893\n",
            "Early stopping:  0.025688266366765064\n",
            "Epoch: 042, Loss: 0.8044, Train: 0.7698, Test: 0.6893\n",
            "Early stopping:  0.025320095544073606\n",
            "Epoch: 043, Loss: 0.7893, Train: 0.7726, Test: 0.6927\n",
            "Early stopping:  0.024694832142014088\n",
            "Epoch: 044, Loss: 0.7744, Train: 0.7796, Test: 0.6915\n",
            "Early stopping:  0.02427385793223068\n",
            "Epoch: 045, Loss: 0.7598, Train: 0.7841, Test: 0.6938\n",
            "Early stopping:  0.02384181583321107\n",
            "Epoch: 046, Loss: 0.7456, Train: 0.7896, Test: 0.6975\n",
            "Early stopping:  0.023250697798509207\n",
            "Epoch: 047, Loss: 0.7311, Train: 0.7932, Test: 0.7014\n",
            "Early stopping:  0.022951608611508037\n",
            "Epoch: 048, Loss: 0.7169, Train: 0.7987, Test: 0.7043\n",
            "Early stopping:  0.022722079128299044\n",
            "Epoch: 049, Loss: 0.7024, Train: 0.8055, Test: 0.7100\n",
            "Early stopping:  0.022713291093110925\n",
            "Epoch: 050, Loss: 0.6880, Train: 0.8115, Test: 0.7134\n",
            "Early stopping:  0.022772411664192922\n",
            "Epoch: 051, Loss: 0.6736, Train: 0.8163, Test: 0.7159\n",
            "Early stopping:  0.022772034493007246\n",
            "Epoch: 052, Loss: 0.6594, Train: 0.8212, Test: 0.7187\n",
            "Early stopping:  0.022727340457029406\n",
            "Epoch: 053, Loss: 0.6452, Train: 0.8257, Test: 0.7210\n",
            "Early stopping:  0.022595750313346152\n",
            "Epoch: 054, Loss: 0.6312, Train: 0.8310, Test: 0.7244\n",
            "Early stopping:  0.022436871968802403\n",
            "Epoch: 055, Loss: 0.6177, Train: 0.8327, Test: 0.7250\n",
            "Early stopping:  0.022122721841467137\n",
            "Epoch: 056, Loss: 0.6051, Train: 0.8425, Test: 0.7267\n",
            "Early stopping:  0.02153335957300117\n",
            "Epoch: 057, Loss: 0.5947, Train: 0.8397, Test: 0.7289\n",
            "Early stopping:  0.020133120002457327\n",
            "Epoch: 058, Loss: 0.5861, Train: 0.8497, Test: 0.7289\n",
            "Early stopping:  0.017991210586796844\n",
            "Epoch: 059, Loss: 0.5693, Train: 0.8569, Test: 0.7349\n",
            "Early stopping:  0.018408540844520472\n",
            "Epoch: 060, Loss: 0.5537, Train: 0.8518, Test: 0.7369\n",
            "Early stopping:  0.020440360580775235\n",
            "Epoch: 061, Loss: 0.5448, Train: 0.8624, Test: 0.7341\n",
            "Early stopping:  0.021037306595719195\n",
            "Epoch: 062, Loss: 0.5339, Train: 0.8660, Test: 0.7414\n",
            "Early stopping:  0.020556761349076327\n",
            "Epoch: 063, Loss: 0.5188, Train: 0.8707, Test: 0.7403\n",
            "Early stopping:  0.01919253070176246\n",
            "Epoch: 064, Loss: 0.5077, Train: 0.8762, Test: 0.7400\n",
            "Early stopping:  0.018725139742095404\n",
            "Epoch: 065, Loss: 0.5001, Train: 0.8752, Test: 0.7434\n",
            "Early stopping:  0.018361826002073176\n",
            "Epoch: 066, Loss: 0.4886, Train: 0.8858, Test: 0.7491\n",
            "Early stopping:  0.01739503187578338\n",
            "Epoch: 067, Loss: 0.4748, Train: 0.8860, Test: 0.7443\n",
            "Early stopping:  0.01700537587948233\n",
            "Epoch: 068, Loss: 0.4666, Train: 0.8866, Test: 0.7479\n",
            "Early stopping:  0.01706782187677984\n",
            "Epoch: 069, Loss: 0.4595, Train: 0.8924, Test: 0.7477\n",
            "Early stopping:  0.016450369181763555\n",
            "Epoch: 070, Loss: 0.4470, Train: 0.8998, Test: 0.7505\n",
            "Early stopping:  0.015674984124393165\n",
            "Epoch: 071, Loss: 0.4360, Train: 0.8930, Test: 0.7454\n",
            "Early stopping:  0.015466520008512194\n",
            "Epoch: 072, Loss: 0.4297, Train: 0.9079, Test: 0.7522\n",
            "Early stopping:  0.015467368016035493\n",
            "Epoch: 073, Loss: 0.4221, Train: 0.9021, Test: 0.7502\n",
            "Early stopping:  0.01471853494204176\n",
            "Epoch: 074, Loss: 0.4090, Train: 0.9115, Test: 0.7550\n",
            "Early stopping:  0.01434082989782817\n",
            "Epoch: 075, Loss: 0.3986, Train: 0.9172, Test: 0.7536\n",
            "Early stopping:  0.01527001101208447\n",
            "Epoch: 076, Loss: 0.3935, Train: 0.9132, Test: 0.7542\n",
            "Early stopping:  0.015314395295164727\n",
            "Epoch: 077, Loss: 0.3849, Train: 0.9229, Test: 0.7567\n",
            "Early stopping:  0.014386830764371486\n",
            "Epoch: 078, Loss: 0.3729, Train: 0.9284, Test: 0.7624\n",
            "Early stopping:  0.013679281124233732\n",
            "Epoch: 079, Loss: 0.3661, Train: 0.9189, Test: 0.7513\n",
            "Early stopping:  0.013631445387480378\n",
            "Epoch: 080, Loss: 0.3614, Train: 0.9304, Test: 0.7604\n",
            "Early stopping:  0.013266111016316601\n",
            "Epoch: 081, Loss: 0.3536, Train: 0.9259, Test: 0.7567\n",
            "Early stopping:  0.011873161234629396\n",
            "Epoch: 082, Loss: 0.3486, Train: 0.9195, Test: 0.7584\n",
            "Early stopping:  0.009668422708466963\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.77      0.78      0.77       379\n",
            "         capital_goods       0.71      0.62      0.67       254\n",
            "conglomerates_industry       0.97      0.70      0.81        40\n",
            "     consumer_cyclical       0.65      0.75      0.70       396\n",
            " consumer_non-cyclical       0.76      0.64      0.69       223\n",
            "                energy       0.84      0.76      0.80       141\n",
            "             financial       0.88      0.72      0.79       384\n",
            "            healthcare       0.83      0.76      0.79       159\n",
            "              services       0.73      0.83      0.77      1038\n",
            "            technology       0.73      0.69      0.71       198\n",
            "        transportation       0.87      0.81      0.84       202\n",
            "             utilities       0.82      0.79      0.80       113\n",
            "\n",
            "              accuracy                           0.76      3527\n",
            "             macro avg       0.80      0.74      0.76      3527\n",
            "          weighted avg       0.77      0.76      0.76      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5159, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2873, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16159186012773794\n",
            "Epoch: 003, Loss: 2.1680, Train: 0.2947, Test: 0.2943\n",
            "Early stopping:  0.1767747073812384\n",
            "Epoch: 004, Loss: 2.1326, Train: 0.3066, Test: 0.3048\n",
            "Early stopping:  0.17310737884235444\n",
            "Epoch: 005, Loss: 2.0616, Train: 0.3446, Test: 0.3442\n",
            "Early stopping:  0.1779381418444492\n",
            "Epoch: 006, Loss: 1.9982, Train: 0.3917, Test: 0.3842\n",
            "Early stopping:  0.10982240527893719\n",
            "Epoch: 007, Loss: 1.9420, Train: 0.4002, Test: 0.3921\n",
            "Early stopping:  0.09311278276096124\n",
            "Epoch: 008, Loss: 1.8804, Train: 0.3955, Test: 0.3856\n",
            "Early stopping:  0.09876690845756546\n",
            "Epoch: 009, Loss: 1.8180, Train: 0.3987, Test: 0.3876\n",
            "Early stopping:  0.09569627926875254\n",
            "Epoch: 010, Loss: 1.7605, Train: 0.4147, Test: 0.4029\n",
            "Early stopping:  0.09478468026529277\n",
            "Epoch: 011, Loss: 1.7036, Train: 0.4408, Test: 0.4338\n",
            "Early stopping:  0.09434123257403931\n",
            "Epoch: 012, Loss: 1.6437, Train: 0.4745, Test: 0.4551\n",
            "Early stopping:  0.09293150742621069\n",
            "Epoch: 013, Loss: 1.5856, Train: 0.4921, Test: 0.4803\n",
            "Early stopping:  0.09196755291002884\n",
            "Epoch: 014, Loss: 1.5327, Train: 0.5195, Test: 0.5033\n",
            "Early stopping:  0.09073501238052939\n",
            "Epoch: 015, Loss: 1.4845, Train: 0.5433, Test: 0.5265\n",
            "Early stopping:  0.08693610632197367\n",
            "Epoch: 016, Loss: 1.4376, Train: 0.5550, Test: 0.5316\n",
            "Early stopping:  0.08123316379436041\n",
            "Epoch: 017, Loss: 1.3913, Train: 0.5635, Test: 0.5370\n",
            "Early stopping:  0.07649934077840863\n",
            "Epoch: 018, Loss: 1.3475, Train: 0.5786, Test: 0.5512\n",
            "Early stopping:  0.07329338166753918\n",
            "Epoch: 019, Loss: 1.3071, Train: 0.5907, Test: 0.5688\n",
            "Early stopping:  0.07036492009343083\n",
            "Epoch: 020, Loss: 1.2695, Train: 0.6055, Test: 0.5838\n",
            "Early stopping:  0.06655043904194553\n",
            "Epoch: 021, Loss: 1.2350, Train: 0.6170, Test: 0.5954\n",
            "Early stopping:  0.0618267130791291\n",
            "Epoch: 022, Loss: 1.2028, Train: 0.6267, Test: 0.5994\n",
            "Early stopping:  0.057237160161670776\n",
            "Epoch: 023, Loss: 1.1714, Train: 0.6374, Test: 0.6019\n",
            "Early stopping:  0.053485882094330695\n",
            "Epoch: 024, Loss: 1.1432, Train: 0.6410, Test: 0.6050\n",
            "Early stopping:  0.05000600479879315\n",
            "Epoch: 025, Loss: 1.1188, Train: 0.6457, Test: 0.6116\n",
            "Early stopping:  0.046242719585294856\n",
            "Epoch: 026, Loss: 1.0945, Train: 0.6561, Test: 0.6170\n",
            "Early stopping:  0.042629519905563816\n",
            "Epoch: 027, Loss: 1.0697, Train: 0.6647, Test: 0.6249\n",
            "Early stopping:  0.03987082080654671\n",
            "Epoch: 028, Loss: 1.0471, Train: 0.6722, Test: 0.6317\n",
            "Early stopping:  0.03814873704206907\n",
            "Epoch: 029, Loss: 1.0255, Train: 0.6796, Test: 0.6368\n",
            "Early stopping:  0.037010704264197\n",
            "Epoch: 030, Loss: 1.0031, Train: 0.6866, Test: 0.6416\n",
            "Early stopping:  0.03592516697431088\n",
            "Epoch: 031, Loss: 0.9815, Train: 0.6994, Test: 0.6462\n",
            "Early stopping:  0.03485474997025906\n",
            "Epoch: 032, Loss: 0.9609, Train: 0.7093, Test: 0.6527\n",
            "Early stopping:  0.034230471268572\n",
            "Epoch: 033, Loss: 0.9403, Train: 0.7172, Test: 0.6589\n",
            "Early stopping:  0.0336116005013474\n",
            "Epoch: 034, Loss: 0.9209, Train: 0.7236, Test: 0.6649\n",
            "Early stopping:  0.03251412879435536\n",
            "Epoch: 035, Loss: 0.9028, Train: 0.7319, Test: 0.6666\n",
            "Early stopping:  0.031243446061022903\n",
            "Epoch: 036, Loss: 0.8854, Train: 0.7388, Test: 0.6705\n",
            "Early stopping:  0.02981634166959128\n",
            "Epoch: 037, Loss: 0.8686, Train: 0.7448, Test: 0.6742\n",
            "Early stopping:  0.028308806921093\n",
            "Epoch: 038, Loss: 0.8521, Train: 0.7497, Test: 0.6742\n",
            "Early stopping:  0.02716890482436518\n",
            "Epoch: 039, Loss: 0.8369, Train: 0.7556, Test: 0.6768\n",
            "Early stopping:  0.02610559713785405\n",
            "Epoch: 040, Loss: 0.8217, Train: 0.7645, Test: 0.6807\n",
            "Early stopping:  0.025157172852106894\n",
            "Epoch: 041, Loss: 0.8065, Train: 0.7694, Test: 0.6864\n",
            "Early stopping:  0.024432883348164337\n",
            "Epoch: 042, Loss: 0.7922, Train: 0.7747, Test: 0.6907\n",
            "Early stopping:  0.023751763707501185\n",
            "Epoch: 043, Loss: 0.7775, Train: 0.7826, Test: 0.6944\n",
            "Early stopping:  0.02347346683223691\n",
            "Epoch: 044, Loss: 0.7629, Train: 0.7898, Test: 0.6961\n",
            "Early stopping:  0.02319544190617966\n",
            "Epoch: 045, Loss: 0.7486, Train: 0.7932, Test: 0.6992\n",
            "Early stopping:  0.022946238825419383\n",
            "Epoch: 046, Loss: 0.7344, Train: 0.7975, Test: 0.7023\n",
            "Early stopping:  0.022842099580773146\n",
            "Epoch: 047, Loss: 0.7204, Train: 0.8032, Test: 0.7048\n",
            "Early stopping:  0.02257081980636873\n",
            "Epoch: 048, Loss: 0.7064, Train: 0.8079, Test: 0.7094\n",
            "Early stopping:  0.022327263563196104\n",
            "Epoch: 049, Loss: 0.6927, Train: 0.8106, Test: 0.7142\n",
            "Early stopping:  0.02208866368221368\n",
            "Epoch: 050, Loss: 0.6791, Train: 0.8136, Test: 0.7156\n",
            "Early stopping:  0.02183974478183736\n",
            "Epoch: 051, Loss: 0.6656, Train: 0.8193, Test: 0.7179\n",
            "Early stopping:  0.021618736971357456\n",
            "Epoch: 052, Loss: 0.6523, Train: 0.8227, Test: 0.7187\n",
            "Early stopping:  0.021387986212502284\n",
            "Epoch: 053, Loss: 0.6390, Train: 0.8268, Test: 0.7216\n",
            "Early stopping:  0.021224774734996607\n",
            "Epoch: 054, Loss: 0.6260, Train: 0.8304, Test: 0.7241\n",
            "Early stopping:  0.02101477695522546\n",
            "Epoch: 055, Loss: 0.6130, Train: 0.8335, Test: 0.7275\n",
            "Early stopping:  0.020800635101848687\n",
            "Epoch: 056, Loss: 0.6001, Train: 0.8378, Test: 0.7261\n",
            "Early stopping:  0.02062354864293475\n",
            "Epoch: 057, Loss: 0.5874, Train: 0.8435, Test: 0.7295\n",
            "Early stopping:  0.020401656779651813\n",
            "Epoch: 058, Loss: 0.5751, Train: 0.8439, Test: 0.7309\n",
            "Early stopping:  0.02012639080147884\n",
            "Epoch: 059, Loss: 0.5640, Train: 0.8529, Test: 0.7281\n",
            "Early stopping:  0.01945639831481464\n",
            "Epoch: 060, Loss: 0.5548, Train: 0.8467, Test: 0.7281\n",
            "Early stopping:  0.01806703469093179\n",
            "Epoch: 061, Loss: 0.5474, Train: 0.8611, Test: 0.7306\n",
            "Early stopping:  0.01596057811864169\n",
            "Epoch: 062, Loss: 0.5352, Train: 0.8618, Test: 0.7366\n",
            "Early stopping:  0.015282372737130451\n",
            "Epoch: 063, Loss: 0.5183, Train: 0.8681, Test: 0.7363\n",
            "Early stopping:  0.017786954338637043\n",
            "Epoch: 064, Loss: 0.5079, Train: 0.8718, Test: 0.7329\n",
            "Early stopping:  0.019584974351854804\n",
            "Epoch: 065, Loss: 0.5012, Train: 0.8743, Test: 0.7386\n",
            "Early stopping:  0.019132259507036873\n",
            "Epoch: 066, Loss: 0.4899, Train: 0.8832, Test: 0.7417\n",
            "Early stopping:  0.01725434161873651\n",
            "Epoch: 067, Loss: 0.4777, Train: 0.8847, Test: 0.7437\n",
            "Early stopping:  0.015753688197716408\n",
            "Epoch: 068, Loss: 0.4694, Train: 0.8843, Test: 0.7443\n",
            "Early stopping:  0.015967581382242833\n",
            "Epoch: 069, Loss: 0.4603, Train: 0.8934, Test: 0.7465\n",
            "Early stopping:  0.01624243884907527\n",
            "Epoch: 070, Loss: 0.4502, Train: 0.8936, Test: 0.7471\n",
            "Early stopping:  0.015314519647822399\n",
            "Epoch: 071, Loss: 0.4423, Train: 0.8947, Test: 0.7468\n",
            "Early stopping:  0.01421660108269952\n",
            "Epoch: 072, Loss: 0.4339, Train: 0.9038, Test: 0.7522\n",
            "Early stopping:  0.014065012172051694\n",
            "Epoch: 073, Loss: 0.4231, Train: 0.9040, Test: 0.7522\n",
            "Early stopping:  0.01436495959373055\n",
            "Epoch: 074, Loss: 0.4155, Train: 0.9079, Test: 0.7474\n",
            "Early stopping:  0.014060798437574042\n",
            "Epoch: 075, Loss: 0.4096, Train: 0.9083, Test: 0.7536\n",
            "Early stopping:  0.013322423691006186\n",
            "Epoch: 076, Loss: 0.4004, Train: 0.9142, Test: 0.7525\n",
            "Early stopping:  0.012768086926945652\n",
            "Epoch: 077, Loss: 0.3901, Train: 0.9166, Test: 0.7519\n",
            "Early stopping:  0.012885080517095644\n",
            "Epoch: 078, Loss: 0.3829, Train: 0.9149, Test: 0.7536\n",
            "Early stopping:  0.01343569172168683\n",
            "Epoch: 079, Loss: 0.3773, Train: 0.9198, Test: 0.7502\n",
            "Early stopping:  0.0130987767372425\n",
            "Epoch: 080, Loss: 0.3697, Train: 0.9204, Test: 0.7533\n",
            "Early stopping:  0.01182396951622425\n",
            "Epoch: 081, Loss: 0.3610, Train: 0.9255, Test: 0.7528\n",
            "Early stopping:  0.011322625027317394\n",
            "Epoch: 082, Loss: 0.3541, Train: 0.9225, Test: 0.7488\n",
            "Early stopping:  0.011720026834209385\n",
            "Epoch: 083, Loss: 0.3501, Train: 0.9280, Test: 0.7477\n",
            "Early stopping:  0.011143542345390233\n",
            "Epoch: 084, Loss: 0.3471, Train: 0.9202, Test: 0.7494\n",
            "Early stopping:  0.009070960133130176\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.81      0.75      0.78       379\n",
            "         capital_goods       0.72      0.59      0.65       254\n",
            "conglomerates_industry       0.96      0.60      0.74        40\n",
            "     consumer_cyclical       0.73      0.71      0.72       396\n",
            " consumer_non-cyclical       0.78      0.62      0.69       223\n",
            "                energy       0.84      0.75      0.79       141\n",
            "             financial       0.81      0.76      0.78       384\n",
            "            healthcare       0.84      0.74      0.79       159\n",
            "              services       0.68      0.86      0.76      1038\n",
            "            technology       0.76      0.52      0.62       198\n",
            "        transportation       0.83      0.81      0.82       202\n",
            "             utilities       0.84      0.80      0.82       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.80      0.71      0.75      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4835, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2740, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.14813285411166308\n",
            "Epoch: 003, Loss: 2.1777, Train: 0.2953, Test: 0.2949\n",
            "Early stopping:  0.15635288514473977\n",
            "Epoch: 004, Loss: 2.1242, Train: 0.3180, Test: 0.3161\n",
            "Early stopping:  0.15841120413259066\n",
            "Epoch: 005, Loss: 2.0664, Train: 0.3667, Test: 0.3609\n",
            "Early stopping:  0.16339445011591464\n",
            "Epoch: 006, Loss: 2.0117, Train: 0.3784, Test: 0.3714\n",
            "Early stopping:  0.1013582875677977\n",
            "Epoch: 007, Loss: 1.9442, Train: 0.3747, Test: 0.3615\n",
            "Early stopping:  0.09169585284850162\n",
            "Epoch: 008, Loss: 1.8806, Train: 0.3796, Test: 0.3683\n",
            "Early stopping:  0.09641592101454508\n",
            "Epoch: 009, Loss: 1.8271, Train: 0.4036, Test: 0.3884\n",
            "Early stopping:  0.09648539808543388\n",
            "Epoch: 010, Loss: 1.7696, Train: 0.4304, Test: 0.4239\n",
            "Early stopping:  0.09519591653113799\n",
            "Epoch: 011, Loss: 1.7046, Train: 0.4618, Test: 0.4517\n",
            "Early stopping:  0.0933775014854834\n",
            "Epoch: 012, Loss: 1.6403, Train: 0.4873, Test: 0.4721\n",
            "Early stopping:  0.09544002968252038\n",
            "Epoch: 013, Loss: 1.5801, Train: 0.4996, Test: 0.4891\n",
            "Early stopping:  0.09857116296194228\n",
            "Epoch: 014, Loss: 1.5242, Train: 0.5159, Test: 0.4976\n",
            "Early stopping:  0.0973217858308876\n",
            "Epoch: 015, Loss: 1.4704, Train: 0.5361, Test: 0.5155\n",
            "Early stopping:  0.09247249527201049\n",
            "Epoch: 016, Loss: 1.4175, Train: 0.5544, Test: 0.5285\n",
            "Early stopping:  0.08782396217731253\n",
            "Epoch: 017, Loss: 1.3701, Train: 0.5773, Test: 0.5503\n",
            "Early stopping:  0.08332517273885998\n",
            "Epoch: 018, Loss: 1.3282, Train: 0.5970, Test: 0.5614\n",
            "Early stopping:  0.0779589451652374\n",
            "Epoch: 019, Loss: 1.2902, Train: 0.6078, Test: 0.5756\n",
            "Early stopping:  0.07127366961257288\n",
            "Epoch: 020, Loss: 1.2528, Train: 0.6210, Test: 0.5923\n",
            "Early stopping:  0.06478015553161096\n",
            "Epoch: 021, Loss: 1.2159, Train: 0.6291, Test: 0.5991\n",
            "Early stopping:  0.060704168620867915\n",
            "Epoch: 022, Loss: 1.1837, Train: 0.6369, Test: 0.6053\n",
            "Early stopping:  0.05746852393019498\n",
            "Epoch: 023, Loss: 1.1548, Train: 0.6482, Test: 0.6141\n",
            "Early stopping:  0.05380672477682204\n",
            "Epoch: 024, Loss: 1.1272, Train: 0.6526, Test: 0.6153\n",
            "Early stopping:  0.04948006965237498\n",
            "Epoch: 025, Loss: 1.0995, Train: 0.6641, Test: 0.6204\n",
            "Early stopping:  0.04575572442789915\n",
            "Epoch: 026, Loss: 1.0738, Train: 0.6701, Test: 0.6257\n",
            "Early stopping:  0.0435052486831024\n",
            "Epoch: 027, Loss: 1.0514, Train: 0.6764, Test: 0.6303\n",
            "Early stopping:  0.0411775040604329\n",
            "Epoch: 028, Loss: 1.0295, Train: 0.6864, Test: 0.6382\n",
            "Early stopping:  0.03853259121760408\n",
            "Epoch: 029, Loss: 1.0063, Train: 0.6953, Test: 0.6422\n",
            "Early stopping:  0.03649293186163017\n",
            "Epoch: 030, Loss: 0.9852, Train: 0.7008, Test: 0.6496\n",
            "Early stopping:  0.035145022158567345\n",
            "Epoch: 031, Loss: 0.9654, Train: 0.7106, Test: 0.6493\n",
            "Early stopping:  0.034217598172928576\n",
            "Epoch: 032, Loss: 0.9446, Train: 0.7161, Test: 0.6530\n",
            "Early stopping:  0.033356690319833335\n",
            "Epoch: 033, Loss: 0.9243, Train: 0.7240, Test: 0.6609\n",
            "Early stopping:  0.032362391923395566\n",
            "Epoch: 034, Loss: 0.9054, Train: 0.7325, Test: 0.6646\n",
            "Early stopping:  0.03175068365007845\n",
            "Epoch: 035, Loss: 0.8880, Train: 0.7405, Test: 0.6671\n",
            "Early stopping:  0.030696767919210315\n",
            "Epoch: 036, Loss: 0.8700, Train: 0.7435, Test: 0.6734\n",
            "Early stopping:  0.029330695770955116\n",
            "Epoch: 037, Loss: 0.8533, Train: 0.7520, Test: 0.6773\n",
            "Early stopping:  0.028038920527097055\n",
            "Epoch: 038, Loss: 0.8379, Train: 0.7569, Test: 0.6819\n",
            "Early stopping:  0.026833370195394043\n",
            "Epoch: 039, Loss: 0.8219, Train: 0.7645, Test: 0.6859\n",
            "Early stopping:  0.025972203298142708\n",
            "Epoch: 040, Loss: 0.8065, Train: 0.7703, Test: 0.6898\n",
            "Early stopping:  0.02506637561026994\n",
            "Epoch: 041, Loss: 0.7916, Train: 0.7758, Test: 0.6935\n",
            "Early stopping:  0.024487702826206225\n",
            "Epoch: 042, Loss: 0.7770, Train: 0.7828, Test: 0.6980\n",
            "Early stopping:  0.02404485387737614\n",
            "Epoch: 043, Loss: 0.7619, Train: 0.7845, Test: 0.7020\n",
            "Early stopping:  0.023638654267449637\n",
            "Epoch: 044, Loss: 0.7479, Train: 0.7924, Test: 0.7057\n",
            "Early stopping:  0.023230737052738516\n",
            "Epoch: 045, Loss: 0.7335, Train: 0.7930, Test: 0.7063\n",
            "Early stopping:  0.022976193151326242\n",
            "Epoch: 046, Loss: 0.7197, Train: 0.8028, Test: 0.7102\n",
            "Early stopping:  0.022619772726371386\n",
            "Epoch: 047, Loss: 0.7077, Train: 0.7930, Test: 0.7063\n",
            "Early stopping:  0.021619333669022238\n",
            "Epoch: 048, Loss: 0.7002, Train: 0.8036, Test: 0.7100\n",
            "Early stopping:  0.019286513667585506\n",
            "Epoch: 049, Loss: 0.6909, Train: 0.8055, Test: 0.7148\n",
            "Early stopping:  0.016692961644029274\n",
            "Epoch: 050, Loss: 0.6708, Train: 0.8161, Test: 0.7221\n",
            "Early stopping:  0.018418455521362235\n",
            "Epoch: 051, Loss: 0.6535, Train: 0.8193, Test: 0.7170\n",
            "Early stopping:  0.022190824300750065\n",
            "Epoch: 052, Loss: 0.6497, Train: 0.8159, Test: 0.7227\n",
            "Early stopping:  0.02227753449727204\n",
            "Epoch: 053, Loss: 0.6369, Train: 0.8251, Test: 0.7292\n",
            "Early stopping:  0.020934326595111855\n",
            "Epoch: 054, Loss: 0.6190, Train: 0.8318, Test: 0.7284\n",
            "Early stopping:  0.019345896580390865\n",
            "Epoch: 055, Loss: 0.6131, Train: 0.8278, Test: 0.7287\n",
            "Early stopping:  0.017996754471072178\n",
            "Epoch: 056, Loss: 0.6031, Train: 0.8384, Test: 0.7349\n",
            "Early stopping:  0.0187529378578903\n",
            "Epoch: 057, Loss: 0.5876, Train: 0.8448, Test: 0.7380\n",
            "Early stopping:  0.018320800938001886\n",
            "Epoch: 058, Loss: 0.5789, Train: 0.8380, Test: 0.7346\n",
            "Early stopping:  0.016896466373993196\n",
            "Epoch: 059, Loss: 0.5717, Train: 0.8509, Test: 0.7386\n",
            "Early stopping:  0.01707805056180646\n",
            "Epoch: 060, Loss: 0.5580, Train: 0.8524, Test: 0.7411\n",
            "Early stopping:  0.016942434586089895\n",
            "Epoch: 061, Loss: 0.5466, Train: 0.8509, Test: 0.7397\n",
            "Early stopping:  0.016394717537078447\n",
            "Epoch: 062, Loss: 0.5406, Train: 0.8643, Test: 0.7397\n",
            "Early stopping:  0.016186632312068024\n",
            "Epoch: 063, Loss: 0.5299, Train: 0.8647, Test: 0.7454\n",
            "Early stopping:  0.016085221958392848\n",
            "Epoch: 064, Loss: 0.5170, Train: 0.8654, Test: 0.7457\n",
            "Early stopping:  0.015700846494758062\n",
            "Epoch: 065, Loss: 0.5087, Train: 0.8754, Test: 0.7414\n",
            "Early stopping:  0.015804466963020923\n",
            "Epoch: 066, Loss: 0.5011, Train: 0.8722, Test: 0.7420\n",
            "Early stopping:  0.01593225589850433\n",
            "Epoch: 067, Loss: 0.4915, Train: 0.8813, Test: 0.7485\n",
            "Early stopping:  0.01470395384857057\n",
            "Epoch: 068, Loss: 0.4795, Train: 0.8841, Test: 0.7485\n",
            "Early stopping:  0.014648503757418356\n",
            "Epoch: 069, Loss: 0.4692, Train: 0.8830, Test: 0.7445\n",
            "Early stopping:  0.015962798874819835\n",
            "Epoch: 070, Loss: 0.4619, Train: 0.8934, Test: 0.7485\n",
            "Early stopping:  0.015971268292520522\n",
            "Epoch: 071, Loss: 0.4542, Train: 0.8868, Test: 0.7485\n",
            "Early stopping:  0.014671641358089618\n",
            "Epoch: 072, Loss: 0.4462, Train: 0.9000, Test: 0.7494\n",
            "Early stopping:  0.012906995062153264\n",
            "Epoch: 073, Loss: 0.4376, Train: 0.8943, Test: 0.7511\n",
            "Early stopping:  0.012479628854702711\n",
            "Epoch: 074, Loss: 0.4279, Train: 0.9072, Test: 0.7513\n",
            "Early stopping:  0.013375578535620693\n",
            "Epoch: 075, Loss: 0.4174, Train: 0.9059, Test: 0.7516\n",
            "Early stopping:  0.014557638491039514\n",
            "Epoch: 076, Loss: 0.4084, Train: 0.9113, Test: 0.7505\n",
            "Early stopping:  0.015169795238948958\n",
            "Epoch: 077, Loss: 0.3995, Train: 0.9134, Test: 0.7505\n",
            "Early stopping:  0.015153215108035377\n",
            "Epoch: 078, Loss: 0.3907, Train: 0.9164, Test: 0.7513\n",
            "Early stopping:  0.014621575990355651\n",
            "Epoch: 079, Loss: 0.3828, Train: 0.9208, Test: 0.7513\n",
            "Early stopping:  0.013729430913486513\n",
            "Epoch: 080, Loss: 0.3755, Train: 0.9191, Test: 0.7496\n",
            "Early stopping:  0.01303290002689039\n",
            "Epoch: 081, Loss: 0.3688, Train: 0.9246, Test: 0.7525\n",
            "Early stopping:  0.012114302685816458\n",
            "Epoch: 082, Loss: 0.3664, Train: 0.8928, Test: 0.7352\n",
            "Early stopping:  0.010034712415282043\n",
            "Epoch: 083, Loss: 0.3843, Train: 0.8875, Test: 0.7213\n",
            "Early stopping:  0.008036165192782142\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11, 11], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.85      0.74       379\n",
            "         capital_goods       0.58      0.67      0.62       254\n",
            "conglomerates_industry       0.93      0.62      0.75        40\n",
            "     consumer_cyclical       0.62      0.76      0.68       396\n",
            " consumer_non-cyclical       0.72      0.73      0.73       223\n",
            "                energy       0.76      0.75      0.75       141\n",
            "             financial       0.71      0.82      0.76       384\n",
            "            healthcare       0.84      0.81      0.82       159\n",
            "              services       0.84      0.61      0.71      1038\n",
            "            technology       0.62      0.63      0.63       198\n",
            "        transportation       0.79      0.83      0.81       202\n",
            "             utilities       0.79      0.79      0.79       113\n",
            "\n",
            "              accuracy                           0.72      3527\n",
            "             macro avg       0.74      0.74      0.73      3527\n",
            "          weighted avg       0.74      0.72      0.72      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4743, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2618, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15024373687248269\n",
            "Epoch: 003, Loss: 2.1673, Train: 0.2947, Test: 0.2943\n",
            "Early stopping:  0.15720636410473798\n",
            "Epoch: 004, Loss: 2.1289, Train: 0.3047, Test: 0.3039\n",
            "Early stopping:  0.15455924537967142\n",
            "Epoch: 005, Loss: 2.0528, Train: 0.3520, Test: 0.3473\n",
            "Early stopping:  0.162319747299466\n",
            "Epoch: 006, Loss: 1.9841, Train: 0.3898, Test: 0.3828\n",
            "Early stopping:  0.10654164521535575\n",
            "Epoch: 007, Loss: 1.9247, Train: 0.3987, Test: 0.3913\n",
            "Early stopping:  0.10004311173064848\n",
            "Epoch: 008, Loss: 1.8632, Train: 0.4028, Test: 0.3950\n",
            "Early stopping:  0.10444097175234363\n",
            "Epoch: 009, Loss: 1.8014, Train: 0.4127, Test: 0.4071\n",
            "Early stopping:  0.09863070785995848\n",
            "Epoch: 010, Loss: 1.7411, Train: 0.4357, Test: 0.4290\n",
            "Early stopping:  0.0963465245462825\n",
            "Epoch: 011, Loss: 1.6787, Train: 0.4647, Test: 0.4508\n",
            "Early stopping:  0.0970780134845104\n",
            "Epoch: 012, Loss: 1.6180, Train: 0.4851, Test: 0.4658\n",
            "Early stopping:  0.09690920591641378\n",
            "Epoch: 013, Loss: 1.5609, Train: 0.5028, Test: 0.4888\n",
            "Early stopping:  0.09554151241910552\n",
            "Epoch: 014, Loss: 1.5035, Train: 0.5255, Test: 0.5035\n",
            "Early stopping:  0.09378370740724415\n",
            "Epoch: 015, Loss: 1.4496, Train: 0.5467, Test: 0.5248\n",
            "Early stopping:  0.09059660953516482\n",
            "Epoch: 016, Loss: 1.3998, Train: 0.5624, Test: 0.5435\n",
            "Early stopping:  0.08664817520493338\n",
            "Epoch: 017, Loss: 1.3528, Train: 0.5837, Test: 0.5597\n",
            "Early stopping:  0.0822506927168059\n",
            "Epoch: 018, Loss: 1.3076, Train: 0.5983, Test: 0.5767\n",
            "Early stopping:  0.07729182172162742\n",
            "Epoch: 019, Loss: 1.2648, Train: 0.6108, Test: 0.5886\n",
            "Early stopping:  0.07303547199007766\n",
            "Epoch: 020, Loss: 1.2265, Train: 0.6219, Test: 0.5974\n",
            "Early stopping:  0.06876790265957984\n",
            "Epoch: 021, Loss: 1.1903, Train: 0.6331, Test: 0.6065\n",
            "Early stopping:  0.06428314478352165\n",
            "Epoch: 022, Loss: 1.1583, Train: 0.6386, Test: 0.6138\n",
            "Early stopping:  0.0590906045087093\n",
            "Epoch: 023, Loss: 1.1269, Train: 0.6476, Test: 0.6189\n",
            "Early stopping:  0.05446894291668245\n",
            "Epoch: 024, Loss: 1.0980, Train: 0.6569, Test: 0.6226\n",
            "Early stopping:  0.05072488515928471\n",
            "Epoch: 025, Loss: 1.0733, Train: 0.6641, Test: 0.6280\n",
            "Early stopping:  0.04659092313162964\n",
            "Epoch: 026, Loss: 1.0495, Train: 0.6732, Test: 0.6328\n",
            "Early stopping:  0.04294066765109151\n",
            "Epoch: 027, Loss: 1.0246, Train: 0.6792, Test: 0.6374\n",
            "Early stopping:  0.04002051798154867\n",
            "Epoch: 028, Loss: 1.0022, Train: 0.6919, Test: 0.6416\n",
            "Early stopping:  0.03800277584814336\n",
            "Epoch: 029, Loss: 0.9812, Train: 0.6989, Test: 0.6487\n",
            "Early stopping:  0.03662363321457337\n",
            "Epoch: 030, Loss: 0.9601, Train: 0.7072, Test: 0.6569\n",
            "Early stopping:  0.03516537939306535\n",
            "Epoch: 031, Loss: 0.9392, Train: 0.7123, Test: 0.6612\n",
            "Early stopping:  0.03367937052099519\n",
            "Epoch: 032, Loss: 0.9186, Train: 0.7216, Test: 0.6637\n",
            "Early stopping:  0.03306610234954329\n",
            "Epoch: 033, Loss: 0.8989, Train: 0.7327, Test: 0.6677\n",
            "Early stopping:  0.03259744765996369\n",
            "Epoch: 034, Loss: 0.8795, Train: 0.7418, Test: 0.6694\n",
            "Early stopping:  0.031840043365733796\n",
            "Epoch: 035, Loss: 0.8612, Train: 0.7476, Test: 0.6734\n",
            "Early stopping:  0.030840618032289834\n",
            "Epoch: 036, Loss: 0.8444, Train: 0.7556, Test: 0.6771\n",
            "Early stopping:  0.029435152676720658\n",
            "Epoch: 037, Loss: 0.8274, Train: 0.7629, Test: 0.6822\n",
            "Early stopping:  0.028156998514111225\n",
            "Epoch: 038, Loss: 0.8118, Train: 0.7656, Test: 0.6844\n",
            "Early stopping:  0.026787356585982608\n",
            "Epoch: 039, Loss: 0.7966, Train: 0.7722, Test: 0.6921\n",
            "Early stopping:  0.02559760372753766\n",
            "Epoch: 040, Loss: 0.7813, Train: 0.7775, Test: 0.6969\n",
            "Early stopping:  0.02483137477233365\n",
            "Epoch: 041, Loss: 0.7667, Train: 0.7845, Test: 0.6997\n",
            "Early stopping:  0.024016033067164117\n",
            "Epoch: 042, Loss: 0.7525, Train: 0.7824, Test: 0.7003\n",
            "Early stopping:  0.023487288313367116\n",
            "Epoch: 043, Loss: 0.7399, Train: 0.7928, Test: 0.6995\n",
            "Early stopping:  0.02253722758329495\n",
            "Epoch: 044, Loss: 0.7304, Train: 0.7875, Test: 0.7012\n",
            "Early stopping:  0.020420660977841856\n",
            "Epoch: 045, Loss: 0.7228, Train: 0.8028, Test: 0.7054\n",
            "Early stopping:  0.017520929458560763\n",
            "Epoch: 046, Loss: 0.7037, Train: 0.8060, Test: 0.7119\n",
            "Early stopping:  0.018344074669761026\n",
            "Epoch: 047, Loss: 0.6856, Train: 0.8042, Test: 0.7065\n",
            "Early stopping:  0.02176530078482534\n",
            "Epoch: 048, Loss: 0.6795, Train: 0.8115, Test: 0.7148\n",
            "Early stopping:  0.02229272508190938\n",
            "Epoch: 049, Loss: 0.6695, Train: 0.8142, Test: 0.7179\n",
            "Early stopping:  0.021141456767754686\n",
            "Epoch: 050, Loss: 0.6522, Train: 0.8180, Test: 0.7199\n",
            "Early stopping:  0.019083830027924287\n",
            "Epoch: 051, Loss: 0.6414, Train: 0.8208, Test: 0.7207\n",
            "Early stopping:  0.018532173923868994\n",
            "Epoch: 052, Loss: 0.6348, Train: 0.8238, Test: 0.7227\n",
            "Early stopping:  0.018756165002386537\n",
            "Epoch: 053, Loss: 0.6222, Train: 0.8284, Test: 0.7221\n",
            "Early stopping:  0.017905516200341882\n",
            "Epoch: 054, Loss: 0.6083, Train: 0.8344, Test: 0.7241\n",
            "Early stopping:  0.01704710664718484\n",
            "Epoch: 055, Loss: 0.6007, Train: 0.8312, Test: 0.7241\n",
            "Early stopping:  0.017183179797088576\n",
            "Epoch: 056, Loss: 0.5942, Train: 0.8440, Test: 0.7295\n",
            "Early stopping:  0.016473035318419333\n",
            "Epoch: 057, Loss: 0.5819, Train: 0.8431, Test: 0.7284\n",
            "Early stopping:  0.015111072510800124\n",
            "Epoch: 058, Loss: 0.5690, Train: 0.8465, Test: 0.7295\n",
            "Early stopping:  0.015554495353202236\n",
            "Epoch: 059, Loss: 0.5606, Train: 0.8526, Test: 0.7318\n",
            "Early stopping:  0.016746512410389963\n",
            "Epoch: 060, Loss: 0.5546, Train: 0.8501, Test: 0.7324\n",
            "Early stopping:  0.016063807776844016\n",
            "Epoch: 061, Loss: 0.5480, Train: 0.8597, Test: 0.7315\n",
            "Early stopping:  0.013195733383614619\n",
            "Epoch: 062, Loss: 0.5375, Train: 0.8611, Test: 0.7355\n",
            "Early stopping:  0.012025903632153997\n",
            "Epoch: 063, Loss: 0.5254, Train: 0.8641, Test: 0.7369\n",
            "Early stopping:  0.014008120952897024\n",
            "Epoch: 064, Loss: 0.5146, Train: 0.8673, Test: 0.7369\n",
            "Early stopping:  0.016297945721682507\n",
            "Epoch: 065, Loss: 0.5065, Train: 0.8662, Test: 0.7383\n",
            "Early stopping:  0.01677758326432415\n",
            "Epoch: 066, Loss: 0.5010, Train: 0.8698, Test: 0.7363\n",
            "Early stopping:  0.01469327576962732\n",
            "Epoch: 067, Loss: 0.4988, Train: 0.8592, Test: 0.7318\n",
            "Early stopping:  0.010886575491744684\n",
            "Epoch: 068, Loss: 0.5021, Train: 0.8673, Test: 0.7363\n",
            "Early stopping:  0.006249573929327453\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.80      0.73       379\n",
            "         capital_goods       0.66      0.62      0.64       254\n",
            "conglomerates_industry       1.00      0.42      0.60        40\n",
            "     consumer_cyclical       0.60      0.77      0.67       396\n",
            " consumer_non-cyclical       0.80      0.63      0.70       223\n",
            "                energy       0.81      0.80      0.81       141\n",
            "             financial       0.85      0.73      0.79       384\n",
            "            healthcare       0.82      0.80      0.81       159\n",
            "              services       0.75      0.75      0.75      1038\n",
            "            technology       0.69      0.61      0.65       198\n",
            "        transportation       0.84      0.82      0.83       202\n",
            "             utilities       0.86      0.75      0.80       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.78      0.71      0.73      3527\n",
            "          weighted avg       0.75      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4728, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2605, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15014561900914933\n",
            "Epoch: 003, Loss: 2.1747, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15346750481063903\n",
            "Epoch: 004, Loss: 2.1313, Train: 0.2998, Test: 0.3005\n",
            "Early stopping:  0.15179980129477552\n",
            "Epoch: 005, Loss: 2.0471, Train: 0.3342, Test: 0.3303\n",
            "Early stopping:  0.16227935588119027\n",
            "Epoch: 006, Loss: 1.9831, Train: 0.3783, Test: 0.3706\n",
            "Early stopping:  0.108284505160983\n",
            "Epoch: 007, Loss: 1.9387, Train: 0.4013, Test: 0.3901\n",
            "Early stopping:  0.09860680027229164\n",
            "Epoch: 008, Loss: 1.8860, Train: 0.4087, Test: 0.3967\n",
            "Early stopping:  0.09545467400319989\n",
            "Epoch: 009, Loss: 1.8224, Train: 0.4130, Test: 0.4035\n",
            "Early stopping:  0.08655177300587459\n",
            "Epoch: 010, Loss: 1.7605, Train: 0.4234, Test: 0.4139\n",
            "Early stopping:  0.08903193997855122\n",
            "Epoch: 011, Loss: 1.7059, Train: 0.4437, Test: 0.4293\n",
            "Early stopping:  0.09352640403881256\n",
            "Epoch: 012, Loss: 1.6516, Train: 0.4694, Test: 0.4568\n",
            "Early stopping:  0.09261569687586205\n",
            "Epoch: 013, Loss: 1.5950, Train: 0.4890, Test: 0.4772\n",
            "Early stopping:  0.0891566943673217\n",
            "Epoch: 014, Loss: 1.5377, Train: 0.5147, Test: 0.5030\n",
            "Early stopping:  0.08798602783115408\n",
            "Epoch: 015, Loss: 1.4829, Train: 0.5416, Test: 0.5240\n",
            "Early stopping:  0.08850736576900897\n",
            "Epoch: 016, Loss: 1.4345, Train: 0.5595, Test: 0.5404\n",
            "Early stopping:  0.086420624301742\n",
            "Epoch: 017, Loss: 1.3881, Train: 0.5732, Test: 0.5534\n",
            "Early stopping:  0.08182294278996945\n",
            "Epoch: 018, Loss: 1.3394, Train: 0.5811, Test: 0.5614\n",
            "Early stopping:  0.07773790529683315\n",
            "Epoch: 019, Loss: 1.2941, Train: 0.5896, Test: 0.5665\n",
            "Early stopping:  0.07474565596572116\n",
            "Epoch: 020, Loss: 1.2556, Train: 0.5960, Test: 0.5730\n",
            "Early stopping:  0.07149156535714465\n",
            "Epoch: 021, Loss: 1.2198, Train: 0.6081, Test: 0.5866\n",
            "Early stopping:  0.06662585037227325\n",
            "Epoch: 022, Loss: 1.1859, Train: 0.6223, Test: 0.5946\n",
            "Early stopping:  0.06038234759009733\n",
            "Epoch: 023, Loss: 1.1541, Train: 0.6372, Test: 0.6031\n",
            "Early stopping:  0.05531363157697825\n",
            "Epoch: 024, Loss: 1.1272, Train: 0.6456, Test: 0.6104\n",
            "Early stopping:  0.05104883630868846\n",
            "Epoch: 025, Loss: 1.1009, Train: 0.6518, Test: 0.6161\n",
            "Early stopping:  0.04697032996954007\n",
            "Epoch: 026, Loss: 1.0737, Train: 0.6571, Test: 0.6221\n",
            "Early stopping:  0.04394005043211163\n",
            "Epoch: 027, Loss: 1.0492, Train: 0.6679, Test: 0.6320\n",
            "Early stopping:  0.0416404407980549\n",
            "Epoch: 028, Loss: 1.0257, Train: 0.6794, Test: 0.6351\n",
            "Early stopping:  0.04026668480568248\n",
            "Epoch: 029, Loss: 1.0027, Train: 0.6864, Test: 0.6439\n",
            "Early stopping:  0.03865435223420509\n",
            "Epoch: 030, Loss: 0.9803, Train: 0.6953, Test: 0.6507\n",
            "Early stopping:  0.03688563171329396\n",
            "Epoch: 031, Loss: 0.9597, Train: 0.7066, Test: 0.6541\n",
            "Early stopping:  0.035504307973301015\n",
            "Epoch: 032, Loss: 0.9387, Train: 0.7166, Test: 0.6620\n",
            "Early stopping:  0.03433816073044762\n",
            "Epoch: 033, Loss: 0.9189, Train: 0.7246, Test: 0.6671\n",
            "Early stopping:  0.03309284618692437\n",
            "Epoch: 034, Loss: 0.8998, Train: 0.7325, Test: 0.6731\n",
            "Early stopping:  0.03191627564560276\n",
            "Epoch: 035, Loss: 0.8813, Train: 0.7423, Test: 0.6708\n",
            "Early stopping:  0.030930491005403564\n",
            "Epoch: 036, Loss: 0.8635, Train: 0.7507, Test: 0.6762\n",
            "Early stopping:  0.029717278430256434\n",
            "Epoch: 037, Loss: 0.8460, Train: 0.7577, Test: 0.6819\n",
            "Early stopping:  0.028785329129358397\n",
            "Epoch: 038, Loss: 0.8293, Train: 0.7637, Test: 0.6836\n",
            "Early stopping:  0.027881803216956276\n",
            "Epoch: 039, Loss: 0.8127, Train: 0.7716, Test: 0.6859\n",
            "Early stopping:  0.02711739812135043\n",
            "Epoch: 040, Loss: 0.7968, Train: 0.7741, Test: 0.6898\n",
            "Early stopping:  0.026376647181863125\n",
            "Epoch: 041, Loss: 0.7813, Train: 0.7807, Test: 0.6921\n",
            "Early stopping:  0.02561511956941845\n",
            "Epoch: 042, Loss: 0.7656, Train: 0.7864, Test: 0.6969\n",
            "Early stopping:  0.02512281157080429\n",
            "Epoch: 043, Loss: 0.7502, Train: 0.7873, Test: 0.6986\n",
            "Early stopping:  0.024687579548045204\n",
            "Epoch: 044, Loss: 0.7351, Train: 0.7900, Test: 0.6995\n",
            "Early stopping:  0.024421441391746214\n",
            "Epoch: 045, Loss: 0.7200, Train: 0.7949, Test: 0.7037\n",
            "Early stopping:  0.024220985452371495\n",
            "Epoch: 046, Loss: 0.7054, Train: 0.7996, Test: 0.7088\n",
            "Early stopping:  0.023808646188654664\n",
            "Epoch: 047, Loss: 0.6909, Train: 0.8047, Test: 0.7119\n",
            "Early stopping:  0.023472093907070068\n",
            "Epoch: 048, Loss: 0.6767, Train: 0.8079, Test: 0.7136\n",
            "Early stopping:  0.023055673256739496\n",
            "Epoch: 049, Loss: 0.6629, Train: 0.8149, Test: 0.7179\n",
            "Early stopping:  0.022574072469491356\n",
            "Epoch: 050, Loss: 0.6491, Train: 0.8155, Test: 0.7196\n",
            "Early stopping:  0.022238476294164217\n",
            "Epoch: 051, Loss: 0.6359, Train: 0.8253, Test: 0.7224\n",
            "Early stopping:  0.021769606165020806\n",
            "Epoch: 052, Loss: 0.6225, Train: 0.8248, Test: 0.7255\n",
            "Early stopping:  0.02142676029241834\n",
            "Epoch: 053, Loss: 0.6100, Train: 0.8352, Test: 0.7264\n",
            "Early stopping:  0.02094089418318771\n",
            "Epoch: 054, Loss: 0.5978, Train: 0.8331, Test: 0.7253\n",
            "Early stopping:  0.020296657615642644\n",
            "Epoch: 055, Loss: 0.5868, Train: 0.8454, Test: 0.7287\n",
            "Early stopping:  0.01941738608970103\n",
            "Epoch: 056, Loss: 0.5751, Train: 0.8482, Test: 0.7332\n",
            "Early stopping:  0.01864616384468499\n",
            "Epoch: 057, Loss: 0.5622, Train: 0.8567, Test: 0.7324\n",
            "Early stopping:  0.018700905458171393\n",
            "Epoch: 058, Loss: 0.5484, Train: 0.8603, Test: 0.7355\n",
            "Early stopping:  0.01955217463884332\n",
            "Epoch: 059, Loss: 0.5362, Train: 0.8616, Test: 0.7377\n",
            "Early stopping:  0.020245312151467725\n",
            "Epoch: 060, Loss: 0.5261, Train: 0.8692, Test: 0.7329\n",
            "Early stopping:  0.019644109970165056\n",
            "Epoch: 061, Loss: 0.5164, Train: 0.8654, Test: 0.7383\n",
            "Early stopping:  0.018058482756376084\n",
            "Epoch: 062, Loss: 0.5064, Train: 0.8767, Test: 0.7355\n",
            "Early stopping:  0.01642018791869663\n",
            "Epoch: 063, Loss: 0.4947, Train: 0.8769, Test: 0.7414\n",
            "Early stopping:  0.01626651874873798\n",
            "Epoch: 064, Loss: 0.4831, Train: 0.8819, Test: 0.7414\n",
            "Early stopping:  0.0170560451662537\n",
            "Epoch: 065, Loss: 0.4726, Train: 0.8862, Test: 0.7420\n",
            "Early stopping:  0.017560777871903754\n",
            "Epoch: 066, Loss: 0.4627, Train: 0.8856, Test: 0.7457\n",
            "Early stopping:  0.017321165254857065\n",
            "Epoch: 067, Loss: 0.4529, Train: 0.8953, Test: 0.7417\n",
            "Early stopping:  0.01642544678489237\n",
            "Epoch: 068, Loss: 0.4426, Train: 0.8926, Test: 0.7482\n",
            "Early stopping:  0.01592845738508402\n",
            "Epoch: 069, Loss: 0.4335, Train: 0.9021, Test: 0.7428\n",
            "Early stopping:  0.015550494020989975\n",
            "Epoch: 070, Loss: 0.4256, Train: 0.9008, Test: 0.7460\n",
            "Early stopping:  0.0148282950832887\n",
            "Epoch: 071, Loss: 0.4194, Train: 0.9074, Test: 0.7426\n",
            "Early stopping:  0.013329315058511511\n",
            "Epoch: 072, Loss: 0.4111, Train: 0.9060, Test: 0.7474\n",
            "Early stopping:  0.012197353848543953\n",
            "Epoch: 073, Loss: 0.3996, Train: 0.9144, Test: 0.7477\n",
            "Early stopping:  0.01309309421867504\n",
            "Epoch: 074, Loss: 0.3898, Train: 0.9112, Test: 0.7454\n",
            "Early stopping:  0.014566039605674567\n",
            "Epoch: 075, Loss: 0.3850, Train: 0.9178, Test: 0.7460\n",
            "Early stopping:  0.014365662936480946\n",
            "Epoch: 076, Loss: 0.3774, Train: 0.9170, Test: 0.7460\n",
            "Early stopping:  0.013096091980395685\n",
            "Epoch: 077, Loss: 0.3676, Train: 0.9255, Test: 0.7485\n",
            "Early stopping:  0.012139817003368896\n",
            "Epoch: 078, Loss: 0.3601, Train: 0.9183, Test: 0.7443\n",
            "Early stopping:  0.012199007897281509\n",
            "Epoch: 079, Loss: 0.3567, Train: 0.9306, Test: 0.7451\n",
            "Early stopping:  0.011822552329391068\n",
            "Epoch: 080, Loss: 0.3513, Train: 0.9210, Test: 0.7443\n",
            "Early stopping:  0.01018505713133052\n",
            "Epoch: 081, Loss: 0.3439, Train: 0.9327, Test: 0.7423\n",
            "Early stopping:  0.008953022650472509\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.78      0.77       379\n",
            "         capital_goods       0.71      0.60      0.65       254\n",
            "conglomerates_industry       0.90      0.68      0.77        40\n",
            "     consumer_cyclical       0.62      0.75      0.68       396\n",
            " consumer_non-cyclical       0.76      0.65      0.70       223\n",
            "                energy       0.82      0.76      0.79       141\n",
            "             financial       0.77      0.80      0.79       384\n",
            "            healthcare       0.82      0.78      0.80       159\n",
            "              services       0.74      0.76      0.75      1038\n",
            "            technology       0.70      0.64      0.67       198\n",
            "        transportation       0.84      0.80      0.82       202\n",
            "             utilities       0.81      0.78      0.79       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.77      0.73      0.75      3527\n",
            "          weighted avg       0.75      0.74      0.74      3527\n",
            "\n",
            "time: 2min 36s (started: 2024-10-16 21:25:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMni8zvQgSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8270c7e9-c2cf-4cce-94db-841a8fd03b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 439 ms (started: 2024-10-16 21:28:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAwNmu-1gSl-"
      },
      "source": [
        "### Training rotulated base = 80% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFF_yl-agSl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bcfb09d-686e-4e9c-ebf7-cb34be3f9325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 469 µs (started: 2024-10-16 21:28:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwA1YKbngSl-"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atPhGNllgSl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fcbcc9-cf0f-420f-fe4d-d31856fce70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 78.0120, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 467.8694, Train: 0.0125, Test: 0.0113\n",
            "Early stopping:  275.67082130892055\n",
            "Epoch: 003, Loss: 615.2813, Train: 0.0651, Test: 0.0680\n",
            "Early stopping:  277.602060422929\n",
            "Epoch: 004, Loss: 677.0381, Train: 0.1136, Test: 0.1139\n",
            "Early stopping:  269.06863366291844\n",
            "Epoch: 005, Loss: 751.7266, Train: 0.1114, Test: 0.1083\n",
            "Early stopping:  267.1551255446567\n",
            "Epoch: 006, Loss: 602.4797, Train: 0.0471, Test: 0.0482\n",
            "Early stopping:  104.92468110680626\n",
            "Epoch: 007, Loss: 443.4893, Train: 0.0934, Test: 0.0952\n",
            "Early stopping:  114.09489063503925\n",
            "Epoch: 008, Loss: 231.3313, Train: 0.2985, Test: 0.3010\n",
            "Early stopping:  207.4214743246863\n",
            "Epoch: 009, Loss: 221.0394, Train: 0.2951, Test: 0.2993\n",
            "Early stopping:  231.60871888233552\n",
            "Epoch: 010, Loss: 281.2063, Train: 0.2934, Test: 0.2902\n",
            "Early stopping:  164.1320580424752\n",
            "Epoch: 011, Loss: 271.8357, Train: 0.2406, Test: 0.2353\n",
            "Early stopping:  89.67103082814616\n",
            "Epoch: 012, Loss: 250.8185, Train: 0.2522, Test: 0.2494\n",
            "Early stopping:  25.645547510707452\n",
            "Epoch: 013, Loss: 206.0494, Train: 0.3036, Test: 0.3050\n",
            "Early stopping:  32.20671250928652\n",
            "Epoch: 014, Loss: 151.4621, Train: 0.3041, Test: 0.3033\n",
            "Early stopping:  53.66958505327809\n",
            "Epoch: 015, Loss: 109.8336, Train: 0.3087, Test: 0.3152\n",
            "Early stopping:  67.53775199824682\n",
            "Epoch: 016, Loss: 68.9979, Train: 0.1027, Test: 0.1037\n",
            "Early stopping:  72.81231975489894\n",
            "Epoch: 017, Loss: 67.3391, Train: 0.1158, Test: 0.1134\n",
            "Early stopping:  58.884178384946004\n",
            "Epoch: 018, Loss: 62.2901, Train: 0.1204, Test: 0.1224\n",
            "Early stopping:  38.320056302315855\n",
            "Epoch: 019, Loss: 51.3010, Train: 0.1262, Test: 0.1207\n",
            "Early stopping:  22.275758586550932\n",
            "Epoch: 020, Loss: 35.8707, Train: 0.1406, Test: 0.1281\n",
            "Early stopping:  13.762541111402077\n",
            "Epoch: 021, Loss: 17.6355, Train: 0.1636, Test: 0.1570\n",
            "Early stopping:  20.32263588104056\n",
            "Epoch: 022, Loss: 7.5558, Train: 0.1993, Test: 0.1990\n",
            "Early stopping:  22.726158770271805\n",
            "Epoch: 023, Loss: 4.0089, Train: 0.2182, Test: 0.2171\n",
            "Early stopping:  19.963638702013398\n",
            "Epoch: 024, Loss: 3.1482, Train: 0.2217, Test: 0.2217\n",
            "Early stopping:  13.693250529998176\n",
            "Epoch: 025, Loss: 2.7669, Train: 0.2463, Test: 0.2460\n",
            "Early stopping:  6.2273945700051\n",
            "Epoch: 026, Loss: 2.4812, Train: 0.3095, Test: 0.2925\n",
            "Early stopping:  2.073428566150461\n",
            "Epoch: 027, Loss: 2.3344, Train: 0.3054, Test: 0.3027\n",
            "Early stopping:  0.669476846992957\n",
            "Epoch: 028, Loss: 2.3252, Train: 0.3102, Test: 0.3022\n",
            "Early stopping:  0.3492774739783608\n",
            "Epoch: 029, Loss: 2.3364, Train: 0.3281, Test: 0.3260\n",
            "Early stopping:  0.18924607893269432\n",
            "Epoch: 030, Loss: 2.3082, Train: 0.3569, Test: 0.3486\n",
            "Early stopping:  0.07026865888254047\n",
            "Epoch: 031, Loss: 2.2540, Train: 0.3696, Test: 0.3690\n",
            "Early stopping:  0.03408218093020437\n",
            "Epoch: 032, Loss: 2.2010, Train: 0.3647, Test: 0.3634\n",
            "Early stopping:  0.05661663757939241\n",
            "Epoch: 033, Loss: 2.1813, Train: 0.3634, Test: 0.3560\n",
            "Early stopping:  0.06673448430477098\n",
            "Epoch: 034, Loss: 2.1496, Train: 0.3606, Test: 0.3560\n",
            "Early stopping:  0.06275478162127035\n",
            "Epoch: 035, Loss: 2.1340, Train: 0.3591, Test: 0.3571\n",
            "Early stopping:  0.04713002973862574\n",
            "Epoch: 036, Loss: 2.1262, Train: 0.3620, Test: 0.3554\n",
            "Early stopping:  0.0318172978999187\n",
            "Epoch: 037, Loss: 2.1143, Train: 0.3642, Test: 0.3600\n",
            "Early stopping:  0.025880706724064208\n",
            "Epoch: 038, Loss: 2.0957, Train: 0.3685, Test: 0.3645\n",
            "Early stopping:  0.02035498167264115\n",
            "Epoch: 039, Loss: 2.0744, Train: 0.3708, Test: 0.3622\n",
            "Early stopping:  0.024077623679138318\n",
            "Epoch: 040, Loss: 2.0575, Train: 0.3739, Test: 0.3594\n",
            "Early stopping:  0.02813391350691762\n",
            "Epoch: 041, Loss: 2.0426, Train: 0.3805, Test: 0.3685\n",
            "Early stopping:  0.02877699660634756\n",
            "Epoch: 042, Loss: 2.0169, Train: 0.3895, Test: 0.3849\n",
            "Early stopping:  0.03006317553489885\n",
            "Epoch: 043, Loss: 1.9987, Train: 0.3960, Test: 0.3883\n",
            "Early stopping:  0.030467646697896422\n",
            "Epoch: 044, Loss: 1.9810, Train: 0.4049, Test: 0.3917\n",
            "Early stopping:  0.031188660743302888\n",
            "Epoch: 045, Loss: 1.9578, Train: 0.4092, Test: 0.4014\n",
            "Early stopping:  0.03253887917034288\n",
            "Epoch: 046, Loss: 1.9369, Train: 0.4143, Test: 0.4042\n",
            "Early stopping:  0.03181994538831002\n",
            "Epoch: 047, Loss: 1.9185, Train: 0.4228, Test: 0.4110\n",
            "Early stopping:  0.032396706388142966\n",
            "Epoch: 048, Loss: 1.8946, Train: 0.4346, Test: 0.4223\n",
            "Early stopping:  0.03358474869247031\n",
            "Epoch: 049, Loss: 1.8765, Train: 0.4408, Test: 0.4246\n",
            "Early stopping:  0.032414795586512286\n",
            "Epoch: 050, Loss: 1.8600, Train: 0.4462, Test: 0.4314\n",
            "Early stopping:  0.030992476476565216\n",
            "Epoch: 051, Loss: 1.8417, Train: 0.4543, Test: 0.4371\n",
            "Early stopping:  0.029824569809781463\n",
            "Epoch: 052, Loss: 1.8240, Train: 0.4639, Test: 0.4439\n",
            "Early stopping:  0.027833964292095947\n",
            "Epoch: 053, Loss: 1.8017, Train: 0.4704, Test: 0.4518\n",
            "Early stopping:  0.02940974665789065\n",
            "Epoch: 054, Loss: 1.7816, Train: 0.4778, Test: 0.4586\n",
            "Early stopping:  0.03114342096782701\n",
            "Epoch: 055, Loss: 1.7650, Train: 0.4788, Test: 0.4615\n",
            "Early stopping:  0.03099559150025152\n",
            "Epoch: 056, Loss: 1.7474, Train: 0.4822, Test: 0.4592\n",
            "Early stopping:  0.030098435755207136\n",
            "Epoch: 057, Loss: 1.7336, Train: 0.4863, Test: 0.4592\n",
            "Early stopping:  0.026997339465144297\n",
            "Epoch: 058, Loss: 1.7160, Train: 0.4901, Test: 0.4603\n",
            "Early stopping:  0.025727346373487028\n",
            "Epoch: 059, Loss: 1.7030, Train: 0.4931, Test: 0.4654\n",
            "Early stopping:  0.024596415598614193\n",
            "Epoch: 060, Loss: 1.6888, Train: 0.4927, Test: 0.4637\n",
            "Early stopping:  0.023379920510686488\n",
            "Epoch: 061, Loss: 1.6756, Train: 0.4971, Test: 0.4637\n",
            "Early stopping:  0.02265900456252018\n",
            "Epoch: 062, Loss: 1.6643, Train: 0.5032, Test: 0.4728\n",
            "Early stopping:  0.02069295224291171\n",
            "Epoch: 063, Loss: 1.6465, Train: 0.5080, Test: 0.4813\n",
            "Early stopping:  0.021805779863930338\n",
            "Epoch: 064, Loss: 1.6327, Train: 0.5118, Test: 0.4870\n",
            "Early stopping:  0.022410094962820603\n",
            "Epoch: 065, Loss: 1.6204, Train: 0.5127, Test: 0.4887\n",
            "Early stopping:  0.022514842470868438\n",
            "Epoch: 066, Loss: 1.6080, Train: 0.5179, Test: 0.4955\n",
            "Early stopping:  0.021984276848678146\n",
            "Epoch: 067, Loss: 1.5936, Train: 0.5152, Test: 0.4921\n",
            "Early stopping:  0.02061272307374039\n",
            "Epoch: 068, Loss: 1.5847, Train: 0.5181, Test: 0.4955\n",
            "Early stopping:  0.019457034414903096\n",
            "Epoch: 069, Loss: 1.5722, Train: 0.5240, Test: 0.4989\n",
            "Early stopping:  0.018974786567701563\n",
            "Epoch: 070, Loss: 1.5611, Train: 0.5267, Test: 0.4983\n",
            "Early stopping:  0.018257167610303857\n",
            "Epoch: 071, Loss: 1.5518, Train: 0.5279, Test: 0.4972\n",
            "Early stopping:  0.016985108618657103\n",
            "Epoch: 072, Loss: 1.5403, Train: 0.5263, Test: 0.4960\n",
            "Early stopping:  0.017265242389408658\n",
            "Epoch: 073, Loss: 1.5317, Train: 0.5291, Test: 0.5017\n",
            "Early stopping:  0.016086029270044655\n",
            "Epoch: 074, Loss: 1.5203, Train: 0.5328, Test: 0.5051\n",
            "Early stopping:  0.016100087866537535\n",
            "Epoch: 075, Loss: 1.5106, Train: 0.5364, Test: 0.5074\n",
            "Early stopping:  0.01618436269533658\n",
            "Epoch: 076, Loss: 1.5002, Train: 0.5379, Test: 0.5091\n",
            "Early stopping:  0.016042880511592524\n",
            "Epoch: 077, Loss: 1.4895, Train: 0.5401, Test: 0.5096\n",
            "Early stopping:  0.016548931810659175\n",
            "Epoch: 078, Loss: 1.4798, Train: 0.5436, Test: 0.5153\n",
            "Early stopping:  0.016142755005713175\n",
            "Epoch: 079, Loss: 1.4695, Train: 0.5467, Test: 0.5153\n",
            "Early stopping:  0.016213185435977096\n",
            "Epoch: 080, Loss: 1.4609, Train: 0.5450, Test: 0.5210\n",
            "Early stopping:  0.015588339518557499\n",
            "Epoch: 081, Loss: 1.4514, Train: 0.5450, Test: 0.5164\n",
            "Early stopping:  0.015047147699552848\n",
            "Epoch: 082, Loss: 1.4434, Train: 0.5501, Test: 0.5193\n",
            "Early stopping:  0.014408841952642783\n",
            "Epoch: 083, Loss: 1.4348, Train: 0.5524, Test: 0.5204\n",
            "Early stopping:  0.013757276196912088\n",
            "Epoch: 084, Loss: 1.4264, Train: 0.5535, Test: 0.5170\n",
            "Early stopping:  0.01352572480248437\n",
            "Epoch: 085, Loss: 1.4182, Train: 0.5535, Test: 0.5193\n",
            "Early stopping:  0.01317130671725053\n",
            "Epoch: 086, Loss: 1.4118, Train: 0.5585, Test: 0.5227\n",
            "Early stopping:  0.012629098229827641\n",
            "Epoch: 087, Loss: 1.4027, Train: 0.5582, Test: 0.5232\n",
            "Early stopping:  0.012479778143943741\n",
            "Epoch: 088, Loss: 1.3941, Train: 0.5603, Test: 0.5249\n",
            "Early stopping:  0.012697461308495544\n",
            "Epoch: 089, Loss: 1.3867, Train: 0.5632, Test: 0.5261\n",
            "Early stopping:  0.01278671484812235\n",
            "Epoch: 090, Loss: 1.3777, Train: 0.5646, Test: 0.5261\n",
            "Early stopping:  0.013320130183282366\n",
            "Epoch: 091, Loss: 1.3695, Train: 0.5646, Test: 0.5346\n",
            "Early stopping:  0.013099354343124462\n",
            "Epoch: 092, Loss: 1.3613, Train: 0.5651, Test: 0.5323\n",
            "Early stopping:  0.013100446918046415\n",
            "Epoch: 093, Loss: 1.3547, Train: 0.5704, Test: 0.5351\n",
            "Early stopping:  0.01273567115596062\n",
            "Epoch: 094, Loss: 1.3476, Train: 0.5714, Test: 0.5397\n",
            "Early stopping:  0.011860598967708814\n",
            "Epoch: 095, Loss: 1.3383, Train: 0.5755, Test: 0.5368\n",
            "Early stopping:  0.012037473896216462\n",
            "Epoch: 096, Loss: 1.3316, Train: 0.5813, Test: 0.5425\n",
            "Early stopping:  0.012004208798764533\n",
            "Epoch: 097, Loss: 1.3252, Train: 0.5820, Test: 0.5351\n",
            "Early stopping:  0.011895220702645003\n",
            "Epoch: 098, Loss: 1.3162, Train: 0.5822, Test: 0.5408\n",
            "Early stopping:  0.01203191449545296\n",
            "Epoch: 099, Loss: 1.3112, Train: 0.5866, Test: 0.5408\n",
            "Early stopping:  0.011041011343898365\n",
            "Epoch: 100, Loss: 1.3047, Train: 0.5894, Test: 0.5385\n",
            "Early stopping:  0.010763181272992653\n",
            "Epoch: 101, Loss: 1.2950, Train: 0.5910, Test: 0.5437\n",
            "Early stopping:  0.011436750753014837\n",
            "Epoch: 102, Loss: 1.2933, Train: 0.5922, Test: 0.5437\n",
            "Early stopping:  0.009973694478781695\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.47      0.51      0.49       190\n",
            "         capital_goods       0.50      0.30      0.37       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.51      0.53      0.52       198\n",
            " consumer_non-cyclical       0.58      0.30      0.40       112\n",
            "                energy       0.56      0.39      0.46        71\n",
            "             financial       0.67      0.52      0.59       192\n",
            "            healthcare       0.64      0.48      0.55        79\n",
            "              services       0.52      0.78      0.62       519\n",
            "            technology       0.65      0.20      0.31        99\n",
            "        transportation       0.69      0.56      0.62       101\n",
            "             utilities       0.62      0.64      0.63        56\n",
            "\n",
            "              accuracy                           0.54      1764\n",
            "             macro avg       0.53      0.44      0.46      1764\n",
            "          weighted avg       0.55      0.54      0.53      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 41.7350, Train: 0.2929, Test: 0.2920\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 524.5143, Train: 0.1224, Test: 0.1241\n",
            "Early stopping:  341.3765200315825\n",
            "Epoch: 003, Loss: 756.6544, Train: 0.1110, Test: 0.1088\n",
            "Early stopping:  364.70866824463985\n",
            "Epoch: 004, Loss: 839.3804, Train: 0.0825, Test: 0.0794\n",
            "Early stopping:  358.2709527482591\n",
            "Epoch: 005, Loss: 702.5474, Train: 0.0540, Test: 0.0488\n",
            "Early stopping:  318.61548347073375\n",
            "Epoch: 006, Loss: 549.3184, Train: 0.0749, Test: 0.0709\n",
            "Early stopping:  134.9885894099055\n",
            "Epoch: 007, Loss: 444.2590, Train: 0.0289, Test: 0.0244\n",
            "Early stopping:  159.77481834085998\n",
            "Epoch: 008, Loss: 371.8979, Train: 0.0630, Test: 0.0527\n",
            "Early stopping:  190.36712153936398\n",
            "Epoch: 009, Loss: 263.9988, Train: 0.0471, Test: 0.0363\n",
            "Early stopping:  168.0983087187419\n",
            "Epoch: 010, Loss: 151.1736, Train: 0.0533, Test: 0.0448\n",
            "Early stopping:  154.78405529183232\n",
            "Epoch: 011, Loss: 80.5393, Train: 0.0553, Test: 0.0505\n",
            "Early stopping:  150.42414595414218\n",
            "Epoch: 012, Loss: 45.0593, Train: 0.0736, Test: 0.0692\n",
            "Early stopping:  134.88967047993305\n",
            "Epoch: 013, Loss: 17.7464, Train: 0.2375, Test: 0.2387\n",
            "Early stopping:  98.73239306458736\n",
            "Epoch: 014, Loss: 5.8935, Train: 0.2848, Test: 0.2800\n",
            "Early stopping:  58.46056387341319\n",
            "Epoch: 015, Loss: 3.9835, Train: 0.3063, Test: 0.2942\n",
            "Early stopping:  32.353755816720614\n",
            "Epoch: 016, Loss: 3.4655, Train: 0.3010, Test: 0.2925\n",
            "Early stopping:  17.664806228414932\n",
            "Epoch: 017, Loss: 3.2643, Train: 0.3026, Test: 0.2965\n",
            "Early stopping:  6.16794343164206\n",
            "Epoch: 018, Loss: 3.0550, Train: 0.3050, Test: 0.2931\n",
            "Early stopping:  1.1491987577168095\n",
            "Epoch: 019, Loss: 2.8254, Train: 0.3080, Test: 0.3022\n",
            "Early stopping:  0.44138772346569805\n",
            "Epoch: 020, Loss: 2.6260, Train: 0.3116, Test: 0.2999\n",
            "Early stopping:  0.3349514295624116\n",
            "Epoch: 021, Loss: 2.4892, Train: 0.3142, Test: 0.2971\n",
            "Early stopping:  0.31410432391727366\n",
            "Epoch: 022, Loss: 2.4093, Train: 0.3041, Test: 0.2920\n",
            "Early stopping:  0.2619293196923579\n",
            "Epoch: 023, Loss: 2.3280, Train: 0.2970, Test: 0.2868\n",
            "Early stopping:  0.19581848668960733\n",
            "Epoch: 024, Loss: 2.2682, Train: 0.3061, Test: 0.2902\n",
            "Early stopping:  0.14031918112860728\n",
            "Epoch: 025, Loss: 2.2283, Train: 0.3087, Test: 0.2897\n",
            "Early stopping:  0.10573136574989715\n",
            "Epoch: 026, Loss: 2.2277, Train: 0.3000, Test: 0.2727\n",
            "Early stopping:  0.07714102406674063\n",
            "Epoch: 027, Loss: 2.2276, Train: 0.3002, Test: 0.2732\n",
            "Early stopping:  0.043874516004213694\n",
            "Epoch: 028, Loss: 2.2166, Train: 0.2992, Test: 0.2687\n",
            "Early stopping:  0.01988705007661193\n",
            "Epoch: 029, Loss: 2.2017, Train: 0.3013, Test: 0.2755\n",
            "Early stopping:  0.011521679050170696\n",
            "Epoch: 030, Loss: 2.1864, Train: 0.3041, Test: 0.2857\n",
            "Early stopping:  0.017820529487701825\n",
            "Epoch: 031, Loss: 2.1703, Train: 0.3038, Test: 0.2851\n",
            "Early stopping:  0.02294279349366641\n",
            "Epoch: 032, Loss: 2.1602, Train: 0.3088, Test: 0.2908\n",
            "Early stopping:  0.022872107960559475\n",
            "Epoch: 033, Loss: 2.1546, Train: 0.3119, Test: 0.2846\n",
            "Early stopping:  0.01937976040867022\n",
            "Epoch: 034, Loss: 2.1480, Train: 0.3156, Test: 0.2942\n",
            "Early stopping:  0.015001567331254585\n",
            "Epoch: 035, Loss: 2.1379, Train: 0.3238, Test: 0.2993\n",
            "Early stopping:  0.012239560457272914\n",
            "Epoch: 036, Loss: 2.1240, Train: 0.3304, Test: 0.3078\n",
            "Early stopping:  0.014326480328393568\n",
            "Epoch: 037, Loss: 2.1101, Train: 0.3365, Test: 0.3101\n",
            "Early stopping:  0.018032473373477558\n",
            "Epoch: 038, Loss: 2.0955, Train: 0.3424, Test: 0.3186\n",
            "Early stopping:  0.02105273282185249\n",
            "Epoch: 039, Loss: 2.0781, Train: 0.3479, Test: 0.3294\n",
            "Early stopping:  0.02344632327203246\n",
            "Epoch: 040, Loss: 2.0601, Train: 0.3555, Test: 0.3373\n",
            "Early stopping:  0.025318121695895028\n",
            "Epoch: 041, Loss: 2.0428, Train: 0.3672, Test: 0.3526\n",
            "Early stopping:  0.02690779649335485\n",
            "Epoch: 042, Loss: 2.0251, Train: 0.3712, Test: 0.3549\n",
            "Early stopping:  0.02785035855471649\n",
            "Epoch: 043, Loss: 2.0060, Train: 0.3784, Test: 0.3600\n",
            "Early stopping:  0.02834757217038362\n",
            "Epoch: 044, Loss: 1.9861, Train: 0.3868, Test: 0.3696\n",
            "Early stopping:  0.02925157794738429\n",
            "Epoch: 045, Loss: 1.9650, Train: 0.3933, Test: 0.3781\n",
            "Early stopping:  0.030770623186169358\n",
            "Epoch: 046, Loss: 1.9426, Train: 0.3995, Test: 0.3889\n",
            "Early stopping:  0.03256148936177148\n",
            "Epoch: 047, Loss: 1.9189, Train: 0.4024, Test: 0.3900\n",
            "Early stopping:  0.03444110178145708\n",
            "Epoch: 048, Loss: 1.8967, Train: 0.4078, Test: 0.3946\n",
            "Early stopping:  0.03557442186465044\n",
            "Epoch: 049, Loss: 1.8764, Train: 0.4157, Test: 0.3980\n",
            "Early stopping:  0.03531330775345086\n",
            "Epoch: 050, Loss: 1.8575, Train: 0.4190, Test: 0.4065\n",
            "Early stopping:  0.033670368242240914\n",
            "Epoch: 051, Loss: 1.8398, Train: 0.4197, Test: 0.4127\n",
            "Early stopping:  0.031229079601302733\n",
            "Epoch: 052, Loss: 1.8231, Train: 0.4195, Test: 0.4167\n",
            "Early stopping:  0.02907351852885148\n",
            "Epoch: 053, Loss: 1.8101, Train: 0.4208, Test: 0.4167\n",
            "Early stopping:  0.026479363189685457\n",
            "Epoch: 054, Loss: 1.7944, Train: 0.4262, Test: 0.4257\n",
            "Early stopping:  0.024702064546504095\n",
            "Epoch: 055, Loss: 1.7795, Train: 0.4314, Test: 0.4325\n",
            "Early stopping:  0.023613975100805484\n",
            "Epoch: 056, Loss: 1.7635, Train: 0.4380, Test: 0.4320\n",
            "Early stopping:  0.023704147103038246\n",
            "Epoch: 057, Loss: 1.7466, Train: 0.4448, Test: 0.4376\n",
            "Early stopping:  0.02496069981452605\n",
            "Epoch: 058, Loss: 1.7295, Train: 0.4469, Test: 0.4359\n",
            "Early stopping:  0.025731087262585123\n",
            "Epoch: 059, Loss: 1.7132, Train: 0.4531, Test: 0.4388\n",
            "Early stopping:  0.02632522054263234\n",
            "Epoch: 060, Loss: 1.6970, Train: 0.4564, Test: 0.4393\n",
            "Early stopping:  0.026281882963430456\n",
            "Epoch: 061, Loss: 1.6819, Train: 0.4553, Test: 0.4371\n",
            "Early stopping:  0.025589171431717832\n",
            "Epoch: 062, Loss: 1.6678, Train: 0.4595, Test: 0.4416\n",
            "Early stopping:  0.024479684829006473\n",
            "Epoch: 063, Loss: 1.6544, Train: 0.4673, Test: 0.4473\n",
            "Early stopping:  0.023239677946239375\n",
            "Epoch: 064, Loss: 1.6402, Train: 0.4711, Test: 0.4529\n",
            "Early stopping:  0.02231814025727935\n",
            "Epoch: 065, Loss: 1.6264, Train: 0.4744, Test: 0.4580\n",
            "Early stopping:  0.021918247388839306\n",
            "Epoch: 066, Loss: 1.6130, Train: 0.4763, Test: 0.4637\n",
            "Early stopping:  0.021766741365173238\n",
            "Epoch: 067, Loss: 1.6001, Train: 0.4812, Test: 0.4688\n",
            "Early stopping:  0.021507774595283417\n",
            "Epoch: 068, Loss: 1.5873, Train: 0.4862, Test: 0.4677\n",
            "Early stopping:  0.020893899478405954\n",
            "Epoch: 069, Loss: 1.5753, Train: 0.4894, Test: 0.4711\n",
            "Early stopping:  0.02021381370346411\n",
            "Epoch: 070, Loss: 1.5635, Train: 0.4904, Test: 0.4790\n",
            "Early stopping:  0.019545973676829826\n",
            "Epoch: 071, Loss: 1.5517, Train: 0.4938, Test: 0.4813\n",
            "Early stopping:  0.019045899718778903\n",
            "Epoch: 072, Loss: 1.5407, Train: 0.4982, Test: 0.4881\n",
            "Early stopping:  0.018479740222207455\n",
            "Epoch: 073, Loss: 1.5297, Train: 0.5036, Test: 0.4949\n",
            "Early stopping:  0.018055157596279722\n",
            "Epoch: 074, Loss: 1.5188, Train: 0.5042, Test: 0.4966\n",
            "Early stopping:  0.01764569147010843\n",
            "Epoch: 075, Loss: 1.5074, Train: 0.5110, Test: 0.4949\n",
            "Early stopping:  0.017476994554586865\n",
            "Epoch: 076, Loss: 1.4971, Train: 0.5152, Test: 0.4909\n",
            "Early stopping:  0.017309238738726654\n",
            "Epoch: 077, Loss: 1.4877, Train: 0.5172, Test: 0.4926\n",
            "Early stopping:  0.016709872625846955\n",
            "Epoch: 078, Loss: 1.4787, Train: 0.5216, Test: 0.4966\n",
            "Early stopping:  0.015815844128741225\n",
            "Epoch: 079, Loss: 1.4691, Train: 0.5225, Test: 0.5023\n",
            "Early stopping:  0.015027216366496501\n",
            "Epoch: 080, Loss: 1.4597, Train: 0.5272, Test: 0.5057\n",
            "Early stopping:  0.014755222135590933\n",
            "Epoch: 081, Loss: 1.4507, Train: 0.5300, Test: 0.5091\n",
            "Early stopping:  0.01471452204985508\n",
            "Epoch: 082, Loss: 1.4419, Train: 0.5304, Test: 0.5147\n",
            "Early stopping:  0.014536312680671943\n",
            "Epoch: 083, Loss: 1.4337, Train: 0.5347, Test: 0.5142\n",
            "Early stopping:  0.014015716600078536\n",
            "Epoch: 084, Loss: 1.4246, Train: 0.5355, Test: 0.5187\n",
            "Early stopping:  0.013783505575164346\n",
            "Epoch: 085, Loss: 1.4159, Train: 0.5375, Test: 0.5227\n",
            "Early stopping:  0.01374365644727236\n",
            "Epoch: 086, Loss: 1.4071, Train: 0.5415, Test: 0.5261\n",
            "Early stopping:  0.01384822511354947\n",
            "Epoch: 087, Loss: 1.3985, Train: 0.5449, Test: 0.5255\n",
            "Early stopping:  0.013895554976413128\n",
            "Epoch: 088, Loss: 1.3901, Train: 0.5522, Test: 0.5312\n",
            "Early stopping:  0.013671666575227323\n",
            "Epoch: 089, Loss: 1.3822, Train: 0.5511, Test: 0.5278\n",
            "Early stopping:  0.013322682860975987\n",
            "Epoch: 090, Loss: 1.3756, Train: 0.5588, Test: 0.5340\n",
            "Early stopping:  0.01254738382910844\n",
            "Epoch: 091, Loss: 1.3694, Train: 0.5579, Test: 0.5380\n",
            "Early stopping:  0.011534526118276553\n",
            "Epoch: 092, Loss: 1.3632, Train: 0.5644, Test: 0.5380\n",
            "Early stopping:  0.01054299660486285\n",
            "Epoch: 093, Loss: 1.3544, Train: 0.5657, Test: 0.5402\n",
            "Early stopping:  0.010784518906684047\n",
            "Epoch: 094, Loss: 1.3430, Train: 0.5677, Test: 0.5437\n",
            "Early stopping:  0.012808351386647288\n",
            "Epoch: 095, Loss: 1.3348, Train: 0.5735, Test: 0.5476\n",
            "Early stopping:  0.014206125684897288\n",
            "Epoch: 096, Loss: 1.3303, Train: 0.5704, Test: 0.5482\n",
            "Early stopping:  0.013650141738879355\n",
            "Epoch: 097, Loss: 1.3251, Train: 0.5752, Test: 0.5465\n",
            "Early stopping:  0.01150911915935139\n",
            "Epoch: 098, Loss: 1.3163, Train: 0.5765, Test: 0.5522\n",
            "Early stopping:  0.010021705315135437\n",
            "Epoch: 099, Loss: 1.3057, Train: 0.5803, Test: 0.5550\n",
            "Early stopping:  0.011596288677256818\n",
            "Epoch: 100, Loss: 1.3013, Train: 0.5824, Test: 0.5612\n",
            "Early stopping:  0.012324557255853597\n",
            "Epoch: 101, Loss: 1.2976, Train: 0.5876, Test: 0.5584\n",
            "Early stopping:  0.011316315530614117\n",
            "Epoch: 102, Loss: 1.2885, Train: 0.5830, Test: 0.5629\n",
            "Early stopping:  0.010269529498557858\n",
            "Epoch: 103, Loss: 1.2858, Train: 0.5884, Test: 0.5624\n",
            "Early stopping:  0.008436331667057463\n",
            "PREDICTIONS -> tensor([ 9,  0,  8,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.55      0.51      0.53       190\n",
            "         capital_goods       0.40      0.26      0.31       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.62      0.41      0.50       198\n",
            " consumer_non-cyclical       0.62      0.46      0.53       112\n",
            "                energy       0.79      0.58      0.67        71\n",
            "             financial       0.66      0.54      0.59       192\n",
            "            healthcare       0.83      0.19      0.31        79\n",
            "              services       0.50      0.84      0.63       519\n",
            "            technology       0.56      0.29      0.38        99\n",
            "        transportation       0.78      0.61      0.69       101\n",
            "             utilities       0.72      0.77      0.74        56\n",
            "\n",
            "              accuracy                           0.56      1764\n",
            "             macro avg       0.58      0.46      0.49      1764\n",
            "          weighted avg       0.58      0.56      0.54      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 66.5985, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 409.7770, Train: 0.0547, Test: 0.0561\n",
            "Early stopping:  242.6638372136626\n",
            "Epoch: 003, Loss: 262.1969, Train: 0.1103, Test: 0.1071\n",
            "Early stopping:  172.1482374034643\n",
            "Epoch: 004, Loss: 300.9800, Train: 0.0767, Test: 0.0771\n",
            "Early stopping:  143.2031429597035\n",
            "Epoch: 005, Loss: 320.2294, Train: 0.1293, Test: 0.1219\n",
            "Early stopping:  126.91953910010615\n",
            "Epoch: 006, Loss: 204.3776, Train: 0.0936, Test: 0.0884\n",
            "Early stopping:  75.8826517024421\n",
            "Epoch: 007, Loss: 137.4511, Train: 0.1338, Test: 0.1310\n",
            "Early stopping:  74.6757952179252\n",
            "Epoch: 008, Loss: 89.2679, Train: 0.3131, Test: 0.3044\n",
            "Early stopping:  100.37181915242672\n",
            "Epoch: 009, Loss: 81.0931, Train: 0.2989, Test: 0.2920\n",
            "Early stopping:  98.95757400469385\n",
            "Epoch: 010, Loss: 64.8056, Train: 0.2573, Test: 0.2523\n",
            "Early stopping:  56.619131017472704\n",
            "Epoch: 011, Loss: 43.7190, Train: 0.2379, Test: 0.2194\n",
            "Early stopping:  34.933933022601714\n",
            "Epoch: 012, Loss: 19.3798, Train: 0.2049, Test: 0.1995\n",
            "Early stopping:  28.456524414943463\n",
            "Epoch: 013, Loss: 7.7391, Train: 0.1456, Test: 0.1315\n",
            "Early stopping:  30.52757416246201\n",
            "Epoch: 014, Loss: 5.6349, Train: 0.1381, Test: 0.1264\n",
            "Early stopping:  25.428434824336264\n",
            "Epoch: 015, Loss: 4.2230, Train: 0.1703, Test: 0.1644\n",
            "Early stopping:  16.538177725162313\n",
            "Epoch: 016, Loss: 3.2779, Train: 0.2001, Test: 0.1842\n",
            "Early stopping:  6.551964091524287\n",
            "Epoch: 017, Loss: 2.7874, Train: 0.1992, Test: 0.1882\n",
            "Early stopping:  2.000787296300805\n",
            "Epoch: 018, Loss: 2.6029, Train: 0.1860, Test: 0.1661\n",
            "Early stopping:  1.2481712336690822\n",
            "Epoch: 019, Loss: 2.5196, Train: 0.1701, Test: 0.1542\n",
            "Early stopping:  0.7022864392584955\n",
            "Epoch: 020, Loss: 2.4663, Train: 0.1555, Test: 0.1474\n",
            "Early stopping:  0.32918410267797216\n",
            "Epoch: 021, Loss: 2.4327, Train: 0.1499, Test: 0.1406\n",
            "Early stopping:  0.14157371655722342\n",
            "Epoch: 022, Loss: 2.4043, Train: 0.1456, Test: 0.1378\n",
            "Early stopping:  0.07858819409876289\n",
            "Epoch: 023, Loss: 2.3804, Train: 0.1462, Test: 0.1406\n",
            "Early stopping:  0.05455954839413631\n",
            "Epoch: 024, Loss: 2.3603, Train: 0.1499, Test: 0.1429\n",
            "Early stopping:  0.04200009755364872\n",
            "Epoch: 025, Loss: 2.3455, Train: 0.1531, Test: 0.1463\n",
            "Early stopping:  0.03478089423213124\n",
            "Epoch: 026, Loss: 2.3364, Train: 0.1541, Test: 0.1514\n",
            "Early stopping:  0.027373311093338248\n",
            "Epoch: 027, Loss: 2.3301, Train: 0.2096, Test: 0.2098\n",
            "Early stopping:  0.020157281391262483\n",
            "Epoch: 028, Loss: 2.3242, Train: 0.2111, Test: 0.2115\n",
            "Early stopping:  0.014145851602993443\n",
            "Epoch: 029, Loss: 2.3176, Train: 0.2157, Test: 0.2160\n",
            "Early stopping:  0.010784375388998103\n",
            "Epoch: 030, Loss: 2.3107, Train: 0.2223, Test: 0.2200\n",
            "Early stopping:  0.010114919165120358\n",
            "Epoch: 031, Loss: 2.3034, Train: 0.2311, Test: 0.2245\n",
            "Early stopping:  0.010607828832293763\n",
            "Epoch: 032, Loss: 2.2948, Train: 0.2362, Test: 0.2296\n",
            "Early stopping:  0.011567480315708936\n",
            "Epoch: 033, Loss: 2.2850, Train: 0.2412, Test: 0.2358\n",
            "Early stopping:  0.012860276244222285\n",
            "Epoch: 034, Loss: 2.2729, Train: 0.2454, Test: 0.2370\n",
            "Early stopping:  0.014931720137127688\n",
            "Epoch: 035, Loss: 2.2601, Train: 0.2439, Test: 0.2347\n",
            "Early stopping:  0.017215152126183655\n",
            "Epoch: 036, Loss: 2.2482, Train: 0.2355, Test: 0.2336\n",
            "Early stopping:  0.01871383384866892\n",
            "Epoch: 037, Loss: 2.2375, Train: 0.2371, Test: 0.2313\n",
            "Early stopping:  0.018939631193932695\n",
            "Epoch: 038, Loss: 2.2196, Train: 0.2491, Test: 0.2404\n",
            "Early stopping:  0.020504173929960613\n",
            "Epoch: 039, Loss: 2.1986, Train: 0.2603, Test: 0.2460\n",
            "Early stopping:  0.02421242130100946\n",
            "Epoch: 040, Loss: 2.1782, Train: 0.2705, Test: 0.2534\n",
            "Early stopping:  0.028458145645355218\n",
            "Epoch: 041, Loss: 2.1530, Train: 0.2732, Test: 0.2772\n",
            "Early stopping:  0.03333491631193076\n",
            "Epoch: 042, Loss: 2.1240, Train: 0.2795, Test: 0.2806\n",
            "Early stopping:  0.03756557500410975\n",
            "Epoch: 043, Loss: 2.1051, Train: 0.3189, Test: 0.3152\n",
            "Early stopping:  0.038219755192068584\n",
            "Epoch: 044, Loss: 2.0710, Train: 0.3306, Test: 0.3118\n",
            "Early stopping:  0.0416001545702063\n",
            "Epoch: 045, Loss: 2.0488, Train: 0.3408, Test: 0.3209\n",
            "Early stopping:  0.04143593655386851\n",
            "Epoch: 046, Loss: 2.0229, Train: 0.3529, Test: 0.3226\n",
            "Early stopping:  0.040984483425660176\n",
            "Epoch: 047, Loss: 1.9974, Train: 0.3625, Test: 0.3345\n",
            "Early stopping:  0.04177087580639766\n",
            "Epoch: 048, Loss: 1.9805, Train: 0.3878, Test: 0.3634\n",
            "Early stopping:  0.03681934038762374\n",
            "Epoch: 049, Loss: 1.9614, Train: 0.3899, Test: 0.3668\n",
            "Early stopping:  0.03449942248523295\n",
            "Epoch: 050, Loss: 1.9386, Train: 0.3902, Test: 0.3685\n",
            "Early stopping:  0.03241802384235673\n",
            "Epoch: 051, Loss: 1.9187, Train: 0.3953, Test: 0.3776\n",
            "Early stopping:  0.031551367749465306\n",
            "Epoch: 052, Loss: 1.9000, Train: 0.4010, Test: 0.3838\n",
            "Early stopping:  0.03221668326749324\n",
            "Epoch: 053, Loss: 1.8826, Train: 0.4079, Test: 0.3878\n",
            "Early stopping:  0.03106889011955547\n",
            "Epoch: 054, Loss: 1.8649, Train: 0.4129, Test: 0.3872\n",
            "Early stopping:  0.029035380774624784\n",
            "Epoch: 055, Loss: 1.8463, Train: 0.4198, Test: 0.3940\n",
            "Early stopping:  0.0284665903237319\n",
            "Epoch: 056, Loss: 1.8319, Train: 0.4246, Test: 0.4014\n",
            "Early stopping:  0.027311560574287904\n",
            "Epoch: 057, Loss: 1.8179, Train: 0.4252, Test: 0.4059\n",
            "Early stopping:  0.025727615844797627\n",
            "Epoch: 058, Loss: 1.8063, Train: 0.4292, Test: 0.4104\n",
            "Early stopping:  0.02310202634137455\n",
            "Epoch: 059, Loss: 1.7927, Train: 0.4331, Test: 0.4104\n",
            "Early stopping:  0.021010979997497917\n",
            "Epoch: 060, Loss: 1.7765, Train: 0.4371, Test: 0.4167\n",
            "Early stopping:  0.02151674275670137\n",
            "Epoch: 061, Loss: 1.7605, Train: 0.4395, Test: 0.4240\n",
            "Early stopping:  0.02288431772547573\n",
            "Epoch: 062, Loss: 1.7460, Train: 0.4405, Test: 0.4240\n",
            "Early stopping:  0.024140852543129608\n",
            "Epoch: 063, Loss: 1.7342, Train: 0.4417, Test: 0.4252\n",
            "Early stopping:  0.02335160056294395\n",
            "Epoch: 064, Loss: 1.7232, Train: 0.4426, Test: 0.4257\n",
            "Early stopping:  0.021101435933244823\n",
            "Epoch: 065, Loss: 1.7119, Train: 0.4451, Test: 0.4291\n",
            "Early stopping:  0.01902617811364112\n",
            "Epoch: 066, Loss: 1.7005, Train: 0.4466, Test: 0.4314\n",
            "Early stopping:  0.017928642795450533\n",
            "Epoch: 067, Loss: 1.6878, Train: 0.4480, Test: 0.4331\n",
            "Early stopping:  0.018278581770126846\n",
            "Epoch: 068, Loss: 1.6765, Train: 0.4537, Test: 0.4371\n",
            "Early stopping:  0.018581138868259076\n",
            "Epoch: 069, Loss: 1.6644, Train: 0.4624, Test: 0.4456\n",
            "Early stopping:  0.01881572092107449\n",
            "Epoch: 070, Loss: 1.6525, Train: 0.4652, Test: 0.4495\n",
            "Early stopping:  0.01888985431964139\n",
            "Epoch: 071, Loss: 1.6425, Train: 0.4669, Test: 0.4541\n",
            "Early stopping:  0.018142738733031565\n",
            "Epoch: 072, Loss: 1.6324, Train: 0.4693, Test: 0.4541\n",
            "Early stopping:  0.017433143636967085\n",
            "Epoch: 073, Loss: 1.6224, Train: 0.4726, Test: 0.4541\n",
            "Early stopping:  0.01645807155833503\n",
            "Epoch: 074, Loss: 1.6128, Train: 0.4743, Test: 0.4586\n",
            "Early stopping:  0.015706318349718865\n",
            "Epoch: 075, Loss: 1.6025, Train: 0.4787, Test: 0.4592\n",
            "Early stopping:  0.015735415715422498\n",
            "Epoch: 076, Loss: 1.5925, Train: 0.4799, Test: 0.4603\n",
            "Early stopping:  0.01576075218388746\n",
            "Epoch: 077, Loss: 1.5829, Train: 0.4832, Test: 0.4615\n",
            "Early stopping:  0.01570883169467594\n",
            "Epoch: 078, Loss: 1.5737, Train: 0.4863, Test: 0.4637\n",
            "Early stopping:  0.015463594147094032\n",
            "Epoch: 079, Loss: 1.5642, Train: 0.4899, Test: 0.4666\n",
            "Early stopping:  0.015080806500432305\n",
            "Epoch: 080, Loss: 1.5547, Train: 0.4906, Test: 0.4666\n",
            "Early stopping:  0.014929348674457442\n",
            "Epoch: 081, Loss: 1.5451, Train: 0.4941, Test: 0.4705\n",
            "Early stopping:  0.014957696914694923\n",
            "Epoch: 082, Loss: 1.5350, Train: 0.4981, Test: 0.4683\n",
            "Early stopping:  0.015264122500560041\n",
            "Epoch: 083, Loss: 1.5248, Train: 0.5016, Test: 0.4694\n",
            "Early stopping:  0.015579338304697864\n",
            "Epoch: 084, Loss: 1.5152, Train: 0.5018, Test: 0.4700\n",
            "Early stopping:  0.015695289712226113\n",
            "Epoch: 085, Loss: 1.5064, Train: 0.5070, Test: 0.4779\n",
            "Early stopping:  0.015397593892333935\n",
            "Epoch: 086, Loss: 1.4971, Train: 0.5140, Test: 0.4881\n",
            "Early stopping:  0.014897371687974868\n",
            "Epoch: 087, Loss: 1.4880, Train: 0.5191, Test: 0.4921\n",
            "Early stopping:  0.014495539721857595\n",
            "Epoch: 088, Loss: 1.4783, Train: 0.5223, Test: 0.4955\n",
            "Early stopping:  0.014588857642270926\n",
            "Epoch: 089, Loss: 1.4688, Train: 0.5276, Test: 0.4955\n",
            "Early stopping:  0.014859223006170976\n",
            "Epoch: 090, Loss: 1.4589, Train: 0.5290, Test: 0.4983\n",
            "Early stopping:  0.015120070002804267\n",
            "Epoch: 091, Loss: 1.4491, Train: 0.5318, Test: 0.5006\n",
            "Early stopping:  0.015358741842102877\n",
            "Epoch: 092, Loss: 1.4394, Train: 0.5355, Test: 0.5051\n",
            "Early stopping:  0.01541277201120053\n",
            "Epoch: 093, Loss: 1.4290, Train: 0.5389, Test: 0.5079\n",
            "Early stopping:  0.01568248050275199\n",
            "Epoch: 094, Loss: 1.4185, Train: 0.5415, Test: 0.5119\n",
            "Early stopping:  0.015947858106481846\n",
            "Epoch: 095, Loss: 1.4088, Train: 0.5426, Test: 0.5130\n",
            "Early stopping:  0.01604222814459607\n",
            "Epoch: 096, Loss: 1.4040, Train: 0.5439, Test: 0.5164\n",
            "Early stopping:  0.014494384018974422\n",
            "Epoch: 097, Loss: 1.4069, Train: 0.5477, Test: 0.5215\n",
            "Early stopping:  0.010261250908749594\n",
            "Epoch: 098, Loss: 1.3820, Train: 0.5474, Test: 0.5170\n",
            "Early stopping:  0.01347575263698234\n",
            "Epoch: 099, Loss: 1.3759, Train: 0.5548, Test: 0.5193\n",
            "Early stopping:  0.015372960526242352\n",
            "Epoch: 100, Loss: 1.3753, Train: 0.5595, Test: 0.5255\n",
            "Early stopping:  0.01544378304569246\n",
            "Epoch: 101, Loss: 1.3553, Train: 0.5581, Test: 0.5227\n",
            "Early stopping:  0.01854006302515651\n",
            "Epoch: 102, Loss: 1.3609, Train: 0.5681, Test: 0.5278\n",
            "Early stopping:  0.011260287497444012\n",
            "Epoch: 103, Loss: 1.3439, Train: 0.5664, Test: 0.5289\n",
            "Early stopping:  0.013618426060063419\n",
            "Epoch: 104, Loss: 1.3458, Train: 0.5708, Test: 0.5329\n",
            "Early stopping:  0.012699032204973194\n",
            "Epoch: 105, Loss: 1.3260, Train: 0.5717, Test: 0.5397\n",
            "Early stopping:  0.013318652482500092\n",
            "Epoch: 106, Loss: 1.3218, Train: 0.5769, Test: 0.5340\n",
            "Early stopping:  0.015909841367136662\n",
            "Epoch: 107, Loss: 1.3119, Train: 0.5764, Test: 0.5357\n",
            "Early stopping:  0.014620935359490744\n",
            "Epoch: 108, Loss: 1.3043, Train: 0.5790, Test: 0.5471\n",
            "Early stopping:  0.015791814157208244\n",
            "Epoch: 109, Loss: 1.2948, Train: 0.5841, Test: 0.5505\n",
            "Early stopping:  0.012728293328194534\n",
            "Epoch: 110, Loss: 1.2891, Train: 0.5857, Test: 0.5402\n",
            "Early stopping:  0.013079620156874456\n",
            "Epoch: 111, Loss: 1.2828, Train: 0.5850, Test: 0.5488\n",
            "Early stopping:  0.011679990718834203\n",
            "Epoch: 112, Loss: 1.2734, Train: 0.5843, Test: 0.5522\n",
            "Early stopping:  0.011734339232071032\n",
            "Epoch: 113, Loss: 1.2627, Train: 0.5907, Test: 0.5442\n",
            "Early stopping:  0.012737412016631759\n",
            "Epoch: 114, Loss: 1.2638, Train: 0.5864, Test: 0.5539\n",
            "Early stopping:  0.011580684655460312\n",
            "Epoch: 115, Loss: 1.2535, Train: 0.5864, Test: 0.5522\n",
            "Early stopping:  0.01116888884473921\n",
            "Epoch: 116, Loss: 1.2514, Train: 0.5944, Test: 0.5522\n",
            "Early stopping:  0.00882720753539239\n",
            "PREDICTIONS -> tensor([ 9, 11, 11,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.55      0.46      0.50       190\n",
            "         capital_goods       0.46      0.18      0.26       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.49      0.41      0.45       198\n",
            " consumer_non-cyclical       0.76      0.28      0.41       112\n",
            "                energy       0.68      0.51      0.58        71\n",
            "             financial       0.72      0.60      0.66       192\n",
            "            healthcare       0.67      0.44      0.53        79\n",
            "              services       0.50      0.84      0.63       519\n",
            "            technology       0.46      0.32      0.38        99\n",
            "        transportation       0.61      0.64      0.63       101\n",
            "             utilities       0.72      0.61      0.66        56\n",
            "\n",
            "              accuracy                           0.55      1764\n",
            "             macro avg       0.55      0.44      0.47      1764\n",
            "          weighted avg       0.56      0.55      0.53      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 52.0266, Train: 0.2938, Test: 0.2925\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 371.2192, Train: 0.1134, Test: 0.1151\n",
            "Early stopping:  225.70327641809925\n",
            "Epoch: 003, Loss: 320.9401, Train: 0.1175, Test: 0.1134\n",
            "Early stopping:  171.62284015870063\n",
            "Epoch: 004, Loss: 383.2656, Train: 0.1129, Test: 0.1083\n",
            "Early stopping:  155.5836384265586\n",
            "Epoch: 005, Loss: 449.7900, Train: 0.0720, Test: 0.0697\n",
            "Early stopping:  154.25500930788047\n",
            "Epoch: 006, Loss: 374.0192, Train: 0.0465, Test: 0.0527\n",
            "Early stopping:  46.04926733708194\n",
            "Epoch: 007, Loss: 323.7908, Train: 0.0610, Test: 0.0601\n",
            "Early stopping:  52.67848689796697\n",
            "Epoch: 008, Loss: 227.6775, Train: 0.2987, Test: 0.2959\n",
            "Early stopping:  82.57829333267958\n",
            "Epoch: 009, Loss: 182.4074, Train: 0.3013, Test: 0.2965\n",
            "Early stopping:  108.25312707417288\n",
            "Epoch: 010, Loss: 188.8425, Train: 0.3020, Test: 0.2931\n",
            "Early stopping:  85.43458231231214\n",
            "Epoch: 011, Loss: 147.0487, Train: 0.2460, Test: 0.2511\n",
            "Early stopping:  67.73948173224251\n",
            "Epoch: 012, Loss: 98.7478, Train: 0.0956, Test: 0.0975\n",
            "Early stopping:  48.56374216859743\n",
            "Epoch: 013, Loss: 68.3669, Train: 0.1002, Test: 0.1032\n",
            "Early stopping:  52.49799393019338\n",
            "Epoch: 014, Loss: 37.5451, Train: 0.2338, Test: 0.2477\n",
            "Early stopping:  60.58161310808526\n",
            "Epoch: 015, Loss: 15.1090, Train: 0.2466, Test: 0.2438\n",
            "Early stopping:  51.902271593634225\n",
            "Epoch: 016, Loss: 5.6704, Train: 0.2115, Test: 0.2041\n",
            "Early stopping:  38.5053970649836\n",
            "Epoch: 017, Loss: 3.8749, Train: 0.1825, Test: 0.1740\n",
            "Early stopping:  27.155137436447422\n",
            "Epoch: 018, Loss: 3.4369, Train: 0.1883, Test: 0.1837\n",
            "Early stopping:  14.4505451136948\n",
            "Epoch: 019, Loss: 3.0278, Train: 0.1975, Test: 0.1973\n",
            "Early stopping:  5.068326647191408\n",
            "Epoch: 020, Loss: 2.7092, Train: 0.1993, Test: 0.1995\n",
            "Early stopping:  1.1625391514920356\n",
            "Epoch: 021, Loss: 2.5543, Train: 0.1867, Test: 0.1910\n",
            "Early stopping:  0.5402702747031709\n",
            "Epoch: 022, Loss: 2.4999, Train: 0.1823, Test: 0.1820\n",
            "Early stopping:  0.3891772854160811\n",
            "Epoch: 023, Loss: 2.4625, Train: 0.1802, Test: 0.1820\n",
            "Early stopping:  0.23079300338210249\n",
            "Epoch: 024, Loss: 2.4229, Train: 0.1863, Test: 0.1803\n",
            "Early stopping:  0.11141801609027562\n",
            "Epoch: 025, Loss: 2.3816, Train: 0.1908, Test: 0.1837\n",
            "Early stopping:  0.06693140913428475\n",
            "Epoch: 026, Loss: 2.3463, Train: 0.1975, Test: 0.1933\n",
            "Early stopping:  0.06138520408112161\n",
            "Epoch: 027, Loss: 2.3212, Train: 0.2080, Test: 0.2041\n",
            "Early stopping:  0.057022682749588834\n",
            "Epoch: 028, Loss: 2.3006, Train: 0.2174, Test: 0.2149\n",
            "Early stopping:  0.048707153489684706\n",
            "Epoch: 029, Loss: 2.2796, Train: 0.2310, Test: 0.2296\n",
            "Early stopping:  0.0397501688423956\n",
            "Epoch: 030, Loss: 2.2587, Train: 0.2446, Test: 0.2336\n",
            "Early stopping:  0.034287036796872054\n",
            "Epoch: 031, Loss: 2.2376, Train: 0.2498, Test: 0.2364\n",
            "Early stopping:  0.03307395218721457\n",
            "Epoch: 032, Loss: 2.2272, Train: 0.2701, Test: 0.2472\n",
            "Early stopping:  0.030060316283643182\n",
            "Epoch: 033, Loss: 2.2009, Train: 0.2863, Test: 0.2721\n",
            "Early stopping:  0.030074944443732488\n",
            "Epoch: 034, Loss: 2.1901, Train: 0.3004, Test: 0.2902\n",
            "Early stopping:  0.027712799666182206\n",
            "Epoch: 035, Loss: 2.1799, Train: 0.3067, Test: 0.2908\n",
            "Early stopping:  0.024466374868651223\n",
            "Epoch: 036, Loss: 2.1609, Train: 0.3016, Test: 0.2846\n",
            "Early stopping:  0.024654423263756456\n",
            "Epoch: 037, Loss: 2.1511, Train: 0.3071, Test: 0.2959\n",
            "Early stopping:  0.020505562915921968\n",
            "Epoch: 038, Loss: 2.1259, Train: 0.3155, Test: 0.3016\n",
            "Early stopping:  0.02519048920429203\n",
            "Epoch: 039, Loss: 2.1023, Train: 0.3186, Test: 0.3084\n",
            "Early stopping:  0.03039052056584021\n",
            "Epoch: 040, Loss: 2.0840, Train: 0.3224, Test: 0.3050\n",
            "Early stopping:  0.03227362961201506\n",
            "Epoch: 041, Loss: 2.0637, Train: 0.3226, Test: 0.3112\n",
            "Early stopping:  0.03432874372252019\n",
            "Epoch: 042, Loss: 2.0438, Train: 0.3380, Test: 0.3209\n",
            "Early stopping:  0.03207464060044947\n",
            "Epoch: 043, Loss: 2.0266, Train: 0.3601, Test: 0.3435\n",
            "Early stopping:  0.030307146459034873\n",
            "Epoch: 044, Loss: 2.0102, Train: 0.3674, Test: 0.3509\n",
            "Early stopping:  0.029230088430346347\n",
            "Epoch: 045, Loss: 1.9845, Train: 0.3699, Test: 0.3492\n",
            "Early stopping:  0.030466406395988764\n",
            "Epoch: 046, Loss: 1.9634, Train: 0.3740, Test: 0.3549\n",
            "Early stopping:  0.0322026805188338\n",
            "Epoch: 047, Loss: 1.9418, Train: 0.3774, Test: 0.3634\n",
            "Early stopping:  0.03427470522553738\n",
            "Epoch: 048, Loss: 1.9243, Train: 0.3874, Test: 0.3673\n",
            "Early stopping:  0.0340074897535257\n",
            "Epoch: 049, Loss: 1.9063, Train: 0.4015, Test: 0.3810\n",
            "Early stopping:  0.030952638012030683\n",
            "Epoch: 050, Loss: 1.8878, Train: 0.4176, Test: 0.3912\n",
            "Early stopping:  0.029558637262115675\n",
            "Epoch: 051, Loss: 1.8711, Train: 0.4218, Test: 0.3991\n",
            "Early stopping:  0.028124804181834842\n",
            "Epoch: 052, Loss: 1.8521, Train: 0.4295, Test: 0.4065\n",
            "Early stopping:  0.028388713302807324\n",
            "Epoch: 053, Loss: 1.8380, Train: 0.4384, Test: 0.4150\n",
            "Early stopping:  0.027248912799367593\n",
            "Epoch: 054, Loss: 1.8229, Train: 0.4309, Test: 0.4121\n",
            "Early stopping:  0.025764664329497238\n",
            "Epoch: 055, Loss: 1.8107, Train: 0.4275, Test: 0.4036\n",
            "Early stopping:  0.0237654489632096\n",
            "Epoch: 056, Loss: 1.7973, Train: 0.4258, Test: 0.4059\n",
            "Early stopping:  0.021650572351012965\n",
            "Epoch: 057, Loss: 1.7833, Train: 0.4265, Test: 0.4121\n",
            "Early stopping:  0.021365039726556585\n",
            "Epoch: 058, Loss: 1.7712, Train: 0.4388, Test: 0.4223\n",
            "Early stopping:  0.020712020232346702\n",
            "Epoch: 059, Loss: 1.7560, Train: 0.4510, Test: 0.4337\n",
            "Early stopping:  0.02143815896526761\n",
            "Epoch: 060, Loss: 1.7448, Train: 0.4548, Test: 0.4337\n",
            "Early stopping:  0.020932630846143\n",
            "Epoch: 061, Loss: 1.7337, Train: 0.4524, Test: 0.4416\n",
            "Early stopping:  0.019897500048260235\n",
            "Epoch: 062, Loss: 1.7229, Train: 0.4538, Test: 0.4382\n",
            "Early stopping:  0.01886297965924554\n",
            "Epoch: 063, Loss: 1.7152, Train: 0.4609, Test: 0.4410\n",
            "Early stopping:  0.016429871420035828\n",
            "Epoch: 064, Loss: 1.7041, Train: 0.4621, Test: 0.4433\n",
            "Early stopping:  0.0158312294927464\n",
            "Epoch: 065, Loss: 1.6963, Train: 0.4663, Test: 0.4484\n",
            "Early stopping:  0.01480146222424875\n",
            "Epoch: 066, Loss: 1.6856, Train: 0.4653, Test: 0.4478\n",
            "Early stopping:  0.014785647486222913\n",
            "Epoch: 067, Loss: 1.6780, Train: 0.4682, Test: 0.4495\n",
            "Early stopping:  0.014700467196344331\n",
            "Epoch: 068, Loss: 1.6682, Train: 0.4706, Test: 0.4518\n",
            "Early stopping:  0.014239846874193008\n",
            "Epoch: 069, Loss: 1.6601, Train: 0.4760, Test: 0.4603\n",
            "Early stopping:  0.014225561988984026\n",
            "Epoch: 070, Loss: 1.6508, Train: 0.4797, Test: 0.4603\n",
            "Early stopping:  0.013867829690632076\n",
            "Epoch: 071, Loss: 1.6423, Train: 0.4811, Test: 0.4592\n",
            "Early stopping:  0.014074374665045044\n",
            "Epoch: 072, Loss: 1.6334, Train: 0.4839, Test: 0.4558\n",
            "Early stopping:  0.013834383706914567\n",
            "Epoch: 073, Loss: 1.6251, Train: 0.4856, Test: 0.4569\n",
            "Early stopping:  0.013812665677586149\n",
            "Epoch: 074, Loss: 1.6168, Train: 0.4875, Test: 0.4546\n",
            "Early stopping:  0.0134694096786754\n",
            "Epoch: 075, Loss: 1.6089, Train: 0.4899, Test: 0.4552\n",
            "Early stopping:  0.01317730820067447\n",
            "Epoch: 076, Loss: 1.6009, Train: 0.4926, Test: 0.4535\n",
            "Early stopping:  0.012824717989769053\n",
            "Epoch: 077, Loss: 1.5926, Train: 0.4938, Test: 0.4575\n",
            "Early stopping:  0.012773487530260138\n",
            "Epoch: 078, Loss: 1.5843, Train: 0.4964, Test: 0.4609\n",
            "Early stopping:  0.012848351539527118\n",
            "Epoch: 079, Loss: 1.5775, Train: 0.4984, Test: 0.4632\n",
            "Early stopping:  0.012574481984193817\n",
            "Epoch: 080, Loss: 1.5698, Train: 0.4999, Test: 0.4666\n",
            "Early stopping:  0.012230589347760534\n",
            "Epoch: 081, Loss: 1.5616, Train: 0.5006, Test: 0.4671\n",
            "Early stopping:  0.012104650667957483\n",
            "Epoch: 082, Loss: 1.5552, Train: 0.5008, Test: 0.4649\n",
            "Early stopping:  0.011726687068813229\n",
            "Epoch: 083, Loss: 1.5517, Train: 0.5028, Test: 0.4643\n",
            "Early stopping:  0.010561615394109752\n",
            "Epoch: 084, Loss: 1.5409, Train: 0.5055, Test: 0.4705\n",
            "Early stopping:  0.010829124924063774\n",
            "Epoch: 085, Loss: 1.5338, Train: 0.5049, Test: 0.4705\n",
            "Early stopping:  0.011201000313952872\n",
            "Epoch: 086, Loss: 1.5282, Train: 0.5069, Test: 0.4700\n",
            "Early stopping:  0.011489965294968965\n",
            "Epoch: 087, Loss: 1.5211, Train: 0.5080, Test: 0.4739\n",
            "Early stopping:  0.011791347618100008\n",
            "Epoch: 088, Loss: 1.5141, Train: 0.5091, Test: 0.4756\n",
            "Early stopping:  0.010482051440360511\n",
            "Epoch: 089, Loss: 1.5082, Train: 0.5128, Test: 0.4756\n",
            "Early stopping:  0.010320365199830834\n",
            "Epoch: 090, Loss: 1.5025, Train: 0.5118, Test: 0.4773\n",
            "Early stopping:  0.010180345629677361\n",
            "Epoch: 091, Loss: 1.4977, Train: 0.5160, Test: 0.4790\n",
            "Early stopping:  0.00923954319616099\n",
            "PREDICTIONS -> tensor([3, 0, 0,  ..., 3, 0, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.34      0.67      0.45       190\n",
            "         capital_goods       0.14      0.01      0.01       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.42      0.44      0.43       198\n",
            " consumer_non-cyclical       0.64      0.38      0.48       112\n",
            "                energy       1.00      0.01      0.03        71\n",
            "             financial       0.61      0.57      0.59       192\n",
            "            healthcare       0.63      0.39      0.48        79\n",
            "              services       0.49      0.74      0.59       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.59      0.62      0.61       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.48      1764\n",
            "             macro avg       0.41      0.32      0.31      1764\n",
            "          weighted avg       0.45      0.48      0.42      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 123.7028, Train: 0.1798, Test: 0.1961\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 227.1917, Train: 0.2936, Test: 0.2937\n",
            "Early stopping:  73.17767005496134\n",
            "Epoch: 003, Loss: 499.0446, Train: 0.0569, Test: 0.0578\n",
            "Early stopping:  193.86222802977153\n",
            "Epoch: 004, Loss: 530.0098, Train: 0.0482, Test: 0.0402\n",
            "Early stopping:  200.67353958165734\n",
            "Epoch: 005, Loss: 489.3004, Train: 0.1640, Test: 0.1582\n",
            "Early stopping:  185.38516027653588\n",
            "Epoch: 006, Loss: 500.2086, Train: 0.1270, Test: 0.1270\n",
            "Early stopping:  125.01230640003534\n",
            "Epoch: 007, Loss: 397.6418, Train: 0.0359, Test: 0.0346\n",
            "Early stopping:  50.22181342522783\n",
            "Epoch: 008, Loss: 284.2118, Train: 0.1307, Test: 0.1293\n",
            "Early stopping:  100.27609127184304\n",
            "Epoch: 009, Loss: 212.8236, Train: 0.2902, Test: 0.2795\n",
            "Early stopping:  126.27471462091887\n",
            "Epoch: 010, Loss: 180.9459, Train: 0.2700, Test: 0.2608\n",
            "Early stopping:  132.74992521933967\n",
            "Epoch: 011, Loss: 162.3636, Train: 0.1928, Test: 0.1939\n",
            "Early stopping:  95.87798901315188\n",
            "Epoch: 012, Loss: 133.8104, Train: 0.2335, Test: 0.2290\n",
            "Early stopping:  57.62615385384126\n",
            "Epoch: 013, Loss: 96.7925, Train: 0.2979, Test: 0.2908\n",
            "Early stopping:  44.38621664077791\n",
            "Epoch: 014, Loss: 60.9484, Train: 0.1642, Test: 0.1548\n",
            "Early stopping:  48.688877210705904\n",
            "Epoch: 015, Loss: 48.7727, Train: 0.1000, Test: 0.0935\n",
            "Early stopping:  47.933468630964235\n",
            "Epoch: 016, Loss: 49.6623, Train: 0.1167, Test: 0.1196\n",
            "Early stopping:  36.79667129394697\n",
            "Epoch: 017, Loss: 46.4606, Train: 0.0888, Test: 0.0896\n",
            "Early stopping:  21.032325004212904\n",
            "Epoch: 018, Loss: 38.6691, Train: 0.0895, Test: 0.0901\n",
            "Early stopping:  8.005987927513162\n",
            "Epoch: 019, Loss: 26.9038, Train: 0.1092, Test: 0.1083\n",
            "Early stopping:  9.531812443504656\n",
            "Epoch: 020, Loss: 14.1313, Train: 0.1170, Test: 0.1156\n",
            "Early stopping:  14.66887288248085\n",
            "Epoch: 021, Loss: 5.7355, Train: 0.1475, Test: 0.1372\n",
            "Early stopping:  16.813332521105593\n",
            "Epoch: 022, Loss: 3.9412, Train: 0.2154, Test: 0.1990\n",
            "Early stopping:  14.743225600416896\n",
            "Epoch: 023, Loss: 3.1389, Train: 0.2744, Test: 0.2562\n",
            "Early stopping:  10.022612843490908\n",
            "Epoch: 024, Loss: 2.7514, Train: 0.3053, Test: 0.2902\n",
            "Early stopping:  4.720892363244157\n",
            "Epoch: 025, Loss: 2.5423, Train: 0.3240, Test: 0.3107\n",
            "Early stopping:  1.2967114047371575\n",
            "Epoch: 026, Loss: 2.4115, Train: 0.3359, Test: 0.3305\n",
            "Early stopping:  0.6151174156263318\n",
            "Epoch: 027, Loss: 2.3271, Train: 0.3418, Test: 0.3322\n",
            "Early stopping:  0.324363638386482\n",
            "Epoch: 028, Loss: 2.2709, Train: 0.3492, Test: 0.3384\n",
            "Early stopping:  0.1919978901229914\n",
            "Epoch: 029, Loss: 2.2283, Train: 0.3530, Test: 0.3464\n",
            "Early stopping:  0.12464224458793667\n",
            "Epoch: 030, Loss: 2.1965, Train: 0.3625, Test: 0.3520\n",
            "Early stopping:  0.08515124078415227\n",
            "Epoch: 031, Loss: 2.1724, Train: 0.3640, Test: 0.3543\n",
            "Early stopping:  0.061514734566726056\n",
            "Epoch: 032, Loss: 2.1492, Train: 0.3702, Test: 0.3571\n",
            "Early stopping:  0.04775511309104887\n",
            "Epoch: 033, Loss: 2.1256, Train: 0.3733, Test: 0.3571\n",
            "Early stopping:  0.04005491585921739\n",
            "Epoch: 034, Loss: 2.1078, Train: 0.3766, Test: 0.3673\n",
            "Early stopping:  0.03550240987967105\n",
            "Epoch: 035, Loss: 2.0898, Train: 0.3858, Test: 0.3810\n",
            "Early stopping:  0.0327415085320073\n",
            "Epoch: 036, Loss: 2.0617, Train: 0.3986, Test: 0.3912\n",
            "Early stopping:  0.033453308002954825\n",
            "Epoch: 037, Loss: 2.0349, Train: 0.4079, Test: 0.3980\n",
            "Early stopping:  0.03618895008394414\n",
            "Epoch: 038, Loss: 2.0133, Train: 0.4157, Test: 0.4093\n",
            "Early stopping:  0.03864593204745827\n",
            "Epoch: 039, Loss: 2.0028, Train: 0.4214, Test: 0.4189\n",
            "Early stopping:  0.03560853196160363\n",
            "Epoch: 040, Loss: 1.9925, Train: 0.4297, Test: 0.4246\n",
            "Early stopping:  0.02762406235446329\n",
            "Epoch: 041, Loss: 1.9715, Train: 0.4317, Test: 0.4240\n",
            "Early stopping:  0.023615377236706304\n",
            "Epoch: 042, Loss: 1.9552, Train: 0.4316, Test: 0.4229\n",
            "Early stopping:  0.02358209705706606\n",
            "Epoch: 043, Loss: 1.9436, Train: 0.4317, Test: 0.4274\n",
            "Early stopping:  0.024751657195267396\n",
            "Epoch: 044, Loss: 1.9327, Train: 0.4340, Test: 0.4297\n",
            "Early stopping:  0.023554626928244175\n",
            "Epoch: 045, Loss: 1.9176, Train: 0.4373, Test: 0.4291\n",
            "Early stopping:  0.020659732806511066\n",
            "Epoch: 046, Loss: 1.8981, Train: 0.4417, Test: 0.4274\n",
            "Early stopping:  0.022358720438999943\n",
            "Epoch: 047, Loss: 1.8792, Train: 0.4354, Test: 0.4235\n",
            "Early stopping:  0.02599606243770305\n",
            "Epoch: 048, Loss: 1.8693, Train: 0.4364, Test: 0.4274\n",
            "Early stopping:  0.02623022849328638\n",
            "Epoch: 049, Loss: 1.8610, Train: 0.4411, Test: 0.4320\n",
            "Early stopping:  0.022837112944340678\n",
            "Epoch: 050, Loss: 1.8445, Train: 0.4446, Test: 0.4342\n",
            "Early stopping:  0.02004726005692775\n",
            "Epoch: 051, Loss: 1.8308, Train: 0.4455, Test: 0.4359\n",
            "Early stopping:  0.019399082475452703\n",
            "Epoch: 052, Loss: 1.8184, Train: 0.4496, Test: 0.4439\n",
            "Early stopping:  0.020972671701729048\n",
            "Epoch: 053, Loss: 1.8060, Train: 0.4513, Test: 0.4524\n",
            "Early stopping:  0.021569479893857525\n",
            "Epoch: 054, Loss: 1.7938, Train: 0.4527, Test: 0.4558\n",
            "Early stopping:  0.019933773530063585\n",
            "Epoch: 055, Loss: 1.7802, Train: 0.4567, Test: 0.4535\n",
            "Early stopping:  0.019889716062099473\n",
            "Epoch: 056, Loss: 1.7663, Train: 0.4570, Test: 0.4501\n",
            "Early stopping:  0.020552912369229338\n",
            "Epoch: 057, Loss: 1.7550, Train: 0.4582, Test: 0.4495\n",
            "Early stopping:  0.020474742449266052\n",
            "Epoch: 058, Loss: 1.7433, Train: 0.4607, Test: 0.4529\n",
            "Early stopping:  0.019974419731407487\n",
            "Epoch: 059, Loss: 1.7281, Train: 0.4652, Test: 0.4563\n",
            "Early stopping:  0.020134785531691766\n",
            "Epoch: 060, Loss: 1.7137, Train: 0.4658, Test: 0.4586\n",
            "Early stopping:  0.02094104336692338\n",
            "Epoch: 061, Loss: 1.7025, Train: 0.4672, Test: 0.4569\n",
            "Early stopping:  0.02132030593304414\n",
            "Epoch: 062, Loss: 1.6897, Train: 0.4710, Test: 0.4643\n",
            "Early stopping:  0.021036613675676923\n",
            "Epoch: 063, Loss: 1.6759, Train: 0.4747, Test: 0.4666\n",
            "Early stopping:  0.02031876749577418\n",
            "Epoch: 064, Loss: 1.6615, Train: 0.4778, Test: 0.4666\n",
            "Early stopping:  0.020743987903516763\n",
            "Epoch: 065, Loss: 1.6482, Train: 0.4831, Test: 0.4671\n",
            "Early stopping:  0.021619875227810347\n",
            "Epoch: 066, Loss: 1.6353, Train: 0.4845, Test: 0.4666\n",
            "Early stopping:  0.02159711731421065\n",
            "Epoch: 067, Loss: 1.6212, Train: 0.4889, Test: 0.4700\n",
            "Early stopping:  0.021445302456047612\n",
            "Epoch: 068, Loss: 1.6061, Train: 0.4886, Test: 0.4717\n",
            "Early stopping:  0.021803837688878603\n",
            "Epoch: 069, Loss: 1.5930, Train: 0.4934, Test: 0.4739\n",
            "Early stopping:  0.02208380664776978\n",
            "Epoch: 070, Loss: 1.5782, Train: 0.4975, Test: 0.4756\n",
            "Early stopping:  0.02250106899084368\n",
            "Epoch: 071, Loss: 1.5645, Train: 0.5032, Test: 0.4813\n",
            "Early stopping:  0.022354252047818206\n",
            "Epoch: 072, Loss: 1.5518, Train: 0.5070, Test: 0.4819\n",
            "Early stopping:  0.021693355666285576\n",
            "Epoch: 073, Loss: 1.5390, Train: 0.5077, Test: 0.4887\n",
            "Early stopping:  0.021281523299278137\n",
            "Epoch: 074, Loss: 1.5251, Train: 0.5083, Test: 0.4904\n",
            "Early stopping:  0.02082933181964981\n",
            "Epoch: 075, Loss: 1.5119, Train: 0.5113, Test: 0.4926\n",
            "Early stopping:  0.0208494183204824\n",
            "Epoch: 076, Loss: 1.4986, Train: 0.5171, Test: 0.4989\n",
            "Early stopping:  0.021109036363221038\n",
            "Epoch: 077, Loss: 1.4834, Train: 0.5229, Test: 0.5011\n",
            "Early stopping:  0.02176922286822253\n",
            "Epoch: 078, Loss: 1.4691, Train: 0.5240, Test: 0.5045\n",
            "Early stopping:  0.02222122713671986\n",
            "Epoch: 079, Loss: 1.4565, Train: 0.5270, Test: 0.5068\n",
            "Early stopping:  0.022185424354205868\n",
            "Epoch: 080, Loss: 1.4443, Train: 0.5325, Test: 0.5102\n",
            "Early stopping:  0.02143129777440598\n",
            "Epoch: 081, Loss: 1.4331, Train: 0.5367, Test: 0.5113\n",
            "Early stopping:  0.01984874169863071\n",
            "Epoch: 082, Loss: 1.4223, Train: 0.5349, Test: 0.5142\n",
            "Early stopping:  0.018511566222810163\n",
            "Epoch: 083, Loss: 1.4132, Train: 0.5372, Test: 0.5040\n",
            "Early stopping:  0.017206293743774713\n",
            "Epoch: 084, Loss: 1.4054, Train: 0.5415, Test: 0.5130\n",
            "Early stopping:  0.015514511821664242\n",
            "Epoch: 085, Loss: 1.3941, Train: 0.5457, Test: 0.5147\n",
            "Early stopping:  0.015015465641213583\n",
            "Epoch: 086, Loss: 1.3833, Train: 0.5504, Test: 0.5170\n",
            "Early stopping:  0.015381280616456801\n",
            "Epoch: 087, Loss: 1.3744, Train: 0.5541, Test: 0.5227\n",
            "Early stopping:  0.01577998934996518\n",
            "Epoch: 088, Loss: 1.3637, Train: 0.5579, Test: 0.5255\n",
            "Early stopping:  0.016318744383819712\n",
            "Epoch: 089, Loss: 1.3545, Train: 0.5592, Test: 0.5289\n",
            "Early stopping:  0.01563677241852345\n",
            "Epoch: 090, Loss: 1.3458, Train: 0.5615, Test: 0.5317\n",
            "Early stopping:  0.0149893817642116\n",
            "Epoch: 091, Loss: 1.3358, Train: 0.5674, Test: 0.5340\n",
            "Early stopping:  0.015043906426238282\n",
            "Epoch: 092, Loss: 1.3253, Train: 0.5703, Test: 0.5317\n",
            "Early stopping:  0.015107108910971086\n",
            "Epoch: 093, Loss: 1.3178, Train: 0.5742, Test: 0.5380\n",
            "Early stopping:  0.01487672539469728\n",
            "Epoch: 094, Loss: 1.3113, Train: 0.5769, Test: 0.5408\n",
            "Early stopping:  0.013842632875930991\n",
            "Epoch: 095, Loss: 1.3013, Train: 0.5802, Test: 0.5357\n",
            "Early stopping:  0.01316987649727575\n",
            "Epoch: 096, Loss: 1.2941, Train: 0.5836, Test: 0.5374\n",
            "Early stopping:  0.012524457756486673\n",
            "Epoch: 097, Loss: 1.2884, Train: 0.5820, Test: 0.5425\n",
            "Early stopping:  0.012056018920813996\n",
            "Epoch: 098, Loss: 1.2782, Train: 0.5850, Test: 0.5431\n",
            "Early stopping:  0.012537152279636333\n",
            "Epoch: 099, Loss: 1.2730, Train: 0.5908, Test: 0.5442\n",
            "Early stopping:  0.011505303456729494\n",
            "Epoch: 100, Loss: 1.2696, Train: 0.5910, Test: 0.5465\n",
            "Early stopping:  0.010348104825851336\n",
            "Epoch: 101, Loss: 1.2562, Train: 0.5936, Test: 0.5505\n",
            "Early stopping:  0.011813167283041647\n",
            "Epoch: 102, Loss: 1.2531, Train: 0.5975, Test: 0.5499\n",
            "Early stopping:  0.010863798015420735\n",
            "Epoch: 103, Loss: 1.2475, Train: 0.5971, Test: 0.5522\n",
            "Early stopping:  0.010927543141257136\n",
            "Epoch: 104, Loss: 1.2343, Train: 0.5995, Test: 0.5533\n",
            "Early stopping:  0.012847324930165202\n",
            "Epoch: 105, Loss: 1.2315, Train: 0.6024, Test: 0.5510\n",
            "Early stopping:  0.011099090095046109\n",
            "Epoch: 106, Loss: 1.2254, Train: 0.6033, Test: 0.5544\n",
            "Early stopping:  0.011531027337063892\n",
            "Epoch: 107, Loss: 1.2143, Train: 0.6020, Test: 0.5527\n",
            "Early stopping:  0.012184047761059379\n",
            "Epoch: 108, Loss: 1.2115, Train: 0.6067, Test: 0.5561\n",
            "Early stopping:  0.010169972035195244\n",
            "Epoch: 109, Loss: 1.2062, Train: 0.6100, Test: 0.5584\n",
            "Early stopping:  0.01040686956630151\n",
            "Epoch: 110, Loss: 1.1956, Train: 0.6091, Test: 0.5595\n",
            "Early stopping:  0.010936184284182734\n",
            "Epoch: 111, Loss: 1.1956, Train: 0.6142, Test: 0.5578\n",
            "Early stopping:  0.008749238280215371\n",
            "PREDICTIONS -> tensor([ 9,  5,  6,  ..., 11,  5,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.60      0.59      0.60       190\n",
            "         capital_goods       0.41      0.40      0.40       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.49      0.30      0.38       198\n",
            " consumer_non-cyclical       0.69      0.39      0.50       112\n",
            "                energy       0.67      0.48      0.56        71\n",
            "             financial       0.72      0.61      0.66       192\n",
            "            healthcare       0.66      0.34      0.45        79\n",
            "              services       0.53      0.79      0.64       519\n",
            "            technology       0.30      0.27      0.28        99\n",
            "        transportation       0.71      0.65      0.68       101\n",
            "             utilities       0.59      0.57      0.58        56\n",
            "\n",
            "              accuracy                           0.56      1764\n",
            "             macro avg       0.53      0.45      0.48      1764\n",
            "          weighted avg       0.56      0.56      0.54      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 51.1940, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 373.8536, Train: 0.0700, Test: 0.0686\n",
            "Early stopping:  228.1547960777301\n",
            "Epoch: 003, Loss: 302.9375, Train: 0.0466, Test: 0.0476\n",
            "Early stopping:  169.56469606512215\n",
            "Epoch: 004, Loss: 313.2175, Train: 0.1140, Test: 0.1156\n",
            "Early stopping:  142.87285523749432\n",
            "Epoch: 005, Loss: 242.4871, Train: 0.0743, Test: 0.0794\n",
            "Early stopping:  123.98771865661902\n",
            "Epoch: 006, Loss: 162.6208, Train: 0.0825, Test: 0.0856\n",
            "Early stopping:  80.02866045064732\n",
            "Epoch: 007, Loss: 111.4844, Train: 0.1031, Test: 0.1015\n",
            "Early stopping:  87.93082304097906\n",
            "Epoch: 008, Loss: 77.3622, Train: 0.0632, Test: 0.0561\n",
            "Early stopping:  96.40042870093119\n",
            "Epoch: 009, Loss: 47.5761, Train: 0.1170, Test: 0.1139\n",
            "Early stopping:  76.8283284221782\n",
            "Epoch: 010, Loss: 23.6979, Train: 0.3125, Test: 0.3124\n",
            "Early stopping:  54.64024914388954\n",
            "Epoch: 011, Loss: 12.5269, Train: 0.3207, Test: 0.3152\n",
            "Early stopping:  40.39909324022375\n",
            "Epoch: 012, Loss: 7.5569, Train: 0.3101, Test: 0.3010\n",
            "Early stopping:  28.856238273986175\n",
            "Epoch: 013, Loss: 6.2084, Train: 0.3277, Test: 0.3112\n",
            "Early stopping:  17.130526795582966\n",
            "Epoch: 014, Loss: 5.1872, Train: 0.3332, Test: 0.3214\n",
            "Early stopping:  7.618497227043954\n",
            "Epoch: 015, Loss: 4.2895, Train: 0.3387, Test: 0.3248\n",
            "Early stopping:  3.2403322423395995\n",
            "Epoch: 016, Loss: 3.5795, Train: 0.3376, Test: 0.3146\n",
            "Early stopping:  1.5725756963608788\n",
            "Epoch: 017, Loss: 3.0406, Train: 0.3287, Test: 0.3214\n",
            "Early stopping:  1.265365876389399\n",
            "Epoch: 018, Loss: 2.6813, Train: 0.3145, Test: 0.3061\n",
            "Early stopping:  1.0038296935638396\n",
            "Epoch: 019, Loss: 2.4874, Train: 0.3038, Test: 0.2948\n",
            "Early stopping:  0.7300588463866725\n",
            "Epoch: 020, Loss: 2.3893, Train: 0.3061, Test: 0.3033\n",
            "Early stopping:  0.48468635022432055\n",
            "Epoch: 021, Loss: 2.3353, Train: 0.3080, Test: 0.2965\n",
            "Early stopping:  0.28592065983161397\n",
            "Epoch: 022, Loss: 2.3063, Train: 0.3122, Test: 0.2982\n",
            "Early stopping:  0.15156545353689968\n",
            "Epoch: 023, Loss: 2.2985, Train: 0.3116, Test: 0.2988\n",
            "Early stopping:  0.07795683054547937\n",
            "Epoch: 024, Loss: 2.3084, Train: 0.3139, Test: 0.2954\n",
            "Early stopping:  0.037191771387123394\n",
            "Epoch: 025, Loss: 2.3166, Train: 0.3121, Test: 0.2982\n",
            "Early stopping:  0.01404123705754578\n",
            "Epoch: 026, Loss: 2.3142, Train: 0.3158, Test: 0.3061\n",
            "Early stopping:  0.007117840454833757\n",
            "PREDICTIONS -> tensor([9, 6, 8,  ..., 5, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.00      0.00      0.00       198\n",
            " consumer_non-cyclical       0.41      0.21      0.28       112\n",
            "                energy       0.42      0.24      0.31        71\n",
            "             financial       0.39      0.46      0.42       192\n",
            "            healthcare       0.06      0.03      0.04        79\n",
            "              services       0.29      0.71      0.41       519\n",
            "            technology       0.44      0.17      0.25        99\n",
            "        transportation       0.46      0.25      0.32       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.31      1764\n",
            "             macro avg       0.21      0.17      0.17      1764\n",
            "          weighted avg       0.22      0.31      0.23      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 61.0272, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 380.2500, Train: 0.1127, Test: 0.1145\n",
            "Early stopping:  225.72458316730805\n",
            "Epoch: 003, Loss: 331.9431, Train: 0.0546, Test: 0.0544\n",
            "Early stopping:  172.0620812258414\n",
            "Epoch: 004, Loss: 595.0558, Train: 0.0367, Test: 0.0346\n",
            "Early stopping:  219.50485283484213\n",
            "Epoch: 005, Loss: 513.2485, Train: 0.1188, Test: 0.1128\n",
            "Early stopping:  204.93235004603667\n",
            "Epoch: 006, Loss: 406.3537, Train: 0.1090, Test: 0.1015\n",
            "Early stopping:  106.81981111642402\n",
            "Epoch: 007, Loss: 323.3003, Train: 0.0776, Test: 0.0771\n",
            "Early stopping:  117.95514755935514\n",
            "Epoch: 008, Loss: 257.2352, Train: 0.0258, Test: 0.0255\n",
            "Early stopping:  137.26312372330975\n",
            "Epoch: 009, Loss: 172.6714, Train: 0.2797, Test: 0.2800\n",
            "Early stopping:  131.71726167085166\n",
            "Epoch: 010, Loss: 117.8129, Train: 0.2864, Test: 0.2812\n",
            "Early stopping:  115.2985554666625\n",
            "Epoch: 011, Loss: 120.5883, Train: 0.2851, Test: 0.2800\n",
            "Early stopping:  89.82277569982934\n",
            "Epoch: 012, Loss: 97.4284, Train: 0.2878, Test: 0.2812\n",
            "Early stopping:  64.47613099379907\n",
            "Epoch: 013, Loss: 63.2915, Train: 0.2871, Test: 0.2789\n",
            "Early stopping:  39.82964335341201\n",
            "Epoch: 014, Loss: 30.1217, Train: 0.2568, Test: 0.2551\n",
            "Early stopping:  38.65541799469988\n",
            "Epoch: 015, Loss: 11.6444, Train: 0.1696, Test: 0.1689\n",
            "Early stopping:  45.29667410260875\n",
            "Epoch: 016, Loss: 6.4188, Train: 0.1517, Test: 0.1485\n",
            "Early stopping:  38.25563754134099\n",
            "Epoch: 017, Loss: 4.1106, Train: 0.1660, Test: 0.1536\n",
            "Early stopping:  24.671601920680196\n",
            "Epoch: 018, Loss: 3.3173, Train: 0.1632, Test: 0.1559\n",
            "Early stopping:  11.106916891310089\n",
            "Epoch: 019, Loss: 3.0625, Train: 0.1591, Test: 0.1468\n",
            "Early stopping:  3.5711229760468735\n",
            "Epoch: 020, Loss: 2.8313, Train: 0.1482, Test: 0.1321\n",
            "Early stopping:  1.4628794148077406\n",
            "Epoch: 021, Loss: 2.6468, Train: 0.1346, Test: 0.1281\n",
            "Early stopping:  0.5709065372581426\n",
            "Epoch: 022, Loss: 2.5475, Train: 0.1245, Test: 0.1134\n",
            "Early stopping:  0.312992119954454\n",
            "Epoch: 023, Loss: 2.4911, Train: 0.1177, Test: 0.1094\n",
            "Early stopping:  0.2329901243065073\n",
            "Epoch: 024, Loss: 2.4480, Train: 0.1137, Test: 0.1088\n",
            "Early stopping:  0.15260061088330984\n",
            "Epoch: 025, Loss: 2.4107, Train: 0.1171, Test: 0.1162\n",
            "Early stopping:  0.0924073799014052\n",
            "Epoch: 026, Loss: 2.3775, Train: 0.1248, Test: 0.1298\n",
            "Early stopping:  0.06683818500936348\n",
            "Epoch: 027, Loss: 2.3574, Train: 0.1297, Test: 0.1383\n",
            "Early stopping:  0.053864325452616664\n",
            "Epoch: 028, Loss: 2.3437, Train: 0.1370, Test: 0.1406\n",
            "Early stopping:  0.04219972290120459\n",
            "Epoch: 029, Loss: 2.3306, Train: 0.1465, Test: 0.1519\n",
            "Early stopping:  0.031380265525975226\n",
            "Epoch: 030, Loss: 2.3165, Train: 0.1660, Test: 0.1672\n",
            "Early stopping:  0.023636052816873587\n",
            "Epoch: 031, Loss: 2.3012, Train: 0.1825, Test: 0.1893\n",
            "Early stopping:  0.022088800319439786\n",
            "Epoch: 032, Loss: 2.2851, Train: 0.2018, Test: 0.2080\n",
            "Early stopping:  0.02319483824996356\n",
            "Epoch: 033, Loss: 2.2679, Train: 0.2288, Test: 0.2319\n",
            "Early stopping:  0.024801686826719425\n",
            "Epoch: 034, Loss: 2.2515, Train: 0.2521, Test: 0.2523\n",
            "Early stopping:  0.025823075346125744\n",
            "Epoch: 035, Loss: 2.2367, Train: 0.2708, Test: 0.2721\n",
            "Early stopping:  0.025726768553564674\n",
            "Epoch: 036, Loss: 2.2231, Train: 0.2837, Test: 0.2834\n",
            "Early stopping:  0.0245786795840181\n",
            "Epoch: 037, Loss: 2.2085, Train: 0.2928, Test: 0.2948\n",
            "Early stopping:  0.02327523386971236\n",
            "Epoch: 038, Loss: 2.1918, Train: 0.3013, Test: 0.2999\n",
            "Early stopping:  0.023341511949879844\n",
            "Epoch: 039, Loss: 2.1742, Train: 0.3071, Test: 0.2988\n",
            "Early stopping:  0.024746067431498247\n",
            "Epoch: 040, Loss: 2.1588, Train: 0.3065, Test: 0.3033\n",
            "Early stopping:  0.02576191202733819\n",
            "Epoch: 041, Loss: 2.1491, Train: 0.3080, Test: 0.3016\n",
            "Early stopping:  0.024135537765203878\n",
            "Epoch: 042, Loss: 2.1425, Train: 0.3104, Test: 0.3056\n",
            "Early stopping:  0.019913298132282677\n",
            "Epoch: 043, Loss: 2.1356, Train: 0.3192, Test: 0.3118\n",
            "Early stopping:  0.015042953548688605\n",
            "Epoch: 044, Loss: 2.1247, Train: 0.3278, Test: 0.3186\n",
            "Early stopping:  0.012955549340811178\n",
            "Epoch: 045, Loss: 2.1095, Train: 0.3404, Test: 0.3265\n",
            "Early stopping:  0.015615699925051203\n",
            "Epoch: 046, Loss: 2.0919, Train: 0.3505, Test: 0.3407\n",
            "Early stopping:  0.020447951523344814\n",
            "Epoch: 047, Loss: 2.0749, Train: 0.3557, Test: 0.3458\n",
            "Early stopping:  0.024491564114380686\n",
            "Epoch: 048, Loss: 2.0600, Train: 0.3594, Test: 0.3498\n",
            "Early stopping:  0.025947875864993065\n",
            "Epoch: 049, Loss: 2.0473, Train: 0.3641, Test: 0.3554\n",
            "Early stopping:  0.0247590760805227\n",
            "Epoch: 050, Loss: 2.0361, Train: 0.3632, Test: 0.3469\n",
            "Early stopping:  0.022084301095892052\n",
            "Epoch: 051, Loss: 2.0257, Train: 0.3658, Test: 0.3475\n",
            "Early stopping:  0.019397502327098276\n",
            "Epoch: 052, Loss: 2.0145, Train: 0.3722, Test: 0.3469\n",
            "Early stopping:  0.017819537976235124\n",
            "Epoch: 053, Loss: 2.0016, Train: 0.3753, Test: 0.3492\n",
            "Early stopping:  0.01786540521658534\n",
            "Epoch: 054, Loss: 1.9883, Train: 0.3743, Test: 0.3526\n",
            "Early stopping:  0.018945135856612956\n",
            "Epoch: 055, Loss: 1.9755, Train: 0.3760, Test: 0.3560\n",
            "Early stopping:  0.02002200892368864\n",
            "Epoch: 056, Loss: 1.9632, Train: 0.3784, Test: 0.3594\n",
            "Early stopping:  0.02035992371800985\n",
            "Epoch: 057, Loss: 1.9505, Train: 0.3808, Test: 0.3634\n",
            "Early stopping:  0.020154848709514443\n",
            "Epoch: 058, Loss: 1.9371, Train: 0.3827, Test: 0.3690\n",
            "Early stopping:  0.020160212899700283\n",
            "Epoch: 059, Loss: 1.9246, Train: 0.3851, Test: 0.3736\n",
            "Early stopping:  0.020212822197854207\n",
            "Epoch: 060, Loss: 1.9140, Train: 0.3868, Test: 0.3781\n",
            "Early stopping:  0.019648435162239887\n",
            "Epoch: 061, Loss: 1.9043, Train: 0.3888, Test: 0.3798\n",
            "Early stopping:  0.018265756944075878\n",
            "Epoch: 062, Loss: 1.8945, Train: 0.3933, Test: 0.3821\n",
            "Early stopping:  0.016685431639166606\n",
            "Epoch: 063, Loss: 1.8837, Train: 0.3977, Test: 0.3844\n",
            "Early stopping:  0.01602356973070688\n",
            "Epoch: 064, Loss: 1.8727, Train: 0.4029, Test: 0.3827\n",
            "Early stopping:  0.016347764214837475\n",
            "Epoch: 065, Loss: 1.8629, Train: 0.4079, Test: 0.3889\n",
            "Early stopping:  0.01657345110625322\n",
            "Epoch: 066, Loss: 1.8544, Train: 0.4116, Test: 0.3946\n",
            "Early stopping:  0.015987878620584473\n",
            "Epoch: 067, Loss: 1.8460, Train: 0.4157, Test: 0.3980\n",
            "Early stopping:  0.014849923059449018\n",
            "Epoch: 068, Loss: 1.8365, Train: 0.4194, Test: 0.3980\n",
            "Early stopping:  0.014124094834439388\n",
            "Epoch: 069, Loss: 1.8264, Train: 0.4218, Test: 0.4036\n",
            "Early stopping:  0.014399693698555326\n",
            "Epoch: 070, Loss: 1.8158, Train: 0.4245, Test: 0.4053\n",
            "Early stopping:  0.015346427531666321\n",
            "Epoch: 071, Loss: 1.8049, Train: 0.4262, Test: 0.4087\n",
            "Early stopping:  0.016278391445320273\n",
            "Epoch: 072, Loss: 1.7938, Train: 0.4278, Test: 0.4082\n",
            "Early stopping:  0.01689689296285832\n",
            "Epoch: 073, Loss: 1.7831, Train: 0.4288, Test: 0.4104\n",
            "Early stopping:  0.017157515026361713\n",
            "Epoch: 074, Loss: 1.7731, Train: 0.4300, Test: 0.4155\n",
            "Early stopping:  0.016942979305643677\n",
            "Epoch: 075, Loss: 1.7639, Train: 0.4336, Test: 0.4184\n",
            "Early stopping:  0.01623162047261537\n",
            "Epoch: 076, Loss: 1.7551, Train: 0.4344, Test: 0.4240\n",
            "Early stopping:  0.0152810006089896\n",
            "Epoch: 077, Loss: 1.7463, Train: 0.4377, Test: 0.4257\n",
            "Early stopping:  0.01448449322906169\n",
            "Epoch: 078, Loss: 1.7377, Train: 0.4418, Test: 0.4240\n",
            "Early stopping:  0.013978312594092625\n",
            "Epoch: 079, Loss: 1.7294, Train: 0.4436, Test: 0.4263\n",
            "Early stopping:  0.013656354893787047\n",
            "Epoch: 080, Loss: 1.7207, Train: 0.4476, Test: 0.4291\n",
            "Early stopping:  0.013551881863319688\n",
            "Epoch: 081, Loss: 1.7113, Train: 0.4475, Test: 0.4308\n",
            "Early stopping:  0.013749522736496404\n",
            "Epoch: 082, Loss: 1.7022, Train: 0.4506, Test: 0.4331\n",
            "Early stopping:  0.0140956796694105\n",
            "Epoch: 083, Loss: 1.6937, Train: 0.4529, Test: 0.4325\n",
            "Early stopping:  0.014228966901472297\n",
            "Epoch: 084, Loss: 1.6853, Train: 0.4543, Test: 0.4308\n",
            "Early stopping:  0.013968246008537851\n",
            "Epoch: 085, Loss: 1.6772, Train: 0.4567, Test: 0.4354\n",
            "Early stopping:  0.01347512269199337\n",
            "Epoch: 086, Loss: 1.6695, Train: 0.4578, Test: 0.4371\n",
            "Early stopping:  0.012949152823924103\n",
            "Epoch: 087, Loss: 1.6621, Train: 0.4591, Test: 0.4371\n",
            "Early stopping:  0.012504194287901933\n",
            "Epoch: 088, Loss: 1.6545, Train: 0.4609, Test: 0.4359\n",
            "Early stopping:  0.012126920909537795\n",
            "Epoch: 089, Loss: 1.6470, Train: 0.4609, Test: 0.4348\n",
            "Early stopping:  0.01191821535207618\n",
            "Epoch: 090, Loss: 1.6394, Train: 0.4635, Test: 0.4365\n",
            "Early stopping:  0.011899842943523632\n",
            "Epoch: 091, Loss: 1.6320, Train: 0.4652, Test: 0.4399\n",
            "Early stopping:  0.011894954789587828\n",
            "Epoch: 092, Loss: 1.6250, Train: 0.4672, Test: 0.4410\n",
            "Early stopping:  0.01170029136639656\n",
            "Epoch: 093, Loss: 1.6186, Train: 0.4707, Test: 0.4416\n",
            "Early stopping:  0.011253925528724413\n",
            "Epoch: 094, Loss: 1.6121, Train: 0.4719, Test: 0.4444\n",
            "Early stopping:  0.010759536117786017\n",
            "Epoch: 095, Loss: 1.6054, Train: 0.4733, Test: 0.4450\n",
            "Early stopping:  0.010436997484754629\n",
            "Epoch: 096, Loss: 1.5985, Train: 0.4765, Test: 0.4456\n",
            "Early stopping:  0.010475985233222301\n",
            "Epoch: 097, Loss: 1.5915, Train: 0.4781, Test: 0.4461\n",
            "Early stopping:  0.01073820730838211\n",
            "Epoch: 098, Loss: 1.5849, Train: 0.4798, Test: 0.4490\n",
            "Early stopping:  0.010814659064419375\n",
            "Epoch: 099, Loss: 1.5785, Train: 0.4818, Test: 0.4507\n",
            "Early stopping:  0.010657831513239653\n",
            "Epoch: 100, Loss: 1.5721, Train: 0.4835, Test: 0.4501\n",
            "Early stopping:  0.010386380092411471\n",
            "Epoch: 101, Loss: 1.5657, Train: 0.4846, Test: 0.4552\n",
            "Early stopping:  0.01017652874220425\n",
            "Epoch: 102, Loss: 1.5593, Train: 0.4880, Test: 0.4598\n",
            "Early stopping:  0.010120622619610494\n",
            "Epoch: 103, Loss: 1.5532, Train: 0.4917, Test: 0.4615\n",
            "Early stopping:  0.010038300491721046\n",
            "Epoch: 104, Loss: 1.5472, Train: 0.4953, Test: 0.4683\n",
            "Early stopping:  0.009846719783115783\n",
            "PREDICTIONS -> tensor([ 0,  8,  8,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.51      0.42      0.46       190\n",
            "         capital_goods       0.45      0.04      0.07       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.42      0.37      0.39       198\n",
            " consumer_non-cyclical       0.52      0.13      0.21       112\n",
            "                energy       1.00      0.08      0.16        71\n",
            "             financial       0.63      0.53      0.57       192\n",
            "            healthcare       0.64      0.29      0.40        79\n",
            "              services       0.41      0.85      0.56       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.74      0.50      0.60       101\n",
            "             utilities       0.58      0.59      0.58        56\n",
            "\n",
            "              accuracy                           0.47      1764\n",
            "             macro avg       0.49      0.32      0.33      1764\n",
            "          weighted avg       0.49      0.47      0.41      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 54.8610, Train: 0.0464, Test: 0.0516\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 291.6961, Train: 0.2028, Test: 0.2024\n",
            "Early stopping:  167.46767703331327\n",
            "Epoch: 003, Loss: 332.8033, Train: 0.2926, Test: 0.2874\n",
            "Early stopping:  150.01809462690088\n",
            "Epoch: 004, Loss: 494.6429, Train: 0.2905, Test: 0.2851\n",
            "Early stopping:  181.61777970428358\n",
            "Epoch: 005, Loss: 368.6690, Train: 0.0285, Test: 0.0255\n",
            "Early stopping:  160.83784765336617\n",
            "Epoch: 006, Loss: 431.4484, Train: 0.1253, Test: 0.1083\n",
            "Early stopping:  80.41960925814892\n",
            "Epoch: 007, Loss: 420.3229, Train: 0.1660, Test: 0.1633\n",
            "Early stopping:  62.04375721540197\n",
            "Epoch: 008, Loss: 447.1299, Train: 0.1116, Test: 0.1105\n",
            "Early stopping:  45.55092982211495\n",
            "Epoch: 009, Loss: 454.0294, Train: 0.1106, Test: 0.1100\n",
            "Early stopping:  33.78892481415182\n",
            "Epoch: 010, Loss: 394.7903, Train: 0.1792, Test: 0.1746\n",
            "Early stopping:  23.480183468230546\n",
            "Epoch: 011, Loss: 272.2713, Train: 0.3033, Test: 0.2965\n",
            "Early stopping:  73.94073081009574\n",
            "Epoch: 012, Loss: 281.2425, Train: 0.3074, Test: 0.2988\n",
            "Early stopping:  88.10965340077696\n",
            "Epoch: 013, Loss: 276.3848, Train: 0.3172, Test: 0.3101\n",
            "Early stopping:  83.66690752767204\n",
            "Epoch: 014, Loss: 231.1751, Train: 0.3034, Test: 0.2959\n",
            "Early stopping:  61.25939993272806\n",
            "Epoch: 015, Loss: 180.6115, Train: 0.2189, Test: 0.2115\n",
            "Early stopping:  42.78893973479188\n",
            "Epoch: 016, Loss: 168.3946, Train: 0.1222, Test: 0.1213\n",
            "Early stopping:  52.40096796088375\n",
            "Epoch: 017, Loss: 153.8336, Train: 0.1574, Test: 0.1582\n",
            "Early stopping:  50.72241068685451\n",
            "Epoch: 018, Loss: 108.7326, Train: 0.2709, Test: 0.2545\n",
            "Early stopping:  44.334254500669694\n",
            "Epoch: 019, Loss: 62.8108, Train: 0.3241, Test: 0.3146\n",
            "Early stopping:  48.6094001042607\n",
            "Epoch: 020, Loss: 35.2227, Train: 0.3214, Test: 0.3084\n",
            "Early stopping:  57.1470467422521\n",
            "Epoch: 021, Loss: 19.9027, Train: 0.2166, Test: 0.2160\n",
            "Early stopping:  55.02976083813022\n",
            "Epoch: 022, Loss: 17.2598, Train: 0.1615, Test: 0.1621\n",
            "Early stopping:  38.08675195321321\n",
            "Epoch: 023, Loss: 19.9978, Train: 0.1553, Test: 0.1514\n",
            "Early stopping:  19.122909130412978\n",
            "Epoch: 024, Loss: 18.8776, Train: 0.1551, Test: 0.1497\n",
            "Early stopping:  7.333960613099294\n",
            "Epoch: 025, Loss: 14.2240, Train: 0.1341, Test: 0.1395\n",
            "Early stopping:  2.406945964847248\n",
            "Epoch: 026, Loss: 10.5594, Train: 0.1294, Test: 0.1366\n",
            "Early stopping:  3.8221505679752346\n",
            "Epoch: 027, Loss: 7.9766, Train: 0.1643, Test: 0.1514\n",
            "Early stopping:  5.181656563131037\n",
            "Epoch: 028, Loss: 5.3340, Train: 0.1917, Test: 0.1684\n",
            "Early stopping:  5.317688860310861\n",
            "Epoch: 029, Loss: 3.8334, Train: 0.2182, Test: 0.2086\n",
            "Early stopping:  4.153745602623646\n",
            "Epoch: 030, Loss: 3.0576, Train: 0.2467, Test: 0.2364\n",
            "Early stopping:  3.097370724261639\n",
            "Epoch: 031, Loss: 2.7491, Train: 0.2339, Test: 0.2251\n",
            "Early stopping:  2.140824299882865\n",
            "Epoch: 032, Loss: 2.6608, Train: 0.2375, Test: 0.2171\n",
            "Early stopping:  1.1107466016945426\n",
            "Epoch: 033, Loss: 2.5508, Train: 0.2718, Test: 0.2472\n",
            "Early stopping:  0.5179890558625129\n",
            "Epoch: 034, Loss: 2.4034, Train: 0.3319, Test: 0.3010\n",
            "Early stopping:  0.24537163144940585\n",
            "Epoch: 035, Loss: 2.2937, Train: 0.3410, Test: 0.3095\n",
            "Early stopping:  0.18532592631752812\n",
            "Epoch: 036, Loss: 2.2285, Train: 0.3416, Test: 0.3146\n",
            "Early stopping:  0.1786926498266662\n",
            "Epoch: 037, Loss: 2.1867, Train: 0.3377, Test: 0.3056\n",
            "Early stopping:  0.1468363784248095\n",
            "Epoch: 038, Loss: 2.1646, Train: 0.3289, Test: 0.3039\n",
            "Early stopping:  0.09625834226511946\n",
            "Epoch: 039, Loss: 2.1528, Train: 0.3264, Test: 0.2954\n",
            "Early stopping:  0.0572658129190087\n",
            "Epoch: 040, Loss: 2.1468, Train: 0.3203, Test: 0.2925\n",
            "Early stopping:  0.033133857050486305\n",
            "Epoch: 041, Loss: 2.1463, Train: 0.3182, Test: 0.2863\n",
            "Early stopping:  0.01693398407028236\n",
            "Epoch: 042, Loss: 2.1461, Train: 0.3186, Test: 0.2891\n",
            "Early stopping:  0.007906893281725518\n",
            "PREDICTIONS -> tensor([9, 8, 1,  ..., 8, 1, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       190\n",
            "         capital_goods       0.10      0.56      0.18       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.40      0.05      0.09       198\n",
            " consumer_non-cyclical       0.42      0.12      0.18       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.36      0.21      0.27       192\n",
            "            healthcare       1.00      0.01      0.03        79\n",
            "              services       0.42      0.68      0.52       519\n",
            "            technology       0.31      0.19      0.24        99\n",
            "        transportation       0.00      0.00      0.00       101\n",
            "             utilities       0.14      0.02      0.03        56\n",
            "\n",
            "              accuracy                           0.29      1764\n",
            "             macro avg       0.26      0.15      0.13      1764\n",
            "          weighted avg       0.31      0.29      0.23      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 70.9015, Train: 0.2919, Test: 0.2868\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 218.3100, Train: 0.1139, Test: 0.1066\n",
            "Early stopping:  104.23357200846573\n",
            "Epoch: 003, Loss: 453.6166, Train: 0.1214, Test: 0.1202\n",
            "Early stopping:  193.0325259054032\n",
            "Epoch: 004, Loss: 572.5329, Train: 0.1119, Test: 0.1128\n",
            "Early stopping:  226.3511929558384\n",
            "Epoch: 005, Loss: 744.7433, Train: 0.0423, Test: 0.0420\n",
            "Early stopping:  270.22439593538127\n",
            "Epoch: 006, Loss: 800.4385, Train: 0.0627, Test: 0.0618\n",
            "Early stopping:  234.59853026805868\n",
            "Epoch: 007, Loss: 639.7764, Train: 0.1029, Test: 0.0998\n",
            "Early stopping:  137.82021366348312\n",
            "Epoch: 008, Loss: 359.1432, Train: 0.0981, Test: 0.0947\n",
            "Early stopping:  172.3035871824876\n",
            "Epoch: 009, Loss: 256.8231, Train: 0.2948, Test: 0.2863\n",
            "Early stopping:  240.08569141532013\n",
            "Epoch: 010, Loss: 239.9736, Train: 0.2951, Test: 0.2851\n",
            "Early stopping:  249.03793026004558\n",
            "Epoch: 011, Loss: 223.4921, Train: 0.2850, Test: 0.2772\n",
            "Early stopping:  173.67810882568833\n",
            "Epoch: 012, Loss: 168.8976, Train: 0.2473, Test: 0.2477\n",
            "Early stopping:  69.5324421911348\n",
            "Epoch: 013, Loss: 113.0851, Train: 0.1279, Test: 0.1236\n",
            "Early stopping:  58.947368486966454\n",
            "Epoch: 014, Loss: 64.6770, Train: 0.1038, Test: 0.0941\n",
            "Early stopping:  73.77866699567491\n",
            "Epoch: 015, Loss: 33.0647, Train: 0.1486, Test: 0.1559\n",
            "Early stopping:  77.0806814335845\n",
            "Epoch: 016, Loss: 10.9882, Train: 0.1570, Test: 0.1531\n",
            "Early stopping:  63.60129802886468\n",
            "Epoch: 017, Loss: 6.8413, Train: 0.1679, Test: 0.1667\n",
            "Early stopping:  44.08918168681816\n",
            "Epoch: 018, Loss: 4.3592, Train: 0.1811, Test: 0.1740\n",
            "Early stopping:  25.42771409176631\n",
            "Epoch: 019, Loss: 3.0518, Train: 0.1855, Test: 0.1791\n",
            "Early stopping:  12.341326609293874\n",
            "Epoch: 020, Loss: 2.5681, Train: 0.1959, Test: 0.2018\n",
            "Early stopping:  3.456577934993134\n",
            "Epoch: 021, Loss: 2.4152, Train: 0.2103, Test: 0.2109\n",
            "Early stopping:  1.8402749375663197\n",
            "Epoch: 022, Loss: 2.3778, Train: 0.2229, Test: 0.2251\n",
            "Early stopping:  0.8299691038178724\n",
            "Epoch: 023, Loss: 2.3614, Train: 0.2276, Test: 0.2398\n",
            "Early stopping:  0.2895604211373751\n",
            "Epoch: 024, Loss: 2.3591, Train: 0.2334, Test: 0.2449\n",
            "Early stopping:  0.08774707562185224\n",
            "Epoch: 025, Loss: 2.3537, Train: 0.2498, Test: 0.2545\n",
            "Early stopping:  0.02501559736262274\n",
            "Epoch: 026, Loss: 2.3358, Train: 0.2626, Test: 0.2608\n",
            "Early stopping:  0.015124308257900709\n",
            "Epoch: 027, Loss: 2.3187, Train: 0.2739, Test: 0.2670\n",
            "Early stopping:  0.0181486213490744\n",
            "Epoch: 028, Loss: 2.3071, Train: 0.2853, Test: 0.2829\n",
            "Early stopping:  0.022243997906500872\n",
            "Epoch: 029, Loss: 2.2983, Train: 0.2997, Test: 0.2937\n",
            "Early stopping:  0.022295816205633288\n",
            "Epoch: 030, Loss: 2.2908, Train: 0.3092, Test: 0.2982\n",
            "Early stopping:  0.01771284614221595\n",
            "Epoch: 031, Loss: 2.2835, Train: 0.3145, Test: 0.3027\n",
            "Early stopping:  0.013780136777299426\n",
            "Epoch: 032, Loss: 2.2753, Train: 0.3207, Test: 0.3090\n",
            "Early stopping:  0.012402875188674608\n",
            "Epoch: 033, Loss: 2.2658, Train: 0.3206, Test: 0.3005\n",
            "Early stopping:  0.01275939751301491\n",
            "Epoch: 034, Loss: 2.2546, Train: 0.3271, Test: 0.3141\n",
            "Early stopping:  0.014311668815440208\n",
            "Epoch: 035, Loss: 2.2419, Train: 0.3270, Test: 0.3158\n",
            "Early stopping:  0.016496924961996513\n",
            "Epoch: 036, Loss: 2.2294, Train: 0.3278, Test: 0.3163\n",
            "Early stopping:  0.018309300708709533\n",
            "Epoch: 037, Loss: 2.2180, Train: 0.3261, Test: 0.3186\n",
            "Early stopping:  0.019081153564626994\n",
            "Epoch: 038, Loss: 2.2087, Train: 0.3241, Test: 0.3186\n",
            "Early stopping:  0.018317332004493803\n",
            "Epoch: 039, Loss: 2.2013, Train: 0.3241, Test: 0.3175\n",
            "Early stopping:  0.01620511062813968\n",
            "Epoch: 040, Loss: 2.1947, Train: 0.3244, Test: 0.3175\n",
            "Early stopping:  0.013713405812968439\n",
            "Epoch: 041, Loss: 2.1875, Train: 0.3268, Test: 0.3135\n",
            "Early stopping:  0.011896353762563773\n",
            "Epoch: 042, Loss: 2.1791, Train: 0.3292, Test: 0.3209\n",
            "Early stopping:  0.01155778948692769\n",
            "Epoch: 043, Loss: 2.1701, Train: 0.3302, Test: 0.3237\n",
            "Early stopping:  0.0123772423942866\n",
            "Epoch: 044, Loss: 2.1613, Train: 0.3313, Test: 0.3254\n",
            "Early stopping:  0.013342086834277622\n",
            "Epoch: 045, Loss: 2.1531, Train: 0.3339, Test: 0.3265\n",
            "Early stopping:  0.013679429617798469\n",
            "Epoch: 046, Loss: 2.1458, Train: 0.3348, Test: 0.3311\n",
            "Early stopping:  0.01320508874723406\n",
            "Epoch: 047, Loss: 2.1393, Train: 0.3366, Test: 0.3362\n",
            "Early stopping:  0.01219853120215761\n",
            "Epoch: 048, Loss: 2.1330, Train: 0.3391, Test: 0.3350\n",
            "Early stopping:  0.011143217343529677\n",
            "Epoch: 049, Loss: 2.1269, Train: 0.3389, Test: 0.3339\n",
            "Early stopping:  0.010335319021358641\n",
            "Epoch: 050, Loss: 2.1202, Train: 0.3413, Test: 0.3339\n",
            "Early stopping:  0.010056670461470001\n",
            "Epoch: 051, Loss: 2.1131, Train: 0.3428, Test: 0.3316\n",
            "Early stopping:  0.010314373540926323\n",
            "Epoch: 052, Loss: 2.1068, Train: 0.3421, Test: 0.3294\n",
            "Early stopping:  0.01045029147697815\n",
            "Epoch: 053, Loss: 2.1002, Train: 0.3420, Test: 0.3311\n",
            "Early stopping:  0.010536782318379184\n",
            "Epoch: 054, Loss: 2.0924, Train: 0.3413, Test: 0.3311\n",
            "Early stopping:  0.010834381536711492\n",
            "Epoch: 055, Loss: 2.0834, Train: 0.3417, Test: 0.3333\n",
            "Early stopping:  0.011708605095016771\n",
            "Epoch: 056, Loss: 2.0737, Train: 0.3420, Test: 0.3328\n",
            "Early stopping:  0.013168977475390592\n",
            "Epoch: 057, Loss: 2.0646, Train: 0.3430, Test: 0.3345\n",
            "Early stopping:  0.014245428904697021\n",
            "Epoch: 058, Loss: 2.0554, Train: 0.3462, Test: 0.3379\n",
            "Early stopping:  0.014692680276050085\n",
            "Epoch: 059, Loss: 2.0455, Train: 0.3486, Test: 0.3373\n",
            "Early stopping:  0.014888276344025843\n",
            "Epoch: 060, Loss: 2.0340, Train: 0.3477, Test: 0.3362\n",
            "Early stopping:  0.015589106653566288\n",
            "Epoch: 061, Loss: 2.0214, Train: 0.3494, Test: 0.3384\n",
            "Early stopping:  0.017044451150594712\n",
            "Epoch: 062, Loss: 2.0111, Train: 0.3547, Test: 0.3396\n",
            "Early stopping:  0.017817173515711302\n",
            "Epoch: 063, Loss: 1.9974, Train: 0.3530, Test: 0.3407\n",
            "Early stopping:  0.018860650099872662\n",
            "Epoch: 064, Loss: 1.9879, Train: 0.3543, Test: 0.3401\n",
            "Early stopping:  0.01841137800522114\n",
            "Epoch: 065, Loss: 1.9761, Train: 0.3535, Test: 0.3418\n",
            "Early stopping:  0.018018268669528475\n",
            "Epoch: 066, Loss: 1.9687, Train: 0.3556, Test: 0.3447\n",
            "Early stopping:  0.016839565083424397\n",
            "Epoch: 067, Loss: 1.9570, Train: 0.3564, Test: 0.3418\n",
            "Early stopping:  0.01584248100952055\n",
            "Epoch: 068, Loss: 1.9478, Train: 0.3598, Test: 0.3475\n",
            "Early stopping:  0.015737616695644146\n",
            "Epoch: 069, Loss: 1.9364, Train: 0.3632, Test: 0.3503\n",
            "Early stopping:  0.015900864421340002\n",
            "Epoch: 070, Loss: 1.9274, Train: 0.3662, Test: 0.3543\n",
            "Early stopping:  0.01632912066696711\n",
            "Epoch: 071, Loss: 1.9163, Train: 0.3727, Test: 0.3571\n",
            "Early stopping:  0.016107493694573028\n",
            "Epoch: 072, Loss: 1.9068, Train: 0.3754, Test: 0.3622\n",
            "Early stopping:  0.016157742037650386\n",
            "Epoch: 073, Loss: 1.8966, Train: 0.3820, Test: 0.3645\n",
            "Early stopping:  0.015849143497846486\n",
            "Epoch: 074, Loss: 1.8866, Train: 0.3840, Test: 0.3673\n",
            "Early stopping:  0.016036126496947953\n",
            "Epoch: 075, Loss: 1.8774, Train: 0.3876, Test: 0.3736\n",
            "Early stopping:  0.015511956963542623\n",
            "Epoch: 076, Loss: 1.8675, Train: 0.3895, Test: 0.3759\n",
            "Early stopping:  0.015461851295994476\n",
            "Epoch: 077, Loss: 1.8584, Train: 0.3920, Test: 0.3787\n",
            "Early stopping:  0.015079141089496904\n",
            "Epoch: 078, Loss: 1.8481, Train: 0.3954, Test: 0.3776\n",
            "Early stopping:  0.01517379400213852\n",
            "Epoch: 079, Loss: 1.8389, Train: 0.3970, Test: 0.3844\n",
            "Early stopping:  0.01525598052197061\n",
            "Epoch: 080, Loss: 1.8285, Train: 0.4022, Test: 0.3855\n",
            "Early stopping:  0.015455070648630167\n",
            "Epoch: 081, Loss: 1.8188, Train: 0.4052, Test: 0.3861\n",
            "Early stopping:  0.01563973552771497\n",
            "Epoch: 082, Loss: 1.8083, Train: 0.4088, Test: 0.3906\n",
            "Early stopping:  0.01576689622258864\n",
            "Epoch: 083, Loss: 1.7986, Train: 0.4126, Test: 0.3900\n",
            "Early stopping:  0.015920978677887\n",
            "Epoch: 084, Loss: 1.7881, Train: 0.4164, Test: 0.3917\n",
            "Early stopping:  0.015947746557820303\n",
            "Epoch: 085, Loss: 1.7782, Train: 0.4194, Test: 0.3946\n",
            "Early stopping:  0.016024676199415815\n",
            "Epoch: 086, Loss: 1.7678, Train: 0.4191, Test: 0.3946\n",
            "Early stopping:  0.01604293731086683\n",
            "Epoch: 087, Loss: 1.7579, Train: 0.4204, Test: 0.3974\n",
            "Early stopping:  0.01610318541383121\n",
            "Epoch: 088, Loss: 1.7478, Train: 0.4222, Test: 0.4014\n",
            "Early stopping:  0.0159605529428699\n",
            "Epoch: 089, Loss: 1.7381, Train: 0.4236, Test: 0.4048\n",
            "Early stopping:  0.01584443525256943\n",
            "Epoch: 090, Loss: 1.7284, Train: 0.4266, Test: 0.4093\n",
            "Early stopping:  0.015585606783264046\n",
            "Epoch: 091, Loss: 1.7191, Train: 0.4310, Test: 0.4099\n",
            "Early stopping:  0.015325324320491648\n",
            "Epoch: 092, Loss: 1.7096, Train: 0.4353, Test: 0.4161\n",
            "Early stopping:  0.015084374624360257\n",
            "Epoch: 093, Loss: 1.7000, Train: 0.4385, Test: 0.4178\n",
            "Early stopping:  0.015012893695642774\n",
            "Epoch: 094, Loss: 1.6905, Train: 0.4425, Test: 0.4223\n",
            "Early stopping:  0.015008604907535805\n",
            "Epoch: 095, Loss: 1.6806, Train: 0.4455, Test: 0.4291\n",
            "Early stopping:  0.01519452049452952\n",
            "Epoch: 096, Loss: 1.6703, Train: 0.4495, Test: 0.4354\n",
            "Early stopping:  0.015491074079452435\n",
            "Epoch: 097, Loss: 1.6598, Train: 0.4531, Test: 0.4405\n",
            "Early stopping:  0.015911328917650146\n",
            "Epoch: 098, Loss: 1.6487, Train: 0.4598, Test: 0.4399\n",
            "Early stopping:  0.01651063738407218\n",
            "Epoch: 099, Loss: 1.6376, Train: 0.4648, Test: 0.4439\n",
            "Early stopping:  0.017010948700965374\n",
            "Epoch: 100, Loss: 1.6267, Train: 0.4659, Test: 0.4478\n",
            "Early stopping:  0.017290783192428247\n",
            "Epoch: 101, Loss: 1.6158, Train: 0.4760, Test: 0.4524\n",
            "Early stopping:  0.017380775857541768\n",
            "Epoch: 102, Loss: 1.6050, Train: 0.4805, Test: 0.4563\n",
            "Early stopping:  0.017254350954769352\n",
            "Epoch: 103, Loss: 1.5926, Train: 0.4832, Test: 0.4609\n",
            "Early stopping:  0.01766111042390663\n",
            "Epoch: 104, Loss: 1.5826, Train: 0.4894, Test: 0.4677\n",
            "Early stopping:  0.017607360408346283\n",
            "Epoch: 105, Loss: 1.5748, Train: 0.4954, Test: 0.4734\n",
            "Early stopping:  0.016577365998160745\n",
            "Epoch: 106, Loss: 1.5580, Train: 0.5001, Test: 0.4785\n",
            "Early stopping:  0.01783008299135677\n",
            "Epoch: 107, Loss: 1.5456, Train: 0.5050, Test: 0.4796\n",
            "Early stopping:  0.018931297904757862\n",
            "Epoch: 108, Loss: 1.5359, Train: 0.5135, Test: 0.4841\n",
            "Early stopping:  0.019501161887296288\n",
            "Epoch: 109, Loss: 1.5245, Train: 0.5165, Test: 0.4847\n",
            "Early stopping:  0.019498281910917646\n",
            "Epoch: 110, Loss: 1.5132, Train: 0.5201, Test: 0.4881\n",
            "Early stopping:  0.01749085617730354\n",
            "Epoch: 111, Loss: 1.5025, Train: 0.5218, Test: 0.4881\n",
            "Early stopping:  0.017214236879676232\n",
            "Epoch: 112, Loss: 1.4936, Train: 0.5250, Test: 0.4926\n",
            "Early stopping:  0.016862535241962615\n",
            "Epoch: 113, Loss: 1.4847, Train: 0.5272, Test: 0.4926\n",
            "Early stopping:  0.015689102627989965\n",
            "Epoch: 114, Loss: 1.4741, Train: 0.5308, Test: 0.4955\n",
            "Early stopping:  0.015200568668497737\n",
            "Epoch: 115, Loss: 1.4647, Train: 0.5372, Test: 0.5006\n",
            "Early stopping:  0.015050043818799212\n",
            "Epoch: 116, Loss: 1.4577, Train: 0.5401, Test: 0.5023\n",
            "Early stopping:  0.01455030991147677\n",
            "Epoch: 117, Loss: 1.4489, Train: 0.5452, Test: 0.5034\n",
            "Early stopping:  0.013960158589064884\n",
            "Epoch: 118, Loss: 1.4393, Train: 0.5462, Test: 0.5051\n",
            "Early stopping:  0.013502595232589636\n",
            "Epoch: 119, Loss: 1.4306, Train: 0.5459, Test: 0.5057\n",
            "Early stopping:  0.013698251370315714\n",
            "Epoch: 120, Loss: 1.4244, Train: 0.5481, Test: 0.5051\n",
            "Early stopping:  0.01346188731763754\n",
            "Epoch: 121, Loss: 1.4207, Train: 0.5498, Test: 0.5091\n",
            "Early stopping:  0.011435979293374986\n",
            "Epoch: 122, Loss: 1.4083, Train: 0.5538, Test: 0.5085\n",
            "Early stopping:  0.011563849539568143\n",
            "Epoch: 123, Loss: 1.3989, Train: 0.5571, Test: 0.5147\n",
            "Early stopping:  0.012806780036231544\n",
            "Epoch: 124, Loss: 1.3926, Train: 0.5572, Test: 0.5153\n",
            "Early stopping:  0.01362825559774936\n",
            "Epoch: 125, Loss: 1.3843, Train: 0.5629, Test: 0.5147\n",
            "Early stopping:  0.014075256470988642\n",
            "Epoch: 126, Loss: 1.3759, Train: 0.5654, Test: 0.5170\n",
            "Early stopping:  0.012557336821392543\n",
            "Epoch: 127, Loss: 1.3697, Train: 0.5664, Test: 0.5193\n",
            "Early stopping:  0.011888396117831204\n",
            "Epoch: 128, Loss: 1.3623, Train: 0.5681, Test: 0.5232\n",
            "Early stopping:  0.01192840180972758\n",
            "Epoch: 129, Loss: 1.3551, Train: 0.5680, Test: 0.5249\n",
            "Early stopping:  0.011421198608463376\n",
            "Epoch: 130, Loss: 1.3473, Train: 0.5700, Test: 0.5283\n",
            "Early stopping:  0.011383888233992771\n",
            "Epoch: 131, Loss: 1.3403, Train: 0.5710, Test: 0.5323\n",
            "Early stopping:  0.01167783070916732\n",
            "Epoch: 132, Loss: 1.3335, Train: 0.5748, Test: 0.5323\n",
            "Early stopping:  0.011430245929736769\n",
            "Epoch: 133, Loss: 1.3271, Train: 0.5766, Test: 0.5357\n",
            "Early stopping:  0.011028058917477844\n",
            "Epoch: 134, Loss: 1.3218, Train: 0.5778, Test: 0.5306\n",
            "Early stopping:  0.010167429895875112\n",
            "Epoch: 135, Loss: 1.3201, Train: 0.5785, Test: 0.5357\n",
            "Early stopping:  0.008404994335856161\n",
            "PREDICTIONS -> tensor([ 8,  0,  8,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.45      0.60      0.51       190\n",
            "         capital_goods       0.46      0.23      0.31       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.52      0.46      0.49       198\n",
            " consumer_non-cyclical       0.74      0.31      0.44       112\n",
            "                energy       0.54      0.42      0.47        71\n",
            "             financial       0.76      0.49      0.60       192\n",
            "            healthcare       0.70      0.54      0.61        79\n",
            "              services       0.50      0.79      0.61       519\n",
            "            technology       0.28      0.08      0.12        99\n",
            "        transportation       0.69      0.52      0.60       101\n",
            "             utilities       0.69      0.61      0.65        56\n",
            "\n",
            "              accuracy                           0.54      1764\n",
            "             macro avg       0.53      0.42      0.45      1764\n",
            "          weighted avg       0.55      0.54      0.51      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 60.6949, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 365.9745, Train: 0.1099, Test: 0.1060\n",
            "Early stopping:  215.86525518885165\n",
            "Epoch: 003, Loss: 430.6253, Train: 0.0535, Test: 0.0482\n",
            "Early stopping:  197.57861919433736\n",
            "Epoch: 004, Loss: 418.5889, Train: 0.0637, Test: 0.0646\n",
            "Early stopping:  174.45751502423178\n",
            "Epoch: 005, Loss: 373.7447, Train: 0.0442, Test: 0.0431\n",
            "Early stopping:  153.05750985488274\n",
            "Epoch: 006, Loss: 296.8939, Train: 0.1157, Test: 0.1173\n",
            "Early stopping:  52.80701091649633\n",
            "Epoch: 007, Loss: 159.4060, Train: 0.2812, Test: 0.2704\n",
            "Early stopping:  111.70736967298083\n",
            "Epoch: 008, Loss: 172.2354, Train: 0.2709, Test: 0.2585\n",
            "Early stopping:  116.56474736583567\n",
            "Epoch: 009, Loss: 169.4147, Train: 0.1042, Test: 0.0941\n",
            "Early stopping:  96.22113473623062\n",
            "Epoch: 010, Loss: 164.8918, Train: 0.1163, Test: 0.1105\n",
            "Early stopping:  58.52151104081021\n",
            "Epoch: 011, Loss: 135.5757, Train: 0.1385, Test: 0.1372\n",
            "Early stopping:  14.65182474460769\n",
            "Epoch: 012, Loss: 99.3674, Train: 0.2562, Test: 0.2551\n",
            "Early stopping:  31.025522992014125\n",
            "Epoch: 013, Loss: 74.8203, Train: 0.2996, Test: 0.2959\n",
            "Early stopping:  41.16481892579952\n",
            "Epoch: 014, Loss: 56.2607, Train: 0.3114, Test: 0.3022\n",
            "Early stopping:  44.25201250091697\n",
            "Epoch: 015, Loss: 34.9595, Train: 0.2738, Test: 0.2619\n",
            "Early stopping:  38.99548496419841\n",
            "Epoch: 016, Loss: 15.0308, Train: 0.1463, Test: 0.1378\n",
            "Early stopping:  33.00052644414965\n",
            "Epoch: 017, Loss: 7.9741, Train: 0.1025, Test: 0.0969\n",
            "Early stopping:  27.961547041541948\n",
            "Epoch: 018, Loss: 6.8949, Train: 0.0974, Test: 0.0918\n",
            "Early stopping:  21.156999539400207\n",
            "Epoch: 019, Loss: 6.0782, Train: 0.1000, Test: 0.0964\n",
            "Early stopping:  12.142252476379138\n",
            "Epoch: 020, Loss: 5.0914, Train: 0.1190, Test: 0.1128\n",
            "Early stopping:  3.9553322139773606\n",
            "Epoch: 021, Loss: 4.0811, Train: 0.1727, Test: 0.1644\n",
            "Early stopping:  1.5172975957152908\n",
            "Epoch: 022, Loss: 3.2780, Train: 0.1998, Test: 0.1893\n",
            "Early stopping:  1.4607559894615718\n",
            "Epoch: 023, Loss: 2.8027, Train: 0.2223, Test: 0.2018\n",
            "Early stopping:  1.333873145661681\n",
            "Epoch: 024, Loss: 2.5819, Train: 0.2330, Test: 0.2205\n",
            "Early stopping:  1.0278943728454024\n",
            "Epoch: 025, Loss: 2.4770, Train: 0.2439, Test: 0.2279\n",
            "Early stopping:  0.6564334535913512\n",
            "Epoch: 026, Loss: 2.4072, Train: 0.2542, Test: 0.2426\n",
            "Early stopping:  0.3512777041423398\n",
            "Epoch: 027, Loss: 2.3413, Train: 0.2663, Test: 0.2562\n",
            "Early stopping:  0.18044669810417457\n",
            "Epoch: 028, Loss: 2.2786, Train: 0.2827, Test: 0.2642\n",
            "Early stopping:  0.11808781980129372\n",
            "Epoch: 029, Loss: 2.2289, Train: 0.3007, Test: 0.2789\n",
            "Early stopping:  0.09895559469499109\n",
            "Epoch: 030, Loss: 2.1989, Train: 0.3087, Test: 0.2880\n",
            "Early stopping:  0.0844372185854025\n",
            "Epoch: 031, Loss: 2.1838, Train: 0.3166, Test: 0.3010\n",
            "Early stopping:  0.06426818158749642\n",
            "Epoch: 032, Loss: 2.1761, Train: 0.3258, Test: 0.3118\n",
            "Early stopping:  0.041746528709904425\n",
            "Epoch: 033, Loss: 2.1695, Train: 0.3360, Test: 0.3220\n",
            "Early stopping:  0.023641263436201324\n",
            "Epoch: 034, Loss: 2.1616, Train: 0.3451, Test: 0.3362\n",
            "Early stopping:  0.014272767966681754\n",
            "Epoch: 035, Loss: 2.1525, Train: 0.3586, Test: 0.3452\n",
            "Early stopping:  0.012210472852054887\n",
            "Epoch: 036, Loss: 2.1428, Train: 0.3661, Test: 0.3566\n",
            "Early stopping:  0.013258680041170055\n",
            "Epoch: 037, Loss: 2.1337, Train: 0.3668, Test: 0.3566\n",
            "Early stopping:  0.01430156695022125\n",
            "Epoch: 038, Loss: 2.1248, Train: 0.3647, Test: 0.3543\n",
            "Early stopping:  0.014621080380548426\n",
            "Epoch: 039, Loss: 2.1163, Train: 0.3650, Test: 0.3571\n",
            "Early stopping:  0.014289197220424078\n",
            "Epoch: 040, Loss: 2.1091, Train: 0.3667, Test: 0.3571\n",
            "Early stopping:  0.013418824845495864\n",
            "Epoch: 041, Loss: 2.0969, Train: 0.3676, Test: 0.3577\n",
            "Early stopping:  0.01415106659426176\n",
            "Epoch: 042, Loss: 2.0847, Train: 0.3686, Test: 0.3583\n",
            "Early stopping:  0.015852708946929583\n",
            "Epoch: 043, Loss: 2.0750, Train: 0.3695, Test: 0.3611\n",
            "Early stopping:  0.017010770442946497\n",
            "Epoch: 044, Loss: 2.0644, Train: 0.3705, Test: 0.3634\n",
            "Early stopping:  0.017637936874319738\n",
            "Epoch: 045, Loss: 2.0531, Train: 0.3739, Test: 0.3645\n",
            "Early stopping:  0.017065706699656992\n",
            "Epoch: 046, Loss: 2.0432, Train: 0.3766, Test: 0.3651\n",
            "Early stopping:  0.016576422039080285\n",
            "Epoch: 047, Loss: 2.0317, Train: 0.3781, Test: 0.3668\n",
            "Early stopping:  0.01703479326779526\n",
            "Epoch: 048, Loss: 2.0191, Train: 0.3788, Test: 0.3662\n",
            "Early stopping:  0.01774597945010323\n",
            "Epoch: 049, Loss: 2.0073, Train: 0.3800, Test: 0.3685\n",
            "Early stopping:  0.018338373743558633\n",
            "Epoch: 050, Loss: 1.9942, Train: 0.3824, Test: 0.3696\n",
            "Early stopping:  0.019357444174623744\n",
            "Epoch: 051, Loss: 1.9809, Train: 0.3858, Test: 0.3747\n",
            "Early stopping:  0.019991095273765285\n",
            "Epoch: 052, Loss: 1.9688, Train: 0.3912, Test: 0.3793\n",
            "Early stopping:  0.02005332664909464\n",
            "Epoch: 053, Loss: 1.9562, Train: 0.3977, Test: 0.3832\n",
            "Early stopping:  0.020171515192212615\n",
            "Epoch: 054, Loss: 1.9424, Train: 0.4032, Test: 0.3872\n",
            "Early stopping:  0.0203140505791707\n",
            "Epoch: 055, Loss: 1.9280, Train: 0.4056, Test: 0.3912\n",
            "Early stopping:  0.0209406720424552\n",
            "Epoch: 056, Loss: 1.9135, Train: 0.4107, Test: 0.3906\n",
            "Early stopping:  0.021949273968484055\n",
            "Epoch: 057, Loss: 1.8990, Train: 0.4167, Test: 0.3957\n",
            "Early stopping:  0.022653476062476755\n",
            "Epoch: 058, Loss: 1.8833, Train: 0.4232, Test: 0.4019\n",
            "Early stopping:  0.023256700364505768\n",
            "Epoch: 059, Loss: 1.8673, Train: 0.4279, Test: 0.4076\n",
            "Early stopping:  0.023962711321561393\n",
            "Epoch: 060, Loss: 1.8514, Train: 0.4340, Test: 0.4070\n",
            "Early stopping:  0.0246476734779401\n",
            "Epoch: 061, Loss: 1.8357, Train: 0.4387, Test: 0.4121\n",
            "Early stopping:  0.025047047451990576\n",
            "Epoch: 062, Loss: 1.8199, Train: 0.4417, Test: 0.4189\n",
            "Early stopping:  0.02504900310667622\n",
            "Epoch: 063, Loss: 1.8051, Train: 0.4435, Test: 0.4223\n",
            "Early stopping:  0.024641206964283773\n",
            "Epoch: 064, Loss: 1.7914, Train: 0.4468, Test: 0.4252\n",
            "Early stopping:  0.023821241330215914\n",
            "Epoch: 065, Loss: 1.7786, Train: 0.4500, Test: 0.4286\n",
            "Early stopping:  0.02260144612608395\n",
            "Epoch: 066, Loss: 1.7664, Train: 0.4520, Test: 0.4308\n",
            "Early stopping:  0.021134299921291705\n",
            "Epoch: 067, Loss: 1.7551, Train: 0.4548, Test: 0.4342\n",
            "Early stopping:  0.019775834509053344\n",
            "Epoch: 068, Loss: 1.7440, Train: 0.4550, Test: 0.4354\n",
            "Early stopping:  0.01871540515219334\n",
            "Epoch: 069, Loss: 1.7330, Train: 0.4558, Test: 0.4348\n",
            "Early stopping:  0.017963803459140927\n",
            "Epoch: 070, Loss: 1.7224, Train: 0.4608, Test: 0.4405\n",
            "Early stopping:  0.017424925376407822\n",
            "Epoch: 071, Loss: 1.7110, Train: 0.4665, Test: 0.4439\n",
            "Early stopping:  0.01735094109364566\n",
            "Epoch: 072, Loss: 1.6999, Train: 0.4714, Test: 0.4495\n",
            "Early stopping:  0.017394324262056157\n",
            "Epoch: 073, Loss: 1.6891, Train: 0.4760, Test: 0.4552\n",
            "Early stopping:  0.017439383126600275\n",
            "Epoch: 074, Loss: 1.6786, Train: 0.4814, Test: 0.4575\n",
            "Early stopping:  0.0173249091011831\n",
            "Epoch: 075, Loss: 1.6686, Train: 0.4866, Test: 0.4615\n",
            "Early stopping:  0.016812412466876793\n",
            "Epoch: 076, Loss: 1.6589, Train: 0.4889, Test: 0.4677\n",
            "Early stopping:  0.016240391671839514\n",
            "Epoch: 077, Loss: 1.6496, Train: 0.4930, Test: 0.4717\n",
            "Early stopping:  0.015611485921105278\n",
            "Epoch: 078, Loss: 1.6406, Train: 0.4948, Test: 0.4756\n",
            "Early stopping:  0.015029079509128074\n",
            "Epoch: 079, Loss: 1.6315, Train: 0.4972, Test: 0.4790\n",
            "Early stopping:  0.014611839457515903\n",
            "Epoch: 080, Loss: 1.6229, Train: 0.5006, Test: 0.4819\n",
            "Early stopping:  0.014245756433065004\n",
            "Epoch: 081, Loss: 1.6145, Train: 0.5026, Test: 0.4807\n",
            "Early stopping:  0.013916445577057474\n",
            "Epoch: 082, Loss: 1.6063, Train: 0.5072, Test: 0.4824\n",
            "Early stopping:  0.01353170273644371\n",
            "Epoch: 083, Loss: 1.5982, Train: 0.5077, Test: 0.4864\n",
            "Early stopping:  0.01314478546898047\n",
            "Epoch: 084, Loss: 1.5902, Train: 0.5099, Test: 0.4875\n",
            "Early stopping:  0.012891594503161016\n",
            "Epoch: 085, Loss: 1.5824, Train: 0.5116, Test: 0.4881\n",
            "Early stopping:  0.012681111684660597\n",
            "Epoch: 086, Loss: 1.5747, Train: 0.5141, Test: 0.4870\n",
            "Early stopping:  0.012490499834461564\n",
            "Epoch: 087, Loss: 1.5674, Train: 0.5160, Test: 0.4898\n",
            "Early stopping:  0.012197877064091846\n",
            "Epoch: 088, Loss: 1.5600, Train: 0.5162, Test: 0.4915\n",
            "Early stopping:  0.01193172369919806\n",
            "Epoch: 089, Loss: 1.5527, Train: 0.5186, Test: 0.4921\n",
            "Early stopping:  0.011711398235322655\n",
            "Epoch: 090, Loss: 1.5455, Train: 0.5205, Test: 0.4949\n",
            "Early stopping:  0.011548366048947762\n",
            "Epoch: 091, Loss: 1.5384, Train: 0.5230, Test: 0.4977\n",
            "Early stopping:  0.0114494071554202\n",
            "Epoch: 092, Loss: 1.5311, Train: 0.5245, Test: 0.4966\n",
            "Early stopping:  0.011405169724261798\n",
            "Epoch: 093, Loss: 1.5241, Train: 0.5247, Test: 0.4960\n",
            "Early stopping:  0.011346257975397855\n",
            "Epoch: 094, Loss: 1.5175, Train: 0.5253, Test: 0.4972\n",
            "Early stopping:  0.011133684698224995\n",
            "Epoch: 095, Loss: 1.5113, Train: 0.5290, Test: 0.4977\n",
            "Early stopping:  0.01071696874396404\n",
            "Epoch: 096, Loss: 1.5057, Train: 0.5272, Test: 0.5074\n",
            "Early stopping:  0.010059577007023474\n",
            "Epoch: 097, Loss: 1.5048, Train: 0.5320, Test: 0.5000\n",
            "Early stopping:  0.008174922193193787\n",
            "PREDICTIONS -> tensor([ 0, 11,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.49      0.49       190\n",
            "         capital_goods       0.36      0.06      0.11       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.53      0.36      0.43       198\n",
            " consumer_non-cyclical       0.53      0.36      0.43       112\n",
            "                energy       0.64      0.23      0.33        71\n",
            "             financial       0.55      0.53      0.54       192\n",
            "            healthcare       0.66      0.49      0.57        79\n",
            "              services       0.46      0.82      0.59       519\n",
            "            technology       0.33      0.01      0.02        99\n",
            "        transportation       0.72      0.52      0.61       101\n",
            "             utilities       0.52      0.61      0.56        56\n",
            "\n",
            "              accuracy                           0.50      1764\n",
            "             macro avg       0.48      0.37      0.39      1764\n",
            "          weighted avg       0.50      0.50      0.46      1764\n",
            "\n",
            "time: 2min 39s (started: 2024-10-16 21:28:17 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un2EkdY1gSl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bf5618-7df8-494c-e1b5-5e817d219ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 495 ms (started: 2024-10-16 21:30:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkjPHHo_gSl_"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt9fcbLFgSl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9aa46e3-e8aa-46ff-a289-d16a95dbc69b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5447, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3165, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1613932641775546\n",
            "Epoch: 003, Loss: 2.1832, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.18284030676730956\n",
            "Epoch: 004, Loss: 2.1538, Train: 0.2969, Test: 0.2948\n",
            "Early stopping:  0.17811434631511947\n",
            "Epoch: 005, Loss: 2.0941, Train: 0.3311, Test: 0.3243\n",
            "Early stopping:  0.1795338268576027\n",
            "Epoch: 006, Loss: 2.0250, Train: 0.3864, Test: 0.3793\n",
            "Early stopping:  0.10894765417976568\n",
            "Epoch: 007, Loss: 1.9622, Train: 0.3995, Test: 0.3866\n",
            "Early stopping:  0.09101145175841528\n",
            "Epoch: 008, Loss: 1.9038, Train: 0.4010, Test: 0.3815\n",
            "Early stopping:  0.09997347188353503\n",
            "Epoch: 009, Loss: 1.8468, Train: 0.4037, Test: 0.3832\n",
            "Early stopping:  0.09744962201270842\n",
            "Epoch: 010, Loss: 1.7907, Train: 0.4201, Test: 0.4025\n",
            "Early stopping:  0.09234401652983311\n",
            "Epoch: 011, Loss: 1.7320, Train: 0.4517, Test: 0.4456\n",
            "Early stopping:  0.0906737279034799\n",
            "Epoch: 012, Loss: 1.6682, Train: 0.4745, Test: 0.4660\n",
            "Early stopping:  0.09267619823129161\n",
            "Epoch: 013, Loss: 1.6082, Train: 0.4879, Test: 0.4751\n",
            "Early stopping:  0.09486268483349966\n",
            "Epoch: 014, Loss: 1.5580, Train: 0.5076, Test: 0.4909\n",
            "Early stopping:  0.09324489066298362\n",
            "Epoch: 015, Loss: 1.5068, Train: 0.5273, Test: 0.5113\n",
            "Early stopping:  0.08876503183438032\n",
            "Epoch: 016, Loss: 1.4530, Train: 0.5432, Test: 0.5244\n",
            "Early stopping:  0.08410209931777195\n",
            "Epoch: 017, Loss: 1.4069, Train: 0.5639, Test: 0.5425\n",
            "Early stopping:  0.08026786836334605\n",
            "Epoch: 018, Loss: 1.3648, Train: 0.5827, Test: 0.5601\n",
            "Early stopping:  0.07696794449806445\n",
            "Epoch: 019, Loss: 1.3251, Train: 0.5942, Test: 0.5635\n",
            "Early stopping:  0.07153615609823169\n",
            "Epoch: 020, Loss: 1.2917, Train: 0.6014, Test: 0.5714\n",
            "Early stopping:  0.06405871548798746\n",
            "Epoch: 021, Loss: 1.2583, Train: 0.6073, Test: 0.5799\n",
            "Early stopping:  0.05862470573842755\n",
            "Epoch: 022, Loss: 1.2258, Train: 0.6132, Test: 0.5862\n",
            "Early stopping:  0.05456703577264122\n",
            "Epoch: 023, Loss: 1.2000, Train: 0.6192, Test: 0.5952\n",
            "Early stopping:  0.05003916379730352\n",
            "Epoch: 024, Loss: 1.1753, Train: 0.6308, Test: 0.6043\n",
            "Early stopping:  0.04614087355557192\n",
            "Epoch: 025, Loss: 1.1500, Train: 0.6410, Test: 0.6139\n",
            "Early stopping:  0.04229909272632781\n",
            "Epoch: 026, Loss: 1.1266, Train: 0.6477, Test: 0.6179\n",
            "Early stopping:  0.03927093754463604\n",
            "Epoch: 027, Loss: 1.1054, Train: 0.6535, Test: 0.6247\n",
            "Early stopping:  0.037647229564340946\n",
            "Epoch: 028, Loss: 1.0847, Train: 0.6613, Test: 0.6327\n",
            "Early stopping:  0.0357503169407307\n",
            "Epoch: 029, Loss: 1.0626, Train: 0.6705, Test: 0.6457\n",
            "Early stopping:  0.03428611904959657\n",
            "Epoch: 030, Loss: 1.0429, Train: 0.6762, Test: 0.6480\n",
            "Early stopping:  0.03322954634105553\n",
            "Epoch: 031, Loss: 1.0249, Train: 0.6807, Test: 0.6497\n",
            "Early stopping:  0.03205730669157262\n",
            "Epoch: 032, Loss: 1.0063, Train: 0.6892, Test: 0.6542\n",
            "Early stopping:  0.0307811848293528\n",
            "Epoch: 033, Loss: 0.9879, Train: 0.6953, Test: 0.6667\n",
            "Early stopping:  0.029413116688512746\n",
            "Epoch: 034, Loss: 0.9708, Train: 0.7006, Test: 0.6661\n",
            "Early stopping:  0.028658480914634\n",
            "Epoch: 035, Loss: 0.9545, Train: 0.7081, Test: 0.6684\n",
            "Early stopping:  0.027888530216104098\n",
            "Epoch: 036, Loss: 0.9379, Train: 0.7119, Test: 0.6723\n",
            "Early stopping:  0.026923205341307686\n",
            "Epoch: 037, Loss: 0.9220, Train: 0.7180, Test: 0.6746\n",
            "Early stopping:  0.026054208134495962\n",
            "Epoch: 038, Loss: 0.9068, Train: 0.7241, Test: 0.6780\n",
            "Early stopping:  0.025370752662783866\n",
            "Epoch: 039, Loss: 0.8917, Train: 0.7327, Test: 0.6729\n",
            "Early stopping:  0.024770435735169457\n",
            "Epoch: 040, Loss: 0.8771, Train: 0.7390, Test: 0.6752\n",
            "Early stopping:  0.0240020254918763\n",
            "Epoch: 041, Loss: 0.8633, Train: 0.7438, Test: 0.6803\n",
            "Early stopping:  0.023253826456704957\n",
            "Epoch: 042, Loss: 0.8489, Train: 0.7495, Test: 0.6825\n",
            "Early stopping:  0.022800774563817523\n",
            "Epoch: 043, Loss: 0.8354, Train: 0.7553, Test: 0.6854\n",
            "Early stopping:  0.022258470936710816\n",
            "Epoch: 044, Loss: 0.8219, Train: 0.7619, Test: 0.6910\n",
            "Early stopping:  0.021880473042261445\n",
            "Epoch: 045, Loss: 0.8082, Train: 0.7679, Test: 0.6927\n",
            "Early stopping:  0.021722119545703234\n",
            "Epoch: 046, Loss: 0.7945, Train: 0.7706, Test: 0.6984\n",
            "Early stopping:  0.021513370717808566\n",
            "Epoch: 047, Loss: 0.7812, Train: 0.7751, Test: 0.7018\n",
            "Early stopping:  0.021481043679074767\n",
            "Epoch: 048, Loss: 0.7676, Train: 0.7790, Test: 0.7001\n",
            "Early stopping:  0.021431335090577434\n",
            "Epoch: 049, Loss: 0.7546, Train: 0.7817, Test: 0.6990\n",
            "Early stopping:  0.021201654877795354\n",
            "Epoch: 050, Loss: 0.7415, Train: 0.7877, Test: 0.7024\n",
            "Early stopping:  0.02098561652320882\n",
            "Epoch: 051, Loss: 0.7283, Train: 0.7929, Test: 0.7069\n",
            "Early stopping:  0.02085519320712119\n",
            "Epoch: 052, Loss: 0.7155, Train: 0.7968, Test: 0.7109\n",
            "Early stopping:  0.020630156748091686\n",
            "Epoch: 053, Loss: 0.7029, Train: 0.8032, Test: 0.7120\n",
            "Early stopping:  0.020454864535173745\n",
            "Epoch: 054, Loss: 0.6904, Train: 0.8048, Test: 0.7103\n",
            "Early stopping:  0.02017599649025244\n",
            "Epoch: 055, Loss: 0.6784, Train: 0.8121, Test: 0.7166\n",
            "Early stopping:  0.019770612838706323\n",
            "Epoch: 056, Loss: 0.6667, Train: 0.8106, Test: 0.7171\n",
            "Early stopping:  0.019306267352793748\n",
            "Epoch: 057, Loss: 0.6564, Train: 0.8168, Test: 0.7256\n",
            "Early stopping:  0.018441166601390918\n",
            "Epoch: 058, Loss: 0.6480, Train: 0.8113, Test: 0.7279\n",
            "Early stopping:  0.016922479182835324\n",
            "Epoch: 059, Loss: 0.6421, Train: 0.8232, Test: 0.7330\n",
            "Early stopping:  0.014557263295031108\n",
            "Epoch: 060, Loss: 0.6280, Train: 0.8282, Test: 0.7336\n",
            "Early stopping:  0.014626371251530183\n",
            "Epoch: 061, Loss: 0.6119, Train: 0.8309, Test: 0.7370\n",
            "Early stopping:  0.017559434995484822\n",
            "Epoch: 062, Loss: 0.6027, Train: 0.8348, Test: 0.7364\n",
            "Early stopping:  0.01926766549481418\n",
            "Epoch: 063, Loss: 0.5972, Train: 0.8330, Test: 0.7432\n",
            "Early stopping:  0.0184973039132894\n",
            "Epoch: 064, Loss: 0.5870, Train: 0.8430, Test: 0.7421\n",
            "Early stopping:  0.015558512482680744\n",
            "Epoch: 065, Loss: 0.5726, Train: 0.8459, Test: 0.7438\n",
            "Early stopping:  0.015098908528148663\n",
            "Epoch: 066, Loss: 0.5647, Train: 0.8419, Test: 0.7500\n",
            "Early stopping:  0.016058651789837994\n",
            "Epoch: 067, Loss: 0.5594, Train: 0.8510, Test: 0.7438\n",
            "Early stopping:  0.015681428937267948\n",
            "Epoch: 068, Loss: 0.5485, Train: 0.8541, Test: 0.7517\n",
            "Early stopping:  0.014448947178523967\n",
            "Epoch: 069, Loss: 0.5361, Train: 0.8550, Test: 0.7545\n",
            "Early stopping:  0.01427639415710018\n",
            "Epoch: 070, Loss: 0.5282, Train: 0.8598, Test: 0.7494\n",
            "Early stopping:  0.01532697830773036\n",
            "Epoch: 071, Loss: 0.5226, Train: 0.8599, Test: 0.7596\n",
            "Early stopping:  0.014998146766236042\n",
            "Epoch: 072, Loss: 0.5148, Train: 0.8671, Test: 0.7528\n",
            "Early stopping:  0.012919421149209931\n",
            "Epoch: 073, Loss: 0.5034, Train: 0.8717, Test: 0.7568\n",
            "Early stopping:  0.012572611472841935\n",
            "Epoch: 074, Loss: 0.4930, Train: 0.8728, Test: 0.7551\n",
            "Early stopping:  0.014299329222104022\n",
            "Epoch: 075, Loss: 0.4855, Train: 0.8764, Test: 0.7523\n",
            "Early stopping:  0.015217118894817108\n",
            "Epoch: 076, Loss: 0.4798, Train: 0.8734, Test: 0.7591\n",
            "Early stopping:  0.014022840146072316\n",
            "Epoch: 077, Loss: 0.4753, Train: 0.8812, Test: 0.7517\n",
            "Early stopping:  0.011138122341030812\n",
            "Epoch: 078, Loss: 0.4686, Train: 0.8783, Test: 0.7608\n",
            "Early stopping:  0.009358584944002506\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.73      0.77       190\n",
            "         capital_goods       0.70      0.64      0.67       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.79      0.71      0.74       198\n",
            " consumer_non-cyclical       0.80      0.61      0.69       112\n",
            "                energy       0.86      0.70      0.78        71\n",
            "             financial       0.80      0.76      0.78       192\n",
            "            healthcare       0.84      0.75      0.79        79\n",
            "              services       0.69      0.87      0.77       519\n",
            "            technology       0.77      0.65      0.70        99\n",
            "        transportation       0.85      0.81      0.83       101\n",
            "             utilities       0.80      0.80      0.80        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.81      0.74      0.77      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4927, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2800, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15037995548686992\n",
            "Epoch: 003, Loss: 2.1599, Train: 0.2946, Test: 0.2942\n",
            "Early stopping:  0.1685365857456077\n",
            "Epoch: 004, Loss: 2.1343, Train: 0.3034, Test: 0.3033\n",
            "Early stopping:  0.16349901542483716\n",
            "Epoch: 005, Loss: 2.0645, Train: 0.3355, Test: 0.3356\n",
            "Early stopping:  0.1680029550923533\n",
            "Epoch: 006, Loss: 1.9837, Train: 0.3808, Test: 0.3753\n",
            "Early stopping:  0.1106378902889299\n",
            "Epoch: 007, Loss: 1.9244, Train: 0.4028, Test: 0.3849\n",
            "Early stopping:  0.09938790107549748\n",
            "Epoch: 008, Loss: 1.8719, Train: 0.4171, Test: 0.4019\n",
            "Early stopping:  0.105476609007352\n",
            "Epoch: 009, Loss: 1.8125, Train: 0.4313, Test: 0.4172\n",
            "Early stopping:  0.09770957241320682\n",
            "Epoch: 010, Loss: 1.7489, Train: 0.4434, Test: 0.4325\n",
            "Early stopping:  0.09199241302017244\n",
            "Epoch: 011, Loss: 1.6880, Train: 0.4540, Test: 0.4422\n",
            "Early stopping:  0.09426859059080778\n",
            "Epoch: 012, Loss: 1.6343, Train: 0.4670, Test: 0.4478\n",
            "Early stopping:  0.09484529626396894\n",
            "Epoch: 013, Loss: 1.5877, Train: 0.4795, Test: 0.4586\n",
            "Early stopping:  0.08935654841555171\n",
            "Epoch: 014, Loss: 1.5377, Train: 0.5053, Test: 0.4802\n",
            "Early stopping:  0.08273382523182837\n",
            "Epoch: 015, Loss: 1.4830, Train: 0.5351, Test: 0.5193\n",
            "Early stopping:  0.0801165498635406\n",
            "Epoch: 016, Loss: 1.4325, Train: 0.5678, Test: 0.5567\n",
            "Early stopping:  0.08041213846010108\n",
            "Epoch: 017, Loss: 1.3891, Train: 0.5898, Test: 0.5737\n",
            "Early stopping:  0.0794858361220592\n",
            "Epoch: 018, Loss: 1.3486, Train: 0.5989, Test: 0.5794\n",
            "Early stopping:  0.0747969088821954\n",
            "Epoch: 019, Loss: 1.3067, Train: 0.6029, Test: 0.5839\n",
            "Early stopping:  0.06908089001056845\n",
            "Epoch: 020, Loss: 1.2642, Train: 0.6030, Test: 0.5913\n",
            "Early stopping:  0.06626174493246535\n",
            "Epoch: 021, Loss: 1.2282, Train: 0.6087, Test: 0.5952\n",
            "Early stopping:  0.06426010380408954\n",
            "Epoch: 022, Loss: 1.1992, Train: 0.6200, Test: 0.6020\n",
            "Early stopping:  0.059810732804535936\n",
            "Epoch: 023, Loss: 1.1699, Train: 0.6318, Test: 0.6122\n",
            "Early stopping:  0.05374111534002929\n",
            "Epoch: 024, Loss: 1.1407, Train: 0.6453, Test: 0.6202\n",
            "Early stopping:  0.04832639757743446\n",
            "Epoch: 025, Loss: 1.1146, Train: 0.6552, Test: 0.6304\n",
            "Early stopping:  0.04520169521621403\n",
            "Epoch: 026, Loss: 1.0925, Train: 0.6617, Test: 0.6293\n",
            "Early stopping:  0.04255447901243617\n",
            "Epoch: 027, Loss: 1.0696, Train: 0.6681, Test: 0.6372\n",
            "Early stopping:  0.03939003422795957\n",
            "Epoch: 028, Loss: 1.0464, Train: 0.6684, Test: 0.6446\n",
            "Early stopping:  0.036941563963137365\n",
            "Epoch: 029, Loss: 1.0269, Train: 0.6718, Test: 0.6474\n",
            "Early stopping:  0.03503336105877837\n",
            "Epoch: 030, Loss: 1.0082, Train: 0.6841, Test: 0.6519\n",
            "Early stopping:  0.033461967158059554\n",
            "Epoch: 031, Loss: 0.9877, Train: 0.6929, Test: 0.6502\n",
            "Early stopping:  0.031961802457614895\n",
            "Epoch: 032, Loss: 0.9680, Train: 0.7023, Test: 0.6536\n",
            "Early stopping:  0.03099598362330397\n",
            "Epoch: 033, Loss: 0.9508, Train: 0.7085, Test: 0.6599\n",
            "Early stopping:  0.03043212258484834\n",
            "Epoch: 034, Loss: 0.9327, Train: 0.7153, Test: 0.6718\n",
            "Early stopping:  0.02971564731246888\n",
            "Epoch: 035, Loss: 0.9151, Train: 0.7208, Test: 0.6774\n",
            "Early stopping:  0.028542112483014227\n",
            "Epoch: 036, Loss: 0.8996, Train: 0.7275, Test: 0.6820\n",
            "Early stopping:  0.02727536622877782\n",
            "Epoch: 037, Loss: 0.8842, Train: 0.7347, Test: 0.6848\n",
            "Early stopping:  0.026319497112739784\n",
            "Epoch: 038, Loss: 0.8683, Train: 0.7411, Test: 0.6910\n",
            "Early stopping:  0.025264693526068263\n",
            "Epoch: 039, Loss: 0.8540, Train: 0.7479, Test: 0.6973\n",
            "Early stopping:  0.024281323473468494\n",
            "Epoch: 040, Loss: 0.8400, Train: 0.7529, Test: 0.6973\n",
            "Early stopping:  0.02364218435308655\n",
            "Epoch: 041, Loss: 0.8256, Train: 0.7566, Test: 0.6961\n",
            "Early stopping:  0.022999975099422823\n",
            "Epoch: 042, Loss: 0.8119, Train: 0.7619, Test: 0.6961\n",
            "Early stopping:  0.022317206499731047\n",
            "Epoch: 043, Loss: 0.7983, Train: 0.7700, Test: 0.7012\n",
            "Early stopping:  0.02205869107262527\n",
            "Epoch: 044, Loss: 0.7842, Train: 0.7754, Test: 0.7018\n",
            "Early stopping:  0.02197408362843273\n",
            "Epoch: 045, Loss: 0.7710, Train: 0.7798, Test: 0.7018\n",
            "Early stopping:  0.021646072834431047\n",
            "Epoch: 046, Loss: 0.7583, Train: 0.7801, Test: 0.7098\n",
            "Early stopping:  0.02125959219368771\n",
            "Epoch: 047, Loss: 0.7470, Train: 0.7858, Test: 0.7035\n",
            "Early stopping:  0.020322153896098056\n",
            "Epoch: 048, Loss: 0.7388, Train: 0.7824, Test: 0.7115\n",
            "Early stopping:  0.018221732218753124\n",
            "Epoch: 049, Loss: 0.7319, Train: 0.7946, Test: 0.7115\n",
            "Early stopping:  0.015578138178240303\n",
            "Epoch: 050, Loss: 0.7142, Train: 0.7990, Test: 0.7137\n",
            "Early stopping:  0.016555526342612373\n",
            "Epoch: 051, Loss: 0.6982, Train: 0.7998, Test: 0.7154\n",
            "Early stopping:  0.019686778815384862\n",
            "Epoch: 052, Loss: 0.6928, Train: 0.8014, Test: 0.7160\n",
            "Early stopping:  0.020179998839635394\n",
            "Epoch: 053, Loss: 0.6833, Train: 0.8089, Test: 0.7200\n",
            "Early stopping:  0.019188061384839236\n",
            "Epoch: 054, Loss: 0.6673, Train: 0.8137, Test: 0.7256\n",
            "Early stopping:  0.017424590033955144\n",
            "Epoch: 055, Loss: 0.6578, Train: 0.8144, Test: 0.7245\n",
            "Early stopping:  0.01702374740896286\n",
            "Epoch: 056, Loss: 0.6520, Train: 0.8167, Test: 0.7313\n",
            "Early stopping:  0.017127871671682268\n",
            "Epoch: 057, Loss: 0.6401, Train: 0.8232, Test: 0.7324\n",
            "Early stopping:  0.016282558019256366\n",
            "Epoch: 058, Loss: 0.6267, Train: 0.8269, Test: 0.7313\n",
            "Early stopping:  0.015785235369807565\n",
            "Epoch: 059, Loss: 0.6201, Train: 0.8256, Test: 0.7387\n",
            "Early stopping:  0.016066309917042203\n",
            "Epoch: 060, Loss: 0.6135, Train: 0.8345, Test: 0.7370\n",
            "Early stopping:  0.0155369335457088\n",
            "Epoch: 061, Loss: 0.6014, Train: 0.8377, Test: 0.7404\n",
            "Early stopping:  0.014456243793902592\n",
            "Epoch: 062, Loss: 0.5897, Train: 0.8387, Test: 0.7409\n",
            "Early stopping:  0.01482074906024839\n",
            "Epoch: 063, Loss: 0.5824, Train: 0.8411, Test: 0.7415\n",
            "Early stopping:  0.015751063056444553\n",
            "Epoch: 064, Loss: 0.5763, Train: 0.8406, Test: 0.7449\n",
            "Early stopping:  0.014938196159340302\n",
            "Epoch: 065, Loss: 0.5678, Train: 0.8491, Test: 0.7483\n",
            "Early stopping:  0.012824915551824594\n",
            "Epoch: 066, Loss: 0.5568, Train: 0.8496, Test: 0.7500\n",
            "Early stopping:  0.012779845426846704\n",
            "Epoch: 067, Loss: 0.5462, Train: 0.8524, Test: 0.7523\n",
            "Early stopping:  0.014614619612084\n",
            "Epoch: 068, Loss: 0.5378, Train: 0.8585, Test: 0.7472\n",
            "Early stopping:  0.015610802732649982\n",
            "Epoch: 069, Loss: 0.5316, Train: 0.8547, Test: 0.7540\n",
            "Early stopping:  0.014543487820605416\n",
            "Epoch: 070, Loss: 0.5269, Train: 0.8605, Test: 0.7523\n",
            "Early stopping:  0.01193217646894852\n",
            "Epoch: 071, Loss: 0.5226, Train: 0.8559, Test: 0.7574\n",
            "Early stopping:  0.009309133737589388\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.77      0.72      0.74       190\n",
            "         capital_goods       0.74      0.59      0.66       127\n",
            "conglomerates_industry       1.00      0.55      0.71        20\n",
            "     consumer_cyclical       0.83      0.72      0.77       198\n",
            " consumer_non-cyclical       0.72      0.65      0.68       112\n",
            "                energy       0.89      0.79      0.84        71\n",
            "             financial       0.76      0.80      0.78       192\n",
            "            healthcare       0.88      0.76      0.82        79\n",
            "              services       0.69      0.85      0.76       519\n",
            "            technology       0.72      0.64      0.68        99\n",
            "        transportation       0.88      0.83      0.85       101\n",
            "             utilities       0.84      0.75      0.79        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.81      0.72      0.76      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5044, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2923, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.14994651729678388\n",
            "Epoch: 003, Loss: 2.1753, Train: 0.2945, Test: 0.2942\n",
            "Early stopping:  0.1668290011889574\n",
            "Epoch: 004, Loss: 2.1247, Train: 0.3081, Test: 0.3061\n",
            "Early stopping:  0.1687575421550097\n",
            "Epoch: 005, Loss: 2.0547, Train: 0.3647, Test: 0.3571\n",
            "Early stopping:  0.17605893167554296\n",
            "Epoch: 006, Loss: 1.9979, Train: 0.3986, Test: 0.3872\n",
            "Early stopping:  0.1135230353168076\n",
            "Epoch: 007, Loss: 1.9352, Train: 0.4015, Test: 0.3900\n",
            "Early stopping:  0.09606979488539365\n",
            "Epoch: 008, Loss: 1.8653, Train: 0.3995, Test: 0.3912\n",
            "Early stopping:  0.10098248446038817\n",
            "Epoch: 009, Loss: 1.8005, Train: 0.4085, Test: 0.3980\n",
            "Early stopping:  0.10138644354255447\n",
            "Epoch: 010, Loss: 1.7386, Train: 0.4263, Test: 0.4144\n",
            "Early stopping:  0.10331188851550419\n",
            "Epoch: 011, Loss: 1.6746, Train: 0.4553, Test: 0.4433\n",
            "Early stopping:  0.10247699058686664\n",
            "Epoch: 012, Loss: 1.6162, Train: 0.4765, Test: 0.4637\n",
            "Early stopping:  0.09872559769876631\n",
            "Epoch: 013, Loss: 1.5591, Train: 0.4967, Test: 0.4802\n",
            "Early stopping:  0.09572365756609329\n",
            "Epoch: 014, Loss: 1.5044, Train: 0.5203, Test: 0.5074\n",
            "Early stopping:  0.0923585935299277\n",
            "Epoch: 015, Loss: 1.4547, Train: 0.5462, Test: 0.5329\n",
            "Early stopping:  0.087244492225528\n",
            "Epoch: 016, Loss: 1.4053, Train: 0.5687, Test: 0.5482\n",
            "Early stopping:  0.08322647086827895\n",
            "Epoch: 017, Loss: 1.3583, Train: 0.5810, Test: 0.5539\n",
            "Early stopping:  0.07919353432584961\n",
            "Epoch: 018, Loss: 1.3164, Train: 0.5876, Test: 0.5658\n",
            "Early stopping:  0.07472824335501577\n",
            "Epoch: 019, Loss: 1.2768, Train: 0.5999, Test: 0.5782\n",
            "Early stopping:  0.07040970590679216\n",
            "Epoch: 020, Loss: 1.2391, Train: 0.6142, Test: 0.5918\n",
            "Early stopping:  0.06550447278047504\n",
            "Epoch: 021, Loss: 1.2059, Train: 0.6234, Test: 0.5935\n",
            "Early stopping:  0.060454467942464334\n",
            "Epoch: 022, Loss: 1.1786, Train: 0.6314, Test: 0.5958\n",
            "Early stopping:  0.05491472470202754\n",
            "Epoch: 023, Loss: 1.1519, Train: 0.6403, Test: 0.6037\n",
            "Early stopping:  0.049204635974523306\n",
            "Epoch: 024, Loss: 1.1266, Train: 0.6465, Test: 0.6083\n",
            "Early stopping:  0.04417895976776882\n",
            "Epoch: 025, Loss: 1.1037, Train: 0.6519, Test: 0.6202\n",
            "Early stopping:  0.040570957896758186\n",
            "Epoch: 026, Loss: 1.0831, Train: 0.6610, Test: 0.6287\n",
            "Early stopping:  0.03788055764666704\n",
            "Epoch: 027, Loss: 1.0610, Train: 0.6664, Test: 0.6287\n",
            "Early stopping:  0.035650648142334915\n",
            "Epoch: 028, Loss: 1.0386, Train: 0.6739, Test: 0.6315\n",
            "Early stopping:  0.03459415546191453\n",
            "Epoch: 029, Loss: 1.0186, Train: 0.6842, Test: 0.6412\n",
            "Early stopping:  0.033950968613373336\n",
            "Epoch: 030, Loss: 0.9984, Train: 0.6901, Test: 0.6474\n",
            "Early stopping:  0.033498034521611604\n",
            "Epoch: 031, Loss: 0.9786, Train: 0.6959, Test: 0.6519\n",
            "Early stopping:  0.03242694526842097\n",
            "Epoch: 032, Loss: 0.9597, Train: 0.7035, Test: 0.6593\n",
            "Early stopping:  0.03127965329165188\n",
            "Epoch: 033, Loss: 0.9419, Train: 0.7109, Test: 0.6661\n",
            "Early stopping:  0.030393963654620045\n",
            "Epoch: 034, Loss: 0.9246, Train: 0.7157, Test: 0.6757\n",
            "Early stopping:  0.029166870715936096\n",
            "Epoch: 035, Loss: 0.9077, Train: 0.7207, Test: 0.6786\n",
            "Early stopping:  0.027974659775914297\n",
            "Epoch: 036, Loss: 0.8923, Train: 0.7313, Test: 0.6848\n",
            "Early stopping:  0.02671580703902046\n",
            "Epoch: 037, Loss: 0.8773, Train: 0.7405, Test: 0.6882\n",
            "Early stopping:  0.02552046938565504\n",
            "Epoch: 038, Loss: 0.8627, Train: 0.7486, Test: 0.6916\n",
            "Early stopping:  0.0243903087674999\n",
            "Epoch: 039, Loss: 0.8487, Train: 0.7549, Test: 0.6950\n",
            "Early stopping:  0.023338478222826274\n",
            "Epoch: 040, Loss: 0.8345, Train: 0.7619, Test: 0.6961\n",
            "Early stopping:  0.022792794838303924\n",
            "Epoch: 041, Loss: 0.8203, Train: 0.7662, Test: 0.7024\n",
            "Early stopping:  0.022469248589110584\n",
            "Epoch: 042, Loss: 0.8068, Train: 0.7707, Test: 0.7024\n",
            "Early stopping:  0.022170922955187657\n",
            "Epoch: 043, Loss: 0.7930, Train: 0.7743, Test: 0.7052\n",
            "Early stopping:  0.022011172782571437\n",
            "Epoch: 044, Loss: 0.7794, Train: 0.7780, Test: 0.7046\n",
            "Early stopping:  0.02176049215413369\n",
            "Epoch: 045, Loss: 0.7659, Train: 0.7805, Test: 0.7080\n",
            "Early stopping:  0.021550351411568695\n",
            "Epoch: 046, Loss: 0.7524, Train: 0.7845, Test: 0.7086\n",
            "Early stopping:  0.02148845574792564\n",
            "Epoch: 047, Loss: 0.7393, Train: 0.7904, Test: 0.7103\n",
            "Early stopping:  0.02127245814374824\n",
            "Epoch: 048, Loss: 0.7261, Train: 0.7926, Test: 0.7115\n",
            "Early stopping:  0.02104003751376632\n",
            "Epoch: 049, Loss: 0.7134, Train: 0.7964, Test: 0.7188\n",
            "Early stopping:  0.020743837052223827\n",
            "Epoch: 050, Loss: 0.7007, Train: 0.8018, Test: 0.7143\n",
            "Early stopping:  0.02041906536606719\n",
            "Epoch: 051, Loss: 0.6888, Train: 0.8033, Test: 0.7222\n",
            "Early stopping:  0.01995815835246572\n",
            "Epoch: 052, Loss: 0.6783, Train: 0.8002, Test: 0.7149\n",
            "Early stopping:  0.019028189773258587\n",
            "Epoch: 053, Loss: 0.6721, Train: 0.8079, Test: 0.7256\n",
            "Early stopping:  0.01672221512621056\n",
            "Epoch: 054, Loss: 0.6679, Train: 0.8070, Test: 0.7211\n",
            "Early stopping:  0.013288877150323298\n",
            "Epoch: 055, Loss: 0.6533, Train: 0.8178, Test: 0.7290\n",
            "Early stopping:  0.0131377294919657\n",
            "Epoch: 056, Loss: 0.6327, Train: 0.8202, Test: 0.7330\n",
            "Early stopping:  0.018246402627485243\n",
            "Epoch: 057, Loss: 0.6292, Train: 0.8151, Test: 0.7245\n",
            "Early stopping:  0.01968033521406878\n",
            "Epoch: 058, Loss: 0.6236, Train: 0.8310, Test: 0.7336\n",
            "Early stopping:  0.01863088921246642\n",
            "Epoch: 059, Loss: 0.6055, Train: 0.8320, Test: 0.7364\n",
            "Early stopping:  0.017207303462556712\n",
            "Epoch: 060, Loss: 0.5990, Train: 0.8262, Test: 0.7324\n",
            "Early stopping:  0.014899997071272177\n",
            "Epoch: 061, Loss: 0.5948, Train: 0.8415, Test: 0.7387\n",
            "Early stopping:  0.015194776558859771\n",
            "Epoch: 062, Loss: 0.5799, Train: 0.8439, Test: 0.7375\n",
            "Early stopping:  0.015949019523479494\n",
            "Epoch: 063, Loss: 0.5717, Train: 0.8378, Test: 0.7370\n",
            "Early stopping:  0.013969082344654087\n",
            "Epoch: 064, Loss: 0.5680, Train: 0.8481, Test: 0.7432\n",
            "Early stopping:  0.013768345834873744\n",
            "Epoch: 065, Loss: 0.5560, Train: 0.8520, Test: 0.7426\n",
            "Early stopping:  0.014421332950195273\n",
            "Epoch: 066, Loss: 0.5459, Train: 0.8494, Test: 0.7489\n",
            "Early stopping:  0.013401266778608223\n",
            "Epoch: 067, Loss: 0.5413, Train: 0.8576, Test: 0.7500\n",
            "Early stopping:  0.013300198984716034\n",
            "Epoch: 068, Loss: 0.5344, Train: 0.8595, Test: 0.7500\n",
            "Early stopping:  0.013136043437611055\n",
            "Epoch: 069, Loss: 0.5231, Train: 0.8618, Test: 0.7517\n",
            "Early stopping:  0.012331168178149153\n",
            "Epoch: 070, Loss: 0.5144, Train: 0.8654, Test: 0.7534\n",
            "Early stopping:  0.012988210934566865\n",
            "Epoch: 071, Loss: 0.5106, Train: 0.8611, Test: 0.7534\n",
            "Early stopping:  0.013018511922303801\n",
            "Epoch: 072, Loss: 0.5042, Train: 0.8732, Test: 0.7568\n",
            "Early stopping:  0.011745445789511862\n",
            "Epoch: 073, Loss: 0.4947, Train: 0.8706, Test: 0.7596\n",
            "Early stopping:  0.010712042621116948\n",
            "Epoch: 074, Loss: 0.4860, Train: 0.8754, Test: 0.7568\n",
            "Early stopping:  0.011638575203645352\n",
            "Epoch: 075, Loss: 0.4776, Train: 0.8810, Test: 0.7602\n",
            "Early stopping:  0.01332660584433697\n",
            "Epoch: 076, Loss: 0.4728, Train: 0.8764, Test: 0.7557\n",
            "Early stopping:  0.01270645990979906\n",
            "Epoch: 077, Loss: 0.4696, Train: 0.8813, Test: 0.7613\n",
            "Early stopping:  0.01020063550189444\n",
            "Epoch: 078, Loss: 0.4650, Train: 0.8772, Test: 0.7511\n",
            "Early stopping:  0.008047251507157061\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.70      0.75       190\n",
            "         capital_goods       0.80      0.65      0.71       127\n",
            "conglomerates_industry       0.94      0.80      0.86        20\n",
            "     consumer_cyclical       0.74      0.67      0.70       198\n",
            " consumer_non-cyclical       0.69      0.60      0.64       112\n",
            "                energy       0.85      0.72      0.78        71\n",
            "             financial       0.81      0.74      0.78       192\n",
            "            healthcare       0.82      0.75      0.78        79\n",
            "              services       0.69      0.86      0.76       519\n",
            "            technology       0.73      0.71      0.72        99\n",
            "        transportation       0.89      0.82      0.86       101\n",
            "             utilities       0.80      0.79      0.79        56\n",
            "\n",
            "              accuracy                           0.75      1764\n",
            "             macro avg       0.80      0.73      0.76      1764\n",
            "          weighted avg       0.76      0.75      0.75      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4717, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2709, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1419610382031218\n",
            "Epoch: 003, Loss: 2.1491, Train: 0.2946, Test: 0.2942\n",
            "Early stopping:  0.16288305853136692\n",
            "Epoch: 004, Loss: 2.1216, Train: 0.3053, Test: 0.3033\n",
            "Early stopping:  0.1593743629492399\n",
            "Epoch: 005, Loss: 2.0568, Train: 0.3452, Test: 0.3379\n",
            "Early stopping:  0.1636352037730196\n",
            "Epoch: 006, Loss: 1.9769, Train: 0.3737, Test: 0.3594\n",
            "Early stopping:  0.1095040858773177\n",
            "Epoch: 007, Loss: 1.9069, Train: 0.3933, Test: 0.3753\n",
            "Early stopping:  0.10063763460997417\n",
            "Epoch: 008, Loss: 1.8477, Train: 0.4185, Test: 0.4014\n",
            "Early stopping:  0.11043402814042681\n",
            "Epoch: 009, Loss: 1.7875, Train: 0.4409, Test: 0.4303\n",
            "Early stopping:  0.10582428391932035\n",
            "Epoch: 010, Loss: 1.7205, Train: 0.4602, Test: 0.4410\n",
            "Early stopping:  0.10002033780077738\n",
            "Epoch: 011, Loss: 1.6526, Train: 0.4805, Test: 0.4586\n",
            "Early stopping:  0.1005704779065613\n",
            "Epoch: 012, Loss: 1.5909, Train: 0.4897, Test: 0.4637\n",
            "Early stopping:  0.10255195331980789\n",
            "Epoch: 013, Loss: 1.5338, Train: 0.5016, Test: 0.4830\n",
            "Early stopping:  0.10077805867909376\n",
            "Epoch: 014, Loss: 1.4759, Train: 0.5284, Test: 0.5051\n",
            "Early stopping:  0.0961953626564168\n",
            "Epoch: 015, Loss: 1.4195, Train: 0.5615, Test: 0.5346\n",
            "Early stopping:  0.09190933598650936\n",
            "Epoch: 016, Loss: 1.3701, Train: 0.5868, Test: 0.5584\n",
            "Early stopping:  0.08792669702398331\n",
            "Epoch: 017, Loss: 1.3273, Train: 0.5971, Test: 0.5726\n",
            "Early stopping:  0.08217295494844405\n",
            "Epoch: 018, Loss: 1.2862, Train: 0.6029, Test: 0.5845\n",
            "Early stopping:  0.07472803327056282\n",
            "Epoch: 019, Loss: 1.2479, Train: 0.6077, Test: 0.5884\n",
            "Early stopping:  0.06761981385253378\n",
            "Epoch: 020, Loss: 1.2151, Train: 0.6136, Test: 0.5913\n",
            "Early stopping:  0.061648888178907985\n",
            "Epoch: 021, Loss: 1.1855, Train: 0.6236, Test: 0.6032\n",
            "Early stopping:  0.05623139874082792\n",
            "Epoch: 022, Loss: 1.1565, Train: 0.6392, Test: 0.6134\n",
            "Early stopping:  0.05098216930211897\n",
            "Epoch: 023, Loss: 1.1307, Train: 0.6474, Test: 0.6213\n",
            "Early stopping:  0.04637567338550669\n",
            "Epoch: 024, Loss: 1.1074, Train: 0.6543, Test: 0.6219\n",
            "Early stopping:  0.04277848073672268\n",
            "Epoch: 025, Loss: 1.0828, Train: 0.6601, Test: 0.6219\n",
            "Early stopping:  0.04026388347587176\n",
            "Epoch: 026, Loss: 1.0589, Train: 0.6679, Test: 0.6293\n",
            "Early stopping:  0.03843587359363474\n",
            "Epoch: 027, Loss: 1.0377, Train: 0.6746, Test: 0.6383\n",
            "Early stopping:  0.0370967091174189\n",
            "Epoch: 028, Loss: 1.0160, Train: 0.6833, Test: 0.6468\n",
            "Early stopping:  0.03607091401447021\n",
            "Epoch: 029, Loss: 0.9939, Train: 0.6913, Test: 0.6519\n",
            "Early stopping:  0.03489180136970187\n",
            "Epoch: 030, Loss: 0.9739, Train: 0.6987, Test: 0.6593\n",
            "Early stopping:  0.03379460139298437\n",
            "Epoch: 031, Loss: 0.9553, Train: 0.7062, Test: 0.6627\n",
            "Early stopping:  0.03272304149286033\n",
            "Epoch: 032, Loss: 0.9364, Train: 0.7129, Test: 0.6604\n",
            "Early stopping:  0.03130317837567735\n",
            "Epoch: 033, Loss: 0.9187, Train: 0.7218, Test: 0.6695\n",
            "Early stopping:  0.029729144074724087\n",
            "Epoch: 034, Loss: 0.9017, Train: 0.7322, Test: 0.6740\n",
            "Early stopping:  0.028618387851060544\n",
            "Epoch: 035, Loss: 0.8853, Train: 0.7400, Test: 0.6780\n",
            "Early stopping:  0.02761864707692267\n",
            "Epoch: 036, Loss: 0.8697, Train: 0.7456, Test: 0.6825\n",
            "Early stopping:  0.026363665968305052\n",
            "Epoch: 037, Loss: 0.8543, Train: 0.7510, Test: 0.6808\n",
            "Early stopping:  0.025420042980791896\n",
            "Epoch: 038, Loss: 0.8392, Train: 0.7544, Test: 0.6848\n",
            "Early stopping:  0.024677066639326197\n",
            "Epoch: 039, Loss: 0.8249, Train: 0.7593, Test: 0.6910\n",
            "Early stopping:  0.02393757705443001\n",
            "Epoch: 040, Loss: 0.8103, Train: 0.7661, Test: 0.6961\n",
            "Early stopping:  0.02344750001556612\n",
            "Epoch: 041, Loss: 0.7959, Train: 0.7693, Test: 0.7001\n",
            "Early stopping:  0.023043160496098898\n",
            "Epoch: 042, Loss: 0.7823, Train: 0.7754, Test: 0.7041\n",
            "Early stopping:  0.022568391253992487\n",
            "Epoch: 043, Loss: 0.7680, Train: 0.7809, Test: 0.7098\n",
            "Early stopping:  0.022412961295839712\n",
            "Epoch: 044, Loss: 0.7540, Train: 0.7849, Test: 0.7098\n",
            "Early stopping:  0.02223017369231326\n",
            "Epoch: 045, Loss: 0.7403, Train: 0.7913, Test: 0.7137\n",
            "Early stopping:  0.022055572332315056\n",
            "Epoch: 046, Loss: 0.7266, Train: 0.7946, Test: 0.7166\n",
            "Early stopping:  0.022006386834900218\n",
            "Epoch: 047, Loss: 0.7134, Train: 0.8019, Test: 0.7177\n",
            "Early stopping:  0.021610334040807293\n",
            "Epoch: 048, Loss: 0.7006, Train: 0.7985, Test: 0.7188\n",
            "Early stopping:  0.02114165261698577\n",
            "Epoch: 049, Loss: 0.6895, Train: 0.8089, Test: 0.7194\n",
            "Early stopping:  0.020201679493536795\n",
            "Epoch: 050, Loss: 0.6824, Train: 0.7967, Test: 0.7132\n",
            "Early stopping:  0.01785031472573226\n",
            "Epoch: 051, Loss: 0.6751, Train: 0.8195, Test: 0.7290\n",
            "Early stopping:  0.015137707190096997\n",
            "Epoch: 052, Loss: 0.6578, Train: 0.8221, Test: 0.7279\n",
            "Early stopping:  0.016051240989694522\n",
            "Epoch: 053, Loss: 0.6417, Train: 0.8133, Test: 0.7217\n",
            "Early stopping:  0.01940053461070892\n",
            "Epoch: 054, Loss: 0.6379, Train: 0.8286, Test: 0.7319\n",
            "Early stopping:  0.01969491223661058\n",
            "Epoch: 055, Loss: 0.6270, Train: 0.8334, Test: 0.7387\n",
            "Early stopping:  0.01879427888016383\n",
            "Epoch: 056, Loss: 0.6118, Train: 0.8253, Test: 0.7319\n",
            "Early stopping:  0.01715388559042419\n",
            "Epoch: 057, Loss: 0.6055, Train: 0.8375, Test: 0.7409\n",
            "Early stopping:  0.015819943255726294\n",
            "Epoch: 058, Loss: 0.5961, Train: 0.8418, Test: 0.7421\n",
            "Early stopping:  0.016728830542256153\n",
            "Epoch: 059, Loss: 0.5852, Train: 0.8360, Test: 0.7421\n",
            "Early stopping:  0.015820929906409643\n",
            "Epoch: 060, Loss: 0.5757, Train: 0.8466, Test: 0.7421\n",
            "Early stopping:  0.014690327752312922\n",
            "Epoch: 061, Loss: 0.5669, Train: 0.8489, Test: 0.7466\n",
            "Early stopping:  0.015451962060222854\n",
            "Epoch: 062, Loss: 0.5600, Train: 0.8477, Test: 0.7472\n",
            "Early stopping:  0.014358176760552201\n",
            "Epoch: 063, Loss: 0.5472, Train: 0.8554, Test: 0.7500\n",
            "Early stopping:  0.01455277434397457\n",
            "Epoch: 064, Loss: 0.5400, Train: 0.8550, Test: 0.7511\n",
            "Early stopping:  0.014490912176636157\n",
            "Epoch: 065, Loss: 0.5352, Train: 0.8625, Test: 0.7506\n",
            "Early stopping:  0.013378608619880434\n",
            "Epoch: 066, Loss: 0.5215, Train: 0.8611, Test: 0.7517\n",
            "Early stopping:  0.014261865554681777\n",
            "Epoch: 067, Loss: 0.5140, Train: 0.8660, Test: 0.7545\n",
            "Early stopping:  0.013575628677920963\n",
            "Epoch: 068, Loss: 0.5070, Train: 0.8704, Test: 0.7528\n",
            "Early stopping:  0.013895657998145792\n",
            "Epoch: 069, Loss: 0.4986, Train: 0.8629, Test: 0.7511\n",
            "Early stopping:  0.013969114239247979\n",
            "Epoch: 070, Loss: 0.4916, Train: 0.8785, Test: 0.7511\n",
            "Early stopping:  0.01188792723693415\n",
            "Epoch: 071, Loss: 0.4802, Train: 0.8793, Test: 0.7568\n",
            "Early stopping:  0.01317253527665002\n",
            "Epoch: 072, Loss: 0.4722, Train: 0.8761, Test: 0.7557\n",
            "Early stopping:  0.013958074401277817\n",
            "Epoch: 073, Loss: 0.4655, Train: 0.8867, Test: 0.7574\n",
            "Early stopping:  0.013611048057903648\n",
            "Epoch: 074, Loss: 0.4575, Train: 0.8813, Test: 0.7619\n",
            "Early stopping:  0.013175714279374405\n",
            "Epoch: 075, Loss: 0.4532, Train: 0.8900, Test: 0.7574\n",
            "Early stopping:  0.010899321397722662\n",
            "Epoch: 076, Loss: 0.4478, Train: 0.8808, Test: 0.7642\n",
            "Early stopping:  0.009701300591340249\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.86      0.73      0.79       190\n",
            "         capital_goods       0.72      0.70      0.71       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.80      0.69      0.74       198\n",
            " consumer_non-cyclical       0.77      0.59      0.67       112\n",
            "                energy       0.91      0.75      0.82        71\n",
            "             financial       0.80      0.79      0.79       192\n",
            "            healthcare       0.84      0.77      0.80        79\n",
            "              services       0.69      0.86      0.77       519\n",
            "            technology       0.75      0.64      0.69        99\n",
            "        transportation       0.83      0.82      0.83       101\n",
            "             utilities       0.79      0.80      0.80        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.81      0.74      0.77      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4757, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2454, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1628498592620911\n",
            "Epoch: 003, Loss: 2.2003, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.14769185849325855\n",
            "Epoch: 004, Loss: 2.1340, Train: 0.3002, Test: 0.3005\n",
            "Early stopping:  0.14844609916519041\n",
            "Epoch: 005, Loss: 2.0737, Train: 0.3291, Test: 0.3282\n",
            "Early stopping:  0.1541268486860817\n",
            "Epoch: 006, Loss: 2.0224, Train: 0.3438, Test: 0.3418\n",
            "Early stopping:  0.09068049668799842\n",
            "Epoch: 007, Loss: 1.9694, Train: 0.3477, Test: 0.3447\n",
            "Early stopping:  0.09082711126671374\n",
            "Epoch: 008, Loss: 1.9138, Train: 0.3503, Test: 0.3464\n",
            "Early stopping:  0.08614972627932482\n",
            "Epoch: 009, Loss: 1.8580, Train: 0.3671, Test: 0.3560\n",
            "Early stopping:  0.08538269547068804\n",
            "Epoch: 010, Loss: 1.8003, Train: 0.3974, Test: 0.3900\n",
            "Early stopping:  0.08784700584296788\n",
            "Epoch: 011, Loss: 1.7376, Train: 0.4370, Test: 0.4201\n",
            "Early stopping:  0.09127082494623022\n",
            "Epoch: 012, Loss: 1.6763, Train: 0.4631, Test: 0.4461\n",
            "Early stopping:  0.09418189039182713\n",
            "Epoch: 013, Loss: 1.6215, Train: 0.4869, Test: 0.4683\n",
            "Early stopping:  0.09443173272859508\n",
            "Epoch: 014, Loss: 1.5662, Train: 0.5100, Test: 0.4921\n",
            "Early stopping:  0.09242684885500059\n",
            "Epoch: 015, Loss: 1.5083, Train: 0.5342, Test: 0.5147\n",
            "Early stopping:  0.08995509952184041\n",
            "Epoch: 016, Loss: 1.4547, Train: 0.5521, Test: 0.5300\n",
            "Early stopping:  0.08799273497469672\n",
            "Epoch: 017, Loss: 1.4066, Train: 0.5673, Test: 0.5471\n",
            "Early stopping:  0.0856268995581944\n",
            "Epoch: 018, Loss: 1.3605, Train: 0.5786, Test: 0.5522\n",
            "Early stopping:  0.08120112906122053\n",
            "Epoch: 019, Loss: 1.3173, Train: 0.5932, Test: 0.5652\n",
            "Early stopping:  0.0753204391815534\n",
            "Epoch: 020, Loss: 1.2769, Train: 0.6033, Test: 0.5777\n",
            "Early stopping:  0.07038574010949253\n",
            "Epoch: 021, Loss: 1.2404, Train: 0.6073, Test: 0.5845\n",
            "Early stopping:  0.06585035523912523\n",
            "Epoch: 022, Loss: 1.2101, Train: 0.6162, Test: 0.5913\n",
            "Early stopping:  0.059864047456672446\n",
            "Epoch: 023, Loss: 1.1804, Train: 0.6275, Test: 0.5941\n",
            "Early stopping:  0.05399578835215077\n",
            "Epoch: 024, Loss: 1.1510, Train: 0.6372, Test: 0.6054\n",
            "Early stopping:  0.049343561040962464\n",
            "Epoch: 025, Loss: 1.1265, Train: 0.6453, Test: 0.6094\n",
            "Early stopping:  0.045412292758609044\n",
            "Epoch: 026, Loss: 1.1032, Train: 0.6533, Test: 0.6202\n",
            "Early stopping:  0.04240345619764035\n",
            "Epoch: 027, Loss: 1.0795, Train: 0.6573, Test: 0.6224\n",
            "Early stopping:  0.03948719417552871\n",
            "Epoch: 028, Loss: 1.0568, Train: 0.6670, Test: 0.6270\n",
            "Early stopping:  0.03720256798077817\n",
            "Epoch: 029, Loss: 1.0358, Train: 0.6745, Test: 0.6395\n",
            "Early stopping:  0.036004999909699575\n",
            "Epoch: 030, Loss: 1.0138, Train: 0.6827, Test: 0.6423\n",
            "Early stopping:  0.03517377307968198\n",
            "Epoch: 031, Loss: 0.9925, Train: 0.6933, Test: 0.6491\n",
            "Early stopping:  0.034330794161166606\n",
            "Epoch: 032, Loss: 0.9725, Train: 0.7020, Test: 0.6548\n",
            "Early stopping:  0.03354040003402832\n",
            "Epoch: 033, Loss: 0.9530, Train: 0.7115, Test: 0.6599\n",
            "Early stopping:  0.032741956711577445\n",
            "Epoch: 034, Loss: 0.9338, Train: 0.7140, Test: 0.6655\n",
            "Early stopping:  0.03154729516462407\n",
            "Epoch: 035, Loss: 0.9154, Train: 0.7210, Test: 0.6689\n",
            "Early stopping:  0.030470728054507706\n",
            "Epoch: 036, Loss: 0.8978, Train: 0.7306, Test: 0.6712\n",
            "Early stopping:  0.02955221501788216\n",
            "Epoch: 037, Loss: 0.8805, Train: 0.7393, Test: 0.6786\n",
            "Early stopping:  0.028648939421269903\n",
            "Epoch: 038, Loss: 0.8641, Train: 0.7476, Test: 0.6803\n",
            "Early stopping:  0.027603691137935402\n",
            "Epoch: 039, Loss: 0.8484, Train: 0.7532, Test: 0.6854\n",
            "Early stopping:  0.02654692700469054\n",
            "Epoch: 040, Loss: 0.8325, Train: 0.7580, Test: 0.6910\n",
            "Early stopping:  0.025728303898263638\n",
            "Epoch: 041, Loss: 0.8177, Train: 0.7611, Test: 0.6922\n",
            "Early stopping:  0.024836465635880833\n",
            "Epoch: 042, Loss: 0.8028, Train: 0.7707, Test: 0.7001\n",
            "Early stopping:  0.024232332912009085\n",
            "Epoch: 043, Loss: 0.7882, Train: 0.7751, Test: 0.7018\n",
            "Early stopping:  0.023731078155130435\n",
            "Epoch: 044, Loss: 0.7739, Train: 0.7777, Test: 0.7035\n",
            "Early stopping:  0.02320401481145208\n",
            "Epoch: 045, Loss: 0.7595, Train: 0.7835, Test: 0.7075\n",
            "Early stopping:  0.022977471889436475\n",
            "Epoch: 046, Loss: 0.7453, Train: 0.7883, Test: 0.7063\n",
            "Early stopping:  0.022704293497459466\n",
            "Epoch: 047, Loss: 0.7312, Train: 0.7927, Test: 0.7143\n",
            "Early stopping:  0.022524942534476997\n",
            "Epoch: 048, Loss: 0.7174, Train: 0.7974, Test: 0.7194\n",
            "Early stopping:  0.022318402748074572\n",
            "Epoch: 049, Loss: 0.7040, Train: 0.7992, Test: 0.7205\n",
            "Early stopping:  0.021959271580387678\n",
            "Epoch: 050, Loss: 0.6909, Train: 0.8066, Test: 0.7279\n",
            "Early stopping:  0.021527359529582162\n",
            "Epoch: 051, Loss: 0.6782, Train: 0.8066, Test: 0.7290\n",
            "Early stopping:  0.020981749020908543\n",
            "Epoch: 052, Loss: 0.6659, Train: 0.8174, Test: 0.7324\n",
            "Early stopping:  0.02036849036351729\n",
            "Epoch: 053, Loss: 0.6542, Train: 0.8145, Test: 0.7302\n",
            "Early stopping:  0.019700146268922702\n",
            "Epoch: 054, Loss: 0.6438, Train: 0.8223, Test: 0.7313\n",
            "Early stopping:  0.01868314350372162\n",
            "Epoch: 055, Loss: 0.6330, Train: 0.8221, Test: 0.7353\n",
            "Early stopping:  0.01778804768786218\n",
            "Epoch: 056, Loss: 0.6207, Train: 0.8328, Test: 0.7455\n",
            "Early stopping:  0.017668430660669553\n",
            "Epoch: 057, Loss: 0.6073, Train: 0.8377, Test: 0.7421\n",
            "Early stopping:  0.01851611256526588\n",
            "Epoch: 058, Loss: 0.5961, Train: 0.8368, Test: 0.7387\n",
            "Early stopping:  0.0191734785733809\n",
            "Epoch: 059, Loss: 0.5874, Train: 0.8428, Test: 0.7455\n",
            "Early stopping:  0.01836907173544446\n",
            "Epoch: 060, Loss: 0.5782, Train: 0.8440, Test: 0.7426\n",
            "Early stopping:  0.016650486329882645\n",
            "Epoch: 061, Loss: 0.5674, Train: 0.8524, Test: 0.7455\n",
            "Early stopping:  0.01544828489153496\n",
            "Epoch: 062, Loss: 0.5554, Train: 0.8541, Test: 0.7443\n",
            "Early stopping:  0.016054704249513274\n",
            "Epoch: 063, Loss: 0.5452, Train: 0.8540, Test: 0.7494\n",
            "Early stopping:  0.016982096508221613\n",
            "Epoch: 064, Loss: 0.5368, Train: 0.8606, Test: 0.7494\n",
            "Early stopping:  0.01665775435904115\n",
            "Epoch: 065, Loss: 0.5287, Train: 0.8574, Test: 0.7534\n",
            "Early stopping:  0.015257515705528876\n",
            "Epoch: 066, Loss: 0.5201, Train: 0.8662, Test: 0.7534\n",
            "Early stopping:  0.013784963251877993\n",
            "Epoch: 067, Loss: 0.5098, Train: 0.8673, Test: 0.7506\n",
            "Early stopping:  0.013840108970883563\n",
            "Epoch: 068, Loss: 0.4995, Train: 0.8713, Test: 0.7568\n",
            "Early stopping:  0.014790017967370898\n",
            "Epoch: 069, Loss: 0.4901, Train: 0.8741, Test: 0.7562\n",
            "Early stopping:  0.015468959642904042\n",
            "Epoch: 070, Loss: 0.4818, Train: 0.8732, Test: 0.7528\n",
            "Early stopping:  0.015236278279335439\n",
            "Epoch: 071, Loss: 0.4746, Train: 0.8808, Test: 0.7619\n",
            "Early stopping:  0.013956184959675964\n",
            "Epoch: 072, Loss: 0.4680, Train: 0.8731, Test: 0.7557\n",
            "Early stopping:  0.01245527755651526\n",
            "Epoch: 073, Loss: 0.4626, Train: 0.8869, Test: 0.7608\n",
            "Early stopping:  0.01091928522427074\n",
            "Epoch: 074, Loss: 0.4563, Train: 0.8792, Test: 0.7591\n",
            "Early stopping:  0.010000524382432636\n",
            "Epoch: 075, Loss: 0.4464, Train: 0.8922, Test: 0.7613\n",
            "Early stopping:  0.010836607370246682\n",
            "Epoch: 076, Loss: 0.4349, Train: 0.8942, Test: 0.7625\n",
            "Early stopping:  0.013195270227164478\n",
            "Epoch: 077, Loss: 0.4267, Train: 0.8900, Test: 0.7630\n",
            "Early stopping:  0.014787606458095546\n",
            "Epoch: 078, Loss: 0.4220, Train: 0.9003, Test: 0.7613\n",
            "Early stopping:  0.014098924971887191\n",
            "Epoch: 079, Loss: 0.4173, Train: 0.8932, Test: 0.7608\n",
            "Early stopping:  0.011495337675319899\n",
            "Epoch: 080, Loss: 0.4099, Train: 0.9066, Test: 0.7642\n",
            "Early stopping:  0.00942769738342447\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.74      0.77      0.75       190\n",
            "         capital_goods       0.74      0.64      0.68       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.67      0.76      0.71       198\n",
            " consumer_non-cyclical       0.77      0.67      0.71       112\n",
            "                energy       0.87      0.75      0.80        71\n",
            "             financial       0.81      0.79      0.80       192\n",
            "            healthcare       0.87      0.75      0.80        79\n",
            "              services       0.75      0.82      0.78       519\n",
            "            technology       0.74      0.69      0.71        99\n",
            "        transportation       0.85      0.80      0.83       101\n",
            "             utilities       0.85      0.79      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.80      0.75      0.77      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5045, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2800, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15872621160378522\n",
            "Epoch: 003, Loss: 2.1575, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.17596001155161076\n",
            "Epoch: 004, Loss: 2.1358, Train: 0.2985, Test: 0.2965\n",
            "Early stopping:  0.16904821085518565\n",
            "Epoch: 005, Loss: 2.0689, Train: 0.3481, Test: 0.3418\n",
            "Early stopping:  0.17169781233820933\n",
            "Epoch: 006, Loss: 2.0049, Train: 0.4064, Test: 0.3923\n",
            "Early stopping:  0.10328444396150509\n",
            "Epoch: 007, Loss: 1.9504, Train: 0.4150, Test: 0.3974\n",
            "Early stopping:  0.08706113136911396\n",
            "Epoch: 008, Loss: 1.8953, Train: 0.4122, Test: 0.3991\n",
            "Early stopping:  0.09488661327598345\n",
            "Epoch: 009, Loss: 1.8360, Train: 0.4102, Test: 0.3951\n",
            "Early stopping:  0.09099332349625673\n",
            "Epoch: 010, Loss: 1.7777, Train: 0.4180, Test: 0.3997\n",
            "Early stopping:  0.08994721619592312\n",
            "Epoch: 011, Loss: 1.7227, Train: 0.4401, Test: 0.4240\n",
            "Early stopping:  0.09061318098347719\n",
            "Epoch: 012, Loss: 1.6665, Train: 0.4648, Test: 0.4569\n",
            "Early stopping:  0.09028955433199752\n",
            "Epoch: 013, Loss: 1.6101, Train: 0.4886, Test: 0.4745\n",
            "Early stopping:  0.08902445776664303\n",
            "Epoch: 014, Loss: 1.5580, Train: 0.5022, Test: 0.4853\n",
            "Early stopping:  0.0872842717769672\n",
            "Epoch: 015, Loss: 1.5063, Train: 0.5230, Test: 0.5051\n",
            "Early stopping:  0.08559563286361437\n",
            "Epoch: 016, Loss: 1.4529, Train: 0.5514, Test: 0.5261\n",
            "Early stopping:  0.083953063161826\n",
            "Epoch: 017, Loss: 1.4035, Train: 0.5673, Test: 0.5454\n",
            "Early stopping:  0.08196290174781767\n",
            "Epoch: 018, Loss: 1.3610, Train: 0.5803, Test: 0.5544\n",
            "Early stopping:  0.07863361746927033\n",
            "Epoch: 019, Loss: 1.3197, Train: 0.5898, Test: 0.5658\n",
            "Early stopping:  0.07367953153214594\n",
            "Epoch: 020, Loss: 1.2779, Train: 0.6009, Test: 0.5777\n",
            "Early stopping:  0.0686542704175555\n",
            "Epoch: 021, Loss: 1.2417, Train: 0.6061, Test: 0.5828\n",
            "Early stopping:  0.06431565347797912\n",
            "Epoch: 022, Loss: 1.2112, Train: 0.6160, Test: 0.5947\n",
            "Early stopping:  0.05981212673078688\n",
            "Epoch: 023, Loss: 1.1815, Train: 0.6263, Test: 0.5992\n",
            "Early stopping:  0.05437833838319502\n",
            "Epoch: 024, Loss: 1.1531, Train: 0.6379, Test: 0.6054\n",
            "Early stopping:  0.04904586739418698\n",
            "Epoch: 025, Loss: 1.1274, Train: 0.6487, Test: 0.6111\n",
            "Early stopping:  0.04538600105352305\n",
            "Epoch: 026, Loss: 1.1028, Train: 0.6552, Test: 0.6185\n",
            "Early stopping:  0.04288716228550718\n",
            "Epoch: 027, Loss: 1.0792, Train: 0.6590, Test: 0.6281\n",
            "Early stopping:  0.04032768977194078\n",
            "Epoch: 028, Loss: 1.0564, Train: 0.6645, Test: 0.6310\n",
            "Early stopping:  0.038171226248186196\n",
            "Epoch: 029, Loss: 1.0346, Train: 0.6729, Test: 0.6389\n",
            "Early stopping:  0.0366544875069109\n",
            "Epoch: 030, Loss: 1.0144, Train: 0.6811, Test: 0.6440\n",
            "Early stopping:  0.035012042948814395\n",
            "Epoch: 031, Loss: 0.9939, Train: 0.6881, Test: 0.6502\n",
            "Early stopping:  0.033652313489922155\n",
            "Epoch: 032, Loss: 0.9740, Train: 0.6946, Test: 0.6519\n",
            "Early stopping:  0.03252211493740813\n",
            "Epoch: 033, Loss: 0.9545, Train: 0.7035, Test: 0.6582\n",
            "Early stopping:  0.031747561878954535\n",
            "Epoch: 034, Loss: 0.9358, Train: 0.7085, Test: 0.6661\n",
            "Early stopping:  0.031078854388295465\n",
            "Epoch: 035, Loss: 0.9184, Train: 0.7180, Test: 0.6695\n",
            "Early stopping:  0.029908686101136492\n",
            "Epoch: 036, Loss: 0.9005, Train: 0.7275, Test: 0.6752\n",
            "Early stopping:  0.02896807479676839\n",
            "Epoch: 037, Loss: 0.8842, Train: 0.7342, Test: 0.6757\n",
            "Early stopping:  0.027823761075284534\n",
            "Epoch: 038, Loss: 0.8677, Train: 0.7390, Test: 0.6803\n",
            "Early stopping:  0.026961104799390834\n",
            "Epoch: 039, Loss: 0.8519, Train: 0.7469, Test: 0.6831\n",
            "Early stopping:  0.02622601670789413\n",
            "Epoch: 040, Loss: 0.8363, Train: 0.7517, Test: 0.6888\n",
            "Early stopping:  0.02539851165719246\n",
            "Epoch: 041, Loss: 0.8207, Train: 0.7610, Test: 0.6922\n",
            "Early stopping:  0.02503453404311281\n",
            "Epoch: 042, Loss: 0.8058, Train: 0.7663, Test: 0.6939\n",
            "Early stopping:  0.024510560211215063\n",
            "Epoch: 043, Loss: 0.7908, Train: 0.7723, Test: 0.6978\n",
            "Early stopping:  0.024139281304841182\n",
            "Epoch: 044, Loss: 0.7759, Train: 0.7761, Test: 0.7046\n",
            "Early stopping:  0.023846013068075447\n",
            "Epoch: 045, Loss: 0.7611, Train: 0.7811, Test: 0.7046\n",
            "Early stopping:  0.02356790215070745\n",
            "Epoch: 046, Loss: 0.7463, Train: 0.7862, Test: 0.7092\n",
            "Early stopping:  0.023495071549948718\n",
            "Epoch: 047, Loss: 0.7317, Train: 0.7924, Test: 0.7120\n",
            "Early stopping:  0.023348581221242636\n",
            "Epoch: 048, Loss: 0.7172, Train: 0.7978, Test: 0.7194\n",
            "Early stopping:  0.023188966207032705\n",
            "Epoch: 049, Loss: 0.7032, Train: 0.8012, Test: 0.7217\n",
            "Early stopping:  0.022927306095301872\n",
            "Epoch: 050, Loss: 0.6893, Train: 0.8031, Test: 0.7211\n",
            "Early stopping:  0.02256041329230161\n",
            "Epoch: 051, Loss: 0.6771, Train: 0.8106, Test: 0.7307\n",
            "Early stopping:  0.021730396489681714\n",
            "Epoch: 052, Loss: 0.6685, Train: 0.8031, Test: 0.7154\n",
            "Early stopping:  0.019619377514815926\n",
            "Epoch: 053, Loss: 0.6661, Train: 0.8172, Test: 0.7336\n",
            "Early stopping:  0.015436292011757852\n",
            "Epoch: 054, Loss: 0.6498, Train: 0.8208, Test: 0.7381\n",
            "Early stopping:  0.014557133992136258\n",
            "Epoch: 055, Loss: 0.6279, Train: 0.8181, Test: 0.7324\n",
            "Early stopping:  0.01943497594739675\n",
            "Epoch: 056, Loss: 0.6250, Train: 0.8263, Test: 0.7392\n",
            "Early stopping:  0.020510167658914384\n",
            "Epoch: 057, Loss: 0.6153, Train: 0.8296, Test: 0.7483\n",
            "Early stopping:  0.020665405872135882\n",
            "Epoch: 058, Loss: 0.5980, Train: 0.8321, Test: 0.7426\n",
            "Early stopping:  0.01890756295558076\n",
            "Epoch: 059, Loss: 0.5930, Train: 0.8395, Test: 0.7443\n",
            "Early stopping:  0.01572325543447385\n",
            "Epoch: 060, Loss: 0.5840, Train: 0.8398, Test: 0.7500\n",
            "Early stopping:  0.016714854357283076\n",
            "Epoch: 061, Loss: 0.5698, Train: 0.8428, Test: 0.7500\n",
            "Early stopping:  0.01684220069020054\n",
            "Epoch: 062, Loss: 0.5642, Train: 0.8497, Test: 0.7523\n",
            "Early stopping:  0.014554410408233546\n",
            "Epoch: 063, Loss: 0.5557, Train: 0.8489, Test: 0.7528\n",
            "Early stopping:  0.015072720190342174\n",
            "Epoch: 064, Loss: 0.5435, Train: 0.8516, Test: 0.7540\n",
            "Early stopping:  0.01517464390164196\n",
            "Epoch: 065, Loss: 0.5367, Train: 0.8571, Test: 0.7540\n",
            "Early stopping:  0.013825683137034884\n",
            "Epoch: 066, Loss: 0.5301, Train: 0.8569, Test: 0.7562\n",
            "Early stopping:  0.013891452623864525\n",
            "Epoch: 067, Loss: 0.5190, Train: 0.8602, Test: 0.7591\n",
            "Early stopping:  0.013832866665995702\n",
            "Epoch: 068, Loss: 0.5101, Train: 0.8683, Test: 0.7579\n",
            "Early stopping:  0.013427311064027479\n",
            "Epoch: 069, Loss: 0.5049, Train: 0.8628, Test: 0.7562\n",
            "Early stopping:  0.013299461377953094\n",
            "Epoch: 070, Loss: 0.4972, Train: 0.8740, Test: 0.7579\n",
            "Early stopping:  0.012728008068662939\n",
            "Epoch: 071, Loss: 0.4868, Train: 0.8751, Test: 0.7619\n",
            "Early stopping:  0.012288259638740386\n",
            "Epoch: 072, Loss: 0.4786, Train: 0.8721, Test: 0.7591\n",
            "Early stopping:  0.012892464962897306\n",
            "Epoch: 073, Loss: 0.4728, Train: 0.8840, Test: 0.7613\n",
            "Early stopping:  0.013134455999588963\n",
            "Epoch: 074, Loss: 0.4667, Train: 0.8778, Test: 0.7574\n",
            "Early stopping:  0.011955639013496342\n",
            "Epoch: 075, Loss: 0.4605, Train: 0.8854, Test: 0.7653\n",
            "Early stopping:  0.010221745675276069\n",
            "Epoch: 076, Loss: 0.4540, Train: 0.8847, Test: 0.7596\n",
            "Early stopping:  0.009743262472156367\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.81      0.75      0.78       190\n",
            "         capital_goods       0.71      0.65      0.68       127\n",
            "conglomerates_industry       0.93      0.70      0.80        20\n",
            "     consumer_cyclical       0.74      0.72      0.73       198\n",
            " consumer_non-cyclical       0.80      0.63      0.71       112\n",
            "                energy       0.83      0.68      0.74        71\n",
            "             financial       0.78      0.79      0.78       192\n",
            "            healthcare       0.82      0.76      0.79        79\n",
            "              services       0.72      0.84      0.78       519\n",
            "            technology       0.70      0.68      0.69        99\n",
            "        transportation       0.92      0.80      0.86       101\n",
            "             utilities       0.77      0.77      0.77        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.79      0.73      0.76      1764\n",
            "          weighted avg       0.76      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5121, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2748, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  0.1678157695410387\n",
            "Epoch: 003, Loss: 2.1839, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  0.1694651490504696\n",
            "Epoch: 004, Loss: 2.1256, Train: 0.3047, Test: 0.3044\n",
            "Early stopping:  0.17013278482474772\n",
            "Epoch: 005, Loss: 2.0691, Train: 0.3625, Test: 0.3509\n",
            "Early stopping:  0.17353808058288936\n",
            "Epoch: 006, Loss: 2.0242, Train: 0.3855, Test: 0.3702\n",
            "Early stopping:  0.09828038312630395\n",
            "Epoch: 007, Loss: 1.9690, Train: 0.3837, Test: 0.3702\n",
            "Early stopping:  0.0840571261239593\n",
            "Epoch: 008, Loss: 1.9072, Train: 0.3703, Test: 0.3600\n",
            "Early stopping:  0.08498069630054013\n",
            "Epoch: 009, Loss: 1.8495, Train: 0.3688, Test: 0.3605\n",
            "Early stopping:  0.08808267686360727\n",
            "Epoch: 010, Loss: 1.7983, Train: 0.3896, Test: 0.3781\n",
            "Early stopping:  0.09037675157754196\n",
            "Epoch: 011, Loss: 1.7439, Train: 0.4290, Test: 0.4212\n",
            "Early stopping:  0.0884826166344063\n",
            "Epoch: 012, Loss: 1.6855, Train: 0.4659, Test: 0.4478\n",
            "Early stopping:  0.08683790314426167\n",
            "Epoch: 013, Loss: 1.6325, Train: 0.4849, Test: 0.4643\n",
            "Early stopping:  0.08646410021735171\n",
            "Epoch: 014, Loss: 1.5840, Train: 0.4970, Test: 0.4756\n",
            "Early stopping:  0.08543107848765125\n",
            "Epoch: 015, Loss: 1.5345, Train: 0.5127, Test: 0.4898\n",
            "Early stopping:  0.08230368822724479\n",
            "Epoch: 016, Loss: 1.4841, Train: 0.5256, Test: 0.5074\n",
            "Early stopping:  0.07919376827652527\n",
            "Epoch: 017, Loss: 1.4373, Train: 0.5396, Test: 0.5221\n",
            "Early stopping:  0.07752728868665192\n",
            "Epoch: 018, Loss: 1.3967, Train: 0.5557, Test: 0.5391\n",
            "Early stopping:  0.07467522689042787\n",
            "Epoch: 019, Loss: 1.3592, Train: 0.5744, Test: 0.5607\n",
            "Early stopping:  0.06941368061432901\n",
            "Epoch: 020, Loss: 1.3205, Train: 0.5871, Test: 0.5714\n",
            "Early stopping:  0.0641317298287915\n",
            "Epoch: 021, Loss: 1.2831, Train: 0.5982, Test: 0.5867\n",
            "Early stopping:  0.060811028887380256\n",
            "Epoch: 022, Loss: 1.2515, Train: 0.6048, Test: 0.5913\n",
            "Early stopping:  0.05795803479666261\n",
            "Epoch: 023, Loss: 1.2222, Train: 0.6159, Test: 0.5901\n",
            "Early stopping:  0.05432932647734866\n",
            "Epoch: 024, Loss: 1.1925, Train: 0.6231, Test: 0.6026\n",
            "Early stopping:  0.05017964330905861\n",
            "Epoch: 025, Loss: 1.1657, Train: 0.6318, Test: 0.6100\n",
            "Early stopping:  0.04648089873722233\n",
            "Epoch: 026, Loss: 1.1398, Train: 0.6406, Test: 0.6173\n",
            "Early stopping:  0.044276426386689056\n",
            "Epoch: 027, Loss: 1.1155, Train: 0.6477, Test: 0.6236\n",
            "Early stopping:  0.04209253595475074\n",
            "Epoch: 028, Loss: 1.0927, Train: 0.6535, Test: 0.6230\n",
            "Early stopping:  0.03950512208877527\n",
            "Epoch: 029, Loss: 1.0703, Train: 0.6607, Test: 0.6321\n",
            "Early stopping:  0.03762866388485209\n",
            "Epoch: 030, Loss: 1.0477, Train: 0.6722, Test: 0.6378\n",
            "Early stopping:  0.036308162132663406\n",
            "Epoch: 031, Loss: 1.0269, Train: 0.6773, Test: 0.6406\n",
            "Early stopping:  0.03515680145713618\n",
            "Epoch: 032, Loss: 1.0072, Train: 0.6871, Test: 0.6491\n",
            "Early stopping:  0.033919846503083814\n",
            "Epoch: 033, Loss: 0.9867, Train: 0.6954, Test: 0.6570\n",
            "Early stopping:  0.03284473841553984\n",
            "Epoch: 034, Loss: 0.9680, Train: 0.7037, Test: 0.6633\n",
            "Early stopping:  0.03154226724327954\n",
            "Epoch: 035, Loss: 0.9493, Train: 0.7108, Test: 0.6678\n",
            "Early stopping:  0.030741306422250383\n",
            "Epoch: 036, Loss: 0.9310, Train: 0.7161, Test: 0.6695\n",
            "Early stopping:  0.030008819107129242\n",
            "Epoch: 037, Loss: 0.9146, Train: 0.7225, Test: 0.6752\n",
            "Early stopping:  0.028656705929827647\n",
            "Epoch: 038, Loss: 0.8978, Train: 0.7293, Test: 0.6769\n",
            "Early stopping:  0.027724527165437537\n",
            "Epoch: 039, Loss: 0.8819, Train: 0.7376, Test: 0.6808\n",
            "Early stopping:  0.026571900626850428\n",
            "Epoch: 040, Loss: 0.8667, Train: 0.7444, Test: 0.6882\n",
            "Early stopping:  0.025507532266519587\n",
            "Epoch: 041, Loss: 0.8517, Train: 0.7499, Test: 0.6905\n",
            "Early stopping:  0.024799032609800622\n",
            "Epoch: 042, Loss: 0.8370, Train: 0.7553, Test: 0.6922\n",
            "Early stopping:  0.02399628368484896\n",
            "Epoch: 043, Loss: 0.8221, Train: 0.7605, Test: 0.6967\n",
            "Early stopping:  0.023614886110131888\n",
            "Epoch: 044, Loss: 0.8081, Train: 0.7666, Test: 0.7012\n",
            "Early stopping:  0.023210936335368606\n",
            "Epoch: 045, Loss: 0.7937, Train: 0.7727, Test: 0.7041\n",
            "Early stopping:  0.02290473225016749\n",
            "Epoch: 046, Loss: 0.7798, Train: 0.7757, Test: 0.7063\n",
            "Early stopping:  0.02256835331990715\n",
            "Epoch: 047, Loss: 0.7658, Train: 0.7790, Test: 0.7115\n",
            "Early stopping:  0.022269508725127774\n",
            "Epoch: 048, Loss: 0.7521, Train: 0.7835, Test: 0.7115\n",
            "Early stopping:  0.022132678078931087\n",
            "Epoch: 049, Loss: 0.7385, Train: 0.7885, Test: 0.7098\n",
            "Early stopping:  0.021859185026487805\n",
            "Epoch: 050, Loss: 0.7252, Train: 0.7937, Test: 0.7160\n",
            "Early stopping:  0.02161043081141499\n",
            "Epoch: 051, Loss: 0.7120, Train: 0.7985, Test: 0.7171\n",
            "Early stopping:  0.021271594766053975\n",
            "Epoch: 052, Loss: 0.6994, Train: 0.8032, Test: 0.7171\n",
            "Early stopping:  0.020851173772714674\n",
            "Epoch: 053, Loss: 0.6865, Train: 0.8041, Test: 0.7205\n",
            "Early stopping:  0.020517259423212364\n",
            "Epoch: 054, Loss: 0.6743, Train: 0.8114, Test: 0.7234\n",
            "Early stopping:  0.02013009897442724\n",
            "Epoch: 055, Loss: 0.6624, Train: 0.8111, Test: 0.7183\n",
            "Early stopping:  0.019669793124688504\n",
            "Epoch: 056, Loss: 0.6519, Train: 0.8191, Test: 0.7319\n",
            "Early stopping:  0.0188253951066561\n",
            "Epoch: 057, Loss: 0.6453, Train: 0.8093, Test: 0.7234\n",
            "Early stopping:  0.01664839081430578\n",
            "Epoch: 058, Loss: 0.6425, Train: 0.8256, Test: 0.7358\n",
            "Early stopping:  0.013083430501455004\n",
            "Epoch: 059, Loss: 0.6274, Train: 0.8283, Test: 0.7336\n",
            "Early stopping:  0.012849176442606197\n",
            "Epoch: 060, Loss: 0.6077, Train: 0.8233, Test: 0.7341\n",
            "Early stopping:  0.017706446575621\n",
            "Epoch: 061, Loss: 0.6047, Train: 0.8374, Test: 0.7432\n",
            "Early stopping:  0.018936151999176122\n",
            "Epoch: 062, Loss: 0.5979, Train: 0.8369, Test: 0.7438\n",
            "Early stopping:  0.018428674892498418\n",
            "Epoch: 063, Loss: 0.5806, Train: 0.8357, Test: 0.7426\n",
            "Early stopping:  0.01693852931850822\n",
            "Epoch: 064, Loss: 0.5749, Train: 0.8469, Test: 0.7432\n",
            "Early stopping:  0.01463909687741008\n",
            "Epoch: 065, Loss: 0.5700, Train: 0.8470, Test: 0.7477\n",
            "Early stopping:  0.014984763060995688\n",
            "Epoch: 066, Loss: 0.5549, Train: 0.8472, Test: 0.7506\n",
            "Early stopping:  0.015671764994222747\n",
            "Epoch: 067, Loss: 0.5479, Train: 0.8538, Test: 0.7483\n",
            "Early stopping:  0.01379867774444576\n",
            "Epoch: 068, Loss: 0.5437, Train: 0.8531, Test: 0.7523\n",
            "Early stopping:  0.013680015436561277\n",
            "Epoch: 069, Loss: 0.5309, Train: 0.8592, Test: 0.7534\n",
            "Early stopping:  0.0144420340721851\n",
            "Epoch: 070, Loss: 0.5219, Train: 0.8647, Test: 0.7568\n",
            "Early stopping:  0.013297414633751751\n",
            "Epoch: 071, Loss: 0.5179, Train: 0.8598, Test: 0.7568\n",
            "Early stopping:  0.01312079146026842\n",
            "Epoch: 072, Loss: 0.5091, Train: 0.8708, Test: 0.7591\n",
            "Early stopping:  0.013193582935632903\n",
            "Epoch: 073, Loss: 0.4978, Train: 0.8747, Test: 0.7625\n",
            "Early stopping:  0.01262102850063952\n",
            "Epoch: 074, Loss: 0.4920, Train: 0.8676, Test: 0.7630\n",
            "Early stopping:  0.012783088424688956\n",
            "Epoch: 075, Loss: 0.4872, Train: 0.8812, Test: 0.7653\n",
            "Early stopping:  0.012583447628151776\n",
            "Epoch: 076, Loss: 0.4784, Train: 0.8795, Test: 0.7630\n",
            "Early stopping:  0.011504316318107569\n",
            "Epoch: 077, Loss: 0.4689, Train: 0.8825, Test: 0.7630\n",
            "Early stopping:  0.011395961930335827\n",
            "Epoch: 078, Loss: 0.4618, Train: 0.8886, Test: 0.7698\n",
            "Early stopping:  0.012494465860365964\n",
            "Epoch: 079, Loss: 0.4564, Train: 0.8854, Test: 0.7647\n",
            "Early stopping:  0.01244233624148074\n",
            "Epoch: 080, Loss: 0.4503, Train: 0.8945, Test: 0.7687\n",
            "Early stopping:  0.010937838388134209\n",
            "Epoch: 081, Loss: 0.4430, Train: 0.8910, Test: 0.7698\n",
            "Early stopping:  0.010022566022793685\n",
            "Epoch: 082, Loss: 0.4369, Train: 0.8958, Test: 0.7630\n",
            "Early stopping:  0.010005144297239492\n",
            "Epoch: 083, Loss: 0.4325, Train: 0.8913, Test: 0.7710\n",
            "Early stopping:  0.009709433866001399\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.75      0.77       190\n",
            "         capital_goods       0.72      0.71      0.71       127\n",
            "conglomerates_industry       0.88      0.75      0.81        20\n",
            "     consumer_cyclical       0.83      0.71      0.77       198\n",
            " consumer_non-cyclical       0.78      0.60      0.68       112\n",
            "                energy       0.92      0.76      0.83        71\n",
            "             financial       0.79      0.79      0.79       192\n",
            "            healthcare       0.89      0.73      0.81        79\n",
            "              services       0.70      0.88      0.78       519\n",
            "            technology       0.77      0.64      0.70        99\n",
            "        transportation       0.88      0.82      0.85       101\n",
            "             utilities       0.84      0.75      0.79        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.82      0.74      0.77      1764\n",
            "          weighted avg       0.78      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4251, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2314, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.13697085133943196\n",
            "Epoch: 003, Loss: 2.1580, Train: 0.2943, Test: 0.2948\n",
            "Early stopping:  0.13799186449824352\n",
            "Epoch: 004, Loss: 2.1105, Train: 0.3071, Test: 0.3056\n",
            "Early stopping:  0.13847429959550964\n",
            "Epoch: 005, Loss: 2.0349, Train: 0.3450, Test: 0.3350\n",
            "Early stopping:  0.1486385099779431\n",
            "Epoch: 006, Loss: 1.9675, Train: 0.3753, Test: 0.3571\n",
            "Early stopping:  0.10312885436271038\n",
            "Epoch: 007, Loss: 1.9071, Train: 0.3947, Test: 0.3753\n",
            "Early stopping:  0.10216950184201617\n",
            "Epoch: 008, Loss: 1.8420, Train: 0.4119, Test: 0.3974\n",
            "Early stopping:  0.10520052011819366\n",
            "Epoch: 009, Loss: 1.7719, Train: 0.4343, Test: 0.4201\n",
            "Early stopping:  0.10302037787784885\n",
            "Epoch: 010, Loss: 1.7021, Train: 0.4590, Test: 0.4461\n",
            "Early stopping:  0.10532639327714101\n",
            "Epoch: 011, Loss: 1.6355, Train: 0.4789, Test: 0.4671\n",
            "Early stopping:  0.1079986889587475\n",
            "Epoch: 012, Loss: 1.5722, Train: 0.4995, Test: 0.4847\n",
            "Early stopping:  0.10690511122089585\n",
            "Epoch: 013, Loss: 1.5101, Train: 0.5272, Test: 0.5091\n",
            "Early stopping:  0.10337367153047305\n",
            "Epoch: 014, Loss: 1.4509, Train: 0.5530, Test: 0.5261\n",
            "Early stopping:  0.09930782308887778\n",
            "Epoch: 015, Loss: 1.3971, Train: 0.5771, Test: 0.5499\n",
            "Early stopping:  0.09462343995766223\n",
            "Epoch: 016, Loss: 1.3470, Train: 0.5873, Test: 0.5612\n",
            "Early stopping:  0.08917499638685715\n",
            "Epoch: 017, Loss: 1.3022, Train: 0.5939, Test: 0.5737\n",
            "Early stopping:  0.08226963230035576\n",
            "Epoch: 018, Loss: 1.2611, Train: 0.6054, Test: 0.5788\n",
            "Early stopping:  0.07513475635956311\n",
            "Epoch: 019, Loss: 1.2229, Train: 0.6142, Test: 0.5907\n",
            "Early stopping:  0.06876493269770075\n",
            "Epoch: 020, Loss: 1.1901, Train: 0.6277, Test: 0.5992\n",
            "Early stopping:  0.06225440211144008\n",
            "Epoch: 021, Loss: 1.1606, Train: 0.6360, Test: 0.6060\n",
            "Early stopping:  0.05613992657565692\n",
            "Epoch: 022, Loss: 1.1323, Train: 0.6437, Test: 0.6162\n",
            "Early stopping:  0.050662991339864544\n",
            "Epoch: 023, Loss: 1.1059, Train: 0.6516, Test: 0.6270\n",
            "Early stopping:  0.0461865817262897\n",
            "Epoch: 024, Loss: 1.0827, Train: 0.6601, Test: 0.6315\n",
            "Early stopping:  0.042669215862680104\n",
            "Epoch: 025, Loss: 1.0587, Train: 0.6692, Test: 0.6344\n",
            "Early stopping:  0.040118529103653315\n",
            "Epoch: 026, Loss: 1.0353, Train: 0.6743, Test: 0.6372\n",
            "Early stopping:  0.038176520254207\n",
            "Epoch: 027, Loss: 1.0138, Train: 0.6816, Test: 0.6474\n",
            "Early stopping:  0.03660651103020729\n",
            "Epoch: 028, Loss: 0.9921, Train: 0.6911, Test: 0.6542\n",
            "Early stopping:  0.0357546026336721\n",
            "Epoch: 029, Loss: 0.9709, Train: 0.6989, Test: 0.6616\n",
            "Early stopping:  0.03458868029351057\n",
            "Epoch: 030, Loss: 0.9517, Train: 0.7059, Test: 0.6672\n",
            "Early stopping:  0.03321499397311312\n",
            "Epoch: 031, Loss: 0.9325, Train: 0.7156, Test: 0.6729\n",
            "Early stopping:  0.03210226653784873\n",
            "Epoch: 032, Loss: 0.9139, Train: 0.7222, Test: 0.6723\n",
            "Early stopping:  0.030790139265112756\n",
            "Epoch: 033, Loss: 0.8971, Train: 0.7283, Test: 0.6780\n",
            "Early stopping:  0.02933509740745941\n",
            "Epoch: 034, Loss: 0.8801, Train: 0.7374, Test: 0.6814\n",
            "Early stopping:  0.0282652925511981\n",
            "Epoch: 035, Loss: 0.8645, Train: 0.7437, Test: 0.6854\n",
            "Early stopping:  0.026879396259229246\n",
            "Epoch: 036, Loss: 0.8496, Train: 0.7498, Test: 0.6871\n",
            "Early stopping:  0.025508408332177872\n",
            "Epoch: 037, Loss: 0.8345, Train: 0.7549, Test: 0.6859\n",
            "Early stopping:  0.024619132922295905\n",
            "Epoch: 038, Loss: 0.8199, Train: 0.7607, Test: 0.6905\n",
            "Early stopping:  0.02377672061675133\n",
            "Epoch: 039, Loss: 0.8052, Train: 0.7666, Test: 0.6927\n",
            "Early stopping:  0.02342916560723675\n",
            "Epoch: 040, Loss: 0.7906, Train: 0.7740, Test: 0.6973\n",
            "Early stopping:  0.023286971808212986\n",
            "Epoch: 041, Loss: 0.7763, Train: 0.7785, Test: 0.7012\n",
            "Early stopping:  0.023060754760084408\n",
            "Epoch: 042, Loss: 0.7621, Train: 0.7835, Test: 0.7035\n",
            "Early stopping:  0.02285273136742272\n",
            "Epoch: 043, Loss: 0.7480, Train: 0.7858, Test: 0.7063\n",
            "Early stopping:  0.022580744689626415\n",
            "Epoch: 044, Loss: 0.7346, Train: 0.7893, Test: 0.7115\n",
            "Early stopping:  0.02216490021441715\n",
            "Epoch: 045, Loss: 0.7210, Train: 0.7970, Test: 0.7137\n",
            "Early stopping:  0.021835708591760778\n",
            "Epoch: 046, Loss: 0.7077, Train: 0.7994, Test: 0.7160\n",
            "Early stopping:  0.021484447215312362\n",
            "Epoch: 047, Loss: 0.6946, Train: 0.8042, Test: 0.7183\n",
            "Early stopping:  0.0211579678446282\n",
            "Epoch: 048, Loss: 0.6816, Train: 0.8075, Test: 0.7194\n",
            "Early stopping:  0.020923783866582155\n",
            "Epoch: 049, Loss: 0.6694, Train: 0.8126, Test: 0.7222\n",
            "Early stopping:  0.020450412210686934\n",
            "Epoch: 050, Loss: 0.6578, Train: 0.8119, Test: 0.7279\n",
            "Early stopping:  0.019767587449935713\n",
            "Epoch: 051, Loss: 0.6483, Train: 0.8205, Test: 0.7285\n",
            "Early stopping:  0.0184183698335855\n",
            "Epoch: 052, Loss: 0.6385, Train: 0.8208, Test: 0.7319\n",
            "Early stopping:  0.016994197276773475\n",
            "Epoch: 053, Loss: 0.6235, Train: 0.8309, Test: 0.7353\n",
            "Early stopping:  0.017613251753621235\n",
            "Epoch: 054, Loss: 0.6079, Train: 0.8369, Test: 0.7347\n",
            "Early stopping:  0.01985052961290542\n",
            "Epoch: 055, Loss: 0.5984, Train: 0.8326, Test: 0.7398\n",
            "Early stopping:  0.020696608126271093\n",
            "Epoch: 056, Loss: 0.5902, Train: 0.8440, Test: 0.7438\n",
            "Early stopping:  0.0194371506454676\n",
            "Epoch: 057, Loss: 0.5766, Train: 0.8470, Test: 0.7426\n",
            "Early stopping:  0.017764028404364973\n",
            "Epoch: 058, Loss: 0.5637, Train: 0.8450, Test: 0.7404\n",
            "Early stopping:  0.01752228035269016\n",
            "Epoch: 059, Loss: 0.5558, Train: 0.8531, Test: 0.7483\n",
            "Early stopping:  0.01774881905788253\n",
            "Epoch: 060, Loss: 0.5464, Train: 0.8547, Test: 0.7460\n",
            "Early stopping:  0.017260839986637747\n",
            "Epoch: 061, Loss: 0.5335, Train: 0.8594, Test: 0.7483\n",
            "Early stopping:  0.01641517886955503\n",
            "Epoch: 062, Loss: 0.5220, Train: 0.8637, Test: 0.7562\n",
            "Early stopping:  0.01679573026093045\n",
            "Epoch: 063, Loss: 0.5140, Train: 0.8635, Test: 0.7494\n",
            "Early stopping:  0.017102085685240898\n",
            "Epoch: 064, Loss: 0.5054, Train: 0.8725, Test: 0.7574\n",
            "Early stopping:  0.016117625630182743\n",
            "Epoch: 065, Loss: 0.4939, Train: 0.8730, Test: 0.7562\n",
            "Early stopping:  0.015189478214363776\n",
            "Epoch: 066, Loss: 0.4831, Train: 0.8744, Test: 0.7551\n",
            "Early stopping:  0.015530849502237733\n",
            "Epoch: 067, Loss: 0.4758, Train: 0.8766, Test: 0.7596\n",
            "Early stopping:  0.015661080312859017\n",
            "Epoch: 068, Loss: 0.4717, Train: 0.8769, Test: 0.7591\n",
            "Early stopping:  0.013741566575820032\n",
            "Epoch: 069, Loss: 0.4710, Train: 0.8749, Test: 0.7591\n",
            "Early stopping:  0.009550665665572104\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.71      0.75       190\n",
            "         capital_goods       0.77      0.65      0.71       127\n",
            "conglomerates_industry       0.92      0.55      0.69        20\n",
            "     consumer_cyclical       0.68      0.82      0.74       198\n",
            " consumer_non-cyclical       0.80      0.62      0.70       112\n",
            "                energy       0.96      0.70      0.81        71\n",
            "             financial       0.84      0.70      0.77       192\n",
            "            healthcare       0.91      0.77      0.84        79\n",
            "              services       0.70      0.85      0.76       519\n",
            "            technology       0.74      0.65      0.69        99\n",
            "        transportation       0.87      0.83      0.85       101\n",
            "             utilities       0.85      0.79      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.82      0.72      0.76      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4663, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2712, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.13800631511364758\n",
            "Epoch: 003, Loss: 2.1578, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15607608991709873\n",
            "Epoch: 004, Loss: 2.1290, Train: 0.3068, Test: 0.3073\n",
            "Early stopping:  0.15302214403334555\n",
            "Epoch: 005, Loss: 2.0592, Train: 0.3647, Test: 0.3554\n",
            "Early stopping:  0.15911508301534377\n",
            "Epoch: 006, Loss: 1.9947, Train: 0.4042, Test: 0.3895\n",
            "Early stopping:  0.10456366645129561\n",
            "Epoch: 007, Loss: 1.9390, Train: 0.4085, Test: 0.3923\n",
            "Early stopping:  0.09103584395592262\n",
            "Epoch: 008, Loss: 1.8800, Train: 0.4049, Test: 0.3861\n",
            "Early stopping:  0.09784736423030094\n",
            "Epoch: 009, Loss: 1.8196, Train: 0.4073, Test: 0.3906\n",
            "Early stopping:  0.09390376143738215\n",
            "Epoch: 010, Loss: 1.7629, Train: 0.4280, Test: 0.4133\n",
            "Early stopping:  0.09218221882143925\n",
            "Epoch: 011, Loss: 1.7055, Train: 0.4591, Test: 0.4439\n",
            "Early stopping:  0.09236566102097953\n",
            "Epoch: 012, Loss: 1.6442, Train: 0.4833, Test: 0.4694\n",
            "Early stopping:  0.09262397363540534\n",
            "Epoch: 013, Loss: 1.5881, Train: 0.4977, Test: 0.4807\n",
            "Early stopping:  0.09199357647274538\n",
            "Epoch: 014, Loss: 1.5349, Train: 0.5133, Test: 0.5017\n",
            "Early stopping:  0.0906955389452353\n",
            "Epoch: 015, Loss: 1.4754, Train: 0.5307, Test: 0.5210\n",
            "Early stopping:  0.09005533372746236\n",
            "Epoch: 016, Loss: 1.4221, Train: 0.5548, Test: 0.5402\n",
            "Early stopping:  0.08804483060823849\n",
            "Epoch: 017, Loss: 1.3777, Train: 0.5762, Test: 0.5561\n",
            "Early stopping:  0.08443937684537806\n",
            "Epoch: 018, Loss: 1.3329, Train: 0.5883, Test: 0.5675\n",
            "Early stopping:  0.0794931809484022\n",
            "Epoch: 019, Loss: 1.2918, Train: 0.5978, Test: 0.5680\n",
            "Early stopping:  0.07226644834441365\n",
            "Epoch: 020, Loss: 1.2551, Train: 0.6041, Test: 0.5760\n",
            "Early stopping:  0.06646012485993116\n",
            "Epoch: 021, Loss: 1.2223, Train: 0.6088, Test: 0.5777\n",
            "Early stopping:  0.061576508181653014\n",
            "Epoch: 022, Loss: 1.1957, Train: 0.6192, Test: 0.5884\n",
            "Early stopping:  0.05454268471611777\n",
            "Epoch: 023, Loss: 1.1676, Train: 0.6299, Test: 0.5947\n",
            "Early stopping:  0.048773524602767744\n",
            "Epoch: 024, Loss: 1.1412, Train: 0.6385, Test: 0.6083\n",
            "Early stopping:  0.04471575123415544\n",
            "Epoch: 025, Loss: 1.1193, Train: 0.6461, Test: 0.6122\n",
            "Early stopping:  0.04122085441005601\n",
            "Epoch: 026, Loss: 1.0954, Train: 0.6498, Test: 0.6202\n",
            "Early stopping:  0.039414258466006255\n",
            "Epoch: 027, Loss: 1.0730, Train: 0.6565, Test: 0.6247\n",
            "Early stopping:  0.03716895260463201\n",
            "Epoch: 028, Loss: 1.0507, Train: 0.6633, Test: 0.6321\n",
            "Early stopping:  0.03594298945568555\n",
            "Epoch: 029, Loss: 1.0304, Train: 0.6725, Test: 0.6451\n",
            "Early stopping:  0.035206327595851405\n",
            "Epoch: 030, Loss: 1.0099, Train: 0.6820, Test: 0.6525\n",
            "Early stopping:  0.03379242040917968\n",
            "Epoch: 031, Loss: 0.9901, Train: 0.6923, Test: 0.6565\n",
            "Early stopping:  0.03268091701139349\n",
            "Epoch: 032, Loss: 0.9729, Train: 0.7018, Test: 0.6599\n",
            "Early stopping:  0.03098681618437519\n",
            "Epoch: 033, Loss: 0.9550, Train: 0.7076, Test: 0.6672\n",
            "Early stopping:  0.02969465535165111\n",
            "Epoch: 034, Loss: 0.9379, Train: 0.7106, Test: 0.6701\n",
            "Early stopping:  0.028316660190786887\n",
            "Epoch: 035, Loss: 0.9216, Train: 0.7179, Test: 0.6735\n",
            "Early stopping:  0.02719965316741962\n",
            "Epoch: 036, Loss: 0.9065, Train: 0.7274, Test: 0.6740\n",
            "Early stopping:  0.026296700795678134\n",
            "Epoch: 037, Loss: 0.8906, Train: 0.7363, Test: 0.6740\n",
            "Early stopping:  0.025337568475474005\n",
            "Epoch: 038, Loss: 0.8759, Train: 0.7403, Test: 0.6757\n",
            "Early stopping:  0.024483346785203468\n",
            "Epoch: 039, Loss: 0.8613, Train: 0.7451, Test: 0.6780\n",
            "Early stopping:  0.023875796602220917\n",
            "Epoch: 040, Loss: 0.8467, Train: 0.7546, Test: 0.6808\n",
            "Early stopping:  0.023543043463085802\n",
            "Epoch: 041, Loss: 0.8325, Train: 0.7597, Test: 0.6882\n",
            "Early stopping:  0.023004019330764187\n",
            "Epoch: 042, Loss: 0.8184, Train: 0.7648, Test: 0.6984\n",
            "Early stopping:  0.0227581971267966\n",
            "Epoch: 043, Loss: 0.8044, Train: 0.7692, Test: 0.6973\n",
            "Early stopping:  0.022470291873360818\n",
            "Epoch: 044, Loss: 0.7903, Train: 0.7736, Test: 0.7029\n",
            "Early stopping:  0.02226670451478309\n",
            "Epoch: 045, Loss: 0.7762, Train: 0.7761, Test: 0.7069\n",
            "Early stopping:  0.02226402324336616\n",
            "Epoch: 046, Loss: 0.7621, Train: 0.7812, Test: 0.7080\n",
            "Early stopping:  0.022288374418095402\n",
            "Epoch: 047, Loss: 0.7480, Train: 0.7866, Test: 0.7075\n",
            "Early stopping:  0.0223117167227012\n",
            "Epoch: 048, Loss: 0.7342, Train: 0.7907, Test: 0.7098\n",
            "Early stopping:  0.022192618788747932\n",
            "Epoch: 049, Loss: 0.7206, Train: 0.7933, Test: 0.7126\n",
            "Early stopping:  0.021966062658526964\n",
            "Epoch: 050, Loss: 0.7071, Train: 0.7990, Test: 0.7160\n",
            "Early stopping:  0.021723661860565784\n",
            "Epoch: 051, Loss: 0.6939, Train: 0.8021, Test: 0.7143\n",
            "Early stopping:  0.02140285223653773\n",
            "Epoch: 052, Loss: 0.6805, Train: 0.8073, Test: 0.7177\n",
            "Early stopping:  0.02119091024128111\n",
            "Epoch: 053, Loss: 0.6673, Train: 0.8131, Test: 0.7211\n",
            "Early stopping:  0.02104445411753413\n",
            "Epoch: 054, Loss: 0.6543, Train: 0.8165, Test: 0.7217\n",
            "Early stopping:  0.02088870611050932\n",
            "Epoch: 055, Loss: 0.6412, Train: 0.8216, Test: 0.7251\n",
            "Early stopping:  0.020809393675515143\n",
            "Epoch: 056, Loss: 0.6282, Train: 0.8269, Test: 0.7273\n",
            "Early stopping:  0.02066620043957997\n",
            "Epoch: 057, Loss: 0.6155, Train: 0.8301, Test: 0.7307\n",
            "Early stopping:  0.020513248967356148\n",
            "Epoch: 058, Loss: 0.6028, Train: 0.8354, Test: 0.7313\n",
            "Early stopping:  0.020352251663584287\n",
            "Epoch: 059, Loss: 0.5904, Train: 0.8406, Test: 0.7358\n",
            "Early stopping:  0.020110907152583624\n",
            "Epoch: 060, Loss: 0.5780, Train: 0.8429, Test: 0.7387\n",
            "Early stopping:  0.019856603215190578\n",
            "Epoch: 061, Loss: 0.5662, Train: 0.8499, Test: 0.7392\n",
            "Early stopping:  0.019493570312051928\n",
            "Epoch: 062, Loss: 0.5560, Train: 0.8408, Test: 0.7415\n",
            "Early stopping:  0.018634829178762566\n",
            "Epoch: 063, Loss: 0.5531, Train: 0.8476, Test: 0.7455\n",
            "Early stopping:  0.015554305877556544\n",
            "Epoch: 064, Loss: 0.5599, Train: 0.8404, Test: 0.7398\n",
            "Early stopping:  0.009907668119901975\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.78      0.65      0.71       190\n",
            "         capital_goods       0.80      0.57      0.67       127\n",
            "conglomerates_industry       0.92      0.55      0.69        20\n",
            "     consumer_cyclical       0.75      0.68      0.71       198\n",
            " consumer_non-cyclical       0.79      0.56      0.66       112\n",
            "                energy       0.96      0.70      0.81        71\n",
            "             financial       0.81      0.72      0.76       192\n",
            "            healthcare       0.88      0.75      0.81        79\n",
            "              services       0.65      0.90      0.75       519\n",
            "            technology       0.73      0.65      0.68        99\n",
            "        transportation       0.85      0.80      0.83       101\n",
            "             utilities       0.84      0.75      0.79        56\n",
            "\n",
            "              accuracy                           0.74      1764\n",
            "             macro avg       0.81      0.69      0.74      1764\n",
            "          weighted avg       0.76      0.74      0.74      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4776, Train: 0.2945, Test: 0.2948\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2382, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.16926224938193257\n",
            "Epoch: 003, Loss: 2.1853, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1557415388251337\n",
            "Epoch: 004, Loss: 2.1356, Train: 0.3028, Test: 0.3027\n",
            "Early stopping:  0.1515303276468036\n",
            "Epoch: 005, Loss: 2.0584, Train: 0.3495, Test: 0.3435\n",
            "Early stopping:  0.15900341872307033\n",
            "Epoch: 006, Loss: 2.0014, Train: 0.3797, Test: 0.3724\n",
            "Early stopping:  0.09521319650485666\n",
            "Epoch: 007, Loss: 1.9478, Train: 0.3847, Test: 0.3764\n",
            "Early stopping:  0.09652414585747084\n",
            "Epoch: 008, Loss: 1.8864, Train: 0.3864, Test: 0.3753\n",
            "Early stopping:  0.09649542051326403\n",
            "Epoch: 009, Loss: 1.8222, Train: 0.4056, Test: 0.3934\n",
            "Early stopping:  0.09292896150716605\n",
            "Epoch: 010, Loss: 1.7594, Train: 0.4394, Test: 0.4212\n",
            "Early stopping:  0.09643273342222446\n",
            "Epoch: 011, Loss: 1.6960, Train: 0.4693, Test: 0.4512\n",
            "Early stopping:  0.09969122295930129\n",
            "Epoch: 012, Loss: 1.6357, Train: 0.4860, Test: 0.4615\n",
            "Early stopping:  0.099248059620256\n",
            "Epoch: 013, Loss: 1.5794, Train: 0.5023, Test: 0.4819\n",
            "Early stopping:  0.09638289731699048\n",
            "Epoch: 014, Loss: 1.5206, Train: 0.5209, Test: 0.5113\n",
            "Early stopping:  0.09399208361823824\n",
            "Epoch: 015, Loss: 1.4640, Train: 0.5409, Test: 0.5278\n",
            "Early stopping:  0.09156426796875763\n",
            "Epoch: 016, Loss: 1.4157, Train: 0.5581, Test: 0.5471\n",
            "Early stopping:  0.08785536913848417\n",
            "Epoch: 017, Loss: 1.3687, Train: 0.5776, Test: 0.5624\n",
            "Early stopping:  0.08333318539646409\n",
            "Epoch: 018, Loss: 1.3234, Train: 0.5894, Test: 0.5646\n",
            "Early stopping:  0.07749200042347092\n",
            "Epoch: 019, Loss: 1.2852, Train: 0.5978, Test: 0.5737\n",
            "Early stopping:  0.07121074946019953\n",
            "Epoch: 020, Loss: 1.2497, Train: 0.6064, Test: 0.5884\n",
            "Early stopping:  0.0658048207367429\n",
            "Epoch: 021, Loss: 1.2166, Train: 0.6149, Test: 0.6003\n",
            "Early stopping:  0.05987527033673887\n",
            "Epoch: 022, Loss: 1.1874, Train: 0.6281, Test: 0.6077\n",
            "Early stopping:  0.05392355799308828\n",
            "Epoch: 023, Loss: 1.1587, Train: 0.6373, Test: 0.6145\n",
            "Early stopping:  0.04989880617032834\n",
            "Epoch: 024, Loss: 1.1331, Train: 0.6430, Test: 0.6168\n",
            "Early stopping:  0.046071336606721\n",
            "Epoch: 025, Loss: 1.1082, Train: 0.6521, Test: 0.6179\n",
            "Early stopping:  0.04287319143486936\n",
            "Epoch: 026, Loss: 1.0833, Train: 0.6611, Test: 0.6236\n",
            "Early stopping:  0.04092506077450302\n",
            "Epoch: 027, Loss: 1.0611, Train: 0.6689, Test: 0.6315\n",
            "Early stopping:  0.03875593742785939\n",
            "Epoch: 028, Loss: 1.0393, Train: 0.6760, Test: 0.6332\n",
            "Early stopping:  0.03714948590126564\n",
            "Epoch: 029, Loss: 1.0187, Train: 0.6816, Test: 0.6395\n",
            "Early stopping:  0.0352932543223183\n",
            "Epoch: 030, Loss: 0.9982, Train: 0.6867, Test: 0.6451\n",
            "Early stopping:  0.0336190177428939\n",
            "Epoch: 031, Loss: 0.9789, Train: 0.6950, Test: 0.6542\n",
            "Early stopping:  0.03249259383477939\n",
            "Epoch: 032, Loss: 0.9600, Train: 0.7017, Test: 0.6542\n",
            "Early stopping:  0.03137213084689147\n",
            "Epoch: 033, Loss: 0.9413, Train: 0.7120, Test: 0.6582\n",
            "Early stopping:  0.03054539671809759\n",
            "Epoch: 034, Loss: 0.9232, Train: 0.7171, Test: 0.6655\n",
            "Early stopping:  0.029672526111587143\n",
            "Epoch: 035, Loss: 0.9066, Train: 0.7245, Test: 0.6672\n",
            "Early stopping:  0.028676436827946707\n",
            "Epoch: 036, Loss: 0.8897, Train: 0.7322, Test: 0.6667\n",
            "Early stopping:  0.027697865318686614\n",
            "Epoch: 037, Loss: 0.8742, Train: 0.7400, Test: 0.6729\n",
            "Early stopping:  0.026519882245488305\n",
            "Epoch: 038, Loss: 0.8584, Train: 0.7471, Test: 0.6763\n",
            "Early stopping:  0.025630627810808448\n",
            "Epoch: 039, Loss: 0.8436, Train: 0.7557, Test: 0.6842\n",
            "Early stopping:  0.024883092886373224\n",
            "Epoch: 040, Loss: 0.8283, Train: 0.7618, Test: 0.6899\n",
            "Early stopping:  0.024264299096347024\n",
            "Epoch: 041, Loss: 0.8139, Train: 0.7672, Test: 0.6933\n",
            "Early stopping:  0.023819742601930205\n",
            "Epoch: 042, Loss: 0.7992, Train: 0.7714, Test: 0.6984\n",
            "Early stopping:  0.02341750009672966\n",
            "Epoch: 043, Loss: 0.7850, Train: 0.7777, Test: 0.7029\n",
            "Early stopping:  0.023124246649362582\n",
            "Epoch: 044, Loss: 0.7708, Train: 0.7812, Test: 0.7035\n",
            "Early stopping:  0.022741417220641654\n",
            "Epoch: 045, Loss: 0.7569, Train: 0.7848, Test: 0.7086\n",
            "Early stopping:  0.022526282195747493\n",
            "Epoch: 046, Loss: 0.7430, Train: 0.7902, Test: 0.7109\n",
            "Early stopping:  0.02222333290789979\n",
            "Epoch: 047, Loss: 0.7291, Train: 0.7947, Test: 0.7120\n",
            "Early stopping:  0.02209229181581572\n",
            "Epoch: 048, Loss: 0.7154, Train: 0.7984, Test: 0.7126\n",
            "Early stopping:  0.021914081867993877\n",
            "Epoch: 049, Loss: 0.7017, Train: 0.8036, Test: 0.7143\n",
            "Early stopping:  0.021820770491257795\n",
            "Epoch: 050, Loss: 0.6885, Train: 0.8021, Test: 0.7217\n",
            "Early stopping:  0.021574560286032437\n",
            "Epoch: 051, Loss: 0.6780, Train: 0.8065, Test: 0.7154\n",
            "Early stopping:  0.020422649044694447\n",
            "Epoch: 052, Loss: 0.6713, Train: 0.8033, Test: 0.7234\n",
            "Early stopping:  0.01783463209855768\n",
            "Epoch: 053, Loss: 0.6684, Train: 0.8192, Test: 0.7268\n",
            "Early stopping:  0.013631273780907796\n",
            "Epoch: 054, Loss: 0.6435, Train: 0.8238, Test: 0.7279\n",
            "Early stopping:  0.01666634419600768\n",
            "Epoch: 055, Loss: 0.6310, Train: 0.8188, Test: 0.7347\n",
            "Early stopping:  0.020153439789814324\n",
            "Epoch: 056, Loss: 0.6310, Train: 0.8314, Test: 0.7381\n",
            "Early stopping:  0.019708613642507342\n",
            "Epoch: 057, Loss: 0.6094, Train: 0.8310, Test: 0.7296\n",
            "Early stopping:  0.021581139803776822\n",
            "Epoch: 058, Loss: 0.6026, Train: 0.8333, Test: 0.7449\n",
            "Early stopping:  0.016958903611939205\n",
            "Epoch: 059, Loss: 0.5979, Train: 0.8413, Test: 0.7443\n",
            "Early stopping:  0.015709114214998512\n",
            "Epoch: 060, Loss: 0.5777, Train: 0.8374, Test: 0.7358\n",
            "Early stopping:  0.019300106771530146\n",
            "Epoch: 061, Loss: 0.5772, Train: 0.8426, Test: 0.7523\n",
            "Early stopping:  0.014754886946541073\n",
            "Epoch: 062, Loss: 0.5650, Train: 0.8513, Test: 0.7483\n",
            "Early stopping:  0.015720224604539187\n",
            "Epoch: 063, Loss: 0.5506, Train: 0.8484, Test: 0.7415\n",
            "Early stopping:  0.017501226284230946\n",
            "Epoch: 064, Loss: 0.5502, Train: 0.8562, Test: 0.7534\n",
            "Early stopping:  0.013514415889381487\n",
            "Epoch: 065, Loss: 0.5338, Train: 0.8584, Test: 0.7568\n",
            "Early stopping:  0.01643529656089221\n",
            "Epoch: 066, Loss: 0.5262, Train: 0.8620, Test: 0.7460\n",
            "Early stopping:  0.015301092768342774\n",
            "Epoch: 067, Loss: 0.5216, Train: 0.8679, Test: 0.7568\n",
            "Early stopping:  0.01345734315562511\n",
            "Epoch: 068, Loss: 0.5064, Train: 0.8649, Test: 0.7608\n",
            "Early stopping:  0.016112519868277404\n",
            "Epoch: 069, Loss: 0.5024, Train: 0.8740, Test: 0.7579\n",
            "Early stopping:  0.013293266012760607\n",
            "Epoch: 070, Loss: 0.4964, Train: 0.8660, Test: 0.7562\n",
            "Early stopping:  0.01272688660544853\n",
            "Epoch: 071, Loss: 0.4921, Train: 0.8720, Test: 0.7540\n",
            "Early stopping:  0.01135958711961635\n",
            "Epoch: 072, Loss: 0.4989, Train: 0.8664, Test: 0.7540\n",
            "Early stopping:  0.005497787691022074\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.83      0.68      0.75       190\n",
            "         capital_goods       0.78      0.63      0.70       127\n",
            "conglomerates_industry       0.93      0.65      0.76        20\n",
            "     consumer_cyclical       0.76      0.76      0.76       198\n",
            " consumer_non-cyclical       0.80      0.60      0.68       112\n",
            "                energy       0.88      0.70      0.78        71\n",
            "             financial       0.81      0.72      0.77       192\n",
            "            healthcare       0.84      0.80      0.82        79\n",
            "              services       0.67      0.87      0.76       519\n",
            "            technology       0.76      0.66      0.71        99\n",
            "        transportation       0.86      0.78      0.82       101\n",
            "             utilities       0.84      0.73      0.78        56\n",
            "\n",
            "              accuracy                           0.75      1764\n",
            "             macro avg       0.81      0.72      0.76      1764\n",
            "          weighted avg       0.77      0.75      0.75      1764\n",
            "\n",
            "time: 2min 22s (started: 2024-10-16 21:30:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK2yQZgHgSl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02695dea-01da-4bc5-8de9-3cc2ad942362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 466 ms (started: 2024-10-16 21:33:19 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXXZu4w_dcAv"
      },
      "source": [
        "--------------------------------------\n",
        "# Hetrogenous Graph for Keyphrase = 2 or 3\n",
        "--------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l4qyKl5dcBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28cf4f28-1e13-4874-a6c7-aa27a7e6fd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basic_materials' 'capital_goods' 'conglomerates_industry'\n",
            " 'consumer_cyclical' 'consumer_non-cyclical' 'energy' 'financial'\n",
            " 'healthcare' 'services' 'technology' 'transportation' 'utilities']\n",
            "12\n",
            "time: 2.91 ms (started: 2024-10-16 21:33:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(df[\"class\"].unique())\n",
        "class_number = len(df[\"class\"].unique())\n",
        "print(class_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uig08WAHdcBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2faf510-3824-4516-f8d9-33afb23ece01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 467 µs (started: 2024-10-16 21:33:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Change here to change wich keypharse to use\n",
        "keyphrase = \"keyphrase23\"\n",
        "\n",
        "model_name = dataset_name+\"_\"+keyphrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oywnn6k1ezSj"
      },
      "source": [
        "## Creating the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gvup-2jezSk"
      },
      "source": [
        "### Defining Graph Nodes and Edges 👀\n",
        "\n",
        "- `Nodes` - documents and contexts\n",
        "- `Edges`\n",
        "  - document <- has -> context\n",
        "- `Labels` - documents classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bwipeK4ezSl"
      },
      "source": [
        "#### Nodes and Edges 👀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUbeMm8SezSm"
      },
      "source": [
        "##### Defining Docmuent nodes, Context nodes and edges between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5GOHYWPezSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc073cd-ec6b-4efa-c425-1e4773fa3ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18.1 s (started: 2024-10-16 21:33:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "all_contexts_list =[]\n",
        "edges1,edges2 = [],[]\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes = []\n",
        "context_nodes = []\n",
        "\n",
        "edges_tuple = []\n",
        "\n",
        "sentences = []\n",
        "cont_sentences = 0\n",
        "dit_sentences = {}\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding document nodes for every doc in df\n",
        "    document_nodes.append(df[\"text_embeddings\"][i])\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list: # if NOT\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes.append(df[keyphrase+\"_embeddings\"][i][j])\n",
        "            # add a new edge between doc and new context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(new_edge_cont)\n",
        "            edges_tuple.append((df[\"id\"][i],new_edge_cont))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges1.append(df[\"id\"][i])\n",
        "            edges2.append(all_contexts_list.index(key[0]))\n",
        "            edges_tuple.append((df[\"id\"][i],all_contexts_list.index(key[0])))\n",
        "            cont+=1\n",
        "\n",
        "    # organize sentences, sentences_embeddings, and a dict with the corresponding document for each sentence\n",
        "    aux = df['sentences_embeddings'][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        sentences.append(key)\n",
        "        dit_sentences[cont_sentences] = df[\"id\"][i]\n",
        "        cont_sentences += 1\n",
        "\n",
        "\n",
        "document_nodes = np.array(document_nodes)\n",
        "context_nodes = np.array(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGzYkYUTezSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6593fdad-b4b7-44fb-d363-73b670ed0177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 8817\n",
            "number of context nodes: 35753\n",
            "number of shared contexts: 7655\n",
            "number of direct edges (first dimension): 43408\n",
            "number of direct edges (second dimension): 43408\n",
            "time: 901 µs (started: 2024-10-16 21:33:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"number of document nodes:\",len(document_nodes))\n",
        "print(\"number of context nodes:\",len(context_nodes))\n",
        "print(\"number of shared contexts:\",cont)\n",
        "print(\"number of direct edges (first dimension):\",len(edges1))\n",
        "print(\"number of direct edges (second dimension):\",len(edges2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlsR62oUezSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5723a7a-7388-4543-9f23-7cc570e91c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 2s (started: 2024-10-16 21:33:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nbrs = NearestNeighbors(n_neighbors=15, metric='cosine').fit(context_nodes)\n",
        "\n",
        "neighbors_list = nbrs.kneighbors(sentences, return_distance=False)\n",
        "\n",
        "# cria aresta para cada vizinho encontrado\n",
        "for i,neighbors in enumerate(neighbors_list):\n",
        "        for n in neighbors:\n",
        "            edges1.append(dit_sentences[i])\n",
        "            edges2.append(n)\n",
        "            edges_tuple.append((dit_sentences[i],n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmkD7gXqezSo"
      },
      "source": [
        "##### Ajusting everything to Tensor Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hAGiWQzezSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a16b469-0837-4643-f07f-8d54b5243548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 217 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# transforms egdes to tensor\n",
        "edges = np.array([edges1,edges2])\n",
        "edges = torch.tensor(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUrNmaRLezSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694032da-befe-4e50-f131-06f26bcc17f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 50.5 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# transforms nodes to tensor\n",
        "document_nodes = np.array(document_nodes)\n",
        "document_nodes = torch.tensor(document_nodes)\n",
        "context_nodes = np.array(context_nodes)\n",
        "context_nodes = torch.tensor(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dktQZftFezSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce704ca-32de-447f-fb44-c084f70f9809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.8602e-02,  1.4557e-02,  1.7552e-02,  ..., -1.8911e-02,\n",
              "         -1.8681e-02, -2.2412e-02],\n",
              "        [-1.2433e-01, -1.2993e-02,  1.4001e-02,  ..., -1.7944e-02,\n",
              "          1.1914e-02, -8.1062e-03],\n",
              "        [-8.5865e-02,  4.3530e-02, -7.1712e-02,  ...,  5.6326e-02,\n",
              "         -4.5610e-02,  3.1375e-02],\n",
              "        ...,\n",
              "        [-1.6584e-02, -3.9770e-02, -2.2835e-02,  ..., -4.2101e-05,\n",
              "          1.2257e-02,  2.9048e-02],\n",
              "        [ 3.1864e-02, -1.0648e-02, -3.8576e-02,  ..., -4.5610e-03,\n",
              "         -1.7202e-02,  4.2433e-02],\n",
              "        [ 4.3771e-02,  3.8128e-02,  2.0526e-02,  ..., -1.0110e-02,\n",
              "          3.8847e-02,  4.4935e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 324
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.52 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show documents nodes\n",
        "document_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhiFco_zezSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059ac432-47dd-476a-c595-42c29eea13ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 325
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.11 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of document nodes\n",
        "len(document_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDakiiWbezSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc01da09-95fc-4a42-9373-616dd85ebab3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0714,  0.0858, -0.0434,  ...,  0.0149,  0.0713,  0.0182],\n",
              "        [-0.0430,  0.0382,  0.0263,  ...,  0.0298,  0.0570, -0.0218],\n",
              "        [-0.0816,  0.0714, -0.0450,  ...,  0.0140,  0.0811,  0.0313],\n",
              "        ...,\n",
              "        [-0.0202,  0.0236, -0.0472,  ..., -0.0497,  0.0594,  0.0337],\n",
              "        [-0.0464,  0.0437,  0.0029,  ...,  0.0401,  0.0498,  0.0721],\n",
              "        [-0.0457, -0.0080, -0.0750,  ...,  0.0681, -0.0170,  0.0555]])"
            ]
          },
          "metadata": {},
          "execution_count": 326
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.71 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Show context nodes\n",
        "context_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW_aQW0JezSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f0b9af-4c93-4ce3-b6d7-892a0c559313"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35753"
            ]
          },
          "metadata": {},
          "execution_count": 327
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.07 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of context nodes\n",
        "len(context_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfBtcgy3ezSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eec3493-1138-4726-910e-c4af85dd10f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1947013\n",
            "1947013\n",
            "time: 1.01 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Quantity of edges\n",
        "print(len(edges[0]))\n",
        "print(len(edges[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXz-gsYvezSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2015a22a-9115-40d5-d0e9-3b79fec7b3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     0,     0,  ...,  8816,  8816,  8816],\n",
            "        [    0,     1,     2,  ..., 19486, 31355, 23119]])\n",
            "time: 1.4 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# showing edges\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPZtNmU3ezSr"
      },
      "source": [
        "#### Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt1SF8dRezSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c52dd3-0297-4f30-9872-b7cd1fa9ef90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['basic_materials' 'capital_goods' 'conglomerates_industry'\n",
            " 'consumer_cyclical' 'consumer_non-cyclical' 'energy' 'financial'\n",
            " 'healthcare' 'services' 'technology' 'transportation' 'utilities']\n",
            "time: 3.15 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# All different classes\n",
        "print(df[\"class\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0TJ12U1ezSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06dd2574-380c-412d-e0ed-a8616cadf77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'basic_materials': 0, 'capital_goods': 1, 'conglomerates_industry': 2, 'consumer_cyclical': 3, 'consumer_non-cyclical': 4, 'energy': 5, 'financial': 6, 'healthcare': 7, 'services': 8, 'technology': 9, 'transportation': 10, 'utilities': 11} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-331-a018e46c97dd>:12: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
            "<ipython-input-331-a018e46c97dd>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  labels[\"class\"][i] = dit[labels[\"class\"][i]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class\n",
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fa9353e-e0f1-4e10-bd41-4a9ae286cda9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fa9353e-e0f1-4e10-bd41-4a9ae286cda9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2fa9353e-e0f1-4e10-bd41-4a9ae286cda9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2fa9353e-e0f1-4e10-bd41-4a9ae286cda9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ffab32f8-fdf2-4930-8d0c-d72ae9aa82d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ffab32f8-fdf2-4930-8d0c-d72ae9aa82d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ffab32f8-fdf2-4930-8d0c-d72ae9aa82d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labels",
              "summary": "{\n  \"name\": \"labels\",\n  \"rows\": 8817,\n  \"fields\": [\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10,\n          9,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 331
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 883 ms (started: 2024-10-16 21:35:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Creating labels for classification\n",
        "# the dictionary is a numeric representation for each possible \"document\" class\n",
        "\n",
        "dit = {}\n",
        "for i,classe in enumerate(df[\"class\"].unique()):\n",
        "  dit[classe] = i\n",
        "\n",
        "print(dit,'\\n')\n",
        "\n",
        "labels = df[[\"class\"]]\n",
        "for i in range(len(df[[\"class\"]])):\n",
        "    labels[\"class\"][i] = dit[labels[\"class\"][i]]\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK0Fa3e8ezSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbdf784-2f73-4889-fdeb-17ef57acd7f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  0,  0,  ..., 11, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 332
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.2 ms (started: 2024-10-16 21:35:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# tranfors class dataframe into tensor\n",
        "y = labels[\"class\"].tolist()\n",
        "y = x_np = torch.tensor(y)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWNve9W7ezSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24199707-620e-49e9-b81c-8d625c168b69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8817"
            ]
          },
          "metadata": {},
          "execution_count": 333
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.3 ms (started: 2024-10-16 21:35:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzuE-CP9ezSt"
      },
      "source": [
        "### Testing Graph with Networkx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhPvsB_-ezSu"
      },
      "source": [
        "#### Defining overal graph in networkx maner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5BG-1ZWezSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa96ff7-9100-466e-a782-2957d7c0b91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14.4 s (started: 2024-10-16 21:35:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Run graph Representation for networkx\n",
        "all_contexts_list =[]\n",
        "edges_test = []\n",
        "cont = 0\n",
        "new_edge_cont = 0\n",
        "\n",
        "document_nodes_test = []\n",
        "context_nodes_test = []\n",
        "\n",
        "# Creating graph nodes and edges\n",
        "for i in range(len(df)):\n",
        "    # adding new documents for every node\n",
        "    document_nodes_test.append(\"doc_\"+str(i)) # in the actual graph nodes -> documents embeddings\n",
        "\n",
        "    # adding context node and edges:\n",
        "    aux = df[keyphrase][i]\n",
        "    for j,key in enumerate(aux):\n",
        "        # testing if keyphrase was already detected in df\n",
        "        if key[0] not in all_contexts_list:\n",
        "            # add a new context node for every new keyphrase in df\n",
        "            context_nodes_test.append(\"contx_\"+str(new_edge_cont)) # in the actual graph nodes -> context embeddings\n",
        "\n",
        "            # add a new edge between doc and new context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(new_edge_cont)))\n",
        "            # update existing contexts list\n",
        "            all_contexts_list.append(key[0])\n",
        "            new_edge_cont += 1\n",
        "        else: # if YES\n",
        "            # we do NOT add context node and (already exists)\n",
        "            # add a new edge between doc and old context\n",
        "            edges_test.append((\"doc_\"+str(df[\"id\"][i]),\"contx_\"+str(all_contexts_list.index(key[0]))))\n",
        "            cont+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYwDFDqtezSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3c5916-6f1c-48fd-ae3f-264a8cfb440b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.28 s (started: 2024-10-16 21:35:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "edges_test = [(\"doc_\"+str(i[0]),\"contx_\"+str(i[1])) for i in edges_tuple]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsJjDZi6ezSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5304b251-7265-4567-da0a-27ff9e8f20d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of document nodes: 8817\n",
            "number of context nodes: 35753\n",
            "number of edges: 1947013\n",
            "time: 854 µs (started: 2024-10-16 21:35:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(\"number of document nodes:\",len(document_nodes_test))\n",
        "print(\"number of context nodes:\",len(context_nodes_test))\n",
        "print(\"number of edges:\",len(edges_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trPJcUUIezSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad92283-c784-417d-e806-bc0a33ba5d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('doc_0', 'contx_0'), ('doc_0', 'contx_1'), ('doc_0', 'contx_2'), ('doc_0', 'contx_3'), ('doc_0', 'contx_4'), ('doc_1', 'contx_5'), ('doc_1', 'contx_6'), ('doc_1', 'contx_7'), ('doc_1', 'contx_8'), ('doc_1', 'contx_9'), ('doc_2', 'contx_10'), ('doc_2', 'contx_11'), ('doc_2', 'contx_12'), ('doc_2', 'contx_13'), ('doc_2', 'contx_14'), ('doc_3', 'contx_15'), ('doc_3', 'contx_16'), ('doc_3', 'contx_17'), ('doc_3', 'contx_18'), ('doc_3', 'contx_19'), ('doc_4', 'contx_20'), ('doc_4', 'contx_21'), ('doc_4', 'contx_22'), ('doc_4', 'contx_23'), ('doc_4', 'contx_24'), ('doc_5', 'contx_25'), ('doc_5', 'contx_26'), ('doc_5', 'contx_27'), ('doc_5', 'contx_28'), ('doc_5', 'contx_29'), ('doc_6', 'contx_30'), ('doc_6', 'contx_31'), ('doc_6', 'contx_32'), ('doc_6', 'contx_33'), ('doc_6', 'contx_34'), ('doc_7', 'contx_35'), ('doc_7', 'contx_36'), ('doc_7', 'contx_37'), ('doc_7', 'contx_38'), ('doc_7', 'contx_39'), ('doc_8', 'contx_40'), ('doc_8', 'contx_41'), ('doc_8', 'contx_42'), ('doc_8', 'contx_43'), ('doc_8', 'contx_44'), ('doc_9', 'contx_45'), ('doc_9', 'contx_46'), ('doc_9', 'contx_47'), ('doc_9', 'contx_48'), ('doc_9', 'contx_49'), ('doc_10', 'contx_50'), ('doc_10', 'contx_51'), ('doc_10', 'contx_52'), ('doc_10', 'contx_53'), ('doc_10', 'contx_54'), ('doc_11', 'contx_55'), ('doc_11', 'contx_56'), ('doc_11', 'contx_57'), ('doc_11', 'contx_58'), ('doc_11', 'contx_59'), ('doc_12', 'contx_60'), ('doc_12', 'contx_61'), ('doc_12', 'contx_62'), ('doc_12', 'contx_63'), ('doc_12', 'contx_64'), ('doc_13', 'contx_65'), ('doc_13', 'contx_66'), ('doc_13', 'contx_67'), ('doc_13', 'contx_68'), ('doc_13', 'contx_69'), ('doc_14', 'contx_70'), ('doc_14', 'contx_71'), ('doc_14', 'contx_72'), ('doc_14', 'contx_73'), ('doc_14', 'contx_74'), ('doc_15', 'contx_75'), ('doc_15', 'contx_76'), ('doc_15', 'contx_77'), ('doc_15', 'contx_78'), ('doc_15', 'contx_79'), ('doc_16', 'contx_80'), ('doc_16', 'contx_81'), ('doc_16', 'contx_82'), ('doc_16', 'contx_83'), ('doc_16', 'contx_84'), ('doc_17', 'contx_85'), ('doc_17', 'contx_86'), ('doc_17', 'contx_87'), ('doc_17', 'contx_88'), ('doc_17', 'contx_89'), ('doc_18', 'contx_90'), ('doc_18', 'contx_91'), ('doc_18', 'contx_92'), ('doc_18', 'contx_93'), ('doc_18', 'contx_94'), ('doc_19', 'contx_95'), ('doc_19', 'contx_96'), ('doc_19', 'contx_97'), ('doc_19', 'contx_98'), ('doc_19', 'contx_99')]\n",
            "time: 644 µs (started: 2024-10-16 21:35:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(edges_test[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvu3pgTlezSw"
      },
      "source": [
        "#### Test graph Conectivity with networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQGjtv2vezSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f0e49f-28db-4e18-a4ce-7790c1266b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.49 s (started: 2024-10-16 21:35:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define Bipartide graph\n",
        "B = nx.Graph()\n",
        "B.add_nodes_from(document_nodes_test, bipartite=0)\n",
        "B.add_nodes_from(context_nodes_test, bipartite=1)\n",
        "B.add_edges_from(edges_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGT1IC92ezSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8eeae81-a388-475d-8096-4f87402cdbd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 339
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 555 ms (started: 2024-10-16 21:36:01 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nx.is_connected(B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhuaVnMTezSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56d859d-736c-4325-a460-b98c7c9b75c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "time: 568 ms (started: 2024-10-16 21:36:01 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Number of conected elements\n",
        "# if == 1 -> all elements of the graph are conected\n",
        "print(nx.number_connected_components(B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_xYJY5iezSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344858e0-a03f-4efc-8310-73f534ea75b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44561\n",
            "time: 560 ms (started: 2024-10-16 21:36:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# size of a cluster\n",
        "print(len(nx.node_connected_component(B,'doc_0')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psEyYySbezSy"
      },
      "source": [
        "### Creating Graph HeteroData Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdfHMacMezSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8303530f-4994-46ba-dd53-e959e2d67539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 801 µs (started: 2024-10-16 21:36:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Defining nodes, edges and class labels\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# nodes\n",
        "data['document'].x = document_nodes\n",
        "data['concept'].x = context_nodes\n",
        "\n",
        "# edges\n",
        "data['document', 'has', 'concept'].edge_index = edges\n",
        "\n",
        "#class labels\n",
        "data['document'].y = y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz5-5BRBezSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88a667e-6066-4bc3-e40a-ff2e6d1b38f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 11.6 ms (started: 2024-10-16 21:36:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Setting graph to undirected\n",
        "data = T.ToUndirected()(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzsvd0YwezSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "733bbb66-6252-4a7e-f1a2-06d8ee0de358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 528 ms (started: 2024-10-16 21:36:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# removing duplicate edges\n",
        "data = T.RemoveDuplicatedEdges()(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmgI63i8ezSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6361bb90-58c3-422c-9b49-59782eeab4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 25.2 ms (started: 2024-10-16 21:36:03 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Ensure date in using gpu\n",
        "data = data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ypz15KezSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e4ecd8-e87b-494f-d7f0-f546be044a0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  document={\n",
              "    x=[8817, 384],\n",
              "    y=[8817],\n",
              "  },\n",
              "  concept={ x=[35753, 384] },\n",
              "  (document, has, concept)={ edge_index=[2, 1094600] },\n",
              "  (concept, rev_has, document)={ edge_index=[2, 1094600] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 346
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.24 ms (started: 2024-10-16 21:36:03 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA-0p8YQgWjK"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1EOhFm1gWjL"
      },
      "source": [
        "### Training rotulated base = 20% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvcuykjKgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340a1ab9-b346-408c-9354-fa312e6f55fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 315 µs (started: 2024-10-16 21:36:03 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1pgo3yRgWjL"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awoAHzaogWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed88bcc-81b4-4790-8199-a5e229d16a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 41.5275, Train: 0.1151, Test: 0.1110\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 343.6941, Train: 0.2938, Test: 0.2872\n",
            "Early stopping:  213.66409725026958\n",
            "Epoch: 003, Loss: 260.1060, Train: 0.0766, Test: 0.0747\n",
            "Early stopping:  156.02791371831276\n",
            "Epoch: 004, Loss: 415.5328, Train: 0.1095, Test: 0.1109\n",
            "Early stopping:  162.0870479137794\n",
            "Epoch: 005, Loss: 382.2715, Train: 0.0584, Test: 0.0608\n",
            "Early stopping:  149.81521062715237\n",
            "Epoch: 006, Loss: 340.3053, Train: 0.0510, Test: 0.0495\n",
            "Early stopping:  58.175556200698445\n",
            "Epoch: 007, Loss: 253.0544, Train: 0.0664, Test: 0.0671\n",
            "Early stopping:  72.38756780212263\n",
            "Epoch: 008, Loss: 195.6918, Train: 0.2138, Test: 0.2193\n",
            "Early stopping:  91.26369758152003\n",
            "Epoch: 009, Loss: 146.9955, Train: 0.0976, Test: 0.0920\n",
            "Early stopping:  97.83397852125599\n",
            "Epoch: 010, Loss: 153.1552, Train: 0.1934, Test: 0.1868\n",
            "Early stopping:  80.51616506109869\n",
            "Epoch: 011, Loss: 119.6956, Train: 0.3199, Test: 0.3208\n",
            "Early stopping:  52.03952529732797\n",
            "Epoch: 012, Loss: 98.4284, Train: 0.2864, Test: 0.2753\n",
            "Early stopping:  36.82738411814141\n",
            "Epoch: 013, Loss: 75.9240, Train: 0.1526, Test: 0.1447\n",
            "Early stopping:  32.51696858293793\n",
            "Epoch: 014, Loss: 54.3982, Train: 0.1202, Test: 0.1194\n",
            "Early stopping:  38.32272487795278\n",
            "Epoch: 015, Loss: 33.9464, Train: 0.1838, Test: 0.1687\n",
            "Early stopping:  34.082050883098304\n",
            "Epoch: 016, Loss: 13.2236, Train: 0.2240, Test: 0.2175\n",
            "Early stopping:  33.58777884873156\n",
            "Epoch: 017, Loss: 3.7241, Train: 0.2093, Test: 0.2067\n",
            "Early stopping:  29.56703309816303\n",
            "Epoch: 018, Loss: 2.9054, Train: 0.2087, Test: 0.1939\n",
            "Early stopping:  22.17740978523498\n",
            "Epoch: 019, Loss: 2.8228, Train: 0.2206, Test: 0.2005\n",
            "Early stopping:  13.38172729166719\n",
            "Epoch: 020, Loss: 2.7539, Train: 0.2195, Test: 0.2039\n",
            "Early stopping:  4.565923149841935\n",
            "Epoch: 021, Loss: 2.6632, Train: 0.2240, Test: 0.2172\n",
            "Early stopping:  0.4287268275915431\n",
            "Epoch: 022, Loss: 2.5727, Train: 0.2275, Test: 0.2264\n",
            "Early stopping:  0.1306082712303029\n",
            "Epoch: 023, Loss: 2.5028, Train: 0.2331, Test: 0.2288\n",
            "Early stopping:  0.13004120317535608\n",
            "Epoch: 024, Loss: 2.4414, Train: 0.2473, Test: 0.2392\n",
            "Early stopping:  0.12466289669684011\n",
            "Epoch: 025, Loss: 2.3700, Train: 0.2626, Test: 0.2523\n",
            "Early stopping:  0.11375263290906025\n",
            "Epoch: 026, Loss: 2.2923, Train: 0.2734, Test: 0.2657\n",
            "Early stopping:  0.10972603735044563\n",
            "Epoch: 027, Loss: 2.2753, Train: 0.3307, Test: 0.3174\n",
            "Early stopping:  0.09685161176969737\n",
            "Epoch: 028, Loss: 2.2449, Train: 0.3426, Test: 0.3344\n",
            "Early stopping:  0.07989084227790604\n",
            "Epoch: 029, Loss: 2.1761, Train: 0.3681, Test: 0.3520\n",
            "Early stopping:  0.07065155043813295\n",
            "Epoch: 030, Loss: 2.1052, Train: 0.3772, Test: 0.3619\n",
            "Early stopping:  0.07748636694372577\n",
            "Epoch: 031, Loss: 2.0573, Train: 0.3789, Test: 0.3660\n",
            "Early stopping:  0.09169037142733216\n",
            "Epoch: 032, Loss: 2.0202, Train: 0.3863, Test: 0.3704\n",
            "Early stopping:  0.09061198691053896\n",
            "Epoch: 033, Loss: 1.9810, Train: 0.3925, Test: 0.3689\n",
            "Early stopping:  0.07587261742861659\n",
            "Epoch: 034, Loss: 1.9519, Train: 0.3891, Test: 0.3666\n",
            "Early stopping:  0.06074227286435706\n",
            "Epoch: 035, Loss: 1.9351, Train: 0.3993, Test: 0.3728\n",
            "Early stopping:  0.04996909097746091\n",
            "Epoch: 036, Loss: 1.9087, Train: 0.4050, Test: 0.3794\n",
            "Early stopping:  0.04298067912384076\n",
            "Epoch: 037, Loss: 1.8826, Train: 0.4197, Test: 0.3823\n",
            "Early stopping:  0.03804752918594001\n",
            "Epoch: 038, Loss: 1.8604, Train: 0.4209, Test: 0.3856\n",
            "Early stopping:  0.03732588717182212\n",
            "Epoch: 039, Loss: 1.8390, Train: 0.4243, Test: 0.3911\n",
            "Early stopping:  0.03807721244769964\n",
            "Epoch: 040, Loss: 1.8131, Train: 0.4282, Test: 0.3952\n",
            "Early stopping:  0.037138769117252675\n",
            "Epoch: 041, Loss: 1.7900, Train: 0.4334, Test: 0.3969\n",
            "Early stopping:  0.0367638432902853\n",
            "Epoch: 042, Loss: 1.7745, Train: 0.4356, Test: 0.3958\n",
            "Early stopping:  0.035001389081181625\n",
            "Epoch: 043, Loss: 1.7505, Train: 0.4368, Test: 0.3965\n",
            "Early stopping:  0.034188244343413235\n",
            "Epoch: 044, Loss: 1.7343, Train: 0.4413, Test: 0.4026\n",
            "Early stopping:  0.03123000069152767\n",
            "Epoch: 045, Loss: 1.7159, Train: 0.4470, Test: 0.4070\n",
            "Early stopping:  0.02983714285342033\n",
            "Epoch: 046, Loss: 1.6926, Train: 0.4470, Test: 0.4067\n",
            "Early stopping:  0.0314157503510623\n",
            "Epoch: 047, Loss: 1.6757, Train: 0.4521, Test: 0.4097\n",
            "Early stopping:  0.030291582301389005\n",
            "Epoch: 048, Loss: 1.6547, Train: 0.4555, Test: 0.4120\n",
            "Early stopping:  0.031559267586685535\n",
            "Epoch: 049, Loss: 1.6319, Train: 0.4606, Test: 0.4151\n",
            "Early stopping:  0.03260707361269231\n",
            "Epoch: 050, Loss: 1.6135, Train: 0.4663, Test: 0.4152\n",
            "Early stopping:  0.03199754993925965\n",
            "Epoch: 051, Loss: 1.5911, Train: 0.4668, Test: 0.4185\n",
            "Early stopping:  0.033307983618033575\n",
            "Epoch: 052, Loss: 1.5724, Train: 0.4708, Test: 0.4198\n",
            "Early stopping:  0.03252091000714478\n",
            "Epoch: 053, Loss: 1.5543, Train: 0.4736, Test: 0.4232\n",
            "Early stopping:  0.03105156031796468\n",
            "Epoch: 054, Loss: 1.5363, Train: 0.4742, Test: 0.4269\n",
            "Early stopping:  0.03024332522472903\n",
            "Epoch: 055, Loss: 1.5187, Train: 0.4702, Test: 0.4259\n",
            "Early stopping:  0.028570892235938943\n",
            "Epoch: 056, Loss: 1.5424, Train: 0.4753, Test: 0.4281\n",
            "Early stopping:  0.020042975479344437\n",
            "Epoch: 057, Loss: 1.4867, Train: 0.4833, Test: 0.4324\n",
            "Early stopping:  0.02627413559213526\n",
            "Epoch: 058, Loss: 1.4700, Train: 0.4838, Test: 0.4363\n",
            "Early stopping:  0.03145649877196953\n",
            "Epoch: 059, Loss: 1.4541, Train: 0.4940, Test: 0.4366\n",
            "Early stopping:  0.035969136979887746\n",
            "Epoch: 060, Loss: 1.4368, Train: 0.4963, Test: 0.4440\n",
            "Early stopping:  0.040475166589474816\n",
            "Epoch: 061, Loss: 1.4231, Train: 0.5026, Test: 0.4429\n",
            "Early stopping:  0.025364129448235073\n",
            "Epoch: 062, Loss: 1.4034, Train: 0.5060, Test: 0.4436\n",
            "Early stopping:  0.02596973834213926\n",
            "Epoch: 063, Loss: 1.3885, Train: 0.5156, Test: 0.4492\n",
            "Early stopping:  0.026048510552752816\n",
            "Epoch: 064, Loss: 1.3738, Train: 0.5286, Test: 0.4534\n",
            "Early stopping:  0.02544346036666088\n",
            "Epoch: 065, Loss: 1.3561, Train: 0.5269, Test: 0.4555\n",
            "Early stopping:  0.02590656498571502\n",
            "Epoch: 066, Loss: 1.3413, Train: 0.5326, Test: 0.4532\n",
            "Early stopping:  0.024760929366064877\n",
            "Epoch: 067, Loss: 1.3293, Train: 0.5360, Test: 0.4566\n",
            "Early stopping:  0.02388435399953887\n",
            "Epoch: 068, Loss: 1.3107, Train: 0.5377, Test: 0.4613\n",
            "Early stopping:  0.02425609259458889\n",
            "Epoch: 069, Loss: 1.3006, Train: 0.5423, Test: 0.4606\n",
            "Early stopping:  0.022486584559890572\n",
            "Epoch: 070, Loss: 1.2896, Train: 0.5462, Test: 0.4621\n",
            "Early stopping:  0.0210327222608033\n",
            "Epoch: 071, Loss: 1.2740, Train: 0.5496, Test: 0.4678\n",
            "Early stopping:  0.020934459815265538\n",
            "Epoch: 072, Loss: 1.2590, Train: 0.5581, Test: 0.4671\n",
            "Early stopping:  0.02061905710841318\n",
            "Epoch: 073, Loss: 1.2422, Train: 0.5621, Test: 0.4667\n",
            "Early stopping:  0.023348586562679807\n",
            "Epoch: 074, Loss: 1.2306, Train: 0.5661, Test: 0.4701\n",
            "Early stopping:  0.02372051829484431\n",
            "Epoch: 075, Loss: 1.2150, Train: 0.5740, Test: 0.4746\n",
            "Early stopping:  0.023203349906817786\n",
            "Epoch: 076, Loss: 1.2042, Train: 0.5791, Test: 0.4738\n",
            "Early stopping:  0.021691819508900645\n",
            "Epoch: 077, Loss: 1.1929, Train: 0.5842, Test: 0.4763\n",
            "Early stopping:  0.019799545990387645\n",
            "Epoch: 078, Loss: 1.1824, Train: 0.5916, Test: 0.4871\n",
            "Early stopping:  0.018779877431610092\n",
            "Epoch: 079, Loss: 1.1782, Train: 0.6029, Test: 0.4888\n",
            "Early stopping:  0.015234092658727065\n",
            "Epoch: 080, Loss: 1.1625, Train: 0.6126, Test: 0.4919\n",
            "Early stopping:  0.015685343318844767\n",
            "Epoch: 081, Loss: 1.1465, Train: 0.6024, Test: 0.4904\n",
            "Early stopping:  0.018164712793915374\n",
            "Epoch: 082, Loss: 1.1457, Train: 0.6058, Test: 0.4929\n",
            "Early stopping:  0.017166867874472542\n",
            "Epoch: 083, Loss: 1.1407, Train: 0.6251, Test: 0.4953\n",
            "Early stopping:  0.015475838953653669\n",
            "Epoch: 084, Loss: 1.1125, Train: 0.6200, Test: 0.4932\n",
            "Early stopping:  0.018198518176544786\n",
            "Epoch: 085, Loss: 1.1167, Train: 0.6296, Test: 0.4942\n",
            "Early stopping:  0.016479631324909955\n",
            "Epoch: 086, Loss: 1.0867, Train: 0.6268, Test: 0.4940\n",
            "Early stopping:  0.023767351207673346\n",
            "Epoch: 087, Loss: 1.0864, Train: 0.6358, Test: 0.4962\n",
            "Early stopping:  0.022793603567639046\n",
            "Epoch: 088, Loss: 1.0771, Train: 0.6466, Test: 0.4957\n",
            "Early stopping:  0.017583184696608994\n",
            "Epoch: 089, Loss: 1.0517, Train: 0.6466, Test: 0.4996\n",
            "Early stopping:  0.023316008515214575\n",
            "Epoch: 090, Loss: 1.0363, Train: 0.6432, Test: 0.5043\n",
            "Early stopping:  0.022621425090479694\n",
            "Epoch: 091, Loss: 1.0234, Train: 0.6540, Test: 0.5034\n",
            "Early stopping:  0.02663177838380013\n",
            "Epoch: 092, Loss: 1.0089, Train: 0.6631, Test: 0.5017\n",
            "Early stopping:  0.026294785417448015\n",
            "Epoch: 093, Loss: 0.9998, Train: 0.6614, Test: 0.5082\n",
            "Early stopping:  0.020783107480325018\n",
            "Epoch: 094, Loss: 0.9736, Train: 0.6568, Test: 0.5129\n",
            "Early stopping:  0.023905954239335586\n",
            "Epoch: 095, Loss: 0.9778, Train: 0.6631, Test: 0.5085\n",
            "Early stopping:  0.020997969517924993\n",
            "Epoch: 096, Loss: 0.9538, Train: 0.6710, Test: 0.5060\n",
            "Early stopping:  0.02193624592842806\n",
            "Epoch: 097, Loss: 0.9482, Train: 0.6790, Test: 0.5123\n",
            "Early stopping:  0.020611606236865136\n",
            "Epoch: 098, Loss: 0.9309, Train: 0.6812, Test: 0.5142\n",
            "Early stopping:  0.01920359428022074\n",
            "Epoch: 099, Loss: 0.9181, Train: 0.6909, Test: 0.5077\n",
            "Early stopping:  0.0227950950870532\n",
            "Epoch: 100, Loss: 0.9028, Train: 0.6965, Test: 0.5146\n",
            "Early stopping:  0.02106644324018794\n",
            "Epoch: 101, Loss: 0.8925, Train: 0.7033, Test: 0.5169\n",
            "Early stopping:  0.02211700226291159\n",
            "Epoch: 102, Loss: 0.8761, Train: 0.7005, Test: 0.5160\n",
            "Early stopping:  0.02142647540763475\n",
            "Epoch: 103, Loss: 0.8660, Train: 0.7085, Test: 0.5156\n",
            "Early stopping:  0.02075740723560594\n",
            "Epoch: 104, Loss: 0.8516, Train: 0.7096, Test: 0.5211\n",
            "Early stopping:  0.020445759971779054\n",
            "Epoch: 105, Loss: 0.8409, Train: 0.7153, Test: 0.5213\n",
            "Early stopping:  0.020273857734717474\n",
            "Epoch: 106, Loss: 0.8277, Train: 0.7198, Test: 0.5211\n",
            "Early stopping:  0.0192828585410547\n",
            "Epoch: 107, Loss: 0.8203, Train: 0.7243, Test: 0.5225\n",
            "Early stopping:  0.018311735180685598\n",
            "Epoch: 108, Loss: 0.8055, Train: 0.7277, Test: 0.5218\n",
            "Early stopping:  0.017875567057099643\n",
            "Epoch: 109, Loss: 0.8031, Train: 0.7306, Test: 0.5197\n",
            "Early stopping:  0.01573303748259458\n",
            "Epoch: 110, Loss: 0.7944, Train: 0.7345, Test: 0.5254\n",
            "Early stopping:  0.013538329290041902\n",
            "Epoch: 111, Loss: 0.7773, Train: 0.7340, Test: 0.5264\n",
            "Early stopping:  0.01580467773569397\n",
            "Epoch: 112, Loss: 0.7732, Train: 0.7419, Test: 0.5296\n",
            "Early stopping:  0.014757115911700472\n",
            "Epoch: 113, Loss: 0.7592, Train: 0.7448, Test: 0.5250\n",
            "Early stopping:  0.017422380777261475\n",
            "Epoch: 114, Loss: 0.7525, Train: 0.7459, Test: 0.5306\n",
            "Early stopping:  0.016383270759015932\n",
            "Epoch: 115, Loss: 0.7419, Train: 0.7589, Test: 0.5298\n",
            "Early stopping:  0.014616640517981592\n",
            "Epoch: 116, Loss: 0.7276, Train: 0.7561, Test: 0.5279\n",
            "Early stopping:  0.017268515851218275\n",
            "Epoch: 117, Loss: 0.7232, Train: 0.7629, Test: 0.5309\n",
            "Early stopping:  0.015512159233682338\n",
            "Epoch: 118, Loss: 0.7120, Train: 0.7652, Test: 0.5323\n",
            "Early stopping:  0.01590765463760933\n",
            "Epoch: 119, Loss: 0.7035, Train: 0.7674, Test: 0.5293\n",
            "Early stopping:  0.014765083462792358\n",
            "Epoch: 120, Loss: 0.6973, Train: 0.7731, Test: 0.5289\n",
            "Early stopping:  0.012781893056923099\n",
            "Epoch: 121, Loss: 0.6869, Train: 0.7777, Test: 0.5326\n",
            "Early stopping:  0.013837266423419266\n",
            "Epoch: 122, Loss: 0.6797, Train: 0.7765, Test: 0.5322\n",
            "Early stopping:  0.012886777983228358\n",
            "Epoch: 123, Loss: 0.6715, Train: 0.7839, Test: 0.5289\n",
            "Early stopping:  0.012923935156175945\n",
            "Epoch: 124, Loss: 0.6646, Train: 0.7884, Test: 0.5333\n",
            "Early stopping:  0.012813366185843625\n",
            "Epoch: 125, Loss: 0.6567, Train: 0.7896, Test: 0.5320\n",
            "Early stopping:  0.011942052870332212\n",
            "Epoch: 126, Loss: 0.6485, Train: 0.7901, Test: 0.5313\n",
            "Early stopping:  0.012214697795039873\n",
            "Epoch: 127, Loss: 0.6409, Train: 0.7896, Test: 0.5299\n",
            "Early stopping:  0.012238771168064527\n",
            "Epoch: 128, Loss: 0.6334, Train: 0.7958, Test: 0.5335\n",
            "Early stopping:  0.012367753279758715\n",
            "Epoch: 129, Loss: 0.6274, Train: 0.7941, Test: 0.5319\n",
            "Early stopping:  0.011681196986588593\n",
            "Epoch: 130, Loss: 0.6190, Train: 0.7969, Test: 0.5336\n",
            "Early stopping:  0.011461615628508266\n",
            "Epoch: 131, Loss: 0.6124, Train: 0.8037, Test: 0.5342\n",
            "Early stopping:  0.011299062223807872\n",
            "Epoch: 132, Loss: 0.6062, Train: 0.8026, Test: 0.5339\n",
            "Early stopping:  0.010981696067899551\n",
            "Epoch: 133, Loss: 0.5965, Train: 0.7998, Test: 0.5366\n",
            "Early stopping:  0.011825248462465476\n",
            "Epoch: 134, Loss: 0.5904, Train: 0.8088, Test: 0.5356\n",
            "Early stopping:  0.011586077440168525\n",
            "Epoch: 135, Loss: 0.5838, Train: 0.8128, Test: 0.5381\n",
            "Early stopping:  0.011555473957930301\n",
            "Epoch: 136, Loss: 0.5766, Train: 0.8134, Test: 0.5405\n",
            "Early stopping:  0.011395277637739332\n",
            "Epoch: 137, Loss: 0.5696, Train: 0.8179, Test: 0.5396\n",
            "Early stopping:  0.010711396688843853\n",
            "Epoch: 138, Loss: 0.5632, Train: 0.8208, Test: 0.5396\n",
            "Early stopping:  0.010878696371505672\n",
            "Epoch: 139, Loss: 0.5555, Train: 0.8247, Test: 0.5417\n",
            "Early stopping:  0.011080689801207072\n",
            "Epoch: 140, Loss: 0.5492, Train: 0.8298, Test: 0.5411\n",
            "Early stopping:  0.01089030010215836\n",
            "Epoch: 141, Loss: 0.5418, Train: 0.8264, Test: 0.5442\n",
            "Early stopping:  0.010994622795387177\n",
            "Epoch: 142, Loss: 0.5373, Train: 0.8287, Test: 0.5410\n",
            "Early stopping:  0.010372847631361535\n",
            "Epoch: 143, Loss: 0.5341, Train: 0.8315, Test: 0.5452\n",
            "Early stopping:  0.008749007719790407\n",
            "PREDICTIONS -> tensor([10,  0,  1,  ..., 11,  5,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.49      0.51      0.50       758\n",
            "         capital_goods       0.39      0.38      0.38       508\n",
            "conglomerates_industry       0.20      0.03      0.04        80\n",
            "     consumer_cyclical       0.51      0.49      0.50       793\n",
            " consumer_non-cyclical       0.62      0.44      0.51       446\n",
            "                energy       0.58      0.51      0.55       283\n",
            "             financial       0.59      0.57      0.58       767\n",
            "            healthcare       0.54      0.57      0.56       318\n",
            "              services       0.59      0.69      0.64      2076\n",
            "            technology       0.46      0.28      0.34       396\n",
            "        transportation       0.56      0.63      0.59       404\n",
            "             utilities       0.49      0.53      0.51       225\n",
            "\n",
            "              accuracy                           0.55      7054\n",
            "             macro avg       0.50      0.47      0.48      7054\n",
            "          weighted avg       0.54      0.55      0.54      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 45.9382, Train: 0.2950, Test: 0.2946\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 361.7232, Train: 0.1134, Test: 0.1136\n",
            "Early stopping:  223.2937174864963\n",
            "Epoch: 003, Loss: 398.3311, Train: 0.0908, Test: 0.0868\n",
            "Early stopping:  193.75288098214185\n",
            "Epoch: 004, Loss: 441.8842, Train: 0.0499, Test: 0.0499\n",
            "Early stopping:  180.355424128823\n",
            "Epoch: 005, Loss: 429.8827, Train: 0.1220, Test: 0.1208\n",
            "Early stopping:  164.8538053500782\n",
            "Epoch: 006, Loss: 244.4476, Train: 0.1208, Test: 0.1175\n",
            "Early stopping:  79.45098130583206\n",
            "Epoch: 007, Loss: 184.5628, Train: 0.3080, Test: 0.3029\n",
            "Early stopping:  117.42298151271615\n",
            "Epoch: 008, Loss: 206.1560, Train: 0.3046, Test: 0.3017\n",
            "Early stopping:  124.70919579242795\n",
            "Epoch: 009, Loss: 192.7842, Train: 0.2774, Test: 0.2647\n",
            "Early stopping:  102.29161313677798\n",
            "Epoch: 010, Loss: 171.5626, Train: 0.3001, Test: 0.2929\n",
            "Early stopping:  27.894855267281823\n",
            "Epoch: 011, Loss: 128.4337, Train: 0.0749, Test: 0.0696\n",
            "Early stopping:  29.766634490567487\n",
            "Epoch: 012, Loss: 120.9095, Train: 0.1611, Test: 0.1523\n",
            "Early stopping:  38.0282873116211\n",
            "Epoch: 013, Loss: 84.1565, Train: 0.2394, Test: 0.2151\n",
            "Early stopping:  43.00629864557694\n",
            "Epoch: 014, Loss: 61.3483, Train: 0.1690, Test: 0.1517\n",
            "Early stopping:  42.514059658724705\n",
            "Epoch: 015, Loss: 40.7963, Train: 0.1685, Test: 0.1581\n",
            "Early stopping:  37.641049819024\n",
            "Epoch: 016, Loss: 19.8379, Train: 0.2950, Test: 0.2906\n",
            "Early stopping:  39.1496318064647\n",
            "Epoch: 017, Loss: 4.8553, Train: 0.2774, Test: 0.2624\n",
            "Early stopping:  31.716199234650453\n",
            "Epoch: 018, Loss: 2.7289, Train: 0.2609, Test: 0.2367\n",
            "Early stopping:  24.97502963050928\n",
            "Epoch: 019, Loss: 2.6412, Train: 0.2484, Test: 0.2329\n",
            "Early stopping:  16.52013226105663\n",
            "Epoch: 020, Loss: 2.5484, Train: 0.2535, Test: 0.2308\n",
            "Early stopping:  7.505460038421882\n",
            "Epoch: 021, Loss: 2.4091, Train: 0.2604, Test: 0.2399\n",
            "Early stopping:  1.023582483665559\n",
            "Epoch: 022, Loss: 2.2668, Train: 0.2734, Test: 0.2487\n",
            "Early stopping:  0.18408091173274\n",
            "Epoch: 023, Loss: 2.1705, Train: 0.3018, Test: 0.2725\n",
            "Early stopping:  0.19391596304986214\n",
            "Epoch: 024, Loss: 2.1285, Train: 0.3091, Test: 0.2844\n",
            "Early stopping:  0.1737634785184608\n",
            "Epoch: 025, Loss: 2.1104, Train: 0.3301, Test: 0.2983\n",
            "Early stopping:  0.12324056760950794\n",
            "Epoch: 026, Loss: 2.0955, Train: 0.3358, Test: 0.3110\n",
            "Early stopping:  0.06885313486702938\n",
            "Epoch: 027, Loss: 2.0726, Train: 0.3488, Test: 0.3225\n",
            "Early stopping:  0.03695682236403206\n",
            "Epoch: 028, Loss: 2.0485, Train: 0.3522, Test: 0.3313\n",
            "Early stopping:  0.03141709396889072\n",
            "Epoch: 029, Loss: 2.0281, Train: 0.3613, Test: 0.3489\n",
            "Early stopping:  0.0335570425863249\n",
            "Epoch: 030, Loss: 1.9972, Train: 0.3670, Test: 0.3609\n",
            "Early stopping:  0.03822864934124365\n",
            "Epoch: 031, Loss: 1.9715, Train: 0.3602, Test: 0.3521\n",
            "Early stopping:  0.040173917953345566\n",
            "Epoch: 032, Loss: 1.9566, Train: 0.3579, Test: 0.3504\n",
            "Early stopping:  0.038232011232700314\n",
            "Epoch: 033, Loss: 1.9415, Train: 0.3607, Test: 0.3544\n",
            "Early stopping:  0.03430615838955225\n",
            "Epoch: 034, Loss: 1.9252, Train: 0.3670, Test: 0.3557\n",
            "Early stopping:  0.027696026324967312\n",
            "Epoch: 035, Loss: 1.9098, Train: 0.3727, Test: 0.3616\n",
            "Early stopping:  0.02446764242123728\n",
            "Epoch: 036, Loss: 1.8878, Train: 0.3806, Test: 0.3707\n",
            "Early stopping:  0.026836084964047574\n",
            "Epoch: 037, Loss: 1.8628, Train: 0.3868, Test: 0.3752\n",
            "Early stopping:  0.030953632664487297\n",
            "Epoch: 038, Loss: 1.8388, Train: 0.3902, Test: 0.3808\n",
            "Early stopping:  0.03485810276117543\n",
            "Epoch: 039, Loss: 1.8171, Train: 0.4056, Test: 0.3862\n",
            "Early stopping:  0.03708258243637591\n",
            "Epoch: 040, Loss: 1.7967, Train: 0.4124, Test: 0.3954\n",
            "Early stopping:  0.03607729852596621\n",
            "Epoch: 041, Loss: 1.7738, Train: 0.4277, Test: 0.4045\n",
            "Early stopping:  0.034840770527251486\n",
            "Epoch: 042, Loss: 1.7545, Train: 0.4339, Test: 0.4063\n",
            "Early stopping:  0.033515481508329246\n",
            "Epoch: 043, Loss: 1.7374, Train: 0.4373, Test: 0.4105\n",
            "Early stopping:  0.031916576052566856\n",
            "Epoch: 044, Loss: 1.7222, Train: 0.4419, Test: 0.4148\n",
            "Early stopping:  0.02940160393259097\n",
            "Epoch: 045, Loss: 1.7052, Train: 0.4402, Test: 0.4192\n",
            "Early stopping:  0.026804345985127688\n",
            "Epoch: 046, Loss: 1.6880, Train: 0.4475, Test: 0.4250\n",
            "Early stopping:  0.0261143093426682\n",
            "Epoch: 047, Loss: 1.6692, Train: 0.4555, Test: 0.4247\n",
            "Early stopping:  0.026991968483749063\n",
            "Epoch: 048, Loss: 1.6465, Train: 0.4566, Test: 0.4274\n",
            "Early stopping:  0.029712597217836006\n",
            "Epoch: 049, Loss: 1.6292, Train: 0.4617, Test: 0.4334\n",
            "Early stopping:  0.03064267378478858\n",
            "Epoch: 050, Loss: 1.6125, Train: 0.4685, Test: 0.4386\n",
            "Early stopping:  0.03025934262239518\n",
            "Epoch: 051, Loss: 1.5961, Train: 0.4787, Test: 0.4426\n",
            "Early stopping:  0.028538067312537843\n",
            "Epoch: 052, Loss: 1.5798, Train: 0.4844, Test: 0.4467\n",
            "Early stopping:  0.02631864147838539\n",
            "Epoch: 053, Loss: 1.5656, Train: 0.4952, Test: 0.4488\n",
            "Early stopping:  0.02528405864744672\n",
            "Epoch: 054, Loss: 1.5491, Train: 0.4986, Test: 0.4509\n",
            "Early stopping:  0.02488202899959442\n",
            "Epoch: 055, Loss: 1.5336, Train: 0.5020, Test: 0.4522\n",
            "Early stopping:  0.02465062435842055\n",
            "Epoch: 056, Loss: 1.5177, Train: 0.4997, Test: 0.4539\n",
            "Early stopping:  0.024693978547132834\n",
            "Epoch: 057, Loss: 1.5049, Train: 0.5105, Test: 0.4552\n",
            "Early stopping:  0.024168393945043703\n",
            "Epoch: 058, Loss: 1.4928, Train: 0.5150, Test: 0.4595\n",
            "Early stopping:  0.022365855666872884\n",
            "Epoch: 059, Loss: 1.4789, Train: 0.5218, Test: 0.4603\n",
            "Early stopping:  0.021256295569126324\n",
            "Epoch: 060, Loss: 1.4682, Train: 0.5247, Test: 0.4634\n",
            "Early stopping:  0.01980155586636569\n",
            "Epoch: 061, Loss: 1.4529, Train: 0.5258, Test: 0.4673\n",
            "Early stopping:  0.020365405243558076\n",
            "Epoch: 062, Loss: 1.4409, Train: 0.5326, Test: 0.4687\n",
            "Early stopping:  0.020535945885759993\n",
            "Epoch: 063, Loss: 1.4274, Train: 0.5355, Test: 0.4687\n",
            "Early stopping:  0.02062649946757874\n",
            "Epoch: 064, Loss: 1.4135, Train: 0.5411, Test: 0.4716\n",
            "Early stopping:  0.021335700398788248\n",
            "Epoch: 065, Loss: 1.4004, Train: 0.5366, Test: 0.4756\n",
            "Early stopping:  0.020946960430695962\n",
            "Epoch: 066, Loss: 1.3882, Train: 0.5462, Test: 0.4793\n",
            "Early stopping:  0.020946334307142194\n",
            "Epoch: 067, Loss: 1.3743, Train: 0.5530, Test: 0.4800\n",
            "Early stopping:  0.020793888178214804\n",
            "Epoch: 068, Loss: 1.3635, Train: 0.5525, Test: 0.4820\n",
            "Early stopping:  0.019935163041910165\n",
            "Epoch: 069, Loss: 1.3510, Train: 0.5491, Test: 0.4833\n",
            "Early stopping:  0.01953498338297868\n",
            "Epoch: 070, Loss: 1.3404, Train: 0.5559, Test: 0.4826\n",
            "Early stopping:  0.01882338742526831\n",
            "Epoch: 071, Loss: 1.3288, Train: 0.5661, Test: 0.4836\n",
            "Early stopping:  0.01805440306072545\n",
            "Epoch: 072, Loss: 1.3176, Train: 0.5678, Test: 0.4834\n",
            "Early stopping:  0.018041260023633588\n",
            "Epoch: 073, Loss: 1.3042, Train: 0.5706, Test: 0.4841\n",
            "Early stopping:  0.018417684739734062\n",
            "Epoch: 074, Loss: 1.2932, Train: 0.5797, Test: 0.4837\n",
            "Early stopping:  0.018817590009510787\n",
            "Epoch: 075, Loss: 1.2819, Train: 0.5854, Test: 0.4870\n",
            "Early stopping:  0.01869163005904954\n",
            "Epoch: 076, Loss: 1.2706, Train: 0.5848, Test: 0.4885\n",
            "Early stopping:  0.018387315704139783\n",
            "Epoch: 077, Loss: 1.2587, Train: 0.5865, Test: 0.4878\n",
            "Early stopping:  0.01793955791465879\n",
            "Epoch: 078, Loss: 1.2485, Train: 0.5927, Test: 0.4868\n",
            "Early stopping:  0.01780567270579535\n",
            "Epoch: 079, Loss: 1.2383, Train: 0.5910, Test: 0.4904\n",
            "Early stopping:  0.01728558415085773\n",
            "Epoch: 080, Loss: 1.2281, Train: 0.5916, Test: 0.4928\n",
            "Early stopping:  0.016687999805698638\n",
            "Epoch: 081, Loss: 1.2168, Train: 0.6012, Test: 0.4946\n",
            "Early stopping:  0.016494916133560435\n",
            "Epoch: 082, Loss: 1.2056, Train: 0.6075, Test: 0.4948\n",
            "Early stopping:  0.01697056354329258\n",
            "Epoch: 083, Loss: 1.1946, Train: 0.6047, Test: 0.4946\n",
            "Early stopping:  0.017381540870239843\n",
            "Epoch: 084, Loss: 1.1864, Train: 0.6154, Test: 0.4956\n",
            "Early stopping:  0.016717292751108833\n",
            "Epoch: 085, Loss: 1.1745, Train: 0.6160, Test: 0.4928\n",
            "Early stopping:  0.0164501589994156\n",
            "Epoch: 086, Loss: 1.1663, Train: 0.6188, Test: 0.4956\n",
            "Early stopping:  0.01564990114291942\n",
            "Epoch: 087, Loss: 1.1538, Train: 0.6103, Test: 0.4959\n",
            "Early stopping:  0.01610820930319265\n",
            "Epoch: 088, Loss: 1.1532, Train: 0.6239, Test: 0.4967\n",
            "Early stopping:  0.014096802616719894\n",
            "Epoch: 089, Loss: 1.1421, Train: 0.6222, Test: 0.4972\n",
            "Early stopping:  0.012563791960865392\n",
            "Epoch: 090, Loss: 1.1509, Train: 0.6256, Test: 0.4960\n",
            "Early stopping:  0.008654436414845364\n",
            "PREDICTIONS -> tensor([ 0,  8,  0,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.50      0.45       758\n",
            "         capital_goods       0.10      0.00      0.01       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.39      0.50      0.44       793\n",
            " consumer_non-cyclical       0.64      0.40      0.49       446\n",
            "                energy       0.47      0.49      0.48       283\n",
            "             financial       0.52      0.58      0.55       767\n",
            "            healthcare       0.80      0.51      0.62       318\n",
            "              services       0.53      0.70      0.60      2076\n",
            "            technology       0.29      0.10      0.15       396\n",
            "        transportation       0.51      0.57      0.54       404\n",
            "             utilities       0.60      0.35      0.44       225\n",
            "\n",
            "              accuracy                           0.50      7054\n",
            "             macro avg       0.44      0.39      0.40      7054\n",
            "          weighted avg       0.47      0.50      0.47      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 55.7783, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 348.6227, Train: 0.1112, Test: 0.1145\n",
            "Early stopping:  207.07222224723355\n",
            "Epoch: 003, Loss: 456.6776, Train: 0.1197, Test: 0.1182\n",
            "Early stopping:  207.42623052619845\n",
            "Epoch: 004, Loss: 592.4072, Train: 0.0675, Test: 0.0672\n",
            "Early stopping:  228.03101964580753\n",
            "Epoch: 005, Loss: 556.1412, Train: 0.0584, Test: 0.0576\n",
            "Early stopping:  215.47771126979845\n",
            "Epoch: 006, Loss: 505.7266, Train: 0.1106, Test: 0.1092\n",
            "Early stopping:  95.1167760151573\n",
            "Epoch: 007, Loss: 425.8822, Train: 0.0340, Test: 0.0339\n",
            "Early stopping:  68.59780091519772\n",
            "Epoch: 008, Loss: 313.3448, Train: 0.3012, Test: 0.3010\n",
            "Early stopping:  111.56718748888864\n",
            "Epoch: 009, Loss: 234.0000, Train: 0.2672, Test: 0.2705\n",
            "Early stopping:  133.27310378058095\n",
            "Epoch: 010, Loss: 196.0497, Train: 0.2711, Test: 0.2681\n",
            "Early stopping:  129.792415549986\n",
            "Epoch: 011, Loss: 132.5934, Train: 0.0709, Test: 0.0662\n",
            "Early stopping:  113.31366960304553\n",
            "Epoch: 012, Loss: 129.3206, Train: 0.1531, Test: 0.1484\n",
            "Early stopping:  76.73369587244251\n",
            "Epoch: 013, Loss: 87.2810, Train: 0.1645, Test: 0.1585\n",
            "Early stopping:  58.45901436451582\n",
            "Epoch: 014, Loss: 59.6579, Train: 0.1843, Test: 0.1762\n",
            "Early stopping:  51.80717111691757\n",
            "Epoch: 015, Loss: 44.5869, Train: 0.1860, Test: 0.1748\n",
            "Early stopping:  39.83820661462516\n",
            "Epoch: 016, Loss: 33.1253, Train: 0.2360, Test: 0.2160\n",
            "Early stopping:  38.49329655305696\n",
            "Epoch: 017, Loss: 24.0435, Train: 0.2604, Test: 0.2407\n",
            "Early stopping:  24.852552167833764\n",
            "Epoch: 018, Loss: 14.2702, Train: 0.2473, Test: 0.2414\n",
            "Early stopping:  17.699644370148146\n",
            "Epoch: 019, Loss: 7.7595, Train: 0.1894, Test: 0.1839\n",
            "Early stopping:  14.68545547139479\n",
            "Epoch: 020, Loss: 5.4857, Train: 0.1798, Test: 0.1707\n",
            "Early stopping:  11.565420286603716\n",
            "Epoch: 021, Loss: 4.5213, Train: 0.1855, Test: 0.1789\n",
            "Early stopping:  8.115987115438758\n",
            "Epoch: 022, Loss: 3.6590, Train: 0.2059, Test: 0.1877\n",
            "Early stopping:  4.270088982293816\n",
            "Epoch: 023, Loss: 2.9911, Train: 0.2229, Test: 0.1982\n",
            "Early stopping:  1.8605144334036143\n",
            "Epoch: 024, Loss: 2.6516, Train: 0.2484, Test: 0.2214\n",
            "Early stopping:  1.1549728531014376\n",
            "Epoch: 025, Loss: 2.4694, Train: 0.2779, Test: 0.2613\n",
            "Early stopping:  0.83927190409657\n",
            "Epoch: 026, Loss: 2.3492, Train: 0.3415, Test: 0.3244\n",
            "Early stopping:  0.5258601186078583\n",
            "Epoch: 027, Loss: 2.2557, Train: 0.3613, Test: 0.3398\n",
            "Early stopping:  0.29072063079095384\n",
            "Epoch: 028, Loss: 2.2248, Train: 0.3625, Test: 0.3394\n",
            "Early stopping:  0.17445823755260426\n",
            "Epoch: 029, Loss: 2.1720, Train: 0.3687, Test: 0.3452\n",
            "Early stopping:  0.11721781147470624\n",
            "Epoch: 030, Loss: 2.1105, Train: 0.3681, Test: 0.3591\n",
            "Early stopping:  0.08979182172044595\n",
            "Epoch: 031, Loss: 2.1111, Train: 0.3846, Test: 0.3713\n",
            "Early stopping:  0.06564729924074895\n",
            "Epoch: 032, Loss: 2.0523, Train: 0.4067, Test: 0.3891\n",
            "Early stopping:  0.06601039924265979\n",
            "Epoch: 033, Loss: 2.0084, Train: 0.4158, Test: 0.3965\n",
            "Early stopping:  0.06258003716555925\n",
            "Epoch: 034, Loss: 1.9903, Train: 0.4220, Test: 0.4016\n",
            "Early stopping:  0.056110156949157115\n",
            "Epoch: 035, Loss: 1.9422, Train: 0.4271, Test: 0.4052\n",
            "Early stopping:  0.06405500805097131\n",
            "Epoch: 036, Loss: 1.8937, Train: 0.4237, Test: 0.3958\n",
            "Early stopping:  0.061186638775462904\n",
            "Epoch: 037, Loss: 1.8797, Train: 0.4299, Test: 0.3959\n",
            "Early stopping:  0.05689095048234896\n",
            "Epoch: 038, Loss: 1.8475, Train: 0.4373, Test: 0.4047\n",
            "Early stopping:  0.05607502592404813\n",
            "Epoch: 039, Loss: 1.8122, Train: 0.4470, Test: 0.4157\n",
            "Early stopping:  0.048959019762656215\n",
            "Epoch: 040, Loss: 1.7777, Train: 0.4566, Test: 0.4270\n",
            "Early stopping:  0.047815231164929924\n",
            "Epoch: 041, Loss: 1.7396, Train: 0.4583, Test: 0.4375\n",
            "Early stopping:  0.05535166957644019\n",
            "Epoch: 042, Loss: 1.7102, Train: 0.4623, Test: 0.4434\n",
            "Early stopping:  0.05493049324692708\n",
            "Epoch: 043, Loss: 1.6909, Train: 0.4651, Test: 0.4470\n",
            "Early stopping:  0.04936167625210262\n",
            "Epoch: 044, Loss: 1.6586, Train: 0.4799, Test: 0.4570\n",
            "Early stopping:  0.04560469798179644\n",
            "Epoch: 045, Loss: 1.6238, Train: 0.4867, Test: 0.4592\n",
            "Early stopping:  0.0449915655610378\n",
            "Epoch: 046, Loss: 1.6008, Train: 0.4929, Test: 0.4609\n",
            "Early stopping:  0.04542091962988986\n",
            "Epoch: 047, Loss: 1.5835, Train: 0.5003, Test: 0.4657\n",
            "Early stopping:  0.04351893356407663\n",
            "Epoch: 048, Loss: 1.5542, Train: 0.5037, Test: 0.4709\n",
            "Early stopping:  0.0396226567670987\n",
            "Epoch: 049, Loss: 1.5287, Train: 0.5111, Test: 0.4762\n",
            "Early stopping:  0.03752757193509222\n",
            "Epoch: 050, Loss: 1.5004, Train: 0.5235, Test: 0.4820\n",
            "Early stopping:  0.04052167259584867\n",
            "Epoch: 051, Loss: 1.4842, Train: 0.5303, Test: 0.4861\n",
            "Early stopping:  0.04009833918344738\n",
            "Epoch: 052, Loss: 1.4642, Train: 0.5332, Test: 0.4871\n",
            "Early stopping:  0.03569445655461698\n",
            "Epoch: 053, Loss: 1.4453, Train: 0.5332, Test: 0.4867\n",
            "Early stopping:  0.03223587660175737\n",
            "Epoch: 054, Loss: 1.4244, Train: 0.5303, Test: 0.4853\n",
            "Early stopping:  0.030200486937835055\n",
            "Epoch: 055, Loss: 1.4085, Train: 0.5360, Test: 0.4877\n",
            "Early stopping:  0.03024946564626759\n",
            "Epoch: 056, Loss: 1.3901, Train: 0.5428, Test: 0.4916\n",
            "Early stopping:  0.029275103418401716\n",
            "Epoch: 057, Loss: 1.3719, Train: 0.5513, Test: 0.4980\n",
            "Early stopping:  0.028667730164180567\n",
            "Epoch: 058, Loss: 1.3596, Train: 0.5547, Test: 0.4994\n",
            "Early stopping:  0.0263560062942767\n",
            "Epoch: 059, Loss: 1.3460, Train: 0.5610, Test: 0.5016\n",
            "Early stopping:  0.02468665793421819\n",
            "Epoch: 060, Loss: 1.3316, Train: 0.5632, Test: 0.5000\n",
            "Early stopping:  0.022644203845775143\n",
            "Epoch: 061, Loss: 1.3135, Train: 0.5649, Test: 0.5003\n",
            "Early stopping:  0.022934902625685483\n",
            "Epoch: 062, Loss: 1.3053, Train: 0.5706, Test: 0.5024\n",
            "Early stopping:  0.022418270584989037\n",
            "Epoch: 063, Loss: 1.2874, Train: 0.5797, Test: 0.5068\n",
            "Early stopping:  0.022819026069331002\n",
            "Epoch: 064, Loss: 1.2765, Train: 0.5814, Test: 0.5092\n",
            "Early stopping:  0.02165924198981077\n",
            "Epoch: 065, Loss: 1.2586, Train: 0.5893, Test: 0.5103\n",
            "Early stopping:  0.022035315949846108\n",
            "Epoch: 066, Loss: 1.2443, Train: 0.5905, Test: 0.5135\n",
            "Early stopping:  0.02387400599055532\n",
            "Epoch: 067, Loss: 1.2296, Train: 0.5933, Test: 0.5140\n",
            "Early stopping:  0.023426708286503235\n",
            "Epoch: 068, Loss: 1.2151, Train: 0.6029, Test: 0.5139\n",
            "Early stopping:  0.024056405107299995\n",
            "Epoch: 069, Loss: 1.2009, Train: 0.6047, Test: 0.5118\n",
            "Early stopping:  0.022887960000330575\n",
            "Epoch: 070, Loss: 1.1885, Train: 0.6064, Test: 0.5126\n",
            "Early stopping:  0.022194611009344358\n",
            "Epoch: 071, Loss: 1.1739, Train: 0.6109, Test: 0.5135\n",
            "Early stopping:  0.02181366047948752\n",
            "Epoch: 072, Loss: 1.1612, Train: 0.6126, Test: 0.5176\n",
            "Early stopping:  0.0212944834510303\n",
            "Epoch: 073, Loss: 1.1462, Train: 0.6160, Test: 0.5147\n",
            "Early stopping:  0.02162455047296876\n",
            "Epoch: 074, Loss: 1.1335, Train: 0.6239, Test: 0.5157\n",
            "Early stopping:  0.021779522559027143\n",
            "Epoch: 075, Loss: 1.1219, Train: 0.6313, Test: 0.5164\n",
            "Early stopping:  0.020851148821121494\n",
            "Epoch: 076, Loss: 1.1087, Train: 0.6285, Test: 0.5139\n",
            "Early stopping:  0.020466633446077156\n",
            "Epoch: 077, Loss: 1.0986, Train: 0.6313, Test: 0.5183\n",
            "Early stopping:  0.018985573278211473\n",
            "Epoch: 078, Loss: 1.0858, Train: 0.6370, Test: 0.5189\n",
            "Early stopping:  0.01880699033126293\n",
            "Epoch: 079, Loss: 1.0703, Train: 0.6358, Test: 0.5125\n",
            "Early stopping:  0.019982077439574017\n",
            "Epoch: 080, Loss: 1.0633, Train: 0.6461, Test: 0.5173\n",
            "Early stopping:  0.018914792926308165\n",
            "Epoch: 081, Loss: 1.0510, Train: 0.6489, Test: 0.5215\n",
            "Early stopping:  0.01868718228338551\n",
            "Epoch: 082, Loss: 1.0389, Train: 0.6534, Test: 0.5201\n",
            "Early stopping:  0.017967796438292924\n",
            "Epoch: 083, Loss: 1.0277, Train: 0.6551, Test: 0.5197\n",
            "Early stopping:  0.017405256169107552\n",
            "Epoch: 084, Loss: 1.0191, Train: 0.6591, Test: 0.5242\n",
            "Early stopping:  0.01772151012783587\n",
            "Epoch: 085, Loss: 1.0076, Train: 0.6676, Test: 0.5221\n",
            "Early stopping:  0.016871684003589196\n",
            "Epoch: 086, Loss: 0.9968, Train: 0.6585, Test: 0.5190\n",
            "Early stopping:  0.01649774652613224\n",
            "Epoch: 087, Loss: 0.9969, Train: 0.6642, Test: 0.5254\n",
            "Early stopping:  0.013665432724047568\n",
            "Epoch: 088, Loss: 0.9821, Train: 0.6670, Test: 0.5213\n",
            "Early stopping:  0.01377775346825354\n",
            "Epoch: 089, Loss: 0.9835, Train: 0.6733, Test: 0.5235\n",
            "Early stopping:  0.010618514432136962\n",
            "Epoch: 090, Loss: 0.9569, Train: 0.6767, Test: 0.5218\n",
            "Early stopping:  0.016302908917438016\n",
            "Epoch: 091, Loss: 0.9519, Train: 0.6778, Test: 0.5247\n",
            "Early stopping:  0.01911286231351015\n",
            "Epoch: 092, Loss: 0.9417, Train: 0.6812, Test: 0.5234\n",
            "Early stopping:  0.018713007595094193\n",
            "Epoch: 093, Loss: 0.9398, Train: 0.6863, Test: 0.5264\n",
            "Early stopping:  0.017569464575621196\n",
            "Epoch: 094, Loss: 0.9166, Train: 0.6886, Test: 0.5240\n",
            "Early stopping:  0.015589350166938996\n",
            "Epoch: 095, Loss: 0.9248, Train: 0.6943, Test: 0.5274\n",
            "Early stopping:  0.014122099604300567\n",
            "Epoch: 096, Loss: 0.9064, Train: 0.6914, Test: 0.5295\n",
            "Early stopping:  0.015093585474729504\n",
            "Epoch: 097, Loss: 0.9032, Train: 0.6943, Test: 0.5284\n",
            "Early stopping:  0.014818094118204107\n",
            "Epoch: 098, Loss: 0.8848, Train: 0.6977, Test: 0.5224\n",
            "Early stopping:  0.015151136421581434\n",
            "Epoch: 099, Loss: 0.8891, Train: 0.7085, Test: 0.5282\n",
            "Early stopping:  0.015846874394934086\n",
            "Epoch: 100, Loss: 0.8682, Train: 0.7056, Test: 0.5264\n",
            "Early stopping:  0.015362968353252494\n",
            "Epoch: 101, Loss: 0.8743, Train: 0.7158, Test: 0.5278\n",
            "Early stopping:  0.01355753120608884\n",
            "Epoch: 102, Loss: 0.8468, Train: 0.7045, Test: 0.5234\n",
            "Early stopping:  0.016630677013601505\n",
            "Epoch: 103, Loss: 0.8632, Train: 0.7198, Test: 0.5285\n",
            "Early stopping:  0.015456640714717711\n",
            "Epoch: 104, Loss: 0.8360, Train: 0.7147, Test: 0.5275\n",
            "Early stopping:  0.015863050068753343\n",
            "Epoch: 105, Loss: 0.8445, Train: 0.7249, Test: 0.5284\n",
            "Early stopping:  0.015476525806621002\n",
            "Epoch: 106, Loss: 0.8179, Train: 0.7170, Test: 0.5274\n",
            "Early stopping:  0.016560150928985762\n",
            "Epoch: 107, Loss: 0.8282, Train: 0.7311, Test: 0.5296\n",
            "Early stopping:  0.0172063260497408\n",
            "Epoch: 108, Loss: 0.8030, Train: 0.7334, Test: 0.5258\n",
            "Early stopping:  0.01616899829439533\n",
            "Epoch: 109, Loss: 0.8062, Train: 0.7413, Test: 0.5293\n",
            "Early stopping:  0.016987191858135065\n",
            "Epoch: 110, Loss: 0.7926, Train: 0.7436, Test: 0.5301\n",
            "Early stopping:  0.013773562556493136\n",
            "Epoch: 111, Loss: 0.7841, Train: 0.7357, Test: 0.5305\n",
            "Early stopping:  0.01666712342426788\n",
            "Epoch: 112, Loss: 0.7821, Train: 0.7436, Test: 0.5329\n",
            "Early stopping:  0.010852938230828111\n",
            "Epoch: 113, Loss: 0.7688, Train: 0.7516, Test: 0.5336\n",
            "Early stopping:  0.013811037446868612\n",
            "Epoch: 114, Loss: 0.7694, Train: 0.7504, Test: 0.5336\n",
            "Early stopping:  0.010188383428723698\n",
            "Epoch: 115, Loss: 0.7601, Train: 0.7470, Test: 0.5340\n",
            "Early stopping:  0.010031109668888165\n",
            "Epoch: 116, Loss: 0.7514, Train: 0.7504, Test: 0.5312\n",
            "Early stopping:  0.011474719778224235\n",
            "Epoch: 117, Loss: 0.7427, Train: 0.7561, Test: 0.5319\n",
            "Early stopping:  0.011489567041559321\n",
            "Epoch: 118, Loss: 0.7376, Train: 0.7589, Test: 0.5327\n",
            "Early stopping:  0.012863599541787188\n",
            "Epoch: 119, Loss: 0.7332, Train: 0.7606, Test: 0.5315\n",
            "Early stopping:  0.010815360307237459\n",
            "Epoch: 120, Loss: 0.7230, Train: 0.7612, Test: 0.5310\n",
            "Early stopping:  0.010599041596319948\n",
            "Epoch: 121, Loss: 0.7176, Train: 0.7623, Test: 0.5313\n",
            "Early stopping:  0.010368083951332532\n",
            "Epoch: 122, Loss: 0.7120, Train: 0.7737, Test: 0.5353\n",
            "Early stopping:  0.010630409573945745\n",
            "Epoch: 123, Loss: 0.7073, Train: 0.7754, Test: 0.5339\n",
            "Early stopping:  0.010035444419574465\n",
            "Epoch: 124, Loss: 0.7013, Train: 0.7674, Test: 0.5339\n",
            "Early stopping:  0.008470445950676834\n",
            "PREDICTIONS -> tensor([ 9,  8, 11,  ..., 11,  7,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.56      0.50      0.53       758\n",
            "         capital_goods       0.33      0.35      0.34       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.45      0.43      0.44       793\n",
            " consumer_non-cyclical       0.74      0.34      0.46       446\n",
            "                energy       0.57      0.53      0.55       283\n",
            "             financial       0.59      0.58      0.59       767\n",
            "            healthcare       0.53      0.53      0.53       318\n",
            "              services       0.55      0.70      0.61      2076\n",
            "            technology       0.46      0.32      0.38       396\n",
            "        transportation       0.62      0.63      0.62       404\n",
            "             utilities       0.62      0.58      0.60       225\n",
            "\n",
            "              accuracy                           0.53      7054\n",
            "             macro avg       0.50      0.46      0.47      7054\n",
            "          weighted avg       0.53      0.53      0.53      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 60.1007, Train: 0.2989, Test: 0.2875\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 197.2133, Train: 0.0874, Test: 0.0849\n",
            "Early stopping:  96.95331826067267\n",
            "Epoch: 003, Loss: 291.4950, Train: 0.1072, Test: 0.1067\n",
            "Early stopping:  116.35596485296387\n",
            "Epoch: 004, Loss: 381.2415, Train: 0.0726, Test: 0.0688\n",
            "Early stopping:  137.32092904607725\n",
            "Epoch: 005, Loss: 392.6317, Train: 0.0425, Test: 0.0418\n",
            "Early stopping:  138.8178648121147\n",
            "Epoch: 006, Loss: 374.2296, Train: 0.1123, Test: 0.1087\n",
            "Early stopping:  83.04337303673562\n",
            "Epoch: 007, Loss: 315.9588, Train: 0.1140, Test: 0.1138\n",
            "Early stopping:  44.59826458720362\n",
            "Epoch: 008, Loss: 205.6164, Train: 0.2450, Test: 0.2441\n",
            "Early stopping:  77.6138627006189\n",
            "Epoch: 009, Loss: 100.6198, Train: 0.2689, Test: 0.2655\n",
            "Early stopping:  123.02882020944807\n",
            "Epoch: 010, Loss: 93.2674, Train: 0.2246, Test: 0.2126\n",
            "Early stopping:  125.98935906674977\n",
            "Epoch: 011, Loss: 78.1750, Train: 0.2507, Test: 0.2474\n",
            "Early stopping:  101.32928018786856\n",
            "Epoch: 012, Loss: 56.2123, Train: 0.0970, Test: 0.0895\n",
            "Early stopping:  57.802494571382894\n",
            "Epoch: 013, Loss: 48.6099, Train: 0.1758, Test: 0.1620\n",
            "Early stopping:  22.632154873124943\n",
            "Epoch: 014, Loss: 31.5237, Train: 0.2683, Test: 0.2549\n",
            "Early stopping:  24.39210976149443\n",
            "Epoch: 015, Loss: 19.7732, Train: 0.3063, Test: 0.3011\n",
            "Early stopping:  22.583464627678687\n",
            "Epoch: 016, Loss: 9.0072, Train: 0.3046, Test: 0.2906\n",
            "Early stopping:  19.591180289586376\n",
            "Epoch: 017, Loss: 4.7740, Train: 0.2728, Test: 0.2504\n",
            "Early stopping:  17.794485272370267\n",
            "Epoch: 018, Loss: 3.6145, Train: 0.2496, Test: 0.2316\n",
            "Early stopping:  11.810760750443443\n",
            "Epoch: 019, Loss: 2.9965, Train: 0.2314, Test: 0.2234\n",
            "Early stopping:  6.969457054542234\n",
            "Epoch: 020, Loss: 2.6395, Train: 0.2360, Test: 0.2331\n",
            "Early stopping:  2.590347080932696\n",
            "Epoch: 021, Loss: 2.4595, Train: 0.2258, Test: 0.2223\n",
            "Early stopping:  0.936237321266437\n",
            "Epoch: 022, Loss: 2.3869, Train: 0.2235, Test: 0.2263\n",
            "Early stopping:  0.5031126274822314\n",
            "Epoch: 023, Loss: 2.3020, Train: 0.2252, Test: 0.2343\n",
            "Early stopping:  0.2754229451066178\n",
            "Epoch: 024, Loss: 2.2382, Train: 0.2354, Test: 0.2418\n",
            "Early stopping:  0.15551721552946607\n",
            "Epoch: 025, Loss: 2.2087, Train: 0.2399, Test: 0.2399\n",
            "Early stopping:  0.10410657568945784\n",
            "Epoch: 026, Loss: 2.1969, Train: 0.2484, Test: 0.2488\n",
            "Early stopping:  0.0786536709772219\n",
            "Epoch: 027, Loss: 2.1914, Train: 0.2558, Test: 0.2516\n",
            "Early stopping:  0.04543069365028969\n",
            "Epoch: 028, Loss: 2.1829, Train: 0.2655, Test: 0.2613\n",
            "Early stopping:  0.021458839843950833\n",
            "Epoch: 029, Loss: 2.1620, Train: 0.2672, Test: 0.2689\n",
            "Early stopping:  0.017470481109266967\n",
            "Epoch: 030, Loss: 2.1294, Train: 0.2853, Test: 0.2912\n",
            "Early stopping:  0.02753032837178681\n",
            "Epoch: 031, Loss: 2.0958, Train: 0.3097, Test: 0.3143\n",
            "Early stopping:  0.03960492462761473\n",
            "Epoch: 032, Loss: 2.0756, Train: 0.3137, Test: 0.3170\n",
            "Early stopping:  0.044549002124286086\n",
            "Epoch: 033, Loss: 2.0758, Train: 0.3205, Test: 0.3183\n",
            "Early stopping:  0.037420185793070423\n",
            "Epoch: 034, Loss: 2.0636, Train: 0.3199, Test: 0.3193\n",
            "Early stopping:  0.025843295705224808\n",
            "Epoch: 035, Loss: 2.0392, Train: 0.3188, Test: 0.3214\n",
            "Early stopping:  0.02076108751826523\n",
            "Epoch: 036, Loss: 2.0176, Train: 0.3313, Test: 0.3256\n",
            "Early stopping:  0.025407855034970957\n",
            "Epoch: 037, Loss: 2.0041, Train: 0.3432, Test: 0.3316\n",
            "Early stopping:  0.030144120186453773\n",
            "Epoch: 038, Loss: 1.9935, Train: 0.3409, Test: 0.3317\n",
            "Early stopping:  0.028137558949991336\n",
            "Epoch: 039, Loss: 1.9817, Train: 0.3534, Test: 0.3341\n",
            "Early stopping:  0.022232400284366332\n",
            "Epoch: 040, Loss: 1.9666, Train: 0.3494, Test: 0.3333\n",
            "Early stopping:  0.019688653520989963\n",
            "Epoch: 041, Loss: 1.9503, Train: 0.3381, Test: 0.3234\n",
            "Early stopping:  0.02134366516488215\n",
            "Epoch: 042, Loss: 1.9359, Train: 0.3267, Test: 0.3202\n",
            "Early stopping:  0.023202221124726998\n",
            "Epoch: 043, Loss: 1.9255, Train: 0.3318, Test: 0.3222\n",
            "Early stopping:  0.022679404281263155\n",
            "Epoch: 044, Loss: 1.9119, Train: 0.3324, Test: 0.3296\n",
            "Early stopping:  0.02127720128391812\n",
            "Epoch: 045, Loss: 1.8951, Train: 0.3488, Test: 0.3407\n",
            "Early stopping:  0.021322042326877497\n",
            "Epoch: 046, Loss: 1.8814, Train: 0.3432, Test: 0.3442\n",
            "Early stopping:  0.02212980978895921\n",
            "Epoch: 047, Loss: 1.8700, Train: 0.3426, Test: 0.3434\n",
            "Early stopping:  0.022415753340208178\n",
            "Epoch: 048, Loss: 1.8597, Train: 0.3415, Test: 0.3438\n",
            "Early stopping:  0.02056391361595461\n",
            "Epoch: 049, Loss: 1.8466, Train: 0.3449, Test: 0.3441\n",
            "Early stopping:  0.018792736329834553\n",
            "Epoch: 050, Loss: 1.8329, Train: 0.3517, Test: 0.3479\n",
            "Early stopping:  0.01907744906879125\n",
            "Epoch: 051, Loss: 1.8199, Train: 0.3568, Test: 0.3533\n",
            "Early stopping:  0.020114498539870688\n",
            "Epoch: 052, Loss: 1.8084, Train: 0.3619, Test: 0.3571\n",
            "Early stopping:  0.02046581331095435\n",
            "Epoch: 053, Loss: 1.7951, Train: 0.3687, Test: 0.3642\n",
            "Early stopping:  0.02016240529026452\n",
            "Epoch: 054, Loss: 1.7816, Train: 0.3693, Test: 0.3669\n",
            "Early stopping:  0.020149895496744098\n",
            "Epoch: 055, Loss: 1.7701, Train: 0.3902, Test: 0.3805\n",
            "Early stopping:  0.020002623066409\n",
            "Epoch: 056, Loss: 1.7597, Train: 0.4005, Test: 0.3856\n",
            "Early stopping:  0.019367810795233496\n",
            "Epoch: 057, Loss: 1.7488, Train: 0.4084, Test: 0.3881\n",
            "Early stopping:  0.01811457295078025\n",
            "Epoch: 058, Loss: 1.7376, Train: 0.4078, Test: 0.3904\n",
            "Early stopping:  0.017269008696075106\n",
            "Epoch: 059, Loss: 1.7273, Train: 0.4112, Test: 0.3910\n",
            "Early stopping:  0.0170315314901596\n",
            "Epoch: 060, Loss: 1.7176, Train: 0.4101, Test: 0.3887\n",
            "Early stopping:  0.01671900383198322\n",
            "Epoch: 061, Loss: 1.7074, Train: 0.4158, Test: 0.3889\n",
            "Early stopping:  0.016259645463780947\n",
            "Epoch: 062, Loss: 1.6969, Train: 0.4158, Test: 0.3910\n",
            "Early stopping:  0.01601426391451424\n",
            "Epoch: 063, Loss: 1.6871, Train: 0.4197, Test: 0.3916\n",
            "Early stopping:  0.015995287815358528\n",
            "Epoch: 064, Loss: 1.6759, Train: 0.4214, Test: 0.3944\n",
            "Early stopping:  0.016413953482576987\n",
            "Epoch: 065, Loss: 1.6653, Train: 0.4311, Test: 0.3998\n",
            "Early stopping:  0.01664324910017094\n",
            "Epoch: 066, Loss: 1.6546, Train: 0.4317, Test: 0.4022\n",
            "Early stopping:  0.0168291275332848\n",
            "Epoch: 067, Loss: 1.6441, Train: 0.4339, Test: 0.4040\n",
            "Early stopping:  0.016973037691751344\n",
            "Epoch: 068, Loss: 1.6334, Train: 0.4368, Test: 0.4047\n",
            "Early stopping:  0.01680906519509202\n",
            "Epoch: 069, Loss: 1.6240, Train: 0.4390, Test: 0.4074\n",
            "Early stopping:  0.016420576160772936\n",
            "Epoch: 070, Loss: 1.6138, Train: 0.4419, Test: 0.4111\n",
            "Early stopping:  0.01608260227348149\n",
            "Epoch: 071, Loss: 1.6033, Train: 0.4436, Test: 0.4111\n",
            "Early stopping:  0.01598158726941437\n",
            "Epoch: 072, Loss: 1.5930, Train: 0.4492, Test: 0.4161\n",
            "Early stopping:  0.01604567193456876\n",
            "Epoch: 073, Loss: 1.5814, Train: 0.4521, Test: 0.4181\n",
            "Early stopping:  0.016750257486174872\n",
            "Epoch: 074, Loss: 1.5719, Train: 0.4577, Test: 0.4203\n",
            "Early stopping:  0.016712935118156858\n",
            "Epoch: 075, Loss: 1.5585, Train: 0.4611, Test: 0.4236\n",
            "Early stopping:  0.017532741994455012\n",
            "Epoch: 076, Loss: 1.5470, Train: 0.4697, Test: 0.4290\n",
            "Early stopping:  0.01818352645351908\n",
            "Epoch: 077, Loss: 1.5369, Train: 0.4748, Test: 0.4327\n",
            "Early stopping:  0.018049891854621553\n",
            "Epoch: 078, Loss: 1.5257, Train: 0.4770, Test: 0.4351\n",
            "Early stopping:  0.018043856388586212\n",
            "Epoch: 079, Loss: 1.5171, Train: 0.4799, Test: 0.4355\n",
            "Early stopping:  0.016482557840541166\n",
            "Epoch: 080, Loss: 1.5047, Train: 0.4827, Test: 0.4398\n",
            "Early stopping:  0.016528497940923856\n",
            "Epoch: 081, Loss: 1.4945, Train: 0.4872, Test: 0.4405\n",
            "Early stopping:  0.0167284758052542\n",
            "Epoch: 082, Loss: 1.4839, Train: 0.4895, Test: 0.4413\n",
            "Early stopping:  0.016794553525774705\n",
            "Epoch: 083, Loss: 1.4715, Train: 0.4952, Test: 0.4430\n",
            "Early stopping:  0.017689484203702047\n",
            "Epoch: 084, Loss: 1.4587, Train: 0.5048, Test: 0.4464\n",
            "Early stopping:  0.018217796210317473\n",
            "Epoch: 085, Loss: 1.4440, Train: 0.5037, Test: 0.4511\n",
            "Early stopping:  0.01999892348323445\n",
            "Epoch: 086, Loss: 1.4361, Train: 0.5145, Test: 0.4552\n",
            "Early stopping:  0.01955242696620506\n",
            "Epoch: 087, Loss: 1.4221, Train: 0.5190, Test: 0.4552\n",
            "Early stopping:  0.01925930918072768\n",
            "Epoch: 088, Loss: 1.4094, Train: 0.5230, Test: 0.4569\n",
            "Early stopping:  0.019089939824025175\n",
            "Epoch: 089, Loss: 1.3987, Train: 0.5230, Test: 0.4589\n",
            "Early stopping:  0.01860394630783243\n",
            "Epoch: 090, Loss: 1.3908, Train: 0.5292, Test: 0.4606\n",
            "Early stopping:  0.018129820643550516\n",
            "Epoch: 091, Loss: 1.3788, Train: 0.5315, Test: 0.4630\n",
            "Early stopping:  0.016691072324683087\n",
            "Epoch: 092, Loss: 1.3679, Train: 0.5337, Test: 0.4660\n",
            "Early stopping:  0.016296379487787265\n",
            "Epoch: 093, Loss: 1.3585, Train: 0.5355, Test: 0.4667\n",
            "Early stopping:  0.016348007403699186\n",
            "Epoch: 094, Loss: 1.3527, Train: 0.5366, Test: 0.4675\n",
            "Early stopping:  0.015376300183179857\n",
            "Epoch: 095, Loss: 1.3393, Train: 0.5445, Test: 0.4714\n",
            "Early stopping:  0.014993793769832977\n",
            "Epoch: 096, Loss: 1.3309, Train: 0.5462, Test: 0.4705\n",
            "Early stopping:  0.014808889041543179\n",
            "Epoch: 097, Loss: 1.3231, Train: 0.5485, Test: 0.4714\n",
            "Early stopping:  0.014719175858027601\n",
            "Epoch: 098, Loss: 1.3120, Train: 0.5525, Test: 0.4698\n",
            "Early stopping:  0.015481147390132588\n",
            "Epoch: 099, Loss: 1.3046, Train: 0.5576, Test: 0.4735\n",
            "Early stopping:  0.01398419086431264\n",
            "Epoch: 100, Loss: 1.2931, Train: 0.5581, Test: 0.4729\n",
            "Early stopping:  0.014927802303111602\n",
            "Epoch: 101, Loss: 1.2856, Train: 0.5553, Test: 0.4741\n",
            "Early stopping:  0.014887453799691133\n",
            "Epoch: 102, Loss: 1.2797, Train: 0.5649, Test: 0.4738\n",
            "Early stopping:  0.013323351894514305\n",
            "Epoch: 103, Loss: 1.2698, Train: 0.5661, Test: 0.4742\n",
            "Early stopping:  0.013220005945854861\n",
            "Epoch: 104, Loss: 1.2643, Train: 0.5666, Test: 0.4763\n",
            "Early stopping:  0.011644770019559087\n",
            "Epoch: 105, Loss: 1.2556, Train: 0.5735, Test: 0.4775\n",
            "Early stopping:  0.011950945871107886\n",
            "Epoch: 106, Loss: 1.2480, Train: 0.5763, Test: 0.4799\n",
            "Early stopping:  0.012295504981787223\n",
            "Epoch: 107, Loss: 1.2405, Train: 0.5763, Test: 0.4802\n",
            "Early stopping:  0.011867071256328552\n",
            "Epoch: 108, Loss: 1.2329, Train: 0.5831, Test: 0.4823\n",
            "Early stopping:  0.012317374535001174\n",
            "Epoch: 109, Loss: 1.2237, Train: 0.5831, Test: 0.4843\n",
            "Early stopping:  0.012474985468800763\n",
            "Epoch: 110, Loss: 1.2177, Train: 0.5882, Test: 0.4843\n",
            "Early stopping:  0.01224404165045327\n",
            "Epoch: 111, Loss: 1.2138, Train: 0.5905, Test: 0.4881\n",
            "Early stopping:  0.010960346923864496\n",
            "Epoch: 112, Loss: 1.2003, Train: 0.5882, Test: 0.4891\n",
            "Early stopping:  0.012117722675955563\n",
            "Epoch: 113, Loss: 1.1984, Train: 0.5910, Test: 0.4844\n",
            "Early stopping:  0.011059528318188495\n",
            "Epoch: 114, Loss: 1.2020, Train: 0.5973, Test: 0.4875\n",
            "Early stopping:  0.008718129994888838\n",
            "PREDICTIONS -> tensor([ 9,  6,  0,  ..., 11,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.40      0.57      0.47       758\n",
            "         capital_goods       0.60      0.23      0.33       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.44      0.43      0.43       793\n",
            " consumer_non-cyclical       0.47      0.41      0.44       446\n",
            "                energy       0.56      0.24      0.34       283\n",
            "             financial       0.59      0.53      0.56       767\n",
            "            healthcare       0.59      0.36      0.45       318\n",
            "              services       0.49      0.73      0.59      2076\n",
            "            technology       0.33      0.23      0.27       396\n",
            "        transportation       0.72      0.44      0.54       404\n",
            "             utilities       0.28      0.02      0.04       225\n",
            "\n",
            "              accuracy                           0.49      7054\n",
            "             macro avg       0.46      0.35      0.37      7054\n",
            "          weighted avg       0.49      0.49      0.47      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 114.0132, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 329.4425, Train: 0.0391, Test: 0.0411\n",
            "Early stopping:  152.33151776552174\n",
            "Epoch: 003, Loss: 316.3992, Train: 0.1208, Test: 0.1255\n",
            "Early stopping:  120.789073009158\n",
            "Epoch: 004, Loss: 322.5912, Train: 0.0743, Test: 0.0724\n",
            "Early stopping:  104.53470554620843\n",
            "Epoch: 005, Loss: 321.7708, Train: 0.1316, Test: 0.1267\n",
            "Early stopping:  93.37602372540998\n",
            "Epoch: 006, Loss: 341.7224, Train: 0.0902, Test: 0.0906\n",
            "Early stopping:  9.746590539606425\n",
            "Epoch: 007, Loss: 306.6445, Train: 0.1395, Test: 0.1337\n",
            "Early stopping:  12.80998278597064\n",
            "Epoch: 008, Loss: 332.5313, Train: 0.1316, Test: 0.1222\n",
            "Early stopping:  13.129161059185508\n",
            "Epoch: 009, Loss: 328.1463, Train: 0.2876, Test: 0.2862\n",
            "Early stopping:  13.103878173558249\n",
            "Epoch: 010, Loss: 397.5221, Train: 0.1560, Test: 0.1523\n",
            "Early stopping:  33.955910077534085\n",
            "Epoch: 011, Loss: 424.8599, Train: 0.2995, Test: 0.2977\n",
            "Early stopping:  50.52105025666785\n",
            "Epoch: 012, Loss: 396.6573, Train: 0.2564, Test: 0.2614\n",
            "Early stopping:  43.17609027627431\n",
            "Epoch: 013, Loss: 340.7597, Train: 0.0669, Test: 0.0740\n",
            "Early stopping:  41.22025796545729\n",
            "Epoch: 014, Loss: 321.1220, Train: 0.2847, Test: 0.2874\n",
            "Early stopping:  43.389270264681414\n",
            "Epoch: 015, Loss: 205.5362, Train: 0.3296, Test: 0.3234\n",
            "Early stopping:  84.88849498331692\n",
            "Epoch: 016, Loss: 146.1649, Train: 0.2484, Test: 0.2458\n",
            "Early stopping:  102.98956124586009\n",
            "Epoch: 017, Loss: 129.6493, Train: 0.2422, Test: 0.2331\n",
            "Early stopping:  97.79933648939927\n",
            "Epoch: 018, Loss: 95.5679, Train: 0.2683, Test: 0.2569\n",
            "Early stopping:  88.5656946961651\n",
            "Epoch: 019, Loss: 62.0935, Train: 0.2240, Test: 0.2263\n",
            "Early stopping:  54.1753119526422\n",
            "Epoch: 020, Loss: 48.3643, Train: 0.2343, Test: 0.2272\n",
            "Early stopping:  42.033050713576394\n",
            "Epoch: 021, Loss: 30.2226, Train: 0.2717, Test: 0.2607\n",
            "Early stopping:  39.60450012832111\n",
            "Epoch: 022, Loss: 12.2495, Train: 0.3625, Test: 0.3409\n",
            "Early stopping:  31.785064524715736\n",
            "Epoch: 023, Loss: 3.9330, Train: 0.3307, Test: 0.3068\n",
            "Early stopping:  24.25096669928588\n",
            "Epoch: 024, Loss: 3.4265, Train: 0.2859, Test: 0.2740\n",
            "Early stopping:  19.373190608354577\n",
            "Epoch: 025, Loss: 3.2392, Train: 0.2796, Test: 0.2720\n",
            "Early stopping:  11.595832341672617\n",
            "Epoch: 026, Loss: 3.0181, Train: 0.2819, Test: 0.2773\n",
            "Early stopping:  3.970106433275109\n",
            "Epoch: 027, Loss: 2.8117, Train: 0.2961, Test: 0.2840\n",
            "Early stopping:  0.42934579132546746\n",
            "Epoch: 028, Loss: 2.6473, Train: 0.3006, Test: 0.2891\n",
            "Early stopping:  0.31435542621826507\n",
            "Epoch: 029, Loss: 2.5194, Train: 0.3018, Test: 0.2917\n",
            "Early stopping:  0.28792096238441145\n",
            "Epoch: 030, Loss: 2.4153, Train: 0.2995, Test: 0.2952\n",
            "Early stopping:  0.23903161172901558\n",
            "Epoch: 031, Loss: 2.3335, Train: 0.3097, Test: 0.2995\n",
            "Early stopping:  0.18959629824798885\n",
            "Epoch: 032, Loss: 2.2834, Train: 0.3069, Test: 0.3018\n",
            "Early stopping:  0.14641533949571225\n",
            "Epoch: 033, Loss: 2.2532, Train: 0.3074, Test: 0.2987\n",
            "Early stopping:  0.10772535169732295\n",
            "Epoch: 034, Loss: 2.2284, Train: 0.3029, Test: 0.2939\n",
            "Early stopping:  0.07410191133529344\n",
            "Epoch: 035, Loss: 2.2066, Train: 0.2984, Test: 0.2896\n",
            "Early stopping:  0.04959260144012563\n",
            "Epoch: 036, Loss: 2.1887, Train: 0.2978, Test: 0.2861\n",
            "Early stopping:  0.03750530004033254\n",
            "Epoch: 037, Loss: 2.1740, Train: 0.2961, Test: 0.2807\n",
            "Early stopping:  0.03150130654661162\n",
            "Epoch: 038, Loss: 2.1588, Train: 0.3023, Test: 0.2790\n",
            "Early stopping:  0.027275136324861737\n",
            "Epoch: 039, Loss: 2.1411, Train: 0.3001, Test: 0.2854\n",
            "Early stopping:  0.025446243893611272\n",
            "Epoch: 040, Loss: 2.1212, Train: 0.3029, Test: 0.2844\n",
            "Early stopping:  0.026603077859313075\n",
            "Epoch: 041, Loss: 2.1030, Train: 0.3040, Test: 0.2893\n",
            "Early stopping:  0.028426476718583747\n",
            "Epoch: 042, Loss: 2.0883, Train: 0.3108, Test: 0.2956\n",
            "Early stopping:  0.0283623331289554\n",
            "Epoch: 043, Loss: 2.0747, Train: 0.3159, Test: 0.3017\n",
            "Early stopping:  0.02632081792850151\n",
            "Epoch: 044, Loss: 2.0615, Train: 0.3216, Test: 0.3055\n",
            "Early stopping:  0.023416881902760995\n",
            "Epoch: 045, Loss: 2.0493, Train: 0.3313, Test: 0.3102\n",
            "Early stopping:  0.021215095007649214\n",
            "Epoch: 046, Loss: 2.0362, Train: 0.3403, Test: 0.3176\n",
            "Early stopping:  0.02046322995949204\n",
            "Epoch: 047, Loss: 2.0227, Train: 0.3392, Test: 0.3259\n",
            "Early stopping:  0.02044632609171561\n",
            "Epoch: 048, Loss: 2.0082, Train: 0.3409, Test: 0.3302\n",
            "Early stopping:  0.021091592144761126\n",
            "Epoch: 049, Loss: 1.9926, Train: 0.3517, Test: 0.3407\n",
            "Early stopping:  0.022390030148994242\n",
            "Epoch: 050, Loss: 1.9763, Train: 0.3642, Test: 0.3507\n",
            "Early stopping:  0.023731481085189993\n",
            "Epoch: 051, Loss: 1.9598, Train: 0.3795, Test: 0.3588\n",
            "Early stopping:  0.02493503360971542\n",
            "Epoch: 052, Loss: 1.9443, Train: 0.3863, Test: 0.3667\n",
            "Early stopping:  0.025388696144446415\n",
            "Epoch: 053, Loss: 1.9294, Train: 0.3971, Test: 0.3741\n",
            "Early stopping:  0.025048958895731346\n",
            "Epoch: 054, Loss: 1.9149, Train: 0.4022, Test: 0.3789\n",
            "Early stopping:  0.024216733683561142\n",
            "Epoch: 055, Loss: 1.9012, Train: 0.4078, Test: 0.3872\n",
            "Early stopping:  0.023172315937599375\n",
            "Epoch: 056, Loss: 1.8887, Train: 0.4192, Test: 0.3900\n",
            "Early stopping:  0.02206404563900095\n",
            "Epoch: 057, Loss: 1.8773, Train: 0.4260, Test: 0.3931\n",
            "Early stopping:  0.02064818516144977\n",
            "Epoch: 058, Loss: 1.8668, Train: 0.4254, Test: 0.3976\n",
            "Early stopping:  0.01901292131279632\n",
            "Epoch: 059, Loss: 1.8575, Train: 0.4288, Test: 0.4003\n",
            "Early stopping:  0.017288610197130815\n",
            "Epoch: 060, Loss: 1.8490, Train: 0.4305, Test: 0.4053\n",
            "Early stopping:  0.015704411869708255\n",
            "Epoch: 061, Loss: 1.8409, Train: 0.4396, Test: 0.4091\n",
            "Early stopping:  0.014350264442295885\n",
            "Epoch: 062, Loss: 1.8324, Train: 0.4458, Test: 0.4113\n",
            "Early stopping:  0.013522572572402422\n",
            "Epoch: 063, Loss: 1.8232, Train: 0.4481, Test: 0.4137\n",
            "Early stopping:  0.01347104687136926\n",
            "Epoch: 064, Loss: 1.8136, Train: 0.4526, Test: 0.4154\n",
            "Early stopping:  0.013976371438991015\n",
            "Epoch: 065, Loss: 1.8039, Train: 0.4566, Test: 0.4172\n",
            "Early stopping:  0.014677353550854212\n",
            "Epoch: 066, Loss: 1.7944, Train: 0.4589, Test: 0.4203\n",
            "Early stopping:  0.015077047015588468\n",
            "Epoch: 067, Loss: 1.7849, Train: 0.4583, Test: 0.4212\n",
            "Early stopping:  0.01517028334105339\n",
            "Epoch: 068, Loss: 1.7749, Train: 0.4645, Test: 0.4220\n",
            "Early stopping:  0.015248681716406025\n",
            "Epoch: 069, Loss: 1.7647, Train: 0.4634, Test: 0.4240\n",
            "Early stopping:  0.015478908002303306\n",
            "Epoch: 070, Loss: 1.7543, Train: 0.4668, Test: 0.4267\n",
            "Early stopping:  0.015865205540432675\n",
            "Epoch: 071, Loss: 1.7439, Train: 0.4697, Test: 0.4311\n",
            "Early stopping:  0.016213646549815316\n",
            "Epoch: 072, Loss: 1.7337, Train: 0.4725, Test: 0.4344\n",
            "Early stopping:  0.01631735969153939\n",
            "Epoch: 073, Loss: 1.7238, Train: 0.4782, Test: 0.4380\n",
            "Early stopping:  0.016193019886375507\n",
            "Epoch: 074, Loss: 1.7140, Train: 0.4804, Test: 0.4400\n",
            "Early stopping:  0.01594393993571943\n",
            "Epoch: 075, Loss: 1.7043, Train: 0.4804, Test: 0.4416\n",
            "Early stopping:  0.015637266964959918\n",
            "Epoch: 076, Loss: 1.6948, Train: 0.4861, Test: 0.4444\n",
            "Early stopping:  0.015368328426927577\n",
            "Epoch: 077, Loss: 1.6855, Train: 0.4867, Test: 0.4478\n",
            "Early stopping:  0.015128038044790027\n",
            "Epoch: 078, Loss: 1.6765, Train: 0.4901, Test: 0.4502\n",
            "Early stopping:  0.01484034467861564\n",
            "Epoch: 079, Loss: 1.6673, Train: 0.4895, Test: 0.4524\n",
            "Early stopping:  0.01461159006284525\n",
            "Epoch: 080, Loss: 1.6584, Train: 0.4878, Test: 0.4536\n",
            "Early stopping:  0.014396659415913832\n",
            "Epoch: 081, Loss: 1.6496, Train: 0.4884, Test: 0.4558\n",
            "Early stopping:  0.014209412556423773\n",
            "Epoch: 082, Loss: 1.6409, Train: 0.4906, Test: 0.4583\n",
            "Early stopping:  0.014045607426691281\n",
            "Epoch: 083, Loss: 1.6324, Train: 0.4935, Test: 0.4595\n",
            "Early stopping:  0.013812702554378324\n",
            "Epoch: 084, Loss: 1.6241, Train: 0.4963, Test: 0.4613\n",
            "Early stopping:  0.013579991306854397\n",
            "Epoch: 085, Loss: 1.6157, Train: 0.5014, Test: 0.4616\n",
            "Early stopping:  0.013398297716310858\n",
            "Epoch: 086, Loss: 1.6076, Train: 0.5020, Test: 0.4640\n",
            "Early stopping:  0.013186210267866604\n",
            "Epoch: 087, Loss: 1.5996, Train: 0.5065, Test: 0.4663\n",
            "Early stopping:  0.012976232187389662\n",
            "Epoch: 088, Loss: 1.5918, Train: 0.5099, Test: 0.4673\n",
            "Early stopping:  0.012732553887279561\n",
            "Epoch: 089, Loss: 1.5842, Train: 0.5105, Test: 0.4704\n",
            "Early stopping:  0.012433872687784115\n",
            "Epoch: 090, Loss: 1.5767, Train: 0.5133, Test: 0.4705\n",
            "Early stopping:  0.012205302834382146\n",
            "Epoch: 091, Loss: 1.5694, Train: 0.5162, Test: 0.4712\n",
            "Early stopping:  0.011962634850200993\n",
            "Epoch: 092, Loss: 1.5621, Train: 0.5190, Test: 0.4722\n",
            "Early stopping:  0.011763569120215983\n",
            "Epoch: 093, Loss: 1.5548, Train: 0.5213, Test: 0.4736\n",
            "Early stopping:  0.011633358293917087\n",
            "Epoch: 094, Loss: 1.5474, Train: 0.5230, Test: 0.4735\n",
            "Early stopping:  0.011586203391525801\n",
            "Epoch: 095, Loss: 1.5398, Train: 0.5269, Test: 0.4753\n",
            "Early stopping:  0.011661626344969275\n",
            "Epoch: 096, Loss: 1.5322, Train: 0.5315, Test: 0.4762\n",
            "Early stopping:  0.011796533561526615\n",
            "Epoch: 097, Loss: 1.5248, Train: 0.5360, Test: 0.4769\n",
            "Early stopping:  0.011871822282382838\n",
            "Epoch: 098, Loss: 1.5172, Train: 0.5360, Test: 0.4789\n",
            "Early stopping:  0.011923072552296509\n",
            "Epoch: 099, Loss: 1.5096, Train: 0.5377, Test: 0.4803\n",
            "Early stopping:  0.011942672960063464\n",
            "Epoch: 100, Loss: 1.5021, Train: 0.5389, Test: 0.4817\n",
            "Early stopping:  0.011928879178744073\n",
            "Epoch: 101, Loss: 1.4946, Train: 0.5411, Test: 0.4837\n",
            "Early stopping:  0.011922527275522959\n",
            "Epoch: 102, Loss: 1.4872, Train: 0.5411, Test: 0.4850\n",
            "Early stopping:  0.01185751973272314\n",
            "Epoch: 103, Loss: 1.4799, Train: 0.5423, Test: 0.4867\n",
            "Early stopping:  0.011741142041842198\n",
            "Epoch: 104, Loss: 1.4728, Train: 0.5434, Test: 0.4888\n",
            "Early stopping:  0.011595336068582424\n",
            "Epoch: 105, Loss: 1.4657, Train: 0.5440, Test: 0.4905\n",
            "Early stopping:  0.011417499690152952\n",
            "Epoch: 106, Loss: 1.4587, Train: 0.5457, Test: 0.4911\n",
            "Early stopping:  0.011255698587520667\n",
            "Epoch: 107, Loss: 1.4517, Train: 0.5468, Test: 0.4911\n",
            "Early stopping:  0.011166300997876256\n",
            "Epoch: 108, Loss: 1.4445, Train: 0.5462, Test: 0.4921\n",
            "Early stopping:  0.011153936246279618\n",
            "Epoch: 109, Loss: 1.4370, Train: 0.5513, Test: 0.4923\n",
            "Early stopping:  0.011306155759163864\n",
            "Epoch: 110, Loss: 1.4298, Train: 0.5519, Test: 0.4932\n",
            "Early stopping:  0.011453806870747804\n",
            "Epoch: 111, Loss: 1.4220, Train: 0.5525, Test: 0.4929\n",
            "Early stopping:  0.011699894241138506\n",
            "Epoch: 112, Loss: 1.4141, Train: 0.5525, Test: 0.4928\n",
            "Early stopping:  0.012003519334837766\n",
            "Epoch: 113, Loss: 1.4075, Train: 0.5553, Test: 0.4936\n",
            "Early stopping:  0.011817083141487899\n",
            "Epoch: 114, Loss: 1.3997, Train: 0.5593, Test: 0.4946\n",
            "Early stopping:  0.011811507148777732\n",
            "Epoch: 115, Loss: 1.3928, Train: 0.5632, Test: 0.4940\n",
            "Early stopping:  0.011530888346731064\n",
            "Epoch: 116, Loss: 1.3857, Train: 0.5627, Test: 0.4953\n",
            "Early stopping:  0.011293252303347042\n",
            "Epoch: 117, Loss: 1.3790, Train: 0.5638, Test: 0.4949\n",
            "Early stopping:  0.011241680731187784\n",
            "Epoch: 118, Loss: 1.3716, Train: 0.5649, Test: 0.4950\n",
            "Early stopping:  0.011057380937365862\n",
            "Epoch: 119, Loss: 1.3647, Train: 0.5632, Test: 0.4952\n",
            "Early stopping:  0.011099509791868137\n",
            "Epoch: 120, Loss: 1.3576, Train: 0.5655, Test: 0.4957\n",
            "Early stopping:  0.011137336232201657\n",
            "Epoch: 121, Loss: 1.3504, Train: 0.5649, Test: 0.4938\n",
            "Early stopping:  0.01125444495955605\n",
            "Epoch: 122, Loss: 1.3447, Train: 0.5735, Test: 0.4979\n",
            "Early stopping:  0.010790001039595244\n",
            "Epoch: 123, Loss: 1.3372, Train: 0.5757, Test: 0.4990\n",
            "Early stopping:  0.01076744051673091\n",
            "Epoch: 124, Loss: 1.3303, Train: 0.5774, Test: 0.4973\n",
            "Early stopping:  0.010731995660201357\n",
            "Epoch: 125, Loss: 1.3234, Train: 0.5740, Test: 0.4977\n",
            "Early stopping:  0.010822902316341955\n",
            "Epoch: 126, Loss: 1.3169, Train: 0.5752, Test: 0.4990\n",
            "Early stopping:  0.010987960252192302\n",
            "Epoch: 127, Loss: 1.3094, Train: 0.5786, Test: 0.4997\n",
            "Early stopping:  0.010925464807144658\n",
            "Epoch: 128, Loss: 1.3024, Train: 0.5797, Test: 0.4980\n",
            "Early stopping:  0.011037632054911385\n",
            "Epoch: 129, Loss: 1.2966, Train: 0.5814, Test: 0.5001\n",
            "Early stopping:  0.010753827441765777\n",
            "Epoch: 130, Loss: 1.2898, Train: 0.5825, Test: 0.4999\n",
            "Early stopping:  0.010590481383969192\n",
            "Epoch: 131, Loss: 1.2829, Train: 0.5831, Test: 0.5001\n",
            "Early stopping:  0.010371824034337156\n",
            "Epoch: 132, Loss: 1.2766, Train: 0.5837, Test: 0.5016\n",
            "Early stopping:  0.010330986193615565\n",
            "Epoch: 133, Loss: 1.2700, Train: 0.5848, Test: 0.5021\n",
            "Early stopping:  0.010513000799172504\n",
            "Epoch: 134, Loss: 1.2637, Train: 0.5888, Test: 0.5023\n",
            "Early stopping:  0.010310218075262409\n",
            "Epoch: 135, Loss: 1.2577, Train: 0.5893, Test: 0.5034\n",
            "Early stopping:  0.010017516552630668\n",
            "Epoch: 136, Loss: 1.2519, Train: 0.5899, Test: 0.5028\n",
            "Early stopping:  0.009749122682134143\n",
            "PREDICTIONS -> tensor([9, 8, 1,  ..., 6, 4, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.52      0.50       758\n",
            "         capital_goods       0.39      0.18      0.25       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.44      0.35      0.39       793\n",
            " consumer_non-cyclical       0.54      0.39      0.46       446\n",
            "                energy       0.69      0.41      0.52       283\n",
            "             financial       0.58      0.57      0.57       767\n",
            "            healthcare       0.72      0.37      0.49       318\n",
            "              services       0.48      0.80      0.60      2076\n",
            "            technology       0.47      0.16      0.24       396\n",
            "        transportation       0.62      0.50      0.55       404\n",
            "             utilities       0.43      0.01      0.03       225\n",
            "\n",
            "              accuracy                           0.50      7054\n",
            "             macro avg       0.49      0.36      0.38      7054\n",
            "          weighted avg       0.50      0.50      0.47      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 47.2533, Train: 0.2944, Test: 0.2927\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 341.9934, Train: 0.0800, Test: 0.0805\n",
            "Early stopping:  208.412775256702\n",
            "Epoch: 003, Loss: 723.1105, Train: 0.1100, Test: 0.1100\n",
            "Early stopping:  338.84732526904276\n",
            "Epoch: 004, Loss: 979.0694, Train: 0.0647, Test: 0.0642\n",
            "Early stopping:  411.1535680174301\n",
            "Epoch: 005, Loss: 772.6002, Train: 0.0908, Test: 0.0938\n",
            "Early stopping:  373.17530130510863\n",
            "Epoch: 006, Loss: 557.3323, Train: 0.0562, Test: 0.0574\n",
            "Early stopping:  239.2893132322824\n",
            "Epoch: 007, Loss: 347.2944, Train: 0.1293, Test: 0.1296\n",
            "Early stopping:  237.4507032622589\n",
            "Epoch: 008, Loss: 239.1229, Train: 0.0959, Test: 0.0982\n",
            "Early stopping:  302.98924205377693\n",
            "Epoch: 009, Loss: 221.1473, Train: 0.3012, Test: 0.2987\n",
            "Early stopping:  234.76098480184965\n",
            "Epoch: 010, Loss: 191.6986, Train: 0.2955, Test: 0.2976\n",
            "Early stopping:  149.5568841789995\n",
            "Epoch: 011, Loss: 165.5077, Train: 0.2065, Test: 0.2114\n",
            "Early stopping:  69.82513949592521\n",
            "Epoch: 012, Loss: 119.8509, Train: 0.0896, Test: 0.0912\n",
            "Early stopping:  47.10292908666485\n",
            "Epoch: 013, Loss: 88.9554, Train: 0.0874, Test: 0.0870\n",
            "Early stopping:  53.3864211694121\n",
            "Epoch: 014, Loss: 60.7499, Train: 0.1265, Test: 0.1170\n",
            "Early stopping:  53.71861118221235\n",
            "Epoch: 015, Loss: 34.6850, Train: 0.1849, Test: 0.1754\n",
            "Early stopping:  51.065870268157205\n",
            "Epoch: 016, Loss: 15.1959, Train: 0.1798, Test: 0.1666\n",
            "Early stopping:  41.81448307429315\n",
            "Epoch: 017, Loss: 5.8761, Train: 0.1299, Test: 0.1293\n",
            "Early stopping:  34.01894659321101\n",
            "Epoch: 018, Loss: 5.2312, Train: 0.1305, Test: 0.1252\n",
            "Early stopping:  23.57106182523414\n",
            "Epoch: 019, Loss: 4.7714, Train: 0.1463, Test: 0.1344\n",
            "Early stopping:  12.784362946642979\n",
            "Epoch: 020, Loss: 4.0482, Train: 0.1656, Test: 0.1537\n",
            "Early stopping:  4.616326868545882\n",
            "Epoch: 021, Loss: 3.4119, Train: 0.2144, Test: 0.2013\n",
            "Early stopping:  0.9681592480287217\n",
            "Epoch: 022, Loss: 2.8747, Train: 0.2564, Test: 0.2383\n",
            "Early stopping:  0.9621051953525976\n",
            "Epoch: 023, Loss: 2.5515, Train: 0.2717, Test: 0.2628\n",
            "Early stopping:  0.8958674410748695\n",
            "Epoch: 024, Loss: 2.3533, Train: 0.2836, Test: 0.2794\n",
            "Early stopping:  0.687738681214872\n",
            "Epoch: 025, Loss: 2.2296, Train: 0.3063, Test: 0.2922\n",
            "Early stopping:  0.4742227991537684\n",
            "Epoch: 026, Loss: 2.1746, Train: 0.3142, Test: 0.2976\n",
            "Early stopping:  0.2844277868062456\n",
            "Epoch: 027, Loss: 2.1465, Train: 0.3182, Test: 0.3024\n",
            "Early stopping:  0.16582209667482206\n",
            "Epoch: 028, Loss: 2.1259, Train: 0.3273, Test: 0.3127\n",
            "Early stopping:  0.09111896768291909\n",
            "Epoch: 029, Loss: 2.1092, Train: 0.3375, Test: 0.3319\n",
            "Early stopping:  0.047285262032975325\n",
            "Epoch: 030, Loss: 2.0919, Train: 0.3488, Test: 0.3426\n",
            "Early stopping:  0.03223920237607988\n",
            "Epoch: 031, Loss: 2.0693, Train: 0.3596, Test: 0.3500\n",
            "Early stopping:  0.029812556732332792\n",
            "Epoch: 032, Loss: 2.0520, Train: 0.3783, Test: 0.3774\n",
            "Early stopping:  0.029714688396453814\n",
            "Epoch: 033, Loss: 2.0183, Train: 0.4005, Test: 0.3845\n",
            "Early stopping:  0.03533802230681002\n",
            "Epoch: 034, Loss: 1.9989, Train: 0.4044, Test: 0.3913\n",
            "Early stopping:  0.037642074685723774\n",
            "Epoch: 035, Loss: 1.9731, Train: 0.3988, Test: 0.3950\n",
            "Early stopping:  0.038952681939334335\n",
            "Epoch: 036, Loss: 1.9493, Train: 0.3993, Test: 0.3976\n",
            "Early stopping:  0.03974819266232156\n",
            "Epoch: 037, Loss: 1.9278, Train: 0.4146, Test: 0.4032\n",
            "Early stopping:  0.036509248901210066\n",
            "Epoch: 038, Loss: 1.9033, Train: 0.4237, Test: 0.4083\n",
            "Early stopping:  0.03742189545334286\n",
            "Epoch: 039, Loss: 1.8830, Train: 0.4237, Test: 0.4103\n",
            "Early stopping:  0.035774227583951136\n",
            "Epoch: 040, Loss: 1.8593, Train: 0.4260, Test: 0.4138\n",
            "Early stopping:  0.035548589006555935\n",
            "Epoch: 041, Loss: 1.8414, Train: 0.4345, Test: 0.4183\n",
            "Early stopping:  0.0343260804730692\n",
            "Epoch: 042, Loss: 1.8174, Train: 0.4447, Test: 0.4277\n",
            "Early stopping:  0.03376715283892187\n",
            "Epoch: 043, Loss: 1.7963, Train: 0.4504, Test: 0.4327\n",
            "Early stopping:  0.03405982503749682\n",
            "Epoch: 044, Loss: 1.7735, Train: 0.4651, Test: 0.4359\n",
            "Early stopping:  0.0342833812736679\n",
            "Epoch: 045, Loss: 1.7512, Train: 0.4691, Test: 0.4363\n",
            "Early stopping:  0.03546583618111008\n",
            "Epoch: 046, Loss: 1.7318, Train: 0.4804, Test: 0.4420\n",
            "Early stopping:  0.034218852892832635\n",
            "Epoch: 047, Loss: 1.7110, Train: 0.4878, Test: 0.4512\n",
            "Early stopping:  0.03358007488527264\n",
            "Epoch: 048, Loss: 1.6923, Train: 0.4884, Test: 0.4534\n",
            "Early stopping:  0.03205886078814513\n",
            "Epoch: 049, Loss: 1.6737, Train: 0.4901, Test: 0.4568\n",
            "Early stopping:  0.030768371652973615\n",
            "Epoch: 050, Loss: 1.6568, Train: 0.4940, Test: 0.4609\n",
            "Early stopping:  0.02961979200479433\n",
            "Epoch: 051, Loss: 1.6384, Train: 0.4963, Test: 0.4654\n",
            "Early stopping:  0.028574469140309908\n",
            "Epoch: 052, Loss: 1.6206, Train: 0.4991, Test: 0.4660\n",
            "Early stopping:  0.028261766910942808\n",
            "Epoch: 053, Loss: 1.6023, Train: 0.4991, Test: 0.4674\n",
            "Early stopping:  0.028329978447639802\n",
            "Epoch: 054, Loss: 1.5845, Train: 0.5048, Test: 0.4724\n",
            "Early stopping:  0.028595323147979513\n",
            "Epoch: 055, Loss: 1.5660, Train: 0.5077, Test: 0.4750\n",
            "Early stopping:  0.028577735078101744\n",
            "Epoch: 056, Loss: 1.5487, Train: 0.5099, Test: 0.4775\n",
            "Early stopping:  0.028462968015212035\n",
            "Epoch: 057, Loss: 1.5318, Train: 0.5167, Test: 0.4783\n",
            "Early stopping:  0.027950543548429223\n",
            "Epoch: 058, Loss: 1.5156, Train: 0.5230, Test: 0.4819\n",
            "Early stopping:  0.027205231806825072\n",
            "Epoch: 059, Loss: 1.4994, Train: 0.5252, Test: 0.4826\n",
            "Early stopping:  0.026295904564049538\n",
            "Epoch: 060, Loss: 1.4830, Train: 0.5303, Test: 0.4843\n",
            "Early stopping:  0.025876127901288286\n",
            "Epoch: 061, Loss: 1.4641, Train: 0.5332, Test: 0.4867\n",
            "Early stopping:  0.026566435378685985\n",
            "Epoch: 062, Loss: 1.4482, Train: 0.5343, Test: 0.4887\n",
            "Early stopping:  0.026911274449455933\n",
            "Epoch: 063, Loss: 1.4330, Train: 0.5406, Test: 0.4902\n",
            "Early stopping:  0.026542174206220518\n",
            "Epoch: 064, Loss: 1.4180, Train: 0.5462, Test: 0.4884\n",
            "Early stopping:  0.025513826720489543\n",
            "Epoch: 065, Loss: 1.4036, Train: 0.5491, Test: 0.4912\n",
            "Early stopping:  0.02390469625509497\n",
            "Epoch: 066, Loss: 1.3897, Train: 0.5513, Test: 0.4922\n",
            "Early stopping:  0.023127197089083024\n",
            "Epoch: 067, Loss: 1.3763, Train: 0.5530, Test: 0.4949\n",
            "Early stopping:  0.022406900419363576\n",
            "Epoch: 068, Loss: 1.3633, Train: 0.5525, Test: 0.4940\n",
            "Early stopping:  0.02161747080971223\n",
            "Epoch: 069, Loss: 1.3503, Train: 0.5553, Test: 0.4959\n",
            "Early stopping:  0.021029366100647515\n",
            "Epoch: 070, Loss: 1.3363, Train: 0.5604, Test: 0.4965\n",
            "Early stopping:  0.02099917812649074\n",
            "Epoch: 071, Loss: 1.3221, Train: 0.5604, Test: 0.4957\n",
            "Early stopping:  0.021406776891297042\n",
            "Epoch: 072, Loss: 1.3091, Train: 0.5666, Test: 0.4982\n",
            "Early stopping:  0.021610323401915092\n",
            "Epoch: 073, Loss: 1.2941, Train: 0.5740, Test: 0.4983\n",
            "Early stopping:  0.022090126107446453\n",
            "Epoch: 074, Loss: 1.2834, Train: 0.5735, Test: 0.5021\n",
            "Early stopping:  0.021170797436194103\n",
            "Epoch: 075, Loss: 1.2778, Train: 0.5763, Test: 0.4986\n",
            "Early stopping:  0.018286052623795197\n",
            "Epoch: 076, Loss: 1.2621, Train: 0.5797, Test: 0.4987\n",
            "Early stopping:  0.01763007717505165\n",
            "Epoch: 077, Loss: 1.2500, Train: 0.5825, Test: 0.5003\n",
            "Early stopping:  0.017493316231733332\n",
            "Epoch: 078, Loss: 1.2388, Train: 0.5905, Test: 0.5010\n",
            "Early stopping:  0.01865870954679332\n",
            "Epoch: 079, Loss: 1.2312, Train: 0.5922, Test: 0.5043\n",
            "Early stopping:  0.01855622471902832\n",
            "Epoch: 080, Loss: 1.2247, Train: 0.5933, Test: 0.5048\n",
            "Early stopping:  0.014921805593470554\n",
            "Epoch: 081, Loss: 1.2070, Train: 0.6001, Test: 0.5069\n",
            "Early stopping:  0.016102532402965157\n",
            "Epoch: 082, Loss: 1.2005, Train: 0.6035, Test: 0.5094\n",
            "Early stopping:  0.016200678671517827\n",
            "Epoch: 083, Loss: 1.1902, Train: 0.6092, Test: 0.5103\n",
            "Early stopping:  0.017000354346625557\n",
            "Epoch: 084, Loss: 1.1768, Train: 0.6064, Test: 0.5038\n",
            "Early stopping:  0.01796565646379448\n",
            "Epoch: 085, Loss: 1.1771, Train: 0.6109, Test: 0.5091\n",
            "Early stopping:  0.013583937451102682\n",
            "Epoch: 086, Loss: 1.1649, Train: 0.6143, Test: 0.5132\n",
            "Early stopping:  0.013726400197226518\n",
            "Epoch: 087, Loss: 1.1514, Train: 0.6177, Test: 0.5098\n",
            "Early stopping:  0.014627044521220201\n",
            "Epoch: 088, Loss: 1.1451, Train: 0.6251, Test: 0.5102\n",
            "Early stopping:  0.01454491667720038\n",
            "Epoch: 089, Loss: 1.1298, Train: 0.6200, Test: 0.5139\n",
            "Early stopping:  0.01815897736245289\n",
            "Epoch: 090, Loss: 1.1268, Train: 0.6273, Test: 0.5118\n",
            "Early stopping:  0.015709934780590334\n",
            "Epoch: 091, Loss: 1.1129, Train: 0.6307, Test: 0.5130\n",
            "Early stopping:  0.015319254437247552\n",
            "Epoch: 092, Loss: 1.1056, Train: 0.6313, Test: 0.5159\n",
            "Early stopping:  0.015409441299453206\n",
            "Epoch: 093, Loss: 1.0997, Train: 0.6449, Test: 0.5186\n",
            "Early stopping:  0.013100916136166534\n",
            "Epoch: 094, Loss: 1.0877, Train: 0.6461, Test: 0.5146\n",
            "Early stopping:  0.01460447467661206\n",
            "Epoch: 095, Loss: 1.0844, Train: 0.6415, Test: 0.5191\n",
            "Early stopping:  0.011961014362867417\n",
            "Epoch: 096, Loss: 1.0730, Train: 0.6517, Test: 0.5227\n",
            "Early stopping:  0.01287443784335932\n",
            "Epoch: 097, Loss: 1.0626, Train: 0.6580, Test: 0.5194\n",
            "Early stopping:  0.014201271020364704\n",
            "Epoch: 098, Loss: 1.0576, Train: 0.6574, Test: 0.5275\n",
            "Early stopping:  0.013164893862941776\n",
            "Epoch: 099, Loss: 1.0472, Train: 0.6602, Test: 0.5281\n",
            "Early stopping:  0.014296356552093116\n",
            "Epoch: 100, Loss: 1.0393, Train: 0.6659, Test: 0.5231\n",
            "Early stopping:  0.01314197601881652\n",
            "Epoch: 101, Loss: 1.0331, Train: 0.6642, Test: 0.5288\n",
            "Early stopping:  0.012274032608198608\n",
            "Epoch: 102, Loss: 1.0222, Train: 0.6756, Test: 0.5286\n",
            "Early stopping:  0.013464899075195949\n",
            "Epoch: 103, Loss: 1.0150, Train: 0.6795, Test: 0.5275\n",
            "Early stopping:  0.012953503917004254\n",
            "Epoch: 104, Loss: 1.0059, Train: 0.6750, Test: 0.5292\n",
            "Early stopping:  0.013490970600419656\n",
            "Epoch: 105, Loss: 1.0048, Train: 0.6869, Test: 0.5262\n",
            "Early stopping:  0.011851090242424792\n",
            "Epoch: 106, Loss: 0.9948, Train: 0.6829, Test: 0.5309\n",
            "Early stopping:  0.010480893356302739\n",
            "Epoch: 107, Loss: 0.9880, Train: 0.6903, Test: 0.5322\n",
            "Early stopping:  0.01046751025534993\n",
            "Epoch: 108, Loss: 0.9735, Train: 0.6818, Test: 0.5264\n",
            "Early stopping:  0.013325823190000112\n",
            "Epoch: 109, Loss: 0.9771, Train: 0.6931, Test: 0.5305\n",
            "Early stopping:  0.012799875511089704\n",
            "Epoch: 110, Loss: 0.9614, Train: 0.6971, Test: 0.5269\n",
            "Early stopping:  0.012980012459960034\n",
            "Epoch: 111, Loss: 0.9566, Train: 0.7016, Test: 0.5284\n",
            "Early stopping:  0.012561256694642144\n",
            "Epoch: 112, Loss: 0.9432, Train: 0.6971, Test: 0.5293\n",
            "Early stopping:  0.013639075537748844\n",
            "Epoch: 113, Loss: 0.9421, Train: 0.7079, Test: 0.5291\n",
            "Early stopping:  0.014410601524701453\n",
            "Epoch: 114, Loss: 0.9299, Train: 0.7096, Test: 0.5315\n",
            "Early stopping:  0.01255588332343407\n",
            "Epoch: 115, Loss: 0.9248, Train: 0.7119, Test: 0.5343\n",
            "Early stopping:  0.012454605539295654\n",
            "Epoch: 116, Loss: 0.9165, Train: 0.7119, Test: 0.5320\n",
            "Early stopping:  0.011409370943051254\n",
            "Epoch: 117, Loss: 0.9051, Train: 0.7113, Test: 0.5291\n",
            "Early stopping:  0.013930224996727534\n",
            "Epoch: 118, Loss: 0.8991, Train: 0.7209, Test: 0.5330\n",
            "Early stopping:  0.012941914507288836\n",
            "Epoch: 119, Loss: 0.8904, Train: 0.7215, Test: 0.5282\n",
            "Early stopping:  0.013679959430406891\n",
            "Epoch: 120, Loss: 0.8956, Train: 0.7226, Test: 0.5335\n",
            "Early stopping:  0.010009522515569181\n",
            "Epoch: 121, Loss: 0.8827, Train: 0.7249, Test: 0.5325\n",
            "Early stopping:  0.008533360735053171\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11,  6, 10], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.45      0.58      0.51       758\n",
            "         capital_goods       0.32      0.22      0.26       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.53      0.45      0.48       793\n",
            " consumer_non-cyclical       0.56      0.46      0.50       446\n",
            "                energy       0.60      0.56      0.58       283\n",
            "             financial       0.63      0.58      0.60       767\n",
            "            healthcare       0.59      0.48      0.53       318\n",
            "              services       0.56      0.73      0.63      2076\n",
            "            technology       0.20      0.10      0.14       396\n",
            "        transportation       0.64      0.58      0.61       404\n",
            "             utilities       0.54      0.48      0.51       225\n",
            "\n",
            "              accuracy                           0.53      7054\n",
            "             macro avg       0.47      0.43      0.45      7054\n",
            "          weighted avg       0.52      0.53      0.52      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 112.2640, Train: 0.1220, Test: 0.1248\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 187.7927, Train: 0.2995, Test: 0.2966\n",
            "Early stopping:  53.40685180963227\n",
            "Epoch: 003, Loss: 349.7934, Train: 0.1117, Test: 0.1150\n",
            "Early stopping:  121.35968388903069\n",
            "Epoch: 004, Loss: 560.9188, Train: 0.1140, Test: 0.1124\n",
            "Early stopping:  198.6322273682542\n",
            "Epoch: 005, Loss: 713.2123, Train: 0.0613, Test: 0.0656\n",
            "Early stopping:  251.58781310799364\n",
            "Epoch: 006, Loss: 741.9650, Train: 0.0720, Test: 0.0699\n",
            "Early stopping:  238.31529719878378\n",
            "Epoch: 007, Loss: 569.2755, Train: 0.0686, Test: 0.0712\n",
            "Early stopping:  155.88862518838212\n",
            "Epoch: 008, Loss: 528.7064, Train: 0.3108, Test: 0.3092\n",
            "Early stopping:  97.36843150989228\n",
            "Epoch: 009, Loss: 389.3070, Train: 0.3018, Test: 0.3010\n",
            "Early stopping:  143.81181741182877\n",
            "Epoch: 010, Loss: 392.1754, Train: 0.2649, Test: 0.2696\n",
            "Early stopping:  145.8597288446698\n",
            "Epoch: 011, Loss: 339.5714, Train: 0.2484, Test: 0.2621\n",
            "Early stopping:  99.31183887126019\n",
            "Epoch: 012, Loss: 259.9184, Train: 0.1452, Test: 0.1402\n",
            "Early stopping:  97.95891538220874\n",
            "Epoch: 013, Loss: 186.1227, Train: 0.2734, Test: 0.2769\n",
            "Early stopping:  89.03959124119623\n",
            "Epoch: 014, Loss: 116.9150, Train: 0.3063, Test: 0.3004\n",
            "Early stopping:  111.49836194117933\n",
            "Epoch: 015, Loss: 64.3117, Train: 0.2921, Test: 0.2865\n",
            "Early stopping:  109.95167344036714\n",
            "Epoch: 016, Loss: 34.7072, Train: 0.2116, Test: 0.1958\n",
            "Early stopping:  91.60615559424366\n",
            "Epoch: 017, Loss: 20.2162, Train: 0.1668, Test: 0.1608\n",
            "Early stopping:  67.81811107232528\n",
            "Epoch: 018, Loss: 37.1260, Train: 0.1889, Test: 0.1861\n",
            "Early stopping:  38.27782241124761\n",
            "Epoch: 019, Loss: 15.0919, Train: 0.1957, Test: 0.1958\n",
            "Early stopping:  19.21038682162195\n",
            "Epoch: 020, Loss: 5.1253, Train: 0.2025, Test: 0.1912\n",
            "Early stopping:  13.461978597207379\n",
            "Epoch: 021, Loss: 3.7189, Train: 0.2053, Test: 0.1922\n",
            "Early stopping:  13.543140532686419\n",
            "Epoch: 022, Loss: 3.1657, Train: 0.2087, Test: 0.1928\n",
            "Early stopping:  14.41513721911308\n",
            "Epoch: 023, Loss: 2.8816, Train: 0.2065, Test: 0.1856\n",
            "Early stopping:  5.157270434137795\n",
            "Epoch: 024, Loss: 2.7216, Train: 0.2031, Test: 0.1868\n",
            "Early stopping:  0.9730391707858578\n",
            "Epoch: 025, Loss: 2.6279, Train: 0.1985, Test: 0.1883\n",
            "Early stopping:  0.4392040680741707\n",
            "Epoch: 026, Loss: 2.5640, Train: 0.2036, Test: 0.1898\n",
            "Early stopping:  0.24058335355890947\n",
            "Epoch: 027, Loss: 2.5180, Train: 0.2042, Test: 0.1873\n",
            "Early stopping:  0.14427890256646783\n",
            "Epoch: 028, Loss: 2.4843, Train: 0.2036, Test: 0.1850\n",
            "Early stopping:  0.09427646672521338\n",
            "Epoch: 029, Loss: 2.4585, Train: 0.1985, Test: 0.1822\n",
            "Early stopping:  0.06725580726415918\n",
            "Epoch: 030, Loss: 2.4372, Train: 0.1923, Test: 0.1788\n",
            "Early stopping:  0.05010002201431408\n",
            "Epoch: 031, Loss: 2.4190, Train: 0.1809, Test: 0.1717\n",
            "Early stopping:  0.039038729467532986\n",
            "Epoch: 032, Loss: 2.4027, Train: 0.1787, Test: 0.1670\n",
            "Early stopping:  0.0321971961318152\n",
            "Epoch: 033, Loss: 2.3871, Train: 0.1781, Test: 0.1647\n",
            "Early stopping:  0.028083147782656637\n",
            "Epoch: 034, Loss: 2.3721, Train: 0.1832, Test: 0.1633\n",
            "Early stopping:  0.025645968314300487\n",
            "Epoch: 035, Loss: 2.3569, Train: 0.1843, Test: 0.1636\n",
            "Early stopping:  0.024469679444756476\n",
            "Epoch: 036, Loss: 2.3412, Train: 0.1832, Test: 0.1639\n",
            "Early stopping:  0.02420812936188657\n",
            "Epoch: 037, Loss: 2.3252, Train: 0.1917, Test: 0.1827\n",
            "Early stopping:  0.024470975668862867\n",
            "Epoch: 038, Loss: 2.3086, Train: 0.1900, Test: 0.1853\n",
            "Early stopping:  0.02510419315856149\n",
            "Epoch: 039, Loss: 2.2903, Train: 0.1980, Test: 0.1956\n",
            "Early stopping:  0.026223185528935974\n",
            "Epoch: 040, Loss: 2.2693, Train: 0.2070, Test: 0.2020\n",
            "Early stopping:  0.028315766396098552\n",
            "Epoch: 041, Loss: 2.2466, Train: 0.2133, Test: 0.2134\n",
            "Early stopping:  0.031119209667313595\n",
            "Epoch: 042, Loss: 2.2268, Train: 0.2223, Test: 0.2224\n",
            "Early stopping:  0.03278259467842083\n",
            "Epoch: 043, Loss: 2.2142, Train: 0.2297, Test: 0.2264\n",
            "Early stopping:  0.03092192109658229\n",
            "Epoch: 044, Loss: 2.1944, Train: 0.2360, Test: 0.2369\n",
            "Early stopping:  0.028917578971163413\n",
            "Epoch: 045, Loss: 2.1769, Train: 0.2530, Test: 0.2570\n",
            "Early stopping:  0.027214717246797276\n",
            "Epoch: 046, Loss: 2.1558, Train: 0.2728, Test: 0.2774\n",
            "Early stopping:  0.028422796897097768\n",
            "Epoch: 047, Loss: 2.1338, Train: 0.2864, Test: 0.2895\n",
            "Early stopping:  0.03156153737585941\n",
            "Epoch: 048, Loss: 2.1189, Train: 0.2898, Test: 0.2932\n",
            "Early stopping:  0.030761587868594897\n",
            "Epoch: 049, Loss: 2.1002, Train: 0.2961, Test: 0.2899\n",
            "Early stopping:  0.03019044464557014\n",
            "Epoch: 050, Loss: 2.0771, Train: 0.2984, Test: 0.2912\n",
            "Early stopping:  0.030286446620750838\n",
            "Epoch: 051, Loss: 2.0600, Train: 0.3137, Test: 0.3085\n",
            "Early stopping:  0.030006008642667694\n",
            "Epoch: 052, Loss: 2.0345, Train: 0.3279, Test: 0.3244\n",
            "Early stopping:  0.033090475686256796\n",
            "Epoch: 053, Loss: 2.0123, Train: 0.3398, Test: 0.3314\n",
            "Early stopping:  0.03457704924876044\n",
            "Epoch: 054, Loss: 1.9939, Train: 0.3517, Test: 0.3382\n",
            "Early stopping:  0.03391718298833815\n",
            "Epoch: 055, Loss: 1.9724, Train: 0.3568, Test: 0.3418\n",
            "Early stopping:  0.03416924109658447\n",
            "Epoch: 056, Loss: 1.9559, Train: 0.3585, Test: 0.3428\n",
            "Early stopping:  0.031185556341791904\n",
            "Epoch: 057, Loss: 1.9468, Train: 0.3710, Test: 0.3499\n",
            "Early stopping:  0.02696497517902122\n",
            "Epoch: 058, Loss: 1.9291, Train: 0.3761, Test: 0.3519\n",
            "Early stopping:  0.02474449605594624\n",
            "Epoch: 059, Loss: 1.9130, Train: 0.3795, Test: 0.3538\n",
            "Early stopping:  0.0231215379492517\n",
            "Epoch: 060, Loss: 1.8955, Train: 0.3812, Test: 0.3553\n",
            "Early stopping:  0.02457179853695204\n",
            "Epoch: 061, Loss: 1.8792, Train: 0.3823, Test: 0.3560\n",
            "Early stopping:  0.0266740618312193\n",
            "Epoch: 062, Loss: 1.8650, Train: 0.3863, Test: 0.3591\n",
            "Early stopping:  0.025633275319292787\n",
            "Epoch: 063, Loss: 1.8498, Train: 0.3959, Test: 0.3646\n",
            "Early stopping:  0.02482975478813407\n",
            "Epoch: 064, Loss: 1.8318, Train: 0.4022, Test: 0.3727\n",
            "Early stopping:  0.024815204572652256\n",
            "Epoch: 065, Loss: 1.8165, Train: 0.4050, Test: 0.3762\n",
            "Early stopping:  0.025092175516748836\n",
            "Epoch: 066, Loss: 1.8044, Train: 0.4078, Test: 0.3768\n",
            "Early stopping:  0.02445701765410048\n",
            "Epoch: 067, Loss: 1.7938, Train: 0.4067, Test: 0.3794\n",
            "Early stopping:  0.022172416135211575\n",
            "Epoch: 068, Loss: 1.7798, Train: 0.4050, Test: 0.3821\n",
            "Early stopping:  0.020062783814485313\n",
            "Epoch: 069, Loss: 1.7635, Train: 0.4073, Test: 0.3850\n",
            "Early stopping:  0.020731516369516343\n",
            "Epoch: 070, Loss: 1.7470, Train: 0.4146, Test: 0.3930\n",
            "Early stopping:  0.023020438850769737\n",
            "Epoch: 071, Loss: 1.7296, Train: 0.4288, Test: 0.3982\n",
            "Early stopping:  0.02551500713542166\n",
            "Epoch: 072, Loss: 1.7118, Train: 0.4419, Test: 0.4029\n",
            "Early stopping:  0.02689407503099061\n",
            "Epoch: 073, Loss: 1.6864, Train: 0.4589, Test: 0.4227\n",
            "Early stopping:  0.030068364989864142\n",
            "Epoch: 074, Loss: 1.6591, Train: 0.4691, Test: 0.4337\n",
            "Early stopping:  0.03483619197297521\n",
            "Epoch: 075, Loss: 1.6468, Train: 0.4674, Test: 0.4294\n",
            "Early stopping:  0.03472333702392765\n",
            "Epoch: 076, Loss: 1.6155, Train: 0.4606, Test: 0.4254\n",
            "Early stopping:  0.03694711298528414\n",
            "Epoch: 077, Loss: 1.6058, Train: 0.4787, Test: 0.4318\n",
            "Early stopping:  0.03276875100249313\n",
            "Epoch: 078, Loss: 1.5893, Train: 0.4872, Test: 0.4453\n",
            "Early stopping:  0.028953408951466197\n",
            "Epoch: 079, Loss: 1.5759, Train: 0.4901, Test: 0.4471\n",
            "Early stopping:  0.027095141024660912\n",
            "Epoch: 080, Loss: 1.5610, Train: 0.4889, Test: 0.4485\n",
            "Early stopping:  0.022032536635546714\n",
            "Epoch: 081, Loss: 1.5496, Train: 0.4974, Test: 0.4549\n",
            "Early stopping:  0.022307356009867906\n",
            "Epoch: 082, Loss: 1.5301, Train: 0.5048, Test: 0.4565\n",
            "Early stopping:  0.022930946408556706\n",
            "Epoch: 083, Loss: 1.5228, Train: 0.5156, Test: 0.4589\n",
            "Early stopping:  0.021796434962301586\n",
            "Epoch: 084, Loss: 1.5044, Train: 0.5133, Test: 0.4604\n",
            "Early stopping:  0.022289496049093808\n",
            "Epoch: 085, Loss: 1.4965, Train: 0.5190, Test: 0.4616\n",
            "Early stopping:  0.021089324534466353\n",
            "Epoch: 086, Loss: 1.4812, Train: 0.5207, Test: 0.4668\n",
            "Early stopping:  0.019787404019722798\n",
            "Epoch: 087, Loss: 1.4702, Train: 0.5235, Test: 0.4692\n",
            "Early stopping:  0.02041948646791037\n",
            "Epoch: 088, Loss: 1.4562, Train: 0.5235, Test: 0.4684\n",
            "Early stopping:  0.019480026643293263\n",
            "Epoch: 089, Loss: 1.4457, Train: 0.5264, Test: 0.4709\n",
            "Early stopping:  0.020080256907433338\n",
            "Epoch: 090, Loss: 1.4338, Train: 0.5343, Test: 0.4697\n",
            "Early stopping:  0.018890594595079312\n",
            "Epoch: 091, Loss: 1.4232, Train: 0.5360, Test: 0.4709\n",
            "Early stopping:  0.018412621556171283\n",
            "Epoch: 092, Loss: 1.4103, Train: 0.5298, Test: 0.4714\n",
            "Early stopping:  0.01807241488592934\n",
            "Epoch: 093, Loss: 1.3999, Train: 0.5372, Test: 0.4745\n",
            "Early stopping:  0.01821985568130847\n",
            "Epoch: 094, Loss: 1.3873, Train: 0.5406, Test: 0.4766\n",
            "Early stopping:  0.01841161482086568\n",
            "Epoch: 095, Loss: 1.3752, Train: 0.5434, Test: 0.4789\n",
            "Early stopping:  0.01884001732732058\n",
            "Epoch: 096, Loss: 1.3646, Train: 0.5411, Test: 0.4840\n",
            "Early stopping:  0.018354036227574977\n",
            "Epoch: 097, Loss: 1.3525, Train: 0.5434, Test: 0.4827\n",
            "Early stopping:  0.0185667973070341\n",
            "Epoch: 098, Loss: 1.3434, Train: 0.5445, Test: 0.4834\n",
            "Early stopping:  0.017486357190063105\n",
            "Epoch: 099, Loss: 1.3327, Train: 0.5513, Test: 0.4823\n",
            "Early stopping:  0.016789851560519133\n",
            "Epoch: 100, Loss: 1.3227, Train: 0.5564, Test: 0.4847\n",
            "Early stopping:  0.016383145609072797\n",
            "Epoch: 101, Loss: 1.3140, Train: 0.5604, Test: 0.4823\n",
            "Early stopping:  0.015457685014671915\n",
            "Epoch: 102, Loss: 1.3046, Train: 0.5604, Test: 0.4827\n",
            "Early stopping:  0.015223638252413733\n",
            "Epoch: 103, Loss: 1.2959, Train: 0.5644, Test: 0.4871\n",
            "Early stopping:  0.014528795246529447\n",
            "Epoch: 104, Loss: 1.2882, Train: 0.5689, Test: 0.4864\n",
            "Early stopping:  0.01381190135619883\n",
            "Epoch: 105, Loss: 1.2797, Train: 0.5678, Test: 0.4899\n",
            "Early stopping:  0.013452481307074429\n",
            "Epoch: 106, Loss: 1.2700, Train: 0.5689, Test: 0.4912\n",
            "Early stopping:  0.013510715156019088\n",
            "Epoch: 107, Loss: 1.2614, Train: 0.5706, Test: 0.4905\n",
            "Early stopping:  0.013780941376844028\n",
            "Epoch: 108, Loss: 1.2544, Train: 0.5729, Test: 0.4948\n",
            "Early stopping:  0.013601157265842332\n",
            "Epoch: 109, Loss: 1.2487, Train: 0.5763, Test: 0.4902\n",
            "Early stopping:  0.012338875927084426\n",
            "Epoch: 110, Loss: 1.2424, Train: 0.5797, Test: 0.4948\n",
            "Early stopping:  0.010763639977406609\n",
            "Epoch: 111, Loss: 1.2332, Train: 0.5888, Test: 0.4929\n",
            "Early stopping:  0.0108341541352753\n",
            "Epoch: 112, Loss: 1.2242, Train: 0.5888, Test: 0.4943\n",
            "Early stopping:  0.012085857299119585\n",
            "Epoch: 113, Loss: 1.2153, Train: 0.5888, Test: 0.4948\n",
            "Early stopping:  0.013478166927739156\n",
            "Epoch: 114, Loss: 1.2082, Train: 0.5905, Test: 0.4940\n",
            "Early stopping:  0.013654641478927235\n",
            "Epoch: 115, Loss: 1.2018, Train: 0.5956, Test: 0.4955\n",
            "Early stopping:  0.012485424139765135\n",
            "Epoch: 116, Loss: 1.1948, Train: 0.5933, Test: 0.4966\n",
            "Early stopping:  0.011452103840268223\n",
            "Epoch: 117, Loss: 1.1874, Train: 0.5950, Test: 0.4967\n",
            "Early stopping:  0.010982394322415065\n",
            "Epoch: 118, Loss: 1.1810, Train: 0.6018, Test: 0.4993\n",
            "Early stopping:  0.010908101174463641\n",
            "Epoch: 119, Loss: 1.1750, Train: 0.6029, Test: 0.4984\n",
            "Early stopping:  0.010663574172925402\n",
            "Epoch: 120, Loss: 1.1687, Train: 0.6069, Test: 0.5016\n",
            "Early stopping:  0.010205483030859762\n",
            "Epoch: 121, Loss: 1.1614, Train: 0.6058, Test: 0.5031\n",
            "Early stopping:  0.010164902667193125\n",
            "Epoch: 122, Loss: 1.1545, Train: 0.6098, Test: 0.5031\n",
            "Early stopping:  0.010545092350110311\n",
            "Epoch: 123, Loss: 1.1473, Train: 0.6109, Test: 0.5035\n",
            "Early stopping:  0.010998729993809317\n",
            "Epoch: 124, Loss: 1.1407, Train: 0.6154, Test: 0.5058\n",
            "Early stopping:  0.011080902646399087\n",
            "Epoch: 125, Loss: 1.1341, Train: 0.6205, Test: 0.5040\n",
            "Early stopping:  0.010802884618816826\n",
            "Epoch: 126, Loss: 1.1281, Train: 0.6205, Test: 0.5074\n",
            "Early stopping:  0.010435259935269243\n",
            "Epoch: 127, Loss: 1.1218, Train: 0.6268, Test: 0.5051\n",
            "Early stopping:  0.010073895704219621\n",
            "Epoch: 128, Loss: 1.1152, Train: 0.6268, Test: 0.5075\n",
            "Early stopping:  0.010022562896554251\n",
            "Epoch: 129, Loss: 1.1078, Train: 0.6279, Test: 0.5067\n",
            "Early stopping:  0.010370801105813468\n",
            "Epoch: 130, Loss: 1.0999, Train: 0.6319, Test: 0.5084\n",
            "Early stopping:  0.011139141852872408\n",
            "Epoch: 131, Loss: 1.0947, Train: 0.6302, Test: 0.5048\n",
            "Early stopping:  0.011006774385057531\n",
            "Epoch: 132, Loss: 1.0958, Train: 0.6290, Test: 0.5096\n",
            "Early stopping:  0.008678805753665532\n",
            "PREDICTIONS -> tensor([ 9,  5,  0,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.54      0.47       758\n",
            "         capital_goods       0.38      0.10      0.16       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.51      0.42      0.46       793\n",
            " consumer_non-cyclical       0.59      0.38      0.46       446\n",
            "                energy       0.47      0.50      0.49       283\n",
            "             financial       0.57      0.63      0.60       767\n",
            "            healthcare       0.78      0.07      0.12       318\n",
            "              services       0.52      0.77      0.62      2076\n",
            "            technology       0.40      0.31      0.35       396\n",
            "        transportation       0.67      0.48      0.56       404\n",
            "             utilities       0.51      0.36      0.42       225\n",
            "\n",
            "              accuracy                           0.51      7054\n",
            "             macro avg       0.48      0.38      0.39      7054\n",
            "          weighted avg       0.51      0.51      0.48      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 80.6743, Train: 0.0630, Test: 0.0645\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 496.4944, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  294.0291793007746\n",
            "Epoch: 003, Loss: 288.1553, Train: 0.0352, Test: 0.0369\n",
            "Early stopping:  207.9101740781483\n",
            "Epoch: 004, Loss: 287.8773, Train: 0.1049, Test: 0.1067\n",
            "Early stopping:  169.75818051117747\n",
            "Epoch: 005, Loss: 223.7228, Train: 0.0732, Test: 0.0705\n",
            "Early stopping:  149.824669064111\n",
            "Epoch: 006, Loss: 173.6322, Train: 0.1129, Test: 0.1026\n",
            "Early stopping:  122.98669803189594\n",
            "Epoch: 007, Loss: 155.4845, Train: 0.2768, Test: 0.2736\n",
            "Early stopping:  62.072238116585744\n",
            "Epoch: 008, Loss: 153.1603, Train: 0.3080, Test: 0.3014\n",
            "Early stopping:  57.333560495434675\n",
            "Epoch: 009, Loss: 142.4413, Train: 0.1656, Test: 0.1615\n",
            "Early stopping:  32.2203432784748\n",
            "Epoch: 010, Loss: 128.7081, Train: 0.2569, Test: 0.2444\n",
            "Early stopping:  16.63335891400412\n",
            "Epoch: 011, Loss: 88.5359, Train: 0.2218, Test: 0.2142\n",
            "Early stopping:  27.360508655836338\n",
            "Epoch: 012, Loss: 61.6555, Train: 0.2853, Test: 0.2767\n",
            "Early stopping:  38.54571535981306\n",
            "Epoch: 013, Loss: 45.2178, Train: 0.2467, Test: 0.2341\n",
            "Early stopping:  41.84667949616083\n",
            "Epoch: 014, Loss: 35.7586, Train: 0.2184, Test: 0.2057\n",
            "Early stopping:  37.50659853144195\n",
            "Epoch: 015, Loss: 30.0453, Train: 0.2019, Test: 0.1985\n",
            "Early stopping:  23.556277108909253\n",
            "Epoch: 016, Loss: 21.1912, Train: 0.2167, Test: 0.2176\n",
            "Early stopping:  15.488063156204023\n",
            "Epoch: 017, Loss: 13.6748, Train: 0.2331, Test: 0.2237\n",
            "Early stopping:  12.303126282280093\n",
            "Epoch: 018, Loss: 8.5063, Train: 0.1713, Test: 0.1673\n",
            "Early stopping:  11.24647307473833\n",
            "Epoch: 019, Loss: 6.1531, Train: 0.1747, Test: 0.1711\n",
            "Early stopping:  9.78143718628784\n",
            "Epoch: 020, Loss: 4.8302, Train: 0.2518, Test: 0.2369\n",
            "Early stopping:  6.684130719369688\n",
            "Epoch: 021, Loss: 3.9889, Train: 0.3165, Test: 0.3052\n",
            "Early stopping:  3.885450683313139\n",
            "Epoch: 022, Loss: 3.3365, Train: 0.3585, Test: 0.3405\n",
            "Early stopping:  2.048368926084397\n",
            "Epoch: 023, Loss: 2.8405, Train: 0.3579, Test: 0.3412\n",
            "Early stopping:  1.3081971064555635\n",
            "Epoch: 024, Loss: 2.5785, Train: 0.3630, Test: 0.3499\n",
            "Early stopping:  0.9108014155888483\n",
            "Epoch: 025, Loss: 2.3584, Train: 0.3766, Test: 0.3631\n",
            "Early stopping:  0.6526570418818836\n",
            "Epoch: 026, Loss: 2.1836, Train: 0.3988, Test: 0.3699\n",
            "Early stopping:  0.4513200579591574\n",
            "Epoch: 027, Loss: 2.0886, Train: 0.3988, Test: 0.3710\n",
            "Early stopping:  0.30452715470121455\n",
            "Epoch: 028, Loss: 2.0411, Train: 0.4022, Test: 0.3744\n",
            "Early stopping:  0.22008146974114576\n",
            "Epoch: 029, Loss: 2.0284, Train: 0.4010, Test: 0.3750\n",
            "Early stopping:  0.1364578791418642\n",
            "Epoch: 030, Loss: 2.0171, Train: 0.4056, Test: 0.3792\n",
            "Early stopping:  0.06820595658509916\n",
            "Epoch: 031, Loss: 2.0059, Train: 0.4044, Test: 0.3774\n",
            "Early stopping:  0.03207096640095285\n",
            "Epoch: 032, Loss: 1.9957, Train: 0.4095, Test: 0.3789\n",
            "Early stopping:  0.017942426579024052\n",
            "Epoch: 033, Loss: 1.9837, Train: 0.4067, Test: 0.3823\n",
            "Early stopping:  0.01752024489153403\n",
            "Epoch: 034, Loss: 1.9665, Train: 0.4152, Test: 0.3896\n",
            "Early stopping:  0.01960182522226978\n",
            "Epoch: 035, Loss: 1.9515, Train: 0.4175, Test: 0.3959\n",
            "Early stopping:  0.021916178383865455\n",
            "Epoch: 036, Loss: 1.9279, Train: 0.4243, Test: 0.3979\n",
            "Early stopping:  0.02671707093236126\n",
            "Epoch: 037, Loss: 1.8977, Train: 0.4265, Test: 0.4006\n",
            "Early stopping:  0.03366901086660028\n",
            "Epoch: 038, Loss: 1.8675, Train: 0.4311, Test: 0.4079\n",
            "Early stopping:  0.04013411492991627\n",
            "Epoch: 039, Loss: 1.8335, Train: 0.4385, Test: 0.4186\n",
            "Early stopping:  0.046933659323674326\n",
            "Epoch: 040, Loss: 1.7971, Train: 0.4492, Test: 0.4270\n",
            "Early stopping:  0.05156193009588673\n",
            "Epoch: 041, Loss: 1.7697, Train: 0.4623, Test: 0.4314\n",
            "Early stopping:  0.05167459926011173\n",
            "Epoch: 042, Loss: 1.7389, Train: 0.4685, Test: 0.4375\n",
            "Early stopping:  0.05081957224755615\n",
            "Epoch: 043, Loss: 1.7114, Train: 0.4782, Test: 0.4416\n",
            "Early stopping:  0.047861859813977156\n",
            "Epoch: 044, Loss: 1.6870, Train: 0.4889, Test: 0.4515\n",
            "Early stopping:  0.044061851216040566\n",
            "Epoch: 045, Loss: 1.6565, Train: 0.4969, Test: 0.4604\n",
            "Early stopping:  0.04403287210347538\n",
            "Epoch: 046, Loss: 1.6378, Train: 0.5048, Test: 0.4630\n",
            "Early stopping:  0.040759427776498325\n",
            "Epoch: 047, Loss: 1.6148, Train: 0.5133, Test: 0.4671\n",
            "Early stopping:  0.03844649281113971\n",
            "Epoch: 048, Loss: 1.5859, Train: 0.5145, Test: 0.4698\n",
            "Early stopping:  0.03866476753146157\n",
            "Epoch: 049, Loss: 1.5668, Train: 0.5230, Test: 0.4750\n",
            "Early stopping:  0.036674212942662314\n",
            "Epoch: 050, Loss: 1.5360, Train: 0.5320, Test: 0.4793\n",
            "Early stopping:  0.039862590076721625\n",
            "Epoch: 051, Loss: 1.5073, Train: 0.5320, Test: 0.4843\n",
            "Early stopping:  0.04196798051811781\n",
            "Epoch: 052, Loss: 1.4862, Train: 0.5298, Test: 0.4800\n",
            "Early stopping:  0.04104207235537698\n",
            "Epoch: 053, Loss: 1.4559, Train: 0.5292, Test: 0.4752\n",
            "Early stopping:  0.042987109274112596\n",
            "Epoch: 054, Loss: 1.4468, Train: 0.5372, Test: 0.4792\n",
            "Early stopping:  0.03669865878246753\n",
            "Epoch: 055, Loss: 1.4195, Train: 0.5417, Test: 0.4854\n",
            "Early stopping:  0.03431000185785689\n",
            "Epoch: 056, Loss: 1.3969, Train: 0.5383, Test: 0.4850\n",
            "Early stopping:  0.03429442527740172\n",
            "Epoch: 057, Loss: 1.3846, Train: 0.5451, Test: 0.4878\n",
            "Early stopping:  0.030808680549515428\n",
            "Epoch: 058, Loss: 1.3608, Train: 0.5547, Test: 0.4874\n",
            "Early stopping:  0.03295021874007074\n",
            "Epoch: 059, Loss: 1.3464, Train: 0.5627, Test: 0.4908\n",
            "Early stopping:  0.028957694791759036\n",
            "Epoch: 060, Loss: 1.3288, Train: 0.5729, Test: 0.4945\n",
            "Early stopping:  0.027696301091984177\n",
            "Epoch: 061, Loss: 1.3095, Train: 0.5757, Test: 0.4955\n",
            "Early stopping:  0.02887959879832257\n",
            "Epoch: 062, Loss: 1.2946, Train: 0.5808, Test: 0.4967\n",
            "Early stopping:  0.02679344608656309\n",
            "Epoch: 063, Loss: 1.2782, Train: 0.5893, Test: 0.5007\n",
            "Early stopping:  0.02697879849262083\n",
            "Epoch: 064, Loss: 1.2622, Train: 0.5956, Test: 0.4997\n",
            "Early stopping:  0.02600417038242289\n",
            "Epoch: 065, Loss: 1.2471, Train: 0.5950, Test: 0.5024\n",
            "Early stopping:  0.024858267321462582\n",
            "Epoch: 066, Loss: 1.2331, Train: 0.6001, Test: 0.5062\n",
            "Early stopping:  0.024381425378582743\n",
            "Epoch: 067, Loss: 1.2158, Train: 0.6029, Test: 0.5058\n",
            "Early stopping:  0.024338795426753754\n",
            "Epoch: 068, Loss: 1.2007, Train: 0.6058, Test: 0.5054\n",
            "Early stopping:  0.024401432313014434\n",
            "Epoch: 069, Loss: 1.1856, Train: 0.6086, Test: 0.5038\n",
            "Early stopping:  0.024570518905783677\n",
            "Epoch: 070, Loss: 1.1711, Train: 0.6120, Test: 0.5060\n",
            "Early stopping:  0.02441531348582804\n",
            "Epoch: 071, Loss: 1.1560, Train: 0.6126, Test: 0.5054\n",
            "Early stopping:  0.023628426190745976\n",
            "Epoch: 072, Loss: 1.1443, Train: 0.6183, Test: 0.5099\n",
            "Early stopping:  0.02254234571345753\n",
            "Epoch: 073, Loss: 1.1290, Train: 0.6268, Test: 0.5088\n",
            "Early stopping:  0.022140888554433576\n",
            "Epoch: 074, Loss: 1.1160, Train: 0.6268, Test: 0.5074\n",
            "Early stopping:  0.021706101756192267\n",
            "Epoch: 075, Loss: 1.1012, Train: 0.6432, Test: 0.5138\n",
            "Early stopping:  0.021821762911101293\n",
            "Epoch: 076, Loss: 1.0860, Train: 0.6517, Test: 0.5157\n",
            "Early stopping:  0.02284737157500374\n",
            "Epoch: 077, Loss: 1.0706, Train: 0.6534, Test: 0.5153\n",
            "Early stopping:  0.02321806064537795\n",
            "Epoch: 078, Loss: 1.0549, Train: 0.6642, Test: 0.5170\n",
            "Early stopping:  0.024136277633698414\n",
            "Epoch: 079, Loss: 1.0469, Train: 0.6653, Test: 0.5146\n",
            "Early stopping:  0.0222119815582488\n",
            "Epoch: 080, Loss: 1.0315, Train: 0.6676, Test: 0.5159\n",
            "Early stopping:  0.021099981706366275\n",
            "Epoch: 081, Loss: 1.0172, Train: 0.6727, Test: 0.5204\n",
            "Early stopping:  0.020686966881136384\n",
            "Epoch: 082, Loss: 1.0064, Train: 0.6727, Test: 0.5213\n",
            "Early stopping:  0.020129754669218385\n",
            "Epoch: 083, Loss: 0.9967, Train: 0.6784, Test: 0.5244\n",
            "Early stopping:  0.019939797750425578\n",
            "Epoch: 084, Loss: 0.9769, Train: 0.6858, Test: 0.5204\n",
            "Early stopping:  0.020666835998464007\n",
            "Epoch: 085, Loss: 0.9698, Train: 0.6880, Test: 0.5227\n",
            "Early stopping:  0.019843289823986233\n",
            "Epoch: 086, Loss: 0.9498, Train: 0.6852, Test: 0.5284\n",
            "Early stopping:  0.022380551155348073\n",
            "Epoch: 087, Loss: 0.9448, Train: 0.6948, Test: 0.5274\n",
            "Early stopping:  0.021050264869329486\n",
            "Epoch: 088, Loss: 0.9256, Train: 0.7016, Test: 0.5261\n",
            "Early stopping:  0.02047865285060578\n",
            "Epoch: 089, Loss: 0.9218, Train: 0.7028, Test: 0.5272\n",
            "Early stopping:  0.019466669769373803\n",
            "Epoch: 090, Loss: 0.9059, Train: 0.7022, Test: 0.5339\n",
            "Early stopping:  0.017877100239663556\n",
            "Epoch: 091, Loss: 0.9035, Train: 0.7096, Test: 0.5340\n",
            "Early stopping:  0.016765243728123914\n",
            "Epoch: 092, Loss: 0.8867, Train: 0.7090, Test: 0.5312\n",
            "Early stopping:  0.015647034657154998\n",
            "Epoch: 093, Loss: 0.8766, Train: 0.7136, Test: 0.5326\n",
            "Early stopping:  0.01759491628543196\n",
            "Epoch: 094, Loss: 0.8670, Train: 0.7130, Test: 0.5384\n",
            "Early stopping:  0.01681250248550481\n",
            "Epoch: 095, Loss: 0.8578, Train: 0.7204, Test: 0.5391\n",
            "Early stopping:  0.017702376516951338\n",
            "Epoch: 096, Loss: 0.8475, Train: 0.7215, Test: 0.5374\n",
            "Early stopping:  0.01536707841541553\n",
            "Epoch: 097, Loss: 0.8330, Train: 0.7153, Test: 0.5346\n",
            "Early stopping:  0.016950626889456873\n",
            "Epoch: 098, Loss: 0.8335, Train: 0.7283, Test: 0.5415\n",
            "Early stopping:  0.014930112182819888\n",
            "Epoch: 099, Loss: 0.8173, Train: 0.7323, Test: 0.5390\n",
            "Early stopping:  0.015452990141682962\n",
            "Epoch: 100, Loss: 0.8168, Train: 0.7334, Test: 0.5410\n",
            "Early stopping:  0.012859011772963388\n",
            "Epoch: 101, Loss: 0.7989, Train: 0.7328, Test: 0.5380\n",
            "Early stopping:  0.014285255763907151\n",
            "Epoch: 102, Loss: 0.7931, Train: 0.7249, Test: 0.5400\n",
            "Early stopping:  0.01613769298202932\n",
            "Epoch: 103, Loss: 0.7940, Train: 0.7374, Test: 0.5441\n",
            "Early stopping:  0.012109140914622781\n",
            "Epoch: 104, Loss: 0.7754, Train: 0.7431, Test: 0.5432\n",
            "Early stopping:  0.014816540327352822\n",
            "Epoch: 105, Loss: 0.7740, Train: 0.7391, Test: 0.5445\n",
            "Early stopping:  0.011512363482629616\n",
            "Epoch: 106, Loss: 0.7684, Train: 0.7499, Test: 0.5462\n",
            "Early stopping:  0.011775337478962816\n",
            "Epoch: 107, Loss: 0.7525, Train: 0.7510, Test: 0.5424\n",
            "Early stopping:  0.014887945604883553\n",
            "Epoch: 108, Loss: 0.7459, Train: 0.7351, Test: 0.5417\n",
            "Early stopping:  0.013273870787498577\n",
            "Epoch: 109, Loss: 0.7626, Train: 0.7527, Test: 0.5437\n",
            "Early stopping:  0.011464890123230497\n",
            "Epoch: 110, Loss: 0.7363, Train: 0.7606, Test: 0.5449\n",
            "Early stopping:  0.012814668409354047\n",
            "Epoch: 111, Loss: 0.7302, Train: 0.7544, Test: 0.5447\n",
            "Early stopping:  0.012850139817432812\n",
            "Epoch: 112, Loss: 0.7276, Train: 0.7635, Test: 0.5427\n",
            "Early stopping:  0.014199684705715643\n",
            "Epoch: 113, Loss: 0.7225, Train: 0.7691, Test: 0.5444\n",
            "Early stopping:  0.015755802388049637\n",
            "Epoch: 114, Loss: 0.7056, Train: 0.7595, Test: 0.5439\n",
            "Early stopping:  0.01163805184602598\n",
            "Epoch: 115, Loss: 0.7118, Train: 0.7657, Test: 0.5439\n",
            "Early stopping:  0.010485016799348828\n",
            "Epoch: 116, Loss: 0.7001, Train: 0.7714, Test: 0.5471\n",
            "Early stopping:  0.011443596179000927\n",
            "Epoch: 117, Loss: 0.6929, Train: 0.7777, Test: 0.5491\n",
            "Early stopping:  0.011294466321407369\n",
            "Epoch: 118, Loss: 0.6833, Train: 0.7788, Test: 0.5472\n",
            "Early stopping:  0.0111043658840869\n",
            "Epoch: 119, Loss: 0.6788, Train: 0.7839, Test: 0.5475\n",
            "Early stopping:  0.013227437802881787\n",
            "Epoch: 120, Loss: 0.6703, Train: 0.7964, Test: 0.5465\n",
            "Early stopping:  0.011724008669928924\n",
            "Epoch: 121, Loss: 0.6621, Train: 0.7845, Test: 0.5495\n",
            "Early stopping:  0.011867663634056703\n",
            "Epoch: 122, Loss: 0.6577, Train: 0.7890, Test: 0.5510\n",
            "Early stopping:  0.010826260093971963\n",
            "Epoch: 123, Loss: 0.6470, Train: 0.7992, Test: 0.5508\n",
            "Early stopping:  0.012119368639861483\n",
            "Epoch: 124, Loss: 0.6423, Train: 0.7992, Test: 0.5496\n",
            "Early stopping:  0.011303556263295626\n",
            "Epoch: 125, Loss: 0.6385, Train: 0.8003, Test: 0.5492\n",
            "Early stopping:  0.010041115308778941\n",
            "Epoch: 126, Loss: 0.6292, Train: 0.7884, Test: 0.5506\n",
            "Early stopping:  0.010506426577396154\n",
            "Epoch: 127, Loss: 0.6321, Train: 0.8066, Test: 0.5502\n",
            "Early stopping:  0.007273139889247144\n",
            "PREDICTIONS -> tensor([ 0,  8,  1,  ..., 11,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.55      0.51       758\n",
            "         capital_goods       0.38      0.36      0.37       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.49      0.51      0.50       793\n",
            " consumer_non-cyclical       0.56      0.49      0.52       446\n",
            "                energy       0.62      0.52      0.57       283\n",
            "             financial       0.67      0.58      0.62       767\n",
            "            healthcare       0.61      0.53      0.57       318\n",
            "              services       0.60      0.71      0.65      2076\n",
            "            technology       0.33      0.18      0.24       396\n",
            "        transportation       0.61      0.62      0.62       404\n",
            "             utilities       0.46      0.48      0.47       225\n",
            "\n",
            "              accuracy                           0.55      7054\n",
            "             macro avg       0.48      0.46      0.47      7054\n",
            "          weighted avg       0.54      0.55      0.54      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 89.3850, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 348.3806, Train: 0.1163, Test: 0.1181\n",
            "Early stopping:  183.13752195048576\n",
            "Epoch: 003, Loss: 494.8129, Train: 0.1378, Test: 0.1320\n",
            "Early stopping:  205.30176620212927\n",
            "Epoch: 004, Loss: 489.0871, Train: 0.1117, Test: 0.1106\n",
            "Early stopping:  189.84330188251812\n",
            "Epoch: 005, Loss: 471.8105, Train: 0.0357, Test: 0.0353\n",
            "Early stopping:  172.45252889295713\n",
            "Epoch: 006, Loss: 444.1104, Train: 0.0601, Test: 0.0593\n",
            "Early stopping:  59.94213742847242\n",
            "Epoch: 007, Loss: 394.0081, Train: 0.0754, Test: 0.0739\n",
            "Early stopping:  41.22283567204216\n",
            "Epoch: 008, Loss: 266.2455, Train: 0.0550, Test: 0.0537\n",
            "Early stopping:  89.60098840507975\n",
            "Epoch: 009, Loss: 220.9707, Train: 0.2734, Test: 0.2713\n",
            "Early stopping:  110.51030438726426\n",
            "Epoch: 010, Loss: 110.3948, Train: 0.1106, Test: 0.1060\n",
            "Early stopping:  134.27053463927953\n",
            "Epoch: 011, Loss: 114.0173, Train: 0.1492, Test: 0.1456\n",
            "Early stopping:  117.95938158979537\n",
            "Epoch: 012, Loss: 93.3295, Train: 0.1775, Test: 0.1732\n",
            "Early stopping:  77.4929862232294\n",
            "Epoch: 013, Loss: 74.9769, Train: 0.2995, Test: 0.2977\n",
            "Early stopping:  57.06148084886745\n",
            "Epoch: 014, Loss: 40.9620, Train: 0.3097, Test: 0.3127\n",
            "Early stopping:  29.920807078071128\n",
            "Epoch: 015, Loss: 21.1736, Train: 0.3244, Test: 0.3289\n",
            "Early stopping:  37.84061338121052\n",
            "Epoch: 016, Loss: 8.6280, Train: 0.3573, Test: 0.3455\n",
            "Early stopping:  35.71305706248282\n",
            "Epoch: 017, Loss: 4.1301, Train: 0.3454, Test: 0.3374\n",
            "Early stopping:  28.926653920511704\n",
            "Epoch: 018, Loss: 3.0380, Train: 0.3261, Test: 0.3228\n",
            "Early stopping:  15.908062118113921\n",
            "Epoch: 019, Loss: 2.5892, Train: 0.3108, Test: 0.3051\n",
            "Early stopping:  7.790581623811354\n",
            "Epoch: 020, Loss: 2.4167, Train: 0.2870, Test: 0.2692\n",
            "Early stopping:  2.58503018518018\n",
            "Epoch: 021, Loss: 2.3425, Train: 0.2632, Test: 0.2467\n",
            "Early stopping:  0.7370822802497011\n",
            "Epoch: 022, Loss: 2.3269, Train: 0.2496, Test: 0.2376\n",
            "Early stopping:  0.29580579170760823\n",
            "Epoch: 023, Loss: 2.3292, Train: 0.2377, Test: 0.2268\n",
            "Early stopping:  0.11150235615485504\n",
            "Epoch: 024, Loss: 2.3379, Train: 0.2258, Test: 0.2200\n",
            "Early stopping:  0.0374629190823739\n",
            "Epoch: 025, Loss: 2.3460, Train: 0.2172, Test: 0.2180\n",
            "Early stopping:  0.008264435034356579\n",
            "PREDICTIONS -> tensor([3, 1, 8,  ..., 8, 8, 3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.14      0.03      0.05       758\n",
            "         capital_goods       0.20      0.19      0.19       508\n",
            "conglomerates_industry       0.01      0.03      0.01        80\n",
            "     consumer_cyclical       0.19      0.53      0.28       793\n",
            " consumer_non-cyclical       0.33      0.07      0.11       446\n",
            "                energy       0.09      0.01      0.02       283\n",
            "             financial       0.53      0.26      0.35       767\n",
            "            healthcare       0.09      0.03      0.05       318\n",
            "              services       0.22      0.36      0.28      2076\n",
            "            technology       0.05      0.01      0.01       396\n",
            "        transportation       0.00      0.00      0.00       404\n",
            "             utilities       0.00      0.00      0.00       225\n",
            "\n",
            "              accuracy                           0.22      7054\n",
            "             macro avg       0.16      0.13      0.11      7054\n",
            "          weighted avg       0.21      0.22      0.18      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 116.5276, Train: 0.1089, Test: 0.1089\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 445.1912, Train: 0.2898, Test: 0.2898\n",
            "Early stopping:  232.40029061225576\n",
            "Epoch: 003, Loss: 331.5057, Train: 0.0250, Test: 0.0221\n",
            "Early stopping:  166.9130420644995\n",
            "Epoch: 004, Loss: 404.1273, Train: 0.1344, Test: 0.1317\n",
            "Early stopping:  146.2969284904127\n",
            "Epoch: 005, Loss: 333.1706, Train: 0.0874, Test: 0.0843\n",
            "Early stopping:  126.75841872374211\n",
            "Epoch: 006, Loss: 351.0881, Train: 0.1078, Test: 0.0995\n",
            "Early stopping:  49.921849645766194\n",
            "Epoch: 007, Loss: 328.1021, Train: 0.1412, Test: 0.1308\n",
            "Early stopping:  31.760518252055363\n",
            "Epoch: 008, Loss: 273.5945, Train: 0.3165, Test: 0.3130\n",
            "Early stopping:  46.94005881398564\n",
            "Epoch: 009, Loss: 208.5713, Train: 0.3069, Test: 0.2944\n",
            "Early stopping:  58.2032707022713\n",
            "Epoch: 010, Loss: 192.4031, Train: 0.2813, Test: 0.2794\n",
            "Early stopping:  70.27802071816924\n",
            "Epoch: 011, Loss: 146.6178, Train: 0.1248, Test: 0.1225\n",
            "Early stopping:  71.32622202595215\n",
            "Epoch: 012, Loss: 119.9161, Train: 0.1787, Test: 0.1761\n",
            "Early stopping:  59.406723443301146\n",
            "Epoch: 013, Loss: 84.2788, Train: 0.2927, Test: 0.2868\n",
            "Early stopping:  51.14620607656629\n",
            "Epoch: 014, Loss: 57.4444, Train: 0.2791, Test: 0.2695\n",
            "Early stopping:  52.77270780624665\n",
            "Epoch: 015, Loss: 40.9831, Train: 0.2762, Test: 0.2627\n",
            "Early stopping:  43.57660727116753\n",
            "Epoch: 016, Loss: 24.2522, Train: 0.2388, Test: 0.2328\n",
            "Early stopping:  37.68630206877639\n",
            "Epoch: 017, Loss: 9.9908, Train: 0.1872, Test: 0.1773\n",
            "Early stopping:  28.969695063396507\n",
            "Epoch: 018, Loss: 3.6428, Train: 0.2269, Test: 0.2263\n",
            "Early stopping:  22.161169125146532\n",
            "Epoch: 019, Loss: 2.8424, Train: 0.2405, Test: 0.2389\n",
            "Early stopping:  16.229631210709794\n",
            "Epoch: 020, Loss: 2.5578, Train: 0.2609, Test: 0.2563\n",
            "Early stopping:  9.235014696202015\n",
            "Epoch: 021, Loss: 2.3801, Train: 0.2558, Test: 0.2538\n",
            "Early stopping:  3.2272986822782834\n",
            "Epoch: 022, Loss: 2.3227, Train: 0.2394, Test: 0.2455\n",
            "Early stopping:  0.5389676413026185\n",
            "Epoch: 023, Loss: 2.3239, Train: 0.2428, Test: 0.2359\n",
            "Early stopping:  0.22154621662859317\n",
            "Epoch: 024, Loss: 2.3288, Train: 0.2450, Test: 0.2316\n",
            "Early stopping:  0.10079250181712371\n",
            "Epoch: 025, Loss: 2.3192, Train: 0.2467, Test: 0.2297\n",
            "Early stopping:  0.025476915591368225\n",
            "Epoch: 026, Loss: 2.2995, Train: 0.2439, Test: 0.2304\n",
            "Early stopping:  0.011323234261229267\n",
            "Epoch: 027, Loss: 2.2735, Train: 0.2428, Test: 0.2282\n",
            "Early stopping:  0.02275213745286301\n",
            "Epoch: 028, Loss: 2.2499, Train: 0.2462, Test: 0.2315\n",
            "Early stopping:  0.03254464979622107\n",
            "Epoch: 029, Loss: 2.2278, Train: 0.2507, Test: 0.2326\n",
            "Early stopping:  0.036758119413121335\n",
            "Epoch: 030, Loss: 2.2070, Train: 0.2530, Test: 0.2396\n",
            "Early stopping:  0.036502379340872866\n",
            "Epoch: 031, Loss: 2.1886, Train: 0.2558, Test: 0.2467\n",
            "Early stopping:  0.03366090288724879\n",
            "Epoch: 032, Loss: 2.1749, Train: 0.2564, Test: 0.2512\n",
            "Early stopping:  0.030047915784224024\n",
            "Epoch: 033, Loss: 2.1654, Train: 0.2598, Test: 0.2560\n",
            "Early stopping:  0.0250894841102356\n",
            "Epoch: 034, Loss: 2.1556, Train: 0.2604, Test: 0.2608\n",
            "Early stopping:  0.020152837793167587\n",
            "Epoch: 035, Loss: 2.1440, Train: 0.2689, Test: 0.2681\n",
            "Early stopping:  0.0171761312866137\n",
            "Epoch: 036, Loss: 2.1318, Train: 0.2745, Test: 0.2745\n",
            "Early stopping:  0.017032318701882734\n",
            "Epoch: 037, Loss: 2.1196, Train: 0.2859, Test: 0.2787\n",
            "Early stopping:  0.01824243869851279\n",
            "Epoch: 038, Loss: 2.1080, Train: 0.2927, Test: 0.2874\n",
            "Early stopping:  0.018904668052577817\n",
            "Epoch: 039, Loss: 2.0961, Train: 0.3012, Test: 0.2944\n",
            "Early stopping:  0.018914872870528853\n",
            "Epoch: 040, Loss: 2.0838, Train: 0.3114, Test: 0.3076\n",
            "Early stopping:  0.018893998129062492\n",
            "Epoch: 041, Loss: 2.0707, Train: 0.3188, Test: 0.3193\n",
            "Early stopping:  0.019299138010851404\n",
            "Epoch: 042, Loss: 2.0575, Train: 0.3290, Test: 0.3292\n",
            "Early stopping:  0.01997902051693406\n",
            "Epoch: 043, Loss: 2.0438, Train: 0.3381, Test: 0.3399\n",
            "Early stopping:  0.020708845529568604\n",
            "Epoch: 044, Loss: 2.0298, Train: 0.3471, Test: 0.3513\n",
            "Early stopping:  0.021338348850286813\n",
            "Epoch: 045, Loss: 2.0152, Train: 0.3562, Test: 0.3597\n",
            "Early stopping:  0.021929618179078864\n",
            "Epoch: 046, Loss: 2.0001, Train: 0.3647, Test: 0.3686\n",
            "Early stopping:  0.02269298097459351\n",
            "Epoch: 047, Loss: 1.9859, Train: 0.3687, Test: 0.3745\n",
            "Early stopping:  0.023025164341374055\n",
            "Epoch: 048, Loss: 1.9721, Train: 0.3789, Test: 0.3833\n",
            "Early stopping:  0.02291076928754309\n",
            "Epoch: 049, Loss: 1.9583, Train: 0.3880, Test: 0.3904\n",
            "Early stopping:  0.022445437377654802\n",
            "Epoch: 050, Loss: 1.9445, Train: 0.3959, Test: 0.3967\n",
            "Early stopping:  0.021936434756935252\n",
            "Epoch: 051, Loss: 1.9310, Train: 0.3999, Test: 0.3999\n",
            "Early stopping:  0.021720464695125966\n",
            "Epoch: 052, Loss: 1.9179, Train: 0.4050, Test: 0.4028\n",
            "Early stopping:  0.021467036923399296\n",
            "Epoch: 053, Loss: 1.9050, Train: 0.4101, Test: 0.4073\n",
            "Early stopping:  0.02105607845035985\n",
            "Epoch: 054, Loss: 1.8919, Train: 0.4158, Test: 0.4081\n",
            "Early stopping:  0.0207239150188944\n",
            "Epoch: 055, Loss: 1.8777, Train: 0.4186, Test: 0.4096\n",
            "Early stopping:  0.020955529987222464\n",
            "Epoch: 056, Loss: 1.8625, Train: 0.4243, Test: 0.4104\n",
            "Early stopping:  0.021836349699337003\n",
            "Epoch: 057, Loss: 1.8484, Train: 0.4311, Test: 0.4132\n",
            "Early stopping:  0.02256886930323479\n",
            "Epoch: 058, Loss: 1.8340, Train: 0.4334, Test: 0.4139\n",
            "Early stopping:  0.022956410712016564\n",
            "Epoch: 059, Loss: 1.8196, Train: 0.4334, Test: 0.4147\n",
            "Early stopping:  0.022883327582026897\n",
            "Epoch: 060, Loss: 1.8053, Train: 0.4339, Test: 0.4148\n",
            "Early stopping:  0.02264252981688816\n",
            "Epoch: 061, Loss: 1.7914, Train: 0.4351, Test: 0.4166\n",
            "Early stopping:  0.022561936334515552\n",
            "Epoch: 062, Loss: 1.7776, Train: 0.4402, Test: 0.4203\n",
            "Early stopping:  0.022311666267442513\n",
            "Epoch: 063, Loss: 1.7643, Train: 0.4458, Test: 0.4230\n",
            "Early stopping:  0.021859557347890798\n",
            "Epoch: 064, Loss: 1.7516, Train: 0.4526, Test: 0.4269\n",
            "Early stopping:  0.02127294168092985\n",
            "Epoch: 065, Loss: 1.7399, Train: 0.4532, Test: 0.4269\n",
            "Early stopping:  0.020400115913824204\n",
            "Epoch: 066, Loss: 1.7280, Train: 0.4577, Test: 0.4304\n",
            "Early stopping:  0.019545681866651343\n",
            "Epoch: 067, Loss: 1.7147, Train: 0.4611, Test: 0.4315\n",
            "Early stopping:  0.01942879979909354\n",
            "Epoch: 068, Loss: 1.7008, Train: 0.4628, Test: 0.4334\n",
            "Early stopping:  0.02004887137171523\n",
            "Epoch: 069, Loss: 1.6860, Train: 0.4634, Test: 0.4355\n",
            "Early stopping:  0.021355083883365086\n",
            "Epoch: 070, Loss: 1.6696, Train: 0.4606, Test: 0.4379\n",
            "Early stopping:  0.023007559058195824\n",
            "Epoch: 071, Loss: 1.6566, Train: 0.4611, Test: 0.4382\n",
            "Early stopping:  0.02330438578640997\n",
            "Epoch: 072, Loss: 1.6414, Train: 0.4714, Test: 0.4422\n",
            "Early stopping:  0.023435301608882957\n",
            "Epoch: 073, Loss: 1.6287, Train: 0.4770, Test: 0.4429\n",
            "Early stopping:  0.02260139447287321\n",
            "Epoch: 074, Loss: 1.6182, Train: 0.4782, Test: 0.4449\n",
            "Early stopping:  0.020722800617910556\n",
            "Epoch: 075, Loss: 1.6047, Train: 0.4810, Test: 0.4490\n",
            "Early stopping:  0.020119658858641806\n",
            "Epoch: 076, Loss: 1.5892, Train: 0.4889, Test: 0.4522\n",
            "Early stopping:  0.02035481976626767\n",
            "Epoch: 077, Loss: 1.5746, Train: 0.4912, Test: 0.4541\n",
            "Early stopping:  0.021735348822935117\n",
            "Epoch: 078, Loss: 1.5618, Train: 0.5077, Test: 0.4568\n",
            "Early stopping:  0.022600243536052375\n",
            "Epoch: 079, Loss: 1.5468, Train: 0.5122, Test: 0.4613\n",
            "Early stopping:  0.02266643988621118\n",
            "Epoch: 080, Loss: 1.5336, Train: 0.5173, Test: 0.4619\n",
            "Early stopping:  0.021974822783441202\n",
            "Epoch: 081, Loss: 1.5215, Train: 0.5190, Test: 0.4630\n",
            "Early stopping:  0.02126312913475934\n",
            "Epoch: 082, Loss: 1.5087, Train: 0.5224, Test: 0.4684\n",
            "Early stopping:  0.020805680484330333\n",
            "Epoch: 083, Loss: 1.4952, Train: 0.5258, Test: 0.4707\n",
            "Early stopping:  0.020246256536731767\n",
            "Epoch: 084, Loss: 1.4813, Train: 0.5309, Test: 0.4726\n",
            "Early stopping:  0.020717067777062045\n",
            "Epoch: 085, Loss: 1.4681, Train: 0.5372, Test: 0.4768\n",
            "Early stopping:  0.02123127204043556\n",
            "Epoch: 086, Loss: 1.4562, Train: 0.5400, Test: 0.4796\n",
            "Early stopping:  0.020891536359227046\n",
            "Epoch: 087, Loss: 1.4457, Train: 0.5451, Test: 0.4790\n",
            "Early stopping:  0.01966023266754928\n",
            "Epoch: 088, Loss: 1.4345, Train: 0.5474, Test: 0.4799\n",
            "Early stopping:  0.018349174395486282\n",
            "Epoch: 089, Loss: 1.4228, Train: 0.5547, Test: 0.4828\n",
            "Early stopping:  0.017743947631457396\n",
            "Epoch: 090, Loss: 1.4108, Train: 0.5621, Test: 0.4858\n",
            "Early stopping:  0.017973724519282756\n",
            "Epoch: 091, Loss: 1.3980, Train: 0.5655, Test: 0.4843\n",
            "Early stopping:  0.01883436174046572\n",
            "Epoch: 092, Loss: 1.3854, Train: 0.5701, Test: 0.4858\n",
            "Early stopping:  0.0194688840337411\n",
            "Epoch: 093, Loss: 1.3738, Train: 0.5723, Test: 0.4885\n",
            "Early stopping:  0.019516329091587844\n",
            "Epoch: 094, Loss: 1.3624, Train: 0.5757, Test: 0.4897\n",
            "Early stopping:  0.019134954259028433\n",
            "Epoch: 095, Loss: 1.3517, Train: 0.5797, Test: 0.4899\n",
            "Early stopping:  0.018273146598312313\n",
            "Epoch: 096, Loss: 1.3407, Train: 0.5808, Test: 0.4905\n",
            "Early stopping:  0.01764465855384815\n",
            "Epoch: 097, Loss: 1.3300, Train: 0.5831, Test: 0.4902\n",
            "Early stopping:  0.017297180625039998\n",
            "Epoch: 098, Loss: 1.3200, Train: 0.5814, Test: 0.4905\n",
            "Early stopping:  0.016837553329797427\n",
            "Epoch: 099, Loss: 1.3106, Train: 0.5871, Test: 0.4933\n",
            "Early stopping:  0.01625738549894034\n",
            "Epoch: 100, Loss: 1.3012, Train: 0.5927, Test: 0.4987\n",
            "Early stopping:  0.015566183657269122\n",
            "Epoch: 101, Loss: 1.2914, Train: 0.5984, Test: 0.5020\n",
            "Early stopping:  0.015214401758973558\n",
            "Epoch: 102, Loss: 1.2823, Train: 0.6047, Test: 0.5045\n",
            "Early stopping:  0.014977841354298674\n",
            "Epoch: 103, Loss: 1.2735, Train: 0.6081, Test: 0.5050\n",
            "Early stopping:  0.014744106111923637\n",
            "Epoch: 104, Loss: 1.2644, Train: 0.6058, Test: 0.5058\n",
            "Early stopping:  0.014443224241582375\n",
            "Epoch: 105, Loss: 1.2556, Train: 0.6064, Test: 0.5071\n",
            "Early stopping:  0.01414694740871509\n",
            "Epoch: 106, Loss: 1.2469, Train: 0.6098, Test: 0.5058\n",
            "Early stopping:  0.014016301345096845\n",
            "Epoch: 107, Loss: 1.2391, Train: 0.6137, Test: 0.5071\n",
            "Early stopping:  0.01365023346634341\n",
            "Epoch: 108, Loss: 1.2313, Train: 0.6160, Test: 0.5079\n",
            "Early stopping:  0.013081987617775173\n",
            "Epoch: 109, Loss: 1.2234, Train: 0.6160, Test: 0.5109\n",
            "Early stopping:  0.012628571951086656\n",
            "Epoch: 110, Loss: 1.2151, Train: 0.6177, Test: 0.5130\n",
            "Early stopping:  0.012538883343859357\n",
            "Epoch: 111, Loss: 1.2077, Train: 0.6211, Test: 0.5142\n",
            "Early stopping:  0.01247949595274195\n",
            "Epoch: 112, Loss: 1.2004, Train: 0.6234, Test: 0.5135\n",
            "Early stopping:  0.012278135452921987\n",
            "Epoch: 113, Loss: 1.1921, Train: 0.6268, Test: 0.5150\n",
            "Early stopping:  0.012225198833223132\n",
            "Epoch: 114, Loss: 1.1860, Train: 0.6296, Test: 0.5164\n",
            "Early stopping:  0.011684808878590795\n",
            "Epoch: 115, Loss: 1.1776, Train: 0.6330, Test: 0.5162\n",
            "Early stopping:  0.011823575813463221\n",
            "Epoch: 116, Loss: 1.1696, Train: 0.6330, Test: 0.5180\n",
            "Early stopping:  0.012037826129184673\n",
            "Epoch: 117, Loss: 1.1639, Train: 0.6410, Test: 0.5179\n",
            "Early stopping:  0.011521009953607152\n",
            "Epoch: 118, Loss: 1.1561, Train: 0.6466, Test: 0.5211\n",
            "Early stopping:  0.011619743795373307\n",
            "Epoch: 119, Loss: 1.1483, Train: 0.6444, Test: 0.5198\n",
            "Early stopping:  0.011406125785487425\n",
            "Epoch: 120, Loss: 1.1414, Train: 0.6495, Test: 0.5231\n",
            "Early stopping:  0.011409184057792862\n",
            "Epoch: 121, Loss: 1.1332, Train: 0.6523, Test: 0.5234\n",
            "Early stopping:  0.012030706892492651\n",
            "Epoch: 122, Loss: 1.1265, Train: 0.6529, Test: 0.5244\n",
            "Early stopping:  0.011744733794638829\n",
            "Epoch: 123, Loss: 1.1184, Train: 0.6529, Test: 0.5240\n",
            "Early stopping:  0.011814643792067068\n",
            "Epoch: 124, Loss: 1.1121, Train: 0.6597, Test: 0.5272\n",
            "Early stopping:  0.011619812690899543\n",
            "Epoch: 125, Loss: 1.1052, Train: 0.6625, Test: 0.5271\n",
            "Early stopping:  0.011159526159453302\n",
            "Epoch: 126, Loss: 1.0983, Train: 0.6619, Test: 0.5264\n",
            "Early stopping:  0.01103892342308993\n",
            "Epoch: 127, Loss: 1.0913, Train: 0.6636, Test: 0.5298\n",
            "Early stopping:  0.010779294393999935\n",
            "Epoch: 128, Loss: 1.0840, Train: 0.6693, Test: 0.5299\n",
            "Early stopping:  0.011082268070753404\n",
            "Epoch: 129, Loss: 1.0773, Train: 0.6682, Test: 0.5306\n",
            "Early stopping:  0.011051984091262748\n",
            "Epoch: 130, Loss: 1.0707, Train: 0.6699, Test: 0.5310\n",
            "Early stopping:  0.010926023274692016\n",
            "Epoch: 131, Loss: 1.0664, Train: 0.6727, Test: 0.5333\n",
            "Early stopping:  0.010020311848074306\n",
            "Epoch: 132, Loss: 1.0585, Train: 0.6756, Test: 0.5336\n",
            "Early stopping:  0.009839804376723271\n",
            "PREDICTIONS -> tensor([ 0,  0,  5,  ..., 11,  5,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.52      0.50       758\n",
            "         capital_goods       0.36      0.34      0.35       508\n",
            "conglomerates_industry       0.00      0.00      0.00        80\n",
            "     consumer_cyclical       0.55      0.43      0.48       793\n",
            " consumer_non-cyclical       0.62      0.38      0.47       446\n",
            "                energy       0.46      0.49      0.47       283\n",
            "             financial       0.61      0.55      0.58       767\n",
            "            healthcare       0.64      0.37      0.47       318\n",
            "              services       0.54      0.75      0.63      2076\n",
            "            technology       0.37      0.27      0.31       396\n",
            "        transportation       0.62      0.62      0.62       404\n",
            "             utilities       0.69      0.42      0.52       225\n",
            "\n",
            "              accuracy                           0.53      7054\n",
            "             macro avg       0.50      0.43      0.45      7054\n",
            "          weighted avg       0.53      0.53      0.52      7054\n",
            "\n",
            "time: 3min 11s (started: 2024-10-16 21:36:03 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuS-bLhtgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368bb909-62e5-4510-b95d-45f3c506ac96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 432 ms (started: 2024-10-16 21:39:14 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AO1mrrwgWjL"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "057NhqlBgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3fca90-1c55-4535-c595-25a6dc9ad659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4893, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2951, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.13734022631978143\n",
            "Epoch: 003, Loss: 2.1681, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16177291560517465\n",
            "Epoch: 004, Loss: 2.1185, Train: 0.3103, Test: 0.3073\n",
            "Early stopping:  0.16537562262715663\n",
            "Epoch: 005, Loss: 2.0560, Train: 0.3681, Test: 0.3578\n",
            "Early stopping:  0.1716979902216828\n",
            "Epoch: 006, Loss: 2.0008, Train: 0.4084, Test: 0.3901\n",
            "Early stopping:  0.11288105895967508\n",
            "Epoch: 007, Loss: 1.9310, Train: 0.4090, Test: 0.3914\n",
            "Early stopping:  0.09370945089202214\n",
            "Epoch: 008, Loss: 1.8553, Train: 0.3993, Test: 0.3833\n",
            "Early stopping:  0.1031492202542417\n",
            "Epoch: 009, Loss: 1.7902, Train: 0.4095, Test: 0.3910\n",
            "Early stopping:  0.10716939426699298\n",
            "Epoch: 010, Loss: 1.7305, Train: 0.4441, Test: 0.4249\n",
            "Early stopping:  0.10784884261508029\n",
            "Epoch: 011, Loss: 1.6631, Train: 0.4963, Test: 0.4651\n",
            "Early stopping:  0.1045472248249589\n",
            "Epoch: 012, Loss: 1.5961, Train: 0.5179, Test: 0.4845\n",
            "Early stopping:  0.10211153985668592\n",
            "Epoch: 013, Loss: 1.5349, Train: 0.5343, Test: 0.4935\n",
            "Early stopping:  0.10202003058253907\n",
            "Epoch: 014, Loss: 1.4716, Train: 0.5406, Test: 0.5035\n",
            "Early stopping:  0.1021538404906728\n",
            "Epoch: 015, Loss: 1.4099, Train: 0.5610, Test: 0.5181\n",
            "Early stopping:  0.09974369002496764\n",
            "Epoch: 016, Loss: 1.3545, Train: 0.5865, Test: 0.5377\n",
            "Early stopping:  0.09617395403379501\n",
            "Epoch: 017, Loss: 1.2985, Train: 0.6098, Test: 0.5567\n",
            "Early stopping:  0.09330546179795222\n",
            "Epoch: 018, Loss: 1.2463, Train: 0.6319, Test: 0.5751\n",
            "Early stopping:  0.08891435829871669\n",
            "Epoch: 019, Loss: 1.1964, Train: 0.6438, Test: 0.5875\n",
            "Early stopping:  0.08467699502342001\n",
            "Epoch: 020, Loss: 1.1498, Train: 0.6512, Test: 0.5950\n",
            "Early stopping:  0.08094900169575059\n",
            "Epoch: 021, Loss: 1.1079, Train: 0.6739, Test: 0.6001\n",
            "Early stopping:  0.07562103772768702\n",
            "Epoch: 022, Loss: 1.0667, Train: 0.6892, Test: 0.6046\n",
            "Early stopping:  0.07083830747585725\n",
            "Epoch: 023, Loss: 1.0297, Train: 0.6960, Test: 0.6063\n",
            "Early stopping:  0.065902055060145\n",
            "Epoch: 024, Loss: 0.9935, Train: 0.7005, Test: 0.6126\n",
            "Early stopping:  0.061821010015178965\n",
            "Epoch: 025, Loss: 0.9595, Train: 0.7085, Test: 0.6171\n",
            "Early stopping:  0.05854516964872746\n",
            "Epoch: 026, Loss: 0.9259, Train: 0.7204, Test: 0.6209\n",
            "Early stopping:  0.05565063363958725\n",
            "Epoch: 027, Loss: 0.8929, Train: 0.7277, Test: 0.6277\n",
            "Early stopping:  0.05394183646985766\n",
            "Epoch: 028, Loss: 0.8611, Train: 0.7345, Test: 0.6324\n",
            "Early stopping:  0.05238114328102305\n",
            "Epoch: 029, Loss: 0.8321, Train: 0.7465, Test: 0.6345\n",
            "Early stopping:  0.05053166379146035\n",
            "Epoch: 030, Loss: 0.8028, Train: 0.7544, Test: 0.6360\n",
            "Early stopping:  0.04855949591957498\n",
            "Epoch: 031, Loss: 0.7739, Train: 0.7646, Test: 0.6399\n",
            "Early stopping:  0.04687828868287343\n",
            "Epoch: 032, Loss: 0.7458, Train: 0.7765, Test: 0.6437\n",
            "Early stopping:  0.04568649911886148\n",
            "Epoch: 033, Loss: 0.7195, Train: 0.7873, Test: 0.6472\n",
            "Early stopping:  0.04461905072063153\n",
            "Epoch: 034, Loss: 0.6925, Train: 0.8009, Test: 0.6506\n",
            "Early stopping:  0.043468975843211535\n",
            "Epoch: 035, Loss: 0.6669, Train: 0.8088, Test: 0.6528\n",
            "Early stopping:  0.04225299626564245\n",
            "Epoch: 036, Loss: 0.6427, Train: 0.8225, Test: 0.6540\n",
            "Early stopping:  0.04091335548914219\n",
            "Epoch: 037, Loss: 0.6186, Train: 0.8321, Test: 0.6586\n",
            "Early stopping:  0.03981918351187583\n",
            "Epoch: 038, Loss: 0.5951, Train: 0.8406, Test: 0.6581\n",
            "Early stopping:  0.03845594390066315\n",
            "Epoch: 039, Loss: 0.5720, Train: 0.8514, Test: 0.6582\n",
            "Early stopping:  0.037556982778789626\n",
            "Epoch: 040, Loss: 0.5495, Train: 0.8627, Test: 0.6579\n",
            "Early stopping:  0.03686803739192765\n",
            "Epoch: 041, Loss: 0.5277, Train: 0.8701, Test: 0.6610\n",
            "Early stopping:  0.035948854750382085\n",
            "Epoch: 042, Loss: 0.5062, Train: 0.8843, Test: 0.6656\n",
            "Early stopping:  0.03510794939221521\n",
            "Epoch: 043, Loss: 0.4850, Train: 0.8917, Test: 0.6657\n",
            "Early stopping:  0.03433939734772621\n",
            "Epoch: 044, Loss: 0.4646, Train: 0.9036, Test: 0.6690\n",
            "Early stopping:  0.03357562263854987\n",
            "Epoch: 045, Loss: 0.4446, Train: 0.9121, Test: 0.6681\n",
            "Early stopping:  0.032852175632575996\n",
            "Epoch: 046, Loss: 0.4247, Train: 0.9166, Test: 0.6678\n",
            "Early stopping:  0.03215241134767111\n",
            "Epoch: 047, Loss: 0.4058, Train: 0.9212, Test: 0.6683\n",
            "Early stopping:  0.03137045568397185\n",
            "Epoch: 048, Loss: 0.3873, Train: 0.9268, Test: 0.6691\n",
            "Early stopping:  0.03059342365180808\n",
            "Epoch: 049, Loss: 0.3705, Train: 0.9387, Test: 0.6683\n",
            "Early stopping:  0.029374056080005602\n",
            "Epoch: 050, Loss: 0.3577, Train: 0.9229, Test: 0.6661\n",
            "Early stopping:  0.026849698603591186\n",
            "Epoch: 051, Loss: 0.3518, Train: 0.9444, Test: 0.6579\n",
            "Early stopping:  0.02211675403672463\n",
            "Epoch: 052, Loss: 0.3408, Train: 0.9490, Test: 0.6725\n",
            "Early stopping:  0.017927472457821534\n",
            "Epoch: 053, Loss: 0.3092, Train: 0.9450, Test: 0.6715\n",
            "Early stopping:  0.023191292852887256\n",
            "Epoch: 054, Loss: 0.3030, Train: 0.9575, Test: 0.6615\n",
            "Early stopping:  0.024959301283506232\n",
            "Epoch: 055, Loss: 0.2970, Train: 0.9631, Test: 0.6713\n",
            "Early stopping:  0.024382715247875487\n",
            "Epoch: 056, Loss: 0.2712, Train: 0.9575, Test: 0.6713\n",
            "Early stopping:  0.02504957704384814\n",
            "Epoch: 057, Loss: 0.2716, Train: 0.9671, Test: 0.6640\n",
            "Early stopping:  0.01788875773780993\n",
            "Epoch: 058, Loss: 0.2582, Train: 0.9711, Test: 0.6671\n",
            "Early stopping:  0.018985586834155504\n",
            "Epoch: 059, Loss: 0.2423, Train: 0.9626, Test: 0.6690\n",
            "Early stopping:  0.020128112559659694\n",
            "Epoch: 060, Loss: 0.2436, Train: 0.9773, Test: 0.6678\n",
            "Early stopping:  0.014227417373963117\n",
            "Epoch: 061, Loss: 0.2249, Train: 0.9807, Test: 0.6643\n",
            "Early stopping:  0.017651641648847232\n",
            "Epoch: 062, Loss: 0.2199, Train: 0.9733, Test: 0.6722\n",
            "Early stopping:  0.015453700883365116\n",
            "Epoch: 063, Loss: 0.2136, Train: 0.9818, Test: 0.6714\n",
            "Early stopping:  0.013467227140306572\n",
            "Epoch: 064, Loss: 0.2005, Train: 0.9853, Test: 0.6659\n",
            "Early stopping:  0.01580982624661537\n",
            "Epoch: 065, Loss: 0.1988, Train: 0.9847, Test: 0.6707\n",
            "Early stopping:  0.011583763152864184\n",
            "Epoch: 066, Loss: 0.1891, Train: 0.9853, Test: 0.6698\n",
            "Early stopping:  0.012329839039149805\n",
            "Epoch: 067, Loss: 0.1830, Train: 0.9904, Test: 0.6661\n",
            "Early stopping:  0.011724434767684462\n",
            "Epoch: 068, Loss: 0.1794, Train: 0.9892, Test: 0.6677\n",
            "Early stopping:  0.009339462300513282\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.68      0.67       758\n",
            "         capital_goods       0.56      0.58      0.57       508\n",
            "conglomerates_industry       0.82      0.17      0.29        80\n",
            "     consumer_cyclical       0.60      0.65      0.62       793\n",
            " consumer_non-cyclical       0.72      0.58      0.64       446\n",
            "                energy       0.81      0.67      0.74       283\n",
            "             financial       0.72      0.68      0.70       767\n",
            "            healthcare       0.79      0.72      0.75       318\n",
            "              services       0.65      0.75      0.69      2076\n",
            "            technology       0.56      0.47      0.51       396\n",
            "        transportation       0.80      0.73      0.76       404\n",
            "             utilities       0.79      0.67      0.72       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.71      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5060, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2868, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1550446000926616\n",
            "Epoch: 003, Loss: 2.1617, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1743179843289044\n",
            "Epoch: 004, Loss: 2.1338, Train: 0.3023, Test: 0.3017\n",
            "Early stopping:  0.16956686472841576\n",
            "Epoch: 005, Loss: 2.0774, Train: 0.3483, Test: 0.3412\n",
            "Early stopping:  0.17071641790210376\n",
            "Epoch: 006, Loss: 2.0101, Train: 0.3834, Test: 0.3744\n",
            "Early stopping:  0.103266890695499\n",
            "Epoch: 007, Loss: 1.9461, Train: 0.3982, Test: 0.3891\n",
            "Early stopping:  0.08857013981911122\n",
            "Epoch: 008, Loss: 1.8860, Train: 0.4090, Test: 0.3950\n",
            "Early stopping:  0.09916186631832924\n",
            "Epoch: 009, Loss: 1.8284, Train: 0.4180, Test: 0.4060\n",
            "Early stopping:  0.09841343589614773\n",
            "Epoch: 010, Loss: 1.7679, Train: 0.4441, Test: 0.4244\n",
            "Early stopping:  0.0952256241420375\n",
            "Epoch: 011, Loss: 1.7013, Train: 0.4725, Test: 0.4444\n",
            "Early stopping:  0.0961187781122954\n",
            "Epoch: 012, Loss: 1.6339, Train: 0.4929, Test: 0.4592\n",
            "Early stopping:  0.09986300957059092\n",
            "Epoch: 013, Loss: 1.5735, Train: 0.5031, Test: 0.4718\n",
            "Early stopping:  0.10180123536297186\n",
            "Epoch: 014, Loss: 1.5150, Train: 0.5218, Test: 0.4881\n",
            "Early stopping:  0.10022294525030442\n",
            "Epoch: 015, Loss: 1.4527, Train: 0.5440, Test: 0.5060\n",
            "Early stopping:  0.09745162753479397\n",
            "Epoch: 016, Loss: 1.3926, Train: 0.5774, Test: 0.5320\n",
            "Early stopping:  0.0954312403540913\n",
            "Epoch: 017, Loss: 1.3371, Train: 0.6052, Test: 0.5542\n",
            "Early stopping:  0.09414439077265922\n",
            "Epoch: 018, Loss: 1.2853, Train: 0.6245, Test: 0.5675\n",
            "Early stopping:  0.09097574726538177\n",
            "Epoch: 019, Loss: 1.2364, Train: 0.6330, Test: 0.5705\n",
            "Early stopping:  0.08542995609588261\n",
            "Epoch: 020, Loss: 1.1882, Train: 0.6415, Test: 0.5757\n",
            "Early stopping:  0.0805681744105073\n",
            "Epoch: 021, Loss: 1.1436, Train: 0.6529, Test: 0.5831\n",
            "Early stopping:  0.07655251467081216\n",
            "Epoch: 022, Loss: 1.1028, Train: 0.6699, Test: 0.5910\n",
            "Early stopping:  0.07244052179859899\n",
            "Epoch: 023, Loss: 1.0630, Train: 0.6824, Test: 0.5987\n",
            "Early stopping:  0.06839689196663147\n",
            "Epoch: 024, Loss: 1.0254, Train: 0.6892, Test: 0.6062\n",
            "Early stopping:  0.06426978405802256\n",
            "Epoch: 025, Loss: 0.9899, Train: 0.6982, Test: 0.6087\n",
            "Early stopping:  0.060868748927999004\n",
            "Epoch: 026, Loss: 0.9554, Train: 0.7073, Test: 0.6140\n",
            "Early stopping:  0.05817989073750077\n",
            "Epoch: 027, Loss: 0.9227, Train: 0.7130, Test: 0.6170\n",
            "Early stopping:  0.055450204631096324\n",
            "Epoch: 028, Loss: 0.8909, Train: 0.7198, Test: 0.6235\n",
            "Early stopping:  0.053178685798699124\n",
            "Epoch: 029, Loss: 0.8592, Train: 0.7323, Test: 0.6293\n",
            "Early stopping:  0.051546030078588025\n",
            "Epoch: 030, Loss: 0.8287, Train: 0.7453, Test: 0.6311\n",
            "Early stopping:  0.050110799836321405\n",
            "Epoch: 031, Loss: 0.7992, Train: 0.7601, Test: 0.6325\n",
            "Early stopping:  0.04888742141206717\n",
            "Epoch: 032, Loss: 0.7701, Train: 0.7697, Test: 0.6345\n",
            "Early stopping:  0.04770609602404527\n",
            "Epoch: 033, Loss: 0.7425, Train: 0.7788, Test: 0.6378\n",
            "Early stopping:  0.046197660514047854\n",
            "Epoch: 034, Loss: 0.7163, Train: 0.7896, Test: 0.6429\n",
            "Early stopping:  0.044548206343325285\n",
            "Epoch: 035, Loss: 0.6897, Train: 0.7986, Test: 0.6443\n",
            "Early stopping:  0.04313753004974221\n",
            "Epoch: 036, Loss: 0.6647, Train: 0.8094, Test: 0.6466\n",
            "Early stopping:  0.041668286736454736\n",
            "Epoch: 037, Loss: 0.6403, Train: 0.8213, Test: 0.6472\n",
            "Early stopping:  0.040469545722305524\n",
            "Epoch: 038, Loss: 0.6164, Train: 0.8338, Test: 0.6473\n",
            "Early stopping:  0.03940128562471061\n",
            "Epoch: 039, Loss: 0.5934, Train: 0.8446, Test: 0.6490\n",
            "Early stopping:  0.03809367988195959\n",
            "Epoch: 040, Loss: 0.5711, Train: 0.8531, Test: 0.6515\n",
            "Early stopping:  0.03702118016971673\n",
            "Epoch: 041, Loss: 0.5484, Train: 0.8627, Test: 0.6530\n",
            "Early stopping:  0.03622422497030363\n",
            "Epoch: 042, Loss: 0.5267, Train: 0.8758, Test: 0.6562\n",
            "Early stopping:  0.035479641504046755\n",
            "Epoch: 043, Loss: 0.5053, Train: 0.8803, Test: 0.6566\n",
            "Early stopping:  0.03487919591867577\n",
            "Epoch: 044, Loss: 0.4841, Train: 0.8917, Test: 0.6572\n",
            "Early stopping:  0.0343411288927529\n",
            "Epoch: 045, Loss: 0.4637, Train: 0.9002, Test: 0.6576\n",
            "Early stopping:  0.03352778490521916\n",
            "Epoch: 046, Loss: 0.4439, Train: 0.9104, Test: 0.6599\n",
            "Early stopping:  0.03276975638953898\n",
            "Epoch: 047, Loss: 0.4245, Train: 0.9166, Test: 0.6618\n",
            "Early stopping:  0.031898906762875885\n",
            "Epoch: 048, Loss: 0.4059, Train: 0.9223, Test: 0.6619\n",
            "Early stopping:  0.03092735211004795\n",
            "Epoch: 049, Loss: 0.3875, Train: 0.9297, Test: 0.6644\n",
            "Early stopping:  0.030109943774926252\n",
            "Epoch: 050, Loss: 0.3698, Train: 0.9348, Test: 0.6639\n",
            "Early stopping:  0.029268173914716592\n",
            "Epoch: 051, Loss: 0.3528, Train: 0.9427, Test: 0.6637\n",
            "Early stopping:  0.028398764662069812\n",
            "Epoch: 052, Loss: 0.3363, Train: 0.9490, Test: 0.6639\n",
            "Early stopping:  0.02749577873075113\n",
            "Epoch: 053, Loss: 0.3206, Train: 0.9529, Test: 0.6659\n",
            "Early stopping:  0.02648387264538508\n",
            "Epoch: 054, Loss: 0.3054, Train: 0.9614, Test: 0.6659\n",
            "Early stopping:  0.025466482191435835\n",
            "Epoch: 055, Loss: 0.2911, Train: 0.9586, Test: 0.6656\n",
            "Early stopping:  0.024403841231385\n",
            "Epoch: 056, Loss: 0.2783, Train: 0.9688, Test: 0.6602\n",
            "Early stopping:  0.023034888871701145\n",
            "Epoch: 057, Loss: 0.2676, Train: 0.9631, Test: 0.6646\n",
            "Early stopping:  0.021090921085290773\n",
            "Epoch: 058, Loss: 0.2587, Train: 0.9773, Test: 0.6619\n",
            "Early stopping:  0.018575842124364902\n",
            "Epoch: 059, Loss: 0.2453, Train: 0.9790, Test: 0.6650\n",
            "Early stopping:  0.01760505837748259\n",
            "Epoch: 060, Loss: 0.2295, Train: 0.9807, Test: 0.6637\n",
            "Early stopping:  0.019083225545320805\n",
            "Epoch: 061, Loss: 0.2213, Train: 0.9864, Test: 0.6619\n",
            "Early stopping:  0.01935376867943969\n",
            "Epoch: 062, Loss: 0.2146, Train: 0.9847, Test: 0.6639\n",
            "Early stopping:  0.01800420598317382\n",
            "Epoch: 063, Loss: 0.2022, Train: 0.9881, Test: 0.6632\n",
            "Early stopping:  0.01617708545178134\n",
            "Epoch: 064, Loss: 0.1926, Train: 0.9921, Test: 0.6601\n",
            "Early stopping:  0.014752405024259204\n",
            "Epoch: 065, Loss: 0.1876, Train: 0.9898, Test: 0.6642\n",
            "Early stopping:  0.014250753160813296\n",
            "Epoch: 066, Loss: 0.1792, Train: 0.9932, Test: 0.6612\n",
            "Early stopping:  0.0136243814820419\n",
            "Epoch: 067, Loss: 0.1710, Train: 0.9943, Test: 0.6615\n",
            "Early stopping:  0.012018345054165304\n",
            "Epoch: 068, Loss: 0.1657, Train: 0.9943, Test: 0.6637\n",
            "Early stopping:  0.011185221954987643\n",
            "Epoch: 069, Loss: 0.1594, Train: 0.9960, Test: 0.6602\n",
            "Early stopping:  0.011105587068850559\n",
            "Epoch: 070, Loss: 0.1541, Train: 0.9949, Test: 0.6620\n",
            "Early stopping:  0.009795164042896867\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.67      0.67       758\n",
            "         capital_goods       0.54      0.59      0.57       508\n",
            "conglomerates_industry       0.80      0.20      0.32        80\n",
            "     consumer_cyclical       0.59      0.62      0.61       793\n",
            " consumer_non-cyclical       0.72      0.59      0.65       446\n",
            "                energy       0.78      0.66      0.72       283\n",
            "             financial       0.69      0.68      0.68       767\n",
            "            healthcare       0.75      0.70      0.72       318\n",
            "              services       0.65      0.74      0.69      2076\n",
            "            technology       0.60      0.52      0.56       396\n",
            "        transportation       0.76      0.72      0.74       404\n",
            "             utilities       0.81      0.62      0.70       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.70      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5288, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2930, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16671691690666265\n",
            "Epoch: 003, Loss: 2.1712, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.18177304836563976\n",
            "Epoch: 004, Loss: 2.1536, Train: 0.2984, Test: 0.2971\n",
            "Early stopping:  0.17290322361408683\n",
            "Epoch: 005, Loss: 2.0905, Train: 0.3381, Test: 0.3279\n",
            "Early stopping:  0.17354022856243878\n",
            "Epoch: 006, Loss: 2.0239, Train: 0.3738, Test: 0.3655\n",
            "Early stopping:  0.10034876302991545\n",
            "Epoch: 007, Loss: 1.9654, Train: 0.3948, Test: 0.3830\n",
            "Early stopping:  0.08677067907014194\n",
            "Epoch: 008, Loss: 1.9092, Train: 0.3959, Test: 0.3879\n",
            "Early stopping:  0.0971232014380942\n",
            "Epoch: 009, Loss: 1.8532, Train: 0.4044, Test: 0.3933\n",
            "Early stopping:  0.09325818555251396\n",
            "Epoch: 010, Loss: 1.7967, Train: 0.4226, Test: 0.4073\n",
            "Early stopping:  0.08959126858777214\n",
            "Epoch: 011, Loss: 1.7382, Train: 0.4441, Test: 0.4271\n",
            "Early stopping:  0.0896219237431441\n",
            "Epoch: 012, Loss: 1.6778, Train: 0.4691, Test: 0.4495\n",
            "Early stopping:  0.09135702626357807\n",
            "Epoch: 013, Loss: 1.6195, Train: 0.4884, Test: 0.4626\n",
            "Early stopping:  0.09269763624028987\n",
            "Epoch: 014, Loss: 1.5639, Train: 0.5060, Test: 0.4743\n",
            "Early stopping:  0.09242279269059937\n",
            "Epoch: 015, Loss: 1.5070, Train: 0.5201, Test: 0.4861\n",
            "Early stopping:  0.09115200093507929\n",
            "Epoch: 016, Loss: 1.4507, Train: 0.5417, Test: 0.5055\n",
            "Early stopping:  0.08960842609093879\n",
            "Epoch: 017, Loss: 1.3975, Train: 0.5678, Test: 0.5252\n",
            "Early stopping:  0.08810275895865133\n",
            "Epoch: 018, Loss: 1.3463, Train: 0.5888, Test: 0.5437\n",
            "Early stopping:  0.0861207814398173\n",
            "Epoch: 019, Loss: 1.2972, Train: 0.6086, Test: 0.5553\n",
            "Early stopping:  0.0828702478850581\n",
            "Epoch: 020, Loss: 1.2498, Train: 0.6205, Test: 0.5645\n",
            "Early stopping:  0.0794203950221199\n",
            "Epoch: 021, Loss: 1.2048, Train: 0.6341, Test: 0.5700\n",
            "Early stopping:  0.07622654476288428\n",
            "Epoch: 022, Loss: 1.1634, Train: 0.6449, Test: 0.5795\n",
            "Early stopping:  0.07249660829814306\n",
            "Epoch: 023, Loss: 1.1239, Train: 0.6540, Test: 0.5835\n",
            "Early stopping:  0.06851925345600642\n",
            "Epoch: 024, Loss: 1.0876, Train: 0.6682, Test: 0.5934\n",
            "Early stopping:  0.06413589495914489\n",
            "Epoch: 025, Loss: 1.0528, Train: 0.6784, Test: 0.5981\n",
            "Early stopping:  0.06007504752301018\n",
            "Epoch: 026, Loss: 1.0188, Train: 0.6841, Test: 0.6052\n",
            "Early stopping:  0.05695727560358974\n",
            "Epoch: 027, Loss: 0.9882, Train: 0.6926, Test: 0.6083\n",
            "Early stopping:  0.053812420677910464\n",
            "Epoch: 028, Loss: 0.9583, Train: 0.7033, Test: 0.6134\n",
            "Early stopping:  0.05114199824211027\n",
            "Epoch: 029, Loss: 0.9281, Train: 0.7198, Test: 0.6167\n",
            "Early stopping:  0.04905040496725663\n",
            "Epoch: 030, Loss: 0.8976, Train: 0.7277, Test: 0.6215\n",
            "Early stopping:  0.04785458648479392\n",
            "Epoch: 031, Loss: 0.8688, Train: 0.7391, Test: 0.6212\n",
            "Early stopping:  0.04733935256581166\n",
            "Epoch: 032, Loss: 0.8412, Train: 0.7521, Test: 0.6276\n",
            "Early stopping:  0.04640527591760639\n",
            "Epoch: 033, Loss: 0.8137, Train: 0.7589, Test: 0.6340\n",
            "Early stopping:  0.04508612946776776\n",
            "Epoch: 034, Loss: 0.7860, Train: 0.7708, Test: 0.6396\n",
            "Early stopping:  0.04400366586132187\n",
            "Epoch: 035, Loss: 0.7601, Train: 0.7788, Test: 0.6412\n",
            "Early stopping:  0.04312003747357333\n",
            "Epoch: 036, Loss: 0.7347, Train: 0.7890, Test: 0.6422\n",
            "Early stopping:  0.04215931032637394\n",
            "Epoch: 037, Loss: 0.7099, Train: 0.7941, Test: 0.6454\n",
            "Early stopping:  0.04094398617974777\n",
            "Epoch: 038, Loss: 0.6852, Train: 0.8049, Test: 0.6483\n",
            "Early stopping:  0.03981668935459712\n",
            "Epoch: 039, Loss: 0.6613, Train: 0.8202, Test: 0.6504\n",
            "Early stopping:  0.039068482418856444\n",
            "Epoch: 040, Loss: 0.6378, Train: 0.8293, Test: 0.6538\n",
            "Early stopping:  0.03832946837249481\n",
            "Epoch: 041, Loss: 0.6148, Train: 0.8366, Test: 0.6575\n",
            "Early stopping:  0.037567551367905565\n",
            "Epoch: 042, Loss: 0.5920, Train: 0.8452, Test: 0.6583\n",
            "Early stopping:  0.03682277298154364\n",
            "Epoch: 043, Loss: 0.5694, Train: 0.8548, Test: 0.6602\n",
            "Early stopping:  0.0362949500302087\n",
            "Epoch: 044, Loss: 0.5474, Train: 0.8656, Test: 0.6622\n",
            "Early stopping:  0.03575883884985739\n",
            "Epoch: 045, Loss: 0.5256, Train: 0.8775, Test: 0.6629\n",
            "Early stopping:  0.03526144316815883\n",
            "Epoch: 046, Loss: 0.5039, Train: 0.8837, Test: 0.6664\n",
            "Early stopping:  0.034794379331481586\n",
            "Epoch: 047, Loss: 0.4828, Train: 0.8922, Test: 0.6671\n",
            "Early stopping:  0.03427084210323076\n",
            "Epoch: 048, Loss: 0.4620, Train: 0.9019, Test: 0.6690\n",
            "Early stopping:  0.03377300487140897\n",
            "Epoch: 049, Loss: 0.4415, Train: 0.9115, Test: 0.6691\n",
            "Early stopping:  0.03323838849421431\n",
            "Epoch: 050, Loss: 0.4212, Train: 0.9155, Test: 0.6694\n",
            "Early stopping:  0.032704235776809476\n",
            "Epoch: 051, Loss: 0.4016, Train: 0.9240, Test: 0.6722\n",
            "Early stopping:  0.03214259137849502\n",
            "Epoch: 052, Loss: 0.3824, Train: 0.9331, Test: 0.6714\n",
            "Early stopping:  0.0314651578965013\n",
            "Epoch: 053, Loss: 0.3637, Train: 0.9370, Test: 0.6727\n",
            "Early stopping:  0.030708456333931492\n",
            "Epoch: 054, Loss: 0.3457, Train: 0.9444, Test: 0.6731\n",
            "Early stopping:  0.029843474284989308\n",
            "Epoch: 055, Loss: 0.3285, Train: 0.9455, Test: 0.6721\n",
            "Early stopping:  0.028908406350141407\n",
            "Epoch: 056, Loss: 0.3125, Train: 0.9569, Test: 0.6725\n",
            "Early stopping:  0.027682305155851024\n",
            "Epoch: 057, Loss: 0.2986, Train: 0.9484, Test: 0.6708\n",
            "Early stopping:  0.025861670494595\n",
            "Epoch: 058, Loss: 0.2890, Train: 0.9631, Test: 0.6677\n",
            "Early stopping:  0.022806478802862802\n",
            "Epoch: 059, Loss: 0.2778, Train: 0.9626, Test: 0.6728\n",
            "Early stopping:  0.01988260149934397\n",
            "Epoch: 060, Loss: 0.2591, Train: 0.9705, Test: 0.6721\n",
            "Early stopping:  0.02032036923863359\n",
            "Epoch: 061, Loss: 0.2444, Train: 0.9762, Test: 0.6686\n",
            "Early stopping:  0.022019118263115336\n",
            "Epoch: 062, Loss: 0.2390, Train: 0.9733, Test: 0.6745\n",
            "Early stopping:  0.02134863093790693\n",
            "Epoch: 063, Loss: 0.2277, Train: 0.9807, Test: 0.6724\n",
            "Early stopping:  0.019400152570085472\n",
            "Epoch: 064, Loss: 0.2136, Train: 0.9841, Test: 0.6695\n",
            "Early stopping:  0.01721381539419263\n",
            "Epoch: 065, Loss: 0.2080, Train: 0.9824, Test: 0.6731\n",
            "Early stopping:  0.015724346647651235\n",
            "Epoch: 066, Loss: 0.2008, Train: 0.9892, Test: 0.6711\n",
            "Early stopping:  0.015398178716643866\n",
            "Epoch: 067, Loss: 0.1888, Train: 0.9909, Test: 0.6691\n",
            "Early stopping:  0.01449272557175396\n",
            "Epoch: 068, Loss: 0.1829, Train: 0.9881, Test: 0.6720\n",
            "Early stopping:  0.012855357823630932\n",
            "Epoch: 069, Loss: 0.1783, Train: 0.9921, Test: 0.6691\n",
            "Early stopping:  0.012379657770732026\n",
            "Epoch: 070, Loss: 0.1690, Train: 0.9932, Test: 0.6693\n",
            "Early stopping:  0.011865924299699945\n",
            "Epoch: 071, Loss: 0.1625, Train: 0.9921, Test: 0.6683\n",
            "Early stopping:  0.010550695338651573\n",
            "Epoch: 072, Loss: 0.1594, Train: 0.9943, Test: 0.6660\n",
            "Early stopping:  0.010038667458090655\n",
            "Epoch: 073, Loss: 0.1536, Train: 0.9926, Test: 0.6677\n",
            "Early stopping:  0.009490754472971263\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  1], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.67      0.67       758\n",
            "         capital_goods       0.57      0.58      0.58       508\n",
            "conglomerates_industry       1.00      0.19      0.32        80\n",
            "     consumer_cyclical       0.59      0.60      0.59       793\n",
            " consumer_non-cyclical       0.72      0.56      0.63       446\n",
            "                energy       0.80      0.70      0.74       283\n",
            "             financial       0.73      0.68      0.70       767\n",
            "            healthcare       0.76      0.71      0.73       318\n",
            "              services       0.65      0.76      0.70      2076\n",
            "            technology       0.59      0.52      0.55       396\n",
            "        transportation       0.80      0.74      0.77       404\n",
            "             utilities       0.77      0.64      0.70       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.72      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5105, Train: 0.2944, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3025, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14708289182155967\n",
            "Epoch: 003, Loss: 2.1805, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16687982586064443\n",
            "Epoch: 004, Loss: 2.1363, Train: 0.3040, Test: 0.3024\n",
            "Early stopping:  0.16749658577745494\n",
            "Epoch: 005, Loss: 2.0673, Train: 0.3539, Test: 0.3472\n",
            "Early stopping:  0.17406119705832343\n",
            "Epoch: 006, Loss: 2.0022, Train: 0.3982, Test: 0.3863\n",
            "Early stopping:  0.11436577830161969\n",
            "Epoch: 007, Loss: 1.9363, Train: 0.4129, Test: 0.3971\n",
            "Early stopping:  0.09867439819851274\n",
            "Epoch: 008, Loss: 1.8689, Train: 0.4118, Test: 0.3971\n",
            "Early stopping:  0.10529004265673172\n",
            "Epoch: 009, Loss: 1.8082, Train: 0.4186, Test: 0.3992\n",
            "Early stopping:  0.10303211426371789\n",
            "Epoch: 010, Loss: 1.7489, Train: 0.4413, Test: 0.4206\n",
            "Early stopping:  0.10037229078952238\n",
            "Epoch: 011, Loss: 1.6818, Train: 0.4770, Test: 0.4551\n",
            "Early stopping:  0.09948008217758414\n",
            "Epoch: 012, Loss: 1.6117, Train: 0.5026, Test: 0.4701\n",
            "Early stopping:  0.10138965252754104\n",
            "Epoch: 013, Loss: 1.5511, Train: 0.5184, Test: 0.4777\n",
            "Early stopping:  0.10304098988715053\n",
            "Epoch: 014, Loss: 1.4939, Train: 0.5349, Test: 0.4887\n",
            "Early stopping:  0.10139313588534632\n",
            "Epoch: 015, Loss: 1.4323, Train: 0.5525, Test: 0.5108\n",
            "Early stopping:  0.09757485739901132\n",
            "Epoch: 016, Loss: 1.3747, Train: 0.5689, Test: 0.5250\n",
            "Early stopping:  0.09373450473564071\n",
            "Epoch: 017, Loss: 1.3215, Train: 0.6007, Test: 0.5459\n",
            "Early stopping:  0.09148043434037166\n",
            "Epoch: 018, Loss: 1.2701, Train: 0.6251, Test: 0.5601\n",
            "Early stopping:  0.08838020723457138\n",
            "Epoch: 019, Loss: 1.2242, Train: 0.6393, Test: 0.5713\n",
            "Early stopping:  0.08243543045899415\n",
            "Epoch: 020, Loss: 1.1774, Train: 0.6546, Test: 0.5821\n",
            "Early stopping:  0.07781633357321885\n",
            "Epoch: 021, Loss: 1.1318, Train: 0.6557, Test: 0.5887\n",
            "Early stopping:  0.07465430469903067\n",
            "Epoch: 022, Loss: 1.0905, Train: 0.6676, Test: 0.5933\n",
            "Early stopping:  0.0714006064174605\n",
            "Epoch: 023, Loss: 1.0525, Train: 0.6795, Test: 0.5955\n",
            "Early stopping:  0.06810105466710199\n",
            "Epoch: 024, Loss: 1.0173, Train: 0.6903, Test: 0.6038\n",
            "Early stopping:  0.06327102501330047\n",
            "Epoch: 025, Loss: 0.9814, Train: 0.7022, Test: 0.6124\n",
            "Early stopping:  0.059188454716356795\n",
            "Epoch: 026, Loss: 0.9479, Train: 0.7141, Test: 0.6167\n",
            "Early stopping:  0.05636698937905503\n",
            "Epoch: 027, Loss: 0.9163, Train: 0.7226, Test: 0.6213\n",
            "Early stopping:  0.05403259847248478\n",
            "Epoch: 028, Loss: 0.8857, Train: 0.7277, Test: 0.6249\n",
            "Early stopping:  0.05190392842217748\n",
            "Epoch: 029, Loss: 0.8547, Train: 0.7374, Test: 0.6279\n",
            "Early stopping:  0.049879681192053354\n",
            "Epoch: 030, Loss: 0.8263, Train: 0.7470, Test: 0.6308\n",
            "Early stopping:  0.048181248408262344\n",
            "Epoch: 031, Loss: 0.7988, Train: 0.7561, Test: 0.6338\n",
            "Early stopping:  0.04658161703070411\n",
            "Epoch: 032, Loss: 0.7728, Train: 0.7714, Test: 0.6364\n",
            "Early stopping:  0.04458736843363723\n",
            "Epoch: 033, Loss: 0.7457, Train: 0.7788, Test: 0.6388\n",
            "Early stopping:  0.042962443137152036\n",
            "Epoch: 034, Loss: 0.7197, Train: 0.7873, Test: 0.6419\n",
            "Early stopping:  0.042124122333788525\n",
            "Epoch: 035, Loss: 0.6947, Train: 0.7986, Test: 0.6429\n",
            "Early stopping:  0.04131486117366765\n",
            "Epoch: 036, Loss: 0.6695, Train: 0.8106, Test: 0.6445\n",
            "Early stopping:  0.04072390251092916\n",
            "Epoch: 037, Loss: 0.6455, Train: 0.8196, Test: 0.6457\n",
            "Early stopping:  0.03962245973947382\n",
            "Epoch: 038, Loss: 0.6223, Train: 0.8270, Test: 0.6476\n",
            "Early stopping:  0.03857923434473582\n",
            "Epoch: 039, Loss: 0.6000, Train: 0.8327, Test: 0.6494\n",
            "Early stopping:  0.03740320365271163\n",
            "Epoch: 040, Loss: 0.5778, Train: 0.8474, Test: 0.6540\n",
            "Early stopping:  0.03617531356272038\n",
            "Epoch: 041, Loss: 0.5556, Train: 0.8622, Test: 0.6555\n",
            "Early stopping:  0.035452014543023214\n",
            "Epoch: 042, Loss: 0.5340, Train: 0.8712, Test: 0.6569\n",
            "Early stopping:  0.034965107795773435\n",
            "Epoch: 043, Loss: 0.5124, Train: 0.8809, Test: 0.6581\n",
            "Early stopping:  0.034647765984025955\n",
            "Epoch: 044, Loss: 0.4918, Train: 0.8883, Test: 0.6595\n",
            "Early stopping:  0.03403762688616462\n",
            "Epoch: 045, Loss: 0.4716, Train: 0.8951, Test: 0.6583\n",
            "Early stopping:  0.03323527040955264\n",
            "Epoch: 046, Loss: 0.4520, Train: 0.8968, Test: 0.6603\n",
            "Early stopping:  0.03238883308291661\n",
            "Epoch: 047, Loss: 0.4330, Train: 0.9121, Test: 0.6608\n",
            "Early stopping:  0.03140428132021464\n",
            "Epoch: 048, Loss: 0.4155, Train: 0.9070, Test: 0.6610\n",
            "Early stopping:  0.03024406385474403\n",
            "Epoch: 049, Loss: 0.3995, Train: 0.9229, Test: 0.6598\n",
            "Early stopping:  0.02859862963956871\n",
            "Epoch: 050, Loss: 0.3829, Train: 0.9234, Test: 0.6578\n",
            "Early stopping:  0.02716331303819535\n",
            "Epoch: 051, Loss: 0.3647, Train: 0.9359, Test: 0.6618\n",
            "Early stopping:  0.026765374168185512\n",
            "Epoch: 052, Loss: 0.3462, Train: 0.9416, Test: 0.6625\n",
            "Early stopping:  0.02740846917251399\n",
            "Epoch: 053, Loss: 0.3309, Train: 0.9416, Test: 0.6620\n",
            "Early stopping:  0.027485219649226532\n",
            "Epoch: 054, Loss: 0.3184, Train: 0.9569, Test: 0.6620\n",
            "Early stopping:  0.02581004523256539\n",
            "Epoch: 055, Loss: 0.3063, Train: 0.9467, Test: 0.6618\n",
            "Early stopping:  0.022967782918727392\n",
            "Epoch: 056, Loss: 0.2926, Train: 0.9637, Test: 0.6629\n",
            "Early stopping:  0.020881406710141364\n",
            "Epoch: 057, Loss: 0.2773, Train: 0.9660, Test: 0.6627\n",
            "Early stopping:  0.021072395446904518\n",
            "Epoch: 058, Loss: 0.2636, Train: 0.9688, Test: 0.6625\n",
            "Early stopping:  0.021952691772217297\n",
            "Epoch: 059, Loss: 0.2529, Train: 0.9767, Test: 0.6626\n",
            "Early stopping:  0.021491824137532856\n",
            "Epoch: 060, Loss: 0.2441, Train: 0.9705, Test: 0.6615\n",
            "Early stopping:  0.01930180109355965\n",
            "Epoch: 061, Loss: 0.2354, Train: 0.9824, Test: 0.6626\n",
            "Early stopping:  0.01640050913843737\n",
            "Epoch: 062, Loss: 0.2252, Train: 0.9790, Test: 0.6637\n",
            "Early stopping:  0.014922109096129878\n",
            "Epoch: 063, Loss: 0.2138, Train: 0.9858, Test: 0.6632\n",
            "Early stopping:  0.015400404588659185\n",
            "Epoch: 064, Loss: 0.2034, Train: 0.9892, Test: 0.6633\n",
            "Early stopping:  0.016299189985894573\n",
            "Epoch: 065, Loss: 0.1959, Train: 0.9870, Test: 0.6630\n",
            "Early stopping:  0.01596228678204182\n",
            "Epoch: 066, Loss: 0.1903, Train: 0.9904, Test: 0.6622\n",
            "Early stopping:  0.013974557030060742\n",
            "Epoch: 067, Loss: 0.1843, Train: 0.9881, Test: 0.6626\n",
            "Early stopping:  0.011481812095816613\n",
            "Epoch: 068, Loss: 0.1771, Train: 0.9915, Test: 0.6649\n",
            "Early stopping:  0.010165057233460692\n",
            "Epoch: 069, Loss: 0.1688, Train: 0.9915, Test: 0.6664\n",
            "Early stopping:  0.010698128167317694\n",
            "Epoch: 070, Loss: 0.1616, Train: 0.9926, Test: 0.6663\n",
            "Early stopping:  0.01155729423077086\n",
            "Epoch: 071, Loss: 0.1568, Train: 0.9949, Test: 0.6633\n",
            "Early stopping:  0.011215120508369023\n",
            "Epoch: 072, Loss: 0.1533, Train: 0.9949, Test: 0.6620\n",
            "Early stopping:  0.00958426010583234\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.65      0.66       758\n",
            "         capital_goods       0.56      0.55      0.55       508\n",
            "conglomerates_industry       0.94      0.21      0.35        80\n",
            "     consumer_cyclical       0.58      0.57      0.58       793\n",
            " consumer_non-cyclical       0.72      0.59      0.65       446\n",
            "                energy       0.80      0.70      0.75       283\n",
            "             financial       0.70      0.69      0.70       767\n",
            "            healthcare       0.79      0.73      0.75       318\n",
            "              services       0.63      0.76      0.69      2076\n",
            "            technology       0.65      0.48      0.55       396\n",
            "        transportation       0.78      0.70      0.74       404\n",
            "             utilities       0.79      0.64      0.71       225\n",
            "\n",
            "              accuracy                           0.66      7054\n",
            "             macro avg       0.72      0.61      0.64      7054\n",
            "          weighted avg       0.67      0.66      0.66      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5167, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2856, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16339709394315655\n",
            "Epoch: 003, Loss: 2.1600, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1809473760241773\n",
            "Epoch: 004, Loss: 2.1306, Train: 0.3040, Test: 0.3037\n",
            "Early stopping:  0.17568503554688847\n",
            "Epoch: 005, Loss: 2.0672, Train: 0.3619, Test: 0.3496\n",
            "Early stopping:  0.17787897659207372\n",
            "Epoch: 006, Loss: 2.0029, Train: 0.3902, Test: 0.3843\n",
            "Early stopping:  0.10632485504050371\n",
            "Epoch: 007, Loss: 1.9415, Train: 0.4067, Test: 0.3931\n",
            "Early stopping:  0.08991560971968914\n",
            "Epoch: 008, Loss: 1.8759, Train: 0.4039, Test: 0.3940\n",
            "Early stopping:  0.10042209542648561\n",
            "Epoch: 009, Loss: 1.8140, Train: 0.4090, Test: 0.4012\n",
            "Early stopping:  0.10015091789691383\n",
            "Epoch: 010, Loss: 1.7584, Train: 0.4328, Test: 0.4182\n",
            "Early stopping:  0.09752364144907033\n",
            "Epoch: 011, Loss: 1.7015, Train: 0.4617, Test: 0.4420\n",
            "Early stopping:  0.09453577735548573\n",
            "Epoch: 012, Loss: 1.6414, Train: 0.4895, Test: 0.4563\n",
            "Early stopping:  0.09196879760471861\n",
            "Epoch: 013, Loss: 1.5822, Train: 0.5009, Test: 0.4677\n",
            "Early stopping:  0.09179008401162392\n",
            "Epoch: 014, Loss: 1.5240, Train: 0.5150, Test: 0.4783\n",
            "Early stopping:  0.09296786370791928\n",
            "Epoch: 015, Loss: 1.4655, Train: 0.5309, Test: 0.4965\n",
            "Early stopping:  0.0931924171662018\n",
            "Epoch: 016, Loss: 1.4109, Train: 0.5513, Test: 0.5128\n",
            "Early stopping:  0.09135354780053144\n",
            "Epoch: 017, Loss: 1.3591, Train: 0.5780, Test: 0.5350\n",
            "Early stopping:  0.08849184752556889\n",
            "Epoch: 018, Loss: 1.3073, Train: 0.6024, Test: 0.5529\n",
            "Early stopping:  0.08538106464790644\n",
            "Epoch: 019, Loss: 1.2587, Train: 0.6166, Test: 0.5610\n",
            "Early stopping:  0.08179019042739882\n",
            "Epoch: 020, Loss: 1.2137, Train: 0.6194, Test: 0.5690\n",
            "Early stopping:  0.07826178426819735\n",
            "Epoch: 021, Loss: 1.1719, Train: 0.6268, Test: 0.5760\n",
            "Early stopping:  0.07406695495865989\n",
            "Epoch: 022, Loss: 1.1325, Train: 0.6506, Test: 0.5838\n",
            "Early stopping:  0.0690609273926776\n",
            "Epoch: 023, Loss: 1.0929, Train: 0.6665, Test: 0.5920\n",
            "Early stopping:  0.065278364432449\n",
            "Epoch: 024, Loss: 1.0553, Train: 0.6778, Test: 0.5978\n",
            "Early stopping:  0.06258804101102093\n",
            "Epoch: 025, Loss: 1.0204, Train: 0.6841, Test: 0.6038\n",
            "Early stopping:  0.06012642243140066\n",
            "Epoch: 026, Loss: 0.9860, Train: 0.6954, Test: 0.6079\n",
            "Early stopping:  0.05782678826680405\n",
            "Epoch: 027, Loss: 0.9526, Train: 0.7107, Test: 0.6136\n",
            "Early stopping:  0.05532720378311304\n",
            "Epoch: 028, Loss: 0.9198, Train: 0.7209, Test: 0.6168\n",
            "Early stopping:  0.053576073611935095\n",
            "Epoch: 029, Loss: 0.8891, Train: 0.7283, Test: 0.6222\n",
            "Early stopping:  0.05199833561263526\n",
            "Epoch: 030, Loss: 0.8588, Train: 0.7391, Test: 0.6297\n",
            "Early stopping:  0.05029406623226492\n",
            "Epoch: 031, Loss: 0.8302, Train: 0.7482, Test: 0.6306\n",
            "Early stopping:  0.04837479566094544\n",
            "Epoch: 032, Loss: 0.8030, Train: 0.7612, Test: 0.6361\n",
            "Early stopping:  0.04626354910258697\n",
            "Epoch: 033, Loss: 0.7754, Train: 0.7686, Test: 0.6360\n",
            "Early stopping:  0.04479259177531128\n",
            "Epoch: 034, Loss: 0.7487, Train: 0.7754, Test: 0.6375\n",
            "Early stopping:  0.04346976148318064\n",
            "Epoch: 035, Loss: 0.7232, Train: 0.7845, Test: 0.6398\n",
            "Early stopping:  0.04241904699811577\n",
            "Epoch: 036, Loss: 0.6977, Train: 0.7975, Test: 0.6435\n",
            "Early stopping:  0.04154464946307287\n",
            "Epoch: 037, Loss: 0.6726, Train: 0.8088, Test: 0.6479\n",
            "Early stopping:  0.040587388105703615\n",
            "Epoch: 038, Loss: 0.6482, Train: 0.8174, Test: 0.6523\n",
            "Early stopping:  0.03979391405464994\n",
            "Epoch: 039, Loss: 0.6245, Train: 0.8247, Test: 0.6521\n",
            "Early stopping:  0.03906965390723891\n",
            "Epoch: 040, Loss: 0.6006, Train: 0.8383, Test: 0.6532\n",
            "Early stopping:  0.03832144222534275\n",
            "Epoch: 041, Loss: 0.5776, Train: 0.8434, Test: 0.6569\n",
            "Early stopping:  0.03755958438653778\n",
            "Epoch: 042, Loss: 0.5548, Train: 0.8559, Test: 0.6593\n",
            "Early stopping:  0.036948634521227056\n",
            "Epoch: 043, Loss: 0.5328, Train: 0.8684, Test: 0.6599\n",
            "Early stopping:  0.036226849020732685\n",
            "Epoch: 044, Loss: 0.5109, Train: 0.8780, Test: 0.6608\n",
            "Early stopping:  0.035435498211801086\n",
            "Epoch: 045, Loss: 0.4894, Train: 0.8877, Test: 0.6632\n",
            "Early stopping:  0.03481133164468533\n",
            "Epoch: 046, Loss: 0.4688, Train: 0.8990, Test: 0.6620\n",
            "Early stopping:  0.034047258938541436\n",
            "Epoch: 047, Loss: 0.4482, Train: 0.9064, Test: 0.6642\n",
            "Early stopping:  0.03343115123450258\n",
            "Epoch: 048, Loss: 0.4281, Train: 0.9126, Test: 0.6650\n",
            "Early stopping:  0.032723868243163354\n",
            "Epoch: 049, Loss: 0.4087, Train: 0.9217, Test: 0.6656\n",
            "Early stopping:  0.03196717055681628\n",
            "Epoch: 050, Loss: 0.3897, Train: 0.9285, Test: 0.6656\n",
            "Early stopping:  0.031255067069581366\n",
            "Epoch: 051, Loss: 0.3713, Train: 0.9336, Test: 0.6676\n",
            "Early stopping:  0.030387573996722757\n",
            "Epoch: 052, Loss: 0.3533, Train: 0.9399, Test: 0.6660\n",
            "Early stopping:  0.029576935459093134\n",
            "Epoch: 053, Loss: 0.3359, Train: 0.9450, Test: 0.6659\n",
            "Early stopping:  0.028776384459186892\n",
            "Epoch: 054, Loss: 0.3192, Train: 0.9501, Test: 0.6664\n",
            "Early stopping:  0.027892420440601688\n",
            "Epoch: 055, Loss: 0.3031, Train: 0.9558, Test: 0.6663\n",
            "Early stopping:  0.02695805373909821\n",
            "Epoch: 056, Loss: 0.2876, Train: 0.9648, Test: 0.6660\n",
            "Early stopping:  0.025965163504490848\n",
            "Epoch: 057, Loss: 0.2730, Train: 0.9688, Test: 0.6659\n",
            "Early stopping:  0.024898209938593645\n",
            "Epoch: 058, Loss: 0.2594, Train: 0.9739, Test: 0.6663\n",
            "Early stopping:  0.023671823167276597\n",
            "Epoch: 059, Loss: 0.2470, Train: 0.9733, Test: 0.6650\n",
            "Early stopping:  0.022211147994637234\n",
            "Epoch: 060, Loss: 0.2376, Train: 0.9824, Test: 0.6622\n",
            "Early stopping:  0.020006521963884676\n",
            "Epoch: 061, Loss: 0.2289, Train: 0.9818, Test: 0.6667\n",
            "Early stopping:  0.017485435729403087\n",
            "Epoch: 062, Loss: 0.2168, Train: 0.9858, Test: 0.6686\n",
            "Early stopping:  0.016388129034167982\n",
            "Epoch: 063, Loss: 0.2018, Train: 0.9881, Test: 0.6691\n",
            "Early stopping:  0.01770412698752367\n",
            "Epoch: 064, Loss: 0.1924, Train: 0.9881, Test: 0.6701\n",
            "Early stopping:  0.018620652064338272\n",
            "Epoch: 065, Loss: 0.1875, Train: 0.9904, Test: 0.6683\n",
            "Early stopping:  0.01719595306222018\n",
            "Epoch: 066, Loss: 0.1780, Train: 0.9915, Test: 0.6691\n",
            "Early stopping:  0.014749665304093659\n",
            "Epoch: 067, Loss: 0.1679, Train: 0.9926, Test: 0.6694\n",
            "Early stopping:  0.013078750577770797\n",
            "Epoch: 068, Loss: 0.1634, Train: 0.9932, Test: 0.6676\n",
            "Early stopping:  0.012382521733688076\n",
            "Epoch: 069, Loss: 0.1583, Train: 0.9943, Test: 0.6703\n",
            "Early stopping:  0.011743734707016087\n",
            "Epoch: 070, Loss: 0.1499, Train: 0.9960, Test: 0.6700\n",
            "Early stopping:  0.010529355000871394\n",
            "Epoch: 071, Loss: 0.1451, Train: 0.9966, Test: 0.6669\n",
            "Early stopping:  0.00939833231507076\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.63      0.70      0.66       758\n",
            "         capital_goods       0.56      0.58      0.57       508\n",
            "conglomerates_industry       0.91      0.26      0.41        80\n",
            "     consumer_cyclical       0.59      0.60      0.59       793\n",
            " consumer_non-cyclical       0.74      0.59      0.65       446\n",
            "                energy       0.77      0.70      0.73       283\n",
            "             financial       0.68      0.70      0.69       767\n",
            "            healthcare       0.77      0.70      0.74       318\n",
            "              services       0.68      0.72      0.70      2076\n",
            "            technology       0.57      0.56      0.56       396\n",
            "        transportation       0.78      0.74      0.76       404\n",
            "             utilities       0.80      0.67      0.73       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.71      0.63      0.65      7054\n",
            "          weighted avg       0.67      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4579, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2328, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15914447692840733\n",
            "Epoch: 003, Loss: 2.1763, Train: 0.2944, Test: 0.2944\n",
            "Early stopping:  0.14898314599626122\n",
            "Epoch: 004, Loss: 2.0975, Train: 0.3227, Test: 0.3183\n",
            "Early stopping:  0.1547906621808341\n",
            "Epoch: 005, Loss: 2.0276, Train: 0.3727, Test: 0.3629\n",
            "Early stopping:  0.16458070394630095\n",
            "Epoch: 006, Loss: 1.9758, Train: 0.4016, Test: 0.3947\n",
            "Early stopping:  0.10502241982838424\n",
            "Epoch: 007, Loss: 1.9125, Train: 0.4152, Test: 0.3969\n",
            "Early stopping:  0.10293269442266136\n",
            "Epoch: 008, Loss: 1.8415, Train: 0.4141, Test: 0.3935\n",
            "Early stopping:  0.0992858882151694\n",
            "Epoch: 009, Loss: 1.7728, Train: 0.4175, Test: 0.3993\n",
            "Early stopping:  0.10197830251123499\n",
            "Epoch: 010, Loss: 1.7075, Train: 0.4390, Test: 0.4200\n",
            "Early stopping:  0.1069262449599475\n",
            "Epoch: 011, Loss: 1.6385, Train: 0.4787, Test: 0.4502\n",
            "Early stopping:  0.10785139778603091\n",
            "Epoch: 012, Loss: 1.5674, Train: 0.5122, Test: 0.4786\n",
            "Early stopping:  0.10792051760042604\n",
            "Epoch: 013, Loss: 1.5050, Train: 0.5309, Test: 0.4955\n",
            "Early stopping:  0.10686032697209033\n",
            "Epoch: 014, Loss: 1.4436, Train: 0.5547, Test: 0.5198\n",
            "Early stopping:  0.1046153014683654\n",
            "Epoch: 015, Loss: 1.3783, Train: 0.5746, Test: 0.5381\n",
            "Early stopping:  0.1018948519845393\n",
            "Epoch: 016, Loss: 1.3218, Train: 0.5865, Test: 0.5509\n",
            "Early stopping:  0.09774069190090592\n",
            "Epoch: 017, Loss: 1.2745, Train: 0.6035, Test: 0.5641\n",
            "Early stopping:  0.09233866256968577\n",
            "Epoch: 018, Loss: 1.2248, Train: 0.6251, Test: 0.5719\n",
            "Early stopping:  0.08581324612606098\n",
            "Epoch: 019, Loss: 1.1759, Train: 0.6438, Test: 0.5807\n",
            "Early stopping:  0.07937822899542393\n",
            "Epoch: 020, Loss: 1.1334, Train: 0.6523, Test: 0.5843\n",
            "Early stopping:  0.07516866056677388\n",
            "Epoch: 021, Loss: 1.0926, Train: 0.6636, Test: 0.5912\n",
            "Early stopping:  0.07202421579033508\n",
            "Epoch: 022, Loss: 1.0538, Train: 0.6784, Test: 0.5980\n",
            "Early stopping:  0.06730377277347627\n",
            "Epoch: 023, Loss: 1.0162, Train: 0.6960, Test: 0.6066\n",
            "Early stopping:  0.06311503542395663\n",
            "Epoch: 024, Loss: 0.9792, Train: 0.7079, Test: 0.6133\n",
            "Early stopping:  0.06088419534833511\n",
            "Epoch: 025, Loss: 0.9449, Train: 0.7158, Test: 0.6179\n",
            "Early stopping:  0.05853334869422874\n",
            "Epoch: 026, Loss: 0.9095, Train: 0.7277, Test: 0.6246\n",
            "Early stopping:  0.05690136256088028\n",
            "Epoch: 027, Loss: 0.8766, Train: 0.7374, Test: 0.6270\n",
            "Early stopping:  0.055142873843610433\n",
            "Epoch: 028, Loss: 0.8456, Train: 0.7459, Test: 0.6297\n",
            "Early stopping:  0.05305482631725825\n",
            "Epoch: 029, Loss: 0.8144, Train: 0.7561, Test: 0.6343\n",
            "Early stopping:  0.05137899908145813\n",
            "Epoch: 030, Loss: 0.7852, Train: 0.7697, Test: 0.6388\n",
            "Early stopping:  0.0491595368307377\n",
            "Epoch: 031, Loss: 0.7564, Train: 0.7754, Test: 0.6425\n",
            "Early stopping:  0.047569853956184543\n",
            "Epoch: 032, Loss: 0.7287, Train: 0.7873, Test: 0.6447\n",
            "Early stopping:  0.0461380768491484\n",
            "Epoch: 033, Loss: 0.7009, Train: 0.7969, Test: 0.6476\n",
            "Early stopping:  0.04485272946493379\n",
            "Epoch: 034, Loss: 0.6753, Train: 0.8106, Test: 0.6506\n",
            "Early stopping:  0.0435360561376029\n",
            "Epoch: 035, Loss: 0.6496, Train: 0.8157, Test: 0.6527\n",
            "Early stopping:  0.04222080418258228\n",
            "Epoch: 036, Loss: 0.6249, Train: 0.8287, Test: 0.6551\n",
            "Early stopping:  0.040936579034012543\n",
            "Epoch: 037, Loss: 0.6010, Train: 0.8355, Test: 0.6579\n",
            "Early stopping:  0.039565568575974715\n",
            "Epoch: 038, Loss: 0.5773, Train: 0.8486, Test: 0.6592\n",
            "Early stopping:  0.03868324249364636\n",
            "Epoch: 039, Loss: 0.5540, Train: 0.8571, Test: 0.6602\n",
            "Early stopping:  0.03777113388343727\n",
            "Epoch: 040, Loss: 0.5321, Train: 0.8718, Test: 0.6644\n",
            "Early stopping:  0.036787076016043335\n",
            "Epoch: 041, Loss: 0.5103, Train: 0.8837, Test: 0.6646\n",
            "Early stopping:  0.03582391358632823\n",
            "Epoch: 042, Loss: 0.4895, Train: 0.8917, Test: 0.6649\n",
            "Early stopping:  0.03469231086305811\n",
            "Epoch: 043, Loss: 0.4693, Train: 0.9013, Test: 0.6654\n",
            "Early stopping:  0.0335017760962544\n",
            "Epoch: 044, Loss: 0.4495, Train: 0.9058, Test: 0.6666\n",
            "Early stopping:  0.032602499128610435\n",
            "Epoch: 045, Loss: 0.4305, Train: 0.9104, Test: 0.6676\n",
            "Early stopping:  0.031559179047477896\n",
            "Epoch: 046, Loss: 0.4120, Train: 0.9161, Test: 0.6678\n",
            "Early stopping:  0.03063435631588856\n",
            "Epoch: 047, Loss: 0.3941, Train: 0.9263, Test: 0.6703\n",
            "Early stopping:  0.02974097319772394\n",
            "Epoch: 048, Loss: 0.3766, Train: 0.9302, Test: 0.6707\n",
            "Early stopping:  0.02882071545272122\n",
            "Epoch: 049, Loss: 0.3596, Train: 0.9438, Test: 0.6707\n",
            "Early stopping:  0.028030273210696215\n",
            "Epoch: 050, Loss: 0.3430, Train: 0.9472, Test: 0.6720\n",
            "Early stopping:  0.0272909520260146\n",
            "Epoch: 051, Loss: 0.3271, Train: 0.9490, Test: 0.6737\n",
            "Early stopping:  0.026499721703118734\n",
            "Epoch: 052, Loss: 0.3118, Train: 0.9541, Test: 0.6731\n",
            "Early stopping:  0.025630600813643732\n",
            "Epoch: 053, Loss: 0.2971, Train: 0.9586, Test: 0.6735\n",
            "Early stopping:  0.02468472436510796\n",
            "Epoch: 054, Loss: 0.2829, Train: 0.9637, Test: 0.6724\n",
            "Early stopping:  0.02374265801643545\n",
            "Epoch: 055, Loss: 0.2692, Train: 0.9699, Test: 0.6713\n",
            "Early stopping:  0.02288943491666104\n",
            "Epoch: 056, Loss: 0.2562, Train: 0.9733, Test: 0.6710\n",
            "Early stopping:  0.022017919512926854\n",
            "Epoch: 057, Loss: 0.2438, Train: 0.9796, Test: 0.6720\n",
            "Early stopping:  0.02108334313270503\n",
            "Epoch: 058, Loss: 0.2321, Train: 0.9818, Test: 0.6704\n",
            "Early stopping:  0.02007808551006602\n",
            "Epoch: 059, Loss: 0.2210, Train: 0.9858, Test: 0.6701\n",
            "Early stopping:  0.01906096696106153\n",
            "Epoch: 060, Loss: 0.2105, Train: 0.9858, Test: 0.6727\n",
            "Early stopping:  0.01808258320858219\n",
            "Epoch: 061, Loss: 0.2008, Train: 0.9892, Test: 0.6705\n",
            "Early stopping:  0.017034469429274043\n",
            "Epoch: 062, Loss: 0.1925, Train: 0.9830, Test: 0.6718\n",
            "Early stopping:  0.015732655132777745\n",
            "Epoch: 063, Loss: 0.1877, Train: 0.9932, Test: 0.6663\n",
            "Early stopping:  0.013477102507315281\n",
            "Epoch: 064, Loss: 0.1860, Train: 0.9864, Test: 0.6721\n",
            "Early stopping:  0.010137188967954222\n",
            "Epoch: 065, Loss: 0.1764, Train: 0.9932, Test: 0.6718\n",
            "Early stopping:  0.008953102490365145\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.64      0.71      0.67       758\n",
            "         capital_goods       0.57      0.57      0.57       508\n",
            "conglomerates_industry       0.94      0.21      0.35        80\n",
            "     consumer_cyclical       0.64      0.60      0.62       793\n",
            " consumer_non-cyclical       0.73      0.57      0.64       446\n",
            "                energy       0.81      0.69      0.75       283\n",
            "             financial       0.70      0.70      0.70       767\n",
            "            healthcare       0.75      0.73      0.74       318\n",
            "              services       0.66      0.75      0.70      2076\n",
            "            technology       0.60      0.50      0.54       396\n",
            "        transportation       0.77      0.74      0.75       404\n",
            "             utilities       0.78      0.65      0.71       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.72      0.62      0.64      7054\n",
            "          weighted avg       0.68      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4879, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2670, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15618526240076044\n",
            "Epoch: 003, Loss: 2.1541, Train: 0.2950, Test: 0.2946\n",
            "Early stopping:  0.16980758945894173\n",
            "Epoch: 004, Loss: 2.1155, Train: 0.3193, Test: 0.3195\n",
            "Early stopping:  0.1673719738540294\n",
            "Epoch: 005, Loss: 2.0439, Train: 0.3749, Test: 0.3648\n",
            "Early stopping:  0.173259285967308\n",
            "Epoch: 006, Loss: 1.9783, Train: 0.3988, Test: 0.3867\n",
            "Early stopping:  0.10991773407762083\n",
            "Epoch: 007, Loss: 1.9148, Train: 0.3988, Test: 0.3903\n",
            "Early stopping:  0.09770788147530049\n",
            "Epoch: 008, Loss: 1.8501, Train: 0.4010, Test: 0.3877\n",
            "Early stopping:  0.10436425752918373\n",
            "Epoch: 009, Loss: 1.7880, Train: 0.4118, Test: 0.4002\n",
            "Early stopping:  0.10119290794358571\n",
            "Epoch: 010, Loss: 1.7254, Train: 0.4453, Test: 0.4278\n",
            "Early stopping:  0.10003128926901522\n",
            "Epoch: 011, Loss: 1.6569, Train: 0.4776, Test: 0.4518\n",
            "Early stopping:  0.10127536523956611\n",
            "Epoch: 012, Loss: 1.5914, Train: 0.5031, Test: 0.4665\n",
            "Early stopping:  0.10254199318637038\n",
            "Epoch: 013, Loss: 1.5321, Train: 0.5258, Test: 0.4773\n",
            "Early stopping:  0.10212472034229213\n",
            "Epoch: 014, Loss: 1.4704, Train: 0.5400, Test: 0.4969\n",
            "Early stopping:  0.10039977082265589\n",
            "Epoch: 015, Loss: 1.4074, Train: 0.5593, Test: 0.5197\n",
            "Early stopping:  0.098034266413224\n",
            "Epoch: 016, Loss: 1.3492, Train: 0.5922, Test: 0.5401\n",
            "Early stopping:  0.09632525096292907\n",
            "Epoch: 017, Loss: 1.2936, Train: 0.6149, Test: 0.5598\n",
            "Early stopping:  0.09464085158800328\n",
            "Epoch: 018, Loss: 1.2413, Train: 0.6324, Test: 0.5747\n",
            "Early stopping:  0.09053090724400197\n",
            "Epoch: 019, Loss: 1.1931, Train: 0.6455, Test: 0.5780\n",
            "Early stopping:  0.08489338349331951\n",
            "Epoch: 020, Loss: 1.1462, Train: 0.6466, Test: 0.5846\n",
            "Early stopping:  0.0801089763981475\n",
            "Epoch: 021, Loss: 1.1026, Train: 0.6568, Test: 0.5910\n",
            "Early stopping:  0.07547155212908793\n",
            "Epoch: 022, Loss: 1.0631, Train: 0.6761, Test: 0.5997\n",
            "Early stopping:  0.07071206004727353\n",
            "Epoch: 023, Loss: 1.0245, Train: 0.6920, Test: 0.6041\n",
            "Early stopping:  0.06653324193235771\n",
            "Epoch: 024, Loss: 0.9888, Train: 0.6982, Test: 0.6063\n",
            "Early stopping:  0.06218455173693456\n",
            "Epoch: 025, Loss: 0.9551, Train: 0.7011, Test: 0.6127\n",
            "Early stopping:  0.05841005976487973\n",
            "Epoch: 026, Loss: 0.9233, Train: 0.7147, Test: 0.6178\n",
            "Early stopping:  0.05521971068575348\n",
            "Epoch: 027, Loss: 0.8918, Train: 0.7266, Test: 0.6249\n",
            "Early stopping:  0.05234563761042667\n",
            "Epoch: 028, Loss: 0.8601, Train: 0.7374, Test: 0.6255\n",
            "Early stopping:  0.05071161556351175\n",
            "Epoch: 029, Loss: 0.8306, Train: 0.7493, Test: 0.6270\n",
            "Early stopping:  0.04936157763102466\n",
            "Epoch: 030, Loss: 0.8018, Train: 0.7561, Test: 0.6317\n",
            "Early stopping:  0.048098417457054024\n",
            "Epoch: 031, Loss: 0.7732, Train: 0.7640, Test: 0.6364\n",
            "Early stopping:  0.04673427849758754\n",
            "Epoch: 032, Loss: 0.7454, Train: 0.7782, Test: 0.6401\n",
            "Early stopping:  0.045355527470121636\n",
            "Epoch: 033, Loss: 0.7192, Train: 0.7913, Test: 0.6416\n",
            "Early stopping:  0.044156651685286936\n",
            "Epoch: 034, Loss: 0.6934, Train: 0.7992, Test: 0.6457\n",
            "Early stopping:  0.04281955472735672\n",
            "Epoch: 035, Loss: 0.6685, Train: 0.8083, Test: 0.6469\n",
            "Early stopping:  0.04132836808333502\n",
            "Epoch: 036, Loss: 0.6442, Train: 0.8213, Test: 0.6467\n",
            "Early stopping:  0.04002933320795776\n",
            "Epoch: 037, Loss: 0.6208, Train: 0.8321, Test: 0.6513\n",
            "Early stopping:  0.03889642531370831\n",
            "Epoch: 038, Loss: 0.5978, Train: 0.8412, Test: 0.6544\n",
            "Early stopping:  0.03778355002822021\n",
            "Epoch: 039, Loss: 0.5753, Train: 0.8491, Test: 0.6572\n",
            "Early stopping:  0.03682353058070518\n",
            "Epoch: 040, Loss: 0.5531, Train: 0.8622, Test: 0.6602\n",
            "Early stopping:  0.03600588965058897\n",
            "Epoch: 041, Loss: 0.5314, Train: 0.8695, Test: 0.6615\n",
            "Early stopping:  0.03535657506974171\n",
            "Epoch: 042, Loss: 0.5103, Train: 0.8758, Test: 0.6636\n",
            "Early stopping:  0.03460357274325655\n",
            "Epoch: 043, Loss: 0.4894, Train: 0.8820, Test: 0.6626\n",
            "Early stopping:  0.03392426546040564\n",
            "Epoch: 044, Loss: 0.4692, Train: 0.8934, Test: 0.6653\n",
            "Early stopping:  0.03316258753306197\n",
            "Epoch: 045, Loss: 0.4495, Train: 0.9030, Test: 0.6666\n",
            "Early stopping:  0.03239420208805083\n",
            "Epoch: 046, Loss: 0.4302, Train: 0.9132, Test: 0.6669\n",
            "Early stopping:  0.031664898849353904\n",
            "Epoch: 047, Loss: 0.4116, Train: 0.9172, Test: 0.6686\n",
            "Early stopping:  0.030797792638105375\n",
            "Epoch: 048, Loss: 0.3932, Train: 0.9285, Test: 0.6705\n",
            "Early stopping:  0.030036446597157967\n",
            "Epoch: 049, Loss: 0.3751, Train: 0.9370, Test: 0.6728\n",
            "Early stopping:  0.02936983559877681\n",
            "Epoch: 050, Loss: 0.3578, Train: 0.9427, Test: 0.6731\n",
            "Early stopping:  0.028666111090228097\n",
            "Epoch: 051, Loss: 0.3408, Train: 0.9478, Test: 0.6730\n",
            "Early stopping:  0.027973521617613774\n",
            "Epoch: 052, Loss: 0.3244, Train: 0.9512, Test: 0.6761\n",
            "Early stopping:  0.027167982991943188\n",
            "Epoch: 053, Loss: 0.3087, Train: 0.9597, Test: 0.6752\n",
            "Early stopping:  0.02626188675966317\n",
            "Epoch: 054, Loss: 0.2934, Train: 0.9660, Test: 0.6721\n",
            "Early stopping:  0.025416731376785413\n",
            "Epoch: 055, Loss: 0.2791, Train: 0.9733, Test: 0.6734\n",
            "Early stopping:  0.024421421668625595\n",
            "Epoch: 056, Loss: 0.2656, Train: 0.9716, Test: 0.6703\n",
            "Early stopping:  0.023295101283264136\n",
            "Epoch: 057, Loss: 0.2536, Train: 0.9801, Test: 0.6691\n",
            "Early stopping:  0.021858335489125278\n",
            "Epoch: 058, Loss: 0.2437, Train: 0.9739, Test: 0.6664\n",
            "Early stopping:  0.019807250591590546\n",
            "Epoch: 059, Loss: 0.2360, Train: 0.9870, Test: 0.6684\n",
            "Early stopping:  0.017208237368578443\n",
            "Epoch: 060, Loss: 0.2244, Train: 0.9853, Test: 0.6688\n",
            "Early stopping:  0.015857912472701186\n",
            "Epoch: 061, Loss: 0.2093, Train: 0.9864, Test: 0.6690\n",
            "Early stopping:  0.01720861308606827\n",
            "Epoch: 062, Loss: 0.1999, Train: 0.9909, Test: 0.6681\n",
            "Early stopping:  0.018163470267975185\n",
            "Epoch: 063, Loss: 0.1953, Train: 0.9881, Test: 0.6690\n",
            "Early stopping:  0.01698528817456889\n",
            "Epoch: 064, Loss: 0.1864, Train: 0.9932, Test: 0.6722\n",
            "Early stopping:  0.01450202815411902\n",
            "Epoch: 065, Loss: 0.1752, Train: 0.9943, Test: 0.6707\n",
            "Early stopping:  0.013008360950917088\n",
            "Epoch: 066, Loss: 0.1707, Train: 0.9938, Test: 0.6691\n",
            "Early stopping:  0.012556263418006827\n",
            "Epoch: 067, Loss: 0.1667, Train: 0.9949, Test: 0.6708\n",
            "Early stopping:  0.011796818501988762\n",
            "Epoch: 068, Loss: 0.1577, Train: 0.9949, Test: 0.6708\n",
            "Early stopping:  0.0106056498135696\n",
            "Epoch: 069, Loss: 0.1514, Train: 0.9960, Test: 0.6700\n",
            "Early stopping:  0.009702666578047046\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.67      0.66      0.66       758\n",
            "         capital_goods       0.57      0.55      0.56       508\n",
            "conglomerates_industry       0.88      0.28      0.42        80\n",
            "     consumer_cyclical       0.63      0.64      0.63       793\n",
            " consumer_non-cyclical       0.72      0.55      0.62       446\n",
            "                energy       0.83      0.67      0.74       283\n",
            "             financial       0.71      0.67      0.69       767\n",
            "            healthcare       0.79      0.73      0.76       318\n",
            "              services       0.64      0.78      0.70      2076\n",
            "            technology       0.58      0.48      0.53       396\n",
            "        transportation       0.81      0.74      0.77       404\n",
            "             utilities       0.79      0.62      0.69       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.72      0.61      0.65      7054\n",
            "          weighted avg       0.68      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4275, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2320, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.13827318895841859\n",
            "Epoch: 003, Loss: 2.1586, Train: 0.2944, Test: 0.2947\n",
            "Early stopping:  0.13902524072600694\n",
            "Epoch: 004, Loss: 2.0899, Train: 0.3210, Test: 0.3183\n",
            "Early stopping:  0.14574507657180308\n",
            "Epoch: 005, Loss: 2.0119, Train: 0.3795, Test: 0.3755\n",
            "Early stopping:  0.1586855829310336\n",
            "Epoch: 006, Loss: 1.9476, Train: 0.4016, Test: 0.3948\n",
            "Early stopping:  0.11315473880685045\n",
            "Epoch: 007, Loss: 1.8790, Train: 0.3948, Test: 0.3887\n",
            "Early stopping:  0.11094913533763755\n",
            "Epoch: 008, Loss: 1.8088, Train: 0.4010, Test: 0.3880\n",
            "Early stopping:  0.10993400618677569\n",
            "Epoch: 009, Loss: 1.7415, Train: 0.4237, Test: 0.4138\n",
            "Early stopping:  0.10745596531756622\n",
            "Epoch: 010, Loss: 1.6703, Train: 0.4719, Test: 0.4508\n",
            "Early stopping:  0.10941208069711514\n",
            "Epoch: 011, Loss: 1.5955, Train: 0.5105, Test: 0.4728\n",
            "Early stopping:  0.11156763571831588\n",
            "Epoch: 012, Loss: 1.5291, Train: 0.5303, Test: 0.4955\n",
            "Early stopping:  0.1115825163720613\n",
            "Epoch: 013, Loss: 1.4657, Train: 0.5513, Test: 0.5119\n",
            "Early stopping:  0.10963327643106745\n",
            "Epoch: 014, Loss: 1.4009, Train: 0.5797, Test: 0.5267\n",
            "Early stopping:  0.10579441280070591\n",
            "Epoch: 015, Loss: 1.3408, Train: 0.5978, Test: 0.5431\n",
            "Early stopping:  0.10081119135492753\n",
            "Epoch: 016, Loss: 1.2856, Train: 0.6103, Test: 0.5584\n",
            "Early stopping:  0.09677747375783638\n",
            "Epoch: 017, Loss: 1.2340, Train: 0.6330, Test: 0.5695\n",
            "Early stopping:  0.09159805944851489\n",
            "Epoch: 018, Loss: 1.1867, Train: 0.6466, Test: 0.5804\n",
            "Early stopping:  0.08471401388007359\n",
            "Epoch: 019, Loss: 1.1399, Train: 0.6591, Test: 0.5890\n",
            "Early stopping:  0.07923582189117637\n",
            "Epoch: 020, Loss: 1.0982, Train: 0.6687, Test: 0.5974\n",
            "Early stopping:  0.07417712203247952\n",
            "Epoch: 021, Loss: 1.0605, Train: 0.6829, Test: 0.6053\n",
            "Early stopping:  0.06893677101973\n",
            "Epoch: 022, Loss: 1.0217, Train: 0.6994, Test: 0.6111\n",
            "Early stopping:  0.06479829628268635\n",
            "Epoch: 023, Loss: 0.9859, Train: 0.7050, Test: 0.6120\n",
            "Early stopping:  0.060797415443538956\n",
            "Epoch: 024, Loss: 0.9500, Train: 0.7130, Test: 0.6188\n",
            "Early stopping:  0.05867788143918709\n",
            "Epoch: 025, Loss: 0.9154, Train: 0.7260, Test: 0.6223\n",
            "Early stopping:  0.05722506360643898\n",
            "Epoch: 026, Loss: 0.8827, Train: 0.7351, Test: 0.6266\n",
            "Early stopping:  0.055114328660305814\n",
            "Epoch: 027, Loss: 0.8502, Train: 0.7510, Test: 0.6311\n",
            "Early stopping:  0.05355828344693136\n",
            "Epoch: 028, Loss: 0.8198, Train: 0.7601, Test: 0.6343\n",
            "Early stopping:  0.05148332826892153\n",
            "Epoch: 029, Loss: 0.7902, Train: 0.7686, Test: 0.6386\n",
            "Early stopping:  0.04955971396468148\n",
            "Epoch: 030, Loss: 0.7612, Train: 0.7794, Test: 0.6419\n",
            "Early stopping:  0.047944257162295\n",
            "Epoch: 031, Loss: 0.7330, Train: 0.7879, Test: 0.6447\n",
            "Early stopping:  0.04633157490473465\n",
            "Epoch: 032, Loss: 0.7063, Train: 0.7975, Test: 0.6473\n",
            "Early stopping:  0.044943380319439376\n",
            "Epoch: 033, Loss: 0.6796, Train: 0.8054, Test: 0.6497\n",
            "Early stopping:  0.043658208111209316\n",
            "Epoch: 034, Loss: 0.6545, Train: 0.8179, Test: 0.6513\n",
            "Early stopping:  0.042184575356930196\n",
            "Epoch: 035, Loss: 0.6295, Train: 0.8293, Test: 0.6525\n",
            "Early stopping:  0.04094249995930926\n",
            "Epoch: 036, Loss: 0.6054, Train: 0.8417, Test: 0.6566\n",
            "Early stopping:  0.039838993972332065\n",
            "Epoch: 037, Loss: 0.5816, Train: 0.8503, Test: 0.6595\n",
            "Early stopping:  0.03877750761993174\n",
            "Epoch: 038, Loss: 0.5590, Train: 0.8593, Test: 0.6595\n",
            "Early stopping:  0.03779654180823115\n",
            "Epoch: 039, Loss: 0.5364, Train: 0.8718, Test: 0.6608\n",
            "Early stopping:  0.03677071226931689\n",
            "Epoch: 040, Loss: 0.5148, Train: 0.8820, Test: 0.6633\n",
            "Early stopping:  0.035796632376083604\n",
            "Epoch: 041, Loss: 0.4936, Train: 0.8871, Test: 0.6635\n",
            "Early stopping:  0.034817927676090925\n",
            "Epoch: 042, Loss: 0.4731, Train: 0.8968, Test: 0.6649\n",
            "Early stopping:  0.033930609937733384\n",
            "Epoch: 043, Loss: 0.4528, Train: 0.9041, Test: 0.6684\n",
            "Early stopping:  0.033026662439008016\n",
            "Epoch: 044, Loss: 0.4333, Train: 0.9115, Test: 0.6677\n",
            "Early stopping:  0.032206979313156324\n",
            "Epoch: 045, Loss: 0.4144, Train: 0.9206, Test: 0.6708\n",
            "Early stopping:  0.03134514927452868\n",
            "Epoch: 046, Loss: 0.3956, Train: 0.9314, Test: 0.6700\n",
            "Early stopping:  0.03058297266890771\n",
            "Epoch: 047, Loss: 0.3776, Train: 0.9370, Test: 0.6693\n",
            "Early stopping:  0.029729703706847976\n",
            "Epoch: 048, Loss: 0.3602, Train: 0.9421, Test: 0.6700\n",
            "Early stopping:  0.0289295229573241\n",
            "Epoch: 049, Loss: 0.3432, Train: 0.9455, Test: 0.6704\n",
            "Early stopping:  0.028113709616443657\n",
            "Epoch: 050, Loss: 0.3268, Train: 0.9529, Test: 0.6711\n",
            "Early stopping:  0.027202283216306387\n",
            "Epoch: 051, Loss: 0.3111, Train: 0.9563, Test: 0.6727\n",
            "Early stopping:  0.02631519115840214\n",
            "Epoch: 052, Loss: 0.2959, Train: 0.9620, Test: 0.6698\n",
            "Early stopping:  0.025394282675003644\n",
            "Epoch: 053, Loss: 0.2814, Train: 0.9677, Test: 0.6710\n",
            "Early stopping:  0.024425522773781242\n",
            "Epoch: 054, Loss: 0.2676, Train: 0.9711, Test: 0.6700\n",
            "Early stopping:  0.023435301552885042\n",
            "Epoch: 055, Loss: 0.2544, Train: 0.9733, Test: 0.6697\n",
            "Early stopping:  0.022414470260761696\n",
            "Epoch: 056, Loss: 0.2423, Train: 0.9773, Test: 0.6715\n",
            "Early stopping:  0.02124627425642804\n",
            "Epoch: 057, Loss: 0.2312, Train: 0.9801, Test: 0.6739\n",
            "Early stopping:  0.01990648930278133\n",
            "Epoch: 058, Loss: 0.2230, Train: 0.9853, Test: 0.6661\n",
            "Early stopping:  0.01783174132368451\n",
            "Epoch: 059, Loss: 0.2178, Train: 0.9790, Test: 0.6704\n",
            "Early stopping:  0.014804393125657598\n",
            "Epoch: 060, Loss: 0.2110, Train: 0.9892, Test: 0.6705\n",
            "Early stopping:  0.012128885957139757\n",
            "Epoch: 061, Loss: 0.1939, Train: 0.9909, Test: 0.6718\n",
            "Early stopping:  0.01409068308859084\n",
            "Epoch: 062, Loss: 0.1858, Train: 0.9875, Test: 0.6722\n",
            "Early stopping:  0.015873154946020897\n",
            "Epoch: 063, Loss: 0.1841, Train: 0.9938, Test: 0.6707\n",
            "Early stopping:  0.015150291076496132\n",
            "Epoch: 064, Loss: 0.1719, Train: 0.9926, Test: 0.6718\n",
            "Early stopping:  0.014464618665877574\n",
            "Epoch: 065, Loss: 0.1660, Train: 0.9921, Test: 0.6748\n",
            "Early stopping:  0.011248579419565854\n",
            "Epoch: 066, Loss: 0.1637, Train: 0.9949, Test: 0.6722\n",
            "Early stopping:  0.010185052934192387\n",
            "Epoch: 067, Loss: 0.1540, Train: 0.9955, Test: 0.6701\n",
            "Early stopping:  0.01110581711531733\n",
            "Epoch: 068, Loss: 0.1510, Train: 0.9949, Test: 0.6751\n",
            "Early stopping:  0.008636492170392479\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.69      0.68      0.69       758\n",
            "         capital_goods       0.59      0.57      0.58       508\n",
            "conglomerates_industry       0.89      0.20      0.33        80\n",
            "     consumer_cyclical       0.67      0.59      0.63       793\n",
            " consumer_non-cyclical       0.73      0.57      0.64       446\n",
            "                energy       0.81      0.68      0.74       283\n",
            "             financial       0.71      0.69      0.70       767\n",
            "            healthcare       0.77      0.69      0.73       318\n",
            "              services       0.63      0.79      0.70      2076\n",
            "            technology       0.61      0.49      0.55       396\n",
            "        transportation       0.78      0.73      0.76       404\n",
            "             utilities       0.77      0.63      0.69       225\n",
            "\n",
            "              accuracy                           0.68      7054\n",
            "             macro avg       0.72      0.61      0.64      7054\n",
            "          weighted avg       0.68      0.68      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4668, Train: 0.2944, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2736, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.13660720833047996\n",
            "Epoch: 003, Loss: 2.1668, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15207992384847996\n",
            "Epoch: 004, Loss: 2.1151, Train: 0.3035, Test: 0.3014\n",
            "Early stopping:  0.15552581856247574\n",
            "Epoch: 005, Loss: 2.0527, Train: 0.3630, Test: 0.3587\n",
            "Early stopping:  0.16239341653601347\n",
            "Epoch: 006, Loss: 1.9878, Train: 0.4027, Test: 0.3859\n",
            "Early stopping:  0.10933136072094393\n",
            "Epoch: 007, Loss: 1.9139, Train: 0.3999, Test: 0.3826\n",
            "Early stopping:  0.10030089497944429\n",
            "Epoch: 008, Loss: 1.8459, Train: 0.4010, Test: 0.3877\n",
            "Early stopping:  0.10712861455671019\n",
            "Epoch: 009, Loss: 1.7865, Train: 0.4209, Test: 0.4069\n",
            "Early stopping:  0.10668463616979638\n",
            "Epoch: 010, Loss: 1.7228, Train: 0.4555, Test: 0.4334\n",
            "Early stopping:  0.10402669282377865\n",
            "Epoch: 011, Loss: 1.6535, Train: 0.4861, Test: 0.4558\n",
            "Early stopping:  0.10183625573421093\n",
            "Epoch: 012, Loss: 1.5889, Train: 0.5043, Test: 0.4677\n",
            "Early stopping:  0.10232584977322455\n",
            "Epoch: 013, Loss: 1.5287, Train: 0.5122, Test: 0.4753\n",
            "Early stopping:  0.10270413195435464\n",
            "Epoch: 014, Loss: 1.4675, Train: 0.5258, Test: 0.4860\n",
            "Early stopping:  0.1005085991893501\n",
            "Epoch: 015, Loss: 1.4095, Train: 0.5502, Test: 0.5048\n",
            "Early stopping:  0.09638718212033602\n",
            "Epoch: 016, Loss: 1.3514, Train: 0.5842, Test: 0.5282\n",
            "Early stopping:  0.09398845421070234\n",
            "Epoch: 017, Loss: 1.2938, Train: 0.6200, Test: 0.5585\n",
            "Early stopping:  0.09265371623424669\n",
            "Epoch: 018, Loss: 1.2430, Train: 0.6370, Test: 0.5700\n",
            "Early stopping:  0.08929284200759685\n",
            "Epoch: 019, Loss: 1.1926, Train: 0.6466, Test: 0.5743\n",
            "Early stopping:  0.08577288064562436\n",
            "Epoch: 020, Loss: 1.1430, Train: 0.6568, Test: 0.5802\n",
            "Early stopping:  0.0819468530099415\n",
            "Epoch: 021, Loss: 1.0965, Train: 0.6704, Test: 0.5882\n",
            "Early stopping:  0.07823371500264369\n",
            "Epoch: 022, Loss: 1.0530, Train: 0.6863, Test: 0.5953\n",
            "Early stopping:  0.07533118455582757\n",
            "Epoch: 023, Loss: 1.0142, Train: 0.6971, Test: 0.5987\n",
            "Early stopping:  0.07071881695308346\n",
            "Epoch: 024, Loss: 0.9747, Train: 0.6994, Test: 0.6058\n",
            "Early stopping:  0.0662808853106907\n",
            "Epoch: 025, Loss: 0.9392, Train: 0.7136, Test: 0.6100\n",
            "Early stopping:  0.06214668120816448\n",
            "Epoch: 026, Loss: 0.9052, Train: 0.7266, Test: 0.6133\n",
            "Early stopping:  0.05861728300470555\n",
            "Epoch: 027, Loss: 0.8723, Train: 0.7357, Test: 0.6198\n",
            "Early stopping:  0.05588546831793888\n",
            "Epoch: 028, Loss: 0.8405, Train: 0.7453, Test: 0.6267\n",
            "Early stopping:  0.05302158354856923\n",
            "Epoch: 029, Loss: 0.8100, Train: 0.7544, Test: 0.6303\n",
            "Early stopping:  0.051112613266797106\n",
            "Epoch: 030, Loss: 0.7804, Train: 0.7663, Test: 0.6316\n",
            "Early stopping:  0.04931958744493631\n",
            "Epoch: 031, Loss: 0.7512, Train: 0.7794, Test: 0.6345\n",
            "Early stopping:  0.04781275056562013\n",
            "Epoch: 032, Loss: 0.7230, Train: 0.7833, Test: 0.6394\n",
            "Early stopping:  0.0464602286868476\n",
            "Epoch: 033, Loss: 0.6962, Train: 0.7952, Test: 0.6409\n",
            "Early stopping:  0.04506636696577057\n",
            "Epoch: 034, Loss: 0.6690, Train: 0.8088, Test: 0.6439\n",
            "Early stopping:  0.04393569224567383\n",
            "Epoch: 035, Loss: 0.6430, Train: 0.8179, Test: 0.6470\n",
            "Early stopping:  0.042739659928318664\n",
            "Epoch: 036, Loss: 0.6180, Train: 0.8298, Test: 0.6486\n",
            "Early stopping:  0.041598071524172095\n",
            "Epoch: 037, Loss: 0.5933, Train: 0.8417, Test: 0.6532\n",
            "Early stopping:  0.04061186346214465\n",
            "Epoch: 038, Loss: 0.5692, Train: 0.8548, Test: 0.6552\n",
            "Early stopping:  0.03943167183047503\n",
            "Epoch: 039, Loss: 0.5461, Train: 0.8622, Test: 0.6575\n",
            "Early stopping:  0.03838871666629916\n",
            "Epoch: 040, Loss: 0.5235, Train: 0.8724, Test: 0.6595\n",
            "Early stopping:  0.03737019362860956\n",
            "Epoch: 041, Loss: 0.5013, Train: 0.8815, Test: 0.6622\n",
            "Early stopping:  0.03631270963076226\n",
            "Epoch: 042, Loss: 0.4798, Train: 0.8905, Test: 0.6627\n",
            "Early stopping:  0.03536859808089266\n",
            "Epoch: 043, Loss: 0.4586, Train: 0.8968, Test: 0.6654\n",
            "Early stopping:  0.03456846587829282\n",
            "Epoch: 044, Loss: 0.4384, Train: 0.9087, Test: 0.6683\n",
            "Early stopping:  0.033672370671809435\n",
            "Epoch: 045, Loss: 0.4187, Train: 0.9144, Test: 0.6677\n",
            "Early stopping:  0.03267812553089208\n",
            "Epoch: 046, Loss: 0.3996, Train: 0.9189, Test: 0.6669\n",
            "Early stopping:  0.03167714162111226\n",
            "Epoch: 047, Loss: 0.3812, Train: 0.9325, Test: 0.6657\n",
            "Early stopping:  0.030609621391399878\n",
            "Epoch: 048, Loss: 0.3635, Train: 0.9393, Test: 0.6667\n",
            "Early stopping:  0.029603298544963096\n",
            "Epoch: 049, Loss: 0.3463, Train: 0.9450, Test: 0.6684\n",
            "Early stopping:  0.028617479440964485\n",
            "Epoch: 050, Loss: 0.3297, Train: 0.9552, Test: 0.6683\n",
            "Early stopping:  0.02762078676857785\n",
            "Epoch: 051, Loss: 0.3138, Train: 0.9552, Test: 0.6683\n",
            "Early stopping:  0.026672296957866874\n",
            "Epoch: 052, Loss: 0.2992, Train: 0.9671, Test: 0.6667\n",
            "Early stopping:  0.025467195284653774\n",
            "Epoch: 053, Loss: 0.2872, Train: 0.9535, Test: 0.6657\n",
            "Early stopping:  0.023542608059177957\n",
            "Epoch: 054, Loss: 0.2824, Train: 0.9660, Test: 0.6569\n",
            "Early stopping:  0.01948295924269185\n",
            "Epoch: 055, Loss: 0.2831, Train: 0.9620, Test: 0.6676\n",
            "Early stopping:  0.013374778871820136\n",
            "Epoch: 056, Loss: 0.2593, Train: 0.9762, Test: 0.6687\n",
            "Early stopping:  0.014478070813576584\n",
            "Epoch: 057, Loss: 0.2393, Train: 0.9784, Test: 0.6613\n",
            "Early stopping:  0.020461263512366313\n",
            "Epoch: 058, Loss: 0.2432, Train: 0.9790, Test: 0.6701\n",
            "Early stopping:  0.020811145840242483\n",
            "Epoch: 059, Loss: 0.2250, Train: 0.9801, Test: 0.6707\n",
            "Early stopping:  0.022160784861463726\n",
            "Epoch: 060, Loss: 0.2156, Train: 0.9881, Test: 0.6644\n",
            "Early stopping:  0.016913753756148113\n",
            "Epoch: 061, Loss: 0.2149, Train: 0.9864, Test: 0.6717\n",
            "Early stopping:  0.013168162084923184\n",
            "Epoch: 062, Loss: 0.1983, Train: 0.9813, Test: 0.6722\n",
            "Early stopping:  0.0164164392548489\n",
            "Epoch: 063, Loss: 0.1977, Train: 0.9892, Test: 0.6681\n",
            "Early stopping:  0.011924936691007765\n",
            "Epoch: 064, Loss: 0.1913, Train: 0.9909, Test: 0.6721\n",
            "Early stopping:  0.011016777286070322\n",
            "Epoch: 065, Loss: 0.1802, Train: 0.9864, Test: 0.6722\n",
            "Early stopping:  0.012610146205224606\n",
            "Epoch: 066, Loss: 0.1806, Train: 0.9909, Test: 0.6700\n",
            "Early stopping:  0.008835178402533101\n",
            "PREDICTIONS -> tensor([ 0,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.68      0.68      0.68       758\n",
            "         capital_goods       0.55      0.57      0.56       508\n",
            "conglomerates_industry       0.74      0.21      0.33        80\n",
            "     consumer_cyclical       0.56      0.68      0.61       793\n",
            " consumer_non-cyclical       0.73      0.56      0.63       446\n",
            "                energy       0.81      0.69      0.75       283\n",
            "             financial       0.69      0.70      0.70       767\n",
            "            healthcare       0.75      0.72      0.73       318\n",
            "              services       0.68      0.72      0.70      2076\n",
            "            technology       0.57      0.54      0.56       396\n",
            "        transportation       0.79      0.74      0.77       404\n",
            "             utilities       0.82      0.65      0.73       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.70      0.62      0.64      7054\n",
            "          weighted avg       0.68      0.67      0.67      7054\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4546, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2436, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.14921906269148583\n",
            "Epoch: 003, Loss: 2.1699, Train: 0.2984, Test: 0.2970\n",
            "Early stopping:  0.14776181304499672\n",
            "Epoch: 004, Loss: 2.0887, Train: 0.3539, Test: 0.3476\n",
            "Early stopping:  0.15690835381637847\n",
            "Epoch: 005, Loss: 2.0423, Train: 0.3897, Test: 0.3863\n",
            "Early stopping:  0.16190866791748254\n",
            "Epoch: 006, Loss: 1.9888, Train: 0.3880, Test: 0.3816\n",
            "Early stopping:  0.10135569615429138\n",
            "Epoch: 007, Loss: 1.9139, Train: 0.3744, Test: 0.3696\n",
            "Early stopping:  0.09717347997044538\n",
            "Epoch: 008, Loss: 1.8457, Train: 0.3800, Test: 0.3690\n",
            "Early stopping:  0.09759877334068613\n",
            "Epoch: 009, Loss: 1.7879, Train: 0.3993, Test: 0.3930\n",
            "Early stopping:  0.10320739346122242\n",
            "Epoch: 010, Loss: 1.7229, Train: 0.4436, Test: 0.4250\n",
            "Early stopping:  0.10410899832737551\n",
            "Epoch: 011, Loss: 1.6538, Train: 0.4765, Test: 0.4545\n",
            "Early stopping:  0.10170216561545467\n",
            "Epoch: 012, Loss: 1.5920, Train: 0.5099, Test: 0.4691\n",
            "Early stopping:  0.10146194655248128\n",
            "Epoch: 013, Loss: 1.5302, Train: 0.5230, Test: 0.4865\n",
            "Early stopping:  0.10223482532129295\n",
            "Epoch: 014, Loss: 1.4655, Train: 0.5332, Test: 0.4993\n",
            "Early stopping:  0.10096670735986686\n",
            "Epoch: 015, Loss: 1.4079, Train: 0.5496, Test: 0.5122\n",
            "Early stopping:  0.09776924048860108\n",
            "Epoch: 016, Loss: 1.3544, Train: 0.5780, Test: 0.5340\n",
            "Early stopping:  0.09453872429827803\n",
            "Epoch: 017, Loss: 1.2973, Train: 0.6041, Test: 0.5493\n",
            "Early stopping:  0.09125074438792073\n",
            "Epoch: 018, Loss: 1.2459, Train: 0.6234, Test: 0.5624\n",
            "Early stopping:  0.08694971031671483\n",
            "Epoch: 019, Loss: 1.2003, Train: 0.6336, Test: 0.5747\n",
            "Early stopping:  0.08288134819582106\n",
            "Epoch: 020, Loss: 1.1542, Train: 0.6455, Test: 0.5831\n",
            "Early stopping:  0.07874175199130712\n",
            "Epoch: 021, Loss: 1.1121, Train: 0.6563, Test: 0.5904\n",
            "Early stopping:  0.07311316942921525\n",
            "Epoch: 022, Loss: 1.0721, Train: 0.6693, Test: 0.5937\n",
            "Early stopping:  0.06893604123195131\n",
            "Epoch: 023, Loss: 1.0335, Train: 0.6835, Test: 0.5982\n",
            "Early stopping:  0.06575745934804574\n",
            "Epoch: 024, Loss: 0.9997, Train: 0.6931, Test: 0.6012\n",
            "Early stopping:  0.061327434677243844\n",
            "Epoch: 025, Loss: 0.9654, Train: 0.6994, Test: 0.6066\n",
            "Early stopping:  0.05789219932367396\n",
            "Epoch: 026, Loss: 0.9326, Train: 0.7119, Test: 0.6134\n",
            "Early stopping:  0.054897469300332395\n",
            "Epoch: 027, Loss: 0.9012, Train: 0.7306, Test: 0.6212\n",
            "Early stopping:  0.052463256465500305\n",
            "Epoch: 028, Loss: 0.8703, Train: 0.7396, Test: 0.6256\n",
            "Early stopping:  0.051089833908448665\n",
            "Epoch: 029, Loss: 0.8409, Train: 0.7516, Test: 0.6289\n",
            "Early stopping:  0.049243299627571846\n",
            "Epoch: 030, Loss: 0.8107, Train: 0.7521, Test: 0.6321\n",
            "Early stopping:  0.04808490798691007\n",
            "Epoch: 031, Loss: 0.7830, Train: 0.7657, Test: 0.6372\n",
            "Early stopping:  0.04677978593965834\n",
            "Epoch: 032, Loss: 0.7552, Train: 0.7805, Test: 0.6403\n",
            "Early stopping:  0.04555814355122838\n",
            "Epoch: 033, Loss: 0.7289, Train: 0.7896, Test: 0.6453\n",
            "Early stopping:  0.044214239320906595\n",
            "Epoch: 034, Loss: 0.7028, Train: 0.7941, Test: 0.6483\n",
            "Early stopping:  0.042702777991628825\n",
            "Epoch: 035, Loss: 0.6777, Train: 0.8032, Test: 0.6503\n",
            "Early stopping:  0.04160549988106585\n",
            "Epoch: 036, Loss: 0.6536, Train: 0.8151, Test: 0.6518\n",
            "Early stopping:  0.04022589306711124\n",
            "Epoch: 037, Loss: 0.6297, Train: 0.8293, Test: 0.6532\n",
            "Early stopping:  0.03916817245382509\n",
            "Epoch: 038, Loss: 0.6064, Train: 0.8355, Test: 0.6562\n",
            "Early stopping:  0.03806519023768617\n",
            "Epoch: 039, Loss: 0.5840, Train: 0.8463, Test: 0.6582\n",
            "Early stopping:  0.03707165728582273\n",
            "Epoch: 040, Loss: 0.5615, Train: 0.8588, Test: 0.6596\n",
            "Early stopping:  0.03632499774453999\n",
            "Epoch: 041, Loss: 0.5398, Train: 0.8661, Test: 0.6635\n",
            "Early stopping:  0.03552833555206303\n",
            "Epoch: 042, Loss: 0.5181, Train: 0.8724, Test: 0.6635\n",
            "Early stopping:  0.03494201470685944\n",
            "Epoch: 043, Loss: 0.4973, Train: 0.8832, Test: 0.6650\n",
            "Early stopping:  0.034294591722579044\n",
            "Epoch: 044, Loss: 0.4765, Train: 0.8934, Test: 0.6653\n",
            "Early stopping:  0.03359580244325571\n",
            "Epoch: 045, Loss: 0.4564, Train: 0.9007, Test: 0.6659\n",
            "Early stopping:  0.0329269830903707\n",
            "Epoch: 046, Loss: 0.4368, Train: 0.9109, Test: 0.6670\n",
            "Early stopping:  0.03215884569033095\n",
            "Epoch: 047, Loss: 0.4178, Train: 0.9212, Test: 0.6695\n",
            "Early stopping:  0.03143027296000439\n",
            "Epoch: 048, Loss: 0.3991, Train: 0.9240, Test: 0.6687\n",
            "Early stopping:  0.03058690396804311\n",
            "Epoch: 049, Loss: 0.3812, Train: 0.9336, Test: 0.6698\n",
            "Early stopping:  0.029765166695278494\n",
            "Epoch: 050, Loss: 0.3636, Train: 0.9410, Test: 0.6700\n",
            "Early stopping:  0.028956688814295403\n",
            "Epoch: 051, Loss: 0.3467, Train: 0.9438, Test: 0.6711\n",
            "Early stopping:  0.028103564918595735\n",
            "Epoch: 052, Loss: 0.3303, Train: 0.9518, Test: 0.6725\n",
            "Early stopping:  0.027224560267384346\n",
            "Epoch: 053, Loss: 0.3146, Train: 0.9586, Test: 0.6724\n",
            "Early stopping:  0.026326013165048668\n",
            "Epoch: 054, Loss: 0.2995, Train: 0.9631, Test: 0.6735\n",
            "Early stopping:  0.02536454047292285\n",
            "Epoch: 055, Loss: 0.2849, Train: 0.9677, Test: 0.6741\n",
            "Early stopping:  0.02442519368471251\n",
            "Epoch: 056, Loss: 0.2709, Train: 0.9722, Test: 0.6747\n",
            "Early stopping:  0.023468121336907746\n",
            "Epoch: 057, Loss: 0.2576, Train: 0.9745, Test: 0.6749\n",
            "Early stopping:  0.022536939553721307\n",
            "Epoch: 058, Loss: 0.2449, Train: 0.9767, Test: 0.6768\n",
            "Early stopping:  0.021589892760262297\n",
            "Epoch: 059, Loss: 0.2327, Train: 0.9790, Test: 0.6768\n",
            "Early stopping:  0.020641972950993598\n",
            "Epoch: 060, Loss: 0.2211, Train: 0.9841, Test: 0.6759\n",
            "Early stopping:  0.019695552364991382\n",
            "Epoch: 061, Loss: 0.2102, Train: 0.9847, Test: 0.6768\n",
            "Early stopping:  0.018756409033265694\n",
            "Epoch: 062, Loss: 0.2000, Train: 0.9870, Test: 0.6776\n",
            "Early stopping:  0.017753282466995212\n",
            "Epoch: 063, Loss: 0.1908, Train: 0.9870, Test: 0.6747\n",
            "Early stopping:  0.016589542212294278\n",
            "Epoch: 064, Loss: 0.1838, Train: 0.9887, Test: 0.6749\n",
            "Early stopping:  0.014909677900265208\n",
            "Epoch: 065, Loss: 0.1808, Train: 0.9898, Test: 0.6718\n",
            "Early stopping:  0.01206975029186471\n",
            "Epoch: 066, Loss: 0.1783, Train: 0.9921, Test: 0.6737\n",
            "Early stopping:  0.008788170551424893\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  1], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.62      0.68      0.65       758\n",
            "         capital_goods       0.58      0.58      0.58       508\n",
            "conglomerates_industry       0.89      0.20      0.33        80\n",
            "     consumer_cyclical       0.58      0.68      0.63       793\n",
            " consumer_non-cyclical       0.74      0.59      0.66       446\n",
            "                energy       0.79      0.66      0.72       283\n",
            "             financial       0.76      0.67      0.71       767\n",
            "            healthcare       0.76      0.68      0.72       318\n",
            "              services       0.68      0.73      0.70      2076\n",
            "            technology       0.56      0.57      0.57       396\n",
            "        transportation       0.83      0.76      0.79       404\n",
            "             utilities       0.82      0.64      0.72       225\n",
            "\n",
            "              accuracy                           0.67      7054\n",
            "             macro avg       0.72      0.62      0.65      7054\n",
            "          weighted avg       0.68      0.67      0.67      7054\n",
            "\n",
            "time: 2min 8s (started: 2024-10-16 21:39:15 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcitz5KzgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae1a1fd-077b-4eed-db65-9cd9f3646234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 467 ms (started: 2024-10-16 21:41:23 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-aLOM-rgWjL"
      },
      "source": [
        "### Training rotulated base = 40% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHu1LT2zgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ab957e-f47d-4dac-fc89-1cd93668528b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 379 µs (started: 2024-10-16 21:41:24 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMAztIHgWjL"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usze5V3lgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42123ef-5d71-430f-e204-4f84fab0e5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 55.5688, Train: 0.1129, Test: 0.1119\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 400.0464, Train: 0.2927, Test: 0.2924\n",
            "Early stopping:  243.58238765315772\n",
            "Epoch: 003, Loss: 465.3965, Train: 0.1438, Test: 0.1344\n",
            "Early stopping:  220.18708919025462\n",
            "Epoch: 004, Loss: 323.3477, Train: 0.0967, Test: 0.0830\n",
            "Early stopping:  179.96763336550976\n",
            "Epoch: 005, Loss: 363.8507, Train: 0.0536, Test: 0.0478\n",
            "Early stopping:  157.63249536211282\n",
            "Epoch: 006, Loss: 387.6730, Train: 0.1710, Test: 0.1690\n",
            "Early stopping:  52.198027434287056\n",
            "Epoch: 007, Loss: 338.8671, Train: 0.2782, Test: 0.2633\n",
            "Early stopping:  55.73726914574455\n",
            "Epoch: 008, Loss: 327.1887, Train: 0.1393, Test: 0.1329\n",
            "Early stopping:  27.153626626867418\n",
            "Epoch: 009, Loss: 302.4108, Train: 0.2998, Test: 0.2939\n",
            "Early stopping:  32.93937187967281\n",
            "Epoch: 010, Loss: 250.6355, Train: 0.2918, Test: 0.2826\n",
            "Early stopping:  50.24668140834111\n",
            "Epoch: 011, Loss: 165.8226, Train: 0.1066, Test: 0.1005\n",
            "Early stopping:  70.79487398995008\n",
            "Epoch: 012, Loss: 128.7174, Train: 0.1081, Test: 0.1057\n",
            "Early stopping:  85.68683932750687\n",
            "Epoch: 013, Loss: 68.8064, Train: 0.1265, Test: 0.1157\n",
            "Early stopping:  93.69492817586594\n",
            "Epoch: 014, Loss: 23.4884, Train: 0.0868, Test: 0.0782\n",
            "Early stopping:  87.88600121395005\n",
            "Epoch: 015, Loss: 8.4626, Train: 0.0811, Test: 0.0779\n",
            "Early stopping:  67.39978855030739\n",
            "Epoch: 016, Loss: 5.9289, Train: 0.0879, Test: 0.0813\n",
            "Early stopping:  52.148764302782716\n",
            "Epoch: 017, Loss: 4.4400, Train: 0.0995, Test: 0.0913\n",
            "Early stopping:  27.123293626770764\n",
            "Epoch: 018, Loss: 3.4868, Train: 0.1112, Test: 0.1043\n",
            "Early stopping:  8.22637362556369\n",
            "Epoch: 019, Loss: 2.9666, Train: 0.1290, Test: 0.1225\n",
            "Early stopping:  2.2121546696409515\n",
            "Epoch: 020, Loss: 2.6840, Train: 0.1418, Test: 0.1334\n",
            "Early stopping:  1.3161185163940694\n",
            "Epoch: 021, Loss: 2.5546, Train: 0.1438, Test: 0.1351\n",
            "Early stopping:  0.7670902918389157\n",
            "Epoch: 022, Loss: 2.4968, Train: 0.1580, Test: 0.1470\n",
            "Early stopping:  0.40554623531013173\n",
            "Epoch: 023, Loss: 2.4746, Train: 0.1597, Test: 0.1493\n",
            "Early stopping:  0.20225338641892324\n",
            "Epoch: 024, Loss: 2.4671, Train: 0.1537, Test: 0.1438\n",
            "Early stopping:  0.08983838003395836\n",
            "Epoch: 025, Loss: 2.4526, Train: 0.1421, Test: 0.1329\n",
            "Early stopping:  0.03993922103004399\n",
            "Epoch: 026, Loss: 2.4369, Train: 0.1347, Test: 0.1276\n",
            "Early stopping:  0.022645170350215803\n",
            "Epoch: 027, Loss: 2.4205, Train: 0.1324, Test: 0.1255\n",
            "Early stopping:  0.022062536957020585\n",
            "Epoch: 028, Loss: 2.4019, Train: 0.1330, Test: 0.1229\n",
            "Early stopping:  0.025732017476185653\n",
            "Epoch: 029, Loss: 2.3859, Train: 0.1410, Test: 0.1319\n",
            "Early stopping:  0.02663609144146211\n",
            "Epoch: 030, Loss: 2.3737, Train: 0.1407, Test: 0.1321\n",
            "Early stopping:  0.02553816678446092\n",
            "Epoch: 031, Loss: 2.3670, Train: 0.1412, Test: 0.1334\n",
            "Early stopping:  0.021697847989855273\n",
            "Epoch: 032, Loss: 2.3622, Train: 0.1446, Test: 0.1393\n",
            "Early stopping:  0.015994553680165856\n",
            "Epoch: 033, Loss: 2.3558, Train: 0.1509, Test: 0.1416\n",
            "Early stopping:  0.011525984316644606\n",
            "Epoch: 034, Loss: 2.3481, Train: 0.1580, Test: 0.1461\n",
            "Early stopping:  0.009872243146733498\n",
            "PREDICTIONS -> tensor([3, 3, 3,  ..., 3, 3, 3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       568\n",
            "         capital_goods       0.14      0.11      0.12       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.12      0.83      0.20       595\n",
            " consumer_non-cyclical       0.39      0.18      0.25       334\n",
            "                energy       0.17      0.06      0.09       213\n",
            "             financial       0.11      0.05      0.06       576\n",
            "            healthcare       1.00      0.03      0.07       238\n",
            "              services       0.54      0.02      0.04      1557\n",
            "            technology       0.06      0.01      0.01       297\n",
            "        transportation       0.89      0.23      0.37       303\n",
            "             utilities       0.30      0.13      0.18       169\n",
            "\n",
            "              accuracy                           0.15      5291\n",
            "             macro avg       0.31      0.14      0.12      5291\n",
            "          weighted avg       0.33      0.15      0.10      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 74.2253, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 457.5911, Train: 0.0474, Test: 0.0478\n",
            "Early stopping:  271.080496769919\n",
            "Epoch: 003, Loss: 530.7856, Train: 0.0584, Test: 0.0590\n",
            "Early stopping:  245.2121382292879\n",
            "Epoch: 004, Loss: 425.1281, Train: 0.1072, Test: 0.1094\n",
            "Early stopping:  203.33143368265755\n",
            "Epoch: 005, Loss: 367.3201, Train: 0.0771, Test: 0.0713\n",
            "Early stopping:  176.10226817476826\n",
            "Epoch: 006, Loss: 281.8610, Train: 0.1208, Test: 0.1176\n",
            "Early stopping:  93.93070881988847\n",
            "Epoch: 007, Loss: 271.3114, Train: 0.1228, Test: 0.1092\n",
            "Early stopping:  107.54854564675763\n",
            "Epoch: 008, Loss: 170.4464, Train: 0.3003, Test: 0.2916\n",
            "Early stopping:  97.56229847220987\n",
            "Epoch: 009, Loss: 113.4060, Train: 0.3120, Test: 0.3096\n",
            "Early stopping:  99.75360143453369\n",
            "Epoch: 010, Loss: 127.2753, Train: 0.1761, Test: 0.1705\n",
            "Early stopping:  79.36067323016624\n",
            "Epoch: 011, Loss: 124.3598, Train: 0.2847, Test: 0.2873\n",
            "Early stopping:  65.1963234608159\n",
            "Epoch: 012, Loss: 106.0935, Train: 0.2343, Test: 0.2253\n",
            "Early stopping:  25.039232010453798\n",
            "Epoch: 013, Loss: 88.9585, Train: 0.2448, Test: 0.2340\n",
            "Early stopping:  15.44239617175378\n",
            "Epoch: 014, Loss: 67.0656, Train: 0.1710, Test: 0.1627\n",
            "Early stopping:  25.21523188821623\n",
            "Epoch: 015, Loss: 47.8801, Train: 0.0822, Test: 0.0714\n",
            "Early stopping:  30.380157139426228\n",
            "Epoch: 016, Loss: 30.9805, Train: 0.0947, Test: 0.0983\n",
            "Early stopping:  30.275044020823238\n",
            "Epoch: 017, Loss: 16.5113, Train: 0.1583, Test: 0.1618\n",
            "Early stopping:  28.706896852116277\n",
            "Epoch: 018, Loss: 6.5605, Train: 0.2042, Test: 0.2028\n",
            "Early stopping:  24.25743062652894\n",
            "Epoch: 019, Loss: 4.5258, Train: 0.2201, Test: 0.2194\n",
            "Early stopping:  18.17808900604255\n",
            "Epoch: 020, Loss: 3.5616, Train: 0.2362, Test: 0.2313\n",
            "Early stopping:  11.579637727694863\n",
            "Epoch: 021, Loss: 2.9888, Train: 0.2286, Test: 0.2351\n",
            "Early stopping:  5.580005442938051\n",
            "Epoch: 022, Loss: 2.6880, Train: 0.2399, Test: 0.2359\n",
            "Early stopping:  1.5610474167325084\n",
            "Epoch: 023, Loss: 2.5256, Train: 0.2354, Test: 0.2345\n",
            "Early stopping:  0.8113589344380936\n",
            "Epoch: 024, Loss: 2.4537, Train: 0.2317, Test: 0.2277\n",
            "Early stopping:  0.45110437165983\n",
            "Epoch: 025, Loss: 2.4706, Train: 0.2425, Test: 0.2372\n",
            "Early stopping:  0.22324732156543015\n",
            "Epoch: 026, Loss: 2.4572, Train: 0.2615, Test: 0.2538\n",
            "Early stopping:  0.09879400639966043\n",
            "Epoch: 027, Loss: 2.3909, Train: 0.3009, Test: 0.2905\n",
            "Early stopping:  0.048085075879313224\n",
            "Epoch: 028, Loss: 2.3061, Train: 0.3242, Test: 0.3132\n",
            "Early stopping:  0.0685833599428542\n",
            "Epoch: 029, Loss: 2.3281, Train: 0.3383, Test: 0.3234\n",
            "Early stopping:  0.07397562987773516\n",
            "Epoch: 030, Loss: 2.2732, Train: 0.3460, Test: 0.3321\n",
            "Early stopping:  0.07325717422721596\n",
            "Epoch: 031, Loss: 2.2087, Train: 0.3469, Test: 0.3302\n",
            "Early stopping:  0.06733402477535087\n",
            "Epoch: 032, Loss: 2.1731, Train: 0.3537, Test: 0.3381\n",
            "Early stopping:  0.06537026075644072\n",
            "Epoch: 033, Loss: 2.1383, Train: 0.3593, Test: 0.3519\n",
            "Early stopping:  0.07651142138133737\n",
            "Epoch: 034, Loss: 2.1060, Train: 0.3698, Test: 0.3617\n",
            "Early stopping:  0.06471779816318131\n",
            "Epoch: 035, Loss: 2.0713, Train: 0.3778, Test: 0.3680\n",
            "Early stopping:  0.05405593045898466\n",
            "Epoch: 036, Loss: 2.0235, Train: 0.3798, Test: 0.3612\n",
            "Early stopping:  0.058104127956181584\n",
            "Epoch: 037, Loss: 1.9933, Train: 0.3815, Test: 0.3627\n",
            "Early stopping:  0.05907772293248476\n",
            "Epoch: 038, Loss: 1.9730, Train: 0.3922, Test: 0.3748\n",
            "Early stopping:  0.0549286102850335\n",
            "Epoch: 039, Loss: 1.9373, Train: 0.3988, Test: 0.3780\n",
            "Early stopping:  0.05082221419676743\n",
            "Epoch: 040, Loss: 1.9014, Train: 0.4027, Test: 0.3767\n",
            "Early stopping:  0.047679841925601964\n",
            "Epoch: 041, Loss: 1.8738, Train: 0.4053, Test: 0.3808\n",
            "Early stopping:  0.04929026775301175\n",
            "Epoch: 042, Loss: 1.8574, Train: 0.4092, Test: 0.3869\n",
            "Early stopping:  0.04705121156823349\n",
            "Epoch: 043, Loss: 1.8326, Train: 0.4161, Test: 0.3897\n",
            "Early stopping:  0.04043068260795618\n",
            "Epoch: 044, Loss: 1.8108, Train: 0.4161, Test: 0.3918\n",
            "Early stopping:  0.035243230652293164\n",
            "Epoch: 045, Loss: 1.7938, Train: 0.4189, Test: 0.3920\n",
            "Early stopping:  0.03274745939099531\n",
            "Epoch: 046, Loss: 1.7758, Train: 0.4226, Test: 0.3961\n",
            "Early stopping:  0.032043457685548456\n",
            "Epoch: 047, Loss: 1.7608, Train: 0.4280, Test: 0.4020\n",
            "Early stopping:  0.0282674509604255\n",
            "Epoch: 048, Loss: 1.7427, Train: 0.4351, Test: 0.4045\n",
            "Early stopping:  0.026758439425984305\n",
            "Epoch: 049, Loss: 1.7270, Train: 0.4325, Test: 0.4081\n",
            "Early stopping:  0.02638156270782882\n",
            "Epoch: 050, Loss: 1.7167, Train: 0.4353, Test: 0.4077\n",
            "Early stopping:  0.02414627217218102\n",
            "Epoch: 051, Loss: 1.7017, Train: 0.4365, Test: 0.4111\n",
            "Early stopping:  0.022895210932147435\n",
            "Epoch: 052, Loss: 1.6883, Train: 0.4407, Test: 0.4139\n",
            "Early stopping:  0.021223664035741517\n",
            "Epoch: 053, Loss: 1.6738, Train: 0.4433, Test: 0.4143\n",
            "Early stopping:  0.02134804824151514\n",
            "Epoch: 054, Loss: 1.6601, Train: 0.4509, Test: 0.4200\n",
            "Early stopping:  0.02232577130825591\n",
            "Epoch: 055, Loss: 1.6468, Train: 0.4563, Test: 0.4232\n",
            "Early stopping:  0.02183695416292974\n",
            "Epoch: 056, Loss: 1.6310, Train: 0.4620, Test: 0.4218\n",
            "Early stopping:  0.022377763971719856\n",
            "Epoch: 057, Loss: 1.6183, Train: 0.4617, Test: 0.4241\n",
            "Early stopping:  0.02214412228139807\n",
            "Epoch: 058, Loss: 1.6046, Train: 0.4674, Test: 0.4273\n",
            "Early stopping:  0.022056695263508872\n",
            "Epoch: 059, Loss: 1.5898, Train: 0.4762, Test: 0.4305\n",
            "Early stopping:  0.022218725541947294\n",
            "Epoch: 060, Loss: 1.5762, Train: 0.4787, Test: 0.4343\n",
            "Early stopping:  0.0218713854980832\n",
            "Epoch: 061, Loss: 1.5654, Train: 0.4804, Test: 0.4349\n",
            "Early stopping:  0.021266053002504223\n",
            "Epoch: 062, Loss: 1.5530, Train: 0.4864, Test: 0.4428\n",
            "Early stopping:  0.02020010895564692\n",
            "Epoch: 063, Loss: 1.5407, Train: 0.4895, Test: 0.4474\n",
            "Early stopping:  0.019185947852085238\n",
            "Epoch: 064, Loss: 1.5272, Train: 0.4940, Test: 0.4506\n",
            "Early stopping:  0.01939242522141433\n",
            "Epoch: 065, Loss: 1.5170, Train: 0.5009, Test: 0.4589\n",
            "Early stopping:  0.019402443850143593\n",
            "Epoch: 066, Loss: 1.5051, Train: 0.5088, Test: 0.4612\n",
            "Early stopping:  0.018926960214097864\n",
            "Epoch: 067, Loss: 1.4940, Train: 0.5122, Test: 0.4610\n",
            "Early stopping:  0.018270693138705635\n",
            "Epoch: 068, Loss: 1.4839, Train: 0.5170, Test: 0.4642\n",
            "Early stopping:  0.01731923631952214\n",
            "Epoch: 069, Loss: 1.4747, Train: 0.5213, Test: 0.4649\n",
            "Early stopping:  0.01675316624668472\n",
            "Epoch: 070, Loss: 1.4654, Train: 0.5227, Test: 0.4716\n",
            "Early stopping:  0.01563120074564486\n",
            "Epoch: 071, Loss: 1.4562, Train: 0.5278, Test: 0.4716\n",
            "Early stopping:  0.014901132651683175\n",
            "Epoch: 072, Loss: 1.4482, Train: 0.5298, Test: 0.4706\n",
            "Early stopping:  0.014220031232179077\n",
            "Epoch: 073, Loss: 1.4387, Train: 0.5335, Test: 0.4763\n",
            "Early stopping:  0.014105690050282034\n",
            "Epoch: 074, Loss: 1.4296, Train: 0.5369, Test: 0.4759\n",
            "Early stopping:  0.014108586505356568\n",
            "Epoch: 075, Loss: 1.4213, Train: 0.5389, Test: 0.4785\n",
            "Early stopping:  0.013976195720497886\n",
            "Epoch: 076, Loss: 1.4121, Train: 0.5417, Test: 0.4767\n",
            "Early stopping:  0.014174638773323981\n",
            "Epoch: 077, Loss: 1.4039, Train: 0.5440, Test: 0.4804\n",
            "Early stopping:  0.013775665801409358\n",
            "Epoch: 078, Loss: 1.3966, Train: 0.5437, Test: 0.4804\n",
            "Early stopping:  0.013190092255637022\n",
            "Epoch: 079, Loss: 1.3885, Train: 0.5428, Test: 0.4855\n",
            "Early stopping:  0.012827123464413599\n",
            "Epoch: 080, Loss: 1.3797, Train: 0.5465, Test: 0.4833\n",
            "Early stopping:  0.012676738016070778\n",
            "Epoch: 081, Loss: 1.3716, Train: 0.5491, Test: 0.4852\n",
            "Early stopping:  0.012874016364724997\n",
            "Epoch: 082, Loss: 1.3649, Train: 0.5528, Test: 0.4893\n",
            "Early stopping:  0.012723606788898492\n",
            "Epoch: 083, Loss: 1.3584, Train: 0.5528, Test: 0.4855\n",
            "Early stopping:  0.011885597284137425\n",
            "Epoch: 084, Loss: 1.3521, Train: 0.5584, Test: 0.4884\n",
            "Early stopping:  0.010834811795896094\n",
            "Epoch: 085, Loss: 1.3439, Train: 0.5601, Test: 0.4888\n",
            "Early stopping:  0.010807641747082418\n",
            "Epoch: 086, Loss: 1.3380, Train: 0.5610, Test: 0.4850\n",
            "Early stopping:  0.010818299400889424\n",
            "Epoch: 087, Loss: 1.3355, Train: 0.5618, Test: 0.4923\n",
            "Early stopping:  0.009601615529361123\n",
            "PREDICTIONS -> tensor([0, 0, 0,  ..., 0, 0, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.36      0.54      0.43       568\n",
            "         capital_goods       0.29      0.21      0.25       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.47      0.31      0.37       595\n",
            " consumer_non-cyclical       0.54      0.33      0.41       334\n",
            "                energy       0.73      0.14      0.24       213\n",
            "             financial       0.62      0.66      0.64       576\n",
            "            healthcare       0.60      0.35      0.44       238\n",
            "              services       0.50      0.75      0.60      1557\n",
            "            technology       0.35      0.15      0.21       297\n",
            "        transportation       0.66      0.62      0.64       303\n",
            "             utilities       0.74      0.18      0.29       169\n",
            "\n",
            "              accuracy                           0.49      5291\n",
            "             macro avg       0.49      0.35      0.38      5291\n",
            "          weighted avg       0.50      0.49      0.47      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 52.9752, Train: 0.1390, Test: 0.1389\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 607.4414, Train: 0.0343, Test: 0.0370\n",
            "Early stopping:  392.06684358774066\n",
            "Epoch: 003, Loss: 410.4365, Train: 0.1140, Test: 0.1123\n",
            "Early stopping:  281.07601321613134\n",
            "Epoch: 004, Loss: 264.9410, Train: 0.0766, Test: 0.0758\n",
            "Early stopping:  234.0632639848149\n",
            "Epoch: 005, Loss: 180.6722, Train: 0.0794, Test: 0.0739\n",
            "Early stopping:  213.98114650451728\n",
            "Epoch: 006, Loss: 125.0816, Train: 0.2124, Test: 0.2119\n",
            "Early stopping:  194.43000615076255\n",
            "Epoch: 007, Loss: 80.7760, Train: 0.1441, Test: 0.1486\n",
            "Early stopping:  130.3217473073717\n",
            "Epoch: 008, Loss: 77.2056, Train: 0.2323, Test: 0.2461\n",
            "Early stopping:  78.69751208442399\n",
            "Epoch: 009, Loss: 59.9313, Train: 0.2535, Test: 0.2663\n",
            "Early stopping:  48.78454151869494\n",
            "Epoch: 010, Loss: 40.9503, Train: 0.1946, Test: 0.2019\n",
            "Early stopping:  31.29231624355598\n",
            "Epoch: 011, Loss: 27.2683, Train: 0.3035, Test: 0.3115\n",
            "Early stopping:  23.04053413041319\n",
            "Epoch: 012, Loss: 11.9000, Train: 0.3449, Test: 0.3340\n",
            "Early stopping:  25.85811080880739\n",
            "Epoch: 013, Loss: 7.4011, Train: 0.3091, Test: 0.2916\n",
            "Early stopping:  21.565437985349483\n",
            "Epoch: 014, Loss: 5.3790, Train: 0.3128, Test: 0.2912\n",
            "Early stopping:  15.163894883262994\n",
            "Epoch: 015, Loss: 4.2539, Train: 0.2998, Test: 0.2905\n",
            "Early stopping:  9.42386396774597\n",
            "Epoch: 016, Loss: 3.7215, Train: 0.2921, Test: 0.2771\n",
            "Early stopping:  3.3162285488146526\n",
            "Epoch: 017, Loss: 3.1639, Train: 0.2992, Test: 0.2909\n",
            "Early stopping:  1.676162646611486\n",
            "Epoch: 018, Loss: 2.8016, Train: 0.3318, Test: 0.3192\n",
            "Early stopping:  1.0107498867610563\n",
            "Epoch: 019, Loss: 2.4934, Train: 0.3440, Test: 0.3417\n",
            "Early stopping:  0.7078463757228871\n",
            "Epoch: 020, Loss: 2.3165, Train: 0.3607, Test: 0.3500\n",
            "Early stopping:  0.5612402556990631\n",
            "Epoch: 021, Loss: 2.2271, Train: 0.3565, Test: 0.3461\n",
            "Early stopping:  0.3838472015257231\n",
            "Epoch: 022, Loss: 2.1834, Train: 0.3545, Test: 0.3408\n",
            "Early stopping:  0.2518380375872311\n",
            "Epoch: 023, Loss: 2.1567, Train: 0.3423, Test: 0.3385\n",
            "Early stopping:  0.13609514757188318\n",
            "Epoch: 024, Loss: 2.1420, Train: 0.3415, Test: 0.3342\n",
            "Early stopping:  0.07014868224354492\n",
            "Epoch: 025, Loss: 2.1410, Train: 0.3341, Test: 0.3171\n",
            "Early stopping:  0.03621714290599908\n",
            "Epoch: 026, Loss: 2.1499, Train: 0.3239, Test: 0.3022\n",
            "Early stopping:  0.01731642962874723\n",
            "Epoch: 027, Loss: 2.1549, Train: 0.3171, Test: 0.2933\n",
            "Early stopping:  0.007223088752363723\n",
            "PREDICTIONS -> tensor([9, 8, 3,  ..., 2, 2, 3], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.36      0.15      0.21       568\n",
            "         capital_goods       0.00      0.00      0.00       381\n",
            "conglomerates_industry       0.01      0.13      0.03        60\n",
            "     consumer_cyclical       0.20      0.64      0.31       595\n",
            " consumer_non-cyclical       0.51      0.18      0.27       334\n",
            "                energy       0.04      0.01      0.02       213\n",
            "             financial       0.45      0.23      0.30       576\n",
            "            healthcare       0.29      0.35      0.31       238\n",
            "              services       0.54      0.36      0.43      1557\n",
            "            technology       0.36      0.11      0.17       297\n",
            "        transportation       0.29      0.70      0.41       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.29      5291\n",
            "             macro avg       0.25      0.24      0.20      5291\n",
            "          weighted avg       0.35      0.29      0.28      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 70.9828, Train: 0.2933, Test: 0.2930\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 382.6440, Train: 0.0562, Test: 0.0556\n",
            "Early stopping:  220.37775978676268\n",
            "Epoch: 003, Loss: 476.3311, Train: 0.1262, Test: 0.1310\n",
            "Early stopping:  212.21734825830603\n",
            "Epoch: 004, Loss: 344.2757, Train: 0.1217, Test: 0.1145\n",
            "Early stopping:  174.12088389961139\n",
            "Epoch: 005, Loss: 264.5404, Train: 0.1029, Test: 0.0979\n",
            "Early stopping:  152.71591657094217\n",
            "Epoch: 006, Loss: 223.8443, Train: 0.1154, Test: 0.1153\n",
            "Early stopping:  99.49281101239497\n",
            "Epoch: 007, Loss: 198.4336, Train: 0.0596, Test: 0.0575\n",
            "Early stopping:  112.26618951797805\n",
            "Epoch: 008, Loss: 156.9998, Train: 0.1183, Test: 0.1196\n",
            "Early stopping:  71.28309299416581\n",
            "Epoch: 009, Loss: 168.3419, Train: 0.2771, Test: 0.2822\n",
            "Early stopping:  43.468787578171266\n",
            "Epoch: 010, Loss: 182.3115, Train: 0.2677, Test: 0.2693\n",
            "Early stopping:  26.23549615330214\n",
            "Epoch: 011, Loss: 165.6746, Train: 0.1237, Test: 0.1278\n",
            "Early stopping:  16.246494167720456\n",
            "Epoch: 012, Loss: 134.7512, Train: 0.2331, Test: 0.2306\n",
            "Early stopping:  17.557336046170864\n",
            "Epoch: 013, Loss: 92.8969, Train: 0.2604, Test: 0.2580\n",
            "Early stopping:  35.74977366737202\n",
            "Epoch: 014, Loss: 59.6298, Train: 0.1551, Test: 0.1450\n",
            "Early stopping:  50.78612471138398\n",
            "Epoch: 015, Loss: 38.8146, Train: 0.1636, Test: 0.1563\n",
            "Early stopping:  52.27594109009864\n",
            "Epoch: 016, Loss: 22.4388, Train: 0.2269, Test: 0.2215\n",
            "Early stopping:  44.88511545992528\n",
            "Epoch: 017, Loss: 9.9799, Train: 0.1597, Test: 0.1525\n",
            "Early stopping:  32.716635719904666\n",
            "Epoch: 018, Loss: 7.8561, Train: 0.1758, Test: 0.1648\n",
            "Early stopping:  21.6708369908069\n",
            "Epoch: 019, Loss: 6.4930, Train: 0.1982, Test: 0.1915\n",
            "Early stopping:  13.681021645177463\n",
            "Epoch: 020, Loss: 5.0465, Train: 0.2388, Test: 0.2294\n",
            "Early stopping:  6.991039115255134\n",
            "Epoch: 021, Loss: 4.0349, Train: 0.2567, Test: 0.2434\n",
            "Early stopping:  2.343726700375396\n",
            "Epoch: 022, Loss: 3.3396, Train: 0.2609, Test: 0.2451\n",
            "Early stopping:  1.8337399408561004\n",
            "Epoch: 023, Loss: 2.8728, Train: 0.2561, Test: 0.2466\n",
            "Early stopping:  1.4473758001575752\n",
            "Epoch: 024, Loss: 2.6039, Train: 0.2490, Test: 0.2402\n",
            "Early stopping:  0.9833828314603604\n",
            "Epoch: 025, Loss: 2.4465, Train: 0.2510, Test: 0.2402\n",
            "Early stopping:  0.6418856478215709\n",
            "Epoch: 026, Loss: 2.3502, Train: 0.2541, Test: 0.2363\n",
            "Early stopping:  0.39756077576850135\n",
            "Epoch: 027, Loss: 2.3009, Train: 0.2518, Test: 0.2332\n",
            "Early stopping:  0.23108006827663122\n",
            "Epoch: 028, Loss: 2.2818, Train: 0.2362, Test: 0.2226\n",
            "Early stopping:  0.13223825583403226\n",
            "Epoch: 029, Loss: 2.2754, Train: 0.2320, Test: 0.2168\n",
            "Early stopping:  0.07092341566991406\n",
            "Epoch: 030, Loss: 2.2720, Train: 0.2326, Test: 0.2158\n",
            "Early stopping:  0.03230021268644951\n",
            "Epoch: 031, Loss: 2.2663, Train: 0.2303, Test: 0.2164\n",
            "Early stopping:  0.01335368768967973\n",
            "Epoch: 032, Loss: 2.2621, Train: 0.2294, Test: 0.2128\n",
            "Early stopping:  0.007682385813462539\n",
            "PREDICTIONS -> tensor([9, 0, 9,  ..., 0, 0, 0], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.14      0.93      0.24       568\n",
            "         capital_goods       0.32      0.13      0.18       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.24      0.06      0.09       595\n",
            " consumer_non-cyclical       0.43      0.24      0.31       334\n",
            "                energy       0.08      0.00      0.01       213\n",
            "             financial       0.49      0.35      0.40       576\n",
            "            healthcare       0.51      0.37      0.43       238\n",
            "              services       0.39      0.06      0.10      1557\n",
            "            technology       0.49      0.15      0.23       297\n",
            "        transportation       0.00      0.00      0.00       303\n",
            "             utilities       0.41      0.07      0.12       169\n",
            "\n",
            "              accuracy                           0.21      5291\n",
            "             macro avg       0.29      0.20      0.18      5291\n",
            "          weighted avg       0.33      0.21      0.18      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 93.8150, Train: 0.0403, Test: 0.0391\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 408.6934, Train: 0.2944, Test: 0.2930\n",
            "Early stopping:  222.65259446263923\n",
            "Epoch: 003, Loss: 256.0997, Train: 0.1242, Test: 0.1208\n",
            "Early stopping:  157.46401240795186\n",
            "Epoch: 004, Loss: 360.5785, Train: 0.0624, Test: 0.0614\n",
            "Early stopping:  139.39246383543093\n",
            "Epoch: 005, Loss: 385.1813, Train: 0.0737, Test: 0.0669\n",
            "Early stopping:  129.59119989883558\n",
            "Epoch: 006, Loss: 337.2433, Train: 0.0780, Test: 0.0752\n",
            "Early stopping:  58.68042490692413\n",
            "Epoch: 007, Loss: 346.4066, Train: 0.0786, Test: 0.0762\n",
            "Early stopping:  48.76149699856988\n",
            "Epoch: 008, Loss: 300.3684, Train: 0.0357, Test: 0.0370\n",
            "Early stopping:  31.25215567918379\n",
            "Epoch: 009, Loss: 257.8503, Train: 0.0754, Test: 0.0694\n",
            "Early stopping:  48.33447483587106\n",
            "Epoch: 010, Loss: 148.4548, Train: 0.3043, Test: 0.2947\n",
            "Early stopping:  80.43244931901572\n",
            "Epoch: 011, Loss: 83.8882, Train: 0.2876, Test: 0.2850\n",
            "Early stopping:  108.67629386310973\n",
            "Epoch: 012, Loss: 67.6473, Train: 0.2794, Test: 0.2805\n",
            "Early stopping:  103.74923812386137\n",
            "Epoch: 013, Loss: 50.5683, Train: 0.2612, Test: 0.2601\n",
            "Early stopping:  84.6520342027915\n",
            "Epoch: 014, Loss: 33.8155, Train: 0.1946, Test: 0.1949\n",
            "Early stopping:  44.17059228482298\n",
            "Epoch: 015, Loss: 20.7139, Train: 0.2802, Test: 0.2771\n",
            "Early stopping:  25.352397572284037\n",
            "Epoch: 016, Loss: 8.4294, Train: 0.2938, Test: 0.2912\n",
            "Early stopping:  23.51643035738132\n",
            "Epoch: 017, Loss: 5.1307, Train: 0.2870, Test: 0.2746\n",
            "Early stopping:  18.782660677599495\n",
            "Epoch: 018, Loss: 3.9984, Train: 0.2618, Test: 0.2508\n",
            "Early stopping:  12.714051495246444\n",
            "Epoch: 019, Loss: 3.5870, Train: 0.2243, Test: 0.2192\n",
            "Early stopping:  7.156406270007029\n",
            "Epoch: 020, Loss: 3.3512, Train: 0.1906, Test: 0.1797\n",
            "Early stopping:  2.088383777674324\n",
            "Epoch: 021, Loss: 3.1026, Train: 0.2002, Test: 0.1899\n",
            "Early stopping:  0.796481863442575\n",
            "Epoch: 022, Loss: 2.7800, Train: 0.2187, Test: 0.2158\n",
            "Early stopping:  0.4641375582728779\n",
            "Epoch: 023, Loss: 2.4842, Train: 0.2425, Test: 0.2328\n",
            "Early stopping:  0.43993531982684503\n",
            "Epoch: 024, Loss: 2.2851, Train: 0.2762, Test: 0.2536\n",
            "Early stopping:  0.43606275284300594\n",
            "Epoch: 025, Loss: 2.1839, Train: 0.2836, Test: 0.2776\n",
            "Early stopping:  0.3759533749809902\n",
            "Epoch: 026, Loss: 2.1856, Train: 0.3063, Test: 0.3015\n",
            "Early stopping:  0.2530435973056815\n",
            "Epoch: 027, Loss: 2.2135, Train: 0.3046, Test: 0.3005\n",
            "Early stopping:  0.1263512299411034\n",
            "Epoch: 028, Loss: 2.2073, Train: 0.2881, Test: 0.2914\n",
            "Early stopping:  0.0412558697396112\n",
            "Epoch: 029, Loss: 2.2003, Train: 0.2845, Test: 0.2892\n",
            "Early stopping:  0.01311622299720351\n",
            "Epoch: 030, Loss: 2.1964, Train: 0.2913, Test: 0.2952\n",
            "Early stopping:  0.010672764641438367\n",
            "Epoch: 031, Loss: 2.1871, Train: 0.2992, Test: 0.3043\n",
            "Early stopping:  0.010139971800488845\n",
            "Epoch: 032, Loss: 2.1725, Train: 0.3103, Test: 0.3139\n",
            "Early stopping:  0.01347561905209749\n",
            "Epoch: 033, Loss: 2.1554, Train: 0.3148, Test: 0.3187\n",
            "Early stopping:  0.01850584077474029\n",
            "Epoch: 034, Loss: 2.1453, Train: 0.3202, Test: 0.3209\n",
            "Early stopping:  0.02127259444292859\n",
            "Epoch: 035, Loss: 2.1437, Train: 0.3242, Test: 0.3221\n",
            "Early stopping:  0.018619755985329556\n",
            "Epoch: 036, Loss: 2.1265, Train: 0.3199, Test: 0.3219\n",
            "Early stopping:  0.016893246824171617\n",
            "Epoch: 037, Loss: 2.1134, Train: 0.3182, Test: 0.3175\n",
            "Early stopping:  0.016732284258725583\n",
            "Epoch: 038, Loss: 2.1071, Train: 0.3276, Test: 0.3221\n",
            "Early stopping:  0.01729316938307475\n",
            "Epoch: 039, Loss: 2.0953, Train: 0.3392, Test: 0.3340\n",
            "Early stopping:  0.018613732458259178\n",
            "Epoch: 040, Loss: 2.0778, Train: 0.3454, Test: 0.3408\n",
            "Early stopping:  0.018466846341549915\n",
            "Epoch: 041, Loss: 2.0636, Train: 0.3486, Test: 0.3444\n",
            "Early stopping:  0.020662009142341953\n",
            "Epoch: 042, Loss: 2.0535, Train: 0.3548, Test: 0.3491\n",
            "Early stopping:  0.02204903732117286\n",
            "Epoch: 043, Loss: 2.0362, Train: 0.3605, Test: 0.3495\n",
            "Early stopping:  0.022621998247147942\n",
            "Epoch: 044, Loss: 2.0190, Train: 0.3573, Test: 0.3428\n",
            "Early stopping:  0.02303297326595415\n",
            "Epoch: 045, Loss: 2.0087, Train: 0.3565, Test: 0.3404\n",
            "Early stopping:  0.02291022419882608\n",
            "Epoch: 046, Loss: 1.9964, Train: 0.3588, Test: 0.3428\n",
            "Early stopping:  0.02253963957489788\n",
            "Epoch: 047, Loss: 1.9817, Train: 0.3588, Test: 0.3444\n",
            "Early stopping:  0.020861648285615108\n",
            "Epoch: 048, Loss: 1.9700, Train: 0.3593, Test: 0.3453\n",
            "Early stopping:  0.01980478343533619\n",
            "Epoch: 049, Loss: 1.9626, Train: 0.3573, Test: 0.3438\n",
            "Early stopping:  0.01887708917819837\n",
            "Epoch: 050, Loss: 1.9496, Train: 0.3576, Test: 0.3432\n",
            "Early stopping:  0.017922656099175507\n",
            "Epoch: 051, Loss: 1.9390, Train: 0.3568, Test: 0.3438\n",
            "Early stopping:  0.016777130062626648\n",
            "Epoch: 052, Loss: 1.9301, Train: 0.3596, Test: 0.3444\n",
            "Early stopping:  0.016393598280703508\n",
            "Epoch: 053, Loss: 1.9188, Train: 0.3605, Test: 0.3453\n",
            "Early stopping:  0.016960289087649086\n",
            "Epoch: 054, Loss: 1.9060, Train: 0.3639, Test: 0.3487\n",
            "Early stopping:  0.017008808825946103\n",
            "Epoch: 055, Loss: 1.8943, Train: 0.3656, Test: 0.3512\n",
            "Early stopping:  0.017981595264142943\n",
            "Epoch: 056, Loss: 1.8833, Train: 0.3650, Test: 0.3551\n",
            "Early stopping:  0.018686925949837726\n",
            "Epoch: 057, Loss: 1.8705, Train: 0.3681, Test: 0.3557\n",
            "Early stopping:  0.018856060074764282\n",
            "Epoch: 058, Loss: 1.8594, Train: 0.3712, Test: 0.3565\n",
            "Early stopping:  0.018496558973761568\n",
            "Epoch: 059, Loss: 1.8497, Train: 0.3741, Test: 0.3608\n",
            "Early stopping:  0.017884046434845594\n",
            "Epoch: 060, Loss: 1.8374, Train: 0.3792, Test: 0.3616\n",
            "Early stopping:  0.01780998791747691\n",
            "Epoch: 061, Loss: 1.8275, Train: 0.3806, Test: 0.3627\n",
            "Early stopping:  0.017076060779688176\n",
            "Epoch: 062, Loss: 1.8169, Train: 0.3868, Test: 0.3651\n",
            "Early stopping:  0.016953297588132753\n",
            "Epoch: 063, Loss: 1.8061, Train: 0.3900, Test: 0.3646\n",
            "Early stopping:  0.017047476565944412\n",
            "Epoch: 064, Loss: 1.7968, Train: 0.3942, Test: 0.3680\n",
            "Early stopping:  0.016240464429191668\n",
            "Epoch: 065, Loss: 1.7871, Train: 0.3959, Test: 0.3716\n",
            "Early stopping:  0.015982745542935387\n",
            "Epoch: 066, Loss: 1.7793, Train: 0.3959, Test: 0.3708\n",
            "Early stopping:  0.014911976836536465\n",
            "Epoch: 067, Loss: 1.7703, Train: 0.3985, Test: 0.3733\n",
            "Early stopping:  0.014083195941999738\n",
            "Epoch: 068, Loss: 1.7633, Train: 0.4005, Test: 0.3776\n",
            "Early stopping:  0.013247240897464072\n",
            "Epoch: 069, Loss: 1.7556, Train: 0.4058, Test: 0.3799\n",
            "Early stopping:  0.01249142170310206\n",
            "Epoch: 070, Loss: 1.7491, Train: 0.4107, Test: 0.3837\n",
            "Early stopping:  0.011900655405260556\n",
            "Epoch: 071, Loss: 1.7411, Train: 0.4132, Test: 0.3850\n",
            "Early stopping:  0.011492057801710446\n",
            "Epoch: 072, Loss: 1.7347, Train: 0.4158, Test: 0.3857\n",
            "Early stopping:  0.011365222864100365\n",
            "Epoch: 073, Loss: 1.7269, Train: 0.4223, Test: 0.3893\n",
            "Early stopping:  0.011363598530447402\n",
            "Epoch: 074, Loss: 1.7200, Train: 0.4243, Test: 0.3937\n",
            "Early stopping:  0.011472142114454537\n",
            "Epoch: 075, Loss: 1.7126, Train: 0.4277, Test: 0.3982\n",
            "Early stopping:  0.011350202972306864\n",
            "Epoch: 076, Loss: 1.7051, Train: 0.4294, Test: 0.3995\n",
            "Early stopping:  0.011618154829833804\n",
            "Epoch: 077, Loss: 1.6977, Train: 0.4336, Test: 0.4005\n",
            "Early stopping:  0.011594891668083012\n",
            "Epoch: 078, Loss: 1.6896, Train: 0.4342, Test: 0.4028\n",
            "Early stopping:  0.011942903727467886\n",
            "Epoch: 079, Loss: 1.6821, Train: 0.4385, Test: 0.4054\n",
            "Early stopping:  0.012078670774412225\n",
            "Epoch: 080, Loss: 1.6732, Train: 0.4433, Test: 0.4115\n",
            "Early stopping:  0.012536638573980144\n",
            "Epoch: 081, Loss: 1.6637, Train: 0.4509, Test: 0.4200\n",
            "Early stopping:  0.013360500470085145\n",
            "Epoch: 082, Loss: 1.6578, Train: 0.4603, Test: 0.4241\n",
            "Early stopping:  0.01301698217281673\n",
            "Epoch: 083, Loss: 1.6479, Train: 0.4663, Test: 0.4275\n",
            "Early stopping:  0.01328751921735879\n",
            "Epoch: 084, Loss: 1.6383, Train: 0.4722, Test: 0.4319\n",
            "Early stopping:  0.013597287416596819\n",
            "Epoch: 085, Loss: 1.6303, Train: 0.4765, Test: 0.4373\n",
            "Early stopping:  0.013691119921111036\n",
            "Epoch: 086, Loss: 1.6214, Train: 0.4787, Test: 0.4406\n",
            "Early stopping:  0.014318740604498122\n",
            "Epoch: 087, Loss: 1.6117, Train: 0.4824, Test: 0.4445\n",
            "Early stopping:  0.014135563962139015\n",
            "Epoch: 088, Loss: 1.6000, Train: 0.4855, Test: 0.4466\n",
            "Early stopping:  0.015070779776623463\n",
            "Epoch: 089, Loss: 1.5912, Train: 0.4895, Test: 0.4510\n",
            "Early stopping:  0.015749749344571688\n",
            "Epoch: 090, Loss: 1.5775, Train: 0.4949, Test: 0.4525\n",
            "Early stopping:  0.017144264629902123\n",
            "Epoch: 091, Loss: 1.5643, Train: 0.4952, Test: 0.4572\n",
            "Early stopping:  0.018583834787041904\n",
            "Epoch: 092, Loss: 1.5505, Train: 0.5011, Test: 0.4602\n",
            "Early stopping:  0.01996702171347476\n",
            "Epoch: 093, Loss: 1.5401, Train: 0.5048, Test: 0.4612\n",
            "Early stopping:  0.020448271459833824\n",
            "Epoch: 094, Loss: 1.5289, Train: 0.5054, Test: 0.4615\n",
            "Early stopping:  0.019242920870290326\n",
            "Epoch: 095, Loss: 1.5186, Train: 0.5065, Test: 0.4632\n",
            "Early stopping:  0.017923849893904695\n",
            "Epoch: 096, Loss: 1.5078, Train: 0.5108, Test: 0.4661\n",
            "Early stopping:  0.016897281252003692\n",
            "Epoch: 097, Loss: 1.4975, Train: 0.5113, Test: 0.4685\n",
            "Early stopping:  0.016813053962036268\n",
            "Epoch: 098, Loss: 1.4875, Train: 0.5142, Test: 0.4695\n",
            "Early stopping:  0.016416319332456002\n",
            "Epoch: 099, Loss: 1.4776, Train: 0.5153, Test: 0.4717\n",
            "Early stopping:  0.016175418973634993\n",
            "Epoch: 100, Loss: 1.4688, Train: 0.5182, Test: 0.4746\n",
            "Early stopping:  0.015496220634143128\n",
            "Epoch: 101, Loss: 1.4587, Train: 0.5210, Test: 0.4757\n",
            "Early stopping:  0.015228983208764146\n",
            "Epoch: 102, Loss: 1.4514, Train: 0.5261, Test: 0.4780\n",
            "Early stopping:  0.014416313748867574\n",
            "Epoch: 103, Loss: 1.4416, Train: 0.5278, Test: 0.4785\n",
            "Early stopping:  0.014130010618860857\n",
            "Epoch: 104, Loss: 1.4322, Train: 0.5267, Test: 0.4797\n",
            "Early stopping:  0.014273171840113992\n",
            "Epoch: 105, Loss: 1.4305, Train: 0.5329, Test: 0.4765\n",
            "Early stopping:  0.01214709364075103\n",
            "Epoch: 106, Loss: 1.4313, Train: 0.5363, Test: 0.4782\n",
            "Early stopping:  0.009031873979008344\n",
            "PREDICTIONS -> tensor([8, 6, 0,  ..., 5, 5, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.36      0.44      0.39       568\n",
            "         capital_goods       0.11      0.00      0.01       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.35      0.31      0.33       595\n",
            " consumer_non-cyclical       0.73      0.34      0.46       334\n",
            "                energy       0.52      0.56      0.54       213\n",
            "             financial       0.56      0.60      0.58       576\n",
            "            healthcare       0.46      0.47      0.47       238\n",
            "              services       0.49      0.80      0.61      1557\n",
            "            technology       0.45      0.04      0.08       297\n",
            "        transportation       0.68      0.42      0.52       303\n",
            "             utilities       0.38      0.14      0.21       169\n",
            "\n",
            "              accuracy                           0.48      5291\n",
            "             macro avg       0.42      0.34      0.35      5291\n",
            "          weighted avg       0.46      0.48      0.43      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 108.7597, Train: 0.0360, Test: 0.0363\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 290.8289, Train: 0.3230, Test: 0.3211\n",
            "Early stopping:  128.74232182282597\n",
            "Epoch: 003, Loss: 192.6302, Train: 0.1375, Test: 0.1376\n",
            "Early stopping:  91.1284836664838\n",
            "Epoch: 004, Loss: 214.6012, Train: 0.1132, Test: 0.1115\n",
            "Early stopping:  74.90115982383372\n",
            "Epoch: 005, Loss: 290.0387, Train: 0.0678, Test: 0.0650\n",
            "Early stopping:  75.94869653324481\n",
            "Epoch: 006, Loss: 242.1193, Train: 0.1168, Test: 0.1108\n",
            "Early stopping:  44.153972000910315\n",
            "Epoch: 007, Loss: 242.7986, Train: 0.1319, Test: 0.1270\n",
            "Early stopping:  36.543567386446476\n",
            "Epoch: 008, Loss: 211.1543, Train: 0.1540, Test: 0.1535\n",
            "Early stopping:  31.59612024574475\n",
            "Epoch: 009, Loss: 169.5418, Train: 0.2691, Test: 0.2652\n",
            "Early stopping:  44.49518189917878\n",
            "Epoch: 010, Loss: 168.9472, Train: 0.2700, Test: 0.2616\n",
            "Early stopping:  36.68535209689423\n",
            "Epoch: 011, Loss: 153.1292, Train: 0.2258, Test: 0.2245\n",
            "Early stopping:  36.92050531027509\n",
            "Epoch: 012, Loss: 124.2116, Train: 0.2518, Test: 0.2485\n",
            "Early stopping:  31.50497935483476\n",
            "Epoch: 013, Loss: 90.9724, Train: 0.1523, Test: 0.1487\n",
            "Early stopping:  33.64058598746017\n",
            "Epoch: 014, Loss: 78.3694, Train: 0.1835, Test: 0.1799\n",
            "Early stopping:  38.8521371265872\n",
            "Epoch: 015, Loss: 53.3371, Train: 0.2056, Test: 0.1964\n",
            "Early stopping:  39.165409671331275\n",
            "Epoch: 016, Loss: 35.9612, Train: 0.2881, Test: 0.2824\n",
            "Early stopping:  34.16589378923345\n",
            "Epoch: 017, Loss: 17.1923, Train: 0.3520, Test: 0.3445\n",
            "Early stopping:  30.14410008306627\n",
            "Epoch: 018, Loss: 8.6310, Train: 0.3525, Test: 0.3381\n",
            "Early stopping:  28.11641790196752\n",
            "Epoch: 019, Loss: 5.1905, Train: 0.3457, Test: 0.3338\n",
            "Early stopping:  20.248848161857776\n",
            "Epoch: 020, Loss: 3.8296, Train: 0.3440, Test: 0.3298\n",
            "Early stopping:  13.249784613297866\n",
            "Epoch: 021, Loss: 3.1406, Train: 0.3213, Test: 0.3075\n",
            "Early stopping:  5.7659359901938885\n",
            "Epoch: 022, Loss: 2.8185, Train: 0.2813, Test: 0.2688\n",
            "Early stopping:  2.367517532091844\n",
            "Epoch: 023, Loss: 2.7338, Train: 0.2643, Test: 0.2446\n",
            "Early stopping:  1.017107624229194\n",
            "Epoch: 024, Loss: 2.6795, Train: 0.2516, Test: 0.2347\n",
            "Early stopping:  0.4760209272730723\n",
            "Epoch: 025, Loss: 2.5927, Train: 0.2518, Test: 0.2406\n",
            "Early stopping:  0.21093239284529874\n",
            "Epoch: 026, Loss: 2.4796, Train: 0.2652, Test: 0.2506\n",
            "Early stopping:  0.13039692231359648\n",
            "Epoch: 027, Loss: 2.3707, Train: 0.2836, Test: 0.2684\n",
            "Early stopping:  0.14766252546650532\n",
            "Epoch: 028, Loss: 2.3016, Train: 0.2907, Test: 0.2784\n",
            "Early stopping:  0.1550402017789107\n",
            "Epoch: 029, Loss: 2.2572, Train: 0.3001, Test: 0.2863\n",
            "Early stopping:  0.13637441140134976\n",
            "Epoch: 030, Loss: 2.2346, Train: 0.3086, Test: 0.2922\n",
            "Early stopping:  0.09903063950559984\n",
            "Epoch: 031, Loss: 2.2253, Train: 0.3162, Test: 0.2988\n",
            "Early stopping:  0.0596711158546963\n",
            "Epoch: 032, Loss: 2.2173, Train: 0.3264, Test: 0.3067\n",
            "Early stopping:  0.03388197623854419\n",
            "Epoch: 033, Loss: 2.2104, Train: 0.3327, Test: 0.3122\n",
            "Early stopping:  0.018212100746049037\n",
            "Epoch: 034, Loss: 2.2016, Train: 0.3406, Test: 0.3228\n",
            "Early stopping:  0.012819602741381386\n",
            "Epoch: 035, Loss: 2.1909, Train: 0.3588, Test: 0.3461\n",
            "Early stopping:  0.013392414733325092\n",
            "Epoch: 036, Loss: 2.1781, Train: 0.3639, Test: 0.3489\n",
            "Early stopping:  0.015565209051569233\n",
            "Epoch: 037, Loss: 2.1638, Train: 0.3678, Test: 0.3532\n",
            "Early stopping:  0.018533654982151853\n",
            "Epoch: 038, Loss: 2.1478, Train: 0.3715, Test: 0.3546\n",
            "Early stopping:  0.021358358146620713\n",
            "Epoch: 039, Loss: 2.1301, Train: 0.3758, Test: 0.3583\n",
            "Early stopping:  0.02407277083296159\n",
            "Epoch: 040, Loss: 2.1111, Train: 0.3769, Test: 0.3617\n",
            "Early stopping:  0.02654709652189962\n",
            "Epoch: 041, Loss: 2.0912, Train: 0.3795, Test: 0.3665\n",
            "Early stopping:  0.028775772638768276\n",
            "Epoch: 042, Loss: 2.0702, Train: 0.3860, Test: 0.3720\n",
            "Early stopping:  0.030709752498063313\n",
            "Epoch: 043, Loss: 2.0486, Train: 0.3900, Test: 0.3772\n",
            "Early stopping:  0.03225696040489059\n",
            "Epoch: 044, Loss: 2.0274, Train: 0.3942, Test: 0.3810\n",
            "Early stopping:  0.0332062462863579\n",
            "Epoch: 045, Loss: 2.0073, Train: 0.4041, Test: 0.3857\n",
            "Early stopping:  0.03327463194972946\n",
            "Epoch: 046, Loss: 1.9889, Train: 0.4084, Test: 0.3861\n",
            "Early stopping:  0.03225339491345911\n",
            "Epoch: 047, Loss: 1.9726, Train: 0.4118, Test: 0.3880\n",
            "Early stopping:  0.03016182066537172\n",
            "Epoch: 048, Loss: 1.9581, Train: 0.4135, Test: 0.3905\n",
            "Early stopping:  0.02747542451677404\n",
            "Epoch: 049, Loss: 1.9447, Train: 0.4172, Test: 0.3944\n",
            "Early stopping:  0.02471555355769293\n",
            "Epoch: 050, Loss: 1.9325, Train: 0.4220, Test: 0.3982\n",
            "Early stopping:  0.022280831419690412\n",
            "Epoch: 051, Loss: 1.9197, Train: 0.4257, Test: 0.4011\n",
            "Early stopping:  0.020811478635633363\n",
            "Epoch: 052, Loss: 1.9061, Train: 0.4291, Test: 0.4018\n",
            "Early stopping:  0.020427503536971422\n",
            "Epoch: 053, Loss: 1.8924, Train: 0.4302, Test: 0.4064\n",
            "Early stopping:  0.020714634448866762\n",
            "Epoch: 054, Loss: 1.8794, Train: 0.4319, Test: 0.4098\n",
            "Early stopping:  0.021095286385940083\n",
            "Epoch: 055, Loss: 1.8672, Train: 0.4356, Test: 0.4090\n",
            "Early stopping:  0.020825688043371114\n",
            "Epoch: 056, Loss: 1.8558, Train: 0.4368, Test: 0.4103\n",
            "Early stopping:  0.019901690070531348\n",
            "Epoch: 057, Loss: 1.8452, Train: 0.4387, Test: 0.4130\n",
            "Early stopping:  0.018664436652253303\n",
            "Epoch: 058, Loss: 1.8351, Train: 0.4385, Test: 0.4150\n",
            "Early stopping:  0.017486054361621265\n",
            "Epoch: 059, Loss: 1.8254, Train: 0.4385, Test: 0.4160\n",
            "Early stopping:  0.016504761993125425\n",
            "Epoch: 060, Loss: 1.8163, Train: 0.4385, Test: 0.4166\n",
            "Early stopping:  0.01565300425982823\n",
            "Epoch: 061, Loss: 1.8072, Train: 0.4410, Test: 0.4183\n",
            "Early stopping:  0.015005928803742\n",
            "Epoch: 062, Loss: 1.7979, Train: 0.4436, Test: 0.4196\n",
            "Early stopping:  0.014612472686953875\n",
            "Epoch: 063, Loss: 1.7884, Train: 0.4467, Test: 0.4207\n",
            "Early stopping:  0.014591245187480197\n",
            "Epoch: 064, Loss: 1.7783, Train: 0.4490, Test: 0.4215\n",
            "Early stopping:  0.014971620828788959\n",
            "Epoch: 065, Loss: 1.7678, Train: 0.4495, Test: 0.4205\n",
            "Early stopping:  0.015549684623976087\n",
            "Epoch: 066, Loss: 1.7574, Train: 0.4501, Test: 0.4211\n",
            "Early stopping:  0.016064028042778667\n",
            "Epoch: 067, Loss: 1.7471, Train: 0.4492, Test: 0.4235\n",
            "Early stopping:  0.016355938727687716\n",
            "Epoch: 068, Loss: 1.7375, Train: 0.4487, Test: 0.4235\n",
            "Early stopping:  0.01619319193839296\n",
            "Epoch: 069, Loss: 1.7283, Train: 0.4509, Test: 0.4262\n",
            "Early stopping:  0.01565260434836705\n",
            "Epoch: 070, Loss: 1.7191, Train: 0.4535, Test: 0.4283\n",
            "Early stopping:  0.015092350330105338\n",
            "Epoch: 071, Loss: 1.7097, Train: 0.4558, Test: 0.4296\n",
            "Early stopping:  0.014738182309871001\n",
            "Epoch: 072, Loss: 1.7002, Train: 0.4592, Test: 0.4305\n",
            "Early stopping:  0.014722779959862478\n",
            "Epoch: 073, Loss: 1.6908, Train: 0.4611, Test: 0.4296\n",
            "Early stopping:  0.014853229215575104\n",
            "Epoch: 074, Loss: 1.6813, Train: 0.4628, Test: 0.4330\n",
            "Early stopping:  0.014958178432388804\n",
            "Epoch: 075, Loss: 1.6716, Train: 0.4637, Test: 0.4332\n",
            "Early stopping:  0.015043874055074988\n",
            "Epoch: 076, Loss: 1.6616, Train: 0.4648, Test: 0.4372\n",
            "Early stopping:  0.015236044216657729\n",
            "Epoch: 077, Loss: 1.6513, Train: 0.4705, Test: 0.4417\n",
            "Early stopping:  0.015594747985152443\n",
            "Epoch: 078, Loss: 1.6411, Train: 0.4776, Test: 0.4455\n",
            "Early stopping:  0.015914835058002483\n",
            "Epoch: 079, Loss: 1.6316, Train: 0.4759, Test: 0.4468\n",
            "Early stopping:  0.015901088518074265\n",
            "Epoch: 080, Loss: 1.6217, Train: 0.4776, Test: 0.4504\n",
            "Early stopping:  0.01572656249509107\n",
            "Epoch: 081, Loss: 1.6103, Train: 0.4779, Test: 0.4508\n",
            "Early stopping:  0.016029445347055686\n",
            "Epoch: 082, Loss: 1.5998, Train: 0.4818, Test: 0.4517\n",
            "Early stopping:  0.016459868279032456\n",
            "Epoch: 083, Loss: 1.5893, Train: 0.4870, Test: 0.4528\n",
            "Early stopping:  0.01685822221299426\n",
            "Epoch: 084, Loss: 1.5790, Train: 0.4881, Test: 0.4527\n",
            "Early stopping:  0.01684857807395452\n",
            "Epoch: 085, Loss: 1.5689, Train: 0.4921, Test: 0.4513\n",
            "Early stopping:  0.016371892106183374\n",
            "Epoch: 086, Loss: 1.5593, Train: 0.4940, Test: 0.4528\n",
            "Early stopping:  0.01602938970084962\n",
            "Epoch: 087, Loss: 1.5496, Train: 0.4957, Test: 0.4551\n",
            "Early stopping:  0.015686425506779483\n",
            "Epoch: 088, Loss: 1.5401, Train: 0.4974, Test: 0.4574\n",
            "Early stopping:  0.015350327675990842\n",
            "Epoch: 089, Loss: 1.5305, Train: 0.5009, Test: 0.4600\n",
            "Early stopping:  0.015170922343932155\n",
            "Epoch: 090, Loss: 1.5211, Train: 0.5023, Test: 0.4610\n",
            "Early stopping:  0.015092619964213854\n",
            "Epoch: 091, Loss: 1.5114, Train: 0.5057, Test: 0.4619\n",
            "Early stopping:  0.015087098111469611\n",
            "Epoch: 092, Loss: 1.5021, Train: 0.5119, Test: 0.4651\n",
            "Early stopping:  0.01505069740037378\n",
            "Epoch: 093, Loss: 1.4925, Train: 0.5159, Test: 0.4682\n",
            "Early stopping:  0.01501434512387464\n",
            "Epoch: 094, Loss: 1.4842, Train: 0.5204, Test: 0.4706\n",
            "Early stopping:  0.0146476026343846\n",
            "Epoch: 095, Loss: 1.4761, Train: 0.5210, Test: 0.4750\n",
            "Early stopping:  0.013992615887942587\n",
            "Epoch: 096, Loss: 1.4668, Train: 0.5227, Test: 0.4772\n",
            "Early stopping:  0.013744365639334708\n",
            "Epoch: 097, Loss: 1.4594, Train: 0.5267, Test: 0.4814\n",
            "Early stopping:  0.013224328801646049\n",
            "Epoch: 098, Loss: 1.4515, Train: 0.5303, Test: 0.4827\n",
            "Early stopping:  0.013001478718361256\n",
            "Epoch: 099, Loss: 1.4425, Train: 0.5332, Test: 0.4837\n",
            "Early stopping:  0.013068517259869813\n",
            "Epoch: 100, Loss: 1.4345, Train: 0.5366, Test: 0.4833\n",
            "Early stopping:  0.01288944891890109\n",
            "Epoch: 101, Loss: 1.4259, Train: 0.5391, Test: 0.4844\n",
            "Early stopping:  0.01327456422173841\n",
            "Epoch: 102, Loss: 1.4179, Train: 0.5431, Test: 0.4863\n",
            "Early stopping:  0.013253027596205166\n",
            "Epoch: 103, Loss: 1.4101, Train: 0.5457, Test: 0.4901\n",
            "Early stopping:  0.012878747157996627\n",
            "Epoch: 104, Loss: 1.4021, Train: 0.5474, Test: 0.4927\n",
            "Early stopping:  0.012755870923145361\n",
            "Epoch: 105, Loss: 1.3947, Train: 0.5493, Test: 0.4948\n",
            "Early stopping:  0.012346384977911856\n",
            "Epoch: 106, Loss: 1.3868, Train: 0.5485, Test: 0.4963\n",
            "Early stopping:  0.012248631513690487\n",
            "Epoch: 107, Loss: 1.3795, Train: 0.5459, Test: 0.4973\n",
            "Early stopping:  0.012109180525035516\n",
            "Epoch: 108, Loss: 1.3721, Train: 0.5513, Test: 0.5003\n",
            "Early stopping:  0.011903899520187312\n",
            "Epoch: 109, Loss: 1.3643, Train: 0.5545, Test: 0.5039\n",
            "Early stopping:  0.011961817729098056\n",
            "Epoch: 110, Loss: 1.3568, Train: 0.5556, Test: 0.5048\n",
            "Early stopping:  0.011912616474334737\n",
            "Epoch: 111, Loss: 1.3495, Train: 0.5618, Test: 0.5043\n",
            "Early stopping:  0.011889224208698083\n",
            "Epoch: 112, Loss: 1.3421, Train: 0.5615, Test: 0.5054\n",
            "Early stopping:  0.011811396546874242\n",
            "Epoch: 113, Loss: 1.3353, Train: 0.5661, Test: 0.5084\n",
            "Early stopping:  0.01149859126842945\n",
            "Epoch: 114, Loss: 1.3280, Train: 0.5686, Test: 0.5099\n",
            "Early stopping:  0.01134235436798455\n",
            "Epoch: 115, Loss: 1.3215, Train: 0.5706, Test: 0.5112\n",
            "Early stopping:  0.011078172461679034\n",
            "Epoch: 116, Loss: 1.3147, Train: 0.5729, Test: 0.5114\n",
            "Early stopping:  0.010826917473167757\n",
            "Epoch: 117, Loss: 1.3079, Train: 0.5769, Test: 0.5128\n",
            "Early stopping:  0.010758109142238802\n",
            "Epoch: 118, Loss: 1.3005, Train: 0.5811, Test: 0.5148\n",
            "Early stopping:  0.010878018879980144\n",
            "Epoch: 119, Loss: 1.2935, Train: 0.5834, Test: 0.5158\n",
            "Early stopping:  0.011111879609174945\n",
            "Epoch: 120, Loss: 1.2866, Train: 0.5842, Test: 0.5167\n",
            "Early stopping:  0.011150487895298516\n",
            "Epoch: 121, Loss: 1.2800, Train: 0.5885, Test: 0.5167\n",
            "Early stopping:  0.010995665464587098\n",
            "Epoch: 122, Loss: 1.2732, Train: 0.5902, Test: 0.5199\n",
            "Early stopping:  0.010750171296059\n",
            "Epoch: 123, Loss: 1.2659, Train: 0.5910, Test: 0.5205\n",
            "Early stopping:  0.010870088399964579\n",
            "Epoch: 124, Loss: 1.2596, Train: 0.5959, Test: 0.5220\n",
            "Early stopping:  0.010783414668412851\n",
            "Epoch: 125, Loss: 1.2533, Train: 0.5950, Test: 0.5222\n",
            "Early stopping:  0.010611118797873395\n",
            "Epoch: 126, Loss: 1.2458, Train: 0.5967, Test: 0.5260\n",
            "Early stopping:  0.010670359162980557\n",
            "Epoch: 127, Loss: 1.2401, Train: 0.5964, Test: 0.5237\n",
            "Early stopping:  0.010348154989277807\n",
            "Epoch: 128, Loss: 1.2352, Train: 0.6007, Test: 0.5290\n",
            "Early stopping:  0.009850143823492181\n",
            "PREDICTIONS -> tensor([9, 0, 1,  ..., 5, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.44      0.53      0.48       568\n",
            "         capital_goods       0.33      0.11      0.16       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.51      0.49      0.50       595\n",
            " consumer_non-cyclical       0.66      0.39      0.49       334\n",
            "                energy       0.62      0.51      0.56       213\n",
            "             financial       0.65      0.57      0.60       576\n",
            "            healthcare       0.74      0.46      0.57       238\n",
            "              services       0.49      0.79      0.61      1557\n",
            "            technology       0.47      0.18      0.26       297\n",
            "        transportation       0.71      0.62      0.66       303\n",
            "             utilities       0.71      0.12      0.20       169\n",
            "\n",
            "              accuracy                           0.53      5291\n",
            "             macro avg       0.53      0.40      0.43      5291\n",
            "          weighted avg       0.53      0.53      0.50      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 48.3674, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 377.9465, Train: 0.1123, Test: 0.1119\n",
            "Early stopping:  233.04763113920512\n",
            "Epoch: 003, Loss: 483.7516, Train: 0.1100, Test: 0.1092\n",
            "Early stopping:  227.07433381064445\n",
            "Epoch: 004, Loss: 513.9568, Train: 0.0638, Test: 0.0614\n",
            "Early stopping:  213.2215447179338\n",
            "Epoch: 005, Loss: 336.8821, Train: 0.1103, Test: 0.1102\n",
            "Early stopping:  184.85321696768895\n",
            "Epoch: 006, Loss: 416.8482, Train: 0.0885, Test: 0.0875\n",
            "Early stopping:  73.15473217169918\n",
            "Epoch: 007, Loss: 347.4646, Train: 0.0746, Test: 0.0714\n",
            "Early stopping:  79.17039104201548\n",
            "Epoch: 008, Loss: 266.5099, Train: 0.0664, Test: 0.0703\n",
            "Early stopping:  93.58505944501452\n",
            "Epoch: 009, Loss: 204.8536, Train: 0.2700, Test: 0.2646\n",
            "Early stopping:  81.22215873049973\n",
            "Epoch: 010, Loss: 161.8553, Train: 0.2643, Test: 0.2671\n",
            "Early stopping:  103.76033278004047\n",
            "Epoch: 011, Loss: 131.6042, Train: 0.2646, Test: 0.2652\n",
            "Early stopping:  86.31952420446795\n",
            "Epoch: 012, Loss: 86.9267, Train: 0.2604, Test: 0.2620\n",
            "Early stopping:  68.86593115592271\n",
            "Epoch: 013, Loss: 37.1169, Train: 0.1838, Test: 0.1847\n",
            "Early stopping:  65.09636392771087\n",
            "Epoch: 014, Loss: 18.2332, Train: 0.1977, Test: 0.2019\n",
            "Early stopping:  60.848064683612634\n",
            "Epoch: 015, Loss: 9.3579, Train: 0.1954, Test: 0.1952\n",
            "Early stopping:  51.54793251964802\n",
            "Epoch: 016, Loss: 5.3140, Train: 0.2150, Test: 0.2117\n",
            "Early stopping:  33.375142899487464\n",
            "Epoch: 017, Loss: 3.4897, Train: 0.2357, Test: 0.2272\n",
            "Early stopping:  13.759600456065115\n",
            "Epoch: 018, Loss: 2.7045, Train: 0.2416, Test: 0.2255\n",
            "Early stopping:  6.363955095611276\n",
            "Epoch: 019, Loss: 2.4899, Train: 0.2221, Test: 0.2087\n",
            "Early stopping:  2.8460538729859755\n",
            "Epoch: 020, Loss: 2.4554, Train: 0.2116, Test: 0.1986\n",
            "Early stopping:  1.205811236753421\n",
            "Epoch: 021, Loss: 2.4587, Train: 0.1971, Test: 0.1862\n",
            "Early stopping:  0.44268235418710766\n",
            "Epoch: 022, Loss: 2.4561, Train: 0.1872, Test: 0.1767\n",
            "Early stopping:  0.10804082469276673\n",
            "Epoch: 023, Loss: 2.4372, Train: 0.1829, Test: 0.1729\n",
            "Early stopping:  0.01905799944091654\n",
            "Epoch: 024, Loss: 2.4123, Train: 0.1767, Test: 0.1684\n",
            "Early stopping:  0.01963274241525079\n",
            "Epoch: 025, Loss: 2.3782, Train: 0.1685, Test: 0.1663\n",
            "Early stopping:  0.0337006267526745\n",
            "Epoch: 026, Loss: 2.3436, Train: 0.1699, Test: 0.1646\n",
            "Early stopping:  0.045253937918052996\n",
            "Epoch: 027, Loss: 2.3241, Train: 0.1713, Test: 0.1678\n",
            "Early stopping:  0.046824371793955225\n",
            "Epoch: 028, Loss: 2.3135, Train: 0.1753, Test: 0.1731\n",
            "Early stopping:  0.0407228322029059\n",
            "Epoch: 029, Loss: 2.3044, Train: 0.1795, Test: 0.1761\n",
            "Early stopping:  0.029300912760552807\n",
            "Epoch: 030, Loss: 2.2986, Train: 0.1832, Test: 0.1792\n",
            "Early stopping:  0.01781137325937976\n",
            "Epoch: 031, Loss: 2.2897, Train: 0.1860, Test: 0.1818\n",
            "Early stopping:  0.013291676578081443\n",
            "Epoch: 032, Loss: 2.2788, Train: 0.1897, Test: 0.1847\n",
            "Early stopping:  0.013360782031095012\n",
            "Epoch: 033, Loss: 2.2690, Train: 0.1931, Test: 0.1884\n",
            "Early stopping:  0.01439681687429778\n",
            "Epoch: 034, Loss: 2.2582, Train: 0.1968, Test: 0.1913\n",
            "Early stopping:  0.016032275793375125\n",
            "Epoch: 035, Loss: 2.2470, Train: 0.2033, Test: 0.1966\n",
            "Early stopping:  0.016761457651783124\n",
            "Epoch: 036, Loss: 2.2365, Train: 0.2065, Test: 0.2013\n",
            "Early stopping:  0.01683734110538833\n",
            "Epoch: 037, Loss: 2.2247, Train: 0.2124, Test: 0.2062\n",
            "Early stopping:  0.017445181065421894\n",
            "Epoch: 038, Loss: 2.2126, Train: 0.2209, Test: 0.2102\n",
            "Early stopping:  0.017958116111110804\n",
            "Epoch: 039, Loss: 2.2001, Train: 0.2263, Test: 0.2136\n",
            "Early stopping:  0.018609546876603388\n",
            "Epoch: 040, Loss: 2.1859, Train: 0.2311, Test: 0.2206\n",
            "Early stopping:  0.019914153009253994\n",
            "Epoch: 041, Loss: 2.1714, Train: 0.2405, Test: 0.2285\n",
            "Early stopping:  0.021082369225963862\n",
            "Epoch: 042, Loss: 2.1564, Train: 0.2663, Test: 0.2429\n",
            "Early stopping:  0.022313423981849694\n",
            "Epoch: 043, Loss: 2.1410, Train: 0.2816, Test: 0.2603\n",
            "Early stopping:  0.02335688523508141\n",
            "Epoch: 044, Loss: 2.1241, Train: 0.2907, Test: 0.2735\n",
            "Early stopping:  0.02435668187074285\n",
            "Epoch: 045, Loss: 2.1071, Train: 0.3040, Test: 0.2905\n",
            "Early stopping:  0.025468998808142778\n",
            "Epoch: 046, Loss: 2.0901, Train: 0.3298, Test: 0.3081\n",
            "Early stopping:  0.0263511857571054\n",
            "Epoch: 047, Loss: 2.0724, Train: 0.3386, Test: 0.3194\n",
            "Early stopping:  0.02708625561172671\n",
            "Epoch: 048, Loss: 2.0551, Train: 0.3503, Test: 0.3260\n",
            "Early stopping:  0.0273147849847806\n",
            "Epoch: 049, Loss: 2.0381, Train: 0.3622, Test: 0.3387\n",
            "Early stopping:  0.027356817081307255\n",
            "Epoch: 050, Loss: 2.0197, Train: 0.3729, Test: 0.3491\n",
            "Early stopping:  0.02769012222553138\n",
            "Epoch: 051, Loss: 2.0015, Train: 0.3778, Test: 0.3523\n",
            "Early stopping:  0.028025558118826562\n",
            "Epoch: 052, Loss: 1.9821, Train: 0.3914, Test: 0.3634\n",
            "Early stopping:  0.028883123413217294\n",
            "Epoch: 053, Loss: 1.9610, Train: 0.3976, Test: 0.3733\n",
            "Early stopping:  0.030341018205116654\n",
            "Epoch: 054, Loss: 1.9392, Train: 0.4033, Test: 0.3761\n",
            "Early stopping:  0.03186379321596233\n",
            "Epoch: 055, Loss: 1.9172, Train: 0.4053, Test: 0.3791\n",
            "Early stopping:  0.03342602349167192\n",
            "Epoch: 056, Loss: 1.8981, Train: 0.4075, Test: 0.3822\n",
            "Early stopping:  0.033491909774867455\n",
            "Epoch: 057, Loss: 1.8811, Train: 0.4002, Test: 0.3793\n",
            "Early stopping:  0.03179400665367364\n",
            "Epoch: 058, Loss: 1.8659, Train: 0.4007, Test: 0.3782\n",
            "Early stopping:  0.02898549839413232\n",
            "Epoch: 059, Loss: 1.8507, Train: 0.4070, Test: 0.3839\n",
            "Early stopping:  0.026174408211621595\n",
            "Epoch: 060, Loss: 1.8361, Train: 0.4098, Test: 0.3850\n",
            "Early stopping:  0.02441394236063478\n",
            "Epoch: 061, Loss: 1.8220, Train: 0.4101, Test: 0.3839\n",
            "Early stopping:  0.02339998381680007\n",
            "Epoch: 062, Loss: 1.8092, Train: 0.4050, Test: 0.3865\n",
            "Early stopping:  0.022445021914952675\n",
            "Epoch: 063, Loss: 1.7982, Train: 0.4078, Test: 0.3848\n",
            "Early stopping:  0.020860934438731885\n",
            "Epoch: 064, Loss: 1.7890, Train: 0.4126, Test: 0.3871\n",
            "Early stopping:  0.018718371740651753\n",
            "Epoch: 065, Loss: 1.7796, Train: 0.4135, Test: 0.3873\n",
            "Early stopping:  0.016657807613560817\n",
            "Epoch: 066, Loss: 1.7688, Train: 0.4178, Test: 0.3884\n",
            "Early stopping:  0.015749067094149114\n",
            "Epoch: 067, Loss: 1.7572, Train: 0.4161, Test: 0.3907\n",
            "Early stopping:  0.016186178498360314\n",
            "Epoch: 068, Loss: 1.7457, Train: 0.4195, Test: 0.3903\n",
            "Early stopping:  0.017233006786799985\n",
            "Epoch: 069, Loss: 1.7339, Train: 0.4192, Test: 0.3916\n",
            "Early stopping:  0.018095880368769218\n",
            "Epoch: 070, Loss: 1.7248, Train: 0.4192, Test: 0.3946\n",
            "Early stopping:  0.01762072718206485\n",
            "Epoch: 071, Loss: 1.7172, Train: 0.4195, Test: 0.3958\n",
            "Early stopping:  0.016051184483703466\n",
            "Epoch: 072, Loss: 1.7092, Train: 0.4209, Test: 0.3992\n",
            "Early stopping:  0.014274780923449847\n",
            "Epoch: 073, Loss: 1.7011, Train: 0.4226, Test: 0.4035\n",
            "Early stopping:  0.01283325145513229\n",
            "Epoch: 074, Loss: 1.6927, Train: 0.4237, Test: 0.4067\n",
            "Early stopping:  0.012678326802884965\n",
            "Epoch: 075, Loss: 1.6842, Train: 0.4299, Test: 0.4065\n",
            "Early stopping:  0.013037838472308248\n",
            "Epoch: 076, Loss: 1.6752, Train: 0.4325, Test: 0.4079\n",
            "Early stopping:  0.013411792222774301\n",
            "Epoch: 077, Loss: 1.6667, Train: 0.4339, Test: 0.4081\n",
            "Early stopping:  0.013651912251619729\n",
            "Epoch: 078, Loss: 1.6585, Train: 0.4370, Test: 0.4107\n",
            "Early stopping:  0.01357581472712593\n",
            "Epoch: 079, Loss: 1.6508, Train: 0.4404, Test: 0.4133\n",
            "Early stopping:  0.013183196306250436\n",
            "Epoch: 080, Loss: 1.6428, Train: 0.4421, Test: 0.4128\n",
            "Early stopping:  0.012763346262773138\n",
            "Epoch: 081, Loss: 1.6353, Train: 0.4444, Test: 0.4137\n",
            "Early stopping:  0.012405209510130993\n",
            "Epoch: 082, Loss: 1.6275, Train: 0.4470, Test: 0.4158\n",
            "Early stopping:  0.012273737237723133\n",
            "Epoch: 083, Loss: 1.6200, Train: 0.4524, Test: 0.4173\n",
            "Early stopping:  0.012196315513613967\n",
            "Epoch: 084, Loss: 1.6121, Train: 0.4541, Test: 0.4183\n",
            "Early stopping:  0.012145017496265221\n",
            "Epoch: 085, Loss: 1.6045, Train: 0.4572, Test: 0.4183\n",
            "Early stopping:  0.012175706550945\n",
            "Epoch: 086, Loss: 1.5968, Train: 0.4594, Test: 0.4224\n",
            "Early stopping:  0.012126533178375426\n",
            "Epoch: 087, Loss: 1.5892, Train: 0.4606, Test: 0.4241\n",
            "Early stopping:  0.012133242009089626\n",
            "Epoch: 088, Loss: 1.5820, Train: 0.4586, Test: 0.4266\n",
            "Early stopping:  0.011929679970409047\n",
            "Epoch: 089, Loss: 1.5753, Train: 0.4620, Test: 0.4285\n",
            "Early stopping:  0.01159443436162105\n",
            "Epoch: 090, Loss: 1.5685, Train: 0.4651, Test: 0.4292\n",
            "Early stopping:  0.011168718962433202\n",
            "Epoch: 091, Loss: 1.5624, Train: 0.4677, Test: 0.4304\n",
            "Early stopping:  0.010633370198488664\n",
            "Epoch: 092, Loss: 1.5594, Train: 0.4668, Test: 0.4281\n",
            "Early stopping:  0.009268477121776792\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 6, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.08      0.14       568\n",
            "         capital_goods       0.00      0.00      0.00       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.30      0.36      0.33       595\n",
            " consumer_non-cyclical       0.74      0.28      0.41       334\n",
            "                energy       0.35      0.04      0.08       213\n",
            "             financial       0.45      0.56      0.50       576\n",
            "            healthcare       0.56      0.55      0.56       238\n",
            "              services       0.41      0.80      0.54      1557\n",
            "            technology       0.00      0.00      0.00       297\n",
            "        transportation       0.63      0.64      0.64       303\n",
            "             utilities       0.40      0.01      0.02       169\n",
            "\n",
            "              accuracy                           0.43      5291\n",
            "             macro avg       0.35      0.28      0.27      5291\n",
            "          weighted avg       0.38      0.43      0.36      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 105.4719, Train: 0.0403, Test: 0.0406\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 326.5340, Train: 0.2541, Test: 0.2601\n",
            "Early stopping:  156.31451228662746\n",
            "Epoch: 003, Loss: 357.6443, Train: 0.2720, Test: 0.2729\n",
            "Early stopping:  137.49375899990173\n",
            "Epoch: 004, Loss: 265.2329, Train: 0.0927, Test: 0.0866\n",
            "Early stopping:  112.267710271076\n",
            "Epoch: 005, Loss: 191.7963, Train: 0.0326, Test: 0.0284\n",
            "Early stopping:  102.40925428955474\n",
            "Epoch: 006, Loss: 264.8824, Train: 0.1214, Test: 0.1183\n",
            "Early stopping:  64.05686863943906\n",
            "Epoch: 007, Loss: 288.7513, Train: 0.0879, Test: 0.0805\n",
            "Early stopping:  59.436592119926466\n",
            "Epoch: 008, Loss: 212.0879, Train: 0.2601, Test: 0.2565\n",
            "Early stopping:  40.71760788290597\n",
            "Epoch: 009, Loss: 131.7332, Train: 0.3037, Test: 0.3001\n",
            "Early stopping:  61.98217674378735\n",
            "Epoch: 010, Loss: 125.3004, Train: 0.2924, Test: 0.2914\n",
            "Early stopping:  74.78214134602963\n",
            "Epoch: 011, Loss: 82.5050, Train: 0.1560, Test: 0.1493\n",
            "Early stopping:  82.11659144593946\n",
            "Epoch: 012, Loss: 85.7386, Train: 0.1551, Test: 0.1446\n",
            "Early stopping:  52.31259902789088\n",
            "Epoch: 013, Loss: 88.3692, Train: 0.2362, Test: 0.2308\n",
            "Early stopping:  23.74131926525946\n",
            "Epoch: 014, Loss: 56.8007, Train: 0.2930, Test: 0.2890\n",
            "Early stopping:  24.494042002952288\n",
            "Epoch: 015, Loss: 42.9761, Train: 0.3372, Test: 0.3336\n",
            "Early stopping:  20.235182392812995\n",
            "Epoch: 016, Loss: 44.1790, Train: 0.2306, Test: 0.2189\n",
            "Early stopping:  22.09286115100618\n",
            "Epoch: 017, Loss: 35.4120, Train: 0.2538, Test: 0.2391\n",
            "Early stopping:  20.925784563392263\n",
            "Epoch: 018, Loss: 20.4028, Train: 0.2674, Test: 0.2589\n",
            "Early stopping:  13.357366818715649\n",
            "Epoch: 019, Loss: 11.3010, Train: 0.2399, Test: 0.2226\n",
            "Early stopping:  14.463967288811885\n",
            "Epoch: 020, Loss: 6.6743, Train: 0.2394, Test: 0.2183\n",
            "Early stopping:  15.897999877596208\n",
            "Epoch: 021, Loss: 5.3598, Train: 0.2473, Test: 0.2325\n",
            "Early stopping:  12.433100717788715\n",
            "Epoch: 022, Loss: 4.4524, Train: 0.2581, Test: 0.2472\n",
            "Early stopping:  6.569307979958692\n",
            "Epoch: 023, Loss: 3.7012, Train: 0.2864, Test: 0.2659\n",
            "Early stopping:  3.0083792445841735\n",
            "Epoch: 024, Loss: 3.1581, Train: 0.3094, Test: 0.2920\n",
            "Early stopping:  1.3932792134562209\n",
            "Epoch: 025, Loss: 2.8090, Train: 0.3191, Test: 0.3079\n",
            "Early stopping:  1.0266724323777228\n",
            "Epoch: 026, Loss: 2.5843, Train: 0.3335, Test: 0.3126\n",
            "Early stopping:  0.7506663315473744\n",
            "Epoch: 027, Loss: 2.4350, Train: 0.3420, Test: 0.3198\n",
            "Early stopping:  0.5063818658984393\n",
            "Epoch: 028, Loss: 2.3313, Train: 0.3284, Test: 0.3147\n",
            "Early stopping:  0.3296066061568134\n",
            "Epoch: 029, Loss: 2.2562, Train: 0.3304, Test: 0.3158\n",
            "Early stopping:  0.21981243091280356\n",
            "Epoch: 030, Loss: 2.2099, Train: 0.3358, Test: 0.3213\n",
            "Early stopping:  0.1500219151851386\n",
            "Epoch: 031, Loss: 2.1736, Train: 0.3452, Test: 0.3321\n",
            "Early stopping:  0.10424710389531396\n",
            "Epoch: 032, Loss: 2.1422, Train: 0.3639, Test: 0.3487\n",
            "Early stopping:  0.07414418691694331\n",
            "Epoch: 033, Loss: 2.1188, Train: 0.3693, Test: 0.3583\n",
            "Early stopping:  0.05459573799917582\n",
            "Epoch: 034, Loss: 2.1037, Train: 0.3724, Test: 0.3565\n",
            "Early stopping:  0.042785934739547116\n",
            "Epoch: 035, Loss: 2.0896, Train: 0.3735, Test: 0.3591\n",
            "Early stopping:  0.0331573211765716\n",
            "Epoch: 036, Loss: 2.0718, Train: 0.3775, Test: 0.3582\n",
            "Early stopping:  0.02699357595609423\n",
            "Epoch: 037, Loss: 2.0556, Train: 0.3772, Test: 0.3610\n",
            "Early stopping:  0.025039402399361078\n",
            "Epoch: 038, Loss: 2.0430, Train: 0.3735, Test: 0.3578\n",
            "Early stopping:  0.024603378046461365\n",
            "Epoch: 039, Loss: 2.0312, Train: 0.3783, Test: 0.3593\n",
            "Early stopping:  0.023128682748985042\n",
            "Epoch: 040, Loss: 2.0204, Train: 0.3775, Test: 0.3599\n",
            "Early stopping:  0.020160274235609288\n",
            "Epoch: 041, Loss: 2.0109, Train: 0.3860, Test: 0.3672\n",
            "Early stopping:  0.01774422235297691\n",
            "Epoch: 042, Loss: 1.9970, Train: 0.3880, Test: 0.3712\n",
            "Early stopping:  0.017771776197639488\n",
            "Epoch: 043, Loss: 1.9851, Train: 0.3891, Test: 0.3755\n",
            "Early stopping:  0.018307093620205193\n",
            "Epoch: 044, Loss: 1.9737, Train: 0.3925, Test: 0.3763\n",
            "Early stopping:  0.018874562932058\n",
            "Epoch: 045, Loss: 1.9607, Train: 0.3996, Test: 0.3848\n",
            "Early stopping:  0.019558930684448684\n",
            "Epoch: 046, Loss: 1.9472, Train: 0.4030, Test: 0.3916\n",
            "Early stopping:  0.01962742253283161\n",
            "Epoch: 047, Loss: 1.9333, Train: 0.4092, Test: 0.3950\n",
            "Early stopping:  0.02059138090350595\n",
            "Epoch: 048, Loss: 1.9194, Train: 0.4112, Test: 0.3982\n",
            "Early stopping:  0.021507295445092313\n",
            "Epoch: 049, Loss: 1.9043, Train: 0.4149, Test: 0.4024\n",
            "Early stopping:  0.022244364833827843\n",
            "Epoch: 050, Loss: 1.8887, Train: 0.4183, Test: 0.4094\n",
            "Early stopping:  0.023094355278470795\n",
            "Epoch: 051, Loss: 1.8733, Train: 0.4254, Test: 0.4150\n",
            "Early stopping:  0.023841381893531983\n",
            "Epoch: 052, Loss: 1.8581, Train: 0.4291, Test: 0.4177\n",
            "Early stopping:  0.0243076690471203\n",
            "Epoch: 053, Loss: 1.8436, Train: 0.4336, Test: 0.4183\n",
            "Early stopping:  0.024030302560492236\n",
            "Epoch: 054, Loss: 1.8309, Train: 0.4387, Test: 0.4192\n",
            "Early stopping:  0.022983692834141374\n",
            "Epoch: 055, Loss: 1.8184, Train: 0.4407, Test: 0.4232\n",
            "Early stopping:  0.021658148139124544\n",
            "Epoch: 056, Loss: 1.8069, Train: 0.4504, Test: 0.4292\n",
            "Early stopping:  0.02016881539927557\n",
            "Epoch: 057, Loss: 1.7959, Train: 0.4580, Test: 0.4345\n",
            "Early stopping:  0.01887416672280591\n",
            "Epoch: 058, Loss: 1.7848, Train: 0.4631, Test: 0.4383\n",
            "Early stopping:  0.018138574106349142\n",
            "Epoch: 059, Loss: 1.7740, Train: 0.4628, Test: 0.4421\n",
            "Early stopping:  0.017556006565080332\n",
            "Epoch: 060, Loss: 1.7625, Train: 0.4623, Test: 0.4443\n",
            "Early stopping:  0.01750875412792623\n",
            "Epoch: 061, Loss: 1.7508, Train: 0.4645, Test: 0.4449\n",
            "Early stopping:  0.017788928397587216\n",
            "Epoch: 062, Loss: 1.7380, Train: 0.4685, Test: 0.4496\n",
            "Early stopping:  0.018455956541393006\n",
            "Epoch: 063, Loss: 1.7249, Train: 0.4742, Test: 0.4498\n",
            "Early stopping:  0.019423293602403774\n",
            "Epoch: 064, Loss: 1.7118, Train: 0.4767, Test: 0.4506\n",
            "Early stopping:  0.020150268736199087\n",
            "Epoch: 065, Loss: 1.6995, Train: 0.4784, Test: 0.4513\n",
            "Early stopping:  0.02036401217911164\n",
            "Epoch: 066, Loss: 1.6879, Train: 0.4801, Test: 0.4540\n",
            "Early stopping:  0.01987796874504317\n",
            "Epoch: 067, Loss: 1.6761, Train: 0.4821, Test: 0.4568\n",
            "Early stopping:  0.01921959143155841\n",
            "Epoch: 068, Loss: 1.6643, Train: 0.4841, Test: 0.4598\n",
            "Early stopping:  0.018733735659339706\n",
            "Epoch: 069, Loss: 1.6516, Train: 0.4895, Test: 0.4653\n",
            "Early stopping:  0.018895500079913426\n",
            "Epoch: 070, Loss: 1.6391, Train: 0.4904, Test: 0.4655\n",
            "Early stopping:  0.019283526635979834\n",
            "Epoch: 071, Loss: 1.6281, Train: 0.4969, Test: 0.4712\n",
            "Early stopping:  0.019162867541985536\n",
            "Epoch: 072, Loss: 1.6180, Train: 0.5017, Test: 0.4700\n",
            "Early stopping:  0.0183866289705277\n",
            "Epoch: 073, Loss: 1.6064, Train: 0.5068, Test: 0.4736\n",
            "Early stopping:  0.0176519296683658\n",
            "Epoch: 074, Loss: 1.5943, Train: 0.5116, Test: 0.4727\n",
            "Early stopping:  0.017607881087344707\n",
            "Epoch: 075, Loss: 1.5841, Train: 0.5153, Test: 0.4761\n",
            "Early stopping:  0.017642546536306456\n",
            "Epoch: 076, Loss: 1.5744, Train: 0.5167, Test: 0.4799\n",
            "Early stopping:  0.017301525465889766\n",
            "Epoch: 077, Loss: 1.5626, Train: 0.5201, Test: 0.4821\n",
            "Early stopping:  0.016986518437888613\n",
            "Epoch: 078, Loss: 1.5518, Train: 0.5207, Test: 0.4852\n",
            "Early stopping:  0.01684568097486149\n",
            "Epoch: 079, Loss: 1.5424, Train: 0.5252, Test: 0.4844\n",
            "Early stopping:  0.016767553967027934\n",
            "Epoch: 080, Loss: 1.5310, Train: 0.5247, Test: 0.4859\n",
            "Early stopping:  0.016944026928469573\n",
            "Epoch: 081, Loss: 1.5227, Train: 0.5284, Test: 0.4895\n",
            "Early stopping:  0.015941100538349667\n",
            "Epoch: 082, Loss: 1.5142, Train: 0.5355, Test: 0.4901\n",
            "Early stopping:  0.015053738089375216\n",
            "Epoch: 083, Loss: 1.5018, Train: 0.5326, Test: 0.4865\n",
            "Early stopping:  0.015536570756760515\n",
            "Epoch: 084, Loss: 1.4963, Train: 0.5386, Test: 0.4893\n",
            "Early stopping:  0.014336447221841625\n",
            "Epoch: 085, Loss: 1.4857, Train: 0.5400, Test: 0.4874\n",
            "Early stopping:  0.01459991527015565\n",
            "Epoch: 086, Loss: 1.4758, Train: 0.5420, Test: 0.4886\n",
            "Early stopping:  0.014777014454392558\n",
            "Epoch: 087, Loss: 1.4641, Train: 0.5403, Test: 0.4872\n",
            "Early stopping:  0.015293618645048267\n",
            "Epoch: 088, Loss: 1.4578, Train: 0.5457, Test: 0.4916\n",
            "Early stopping:  0.015643490337764774\n",
            "Epoch: 089, Loss: 1.4517, Train: 0.5505, Test: 0.4906\n",
            "Early stopping:  0.013713891585305824\n",
            "Epoch: 090, Loss: 1.4342, Train: 0.5471, Test: 0.4886\n",
            "Early stopping:  0.01540159534895056\n",
            "Epoch: 091, Loss: 1.4324, Train: 0.5530, Test: 0.4944\n",
            "Early stopping:  0.014151630266071259\n",
            "Epoch: 092, Loss: 1.4242, Train: 0.5542, Test: 0.4956\n",
            "Early stopping:  0.01410692735777052\n",
            "Epoch: 093, Loss: 1.4087, Train: 0.5567, Test: 0.4918\n",
            "Early stopping:  0.015670364285219167\n",
            "Epoch: 094, Loss: 1.4050, Train: 0.5652, Test: 0.4974\n",
            "Early stopping:  0.01343204970751306\n",
            "Epoch: 095, Loss: 1.3888, Train: 0.5675, Test: 0.4993\n",
            "Early stopping:  0.017042061400923594\n",
            "Epoch: 096, Loss: 1.3887, Train: 0.5726, Test: 0.5022\n",
            "Early stopping:  0.014916359962434525\n",
            "Epoch: 097, Loss: 1.3753, Train: 0.5681, Test: 0.5018\n",
            "Early stopping:  0.0135889784164246\n",
            "Epoch: 098, Loss: 1.3738, Train: 0.5763, Test: 0.5056\n",
            "Early stopping:  0.01265125696237277\n",
            "Epoch: 099, Loss: 1.3602, Train: 0.5743, Test: 0.5056\n",
            "Early stopping:  0.011971682326982582\n",
            "Epoch: 100, Loss: 1.3578, Train: 0.5783, Test: 0.5071\n",
            "Early stopping:  0.012572623056530273\n",
            "Epoch: 101, Loss: 1.3455, Train: 0.5760, Test: 0.5020\n",
            "Early stopping:  0.012323968899995326\n",
            "Epoch: 102, Loss: 1.3497, Train: 0.5749, Test: 0.5073\n",
            "Early stopping:  0.01091409920727512\n",
            "Epoch: 103, Loss: 1.3418, Train: 0.5737, Test: 0.5086\n",
            "Early stopping:  0.007850391531969932\n",
            "PREDICTIONS -> tensor([ 9,  8,  1,  ..., 11,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.58      0.43      0.49       568\n",
            "         capital_goods       0.34      0.16      0.22       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.50      0.31      0.39       595\n",
            " consumer_non-cyclical       0.80      0.26      0.39       334\n",
            "                energy       0.57      0.54      0.55       213\n",
            "             financial       0.62      0.51      0.56       576\n",
            "            healthcare       0.75      0.29      0.42       238\n",
            "              services       0.45      0.82      0.58      1557\n",
            "            technology       0.46      0.36      0.40       297\n",
            "        transportation       0.69      0.55      0.62       303\n",
            "             utilities       0.55      0.49      0.52       169\n",
            "\n",
            "              accuracy                           0.51      5291\n",
            "             macro avg       0.53      0.39      0.43      5291\n",
            "          weighted avg       0.53      0.51      0.49      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 77.8321, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 447.1064, Train: 0.1234, Test: 0.1225\n",
            "Early stopping:  261.11639914390986\n",
            "Epoch: 003, Loss: 322.9109, Train: 0.1566, Test: 0.1493\n",
            "Early stopping:  187.905878605781\n",
            "Epoch: 004, Loss: 374.3645, Train: 0.0675, Test: 0.0643\n",
            "Early stopping:  160.13588678422363\n",
            "Epoch: 005, Loss: 447.1007, Train: 0.0627, Test: 0.0618\n",
            "Early stopping:  152.44588532406507\n",
            "Epoch: 006, Loss: 402.8055, Train: 0.0604, Test: 0.0592\n",
            "Early stopping:  52.53269972219188\n",
            "Epoch: 007, Loss: 318.8773, Train: 0.0519, Test: 0.0505\n",
            "Early stopping:  54.357812289155234\n",
            "Epoch: 008, Loss: 248.2870, Train: 0.2981, Test: 0.3003\n",
            "Early stopping:  77.1060255695611\n",
            "Epoch: 009, Loss: 222.8594, Train: 0.2921, Test: 0.2894\n",
            "Early stopping:  96.53075653432167\n",
            "Epoch: 010, Loss: 188.9616, Train: 0.2867, Test: 0.2880\n",
            "Early stopping:  85.27170879847907\n",
            "Epoch: 011, Loss: 147.4758, Train: 0.1693, Test: 0.1618\n",
            "Early stopping:  64.5526371572904\n",
            "Epoch: 012, Loss: 141.7400, Train: 0.2005, Test: 0.1939\n",
            "Early stopping:  46.40972503777947\n",
            "Epoch: 013, Loss: 99.0490, Train: 0.3330, Test: 0.3217\n",
            "Early stopping:  47.42201187504596\n",
            "Epoch: 014, Loss: 47.0626, Train: 0.3321, Test: 0.3317\n",
            "Early stopping:  53.90747558170307\n",
            "Epoch: 015, Loss: 25.8726, Train: 0.2964, Test: 0.2984\n",
            "Early stopping:  54.75678216178408\n",
            "Epoch: 016, Loss: 12.8279, Train: 0.2643, Test: 0.2665\n",
            "Early stopping:  53.903865342650356\n",
            "Epoch: 017, Loss: 4.6091, Train: 0.1645, Test: 0.1540\n",
            "Early stopping:  37.764758146103134\n",
            "Epoch: 018, Loss: 3.2874, Train: 0.1327, Test: 0.1281\n",
            "Early stopping:  18.21033505132908\n",
            "Epoch: 019, Loss: 3.3887, Train: 0.1350, Test: 0.1266\n",
            "Early stopping:  9.718121750197863\n",
            "Epoch: 020, Loss: 3.1346, Train: 0.1463, Test: 0.1400\n",
            "Early stopping:  4.1661452158170365\n",
            "Epoch: 021, Loss: 2.8371, Train: 0.1767, Test: 0.1631\n",
            "Early stopping:  0.6798482581266369\n",
            "Epoch: 022, Loss: 2.6192, Train: 0.2096, Test: 0.1860\n",
            "Early stopping:  0.31981081888918655\n",
            "Epoch: 023, Loss: 2.4887, Train: 0.2521, Test: 0.2327\n",
            "Early stopping:  0.36933992626592616\n",
            "Epoch: 024, Loss: 2.4165, Train: 0.3077, Test: 0.2918\n",
            "Early stopping:  0.2912139634576922\n",
            "Epoch: 025, Loss: 2.3698, Train: 0.3148, Test: 0.3020\n",
            "Early stopping:  0.18789465288961957\n",
            "Epoch: 026, Loss: 2.2991, Train: 0.3287, Test: 0.3115\n",
            "Early stopping:  0.12228229791865457\n",
            "Epoch: 027, Loss: 2.2384, Train: 0.3480, Test: 0.3294\n",
            "Early stopping:  0.09788647571482174\n",
            "Epoch: 028, Loss: 2.1760, Train: 0.3613, Test: 0.3485\n",
            "Early stopping:  0.09696087489740601\n",
            "Epoch: 029, Loss: 2.1287, Train: 0.3741, Test: 0.3589\n",
            "Early stopping:  0.0959128247287474\n",
            "Epoch: 030, Loss: 2.0851, Train: 0.3820, Test: 0.3670\n",
            "Early stopping:  0.0852979796794299\n",
            "Epoch: 031, Loss: 2.0481, Train: 0.3988, Test: 0.3818\n",
            "Early stopping:  0.07491440179219251\n",
            "Epoch: 032, Loss: 2.0213, Train: 0.4098, Test: 0.3975\n",
            "Early stopping:  0.06198067693441708\n",
            "Epoch: 033, Loss: 1.9970, Train: 0.4195, Test: 0.4041\n",
            "Early stopping:  0.052142082108219544\n",
            "Epoch: 034, Loss: 1.9740, Train: 0.4223, Test: 0.4029\n",
            "Early stopping:  0.04342065334911705\n",
            "Epoch: 035, Loss: 1.9483, Train: 0.4317, Test: 0.4090\n",
            "Early stopping:  0.03907429847362268\n",
            "Epoch: 036, Loss: 1.9146, Train: 0.4385, Test: 0.4071\n",
            "Early stopping:  0.04155969487988444\n",
            "Epoch: 037, Loss: 1.8883, Train: 0.4396, Test: 0.4081\n",
            "Early stopping:  0.043838120032854616\n",
            "Epoch: 038, Loss: 1.8662, Train: 0.4464, Test: 0.4126\n",
            "Early stopping:  0.04367910707395359\n",
            "Epoch: 039, Loss: 1.8457, Train: 0.4495, Test: 0.4149\n",
            "Early stopping:  0.04030713068103122\n",
            "Epoch: 040, Loss: 1.8256, Train: 0.4535, Test: 0.4188\n",
            "Early stopping:  0.03494940682161561\n",
            "Epoch: 041, Loss: 1.8040, Train: 0.4563, Test: 0.4249\n",
            "Early stopping:  0.033093123608223654\n",
            "Epoch: 042, Loss: 1.7847, Train: 0.4628, Test: 0.4315\n",
            "Early stopping:  0.032369484063974004\n",
            "Epoch: 043, Loss: 1.7633, Train: 0.4688, Test: 0.4349\n",
            "Early stopping:  0.03251956765287159\n",
            "Epoch: 044, Loss: 1.7430, Train: 0.4685, Test: 0.4373\n",
            "Early stopping:  0.032547409253434356\n",
            "Epoch: 045, Loss: 1.7241, Train: 0.4697, Test: 0.4358\n",
            "Early stopping:  0.03187673195978455\n",
            "Epoch: 046, Loss: 1.7069, Train: 0.4750, Test: 0.4390\n",
            "Early stopping:  0.030846179996962722\n",
            "Epoch: 047, Loss: 1.6898, Train: 0.4759, Test: 0.4404\n",
            "Early stopping:  0.02898377229155612\n",
            "Epoch: 048, Loss: 1.6739, Train: 0.4796, Test: 0.4409\n",
            "Early stopping:  0.027261587531091327\n",
            "Epoch: 049, Loss: 1.6543, Train: 0.4773, Test: 0.4404\n",
            "Early stopping:  0.02728707667198393\n",
            "Epoch: 050, Loss: 1.6367, Train: 0.4861, Test: 0.4479\n",
            "Early stopping:  0.027813585717466522\n",
            "Epoch: 051, Loss: 1.6159, Train: 0.4946, Test: 0.4574\n",
            "Early stopping:  0.02928672414406739\n",
            "Epoch: 052, Loss: 1.5999, Train: 0.5017, Test: 0.4636\n",
            "Early stopping:  0.0295201270805019\n",
            "Epoch: 053, Loss: 1.5809, Train: 0.5028, Test: 0.4659\n",
            "Early stopping:  0.029052668281104133\n",
            "Epoch: 054, Loss: 1.5695, Train: 0.5074, Test: 0.4734\n",
            "Early stopping:  0.026900243187250147\n",
            "Epoch: 055, Loss: 1.5490, Train: 0.5133, Test: 0.4750\n",
            "Early stopping:  0.02602915068309583\n",
            "Epoch: 056, Loss: 1.5358, Train: 0.5173, Test: 0.4776\n",
            "Early stopping:  0.025403634025011328\n",
            "Epoch: 057, Loss: 1.5172, Train: 0.5187, Test: 0.4804\n",
            "Early stopping:  0.02557924217857672\n",
            "Epoch: 058, Loss: 1.5045, Train: 0.5227, Test: 0.4806\n",
            "Early stopping:  0.025656572052043806\n",
            "Epoch: 059, Loss: 1.4886, Train: 0.5272, Test: 0.4838\n",
            "Early stopping:  0.02406286840614713\n",
            "Epoch: 060, Loss: 1.4794, Train: 0.5261, Test: 0.4844\n",
            "Early stopping:  0.022479226815226332\n",
            "Epoch: 061, Loss: 1.4649, Train: 0.5303, Test: 0.4888\n",
            "Early stopping:  0.02055556377671964\n",
            "Epoch: 062, Loss: 1.4533, Train: 0.5360, Test: 0.4910\n",
            "Early stopping:  0.019984066835851064\n",
            "Epoch: 063, Loss: 1.4415, Train: 0.5411, Test: 0.4944\n",
            "Early stopping:  0.019050945865205837\n",
            "Epoch: 064, Loss: 1.4271, Train: 0.5445, Test: 0.4950\n",
            "Early stopping:  0.02025925455508844\n",
            "Epoch: 065, Loss: 1.4152, Train: 0.5496, Test: 0.4982\n",
            "Early stopping:  0.019896396977464783\n",
            "Epoch: 066, Loss: 1.4020, Train: 0.5510, Test: 0.5016\n",
            "Early stopping:  0.020382512281722973\n",
            "Epoch: 067, Loss: 1.3909, Train: 0.5559, Test: 0.5012\n",
            "Early stopping:  0.019957601335363655\n",
            "Epoch: 068, Loss: 1.3775, Train: 0.5579, Test: 0.5063\n",
            "Early stopping:  0.01952359038925036\n",
            "Epoch: 069, Loss: 1.3647, Train: 0.5581, Test: 0.5063\n",
            "Early stopping:  0.01984269631501057\n",
            "Epoch: 070, Loss: 1.3540, Train: 0.5615, Test: 0.5071\n",
            "Early stopping:  0.019345099097747034\n",
            "Epoch: 071, Loss: 1.3436, Train: 0.5624, Test: 0.5086\n",
            "Early stopping:  0.01868896721039502\n",
            "Epoch: 072, Loss: 1.3327, Train: 0.5593, Test: 0.5105\n",
            "Early stopping:  0.017492971703635427\n",
            "Epoch: 073, Loss: 1.3210, Train: 0.5655, Test: 0.5129\n",
            "Early stopping:  0.017179697066936575\n",
            "Epoch: 074, Loss: 1.3088, Train: 0.5689, Test: 0.5154\n",
            "Early stopping:  0.01787687386986277\n",
            "Epoch: 075, Loss: 1.2979, Train: 0.5729, Test: 0.5201\n",
            "Early stopping:  0.018241290780166924\n",
            "Epoch: 076, Loss: 1.2881, Train: 0.5786, Test: 0.5216\n",
            "Early stopping:  0.01779776100730207\n",
            "Epoch: 077, Loss: 1.2782, Train: 0.5831, Test: 0.5224\n",
            "Early stopping:  0.016860134039331627\n",
            "Epoch: 078, Loss: 1.2686, Train: 0.5845, Test: 0.5266\n",
            "Early stopping:  0.01584610112847644\n",
            "Epoch: 079, Loss: 1.2601, Train: 0.5879, Test: 0.5281\n",
            "Early stopping:  0.015045268395916769\n",
            "Epoch: 080, Loss: 1.2530, Train: 0.5893, Test: 0.5284\n",
            "Early stopping:  0.013969310205536685\n",
            "Epoch: 081, Loss: 1.2447, Train: 0.5942, Test: 0.5313\n",
            "Early stopping:  0.01307430054581506\n",
            "Epoch: 082, Loss: 1.2332, Train: 0.6007, Test: 0.5328\n",
            "Early stopping:  0.013689642491658048\n",
            "Epoch: 083, Loss: 1.2216, Train: 0.6044, Test: 0.5343\n",
            "Early stopping:  0.015408792305317411\n",
            "Epoch: 084, Loss: 1.2139, Train: 0.6066, Test: 0.5404\n",
            "Early stopping:  0.016070397949062648\n",
            "Epoch: 085, Loss: 1.2094, Train: 0.6089, Test: 0.5341\n",
            "Early stopping:  0.014425052495268053\n",
            "Epoch: 086, Loss: 1.2064, Train: 0.6151, Test: 0.5430\n",
            "Early stopping:  0.010765349734016853\n",
            "Epoch: 087, Loss: 1.1896, Train: 0.6171, Test: 0.5451\n",
            "Early stopping:  0.011830415902515578\n",
            "Epoch: 088, Loss: 1.1846, Train: 0.6228, Test: 0.5434\n",
            "Early stopping:  0.012882022341931814\n",
            "Epoch: 089, Loss: 1.1845, Train: 0.6248, Test: 0.5492\n",
            "Early stopping:  0.012083984992389236\n",
            "Epoch: 090, Loss: 1.1657, Train: 0.6242, Test: 0.5534\n",
            "Early stopping:  0.014531417556542773\n",
            "Epoch: 091, Loss: 1.1640, Train: 0.6310, Test: 0.5455\n",
            "Early stopping:  0.011908453202962684\n",
            "Epoch: 092, Loss: 1.1582, Train: 0.6370, Test: 0.5543\n",
            "Early stopping:  0.012326669452530803\n",
            "Epoch: 093, Loss: 1.1409, Train: 0.6336, Test: 0.5596\n",
            "Early stopping:  0.015661720945911055\n",
            "Epoch: 094, Loss: 1.1416, Train: 0.6378, Test: 0.5464\n",
            "Early stopping:  0.012020553773441996\n",
            "Epoch: 095, Loss: 1.1380, Train: 0.6424, Test: 0.5536\n",
            "Early stopping:  0.011709542576120105\n",
            "Epoch: 096, Loss: 1.1247, Train: 0.6356, Test: 0.5602\n",
            "Early stopping:  0.011925768883716199\n",
            "Epoch: 097, Loss: 1.1318, Train: 0.6452, Test: 0.5553\n",
            "Early stopping:  0.007143385325796712\n",
            "PREDICTIONS -> tensor([ 9,  0, 11,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.52      0.50       568\n",
            "         capital_goods       0.37      0.33      0.35       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.46      0.44      0.45       595\n",
            " consumer_non-cyclical       0.67      0.36      0.47       334\n",
            "                energy       0.50      0.44      0.47       213\n",
            "             financial       0.69      0.60      0.64       576\n",
            "            healthcare       0.59      0.58      0.58       238\n",
            "              services       0.57      0.77      0.66      1557\n",
            "            technology       0.51      0.17      0.26       297\n",
            "        transportation       0.70      0.69      0.69       303\n",
            "             utilities       0.61      0.56      0.59       169\n",
            "\n",
            "              accuracy                           0.56      5291\n",
            "             macro avg       0.51      0.46      0.47      5291\n",
            "          weighted avg       0.55      0.56      0.54      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 65.8327, Train: 0.2629, Test: 0.2586\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 284.4404, Train: 0.0834, Test: 0.0841\n",
            "Early stopping:  154.5789576642158\n",
            "Epoch: 003, Loss: 444.1395, Train: 0.0743, Test: 0.0745\n",
            "Early stopping:  189.91627258312042\n",
            "Epoch: 004, Loss: 426.9696, Train: 0.1531, Test: 0.1529\n",
            "Early stopping:  174.98532282913413\n",
            "Epoch: 005, Loss: 255.3884, Train: 0.2819, Test: 0.2844\n",
            "Early stopping:  153.17976653559813\n",
            "Epoch: 006, Loss: 322.4883, Train: 0.2399, Test: 0.2442\n",
            "Early stopping:  84.76133832452409\n",
            "Epoch: 007, Loss: 279.8001, Train: 0.1018, Test: 0.0947\n",
            "Early stopping:  85.63415797421908\n",
            "Epoch: 008, Loss: 249.2252, Train: 0.0794, Test: 0.0748\n",
            "Early stopping:  73.09900908412011\n",
            "Epoch: 009, Loss: 220.7157, Train: 0.2672, Test: 0.2642\n",
            "Early stopping:  38.160551428691356\n",
            "Epoch: 010, Loss: 159.8905, Train: 0.1605, Test: 0.1605\n",
            "Early stopping:  61.352567268954076\n",
            "Epoch: 011, Loss: 121.7891, Train: 0.1154, Test: 0.1106\n",
            "Early stopping:  64.70814899048578\n",
            "Epoch: 012, Loss: 83.0197, Train: 0.0905, Test: 0.0815\n",
            "Early stopping:  68.54485228784294\n",
            "Epoch: 013, Loss: 54.0287, Train: 0.0825, Test: 0.0820\n",
            "Early stopping:  65.47270274202262\n",
            "Epoch: 014, Loss: 31.8533, Train: 0.1912, Test: 0.1879\n",
            "Early stopping:  51.52393539161192\n",
            "Epoch: 015, Loss: 16.4810, Train: 0.1682, Test: 0.1635\n",
            "Early stopping:  42.009843487443966\n",
            "Epoch: 016, Loss: 5.8334, Train: 0.1438, Test: 0.1387\n",
            "Early stopping:  30.898780257004372\n",
            "Epoch: 017, Loss: 3.7923, Train: 0.1492, Test: 0.1444\n",
            "Early stopping:  20.889413685136848\n",
            "Epoch: 018, Loss: 3.2350, Train: 0.1500, Test: 0.1487\n",
            "Early stopping:  12.208255513208028\n",
            "Epoch: 019, Loss: 2.7926, Train: 0.1410, Test: 0.1425\n",
            "Early stopping:  5.7396490183587225\n",
            "Epoch: 020, Loss: 2.5405, Train: 0.1486, Test: 0.1574\n",
            "Early stopping:  1.315799910373923\n",
            "Epoch: 021, Loss: 2.4231, Train: 0.2136, Test: 0.2073\n",
            "Early stopping:  0.5613927600439192\n",
            "Epoch: 022, Loss: 2.3597, Train: 0.2813, Test: 0.2788\n",
            "Early stopping:  0.3564624771404989\n",
            "Epoch: 023, Loss: 2.3350, Train: 0.2853, Test: 0.2824\n",
            "Early stopping:  0.18678836052106557\n",
            "Epoch: 024, Loss: 2.3320, Train: 0.2938, Test: 0.2894\n",
            "Early stopping:  0.08761553049802194\n",
            "Epoch: 025, Loss: 2.3358, Train: 0.2947, Test: 0.2899\n",
            "Early stopping:  0.03849996230833919\n",
            "Epoch: 026, Loss: 2.3370, Train: 0.2947, Test: 0.2916\n",
            "Early stopping:  0.01122347833143579\n",
            "Epoch: 027, Loss: 2.3358, Train: 0.2933, Test: 0.2930\n",
            "Early stopping:  0.001879978719043224\n",
            "PREDICTIONS -> tensor([0, 8, 9,  ..., 8, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.52      0.04      0.08       568\n",
            "         capital_goods       0.00      0.00      0.00       381\n",
            "conglomerates_industry       0.00      0.00      0.00        60\n",
            "     consumer_cyclical       0.00      0.00      0.00       595\n",
            " consumer_non-cyclical       0.00      0.00      0.00       334\n",
            "                energy       0.13      0.08      0.09       213\n",
            "             financial       0.00      0.00      0.00       576\n",
            "            healthcare       0.29      0.11      0.15       238\n",
            "              services       0.30      0.93      0.46      1557\n",
            "            technology       0.11      0.08      0.09       297\n",
            "        transportation       0.71      0.02      0.03       303\n",
            "             utilities       0.00      0.00      0.00       169\n",
            "\n",
            "              accuracy                           0.29      5291\n",
            "             macro avg       0.17      0.10      0.08      5291\n",
            "          weighted avg       0.21      0.29      0.16      5291\n",
            "\n",
            "time: 2min 4s (started: 2024-10-16 21:41:24 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lopb6RLBgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f024cb6d-71a7-403e-adc7-d7b1c21674e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 530 ms (started: 2024-10-16 21:43:28 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVfTPRH3gWjL"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZWGV0SxgWjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc67823-05d4-4eee-e4e6-97901b2ee80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4742, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2547, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15525465998563992\n",
            "Epoch: 003, Loss: 2.1587, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1617309272398368\n",
            "Epoch: 004, Loss: 2.1243, Train: 0.2989, Test: 0.2990\n",
            "Early stopping:  0.15746496134341417\n",
            "Epoch: 005, Loss: 2.0498, Train: 0.3446, Test: 0.3391\n",
            "Early stopping:  0.16386624540705622\n",
            "Epoch: 006, Loss: 1.9798, Train: 0.3965, Test: 0.3859\n",
            "Early stopping:  0.10489401807904415\n",
            "Epoch: 007, Loss: 1.9162, Train: 0.4149, Test: 0.3973\n",
            "Early stopping:  0.10011262414023951\n",
            "Epoch: 008, Loss: 1.8514, Train: 0.4243, Test: 0.4060\n",
            "Early stopping:  0.10750479258142429\n",
            "Epoch: 009, Loss: 1.7870, Train: 0.4348, Test: 0.4184\n",
            "Early stopping:  0.10344096988853532\n",
            "Epoch: 010, Loss: 1.7208, Train: 0.4583, Test: 0.4407\n",
            "Early stopping:  0.10235765843513896\n",
            "Epoch: 011, Loss: 1.6510, Train: 0.4853, Test: 0.4644\n",
            "Early stopping:  0.10452182573146597\n",
            "Epoch: 012, Loss: 1.5843, Train: 0.5091, Test: 0.4774\n",
            "Early stopping:  0.10596292203251825\n",
            "Epoch: 013, Loss: 1.5244, Train: 0.5261, Test: 0.4948\n",
            "Early stopping:  0.10464219596680652\n",
            "Epoch: 014, Loss: 1.4625, Train: 0.5451, Test: 0.5107\n",
            "Early stopping:  0.10174371704006892\n",
            "Epoch: 015, Loss: 1.4020, Train: 0.5570, Test: 0.5216\n",
            "Early stopping:  0.09802389375139105\n",
            "Epoch: 016, Loss: 1.3484, Train: 0.5837, Test: 0.5430\n",
            "Early stopping:  0.09399189474078974\n",
            "Epoch: 017, Loss: 1.2967, Train: 0.6021, Test: 0.5619\n",
            "Early stopping:  0.09013155215719658\n",
            "Epoch: 018, Loss: 1.2487, Train: 0.6183, Test: 0.5778\n",
            "Early stopping:  0.08432964624794877\n",
            "Epoch: 019, Loss: 1.2075, Train: 0.6276, Test: 0.5857\n",
            "Early stopping:  0.07736378391041004\n",
            "Epoch: 020, Loss: 1.1707, Train: 0.6398, Test: 0.5936\n",
            "Early stopping:  0.07046621388900957\n",
            "Epoch: 021, Loss: 1.1350, Train: 0.6446, Test: 0.5988\n",
            "Early stopping:  0.0635995347901405\n",
            "Epoch: 022, Loss: 1.1021, Train: 0.6478, Test: 0.6044\n",
            "Early stopping:  0.05788176339623041\n",
            "Epoch: 023, Loss: 1.0747, Train: 0.6571, Test: 0.6059\n",
            "Early stopping:  0.0529195528087224\n",
            "Epoch: 024, Loss: 1.0484, Train: 0.6685, Test: 0.6144\n",
            "Early stopping:  0.048313579069359736\n",
            "Epoch: 025, Loss: 1.0203, Train: 0.6807, Test: 0.6207\n",
            "Early stopping:  0.044802158779111007\n",
            "Epoch: 026, Loss: 0.9947, Train: 0.6912, Test: 0.6245\n",
            "Early stopping:  0.04256902212518261\n",
            "Epoch: 027, Loss: 0.9716, Train: 0.6974, Test: 0.6294\n",
            "Early stopping:  0.041101415104712866\n",
            "Epoch: 028, Loss: 0.9480, Train: 0.7082, Test: 0.6350\n",
            "Early stopping:  0.03947910137969383\n",
            "Epoch: 029, Loss: 0.9255, Train: 0.7147, Test: 0.6405\n",
            "Early stopping:  0.03737655253884958\n",
            "Epoch: 030, Loss: 0.9039, Train: 0.7240, Test: 0.6469\n",
            "Early stopping:  0.036034169944296424\n",
            "Epoch: 031, Loss: 0.8832, Train: 0.7331, Test: 0.6502\n",
            "Early stopping:  0.03494957305212455\n",
            "Epoch: 032, Loss: 0.8617, Train: 0.7419, Test: 0.6549\n",
            "Early stopping:  0.033949804620672756\n",
            "Epoch: 033, Loss: 0.8411, Train: 0.7521, Test: 0.6609\n",
            "Early stopping:  0.033351688555094086\n",
            "Epoch: 034, Loss: 0.8211, Train: 0.7612, Test: 0.6651\n",
            "Early stopping:  0.032848062420896953\n",
            "Epoch: 035, Loss: 0.8017, Train: 0.7686, Test: 0.6706\n",
            "Early stopping:  0.03220269372047673\n",
            "Epoch: 036, Loss: 0.7833, Train: 0.7745, Test: 0.6732\n",
            "Early stopping:  0.03104783122297491\n",
            "Epoch: 037, Loss: 0.7654, Train: 0.7813, Test: 0.6749\n",
            "Early stopping:  0.029930292202045346\n",
            "Epoch: 038, Loss: 0.7478, Train: 0.7864, Test: 0.6783\n",
            "Early stopping:  0.028916829981768893\n",
            "Epoch: 039, Loss: 0.7311, Train: 0.7938, Test: 0.6823\n",
            "Early stopping:  0.027940637125298425\n",
            "Epoch: 040, Loss: 0.7136, Train: 0.8001, Test: 0.6849\n",
            "Early stopping:  0.027450346952931214\n",
            "Epoch: 041, Loss: 0.6967, Train: 0.8077, Test: 0.6864\n",
            "Early stopping:  0.02711919346750494\n",
            "Epoch: 042, Loss: 0.6801, Train: 0.8125, Test: 0.6900\n",
            "Early stopping:  0.026866923026851286\n",
            "Epoch: 043, Loss: 0.6632, Train: 0.8210, Test: 0.6917\n",
            "Early stopping:  0.02677655154366402\n",
            "Epoch: 044, Loss: 0.6467, Train: 0.8261, Test: 0.6927\n",
            "Early stopping:  0.026451682026040802\n",
            "Epoch: 045, Loss: 0.6302, Train: 0.8296, Test: 0.6951\n",
            "Early stopping:  0.026324712349950617\n",
            "Epoch: 046, Loss: 0.6137, Train: 0.8358, Test: 0.6984\n",
            "Early stopping:  0.0262212867611748\n",
            "Epoch: 047, Loss: 0.5972, Train: 0.8417, Test: 0.6997\n",
            "Early stopping:  0.026084577921236096\n",
            "Epoch: 048, Loss: 0.5813, Train: 0.8457, Test: 0.7021\n",
            "Early stopping:  0.025871625126069466\n",
            "Epoch: 049, Loss: 0.5654, Train: 0.8534, Test: 0.7025\n",
            "Early stopping:  0.025595372450450814\n",
            "Epoch: 050, Loss: 0.5496, Train: 0.8562, Test: 0.7033\n",
            "Early stopping:  0.025283881056606304\n",
            "Epoch: 051, Loss: 0.5345, Train: 0.8659, Test: 0.7052\n",
            "Early stopping:  0.024840050290929738\n",
            "Epoch: 052, Loss: 0.5207, Train: 0.8610, Test: 0.7029\n",
            "Early stopping:  0.024050581320220515\n",
            "Epoch: 053, Loss: 0.5116, Train: 0.8695, Test: 0.7069\n",
            "Early stopping:  0.021666022964444743\n",
            "Epoch: 054, Loss: 0.5050, Train: 0.8746, Test: 0.7074\n",
            "Early stopping:  0.017989505338502545\n",
            "Epoch: 055, Loss: 0.4827, Train: 0.8832, Test: 0.7135\n",
            "Early stopping:  0.019263948771845424\n",
            "Epoch: 056, Loss: 0.4625, Train: 0.8880, Test: 0.7116\n",
            "Early stopping:  0.023627178172674904\n",
            "Epoch: 057, Loss: 0.4577, Train: 0.8866, Test: 0.7139\n",
            "Early stopping:  0.024268123028391396\n",
            "Epoch: 058, Loss: 0.4431, Train: 0.8996, Test: 0.7176\n",
            "Early stopping:  0.024051841250794764\n",
            "Epoch: 059, Loss: 0.4242, Train: 0.9050, Test: 0.7150\n",
            "Early stopping:  0.021892719514047664\n",
            "Epoch: 060, Loss: 0.4178, Train: 0.9019, Test: 0.7180\n",
            "Early stopping:  0.019798582991379755\n",
            "Epoch: 061, Loss: 0.4055, Train: 0.9155, Test: 0.7186\n",
            "Early stopping:  0.02076113466671258\n",
            "Epoch: 062, Loss: 0.3890, Train: 0.9223, Test: 0.7173\n",
            "Early stopping:  0.020276729029131992\n",
            "Epoch: 063, Loss: 0.3829, Train: 0.9121, Test: 0.7180\n",
            "Early stopping:  0.017829840075245386\n",
            "Epoch: 064, Loss: 0.3755, Train: 0.9243, Test: 0.7203\n",
            "Early stopping:  0.017227240926493866\n",
            "Epoch: 065, Loss: 0.3670, Train: 0.9217, Test: 0.7099\n",
            "Early stopping:  0.014568335377720964\n",
            "Epoch: 066, Loss: 0.3638, Train: 0.9257, Test: 0.7197\n",
            "Early stopping:  0.010555145202603183\n",
            "Epoch: 067, Loss: 0.3443, Train: 0.9359, Test: 0.7242\n",
            "Early stopping:  0.01458590797997\n",
            "Epoch: 068, Loss: 0.3295, Train: 0.9345, Test: 0.7133\n",
            "Early stopping:  0.018709908320198643\n",
            "Epoch: 069, Loss: 0.3321, Train: 0.9433, Test: 0.7259\n",
            "Early stopping:  0.01743806391073915\n",
            "Epoch: 070, Loss: 0.3132, Train: 0.9433, Test: 0.7259\n",
            "Early stopping:  0.018827603588452688\n",
            "Epoch: 071, Loss: 0.3064, Train: 0.9470, Test: 0.7171\n",
            "Early stopping:  0.015232019452797594\n",
            "Epoch: 072, Loss: 0.3018, Train: 0.9521, Test: 0.7259\n",
            "Early stopping:  0.0136283728921602\n",
            "Epoch: 073, Loss: 0.2869, Train: 0.9472, Test: 0.7242\n",
            "Early stopping:  0.016531120210632466\n",
            "Epoch: 074, Loss: 0.2859, Train: 0.9558, Test: 0.7222\n",
            "Early stopping:  0.012042756749969463\n",
            "Epoch: 075, Loss: 0.2732, Train: 0.9594, Test: 0.7222\n",
            "Early stopping:  0.013342644022478018\n",
            "Epoch: 076, Loss: 0.2673, Train: 0.9558, Test: 0.7224\n",
            "Early stopping:  0.01341478792502306\n",
            "Epoch: 077, Loss: 0.2628, Train: 0.9654, Test: 0.7254\n",
            "Early stopping:  0.01085801923821535\n",
            "Epoch: 078, Loss: 0.2512, Train: 0.9626, Test: 0.7214\n",
            "Early stopping:  0.012832070945076757\n",
            "Epoch: 079, Loss: 0.2492, Train: 0.9668, Test: 0.7248\n",
            "Early stopping:  0.010351389041049664\n",
            "Epoch: 080, Loss: 0.2407, Train: 0.9671, Test: 0.7235\n",
            "Early stopping:  0.01076483675601429\n",
            "Epoch: 081, Loss: 0.2336, Train: 0.9716, Test: 0.7216\n",
            "Early stopping:  0.011079676692029455\n",
            "Epoch: 082, Loss: 0.2317, Train: 0.9688, Test: 0.7216\n",
            "Early stopping:  0.008802798174059167\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.72      0.72       568\n",
            "         capital_goods       0.71      0.60      0.65       381\n",
            "conglomerates_industry       1.00      0.43      0.60        60\n",
            "     consumer_cyclical       0.71      0.66      0.68       595\n",
            " consumer_non-cyclical       0.73      0.59      0.65       334\n",
            "                energy       0.81      0.71      0.76       213\n",
            "             financial       0.79      0.74      0.76       576\n",
            "            healthcare       0.81      0.75      0.78       238\n",
            "              services       0.67      0.85      0.75      1557\n",
            "            technology       0.72      0.51      0.59       297\n",
            "        transportation       0.81      0.76      0.78       303\n",
            "             utilities       0.81      0.67      0.74       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.67      0.71      5291\n",
            "          weighted avg       0.73      0.72      0.72      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4859, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2691, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15334894808337132\n",
            "Epoch: 003, Loss: 2.1631, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16457436622362642\n",
            "Epoch: 004, Loss: 2.1246, Train: 0.2998, Test: 0.3015\n",
            "Early stopping:  0.16212747970454544\n",
            "Epoch: 005, Loss: 2.0605, Train: 0.3488, Test: 0.3485\n",
            "Early stopping:  0.16652056137727106\n",
            "Epoch: 006, Loss: 2.0061, Train: 0.3834, Test: 0.3893\n",
            "Early stopping:  0.1006285362575647\n",
            "Epoch: 007, Loss: 1.9413, Train: 0.3968, Test: 0.3916\n",
            "Early stopping:  0.08915583535179115\n",
            "Epoch: 008, Loss: 1.8717, Train: 0.3951, Test: 0.3920\n",
            "Early stopping:  0.09890518019460735\n",
            "Epoch: 009, Loss: 1.8099, Train: 0.4104, Test: 0.4005\n",
            "Early stopping:  0.10057217079505248\n",
            "Epoch: 010, Loss: 1.7529, Train: 0.4370, Test: 0.4241\n",
            "Early stopping:  0.100900043328471\n",
            "Epoch: 011, Loss: 1.6897, Train: 0.4708, Test: 0.4493\n",
            "Early stopping:  0.0983758826136071\n",
            "Epoch: 012, Loss: 1.6206, Train: 0.5068, Test: 0.4770\n",
            "Early stopping:  0.09847110973128896\n",
            "Epoch: 013, Loss: 1.5538, Train: 0.5286, Test: 0.5007\n",
            "Early stopping:  0.10196827975318415\n",
            "Epoch: 014, Loss: 1.4935, Train: 0.5462, Test: 0.5131\n",
            "Early stopping:  0.1035502687998221\n",
            "Epoch: 015, Loss: 1.4372, Train: 0.5607, Test: 0.5330\n",
            "Early stopping:  0.10005190091186666\n",
            "Epoch: 016, Loss: 1.3818, Train: 0.5757, Test: 0.5424\n",
            "Early stopping:  0.09402888550964031\n",
            "Epoch: 017, Loss: 1.3290, Train: 0.5944, Test: 0.5515\n",
            "Early stopping:  0.08879833986315049\n",
            "Epoch: 018, Loss: 1.2828, Train: 0.6061, Test: 0.5645\n",
            "Early stopping:  0.08381352388920961\n",
            "Epoch: 019, Loss: 1.2426, Train: 0.6143, Test: 0.5772\n",
            "Early stopping:  0.07733786041478209\n",
            "Epoch: 020, Loss: 1.2039, Train: 0.6293, Test: 0.5857\n",
            "Early stopping:  0.07004706450880513\n",
            "Epoch: 021, Loss: 1.1675, Train: 0.6393, Test: 0.5940\n",
            "Early stopping:  0.06358667480553269\n",
            "Epoch: 022, Loss: 1.1345, Train: 0.6495, Test: 0.6018\n",
            "Early stopping:  0.05879278954939477\n",
            "Epoch: 023, Loss: 1.1034, Train: 0.6529, Test: 0.6046\n",
            "Early stopping:  0.05505618023918297\n",
            "Epoch: 024, Loss: 1.0744, Train: 0.6608, Test: 0.6091\n",
            "Early stopping:  0.05115900857610955\n",
            "Epoch: 025, Loss: 1.0460, Train: 0.6719, Test: 0.6133\n",
            "Early stopping:  0.04796453579196832\n",
            "Epoch: 026, Loss: 1.0187, Train: 0.6778, Test: 0.6184\n",
            "Early stopping:  0.04572585108549506\n",
            "Epoch: 027, Loss: 0.9926, Train: 0.6855, Test: 0.6250\n",
            "Early stopping:  0.04384898662337983\n",
            "Epoch: 028, Loss: 0.9685, Train: 0.6963, Test: 0.6316\n",
            "Early stopping:  0.04194945901761596\n",
            "Epoch: 029, Loss: 0.9444, Train: 0.7059, Test: 0.6330\n",
            "Early stopping:  0.040079868744519\n",
            "Epoch: 030, Loss: 0.9216, Train: 0.7172, Test: 0.6394\n",
            "Early stopping:  0.03832864250678471\n",
            "Epoch: 031, Loss: 0.9000, Train: 0.7277, Test: 0.6422\n",
            "Early stopping:  0.036705211423582046\n",
            "Epoch: 032, Loss: 0.8792, Train: 0.7334, Test: 0.6486\n",
            "Early stopping:  0.035270346426940685\n",
            "Epoch: 033, Loss: 0.8580, Train: 0.7377, Test: 0.6543\n",
            "Early stopping:  0.03403528159916882\n",
            "Epoch: 034, Loss: 0.8376, Train: 0.7484, Test: 0.6581\n",
            "Early stopping:  0.033210931970303136\n",
            "Epoch: 035, Loss: 0.8181, Train: 0.7606, Test: 0.6617\n",
            "Early stopping:  0.03248873020615107\n",
            "Epoch: 036, Loss: 0.7982, Train: 0.7691, Test: 0.6666\n",
            "Early stopping:  0.0319259702641097\n",
            "Epoch: 037, Loss: 0.7791, Train: 0.7771, Test: 0.6711\n",
            "Early stopping:  0.03119029330549369\n",
            "Epoch: 038, Loss: 0.7611, Train: 0.7850, Test: 0.6749\n",
            "Early stopping:  0.030352189516320204\n",
            "Epoch: 039, Loss: 0.7428, Train: 0.7918, Test: 0.6789\n",
            "Early stopping:  0.029689377737961747\n",
            "Epoch: 040, Loss: 0.7252, Train: 0.7955, Test: 0.6842\n",
            "Early stopping:  0.02884712715081146\n",
            "Epoch: 041, Loss: 0.7081, Train: 0.8032, Test: 0.6866\n",
            "Early stopping:  0.028128588200047433\n",
            "Epoch: 042, Loss: 0.6912, Train: 0.8103, Test: 0.6874\n",
            "Early stopping:  0.027588220708851023\n",
            "Epoch: 043, Loss: 0.6744, Train: 0.8145, Test: 0.6902\n",
            "Early stopping:  0.026995465620408055\n",
            "Epoch: 044, Loss: 0.6576, Train: 0.8227, Test: 0.6917\n",
            "Early stopping:  0.02668845231994489\n",
            "Epoch: 045, Loss: 0.6412, Train: 0.8270, Test: 0.6925\n",
            "Early stopping:  0.026471926614370995\n",
            "Epoch: 046, Loss: 0.6245, Train: 0.8332, Test: 0.6967\n",
            "Early stopping:  0.026329750433032685\n",
            "Epoch: 047, Loss: 0.6085, Train: 0.8378, Test: 0.7006\n",
            "Early stopping:  0.026083461784182235\n",
            "Epoch: 048, Loss: 0.5928, Train: 0.8440, Test: 0.7008\n",
            "Early stopping:  0.025686757055712325\n",
            "Epoch: 049, Loss: 0.5771, Train: 0.8522, Test: 0.7052\n",
            "Early stopping:  0.025300058597729897\n",
            "Epoch: 050, Loss: 0.5617, Train: 0.8573, Test: 0.7053\n",
            "Early stopping:  0.024827320718630582\n",
            "Epoch: 051, Loss: 0.5464, Train: 0.8625, Test: 0.7086\n",
            "Early stopping:  0.024550128959025114\n",
            "Epoch: 052, Loss: 0.5310, Train: 0.8678, Test: 0.7086\n",
            "Early stopping:  0.024389956721906306\n",
            "Epoch: 053, Loss: 0.5164, Train: 0.8727, Test: 0.7097\n",
            "Early stopping:  0.024033156289372946\n",
            "Epoch: 054, Loss: 0.5025, Train: 0.8721, Test: 0.7072\n",
            "Early stopping:  0.023449401975191256\n",
            "Epoch: 055, Loss: 0.4906, Train: 0.8806, Test: 0.7078\n",
            "Early stopping:  0.022160320040822187\n",
            "Epoch: 056, Loss: 0.4804, Train: 0.8763, Test: 0.7086\n",
            "Early stopping:  0.02012573176685896\n",
            "Epoch: 057, Loss: 0.4694, Train: 0.8908, Test: 0.7140\n",
            "Early stopping:  0.018400839146760276\n",
            "Epoch: 058, Loss: 0.4523, Train: 0.8936, Test: 0.7150\n",
            "Early stopping:  0.019354853475002824\n",
            "Epoch: 059, Loss: 0.4356, Train: 0.8968, Test: 0.7108\n",
            "Early stopping:  0.02202092566927265\n",
            "Epoch: 060, Loss: 0.4266, Train: 0.9056, Test: 0.7140\n",
            "Early stopping:  0.022481165457481755\n",
            "Epoch: 061, Loss: 0.4191, Train: 0.9022, Test: 0.7137\n",
            "Early stopping:  0.020283499990214806\n",
            "Epoch: 062, Loss: 0.4059, Train: 0.9126, Test: 0.7165\n",
            "Early stopping:  0.017431316856091453\n",
            "Epoch: 063, Loss: 0.3907, Train: 0.9192, Test: 0.7190\n",
            "Early stopping:  0.017665348464961032\n",
            "Epoch: 064, Loss: 0.3818, Train: 0.9104, Test: 0.7135\n",
            "Early stopping:  0.01877647392383418\n",
            "Epoch: 065, Loss: 0.3755, Train: 0.9254, Test: 0.7178\n",
            "Early stopping:  0.017829572115939763\n",
            "Epoch: 066, Loss: 0.3637, Train: 0.9260, Test: 0.7176\n",
            "Early stopping:  0.015902239720875812\n",
            "Epoch: 067, Loss: 0.3506, Train: 0.9271, Test: 0.7193\n",
            "Early stopping:  0.015685854711766225\n",
            "Epoch: 068, Loss: 0.3419, Train: 0.9365, Test: 0.7201\n",
            "Early stopping:  0.01664946986368058\n",
            "Epoch: 069, Loss: 0.3356, Train: 0.9328, Test: 0.7174\n",
            "Early stopping:  0.016222214936148595\n",
            "Epoch: 070, Loss: 0.3272, Train: 0.9447, Test: 0.7191\n",
            "Early stopping:  0.014038865728440124\n",
            "Epoch: 071, Loss: 0.3158, Train: 0.9467, Test: 0.7220\n",
            "Early stopping:  0.01339871737384834\n",
            "Epoch: 072, Loss: 0.3060, Train: 0.9467, Test: 0.7203\n",
            "Early stopping:  0.014577485295638888\n",
            "Epoch: 073, Loss: 0.2991, Train: 0.9541, Test: 0.7203\n",
            "Early stopping:  0.014928171039267513\n",
            "Epoch: 074, Loss: 0.2933, Train: 0.9478, Test: 0.7199\n",
            "Early stopping:  0.013491868528098899\n",
            "Epoch: 075, Loss: 0.2872, Train: 0.9586, Test: 0.7197\n",
            "Early stopping:  0.01111824978249086\n",
            "Epoch: 076, Loss: 0.2790, Train: 0.9563, Test: 0.7191\n",
            "Early stopping:  0.010437718004259432\n",
            "Epoch: 077, Loss: 0.2696, Train: 0.9626, Test: 0.7225\n",
            "Early stopping:  0.011668608487909791\n",
            "Epoch: 078, Loss: 0.2605, Train: 0.9648, Test: 0.7222\n",
            "Early stopping:  0.013182058742057492\n",
            "Epoch: 079, Loss: 0.2536, Train: 0.9634, Test: 0.7218\n",
            "Early stopping:  0.013564555444115247\n",
            "Epoch: 080, Loss: 0.2487, Train: 0.9719, Test: 0.7186\n",
            "Early stopping:  0.012213424863015099\n",
            "Epoch: 081, Loss: 0.2443, Train: 0.9631, Test: 0.7190\n",
            "Early stopping:  0.009993168512887174\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.69      0.70      0.70       568\n",
            "         capital_goods       0.71      0.59      0.64       381\n",
            "conglomerates_industry       0.96      0.37      0.53        60\n",
            "     consumer_cyclical       0.70      0.63      0.67       595\n",
            " consumer_non-cyclical       0.76      0.59      0.66       334\n",
            "                energy       0.84      0.71      0.77       213\n",
            "             financial       0.77      0.75      0.76       576\n",
            "            healthcare       0.83      0.74      0.78       238\n",
            "              services       0.67      0.84      0.74      1557\n",
            "            technology       0.73      0.56      0.63       297\n",
            "        transportation       0.80      0.76      0.78       303\n",
            "             utilities       0.84      0.70      0.77       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.66      0.70      5291\n",
            "          weighted avg       0.73      0.72      0.72      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4862, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2567, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16229756695920436\n",
            "Epoch: 003, Loss: 2.1710, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16298918816838856\n",
            "Epoch: 004, Loss: 2.1189, Train: 0.2992, Test: 0.2994\n",
            "Early stopping:  0.1622799337927565\n",
            "Epoch: 005, Loss: 2.0476, Train: 0.3364, Test: 0.3321\n",
            "Early stopping:  0.16919516827529596\n",
            "Epoch: 006, Loss: 1.9867, Train: 0.3846, Test: 0.3755\n",
            "Early stopping:  0.10515344883936621\n",
            "Epoch: 007, Loss: 1.9277, Train: 0.4056, Test: 0.3888\n",
            "Early stopping:  0.09793633019983894\n",
            "Epoch: 008, Loss: 1.8614, Train: 0.4126, Test: 0.3988\n",
            "Early stopping:  0.1004271269294064\n",
            "Epoch: 009, Loss: 1.7914, Train: 0.4200, Test: 0.4065\n",
            "Early stopping:  0.10089248299077543\n",
            "Epoch: 010, Loss: 1.7254, Train: 0.4385, Test: 0.4258\n",
            "Early stopping:  0.10422164639750638\n",
            "Epoch: 011, Loss: 1.6625, Train: 0.4660, Test: 0.4508\n",
            "Early stopping:  0.10537275515041027\n",
            "Epoch: 012, Loss: 1.6004, Train: 0.4892, Test: 0.4649\n",
            "Early stopping:  0.1029558076397607\n",
            "Epoch: 013, Loss: 1.5411, Train: 0.5113, Test: 0.4821\n",
            "Early stopping:  0.09891778993993638\n",
            "Epoch: 014, Loss: 1.4815, Train: 0.5366, Test: 0.5014\n",
            "Early stopping:  0.09634858287326774\n",
            "Epoch: 015, Loss: 1.4232, Train: 0.5638, Test: 0.5235\n",
            "Early stopping:  0.09447385491141769\n",
            "Epoch: 016, Loss: 1.3690, Train: 0.5868, Test: 0.5424\n",
            "Early stopping:  0.09183060103596008\n",
            "Epoch: 017, Loss: 1.3176, Train: 0.5970, Test: 0.5534\n",
            "Early stopping:  0.08852655089883876\n",
            "Epoch: 018, Loss: 1.2708, Train: 0.6064, Test: 0.5621\n",
            "Early stopping:  0.08338649699238745\n",
            "Epoch: 019, Loss: 1.2279, Train: 0.6180, Test: 0.5770\n",
            "Early stopping:  0.07736409887921587\n",
            "Epoch: 020, Loss: 1.1870, Train: 0.6282, Test: 0.5859\n",
            "Early stopping:  0.07181489131063616\n",
            "Epoch: 021, Loss: 1.1499, Train: 0.6339, Test: 0.5929\n",
            "Early stopping:  0.06633874559710597\n",
            "Epoch: 022, Loss: 1.1173, Train: 0.6446, Test: 0.5991\n",
            "Early stopping:  0.060995980607940786\n",
            "Epoch: 023, Loss: 1.0887, Train: 0.6543, Test: 0.6027\n",
            "Early stopping:  0.05518925399249484\n",
            "Epoch: 024, Loss: 1.0584, Train: 0.6636, Test: 0.6095\n",
            "Early stopping:  0.050407524495851765\n",
            "Epoch: 025, Loss: 1.0305, Train: 0.6767, Test: 0.6177\n",
            "Early stopping:  0.04707735332058848\n",
            "Epoch: 026, Loss: 1.0046, Train: 0.6866, Test: 0.6222\n",
            "Early stopping:  0.04484205299140154\n",
            "Epoch: 027, Loss: 0.9796, Train: 0.6946, Test: 0.6301\n",
            "Early stopping:  0.04302542645965029\n",
            "Epoch: 028, Loss: 0.9545, Train: 0.7039, Test: 0.6366\n",
            "Early stopping:  0.04089400087837521\n",
            "Epoch: 029, Loss: 0.9303, Train: 0.7133, Test: 0.6437\n",
            "Early stopping:  0.03959779876227795\n",
            "Epoch: 030, Loss: 0.9075, Train: 0.7249, Test: 0.6483\n",
            "Early stopping:  0.03851035232145892\n",
            "Epoch: 031, Loss: 0.8860, Train: 0.7345, Test: 0.6528\n",
            "Early stopping:  0.0370665688767954\n",
            "Epoch: 032, Loss: 0.8638, Train: 0.7402, Test: 0.6553\n",
            "Early stopping:  0.03569596558975214\n",
            "Epoch: 033, Loss: 0.8436, Train: 0.7493, Test: 0.6592\n",
            "Early stopping:  0.03434559285621762\n",
            "Epoch: 034, Loss: 0.8240, Train: 0.7569, Test: 0.6634\n",
            "Early stopping:  0.033106558844168\n",
            "Epoch: 035, Loss: 0.8040, Train: 0.7663, Test: 0.6658\n",
            "Early stopping:  0.03224936240901451\n",
            "Epoch: 036, Loss: 0.7854, Train: 0.7737, Test: 0.6683\n",
            "Early stopping:  0.031062258975276088\n",
            "Epoch: 037, Loss: 0.7671, Train: 0.7819, Test: 0.6744\n",
            "Early stopping:  0.030283640900997343\n",
            "Epoch: 038, Loss: 0.7491, Train: 0.7842, Test: 0.6762\n",
            "Early stopping:  0.029516995472726588\n",
            "Epoch: 039, Loss: 0.7317, Train: 0.7941, Test: 0.6783\n",
            "Early stopping:  0.02859662228951412\n",
            "Epoch: 040, Loss: 0.7143, Train: 0.7986, Test: 0.6793\n",
            "Early stopping:  0.02809993984580476\n",
            "Epoch: 041, Loss: 0.6973, Train: 0.8088, Test: 0.6806\n",
            "Early stopping:  0.027592720958533504\n",
            "Epoch: 042, Loss: 0.6803, Train: 0.8114, Test: 0.6864\n",
            "Early stopping:  0.027207287745903686\n",
            "Epoch: 043, Loss: 0.6635, Train: 0.8205, Test: 0.6887\n",
            "Early stopping:  0.026946572166023898\n",
            "Epoch: 044, Loss: 0.6468, Train: 0.8208, Test: 0.6916\n",
            "Early stopping:  0.02670450227911862\n",
            "Epoch: 045, Loss: 0.6306, Train: 0.8304, Test: 0.6933\n",
            "Early stopping:  0.026400702587960654\n",
            "Epoch: 046, Loss: 0.6145, Train: 0.8330, Test: 0.6982\n",
            "Early stopping:  0.026013703385035124\n",
            "Epoch: 047, Loss: 0.5995, Train: 0.8420, Test: 0.6984\n",
            "Early stopping:  0.025325796606685075\n",
            "Epoch: 048, Loss: 0.5863, Train: 0.8392, Test: 0.7004\n",
            "Early stopping:  0.024056165424983268\n",
            "Epoch: 049, Loss: 0.5772, Train: 0.8497, Test: 0.7008\n",
            "Early stopping:  0.021452429035187823\n",
            "Epoch: 050, Loss: 0.5681, Train: 0.8505, Test: 0.7038\n",
            "Early stopping:  0.01833275302019924\n",
            "Epoch: 051, Loss: 0.5466, Train: 0.8590, Test: 0.7033\n",
            "Early stopping:  0.019927082220529063\n",
            "Epoch: 052, Loss: 0.5277, Train: 0.8667, Test: 0.7072\n",
            "Early stopping:  0.023806598013739088\n",
            "Epoch: 053, Loss: 0.5230, Train: 0.8642, Test: 0.7061\n",
            "Early stopping:  0.023957737927925906\n",
            "Epoch: 054, Loss: 0.5095, Train: 0.8741, Test: 0.7112\n",
            "Early stopping:  0.02278162513495851\n",
            "Epoch: 055, Loss: 0.4903, Train: 0.8800, Test: 0.7105\n",
            "Early stopping:  0.020987143180357486\n",
            "Epoch: 056, Loss: 0.4831, Train: 0.8761, Test: 0.7093\n",
            "Early stopping:  0.01959866610280876\n",
            "Epoch: 057, Loss: 0.4738, Train: 0.8905, Test: 0.7142\n",
            "Early stopping:  0.01999857405461235\n",
            "Epoch: 058, Loss: 0.4565, Train: 0.8936, Test: 0.7157\n",
            "Early stopping:  0.019643831181953467\n",
            "Epoch: 059, Loss: 0.4459, Train: 0.8857, Test: 0.7095\n",
            "Early stopping:  0.01847672125966\n",
            "Epoch: 060, Loss: 0.4401, Train: 0.8982, Test: 0.7142\n",
            "Early stopping:  0.01826156194748275\n",
            "Epoch: 061, Loss: 0.4279, Train: 0.9044, Test: 0.7146\n",
            "Early stopping:  0.017349394397879848\n",
            "Epoch: 062, Loss: 0.4129, Train: 0.9050, Test: 0.7118\n",
            "Early stopping:  0.01681206755835412\n",
            "Epoch: 063, Loss: 0.4034, Train: 0.9098, Test: 0.7193\n",
            "Early stopping:  0.017895841903421774\n",
            "Epoch: 064, Loss: 0.3978, Train: 0.9039, Test: 0.7133\n",
            "Early stopping:  0.017497175005432734\n",
            "Epoch: 065, Loss: 0.3898, Train: 0.9155, Test: 0.7161\n",
            "Early stopping:  0.014707700876636937\n",
            "Epoch: 066, Loss: 0.3775, Train: 0.9200, Test: 0.7165\n",
            "Early stopping:  0.013439410931965028\n",
            "Epoch: 067, Loss: 0.3643, Train: 0.9229, Test: 0.7171\n",
            "Early stopping:  0.015772200678620805\n",
            "Epoch: 068, Loss: 0.3545, Train: 0.9268, Test: 0.7186\n",
            "Early stopping:  0.01777746264931252\n",
            "Epoch: 069, Loss: 0.3487, Train: 0.9223, Test: 0.7165\n",
            "Early stopping:  0.016805754174354027\n",
            "Epoch: 070, Loss: 0.3446, Train: 0.9305, Test: 0.7214\n",
            "Early stopping:  0.013219372201010968\n",
            "Epoch: 071, Loss: 0.3398, Train: 0.9274, Test: 0.7163\n",
            "Early stopping:  0.009465527456000164\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.72      0.74       568\n",
            "         capital_goods       0.67      0.60      0.63       381\n",
            "conglomerates_industry       1.00      0.35      0.52        60\n",
            "     consumer_cyclical       0.74      0.62      0.68       595\n",
            " consumer_non-cyclical       0.76      0.58      0.66       334\n",
            "                energy       0.83      0.74      0.78       213\n",
            "             financial       0.75      0.74      0.75       576\n",
            "            healthcare       0.82      0.71      0.76       238\n",
            "              services       0.65      0.84      0.73      1557\n",
            "            technology       0.66      0.53      0.59       297\n",
            "        transportation       0.83      0.75      0.79       303\n",
            "             utilities       0.81      0.73      0.77       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.66      0.70      5291\n",
            "          weighted avg       0.73      0.72      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4629, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2659, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.13934321314841316\n",
            "Epoch: 003, Loss: 2.1802, Train: 0.2944, Test: 0.2945\n",
            "Early stopping:  0.1449716367408975\n",
            "Epoch: 004, Loss: 2.0907, Train: 0.3205, Test: 0.3211\n",
            "Early stopping:  0.1589855798095426\n",
            "Epoch: 005, Loss: 2.0176, Train: 0.4158, Test: 0.4020\n",
            "Early stopping:  0.17250327284805647\n",
            "Epoch: 006, Loss: 1.9641, Train: 0.4353, Test: 0.4162\n",
            "Early stopping:  0.121688508828778\n",
            "Epoch: 007, Loss: 1.8857, Train: 0.4158, Test: 0.4041\n",
            "Early stopping:  0.11351676880160314\n",
            "Epoch: 008, Loss: 1.8032, Train: 0.4067, Test: 0.3937\n",
            "Early stopping:  0.11206679305874809\n",
            "Epoch: 009, Loss: 1.7363, Train: 0.4203, Test: 0.4071\n",
            "Early stopping:  0.11464392944615989\n",
            "Epoch: 010, Loss: 1.6675, Train: 0.4663, Test: 0.4442\n",
            "Early stopping:  0.11754268052913537\n",
            "Epoch: 011, Loss: 1.5886, Train: 0.5142, Test: 0.4763\n",
            "Early stopping:  0.11547687719565591\n",
            "Epoch: 012, Loss: 1.5200, Train: 0.5408, Test: 0.4986\n",
            "Early stopping:  0.1129405649629299\n",
            "Epoch: 013, Loss: 1.4600, Train: 0.5590, Test: 0.5148\n",
            "Early stopping:  0.11080544535939506\n",
            "Epoch: 014, Loss: 1.3975, Train: 0.5664, Test: 0.5320\n",
            "Early stopping:  0.10588515361828701\n",
            "Epoch: 015, Loss: 1.3390, Train: 0.5769, Test: 0.5426\n",
            "Early stopping:  0.09836124799674058\n",
            "Epoch: 016, Loss: 1.2896, Train: 0.5925, Test: 0.5555\n",
            "Early stopping:  0.09207979783796874\n",
            "Epoch: 017, Loss: 1.2434, Train: 0.6106, Test: 0.5765\n",
            "Early stopping:  0.08573726758231345\n",
            "Epoch: 018, Loss: 1.1992, Train: 0.6285, Test: 0.5808\n",
            "Early stopping:  0.07793515091420859\n",
            "Epoch: 019, Loss: 1.1602, Train: 0.6341, Test: 0.5914\n",
            "Early stopping:  0.0708751236472028\n",
            "Epoch: 020, Loss: 1.1247, Train: 0.6412, Test: 0.5954\n",
            "Early stopping:  0.06538015275625411\n",
            "Epoch: 021, Loss: 1.0918, Train: 0.6517, Test: 0.6037\n",
            "Early stopping:  0.05981486328704806\n",
            "Epoch: 022, Loss: 1.0623, Train: 0.6619, Test: 0.6080\n",
            "Early stopping:  0.05417915430070215\n",
            "Epoch: 023, Loss: 1.0335, Train: 0.6733, Test: 0.6188\n",
            "Early stopping:  0.05000012487297493\n",
            "Epoch: 024, Loss: 1.0067, Train: 0.6829, Test: 0.6254\n",
            "Early stopping:  0.046562469792055514\n",
            "Epoch: 025, Loss: 0.9803, Train: 0.6903, Test: 0.6271\n",
            "Early stopping:  0.04405510646242593\n",
            "Epoch: 026, Loss: 0.9548, Train: 0.6985, Test: 0.6328\n",
            "Early stopping:  0.04240449248025013\n",
            "Epoch: 027, Loss: 0.9308, Train: 0.7096, Test: 0.6420\n",
            "Early stopping:  0.0406799527089239\n",
            "Epoch: 028, Loss: 0.9051, Train: 0.7189, Test: 0.6469\n",
            "Early stopping:  0.03998611683116742\n",
            "Epoch: 029, Loss: 0.8822, Train: 0.7297, Test: 0.6519\n",
            "Early stopping:  0.038914758671699974\n",
            "Epoch: 030, Loss: 0.8599, Train: 0.7422, Test: 0.6560\n",
            "Early stopping:  0.037716528439658135\n",
            "Epoch: 031, Loss: 0.8375, Train: 0.7479, Test: 0.6590\n",
            "Early stopping:  0.03666111125981673\n",
            "Epoch: 032, Loss: 0.8180, Train: 0.7578, Test: 0.6600\n",
            "Early stopping:  0.03461172533818737\n",
            "Epoch: 033, Loss: 0.7980, Train: 0.7660, Test: 0.6651\n",
            "Early stopping:  0.03327492115042311\n",
            "Epoch: 034, Loss: 0.7793, Train: 0.7728, Test: 0.6681\n",
            "Early stopping:  0.03174506590809439\n",
            "Epoch: 035, Loss: 0.7612, Train: 0.7811, Test: 0.6711\n",
            "Early stopping:  0.030257386345435202\n",
            "Epoch: 036, Loss: 0.7433, Train: 0.7862, Test: 0.6727\n",
            "Early stopping:  0.02943457029527867\n",
            "Epoch: 037, Loss: 0.7254, Train: 0.7927, Test: 0.6755\n",
            "Early stopping:  0.02864246879901959\n",
            "Epoch: 038, Loss: 0.7082, Train: 0.7989, Test: 0.6800\n",
            "Early stopping:  0.028163199677498436\n",
            "Epoch: 039, Loss: 0.6908, Train: 0.8043, Test: 0.6823\n",
            "Early stopping:  0.027818719967021358\n",
            "Epoch: 040, Loss: 0.6742, Train: 0.8111, Test: 0.6861\n",
            "Early stopping:  0.027337580559580132\n",
            "Epoch: 041, Loss: 0.6575, Train: 0.8216, Test: 0.6919\n",
            "Early stopping:  0.026850869764484992\n",
            "Epoch: 042, Loss: 0.6413, Train: 0.8261, Test: 0.6948\n",
            "Early stopping:  0.026399063571181075\n",
            "Epoch: 043, Loss: 0.6253, Train: 0.8324, Test: 0.6972\n",
            "Early stopping:  0.025908157949145177\n",
            "Epoch: 044, Loss: 0.6095, Train: 0.8372, Test: 0.6982\n",
            "Early stopping:  0.025559471371277735\n",
            "Epoch: 045, Loss: 0.5938, Train: 0.8437, Test: 0.7048\n",
            "Early stopping:  0.025182982209320585\n",
            "Epoch: 046, Loss: 0.5781, Train: 0.8505, Test: 0.7057\n",
            "Early stopping:  0.0249816114636942\n",
            "Epoch: 047, Loss: 0.5628, Train: 0.8551, Test: 0.7082\n",
            "Early stopping:  0.024724965622761687\n",
            "Epoch: 048, Loss: 0.5477, Train: 0.8593, Test: 0.7084\n",
            "Early stopping:  0.024451199325000863\n",
            "Epoch: 049, Loss: 0.5328, Train: 0.8642, Test: 0.7091\n",
            "Early stopping:  0.02408600189484984\n",
            "Epoch: 050, Loss: 0.5180, Train: 0.8678, Test: 0.7105\n",
            "Early stopping:  0.02374215100818908\n",
            "Epoch: 051, Loss: 0.5036, Train: 0.8738, Test: 0.7137\n",
            "Early stopping:  0.023409669973785848\n",
            "Epoch: 052, Loss: 0.4891, Train: 0.8755, Test: 0.7140\n",
            "Early stopping:  0.023158072399829435\n",
            "Epoch: 053, Loss: 0.4757, Train: 0.8820, Test: 0.7129\n",
            "Early stopping:  0.022648485788037395\n",
            "Epoch: 054, Loss: 0.4657, Train: 0.8721, Test: 0.7093\n",
            "Early stopping:  0.02099969980513929\n",
            "Epoch: 055, Loss: 0.4698, Train: 0.8832, Test: 0.7074\n",
            "Early stopping:  0.015505531543318199\n",
            "Epoch: 056, Loss: 0.4692, Train: 0.8922, Test: 0.7159\n",
            "Early stopping:  0.0092063510852578\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.72      0.74       568\n",
            "         capital_goods       0.66      0.60      0.63       381\n",
            "conglomerates_industry       1.00      0.28      0.44        60\n",
            "     consumer_cyclical       0.69      0.60      0.64       595\n",
            " consumer_non-cyclical       0.75      0.58      0.66       334\n",
            "                energy       0.82      0.72      0.77       213\n",
            "             financial       0.78      0.75      0.77       576\n",
            "            healthcare       0.81      0.76      0.78       238\n",
            "              services       0.65      0.83      0.73      1557\n",
            "            technology       0.72      0.56      0.63       297\n",
            "        transportation       0.79      0.78      0.78       303\n",
            "             utilities       0.81      0.72      0.76       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.66      0.69      5291\n",
            "          weighted avg       0.72      0.72      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4729, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2439, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16191419922514924\n",
            "Epoch: 003, Loss: 2.1761, Train: 0.2944, Test: 0.2945\n",
            "Early stopping:  0.15552699343269571\n",
            "Epoch: 004, Loss: 2.1196, Train: 0.3148, Test: 0.3147\n",
            "Early stopping:  0.1550889083076773\n",
            "Epoch: 005, Loss: 2.0553, Train: 0.3619, Test: 0.3570\n",
            "Early stopping:  0.1608233744955766\n",
            "Epoch: 006, Loss: 1.9999, Train: 0.3735, Test: 0.3672\n",
            "Early stopping:  0.09632671194219032\n",
            "Epoch: 007, Loss: 1.9420, Train: 0.3732, Test: 0.3661\n",
            "Early stopping:  0.09298371107968531\n",
            "Epoch: 008, Loss: 1.8825, Train: 0.3735, Test: 0.3670\n",
            "Early stopping:  0.09291382513342197\n",
            "Epoch: 009, Loss: 1.8245, Train: 0.3849, Test: 0.3788\n",
            "Early stopping:  0.09155726408156388\n",
            "Epoch: 010, Loss: 1.7647, Train: 0.4129, Test: 0.4037\n",
            "Early stopping:  0.09293595435456005\n",
            "Epoch: 011, Loss: 1.6989, Train: 0.4532, Test: 0.4419\n",
            "Early stopping:  0.09551526021115403\n",
            "Epoch: 012, Loss: 1.6369, Train: 0.4827, Test: 0.4547\n",
            "Early stopping:  0.09754898998065488\n",
            "Epoch: 013, Loss: 1.5824, Train: 0.5031, Test: 0.4712\n",
            "Early stopping:  0.09682482588474563\n",
            "Epoch: 014, Loss: 1.5236, Train: 0.5139, Test: 0.4818\n",
            "Early stopping:  0.09474604078604096\n",
            "Epoch: 015, Loss: 1.4642, Train: 0.5335, Test: 0.5058\n",
            "Early stopping:  0.09214673634828197\n",
            "Epoch: 016, Loss: 1.4129, Train: 0.5610, Test: 0.5262\n",
            "Early stopping:  0.08952171428830537\n",
            "Epoch: 017, Loss: 1.3647, Train: 0.5842, Test: 0.5436\n",
            "Early stopping:  0.0864045382495713\n",
            "Epoch: 018, Loss: 1.3162, Train: 0.5998, Test: 0.5536\n",
            "Early stopping:  0.08137513387176482\n",
            "Epoch: 019, Loss: 1.2717, Train: 0.6100, Test: 0.5604\n",
            "Early stopping:  0.07621225372019615\n",
            "Epoch: 020, Loss: 1.2307, Train: 0.6140, Test: 0.5687\n",
            "Early stopping:  0.07240706829059754\n",
            "Epoch: 021, Loss: 1.1935, Train: 0.6225, Test: 0.5742\n",
            "Early stopping:  0.06776952765689674\n",
            "Epoch: 022, Loss: 1.1611, Train: 0.6364, Test: 0.5836\n",
            "Early stopping:  0.06153096235608746\n",
            "Epoch: 023, Loss: 1.1282, Train: 0.6489, Test: 0.5952\n",
            "Early stopping:  0.05643149425095387\n",
            "Epoch: 024, Loss: 1.0989, Train: 0.6588, Test: 0.6046\n",
            "Early stopping:  0.05202027496099658\n",
            "Epoch: 025, Loss: 1.0727, Train: 0.6645, Test: 0.6097\n",
            "Early stopping:  0.048067849427956104\n",
            "Epoch: 026, Loss: 1.0445, Train: 0.6719, Test: 0.6156\n",
            "Early stopping:  0.04568546359221473\n",
            "Epoch: 027, Loss: 1.0192, Train: 0.6775, Test: 0.6207\n",
            "Early stopping:  0.04308798971855155\n",
            "Epoch: 028, Loss: 0.9942, Train: 0.6886, Test: 0.6254\n",
            "Early stopping:  0.041600250733139874\n",
            "Epoch: 029, Loss: 0.9695, Train: 0.6977, Test: 0.6303\n",
            "Early stopping:  0.040616554537027325\n",
            "Epoch: 030, Loss: 0.9452, Train: 0.7127, Test: 0.6354\n",
            "Early stopping:  0.039258838963860176\n",
            "Epoch: 031, Loss: 0.9224, Train: 0.7221, Test: 0.6388\n",
            "Early stopping:  0.03837411441451549\n",
            "Epoch: 032, Loss: 0.9000, Train: 0.7357, Test: 0.6420\n",
            "Early stopping:  0.037237946715429585\n",
            "Epoch: 033, Loss: 0.8776, Train: 0.7399, Test: 0.6475\n",
            "Early stopping:  0.03621009788578085\n",
            "Epoch: 034, Loss: 0.8571, Train: 0.7476, Test: 0.6515\n",
            "Early stopping:  0.0349301286084176\n",
            "Epoch: 035, Loss: 0.8367, Train: 0.7578, Test: 0.6553\n",
            "Early stopping:  0.03388079624784362\n",
            "Epoch: 036, Loss: 0.8171, Train: 0.7689, Test: 0.6604\n",
            "Early stopping:  0.03270250152743338\n",
            "Epoch: 037, Loss: 0.7978, Train: 0.7742, Test: 0.6623\n",
            "Early stopping:  0.031578969736990314\n",
            "Epoch: 038, Loss: 0.7792, Train: 0.7794, Test: 0.6657\n",
            "Early stopping:  0.03079935852799203\n",
            "Epoch: 039, Loss: 0.7605, Train: 0.7847, Test: 0.6700\n",
            "Early stopping:  0.030079418387011257\n",
            "Epoch: 040, Loss: 0.7424, Train: 0.7944, Test: 0.6759\n",
            "Early stopping:  0.02950588326574411\n",
            "Epoch: 041, Loss: 0.7244, Train: 0.7975, Test: 0.6796\n",
            "Early stopping:  0.0290389800563414\n",
            "Epoch: 042, Loss: 0.7063, Train: 0.8035, Test: 0.6827\n",
            "Early stopping:  0.02877478513258794\n",
            "Epoch: 043, Loss: 0.6887, Train: 0.8083, Test: 0.6859\n",
            "Early stopping:  0.02841725081443243\n",
            "Epoch: 044, Loss: 0.6706, Train: 0.8131, Test: 0.6881\n",
            "Early stopping:  0.028333306157942606\n",
            "Epoch: 045, Loss: 0.6529, Train: 0.8185, Test: 0.6904\n",
            "Early stopping:  0.028232849874861368\n",
            "Epoch: 046, Loss: 0.6354, Train: 0.8256, Test: 0.6946\n",
            "Early stopping:  0.028086960368893302\n",
            "Epoch: 047, Loss: 0.6180, Train: 0.8338, Test: 0.6963\n",
            "Early stopping:  0.02792817561481617\n",
            "Epoch: 048, Loss: 0.6007, Train: 0.8361, Test: 0.6972\n",
            "Early stopping:  0.02762773272203807\n",
            "Epoch: 049, Loss: 0.5837, Train: 0.8449, Test: 0.7008\n",
            "Early stopping:  0.027358487990664503\n",
            "Epoch: 050, Loss: 0.5672, Train: 0.8460, Test: 0.7008\n",
            "Early stopping:  0.026978334507369594\n",
            "Epoch: 051, Loss: 0.5524, Train: 0.8613, Test: 0.7019\n",
            "Early stopping:  0.026057346795961837\n",
            "Epoch: 052, Loss: 0.5394, Train: 0.8469, Test: 0.7027\n",
            "Early stopping:  0.024370230518367317\n",
            "Epoch: 053, Loss: 0.5289, Train: 0.8676, Test: 0.7059\n",
            "Early stopping:  0.02181047539508187\n",
            "Epoch: 054, Loss: 0.5135, Train: 0.8695, Test: 0.7112\n",
            "Early stopping:  0.020741917111950413\n",
            "Epoch: 055, Loss: 0.4949, Train: 0.8712, Test: 0.7080\n",
            "Early stopping:  0.02240162750506706\n",
            "Epoch: 056, Loss: 0.4819, Train: 0.8851, Test: 0.7097\n",
            "Early stopping:  0.023637558451139788\n",
            "Epoch: 057, Loss: 0.4725, Train: 0.8851, Test: 0.7150\n",
            "Early stopping:  0.022996296978587762\n",
            "Epoch: 058, Loss: 0.4579, Train: 0.8877, Test: 0.7137\n",
            "Early stopping:  0.021248369567376284\n",
            "Epoch: 059, Loss: 0.4439, Train: 0.8990, Test: 0.7140\n",
            "Early stopping:  0.019975860142496728\n",
            "Epoch: 060, Loss: 0.4348, Train: 0.8973, Test: 0.7137\n",
            "Early stopping:  0.019480388081709085\n",
            "Epoch: 061, Loss: 0.4225, Train: 0.9039, Test: 0.7169\n",
            "Early stopping:  0.019516151348377843\n",
            "Epoch: 062, Loss: 0.4093, Train: 0.9124, Test: 0.7176\n",
            "Early stopping:  0.018799221715039983\n",
            "Epoch: 063, Loss: 0.4000, Train: 0.9078, Test: 0.7167\n",
            "Early stopping:  0.017959524373831776\n",
            "Epoch: 064, Loss: 0.3897, Train: 0.9178, Test: 0.7171\n",
            "Early stopping:  0.017874267691147597\n",
            "Epoch: 065, Loss: 0.3771, Train: 0.9217, Test: 0.7203\n",
            "Early stopping:  0.017474269617837913\n",
            "Epoch: 066, Loss: 0.3682, Train: 0.9192, Test: 0.7157\n",
            "Early stopping:  0.016625071398760877\n",
            "Epoch: 067, Loss: 0.3601, Train: 0.9325, Test: 0.7201\n",
            "Early stopping:  0.016057013260196592\n",
            "Epoch: 068, Loss: 0.3484, Train: 0.9345, Test: 0.7225\n",
            "Early stopping:  0.015786813129118112\n",
            "Epoch: 069, Loss: 0.3386, Train: 0.9356, Test: 0.7182\n",
            "Early stopping:  0.015360313777931017\n",
            "Epoch: 070, Loss: 0.3322, Train: 0.9385, Test: 0.7210\n",
            "Early stopping:  0.014861737044755654\n",
            "Epoch: 071, Loss: 0.3240, Train: 0.9441, Test: 0.7195\n",
            "Early stopping:  0.01405676003524376\n",
            "Epoch: 072, Loss: 0.3133, Train: 0.9467, Test: 0.7201\n",
            "Early stopping:  0.01346331036816762\n",
            "Epoch: 073, Loss: 0.3037, Train: 0.9535, Test: 0.7227\n",
            "Early stopping:  0.014090318428730876\n",
            "Epoch: 074, Loss: 0.2972, Train: 0.9484, Test: 0.7195\n",
            "Early stopping:  0.0143283606180342\n",
            "Epoch: 075, Loss: 0.2908, Train: 0.9603, Test: 0.7222\n",
            "Early stopping:  0.013161830706909142\n",
            "Epoch: 076, Loss: 0.2822, Train: 0.9541, Test: 0.7227\n",
            "Early stopping:  0.01189419750610342\n",
            "Epoch: 077, Loss: 0.2748, Train: 0.9626, Test: 0.7197\n",
            "Early stopping:  0.011502548114346476\n",
            "Epoch: 078, Loss: 0.2743, Train: 0.9362, Test: 0.7148\n",
            "Early stopping:  0.010014788124881137\n",
            "Epoch: 079, Loss: 0.2911, Train: 0.9447, Test: 0.7078\n",
            "Early stopping:  0.008201194730791972\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  1], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.66      0.73      0.69       568\n",
            "         capital_goods       0.54      0.66      0.60       381\n",
            "conglomerates_industry       0.85      0.37      0.51        60\n",
            "     consumer_cyclical       0.56      0.77      0.65       595\n",
            " consumer_non-cyclical       0.69      0.62      0.65       334\n",
            "                energy       0.84      0.76      0.80       213\n",
            "             financial       0.84      0.67      0.75       576\n",
            "            healthcare       0.78      0.68      0.73       238\n",
            "              services       0.75      0.74      0.75      1557\n",
            "            technology       0.68      0.60      0.63       297\n",
            "        transportation       0.85      0.76      0.80       303\n",
            "             utilities       0.90      0.67      0.77       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.75      0.67      0.69      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4824, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2704, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1499207234254952\n",
            "Epoch: 003, Loss: 2.1789, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.15569423883745276\n",
            "Epoch: 004, Loss: 2.1279, Train: 0.3018, Test: 0.3049\n",
            "Early stopping:  0.15653443289858535\n",
            "Epoch: 005, Loss: 2.0557, Train: 0.3520, Test: 0.3510\n",
            "Early stopping:  0.16471322342466216\n",
            "Epoch: 006, Loss: 1.9960, Train: 0.3928, Test: 0.3840\n",
            "Early stopping:  0.10662462065558459\n",
            "Epoch: 007, Loss: 1.9418, Train: 0.3956, Test: 0.3886\n",
            "Early stopping:  0.09595190800879798\n",
            "Epoch: 008, Loss: 1.8835, Train: 0.3934, Test: 0.3873\n",
            "Early stopping:  0.09541178311279004\n",
            "Epoch: 009, Loss: 1.8213, Train: 0.4081, Test: 0.3973\n",
            "Early stopping:  0.09193484000052014\n",
            "Epoch: 010, Loss: 1.7575, Train: 0.4356, Test: 0.4222\n",
            "Early stopping:  0.09454213009646363\n",
            "Epoch: 011, Loss: 1.6895, Train: 0.4714, Test: 0.4576\n",
            "Early stopping:  0.09974722111721107\n",
            "Epoch: 012, Loss: 1.6247, Train: 0.4921, Test: 0.4723\n",
            "Early stopping:  0.10269145848787965\n",
            "Epoch: 013, Loss: 1.5694, Train: 0.5085, Test: 0.4865\n",
            "Early stopping:  0.10068327487414305\n",
            "Epoch: 014, Loss: 1.5106, Train: 0.5241, Test: 0.4986\n",
            "Early stopping:  0.09714958619682738\n",
            "Epoch: 015, Loss: 1.4518, Train: 0.5360, Test: 0.5139\n",
            "Early stopping:  0.09324028590852562\n",
            "Epoch: 016, Loss: 1.3969, Train: 0.5632, Test: 0.5343\n",
            "Early stopping:  0.0906418794178321\n",
            "Epoch: 017, Loss: 1.3476, Train: 0.5876, Test: 0.5472\n",
            "Early stopping:  0.08816224148309723\n",
            "Epoch: 018, Loss: 1.3050, Train: 0.6018, Test: 0.5545\n",
            "Early stopping:  0.08163551409676315\n",
            "Epoch: 019, Loss: 1.2616, Train: 0.6120, Test: 0.5647\n",
            "Early stopping:  0.07479903952627799\n",
            "Epoch: 020, Loss: 1.2212, Train: 0.6168, Test: 0.5710\n",
            "Early stopping:  0.06921682497593014\n",
            "Epoch: 021, Loss: 1.1849, Train: 0.6242, Test: 0.5759\n",
            "Early stopping:  0.06474397691863715\n",
            "Epoch: 022, Loss: 1.1527, Train: 0.6313, Test: 0.5872\n",
            "Early stopping:  0.06039698668367033\n",
            "Epoch: 023, Loss: 1.1215, Train: 0.6446, Test: 0.5997\n",
            "Early stopping:  0.055230982326245764\n",
            "Epoch: 024, Loss: 1.0905, Train: 0.6580, Test: 0.6050\n",
            "Early stopping:  0.051401599830386356\n",
            "Epoch: 025, Loss: 1.0640, Train: 0.6685, Test: 0.6133\n",
            "Early stopping:  0.048107845990515874\n",
            "Epoch: 026, Loss: 1.0368, Train: 0.6753, Test: 0.6173\n",
            "Early stopping:  0.04575892729097156\n",
            "Epoch: 027, Loss: 1.0110, Train: 0.6838, Test: 0.6209\n",
            "Early stopping:  0.04345365826886392\n",
            "Epoch: 028, Loss: 0.9853, Train: 0.6897, Test: 0.6277\n",
            "Early stopping:  0.041627817740536616\n",
            "Epoch: 029, Loss: 0.9617, Train: 0.7014, Test: 0.6337\n",
            "Early stopping:  0.04048024821547036\n",
            "Epoch: 030, Loss: 0.9396, Train: 0.7096, Test: 0.6383\n",
            "Early stopping:  0.03854427795027632\n",
            "Epoch: 031, Loss: 0.9170, Train: 0.7195, Test: 0.6418\n",
            "Early stopping:  0.03694936909789209\n",
            "Epoch: 032, Loss: 0.8955, Train: 0.7314, Test: 0.6494\n",
            "Early stopping:  0.03549595119296498\n",
            "Epoch: 033, Loss: 0.8744, Train: 0.7405, Test: 0.6517\n",
            "Early stopping:  0.034594289511037685\n",
            "Epoch: 034, Loss: 0.8536, Train: 0.7467, Test: 0.6555\n",
            "Early stopping:  0.03395405798608786\n",
            "Epoch: 035, Loss: 0.8329, Train: 0.7513, Test: 0.6575\n",
            "Early stopping:  0.033226071895127426\n",
            "Epoch: 036, Loss: 0.8145, Train: 0.7601, Test: 0.6611\n",
            "Early stopping:  0.032170698076978164\n",
            "Epoch: 037, Loss: 0.7959, Train: 0.7711, Test: 0.6636\n",
            "Early stopping:  0.031012018302810345\n",
            "Epoch: 038, Loss: 0.7780, Train: 0.7734, Test: 0.6666\n",
            "Early stopping:  0.029742627720819623\n",
            "Epoch: 039, Loss: 0.7610, Train: 0.7799, Test: 0.6694\n",
            "Early stopping:  0.028510192995211785\n",
            "Epoch: 040, Loss: 0.7439, Train: 0.7893, Test: 0.6742\n",
            "Early stopping:  0.02786386456344644\n",
            "Epoch: 041, Loss: 0.7267, Train: 0.7950, Test: 0.6766\n",
            "Early stopping:  0.027291349400182435\n",
            "Epoch: 042, Loss: 0.7104, Train: 0.8001, Test: 0.6766\n",
            "Early stopping:  0.02679855205497997\n",
            "Epoch: 043, Loss: 0.6934, Train: 0.8052, Test: 0.6812\n",
            "Early stopping:  0.02666341872203283\n",
            "Epoch: 044, Loss: 0.6770, Train: 0.8114, Test: 0.6859\n",
            "Early stopping:  0.026409913897055024\n",
            "Epoch: 045, Loss: 0.6605, Train: 0.8196, Test: 0.6899\n",
            "Early stopping:  0.02621949789916888\n",
            "Epoch: 046, Loss: 0.6443, Train: 0.8219, Test: 0.6946\n",
            "Early stopping:  0.02612540785531381\n",
            "Epoch: 047, Loss: 0.6284, Train: 0.8276, Test: 0.6959\n",
            "Early stopping:  0.025728035769304526\n",
            "Epoch: 048, Loss: 0.6131, Train: 0.8347, Test: 0.6972\n",
            "Early stopping:  0.025278849160092952\n",
            "Epoch: 049, Loss: 0.5988, Train: 0.8409, Test: 0.6953\n",
            "Early stopping:  0.024444471404776395\n",
            "Epoch: 050, Loss: 0.5875, Train: 0.8298, Test: 0.6936\n",
            "Early stopping:  0.02268955624672184\n",
            "Epoch: 051, Loss: 0.5831, Train: 0.8471, Test: 0.6968\n",
            "Early stopping:  0.018716621038413997\n",
            "Epoch: 052, Loss: 0.5747, Train: 0.8500, Test: 0.7014\n",
            "Early stopping:  0.01492054406327987\n",
            "Epoch: 053, Loss: 0.5471, Train: 0.8539, Test: 0.7004\n",
            "Early stopping:  0.019455556357760292\n",
            "Epoch: 054, Loss: 0.5336, Train: 0.8562, Test: 0.7038\n",
            "Early stopping:  0.023637581142636435\n",
            "Epoch: 055, Loss: 0.5328, Train: 0.8647, Test: 0.7061\n",
            "Early stopping:  0.02338837740451325\n",
            "Epoch: 056, Loss: 0.5114, Train: 0.8681, Test: 0.7069\n",
            "Early stopping:  0.023275155688403038\n",
            "Epoch: 057, Loss: 0.4982, Train: 0.8718, Test: 0.7082\n",
            "Early stopping:  0.019539584056404633\n",
            "Epoch: 058, Loss: 0.4961, Train: 0.8761, Test: 0.7088\n",
            "Early stopping:  0.018124949422820124\n",
            "Epoch: 059, Loss: 0.4766, Train: 0.8817, Test: 0.7101\n",
            "Early stopping:  0.020770112882614693\n",
            "Epoch: 060, Loss: 0.4666, Train: 0.8868, Test: 0.7097\n",
            "Early stopping:  0.017950904452797747\n",
            "Epoch: 061, Loss: 0.4625, Train: 0.8874, Test: 0.7114\n",
            "Early stopping:  0.016483470594622213\n",
            "Epoch: 062, Loss: 0.4459, Train: 0.8953, Test: 0.7142\n",
            "Early stopping:  0.018519097103344716\n",
            "Epoch: 063, Loss: 0.4353, Train: 0.8993, Test: 0.7142\n",
            "Early stopping:  0.016570993531245738\n",
            "Epoch: 064, Loss: 0.4315, Train: 0.8988, Test: 0.7120\n",
            "Early stopping:  0.01575134630063512\n",
            "Epoch: 065, Loss: 0.4181, Train: 0.9033, Test: 0.7144\n",
            "Early stopping:  0.01664657881347622\n",
            "Epoch: 066, Loss: 0.4055, Train: 0.9101, Test: 0.7169\n",
            "Early stopping:  0.01571147612548528\n",
            "Epoch: 067, Loss: 0.4009, Train: 0.9078, Test: 0.7125\n",
            "Early stopping:  0.015253319497211945\n",
            "Epoch: 068, Loss: 0.3934, Train: 0.9178, Test: 0.7182\n",
            "Early stopping:  0.015048912216016196\n",
            "Epoch: 069, Loss: 0.3806, Train: 0.9237, Test: 0.7167\n",
            "Early stopping:  0.013936802785150108\n",
            "Epoch: 070, Loss: 0.3702, Train: 0.9203, Test: 0.7157\n",
            "Early stopping:  0.014583018862158835\n",
            "Epoch: 071, Loss: 0.3650, Train: 0.9254, Test: 0.7184\n",
            "Early stopping:  0.01513313920348352\n",
            "Epoch: 072, Loss: 0.3604, Train: 0.9246, Test: 0.7148\n",
            "Early stopping:  0.013215885244763748\n",
            "Epoch: 073, Loss: 0.3527, Train: 0.9356, Test: 0.7191\n",
            "Early stopping:  0.010485189258421844\n",
            "Epoch: 074, Loss: 0.3424, Train: 0.9370, Test: 0.7176\n",
            "Early stopping:  0.01089841192974347\n",
            "Epoch: 075, Loss: 0.3314, Train: 0.9396, Test: 0.7184\n",
            "Early stopping:  0.013648915789753193\n",
            "Epoch: 076, Loss: 0.3230, Train: 0.9458, Test: 0.7182\n",
            "Early stopping:  0.01522341079390993\n",
            "Epoch: 077, Loss: 0.3176, Train: 0.9404, Test: 0.7182\n",
            "Early stopping:  0.014278483559279484\n",
            "Epoch: 078, Loss: 0.3143, Train: 0.9461, Test: 0.7184\n",
            "Early stopping:  0.011340883227593523\n",
            "Epoch: 079, Loss: 0.3143, Train: 0.9308, Test: 0.7152\n",
            "Early stopping:  0.007245294638405636\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.71      0.73       568\n",
            "         capital_goods       0.70      0.56      0.62       381\n",
            "conglomerates_industry       1.00      0.35      0.52        60\n",
            "     consumer_cyclical       0.74      0.59      0.65       595\n",
            " consumer_non-cyclical       0.73      0.64      0.68       334\n",
            "                energy       0.81      0.71      0.76       213\n",
            "             financial       0.75      0.77      0.76       576\n",
            "            healthcare       0.84      0.74      0.79       238\n",
            "              services       0.65      0.84      0.73      1557\n",
            "            technology       0.70      0.55      0.62       297\n",
            "        transportation       0.81      0.76      0.78       303\n",
            "             utilities       0.82      0.69      0.75       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.77      0.66      0.70      5291\n",
            "          weighted avg       0.73      0.72      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4893, Train: 0.2941, Test: 0.2939\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2594, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16257910790725713\n",
            "Epoch: 003, Loss: 2.1626, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16781956111720056\n",
            "Epoch: 004, Loss: 2.1390, Train: 0.3023, Test: 0.3054\n",
            "Early stopping:  0.15987657424902907\n",
            "Epoch: 005, Loss: 2.0669, Train: 0.3417, Test: 0.3417\n",
            "Early stopping:  0.16379884955997467\n",
            "Epoch: 006, Loss: 1.9950, Train: 0.3715, Test: 0.3665\n",
            "Early stopping:  0.09993577242787183\n",
            "Epoch: 007, Loss: 1.9378, Train: 0.3815, Test: 0.3754\n",
            "Early stopping:  0.09482715314974804\n",
            "Epoch: 008, Loss: 1.8867, Train: 0.3971, Test: 0.3859\n",
            "Early stopping:  0.10049319984348905\n",
            "Epoch: 009, Loss: 1.8300, Train: 0.4163, Test: 0.4007\n",
            "Early stopping:  0.09221975535661574\n",
            "Epoch: 010, Loss: 1.7641, Train: 0.4407, Test: 0.4254\n",
            "Early stopping:  0.09015744317189396\n",
            "Epoch: 011, Loss: 1.6953, Train: 0.4603, Test: 0.4421\n",
            "Early stopping:  0.09627273254143201\n",
            "Epoch: 012, Loss: 1.6318, Train: 0.4745, Test: 0.4517\n",
            "Early stopping:  0.10195609086204635\n",
            "Epoch: 013, Loss: 1.5744, Train: 0.4909, Test: 0.4644\n",
            "Early stopping:  0.10181356397053842\n",
            "Epoch: 014, Loss: 1.5154, Train: 0.5111, Test: 0.4835\n",
            "Early stopping:  0.09785252254270232\n",
            "Epoch: 015, Loss: 1.4545, Train: 0.5374, Test: 0.5024\n",
            "Early stopping:  0.0945762893371244\n",
            "Epoch: 016, Loss: 1.3978, Train: 0.5771, Test: 0.5266\n",
            "Early stopping:  0.0929660538658126\n",
            "Epoch: 017, Loss: 1.3475, Train: 0.6027, Test: 0.5504\n",
            "Early stopping:  0.09039114136614572\n",
            "Epoch: 018, Loss: 1.3025, Train: 0.6146, Test: 0.5610\n",
            "Early stopping:  0.08438708837189521\n",
            "Epoch: 019, Loss: 1.2588, Train: 0.6222, Test: 0.5715\n",
            "Early stopping:  0.07706711257388926\n",
            "Epoch: 020, Loss: 1.2158, Train: 0.6271, Test: 0.5759\n",
            "Early stopping:  0.07161278650028524\n",
            "Epoch: 021, Loss: 1.1769, Train: 0.6344, Test: 0.5829\n",
            "Early stopping:  0.06768408622358195\n",
            "Epoch: 022, Loss: 1.1416, Train: 0.6455, Test: 0.5923\n",
            "Early stopping:  0.06389104336441205\n",
            "Epoch: 023, Loss: 1.1092, Train: 0.6563, Test: 0.5988\n",
            "Early stopping:  0.059128903873917595\n",
            "Epoch: 024, Loss: 1.0781, Train: 0.6648, Test: 0.6080\n",
            "Early stopping:  0.054297549526044576\n",
            "Epoch: 025, Loss: 1.0501, Train: 0.6739, Test: 0.6114\n",
            "Early stopping:  0.050171915719672236\n",
            "Epoch: 026, Loss: 1.0249, Train: 0.6824, Test: 0.6190\n",
            "Early stopping:  0.04630724796098833\n",
            "Epoch: 027, Loss: 0.9995, Train: 0.6914, Test: 0.6220\n",
            "Early stopping:  0.04313573047308241\n",
            "Epoch: 028, Loss: 0.9750, Train: 0.6974, Test: 0.6256\n",
            "Early stopping:  0.040619716755427054\n",
            "Epoch: 029, Loss: 0.9512, Train: 0.7079, Test: 0.6316\n",
            "Early stopping:  0.039193327455885066\n",
            "Epoch: 030, Loss: 0.9277, Train: 0.7181, Test: 0.6367\n",
            "Early stopping:  0.0384027182492114\n",
            "Epoch: 031, Loss: 0.9036, Train: 0.7289, Test: 0.6439\n",
            "Early stopping:  0.037801999966122615\n",
            "Epoch: 032, Loss: 0.8816, Train: 0.7374, Test: 0.6488\n",
            "Early stopping:  0.03706237555200036\n",
            "Epoch: 033, Loss: 0.8602, Train: 0.7484, Test: 0.6545\n",
            "Early stopping:  0.036078814149231024\n",
            "Epoch: 034, Loss: 0.8390, Train: 0.7547, Test: 0.6575\n",
            "Early stopping:  0.03493360284007706\n",
            "Epoch: 035, Loss: 0.8189, Train: 0.7618, Test: 0.6607\n",
            "Early stopping:  0.033532842650066445\n",
            "Epoch: 036, Loss: 0.7996, Train: 0.7677, Test: 0.6634\n",
            "Early stopping:  0.032461802751870425\n",
            "Epoch: 037, Loss: 0.7807, Train: 0.7771, Test: 0.6670\n",
            "Early stopping:  0.031380421651642204\n",
            "Epoch: 038, Loss: 0.7622, Train: 0.7876, Test: 0.6702\n",
            "Early stopping:  0.03033044159646435\n",
            "Epoch: 039, Loss: 0.7440, Train: 0.7950, Test: 0.6749\n",
            "Early stopping:  0.029592613421454944\n",
            "Epoch: 040, Loss: 0.7265, Train: 0.7992, Test: 0.6802\n",
            "Early stopping:  0.028898374050512182\n",
            "Epoch: 041, Loss: 0.7095, Train: 0.8074, Test: 0.6829\n",
            "Early stopping:  0.028137274651772324\n",
            "Epoch: 042, Loss: 0.6923, Train: 0.8120, Test: 0.6864\n",
            "Early stopping:  0.027558212703074663\n",
            "Epoch: 043, Loss: 0.6756, Train: 0.8179, Test: 0.6904\n",
            "Early stopping:  0.027053256879669894\n",
            "Epoch: 044, Loss: 0.6591, Train: 0.8247, Test: 0.6946\n",
            "Early stopping:  0.026670799256337\n",
            "Epoch: 045, Loss: 0.6425, Train: 0.8244, Test: 0.6965\n",
            "Early stopping:  0.02645199993272162\n",
            "Epoch: 046, Loss: 0.6262, Train: 0.8310, Test: 0.6997\n",
            "Early stopping:  0.026146797959064366\n",
            "Epoch: 047, Loss: 0.6100, Train: 0.8330, Test: 0.7031\n",
            "Early stopping:  0.025961931234879852\n",
            "Epoch: 048, Loss: 0.5943, Train: 0.8429, Test: 0.7059\n",
            "Early stopping:  0.025644759033796594\n",
            "Epoch: 049, Loss: 0.5796, Train: 0.8420, Test: 0.7042\n",
            "Early stopping:  0.024913988919653973\n",
            "Epoch: 050, Loss: 0.5689, Train: 0.8491, Test: 0.7029\n",
            "Early stopping:  0.022976489240178255\n",
            "Epoch: 051, Loss: 0.5626, Train: 0.8497, Test: 0.7040\n",
            "Early stopping:  0.01923841112915932\n",
            "Epoch: 052, Loss: 0.5476, Train: 0.8636, Test: 0.7135\n",
            "Early stopping:  0.01758998352119863\n",
            "Epoch: 053, Loss: 0.5233, Train: 0.8593, Test: 0.7091\n",
            "Early stopping:  0.021817068768865196\n",
            "Epoch: 054, Loss: 0.5177, Train: 0.8627, Test: 0.7105\n",
            "Early stopping:  0.02290597150917327\n",
            "Epoch: 055, Loss: 0.5072, Train: 0.8749, Test: 0.7188\n",
            "Early stopping:  0.022798307698895383\n",
            "Epoch: 056, Loss: 0.4869, Train: 0.8710, Test: 0.7118\n",
            "Early stopping:  0.022258709909775385\n",
            "Epoch: 057, Loss: 0.4841, Train: 0.8789, Test: 0.7159\n",
            "Early stopping:  0.0177403293579364\n",
            "Epoch: 058, Loss: 0.4690, Train: 0.8877, Test: 0.7184\n",
            "Early stopping:  0.019379657760800816\n",
            "Epoch: 059, Loss: 0.4547, Train: 0.8849, Test: 0.7169\n",
            "Early stopping:  0.019774598046098837\n",
            "Epoch: 060, Loss: 0.4510, Train: 0.8945, Test: 0.7186\n",
            "Early stopping:  0.016406057465607346\n",
            "Epoch: 061, Loss: 0.4331, Train: 0.9007, Test: 0.7184\n",
            "Early stopping:  0.019241690367180802\n",
            "Epoch: 062, Loss: 0.4252, Train: 0.9036, Test: 0.7214\n",
            "Early stopping:  0.01749734455610972\n",
            "Epoch: 063, Loss: 0.4164, Train: 0.9081, Test: 0.7222\n",
            "Early stopping:  0.01647102765428686\n",
            "Epoch: 064, Loss: 0.4027, Train: 0.9104, Test: 0.7197\n",
            "Early stopping:  0.018115655284511523\n",
            "Epoch: 065, Loss: 0.3955, Train: 0.9166, Test: 0.7220\n",
            "Early stopping:  0.015544259052962337\n",
            "Epoch: 066, Loss: 0.3857, Train: 0.9175, Test: 0.7222\n",
            "Early stopping:  0.01584683181482205\n",
            "Epoch: 067, Loss: 0.3750, Train: 0.9231, Test: 0.7205\n",
            "Early stopping:  0.015820359023798015\n",
            "Epoch: 068, Loss: 0.3666, Train: 0.9240, Test: 0.7239\n",
            "Early stopping:  0.014671279766098324\n",
            "Epoch: 069, Loss: 0.3595, Train: 0.9314, Test: 0.7227\n",
            "Early stopping:  0.014451034375564232\n",
            "Epoch: 070, Loss: 0.3479, Train: 0.9294, Test: 0.7237\n",
            "Early stopping:  0.014469816405179765\n",
            "Epoch: 071, Loss: 0.3413, Train: 0.9362, Test: 0.7231\n",
            "Early stopping:  0.013664778008308781\n",
            "Epoch: 072, Loss: 0.3379, Train: 0.9217, Test: 0.7190\n",
            "Early stopping:  0.012159890236267042\n",
            "Epoch: 073, Loss: 0.3380, Train: 0.9263, Test: 0.7074\n",
            "Early stopping:  0.009092338609030987\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.65      0.78      0.71       568\n",
            "         capital_goods       0.58      0.67      0.62       381\n",
            "conglomerates_industry       0.88      0.37      0.52        60\n",
            "     consumer_cyclical       0.67      0.69      0.68       595\n",
            " consumer_non-cyclical       0.66      0.62      0.64       334\n",
            "                energy       0.82      0.76      0.79       213\n",
            "             financial       0.65      0.82      0.72       576\n",
            "            healthcare       0.81      0.76      0.79       238\n",
            "              services       0.77      0.68      0.73      1557\n",
            "            technology       0.65      0.59      0.62       297\n",
            "        transportation       0.82      0.76      0.79       303\n",
            "             utilities       0.83      0.75      0.79       169\n",
            "\n",
            "              accuracy                           0.71      5291\n",
            "             macro avg       0.73      0.69      0.70      5291\n",
            "          weighted avg       0.72      0.71      0.71      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4772, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2669, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.148689698274189\n",
            "Epoch: 003, Loss: 2.1754, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1547306293395429\n",
            "Epoch: 004, Loss: 2.1284, Train: 0.2972, Test: 0.2984\n",
            "Early stopping:  0.15455985710872552\n",
            "Epoch: 005, Loss: 2.0585, Train: 0.3395, Test: 0.3370\n",
            "Early stopping:  0.1618559713836658\n",
            "Epoch: 006, Loss: 2.0019, Train: 0.3894, Test: 0.3837\n",
            "Early stopping:  0.10277998807198443\n",
            "Epoch: 007, Loss: 1.9442, Train: 0.4067, Test: 0.3958\n",
            "Early stopping:  0.09324552164093483\n",
            "Epoch: 008, Loss: 1.8768, Train: 0.4087, Test: 0.3986\n",
            "Early stopping:  0.0977071812731853\n",
            "Epoch: 009, Loss: 1.8118, Train: 0.4186, Test: 0.4060\n",
            "Early stopping:  0.09784017896864305\n",
            "Epoch: 010, Loss: 1.7525, Train: 0.4407, Test: 0.4228\n",
            "Early stopping:  0.09979912661400656\n",
            "Epoch: 011, Loss: 1.6893, Train: 0.4733, Test: 0.4508\n",
            "Early stopping:  0.10027233132301938\n",
            "Epoch: 012, Loss: 1.6224, Train: 0.4997, Test: 0.4782\n",
            "Early stopping:  0.09985454244442979\n",
            "Epoch: 013, Loss: 1.5601, Train: 0.5193, Test: 0.4944\n",
            "Early stopping:  0.10022404046683363\n",
            "Epoch: 014, Loss: 1.5000, Train: 0.5380, Test: 0.5111\n",
            "Early stopping:  0.10032014790672648\n",
            "Epoch: 015, Loss: 1.4389, Train: 0.5590, Test: 0.5279\n",
            "Early stopping:  0.09855099651312726\n",
            "Epoch: 016, Loss: 1.3811, Train: 0.5749, Test: 0.5445\n",
            "Early stopping:  0.09547241170947245\n",
            "Epoch: 017, Loss: 1.3273, Train: 0.5922, Test: 0.5572\n",
            "Early stopping:  0.09243297771263724\n",
            "Epoch: 018, Loss: 1.2764, Train: 0.6143, Test: 0.5751\n",
            "Early stopping:  0.08840827757334996\n",
            "Epoch: 019, Loss: 1.2282, Train: 0.6310, Test: 0.5893\n",
            "Early stopping:  0.0832176453112895\n",
            "Epoch: 020, Loss: 1.1857, Train: 0.6401, Test: 0.5971\n",
            "Early stopping:  0.07751415403927381\n",
            "Epoch: 021, Loss: 1.1490, Train: 0.6410, Test: 0.6003\n",
            "Early stopping:  0.07087468481483478\n",
            "Epoch: 022, Loss: 1.1132, Train: 0.6486, Test: 0.6046\n",
            "Early stopping:  0.0642736921149897\n",
            "Epoch: 023, Loss: 1.0788, Train: 0.6563, Test: 0.6103\n",
            "Early stopping:  0.05877427850467427\n",
            "Epoch: 024, Loss: 1.0504, Train: 0.6690, Test: 0.6141\n",
            "Early stopping:  0.0539509215445149\n",
            "Epoch: 025, Loss: 1.0233, Train: 0.6784, Test: 0.6199\n",
            "Early stopping:  0.0497661233211797\n",
            "Epoch: 026, Loss: 0.9959, Train: 0.6909, Test: 0.6248\n",
            "Early stopping:  0.04591226520424487\n",
            "Epoch: 027, Loss: 0.9710, Train: 0.6988, Test: 0.6284\n",
            "Early stopping:  0.04271999497351758\n",
            "Epoch: 028, Loss: 0.9467, Train: 0.7050, Test: 0.6360\n",
            "Early stopping:  0.04106441927962598\n",
            "Epoch: 029, Loss: 0.9231, Train: 0.7141, Test: 0.6434\n",
            "Early stopping:  0.03949920559869881\n",
            "Epoch: 030, Loss: 0.8998, Train: 0.7255, Test: 0.6462\n",
            "Early stopping:  0.03797085669620002\n",
            "Epoch: 031, Loss: 0.8775, Train: 0.7351, Test: 0.6503\n",
            "Early stopping:  0.03697944083671506\n",
            "Epoch: 032, Loss: 0.8558, Train: 0.7476, Test: 0.6553\n",
            "Early stopping:  0.035956191257418085\n",
            "Epoch: 033, Loss: 0.8354, Train: 0.7555, Test: 0.6594\n",
            "Early stopping:  0.034696769850444245\n",
            "Epoch: 034, Loss: 0.8153, Train: 0.7649, Test: 0.6634\n",
            "Early stopping:  0.03339718351685953\n",
            "Epoch: 035, Loss: 0.7961, Train: 0.7754, Test: 0.6681\n",
            "Early stopping:  0.03217384220249272\n",
            "Epoch: 036, Loss: 0.7776, Train: 0.7796, Test: 0.6715\n",
            "Early stopping:  0.030966642927928847\n",
            "Epoch: 037, Loss: 0.7592, Train: 0.7847, Test: 0.6755\n",
            "Early stopping:  0.030041521016037003\n",
            "Epoch: 038, Loss: 0.7416, Train: 0.7941, Test: 0.6789\n",
            "Early stopping:  0.029143330849791857\n",
            "Epoch: 039, Loss: 0.7246, Train: 0.7995, Test: 0.6787\n",
            "Early stopping:  0.02830367873202853\n",
            "Epoch: 040, Loss: 0.7082, Train: 0.8063, Test: 0.6836\n",
            "Early stopping:  0.027413158122993445\n",
            "Epoch: 041, Loss: 0.6920, Train: 0.8100, Test: 0.6866\n",
            "Early stopping:  0.0265240732356731\n",
            "Epoch: 042, Loss: 0.6760, Train: 0.8165, Test: 0.6916\n",
            "Early stopping:  0.025881137375900736\n",
            "Epoch: 043, Loss: 0.6603, Train: 0.8216, Test: 0.6929\n",
            "Early stopping:  0.025436274972775244\n",
            "Epoch: 044, Loss: 0.6441, Train: 0.8273, Test: 0.6912\n",
            "Early stopping:  0.02530875290644211\n",
            "Epoch: 045, Loss: 0.6287, Train: 0.8338, Test: 0.6940\n",
            "Early stopping:  0.02506317620081173\n",
            "Epoch: 046, Loss: 0.6132, Train: 0.8366, Test: 0.6967\n",
            "Early stopping:  0.02485562623509066\n",
            "Epoch: 047, Loss: 0.5983, Train: 0.8471, Test: 0.7016\n",
            "Early stopping:  0.024498770898347403\n",
            "Epoch: 048, Loss: 0.5834, Train: 0.8471, Test: 0.7006\n",
            "Early stopping:  0.024000799077480733\n",
            "Epoch: 049, Loss: 0.5697, Train: 0.8505, Test: 0.7040\n",
            "Early stopping:  0.023373410786978736\n",
            "Epoch: 050, Loss: 0.5594, Train: 0.8449, Test: 0.6950\n",
            "Early stopping:  0.021573940091828198\n",
            "Epoch: 051, Loss: 0.5561, Train: 0.8508, Test: 0.7010\n",
            "Early stopping:  0.017521421487951924\n",
            "Epoch: 052, Loss: 0.5438, Train: 0.8636, Test: 0.7106\n",
            "Early stopping:  0.014922454395547163\n",
            "Epoch: 053, Loss: 0.5160, Train: 0.8650, Test: 0.7086\n",
            "Early stopping:  0.020627502046218448\n",
            "Epoch: 054, Loss: 0.5105, Train: 0.8710, Test: 0.7080\n",
            "Early stopping:  0.022659564420676546\n",
            "Epoch: 055, Loss: 0.5025, Train: 0.8783, Test: 0.7137\n",
            "Early stopping:  0.022987409744590872\n",
            "Epoch: 056, Loss: 0.4791, Train: 0.8792, Test: 0.7105\n",
            "Early stopping:  0.023406514453405896\n",
            "Epoch: 057, Loss: 0.4763, Train: 0.8888, Test: 0.7114\n",
            "Early stopping:  0.018198850533080957\n",
            "Epoch: 058, Loss: 0.4621, Train: 0.8908, Test: 0.7161\n",
            "Early stopping:  0.01990665802983999\n",
            "Epoch: 059, Loss: 0.4462, Train: 0.8897, Test: 0.7118\n",
            "Early stopping:  0.020924427817256373\n",
            "Epoch: 060, Loss: 0.4431, Train: 0.9033, Test: 0.7156\n",
            "Early stopping:  0.016569395901067648\n",
            "Epoch: 061, Loss: 0.4257, Train: 0.9061, Test: 0.7180\n",
            "Early stopping:  0.019296481321893866\n",
            "Epoch: 062, Loss: 0.4156, Train: 0.9016, Test: 0.7154\n",
            "Early stopping:  0.018234437304475517\n",
            "Epoch: 063, Loss: 0.4094, Train: 0.9155, Test: 0.7184\n",
            "Early stopping:  0.016324040824278446\n",
            "Epoch: 064, Loss: 0.3933, Train: 0.9203, Test: 0.7201\n",
            "Early stopping:  0.01857239844125853\n",
            "Epoch: 065, Loss: 0.3850, Train: 0.9149, Test: 0.7176\n",
            "Early stopping:  0.016548967749443613\n",
            "Epoch: 066, Loss: 0.3784, Train: 0.9280, Test: 0.7212\n",
            "Early stopping:  0.015829240897683512\n",
            "Epoch: 067, Loss: 0.3640, Train: 0.9285, Test: 0.7229\n",
            "Early stopping:  0.016930525684392985\n",
            "Epoch: 068, Loss: 0.3556, Train: 0.9274, Test: 0.7195\n",
            "Early stopping:  0.015353140158570517\n",
            "Epoch: 069, Loss: 0.3511, Train: 0.9288, Test: 0.7235\n",
            "Early stopping:  0.014538798580428374\n",
            "Epoch: 070, Loss: 0.3420, Train: 0.9382, Test: 0.7208\n",
            "Early stopping:  0.013785299340796645\n",
            "Epoch: 071, Loss: 0.3328, Train: 0.9294, Test: 0.7216\n",
            "Early stopping:  0.012067313762284156\n",
            "Epoch: 072, Loss: 0.3258, Train: 0.9436, Test: 0.7252\n",
            "Early stopping:  0.012358987898001206\n",
            "Epoch: 073, Loss: 0.3143, Train: 0.9450, Test: 0.7246\n",
            "Early stopping:  0.014232726223166618\n",
            "Epoch: 074, Loss: 0.3046, Train: 0.9396, Test: 0.7216\n",
            "Early stopping:  0.014799727311415697\n",
            "Epoch: 075, Loss: 0.3010, Train: 0.9558, Test: 0.7227\n",
            "Early stopping:  0.013569556298119661\n",
            "Epoch: 076, Loss: 0.2939, Train: 0.9501, Test: 0.7239\n",
            "Early stopping:  0.012411155769000468\n",
            "Epoch: 077, Loss: 0.2822, Train: 0.9492, Test: 0.7242\n",
            "Early stopping:  0.01200052393513948\n",
            "Epoch: 078, Loss: 0.2777, Train: 0.9617, Test: 0.7242\n",
            "Early stopping:  0.011670953116682892\n",
            "Epoch: 079, Loss: 0.2744, Train: 0.9555, Test: 0.7258\n",
            "Early stopping:  0.011256047847575716\n",
            "Epoch: 080, Loss: 0.2641, Train: 0.9603, Test: 0.7259\n",
            "Early stopping:  0.010919521282952687\n",
            "Epoch: 081, Loss: 0.2569, Train: 0.9668, Test: 0.7265\n",
            "Early stopping:  0.010329454965549318\n",
            "Epoch: 082, Loss: 0.2544, Train: 0.9606, Test: 0.7254\n",
            "Early stopping:  0.010304055554468412\n",
            "Epoch: 083, Loss: 0.2479, Train: 0.9660, Test: 0.7286\n",
            "Early stopping:  0.010110613190108439\n",
            "Epoch: 084, Loss: 0.2409, Train: 0.9702, Test: 0.7271\n",
            "Early stopping:  0.008833401275758136\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.73      0.76      0.74       568\n",
            "         capital_goods       0.65      0.65      0.65       381\n",
            "conglomerates_industry       1.00      0.40      0.57        60\n",
            "     consumer_cyclical       0.64      0.72      0.68       595\n",
            " consumer_non-cyclical       0.70      0.66      0.68       334\n",
            "                energy       0.87      0.73      0.79       213\n",
            "             financial       0.80      0.72      0.75       576\n",
            "            healthcare       0.80      0.73      0.76       238\n",
            "              services       0.71      0.79      0.75      1557\n",
            "            technology       0.68      0.56      0.61       297\n",
            "        transportation       0.83      0.78      0.80       303\n",
            "             utilities       0.85      0.71      0.77       169\n",
            "\n",
            "              accuracy                           0.73      5291\n",
            "             macro avg       0.77      0.68      0.71      5291\n",
            "          weighted avg       0.73      0.73      0.73      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5110, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2676, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.17209519195144082\n",
            "Epoch: 003, Loss: 2.1660, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1772587406155976\n",
            "Epoch: 004, Loss: 2.1341, Train: 0.2986, Test: 0.2998\n",
            "Early stopping:  0.17062315394939168\n",
            "Epoch: 005, Loss: 2.0560, Train: 0.3287, Test: 0.3304\n",
            "Early stopping:  0.1759839254730489\n",
            "Epoch: 006, Loss: 1.9858, Train: 0.3670, Test: 0.3668\n",
            "Early stopping:  0.10745234448071238\n",
            "Epoch: 007, Loss: 1.9300, Train: 0.3914, Test: 0.3829\n",
            "Early stopping:  0.09872644018809582\n",
            "Epoch: 008, Loss: 1.8689, Train: 0.4050, Test: 0.3922\n",
            "Early stopping:  0.10402660269074457\n",
            "Epoch: 009, Loss: 1.8029, Train: 0.4169, Test: 0.4084\n",
            "Early stopping:  0.09857087680190783\n",
            "Epoch: 010, Loss: 1.7398, Train: 0.4368, Test: 0.4254\n",
            "Early stopping:  0.09796089228946728\n",
            "Epoch: 011, Loss: 1.6790, Train: 0.4657, Test: 0.4470\n",
            "Early stopping:  0.09980049861104102\n",
            "Epoch: 012, Loss: 1.6189, Train: 0.4850, Test: 0.4632\n",
            "Early stopping:  0.09865969337452338\n",
            "Epoch: 013, Loss: 1.5580, Train: 0.5111, Test: 0.4837\n",
            "Early stopping:  0.0965590483628191\n",
            "Epoch: 014, Loss: 1.4987, Train: 0.5397, Test: 0.5056\n",
            "Early stopping:  0.09537890832369278\n",
            "Epoch: 015, Loss: 1.4454, Train: 0.5658, Test: 0.5237\n",
            "Early stopping:  0.09291498785102659\n",
            "Epoch: 016, Loss: 1.3939, Train: 0.5822, Test: 0.5455\n",
            "Early stopping:  0.08901846376750089\n",
            "Epoch: 017, Loss: 1.3412, Train: 0.5927, Test: 0.5517\n",
            "Early stopping:  0.085175760299983\n",
            "Epoch: 018, Loss: 1.2920, Train: 0.6001, Test: 0.5602\n",
            "Early stopping:  0.08185301181543858\n",
            "Epoch: 019, Loss: 1.2491, Train: 0.6112, Test: 0.5672\n",
            "Early stopping:  0.07824185179713514\n",
            "Epoch: 020, Loss: 1.2097, Train: 0.6279, Test: 0.5785\n",
            "Early stopping:  0.07294146505714735\n",
            "Epoch: 021, Loss: 1.1708, Train: 0.6412, Test: 0.5901\n",
            "Early stopping:  0.06696990359433577\n",
            "Epoch: 022, Loss: 1.1355, Train: 0.6478, Test: 0.6018\n",
            "Early stopping:  0.06190887750423358\n",
            "Epoch: 023, Loss: 1.1037, Train: 0.6577, Test: 0.6080\n",
            "Early stopping:  0.05776670363169118\n",
            "Epoch: 024, Loss: 1.0730, Train: 0.6634, Test: 0.6146\n",
            "Early stopping:  0.05387842906771203\n",
            "Epoch: 025, Loss: 1.0431, Train: 0.6716, Test: 0.6178\n",
            "Early stopping:  0.05026896794083745\n",
            "Epoch: 026, Loss: 1.0160, Train: 0.6826, Test: 0.6229\n",
            "Early stopping:  0.047373746625228794\n",
            "Epoch: 027, Loss: 0.9891, Train: 0.6889, Test: 0.6335\n",
            "Early stopping:  0.0452989352559348\n",
            "Epoch: 028, Loss: 0.9628, Train: 0.6994, Test: 0.6386\n",
            "Early stopping:  0.043430268372607696\n",
            "Epoch: 029, Loss: 0.9374, Train: 0.7099, Test: 0.6434\n",
            "Early stopping:  0.04187404493095583\n",
            "Epoch: 030, Loss: 0.9131, Train: 0.7195, Test: 0.6452\n",
            "Early stopping:  0.04072967086732253\n",
            "Epoch: 031, Loss: 0.8895, Train: 0.7294, Test: 0.6485\n",
            "Early stopping:  0.039341451972032114\n",
            "Epoch: 032, Loss: 0.8667, Train: 0.7396, Test: 0.6522\n",
            "Early stopping:  0.03794196042353512\n",
            "Epoch: 033, Loss: 0.8444, Train: 0.7465, Test: 0.6575\n",
            "Early stopping:  0.036714709976687805\n",
            "Epoch: 034, Loss: 0.8234, Train: 0.7575, Test: 0.6640\n",
            "Early stopping:  0.03548673945419939\n",
            "Epoch: 035, Loss: 0.8030, Train: 0.7677, Test: 0.6675\n",
            "Early stopping:  0.03421917760242794\n",
            "Epoch: 036, Loss: 0.7820, Train: 0.7734, Test: 0.6666\n",
            "Early stopping:  0.03336216496325141\n",
            "Epoch: 037, Loss: 0.7621, Train: 0.7802, Test: 0.6711\n",
            "Early stopping:  0.032582777781826514\n",
            "Epoch: 038, Loss: 0.7431, Train: 0.7901, Test: 0.6757\n",
            "Early stopping:  0.031882068670505664\n",
            "Epoch: 039, Loss: 0.7237, Train: 0.7995, Test: 0.6796\n",
            "Early stopping:  0.03122497260100213\n",
            "Epoch: 040, Loss: 0.7050, Train: 0.8077, Test: 0.6832\n",
            "Early stopping:  0.03042376898335412\n",
            "Epoch: 041, Loss: 0.6866, Train: 0.8108, Test: 0.6846\n",
            "Early stopping:  0.029899144366952797\n",
            "Epoch: 042, Loss: 0.6684, Train: 0.8182, Test: 0.6883\n",
            "Early stopping:  0.029471489686052673\n",
            "Epoch: 043, Loss: 0.6503, Train: 0.8179, Test: 0.6910\n",
            "Early stopping:  0.029008234397166825\n",
            "Epoch: 044, Loss: 0.6339, Train: 0.8301, Test: 0.6908\n",
            "Early stopping:  0.028243702080063323\n",
            "Epoch: 045, Loss: 0.6201, Train: 0.8210, Test: 0.6940\n",
            "Early stopping:  0.026532968228193985\n",
            "Epoch: 046, Loss: 0.6099, Train: 0.8398, Test: 0.6942\n",
            "Early stopping:  0.02340893752185574\n",
            "Epoch: 047, Loss: 0.5905, Train: 0.8463, Test: 0.6984\n",
            "Early stopping:  0.02278473391224719\n",
            "Epoch: 048, Loss: 0.5687, Train: 0.8460, Test: 0.7001\n",
            "Early stopping:  0.02557428959672516\n",
            "Epoch: 049, Loss: 0.5577, Train: 0.8531, Test: 0.6980\n",
            "Early stopping:  0.02647418721687252\n",
            "Epoch: 050, Loss: 0.5458, Train: 0.8599, Test: 0.7053\n",
            "Early stopping:  0.025736256787154882\n",
            "Epoch: 051, Loss: 0.5269, Train: 0.8639, Test: 0.7080\n",
            "Early stopping:  0.02392096470922826\n",
            "Epoch: 052, Loss: 0.5131, Train: 0.8704, Test: 0.7067\n",
            "Early stopping:  0.02253505590253581\n",
            "Epoch: 053, Loss: 0.5033, Train: 0.8758, Test: 0.7120\n",
            "Early stopping:  0.022467094638592994\n",
            "Epoch: 054, Loss: 0.4879, Train: 0.8806, Test: 0.7142\n",
            "Early stopping:  0.02215042049143662\n",
            "Epoch: 055, Loss: 0.4724, Train: 0.8834, Test: 0.7105\n",
            "Early stopping:  0.021272918348264468\n",
            "Epoch: 056, Loss: 0.4629, Train: 0.8860, Test: 0.7142\n",
            "Early stopping:  0.020848229277755898\n",
            "Epoch: 057, Loss: 0.4516, Train: 0.8951, Test: 0.7133\n",
            "Early stopping:  0.0204272843413996\n",
            "Epoch: 058, Loss: 0.4359, Train: 0.9002, Test: 0.7186\n",
            "Early stopping:  0.019823353114402784\n",
            "Epoch: 059, Loss: 0.4237, Train: 0.8973, Test: 0.7195\n",
            "Early stopping:  0.019734524441026437\n",
            "Epoch: 060, Loss: 0.4146, Train: 0.9067, Test: 0.7171\n",
            "Early stopping:  0.01973781242587588\n",
            "Epoch: 061, Loss: 0.4035, Train: 0.9081, Test: 0.7205\n",
            "Early stopping:  0.01866315380258667\n",
            "Epoch: 062, Loss: 0.3902, Train: 0.9161, Test: 0.7216\n",
            "Early stopping:  0.01767365765547603\n",
            "Epoch: 063, Loss: 0.3782, Train: 0.9175, Test: 0.7227\n",
            "Early stopping:  0.018303484044163452\n",
            "Epoch: 064, Loss: 0.3687, Train: 0.9155, Test: 0.7218\n",
            "Early stopping:  0.018560134857110645\n",
            "Epoch: 065, Loss: 0.3608, Train: 0.9277, Test: 0.7222\n",
            "Early stopping:  0.016994957603612488\n",
            "Epoch: 066, Loss: 0.3520, Train: 0.9240, Test: 0.7229\n",
            "Early stopping:  0.014904673070980909\n",
            "Epoch: 067, Loss: 0.3416, Train: 0.9345, Test: 0.7267\n",
            "Early stopping:  0.01422129734066621\n",
            "Epoch: 068, Loss: 0.3296, Train: 0.9373, Test: 0.7248\n",
            "Early stopping:  0.015448125666784114\n",
            "Epoch: 069, Loss: 0.3194, Train: 0.9402, Test: 0.7246\n",
            "Early stopping:  0.016633223543127622\n",
            "Epoch: 070, Loss: 0.3124, Train: 0.9410, Test: 0.7235\n",
            "Early stopping:  0.01607837524147635\n",
            "Epoch: 071, Loss: 0.3080, Train: 0.9396, Test: 0.7186\n",
            "Early stopping:  0.013580332373273643\n",
            "Epoch: 072, Loss: 0.3066, Train: 0.9399, Test: 0.7197\n",
            "Early stopping:  0.009469427427834258\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.70      0.72      0.71       568\n",
            "         capital_goods       0.65      0.61      0.63       381\n",
            "conglomerates_industry       0.90      0.30      0.45        60\n",
            "     consumer_cyclical       0.74      0.61      0.67       595\n",
            " consumer_non-cyclical       0.73      0.60      0.66       334\n",
            "                energy       0.84      0.71      0.77       213\n",
            "             financial       0.69      0.82      0.75       576\n",
            "            healthcare       0.86      0.71      0.78       238\n",
            "              services       0.70      0.81      0.75      1557\n",
            "            technology       0.68      0.56      0.61       297\n",
            "        transportation       0.80      0.79      0.79       303\n",
            "             utilities       0.84      0.75      0.79       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.76      0.67      0.70      5291\n",
            "          weighted avg       0.72      0.72      0.72      5291\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5115, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2764, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.1662710032494174\n",
            "Epoch: 003, Loss: 2.1841, Train: 0.2944, Test: 0.2943\n",
            "Early stopping:  0.16884493649260712\n",
            "Epoch: 004, Loss: 2.1512, Train: 0.2952, Test: 0.2964\n",
            "Early stopping:  0.16269749167961428\n",
            "Epoch: 005, Loss: 2.0832, Train: 0.3191, Test: 0.3188\n",
            "Early stopping:  0.16631196995614392\n",
            "Epoch: 006, Loss: 2.0215, Train: 0.3372, Test: 0.3368\n",
            "Early stopping:  0.09730418895399377\n",
            "Epoch: 007, Loss: 1.9674, Train: 0.3488, Test: 0.3483\n",
            "Early stopping:  0.0894515508201657\n",
            "Epoch: 008, Loss: 1.9167, Train: 0.3633, Test: 0.3551\n",
            "Early stopping:  0.09266191779845637\n",
            "Epoch: 009, Loss: 1.8665, Train: 0.3775, Test: 0.3680\n",
            "Early stopping:  0.08519150711245772\n",
            "Epoch: 010, Loss: 1.8128, Train: 0.4002, Test: 0.3893\n",
            "Early stopping:  0.08197074528820489\n",
            "Epoch: 011, Loss: 1.7528, Train: 0.4342, Test: 0.4215\n",
            "Early stopping:  0.08432726521098512\n",
            "Epoch: 012, Loss: 1.6913, Train: 0.4617, Test: 0.4470\n",
            "Early stopping:  0.0893311682786647\n",
            "Epoch: 013, Loss: 1.6350, Train: 0.4818, Test: 0.4608\n",
            "Early stopping:  0.09243749216264198\n",
            "Epoch: 014, Loss: 1.5796, Train: 0.4955, Test: 0.4725\n",
            "Early stopping:  0.09237712479360119\n",
            "Epoch: 015, Loss: 1.5206, Train: 0.5130, Test: 0.4889\n",
            "Early stopping:  0.09110789590200416\n",
            "Epoch: 016, Loss: 1.4630, Train: 0.5383, Test: 0.5086\n",
            "Early stopping:  0.09029783112173023\n",
            "Epoch: 017, Loss: 1.4093, Train: 0.5703, Test: 0.5305\n",
            "Early stopping:  0.08982760270610003\n",
            "Epoch: 018, Loss: 1.3576, Train: 0.5916, Test: 0.5496\n",
            "Early stopping:  0.08783649168521535\n",
            "Epoch: 019, Loss: 1.3104, Train: 0.6049, Test: 0.5570\n",
            "Early stopping:  0.08319383578564464\n",
            "Epoch: 020, Loss: 1.2678, Train: 0.6117, Test: 0.5638\n",
            "Early stopping:  0.07745123777910703\n",
            "Epoch: 021, Loss: 1.2260, Train: 0.6183, Test: 0.5723\n",
            "Early stopping:  0.07225402506442638\n",
            "Epoch: 022, Loss: 1.1873, Train: 0.6285, Test: 0.5776\n",
            "Early stopping:  0.06723697210803264\n",
            "Epoch: 023, Loss: 1.1541, Train: 0.6421, Test: 0.5868\n",
            "Early stopping:  0.0622125447138\n",
            "Epoch: 024, Loss: 1.1231, Train: 0.6483, Test: 0.5921\n",
            "Early stopping:  0.057228985692729266\n",
            "Epoch: 025, Loss: 1.0950, Train: 0.6585, Test: 0.6006\n",
            "Early stopping:  0.05166104604987369\n",
            "Epoch: 026, Loss: 1.0676, Train: 0.6690, Test: 0.6103\n",
            "Early stopping:  0.04725736909989827\n",
            "Epoch: 027, Loss: 1.0398, Train: 0.6750, Test: 0.6169\n",
            "Early stopping:  0.044931849551515496\n",
            "Epoch: 028, Loss: 1.0148, Train: 0.6858, Test: 0.6220\n",
            "Early stopping:  0.04299817578146923\n",
            "Epoch: 029, Loss: 0.9890, Train: 0.6903, Test: 0.6260\n",
            "Early stopping:  0.04187110907140707\n",
            "Epoch: 030, Loss: 0.9632, Train: 0.7028, Test: 0.6333\n",
            "Early stopping:  0.041037602063747354\n",
            "Epoch: 031, Loss: 0.9397, Train: 0.7121, Test: 0.6390\n",
            "Early stopping:  0.039837551695189294\n",
            "Epoch: 032, Loss: 0.9178, Train: 0.7229, Test: 0.6449\n",
            "Early stopping:  0.038484664973910544\n",
            "Epoch: 033, Loss: 0.8958, Train: 0.7348, Test: 0.6485\n",
            "Early stopping:  0.03666990453184313\n",
            "Epoch: 034, Loss: 0.8736, Train: 0.7419, Test: 0.6532\n",
            "Early stopping:  0.035271690462595325\n",
            "Epoch: 035, Loss: 0.8535, Train: 0.7490, Test: 0.6579\n",
            "Early stopping:  0.034265182568358823\n",
            "Epoch: 036, Loss: 0.8333, Train: 0.7552, Test: 0.6617\n",
            "Early stopping:  0.03345563515121892\n",
            "Epoch: 037, Loss: 0.8133, Train: 0.7672, Test: 0.6653\n",
            "Early stopping:  0.032500056392102586\n",
            "Epoch: 038, Loss: 0.7940, Train: 0.7760, Test: 0.6692\n",
            "Early stopping:  0.031511925007040635\n",
            "Epoch: 039, Loss: 0.7761, Train: 0.7828, Test: 0.6736\n",
            "Early stopping:  0.030677888878442642\n",
            "Epoch: 040, Loss: 0.7577, Train: 0.7890, Test: 0.6766\n",
            "Early stopping:  0.029774838865152715\n",
            "Epoch: 041, Loss: 0.7399, Train: 0.7964, Test: 0.6808\n",
            "Early stopping:  0.02893816190996604\n",
            "Epoch: 042, Loss: 0.7224, Train: 0.8009, Test: 0.6855\n",
            "Early stopping:  0.028362663442092807\n",
            "Epoch: 043, Loss: 0.7049, Train: 0.8054, Test: 0.6857\n",
            "Early stopping:  0.028079714042140155\n",
            "Epoch: 044, Loss: 0.6874, Train: 0.8111, Test: 0.6883\n",
            "Early stopping:  0.027760882853891694\n",
            "Epoch: 045, Loss: 0.6706, Train: 0.8162, Test: 0.6902\n",
            "Early stopping:  0.027441354641826215\n",
            "Epoch: 046, Loss: 0.6537, Train: 0.8213, Test: 0.6927\n",
            "Early stopping:  0.02714825099704497\n",
            "Epoch: 047, Loss: 0.6369, Train: 0.8279, Test: 0.6953\n",
            "Early stopping:  0.026850007143508065\n",
            "Epoch: 048, Loss: 0.6204, Train: 0.8321, Test: 0.6976\n",
            "Early stopping:  0.02654187261844676\n",
            "Epoch: 049, Loss: 0.6037, Train: 0.8378, Test: 0.6987\n",
            "Early stopping:  0.026429721640423398\n",
            "Epoch: 050, Loss: 0.5874, Train: 0.8429, Test: 0.7014\n",
            "Early stopping:  0.026209019760499934\n",
            "Epoch: 051, Loss: 0.5713, Train: 0.8483, Test: 0.7019\n",
            "Early stopping:  0.025942708049092943\n",
            "Epoch: 052, Loss: 0.5552, Train: 0.8545, Test: 0.7035\n",
            "Early stopping:  0.02574813659139174\n",
            "Epoch: 053, Loss: 0.5392, Train: 0.8607, Test: 0.7057\n",
            "Early stopping:  0.02552479155827295\n",
            "Epoch: 054, Loss: 0.5237, Train: 0.8661, Test: 0.7089\n",
            "Early stopping:  0.02523983202550931\n",
            "Epoch: 055, Loss: 0.5102, Train: 0.8582, Test: 0.7065\n",
            "Early stopping:  0.024324352120678943\n",
            "Epoch: 056, Loss: 0.5028, Train: 0.8749, Test: 0.7074\n",
            "Early stopping:  0.02131640433574254\n",
            "Epoch: 057, Loss: 0.4939, Train: 0.8746, Test: 0.7108\n",
            "Early stopping:  0.01781710198230364\n",
            "Epoch: 058, Loss: 0.4697, Train: 0.8840, Test: 0.7129\n",
            "Early stopping:  0.020156979667898874\n",
            "Epoch: 059, Loss: 0.4542, Train: 0.8922, Test: 0.7114\n",
            "Early stopping:  0.023496278224466018\n",
            "Epoch: 060, Loss: 0.4504, Train: 0.8917, Test: 0.7131\n",
            "Early stopping:  0.02343746801146358\n",
            "Epoch: 061, Loss: 0.4301, Train: 0.8996, Test: 0.7129\n",
            "Early stopping:  0.023791089682547697\n",
            "Epoch: 062, Loss: 0.4170, Train: 0.9073, Test: 0.7148\n",
            "Early stopping:  0.020764572358526013\n",
            "Epoch: 063, Loss: 0.4108, Train: 0.9075, Test: 0.7133\n",
            "Early stopping:  0.019391620876588963\n",
            "Epoch: 064, Loss: 0.3929, Train: 0.9098, Test: 0.7131\n",
            "Early stopping:  0.0215140498347833\n",
            "Epoch: 065, Loss: 0.3825, Train: 0.9212, Test: 0.7182\n",
            "Early stopping:  0.019043139034647594\n",
            "Epoch: 066, Loss: 0.3743, Train: 0.9217, Test: 0.7148\n",
            "Early stopping:  0.018188190662502834\n",
            "Epoch: 067, Loss: 0.3589, Train: 0.9226, Test: 0.7144\n",
            "Early stopping:  0.019515362824038367\n",
            "Epoch: 068, Loss: 0.3502, Train: 0.9325, Test: 0.7182\n",
            "Early stopping:  0.01731023225053901\n",
            "Epoch: 069, Loss: 0.3416, Train: 0.9356, Test: 0.7165\n",
            "Early stopping:  0.016838200952175406\n",
            "Epoch: 070, Loss: 0.3273, Train: 0.9353, Test: 0.7150\n",
            "Early stopping:  0.017726833743026988\n",
            "Epoch: 071, Loss: 0.3203, Train: 0.9467, Test: 0.7167\n",
            "Early stopping:  0.015909514188950067\n",
            "Epoch: 072, Loss: 0.3116, Train: 0.9467, Test: 0.7167\n",
            "Early stopping:  0.015666115436046648\n",
            "Epoch: 073, Loss: 0.2991, Train: 0.9458, Test: 0.7180\n",
            "Early stopping:  0.016025181446116366\n",
            "Epoch: 074, Loss: 0.2924, Train: 0.9532, Test: 0.7191\n",
            "Early stopping:  0.014462904490143471\n",
            "Epoch: 075, Loss: 0.2846, Train: 0.9566, Test: 0.7195\n",
            "Early stopping:  0.0144080609154933\n",
            "Epoch: 076, Loss: 0.2743, Train: 0.9549, Test: 0.7176\n",
            "Early stopping:  0.01417075870505241\n",
            "Epoch: 077, Loss: 0.2675, Train: 0.9609, Test: 0.7193\n",
            "Early stopping:  0.012888300254299939\n",
            "Epoch: 078, Loss: 0.2622, Train: 0.9597, Test: 0.7169\n",
            "Early stopping:  0.012327099054942168\n",
            "Epoch: 079, Loss: 0.2559, Train: 0.9620, Test: 0.7191\n",
            "Early stopping:  0.011069733539125591\n",
            "Epoch: 080, Loss: 0.2510, Train: 0.9648, Test: 0.7167\n",
            "Early stopping:  0.009181100738557517\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  2], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.64      0.78      0.71       568\n",
            "         capital_goods       0.64      0.60      0.62       381\n",
            "conglomerates_industry       0.91      0.35      0.51        60\n",
            "     consumer_cyclical       0.71      0.62      0.66       595\n",
            " consumer_non-cyclical       0.71      0.64      0.68       334\n",
            "                energy       0.78      0.79      0.79       213\n",
            "             financial       0.74      0.76      0.75       576\n",
            "            healthcare       0.81      0.77      0.79       238\n",
            "              services       0.74      0.76      0.75      1557\n",
            "            technology       0.65      0.57      0.61       297\n",
            "        transportation       0.74      0.80      0.77       303\n",
            "             utilities       0.76      0.74      0.75       169\n",
            "\n",
            "              accuracy                           0.72      5291\n",
            "             macro avg       0.74      0.68      0.70      5291\n",
            "          weighted avg       0.72      0.72      0.71      5291\n",
            "\n",
            "time: 2min 22s (started: 2024-10-16 21:43:29 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmIkQ6s_gWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef676a9-6320-4a8c-fb4e-96e50f1d668f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 439 ms (started: 2024-10-16 21:45:51 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heWth3vvgWjM"
      },
      "source": [
        "### Training rotulated base = 60% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSX_-Z5kgWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6480098c-b63e-46d5-a8f7-00a33ce5e348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 643 µs (started: 2024-10-16 21:45:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pAnFSX5gWjM"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBxFw2p4gWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cc0eaf-2aa3-4bc7-d735-4cc4c58fa28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 72.8832, Train: 0.2945, Test: 0.2940\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 228.0030, Train: 0.1115, Test: 0.1111\n",
            "Early stopping:  109.68625478547897\n",
            "Epoch: 003, Loss: 540.5949, Train: 0.1161, Test: 0.1109\n",
            "Early stopping:  238.23310531891005\n",
            "Epoch: 004, Loss: 348.7739, Train: 0.1083, Test: 0.1086\n",
            "Early stopping:  197.48980206121362\n",
            "Epoch: 005, Loss: 386.3484, Train: 0.0788, Test: 0.0794\n",
            "Early stopping:  175.57963221895253\n",
            "Epoch: 006, Loss: 336.7390, Train: 0.0641, Test: 0.0649\n",
            "Early stopping:  112.99521701431166\n",
            "Epoch: 007, Loss: 281.9962, Train: 0.2781, Test: 0.2659\n",
            "Early stopping:  97.82152749108369\n",
            "Epoch: 008, Loss: 141.9578, Train: 0.2905, Test: 0.2889\n",
            "Early stopping:  95.50216530683569\n",
            "Epoch: 009, Loss: 148.7307, Train: 0.2909, Test: 0.2883\n",
            "Early stopping:  110.28127416596163\n",
            "Epoch: 010, Loss: 109.0522, Train: 0.2015, Test: 0.2090\n",
            "Early stopping:  99.52583837842187\n",
            "Epoch: 011, Loss: 66.4484, Train: 0.0953, Test: 0.0964\n",
            "Early stopping:  80.84893723150375\n",
            "Epoch: 012, Loss: 47.3791, Train: 0.0996, Test: 0.0887\n",
            "Early stopping:  44.93032662129915\n",
            "Epoch: 013, Loss: 26.2335, Train: 0.1136, Test: 0.1024\n",
            "Early stopping:  49.258575027505024\n",
            "Epoch: 014, Loss: 13.8874, Train: 0.0722, Test: 0.0706\n",
            "Early stopping:  37.41576186878094\n",
            "Epoch: 015, Loss: 8.7276, Train: 0.0669, Test: 0.0635\n",
            "Early stopping:  24.10759417157333\n",
            "Epoch: 016, Loss: 5.4088, Train: 0.1478, Test: 0.1449\n",
            "Early stopping:  17.0680174021711\n",
            "Epoch: 017, Loss: 3.3481, Train: 0.2042, Test: 0.1988\n",
            "Early stopping:  9.138034237147911\n",
            "Epoch: 018, Loss: 3.0402, Train: 0.2202, Test: 0.2180\n",
            "Early stopping:  4.5249071569397765\n",
            "Epoch: 019, Loss: 2.8729, Train: 0.2393, Test: 0.2362\n",
            "Early stopping:  2.482055473464758\n",
            "Epoch: 020, Loss: 2.6614, Train: 0.2569, Test: 0.2506\n",
            "Early stopping:  1.114522040642605\n",
            "Epoch: 021, Loss: 2.5005, Train: 0.2667, Test: 0.2580\n",
            "Early stopping:  0.33036664919448727\n",
            "Epoch: 022, Loss: 2.4226, Train: 0.2611, Test: 0.2569\n",
            "Early stopping:  0.2568459573019695\n",
            "Epoch: 023, Loss: 2.3811, Train: 0.2597, Test: 0.2574\n",
            "Early stopping:  0.2014447063283372\n",
            "Epoch: 024, Loss: 2.3424, Train: 0.2692, Test: 0.2674\n",
            "Early stopping:  0.12612963856783715\n",
            "Epoch: 025, Loss: 2.2907, Train: 0.2919, Test: 0.2952\n",
            "Early stopping:  0.07974472374964849\n",
            "Epoch: 026, Loss: 2.2180, Train: 0.3259, Test: 0.3218\n",
            "Early stopping:  0.07971782314860965\n",
            "Epoch: 027, Loss: 2.1630, Train: 0.3261, Test: 0.3283\n",
            "Early stopping:  0.08911007808078383\n",
            "Epoch: 028, Loss: 2.1579, Train: 0.3261, Test: 0.3176\n",
            "Early stopping:  0.08072234383905966\n",
            "Epoch: 029, Loss: 2.1158, Train: 0.3163, Test: 0.3153\n",
            "Early stopping:  0.06746688721094533\n",
            "Epoch: 030, Loss: 2.0994, Train: 0.3085, Test: 0.3065\n",
            "Early stopping:  0.046353154826917084\n",
            "Epoch: 031, Loss: 2.0939, Train: 0.3081, Test: 0.3076\n",
            "Early stopping:  0.0325160144107543\n",
            "Epoch: 032, Loss: 2.0773, Train: 0.3140, Test: 0.3139\n",
            "Early stopping:  0.030646431968203155\n",
            "Epoch: 033, Loss: 2.0513, Train: 0.3221, Test: 0.3258\n",
            "Early stopping:  0.024454330691359008\n",
            "Epoch: 034, Loss: 2.0294, Train: 0.3321, Test: 0.3346\n",
            "Early stopping:  0.029525221135969285\n",
            "Epoch: 035, Loss: 2.0120, Train: 0.3512, Test: 0.3519\n",
            "Early stopping:  0.03356156913739996\n",
            "Epoch: 036, Loss: 1.9881, Train: 0.3609, Test: 0.3623\n",
            "Early stopping:  0.0344921404935998\n",
            "Epoch: 037, Loss: 1.9619, Train: 0.3688, Test: 0.3604\n",
            "Early stopping:  0.03488672725252367\n",
            "Epoch: 038, Loss: 1.9457, Train: 0.3741, Test: 0.3672\n",
            "Early stopping:  0.03448650098475131\n",
            "Epoch: 039, Loss: 1.9311, Train: 0.3800, Test: 0.3675\n",
            "Early stopping:  0.03256237137219137\n",
            "Epoch: 040, Loss: 1.9122, Train: 0.3802, Test: 0.3777\n",
            "Early stopping:  0.02906575909206681\n",
            "Epoch: 041, Loss: 1.8984, Train: 0.3836, Test: 0.3811\n",
            "Early stopping:  0.025407704278856105\n",
            "Epoch: 042, Loss: 1.8874, Train: 0.3843, Test: 0.3828\n",
            "Early stopping:  0.023712062033026702\n",
            "Epoch: 043, Loss: 1.8688, Train: 0.3875, Test: 0.3893\n",
            "Early stopping:  0.023693296616401363\n",
            "Epoch: 044, Loss: 1.8537, Train: 0.3926, Test: 0.3873\n",
            "Early stopping:  0.02323745370892361\n",
            "Epoch: 045, Loss: 1.8416, Train: 0.3941, Test: 0.3916\n",
            "Early stopping:  0.023354674734638783\n",
            "Epoch: 046, Loss: 1.8271, Train: 0.4055, Test: 0.3950\n",
            "Early stopping:  0.023435500795145586\n",
            "Epoch: 047, Loss: 1.8124, Train: 0.4093, Test: 0.3986\n",
            "Early stopping:  0.02205203335074349\n",
            "Epoch: 048, Loss: 1.8001, Train: 0.4138, Test: 0.4040\n",
            "Early stopping:  0.021567513714700438\n",
            "Epoch: 049, Loss: 1.7878, Train: 0.4168, Test: 0.4077\n",
            "Early stopping:  0.02129858939530587\n",
            "Epoch: 050, Loss: 1.7743, Train: 0.4208, Test: 0.4100\n",
            "Early stopping:  0.02060700375207259\n",
            "Epoch: 051, Loss: 1.7633, Train: 0.4234, Test: 0.4094\n",
            "Early stopping:  0.019628568558014647\n",
            "Epoch: 052, Loss: 1.7521, Train: 0.4244, Test: 0.4063\n",
            "Early stopping:  0.019076926553240208\n",
            "Epoch: 053, Loss: 1.7404, Train: 0.4246, Test: 0.4060\n",
            "Early stopping:  0.018520992337965413\n",
            "Epoch: 054, Loss: 1.7318, Train: 0.4272, Test: 0.4105\n",
            "Early stopping:  0.017080807688788932\n",
            "Epoch: 055, Loss: 1.7187, Train: 0.4276, Test: 0.4125\n",
            "Early stopping:  0.017335960145346052\n",
            "Epoch: 056, Loss: 1.7091, Train: 0.4282, Test: 0.4174\n",
            "Early stopping:  0.017044492947809735\n",
            "Epoch: 057, Loss: 1.6987, Train: 0.4314, Test: 0.4191\n",
            "Early stopping:  0.016803553639268498\n",
            "Epoch: 058, Loss: 1.6882, Train: 0.4353, Test: 0.4216\n",
            "Early stopping:  0.01698321885873184\n",
            "Epoch: 059, Loss: 1.6765, Train: 0.4376, Test: 0.4247\n",
            "Early stopping:  0.01665406149211808\n",
            "Epoch: 060, Loss: 1.6633, Train: 0.4405, Test: 0.4284\n",
            "Early stopping:  0.018011257896811437\n",
            "Epoch: 061, Loss: 1.6515, Train: 0.4437, Test: 0.4318\n",
            "Early stopping:  0.01887111505374816\n",
            "Epoch: 062, Loss: 1.6448, Train: 0.4448, Test: 0.4346\n",
            "Early stopping:  0.017757930020436935\n",
            "Epoch: 063, Loss: 1.6352, Train: 0.4512, Test: 0.4366\n",
            "Early stopping:  0.01609301672212954\n",
            "Epoch: 064, Loss: 1.6230, Train: 0.4558, Test: 0.4398\n",
            "Early stopping:  0.015371838233735767\n",
            "Epoch: 065, Loss: 1.6131, Train: 0.4614, Test: 0.4432\n",
            "Early stopping:  0.015657918163171476\n",
            "Epoch: 066, Loss: 1.6047, Train: 0.4647, Test: 0.4463\n",
            "Early stopping:  0.016224503807814445\n",
            "Epoch: 067, Loss: 1.5956, Train: 0.4673, Test: 0.4488\n",
            "Early stopping:  0.015456504098765967\n",
            "Epoch: 068, Loss: 1.5850, Train: 0.4754, Test: 0.4522\n",
            "Early stopping:  0.014786516683482244\n",
            "Epoch: 069, Loss: 1.5739, Train: 0.4771, Test: 0.4531\n",
            "Early stopping:  0.015535121329522268\n",
            "Epoch: 070, Loss: 1.5662, Train: 0.4800, Test: 0.4576\n",
            "Early stopping:  0.015629228219613094\n",
            "Epoch: 071, Loss: 1.5573, Train: 0.4851, Test: 0.4647\n",
            "Early stopping:  0.015151282920724152\n",
            "Epoch: 072, Loss: 1.5456, Train: 0.4917, Test: 0.4684\n",
            "Early stopping:  0.015135738219487825\n",
            "Epoch: 073, Loss: 1.5379, Train: 0.4960, Test: 0.4698\n",
            "Early stopping:  0.014677046984918228\n",
            "Epoch: 074, Loss: 1.5299, Train: 0.5008, Test: 0.4743\n",
            "Early stopping:  0.014579659471288248\n",
            "Epoch: 075, Loss: 1.5190, Train: 0.5013, Test: 0.4741\n",
            "Early stopping:  0.01463073108723557\n",
            "Epoch: 076, Loss: 1.5101, Train: 0.5028, Test: 0.4777\n",
            "Early stopping:  0.014256802396327574\n",
            "Epoch: 077, Loss: 1.5029, Train: 0.5087, Test: 0.4820\n",
            "Early stopping:  0.014225185464423374\n",
            "Epoch: 078, Loss: 1.4906, Train: 0.5142, Test: 0.4885\n",
            "Early stopping:  0.015023601733021207\n",
            "Epoch: 079, Loss: 1.4834, Train: 0.5164, Test: 0.4865\n",
            "Early stopping:  0.014367200500498518\n",
            "Epoch: 080, Loss: 1.4752, Train: 0.5181, Test: 0.4894\n",
            "Early stopping:  0.014163332199735347\n",
            "Epoch: 081, Loss: 1.4641, Train: 0.5202, Test: 0.4874\n",
            "Early stopping:  0.0147840059307374\n",
            "Epoch: 082, Loss: 1.4592, Train: 0.5246, Test: 0.4950\n",
            "Early stopping:  0.01306066591323872\n",
            "Epoch: 083, Loss: 1.4467, Train: 0.5280, Test: 0.4967\n",
            "Early stopping:  0.014252350103632603\n",
            "Epoch: 084, Loss: 1.4392, Train: 0.5316, Test: 0.4987\n",
            "Early stopping:  0.01423094974564097\n",
            "Epoch: 085, Loss: 1.4321, Train: 0.5323, Test: 0.5033\n",
            "Early stopping:  0.013365512610992624\n",
            "Epoch: 086, Loss: 1.4215, Train: 0.5350, Test: 0.5064\n",
            "Early stopping:  0.01430071351359178\n",
            "Epoch: 087, Loss: 1.4180, Train: 0.5365, Test: 0.5078\n",
            "Early stopping:  0.01197307176461845\n",
            "Epoch: 088, Loss: 1.4066, Train: 0.5388, Test: 0.5067\n",
            "Early stopping:  0.012655060200974809\n",
            "Epoch: 089, Loss: 1.4030, Train: 0.5444, Test: 0.5072\n",
            "Early stopping:  0.01174275442521014\n",
            "Epoch: 090, Loss: 1.3931, Train: 0.5482, Test: 0.5084\n",
            "Early stopping:  0.011517253178248633\n",
            "Epoch: 091, Loss: 1.3874, Train: 0.5503, Test: 0.5109\n",
            "Early stopping:  0.01193590669768122\n",
            "Epoch: 092, Loss: 1.3789, Train: 0.5520, Test: 0.5152\n",
            "Early stopping:  0.01131389621765605\n",
            "Epoch: 093, Loss: 1.3722, Train: 0.5565, Test: 0.5138\n",
            "Early stopping:  0.011992220530480883\n",
            "Epoch: 094, Loss: 1.3656, Train: 0.5584, Test: 0.5186\n",
            "Early stopping:  0.01107546684087635\n",
            "Epoch: 095, Loss: 1.3589, Train: 0.5605, Test: 0.5177\n",
            "Early stopping:  0.011098187644911358\n",
            "Epoch: 096, Loss: 1.3531, Train: 0.5616, Test: 0.5186\n",
            "Early stopping:  0.010252560858170563\n",
            "Epoch: 097, Loss: 1.3463, Train: 0.5628, Test: 0.5183\n",
            "Early stopping:  0.010174546480891486\n",
            "Epoch: 098, Loss: 1.3393, Train: 0.5660, Test: 0.5223\n",
            "Early stopping:  0.010330367634267663\n",
            "Epoch: 099, Loss: 1.3333, Train: 0.5690, Test: 0.5217\n",
            "Early stopping:  0.010315090502339322\n",
            "Epoch: 100, Loss: 1.3257, Train: 0.5699, Test: 0.5242\n",
            "Early stopping:  0.010732555907030258\n",
            "Epoch: 101, Loss: 1.3197, Train: 0.5728, Test: 0.5257\n",
            "Early stopping:  0.010572679376086072\n",
            "Epoch: 102, Loss: 1.3132, Train: 0.5747, Test: 0.5254\n",
            "Early stopping:  0.010415513114780442\n",
            "Epoch: 103, Loss: 1.3079, Train: 0.5739, Test: 0.5268\n",
            "Early stopping:  0.010016678698903346\n",
            "Epoch: 104, Loss: 1.3051, Train: 0.5771, Test: 0.5319\n",
            "Early stopping:  0.008456015057106397\n",
            "PREDICTIONS -> tensor([ 9, 11,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.47      0.61      0.53       379\n",
            "         capital_goods       0.28      0.02      0.04       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.40      0.48      0.44       396\n",
            " consumer_non-cyclical       0.67      0.38      0.48       223\n",
            "                energy       0.66      0.42      0.51       141\n",
            "             financial       0.76      0.58      0.66       384\n",
            "            healthcare       0.72      0.51      0.60       159\n",
            "              services       0.51      0.76      0.61      1038\n",
            "            technology       0.41      0.19      0.26       198\n",
            "        transportation       0.78      0.53      0.63       202\n",
            "             utilities       0.48      0.57      0.52       113\n",
            "\n",
            "              accuracy                           0.53      3527\n",
            "             macro avg       0.51      0.42      0.44      3527\n",
            "          weighted avg       0.53      0.53      0.51      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 97.8822, Train: 0.0363, Test: 0.0380\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 301.2881, Train: 0.2444, Test: 0.2470\n",
            "Early stopping:  143.82974989284486\n",
            "Epoch: 003, Loss: 211.7414, Train: 0.1323, Test: 0.1310\n",
            "Early stopping:  101.94487001509788\n",
            "Epoch: 004, Loss: 221.5844, Train: 0.2163, Test: 0.2070\n",
            "Early stopping:  83.719946019617\n",
            "Epoch: 005, Loss: 178.4780, Train: 0.1531, Test: 0.1395\n",
            "Early stopping:  73.70583000886347\n",
            "Epoch: 006, Loss: 276.8264, Train: 0.2794, Test: 0.2705\n",
            "Early stopping:  50.03682290673954\n",
            "Epoch: 007, Loss: 346.4602, Train: 0.1198, Test: 0.1179\n",
            "Early stopping:  65.89069310080363\n",
            "Epoch: 008, Loss: 382.2428, Train: 0.2841, Test: 0.2804\n",
            "Early stopping:  84.5480254057158\n",
            "Epoch: 009, Loss: 375.2343, Train: 0.1391, Test: 0.1429\n",
            "Early stopping:  85.41857871542845\n",
            "Epoch: 010, Loss: 351.5897, Train: 0.0794, Test: 0.0794\n",
            "Early stopping:  41.7837375192993\n",
            "Epoch: 011, Loss: 402.8260, Train: 0.2433, Test: 0.2390\n",
            "Early stopping:  23.097270115238853\n",
            "Epoch: 012, Loss: 305.8722, Train: 0.2815, Test: 0.2776\n",
            "Early stopping:  37.07533394458424\n",
            "Epoch: 013, Loss: 262.8267, Train: 0.0941, Test: 0.0862\n",
            "Early stopping:  55.773571952923135\n",
            "Epoch: 014, Loss: 246.6869, Train: 0.1524, Test: 0.1457\n",
            "Early stopping:  64.26283527804088\n",
            "Epoch: 015, Loss: 207.7386, Train: 0.2577, Test: 0.2566\n",
            "Early stopping:  74.57764315333336\n",
            "Epoch: 016, Loss: 171.6862, Train: 0.3248, Test: 0.3275\n",
            "Early stopping:  51.49539498275999\n",
            "Epoch: 017, Loss: 148.9074, Train: 0.1325, Test: 0.1316\n",
            "Early stopping:  48.25039137737235\n",
            "Epoch: 018, Loss: 156.5692, Train: 0.1227, Test: 0.1256\n",
            "Early stopping:  40.62596886411546\n",
            "Epoch: 019, Loss: 140.3885, Train: 0.1333, Test: 0.1304\n",
            "Early stopping:  26.49079311096374\n",
            "Epoch: 020, Loss: 108.5025, Train: 0.1900, Test: 0.1902\n",
            "Early stopping:  23.528621098531023\n",
            "Epoch: 021, Loss: 76.6776, Train: 0.3250, Test: 0.3232\n",
            "Early stopping:  33.18472985367869\n",
            "Epoch: 022, Loss: 50.9692, Train: 0.3198, Test: 0.3144\n",
            "Early stopping:  43.68164473372613\n",
            "Epoch: 023, Loss: 28.9376, Train: 0.2633, Test: 0.2634\n",
            "Early stopping:  44.48134879695561\n",
            "Epoch: 024, Loss: 12.0334, Train: 0.2924, Test: 0.2875\n",
            "Early stopping:  38.31826168237206\n",
            "Epoch: 025, Loss: 5.5162, Train: 0.2745, Test: 0.2648\n",
            "Early stopping:  29.263453152583512\n",
            "Epoch: 026, Loss: 4.5579, Train: 0.2641, Test: 0.2535\n",
            "Early stopping:  19.68417415896482\n",
            "Epoch: 027, Loss: 4.1076, Train: 0.2741, Test: 0.2611\n",
            "Early stopping:  10.510702828043414\n",
            "Epoch: 028, Loss: 3.6495, Train: 0.2817, Test: 0.2767\n",
            "Early stopping:  3.457526056888687\n",
            "Epoch: 029, Loss: 3.2341, Train: 0.2951, Test: 0.2878\n",
            "Early stopping:  0.8809381394850021\n",
            "Epoch: 030, Loss: 2.8827, Train: 0.3053, Test: 0.2940\n",
            "Early stopping:  0.6687369928906021\n",
            "Epoch: 031, Loss: 2.6041, Train: 0.3123, Test: 0.3076\n",
            "Early stopping:  0.5993803060697045\n",
            "Epoch: 032, Loss: 2.4133, Train: 0.3259, Test: 0.3187\n",
            "Early stopping:  0.4954640833516661\n",
            "Epoch: 033, Loss: 2.2895, Train: 0.3325, Test: 0.3249\n",
            "Early stopping:  0.37991697178746975\n",
            "Epoch: 034, Loss: 2.2082, Train: 0.3329, Test: 0.3235\n",
            "Early stopping:  0.2702626928040172\n",
            "Epoch: 035, Loss: 2.1554, Train: 0.3371, Test: 0.3261\n",
            "Early stopping:  0.17952961587312374\n",
            "Epoch: 036, Loss: 2.1216, Train: 0.3374, Test: 0.3312\n",
            "Early stopping:  0.11686788831085333\n",
            "Epoch: 037, Loss: 2.1001, Train: 0.3476, Test: 0.3329\n",
            "Early stopping:  0.07594481828580013\n",
            "Epoch: 038, Loss: 2.0867, Train: 0.3533, Test: 0.3436\n",
            "Early stopping:  0.04875216697105995\n",
            "Epoch: 039, Loss: 2.0769, Train: 0.3590, Test: 0.3493\n",
            "Early stopping:  0.03130147624547837\n",
            "Epoch: 040, Loss: 2.0679, Train: 0.3611, Test: 0.3536\n",
            "Early stopping:  0.021033372508335695\n",
            "Epoch: 041, Loss: 2.0599, Train: 0.3643, Test: 0.3598\n",
            "Early stopping:  0.015771249062990596\n",
            "Epoch: 042, Loss: 2.0515, Train: 0.3694, Test: 0.3657\n",
            "Early stopping:  0.013837713148000507\n",
            "Epoch: 043, Loss: 2.0429, Train: 0.3709, Test: 0.3703\n",
            "Early stopping:  0.013330554635823184\n",
            "Epoch: 044, Loss: 2.0346, Train: 0.3722, Test: 0.3706\n",
            "Early stopping:  0.013217567600944096\n",
            "Epoch: 045, Loss: 2.0273, Train: 0.3756, Test: 0.3700\n",
            "Early stopping:  0.01297915746639531\n",
            "Epoch: 046, Loss: 2.0212, Train: 0.3777, Test: 0.3717\n",
            "Early stopping:  0.012081101773807744\n",
            "Epoch: 047, Loss: 2.0160, Train: 0.3809, Test: 0.3771\n",
            "Early stopping:  0.010672053166410912\n",
            "Epoch: 048, Loss: 2.0113, Train: 0.3815, Test: 0.3762\n",
            "Early stopping:  0.009198806687805318\n",
            "PREDICTIONS -> tensor([3, 6, 3,  ..., 8, 3, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.25      0.01      0.01       379\n",
            "         capital_goods       0.00      0.00      0.00       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.15      0.44      0.23       396\n",
            " consumer_non-cyclical       0.54      0.25      0.34       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.53      0.50      0.52       384\n",
            "            healthcare       0.82      0.23      0.36       159\n",
            "              services       0.46      0.78      0.58      1038\n",
            "            technology       0.33      0.02      0.03       198\n",
            "        transportation       0.65      0.25      0.36       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.38      3527\n",
            "             macro avg       0.31      0.21      0.20      3527\n",
            "          weighted avg       0.36      0.38      0.31      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 55.5327, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 512.4736, Train: 0.1164, Test: 0.1162\n",
            "Early stopping:  323.1060432764955\n",
            "Epoch: 003, Loss: 372.3509, Train: 0.0737, Test: 0.0695\n",
            "Early stopping:  234.09513738411567\n",
            "Epoch: 004, Loss: 236.6483, Train: 0.1219, Test: 0.1222\n",
            "Early stopping:  194.95745328083424\n",
            "Epoch: 005, Loss: 252.0988, Train: 0.1147, Test: 0.1148\n",
            "Early stopping:  169.88723773836486\n",
            "Epoch: 006, Loss: 227.4258, Train: 0.0970, Test: 0.0930\n",
            "Early stopping:  122.38727158294289\n",
            "Epoch: 007, Loss: 146.4789, Train: 0.1030, Test: 0.1024\n",
            "Early stopping:  81.1382367765386\n",
            "Epoch: 008, Loss: 78.0980, Train: 0.2892, Test: 0.2915\n",
            "Early stopping:  73.87837896119042\n",
            "Epoch: 009, Loss: 50.7733, Train: 0.2355, Test: 0.2288\n",
            "Early stopping:  88.66004942479682\n",
            "Epoch: 010, Loss: 35.8971, Train: 0.1563, Test: 0.1554\n",
            "Early stopping:  79.23171457012995\n",
            "Epoch: 011, Loss: 25.4882, Train: 0.1928, Test: 0.1919\n",
            "Early stopping:  48.45973540180207\n",
            "Epoch: 012, Loss: 12.8774, Train: 0.2692, Test: 0.2676\n",
            "Early stopping:  25.14268999898028\n",
            "Epoch: 013, Loss: 5.7783, Train: 0.3104, Test: 0.3141\n",
            "Early stopping:  17.97858895532356\n",
            "Epoch: 014, Loss: 4.4646, Train: 0.3291, Test: 0.3292\n",
            "Early stopping:  13.50146808936976\n",
            "Epoch: 015, Loss: 4.0701, Train: 0.3584, Test: 0.3555\n",
            "Early stopping:  9.08783684594278\n",
            "Epoch: 016, Loss: 3.6574, Train: 0.3633, Test: 0.3598\n",
            "Early stopping:  3.8331888393789626\n",
            "Epoch: 017, Loss: 3.3372, Train: 0.3425, Test: 0.3399\n",
            "Early stopping:  0.9484010502385366\n",
            "Epoch: 018, Loss: 3.1378, Train: 0.3319, Test: 0.3340\n",
            "Early stopping:  0.539781661523607\n",
            "Epoch: 019, Loss: 2.9510, Train: 0.3414, Test: 0.3408\n",
            "Early stopping:  0.4429685142443356\n",
            "Epoch: 020, Loss: 2.7569, Train: 0.3408, Test: 0.3405\n",
            "Early stopping:  0.3482543752085389\n",
            "Epoch: 021, Loss: 2.6402, Train: 0.3359, Test: 0.3306\n",
            "Early stopping:  0.28168842684971107\n",
            "Epoch: 022, Loss: 2.5681, Train: 0.3539, Test: 0.3402\n",
            "Early stopping:  0.23318431705804754\n",
            "Epoch: 023, Loss: 2.4741, Train: 0.3707, Test: 0.3672\n",
            "Early stopping:  0.1842796105836963\n",
            "Epoch: 024, Loss: 2.3644, Train: 0.3858, Test: 0.3813\n",
            "Early stopping:  0.1507750134371522\n",
            "Epoch: 025, Loss: 2.2558, Train: 0.3998, Test: 0.4071\n",
            "Early stopping:  0.15426496969930611\n",
            "Epoch: 026, Loss: 2.1704, Train: 0.4147, Test: 0.4185\n",
            "Early stopping:  0.16041471403112484\n",
            "Epoch: 027, Loss: 2.1185, Train: 0.4284, Test: 0.4261\n",
            "Early stopping:  0.14439510877383369\n",
            "Epoch: 028, Loss: 2.0919, Train: 0.4319, Test: 0.4253\n",
            "Early stopping:  0.1110664545858349\n",
            "Epoch: 029, Loss: 2.0780, Train: 0.4359, Test: 0.4242\n",
            "Early stopping:  0.07231125911358705\n",
            "Epoch: 030, Loss: 2.0667, Train: 0.4389, Test: 0.4270\n",
            "Early stopping:  0.04130756852482399\n",
            "Epoch: 031, Loss: 2.0451, Train: 0.4444, Test: 0.4276\n",
            "Early stopping:  0.02750665318059583\n",
            "Epoch: 032, Loss: 2.0116, Train: 0.4433, Test: 0.4332\n",
            "Early stopping:  0.0313883850426899\n",
            "Epoch: 033, Loss: 1.9840, Train: 0.4399, Test: 0.4312\n",
            "Early stopping:  0.03898894271512369\n",
            "Epoch: 034, Loss: 1.9590, Train: 0.4348, Test: 0.4233\n",
            "Early stopping:  0.043795843894907566\n",
            "Epoch: 035, Loss: 1.9318, Train: 0.4478, Test: 0.4358\n",
            "Early stopping:  0.04420583188141493\n",
            "Epoch: 036, Loss: 1.8965, Train: 0.4522, Test: 0.4412\n",
            "Early stopping:  0.04472671994949797\n",
            "Epoch: 037, Loss: 1.8655, Train: 0.4548, Test: 0.4440\n",
            "Early stopping:  0.04743924820193184\n",
            "Epoch: 038, Loss: 1.8214, Train: 0.4563, Test: 0.4417\n",
            "Early stopping:  0.054177717648176756\n",
            "Epoch: 039, Loss: 1.7916, Train: 0.4595, Test: 0.4488\n",
            "Early stopping:  0.05632078305684738\n",
            "Epoch: 040, Loss: 1.7659, Train: 0.4698, Test: 0.4610\n",
            "Early stopping:  0.053226081636679336\n",
            "Epoch: 041, Loss: 1.7336, Train: 0.4779, Test: 0.4670\n",
            "Early stopping:  0.05074140152054407\n",
            "Epoch: 042, Loss: 1.6972, Train: 0.4905, Test: 0.4698\n",
            "Early stopping:  0.04851834691425497\n",
            "Epoch: 043, Loss: 1.6675, Train: 0.4949, Test: 0.4695\n",
            "Early stopping:  0.0501684667087379\n",
            "Epoch: 044, Loss: 1.6462, Train: 0.4987, Test: 0.4809\n",
            "Early stopping:  0.04851038380903993\n",
            "Epoch: 045, Loss: 1.6225, Train: 0.5083, Test: 0.4919\n",
            "Early stopping:  0.04346157612266836\n",
            "Epoch: 046, Loss: 1.6005, Train: 0.5168, Test: 0.4953\n",
            "Early stopping:  0.03775204412393502\n",
            "Epoch: 047, Loss: 1.5767, Train: 0.5208, Test: 0.4970\n",
            "Early stopping:  0.035913410552235896\n",
            "Epoch: 048, Loss: 1.5546, Train: 0.5242, Test: 0.4987\n",
            "Early stopping:  0.036188855963943375\n",
            "Epoch: 049, Loss: 1.5370, Train: 0.5310, Test: 0.5027\n",
            "Early stopping:  0.03434327701697829\n",
            "Epoch: 050, Loss: 1.5154, Train: 0.5293, Test: 0.5067\n",
            "Early stopping:  0.03326332263181568\n",
            "Epoch: 051, Loss: 1.5013, Train: 0.5331, Test: 0.5061\n",
            "Early stopping:  0.030124693552655468\n",
            "Epoch: 052, Loss: 1.4868, Train: 0.5376, Test: 0.5078\n",
            "Early stopping:  0.027169383795164353\n",
            "Epoch: 053, Loss: 1.4701, Train: 0.5412, Test: 0.5112\n",
            "Early stopping:  0.02575054933638996\n",
            "Epoch: 054, Loss: 1.4571, Train: 0.5422, Test: 0.5138\n",
            "Early stopping:  0.023357914020419736\n",
            "Epoch: 055, Loss: 1.4446, Train: 0.5507, Test: 0.5194\n",
            "Early stopping:  0.0226671482151992\n",
            "Epoch: 056, Loss: 1.4316, Train: 0.5541, Test: 0.5217\n",
            "Early stopping:  0.021541357219283196\n",
            "Epoch: 057, Loss: 1.4168, Train: 0.5569, Test: 0.5282\n",
            "Early stopping:  0.020915059017765466\n",
            "Epoch: 058, Loss: 1.4018, Train: 0.5586, Test: 0.5288\n",
            "Early stopping:  0.021909825047668044\n",
            "Epoch: 059, Loss: 1.3939, Train: 0.5671, Test: 0.5359\n",
            "Early stopping:  0.020834660438712398\n",
            "Epoch: 060, Loss: 1.3758, Train: 0.5709, Test: 0.5356\n",
            "Early stopping:  0.021350104908989878\n",
            "Epoch: 061, Loss: 1.3648, Train: 0.5735, Test: 0.5356\n",
            "Early stopping:  0.020636225939098588\n",
            "Epoch: 062, Loss: 1.3510, Train: 0.5756, Test: 0.5356\n",
            "Early stopping:  0.020775553733761762\n",
            "Epoch: 063, Loss: 1.3479, Train: 0.5775, Test: 0.5376\n",
            "Early stopping:  0.018889592848238105\n",
            "Epoch: 064, Loss: 1.3347, Train: 0.5811, Test: 0.5410\n",
            "Early stopping:  0.015901551270220776\n",
            "Epoch: 065, Loss: 1.3273, Train: 0.5858, Test: 0.5441\n",
            "Early stopping:  0.014648296823389154\n",
            "Epoch: 066, Loss: 1.3145, Train: 0.5894, Test: 0.5472\n",
            "Early stopping:  0.01500785121826237\n",
            "Epoch: 067, Loss: 1.2989, Train: 0.5926, Test: 0.5464\n",
            "Early stopping:  0.01879691652254258\n",
            "Epoch: 068, Loss: 1.2960, Train: 0.5958, Test: 0.5495\n",
            "Early stopping:  0.016991381763998063\n",
            "Epoch: 069, Loss: 1.2775, Train: 0.5972, Test: 0.5500\n",
            "Early stopping:  0.018957985780297004\n",
            "Epoch: 070, Loss: 1.2741, Train: 0.5996, Test: 0.5489\n",
            "Early stopping:  0.016599609898937658\n",
            "Epoch: 071, Loss: 1.2625, Train: 0.6043, Test: 0.5580\n",
            "Early stopping:  0.015390076266050622\n",
            "Epoch: 072, Loss: 1.2509, Train: 0.6045, Test: 0.5520\n",
            "Early stopping:  0.016946657034259464\n",
            "Epoch: 073, Loss: 1.2454, Train: 0.6096, Test: 0.5532\n",
            "Early stopping:  0.013994128055333503\n",
            "Epoch: 074, Loss: 1.2308, Train: 0.6125, Test: 0.5551\n",
            "Early stopping:  0.016492394172962638\n",
            "Epoch: 075, Loss: 1.2252, Train: 0.6104, Test: 0.5568\n",
            "Early stopping:  0.015112616683012212\n",
            "Epoch: 076, Loss: 1.2179, Train: 0.6164, Test: 0.5568\n",
            "Early stopping:  0.013803419127455679\n",
            "Epoch: 077, Loss: 1.2080, Train: 0.6185, Test: 0.5540\n",
            "Early stopping:  0.01403834916308295\n",
            "Epoch: 078, Loss: 1.1979, Train: 0.6214, Test: 0.5588\n",
            "Early stopping:  0.01319085891249453\n",
            "Epoch: 079, Loss: 1.1910, Train: 0.6253, Test: 0.5631\n",
            "Early stopping:  0.013988375740813686\n",
            "Epoch: 080, Loss: 1.1858, Train: 0.6282, Test: 0.5639\n",
            "Early stopping:  0.012963026494322396\n",
            "Epoch: 081, Loss: 1.1730, Train: 0.6267, Test: 0.5656\n",
            "Early stopping:  0.01312292970305908\n",
            "Epoch: 082, Loss: 1.1684, Train: 0.6299, Test: 0.5651\n",
            "Early stopping:  0.012322568166927252\n",
            "Epoch: 083, Loss: 1.1625, Train: 0.6333, Test: 0.5673\n",
            "Early stopping:  0.011940462748386236\n",
            "Epoch: 084, Loss: 1.1515, Train: 0.6302, Test: 0.5634\n",
            "Early stopping:  0.012676537425867222\n",
            "Epoch: 085, Loss: 1.1495, Train: 0.6327, Test: 0.5693\n",
            "Early stopping:  0.010285472698245314\n",
            "Epoch: 086, Loss: 1.1454, Train: 0.6365, Test: 0.5688\n",
            "Early stopping:  0.009613485082834949\n",
            "PREDICTIONS -> tensor([ 9,  0,  3,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.54      0.53      0.53       379\n",
            "         capital_goods       0.34      0.19      0.25       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.43      0.51      0.47       396\n",
            " consumer_non-cyclical       0.68      0.43      0.53       223\n",
            "                energy       0.69      0.52      0.59       141\n",
            "             financial       0.71      0.62      0.66       384\n",
            "            healthcare       0.66      0.53      0.59       159\n",
            "              services       0.56      0.78      0.65      1038\n",
            "            technology       0.57      0.25      0.35       198\n",
            "        transportation       0.66      0.68      0.67       202\n",
            "             utilities       0.73      0.61      0.66       113\n",
            "\n",
            "              accuracy                           0.57      3527\n",
            "             macro avg       0.55      0.47      0.50      3527\n",
            "          weighted avg       0.57      0.57      0.55      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 68.9502, Train: 0.1688, Test: 0.1659\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 279.0587, Train: 0.2902, Test: 0.2889\n",
            "Early stopping:  148.56912726736735\n",
            "Epoch: 003, Loss: 292.3348, Train: 0.0372, Test: 0.0369\n",
            "Early stopping:  125.31460750295109\n",
            "Epoch: 004, Loss: 465.4970, Train: 0.0643, Test: 0.0593\n",
            "Early stopping:  162.3310176034101\n",
            "Epoch: 005, Loss: 507.8399, Train: 0.1102, Test: 0.1100\n",
            "Early stopping:  174.55897086574856\n",
            "Epoch: 006, Loss: 581.3138, Train: 0.0115, Test: 0.0113\n",
            "Early stopping:  134.01043740516494\n",
            "Epoch: 007, Loss: 465.9918, Train: 0.0611, Test: 0.0610\n",
            "Early stopping:  106.24237099819602\n",
            "Epoch: 008, Loss: 372.3597, Train: 0.1968, Test: 0.1965\n",
            "Early stopping:  75.86625954878407\n",
            "Epoch: 009, Loss: 248.1830, Train: 0.3045, Test: 0.3034\n",
            "Early stopping:  128.93622005299608\n",
            "Epoch: 010, Loss: 245.4685, Train: 0.3159, Test: 0.3133\n",
            "Early stopping:  144.41192087798936\n",
            "Epoch: 011, Loss: 206.3668, Train: 0.3153, Test: 0.3181\n",
            "Early stopping:  108.30585741929126\n",
            "Epoch: 012, Loss: 143.6606, Train: 0.2474, Test: 0.2382\n",
            "Early stopping:  83.63162648307704\n",
            "Epoch: 013, Loss: 89.8819, Train: 0.3325, Test: 0.3380\n",
            "Early stopping:  68.64184991935551\n",
            "Epoch: 014, Loss: 33.0590, Train: 0.3057, Test: 0.3073\n",
            "Early stopping:  85.76388740600929\n",
            "Epoch: 015, Loss: 8.9508, Train: 0.2526, Test: 0.2470\n",
            "Early stopping:  80.66625400995018\n",
            "Epoch: 016, Loss: 5.1821, Train: 0.2465, Test: 0.2399\n",
            "Early stopping:  59.48661248344332\n",
            "Epoch: 017, Loss: 4.6893, Train: 0.2459, Test: 0.2492\n",
            "Early stopping:  36.33614214695102\n",
            "Epoch: 018, Loss: 4.0549, Train: 0.2563, Test: 0.2472\n",
            "Early stopping:  12.37467908986087\n",
            "Epoch: 019, Loss: 3.3900, Train: 0.2592, Test: 0.2560\n",
            "Early stopping:  2.1738594639899262\n",
            "Epoch: 020, Loss: 2.8485, Train: 0.2554, Test: 0.2444\n",
            "Early stopping:  0.944438452628383\n",
            "Epoch: 021, Loss: 2.5249, Train: 0.2312, Test: 0.2223\n",
            "Early stopping:  0.8817007174665064\n",
            "Epoch: 022, Loss: 2.4084, Train: 0.2085, Test: 0.1917\n",
            "Early stopping:  0.6806850175746347\n",
            "Epoch: 023, Loss: 2.3729, Train: 0.1987, Test: 0.1885\n",
            "Early stopping:  0.4244119759332304\n",
            "Epoch: 024, Loss: 2.3595, Train: 0.1781, Test: 0.1715\n",
            "Early stopping:  0.20394201374214244\n",
            "Epoch: 025, Loss: 2.3565, Train: 0.1667, Test: 0.1562\n",
            "Early stopping:  0.07043936242526468\n",
            "Epoch: 026, Loss: 2.3573, Train: 0.1569, Test: 0.1537\n",
            "Early stopping:  0.021972382663235934\n",
            "Epoch: 027, Loss: 2.3585, Train: 0.1501, Test: 0.1500\n",
            "Early stopping:  0.00676914528402096\n",
            "PREDICTIONS -> tensor([ 9, 10,  9,  ..., 10, 10,  7], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.76      0.05      0.09       379\n",
            "         capital_goods       0.00      0.00      0.00       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.20      0.37      0.26       396\n",
            " consumer_non-cyclical       0.50      0.09      0.15       223\n",
            "                energy       0.17      0.16      0.16       141\n",
            "             financial       0.85      0.11      0.19       384\n",
            "            healthcare       0.10      0.56      0.16       159\n",
            "              services       0.72      0.03      0.05      1038\n",
            "            technology       0.31      0.14      0.20       198\n",
            "        transportation       0.07      0.52      0.13       202\n",
            "             utilities       0.82      0.29      0.43       113\n",
            "\n",
            "              accuracy                           0.15      3527\n",
            "             macro avg       0.38      0.19      0.15      3527\n",
            "          weighted avg       0.50      0.15      0.13      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 36.3268, Train: 0.2820, Test: 0.2815\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 370.1721, Train: 0.1060, Test: 0.1069\n",
            "Early stopping:  236.0642827176531\n",
            "Epoch: 003, Loss: 847.6321, Train: 0.1140, Test: 0.1151\n",
            "Early stopping:  407.76566536473376\n",
            "Epoch: 004, Loss: 486.4513, Train: 0.0921, Test: 0.0882\n",
            "Early stopping:  334.6915878738673\n",
            "Epoch: 005, Loss: 386.2407, Train: 0.1013, Test: 0.1058\n",
            "Early stopping:  290.6753892543504\n",
            "Epoch: 006, Loss: 213.0132, Train: 0.0803, Test: 0.0814\n",
            "Early stopping:  237.41727757808258\n",
            "Epoch: 007, Loss: 214.6248, Train: 0.3244, Test: 0.3195\n",
            "Early stopping:  261.2408701108302\n",
            "Epoch: 008, Loss: 157.5883, Train: 0.3004, Test: 0.3003\n",
            "Early stopping:  138.7300436887784\n",
            "Epoch: 009, Loss: 141.4199, Train: 0.1985, Test: 0.2036\n",
            "Early stopping:  97.14746498503247\n",
            "Epoch: 010, Loss: 115.5514, Train: 0.0881, Test: 0.0958\n",
            "Early stopping:  44.05919006854285\n",
            "Epoch: 011, Loss: 94.1385, Train: 0.0809, Test: 0.0822\n",
            "Early stopping:  46.0229498592362\n",
            "Epoch: 012, Loss: 77.2968, Train: 0.0851, Test: 0.0836\n",
            "Early stopping:  32.95693635458131\n",
            "Epoch: 013, Loss: 64.6516, Train: 0.2072, Test: 0.2104\n",
            "Early stopping:  30.606843600459776\n",
            "Epoch: 014, Loss: 44.3034, Train: 0.3051, Test: 0.3025\n",
            "Early stopping:  27.28384538257794\n",
            "Epoch: 015, Loss: 29.4463, Train: 0.2548, Test: 0.2560\n",
            "Early stopping:  25.723595345083123\n",
            "Epoch: 016, Loss: 15.6499, Train: 0.1060, Test: 0.1069\n",
            "Early stopping:  25.124909341011723\n",
            "Epoch: 017, Loss: 8.4822, Train: 0.0673, Test: 0.0629\n",
            "Early stopping:  22.600641431409816\n",
            "Epoch: 018, Loss: 6.6562, Train: 0.0747, Test: 0.0712\n",
            "Early stopping:  15.854106773463704\n",
            "Epoch: 019, Loss: 5.1382, Train: 0.0983, Test: 0.1004\n",
            "Early stopping:  9.999210571772139\n",
            "Epoch: 020, Loss: 3.9269, Train: 0.1580, Test: 0.1508\n",
            "Early stopping:  4.618833355585179\n",
            "Epoch: 021, Loss: 3.0748, Train: 0.2265, Test: 0.2282\n",
            "Early stopping:  2.1626016612360015\n",
            "Epoch: 022, Loss: 2.6521, Train: 0.2996, Test: 0.3034\n",
            "Early stopping:  1.6286063307654797\n",
            "Epoch: 023, Loss: 2.4585, Train: 0.3217, Test: 0.3317\n",
            "Early stopping:  1.0997775677981818\n",
            "Epoch: 024, Loss: 2.4059, Train: 0.3113, Test: 0.3314\n",
            "Early stopping:  0.629593220804328\n",
            "Epoch: 025, Loss: 2.3243, Train: 0.3042, Test: 0.3193\n",
            "Early stopping:  0.30017367033744563\n",
            "Epoch: 026, Loss: 2.2451, Train: 0.3062, Test: 0.3122\n",
            "Early stopping:  0.15434769272693571\n",
            "Epoch: 027, Loss: 2.2025, Train: 0.3127, Test: 0.3139\n",
            "Early stopping:  0.10692905819492342\n",
            "Epoch: 028, Loss: 2.1687, Train: 0.3174, Test: 0.3210\n",
            "Early stopping:  0.09598714030171893\n",
            "Epoch: 029, Loss: 2.1415, Train: 0.3278, Test: 0.3275\n",
            "Early stopping:  0.07168135868473123\n",
            "Epoch: 030, Loss: 2.1256, Train: 0.3344, Test: 0.3439\n",
            "Early stopping:  0.048104287812618135\n",
            "Epoch: 031, Loss: 2.1103, Train: 0.3509, Test: 0.3527\n",
            "Early stopping:  0.03655080662487374\n",
            "Epoch: 032, Loss: 2.0923, Train: 0.3648, Test: 0.3655\n",
            "Early stopping:  0.029259663419477524\n",
            "Epoch: 033, Loss: 2.0744, Train: 0.3807, Test: 0.3757\n",
            "Early stopping:  0.026498559034970035\n",
            "Epoch: 034, Loss: 2.0555, Train: 0.3868, Test: 0.3791\n",
            "Early stopping:  0.027876275235121414\n",
            "Epoch: 035, Loss: 2.0332, Train: 0.3881, Test: 0.3777\n",
            "Early stopping:  0.030236077026008203\n",
            "Epoch: 036, Loss: 2.0129, Train: 0.3843, Test: 0.3791\n",
            "Early stopping:  0.03166308110777569\n",
            "Epoch: 037, Loss: 1.9970, Train: 0.3858, Test: 0.3794\n",
            "Early stopping:  0.031250623126533525\n",
            "Epoch: 038, Loss: 1.9805, Train: 0.3934, Test: 0.3862\n",
            "Early stopping:  0.029525051934445982\n",
            "Epoch: 039, Loss: 1.9623, Train: 0.4028, Test: 0.3941\n",
            "Early stopping:  0.027565652217173043\n",
            "Epoch: 040, Loss: 1.9446, Train: 0.4060, Test: 0.3969\n",
            "Early stopping:  0.027101418320287157\n",
            "Epoch: 041, Loss: 1.9264, Train: 0.4091, Test: 0.3992\n",
            "Early stopping:  0.028005646303387675\n",
            "Epoch: 042, Loss: 1.9081, Train: 0.4166, Test: 0.3998\n",
            "Early stopping:  0.0285587557875459\n",
            "Epoch: 043, Loss: 1.8901, Train: 0.4195, Test: 0.4029\n",
            "Early stopping:  0.028579081575338144\n",
            "Epoch: 044, Loss: 1.8737, Train: 0.4380, Test: 0.4202\n",
            "Early stopping:  0.028157959513846836\n",
            "Epoch: 045, Loss: 1.8592, Train: 0.4350, Test: 0.4171\n",
            "Early stopping:  0.026742814117090383\n",
            "Epoch: 046, Loss: 1.8466, Train: 0.4363, Test: 0.4216\n",
            "Early stopping:  0.024399917432664512\n",
            "Epoch: 047, Loss: 1.8319, Train: 0.4382, Test: 0.4230\n",
            "Early stopping:  0.022696370453294694\n",
            "Epoch: 048, Loss: 1.8179, Train: 0.4380, Test: 0.4244\n",
            "Early stopping:  0.021951481343258777\n",
            "Epoch: 049, Loss: 1.8048, Train: 0.4408, Test: 0.4253\n",
            "Early stopping:  0.02173432847713621\n",
            "Epoch: 050, Loss: 1.7919, Train: 0.4448, Test: 0.4247\n",
            "Early stopping:  0.021592848095787622\n",
            "Epoch: 051, Loss: 1.7791, Train: 0.4510, Test: 0.4244\n",
            "Early stopping:  0.02083210924137251\n",
            "Epoch: 052, Loss: 1.7663, Train: 0.4495, Test: 0.4270\n",
            "Early stopping:  0.020395270090962498\n",
            "Epoch: 053, Loss: 1.7518, Train: 0.4482, Test: 0.4270\n",
            "Early stopping:  0.020841901420462577\n",
            "Epoch: 054, Loss: 1.7381, Train: 0.4524, Test: 0.4298\n",
            "Early stopping:  0.021331862170078524\n",
            "Epoch: 055, Loss: 1.7239, Train: 0.4529, Test: 0.4310\n",
            "Early stopping:  0.021883209350675694\n",
            "Epoch: 056, Loss: 1.7103, Train: 0.4522, Test: 0.4318\n",
            "Early stopping:  0.022101842259247734\n",
            "Epoch: 057, Loss: 1.6983, Train: 0.4560, Test: 0.4341\n",
            "Early stopping:  0.021306076576049174\n",
            "Epoch: 058, Loss: 1.6852, Train: 0.4575, Test: 0.4369\n",
            "Early stopping:  0.020803832836411986\n",
            "Epoch: 059, Loss: 1.6726, Train: 0.4612, Test: 0.4409\n",
            "Early stopping:  0.020210291818963316\n",
            "Epoch: 060, Loss: 1.6609, Train: 0.4677, Test: 0.4491\n",
            "Early stopping:  0.019690996346035175\n",
            "Epoch: 061, Loss: 1.6482, Train: 0.4707, Test: 0.4514\n",
            "Early stopping:  0.019702784238191155\n",
            "Epoch: 062, Loss: 1.6354, Train: 0.4732, Test: 0.4568\n",
            "Early stopping:  0.01959239872476112\n",
            "Epoch: 063, Loss: 1.6189, Train: 0.4764, Test: 0.4519\n",
            "Early stopping:  0.021070901142394294\n",
            "Epoch: 064, Loss: 1.6042, Train: 0.4802, Test: 0.4536\n",
            "Early stopping:  0.022608782354459858\n",
            "Epoch: 065, Loss: 1.5904, Train: 0.4819, Test: 0.4562\n",
            "Early stopping:  0.023242068670480696\n",
            "Epoch: 066, Loss: 1.5798, Train: 0.4883, Test: 0.4579\n",
            "Early stopping:  0.022176480465734446\n",
            "Epoch: 067, Loss: 1.5694, Train: 0.4911, Test: 0.4593\n",
            "Early stopping:  0.01956430803364956\n",
            "Epoch: 068, Loss: 1.5600, Train: 0.4943, Test: 0.4627\n",
            "Early stopping:  0.017320958255607564\n",
            "Epoch: 069, Loss: 1.5530, Train: 0.4972, Test: 0.4636\n",
            "Early stopping:  0.01497178232505501\n",
            "Epoch: 070, Loss: 1.5455, Train: 0.5008, Test: 0.4661\n",
            "Early stopping:  0.013478630878908297\n",
            "Epoch: 071, Loss: 1.5386, Train: 0.5021, Test: 0.4687\n",
            "Early stopping:  0.012062155143686122\n",
            "Epoch: 072, Loss: 1.5293, Train: 0.5051, Test: 0.4718\n",
            "Early stopping:  0.012032304915107046\n",
            "Epoch: 073, Loss: 1.5209, Train: 0.5091, Test: 0.4741\n",
            "Early stopping:  0.012737007750138142\n",
            "Epoch: 074, Loss: 1.5114, Train: 0.5136, Test: 0.4741\n",
            "Early stopping:  0.01359768784503771\n",
            "Epoch: 075, Loss: 1.5033, Train: 0.5170, Test: 0.4777\n",
            "Early stopping:  0.013993830130117778\n",
            "Epoch: 076, Loss: 1.4957, Train: 0.5183, Test: 0.4817\n",
            "Early stopping:  0.01340561712675767\n",
            "Epoch: 077, Loss: 1.4888, Train: 0.5181, Test: 0.4814\n",
            "Early stopping:  0.012662444500145514\n",
            "Epoch: 078, Loss: 1.4823, Train: 0.5193, Test: 0.4826\n",
            "Early stopping:  0.011517339083599403\n",
            "Epoch: 079, Loss: 1.4754, Train: 0.5248, Test: 0.4865\n",
            "Early stopping:  0.010957955926846705\n",
            "Epoch: 080, Loss: 1.4689, Train: 0.5270, Test: 0.4882\n",
            "Early stopping:  0.01058823950442824\n",
            "Epoch: 081, Loss: 1.4617, Train: 0.5289, Test: 0.4897\n",
            "Early stopping:  0.010673210293393557\n",
            "Epoch: 082, Loss: 1.4554, Train: 0.5301, Test: 0.4908\n",
            "Early stopping:  0.010655671549644568\n",
            "Epoch: 083, Loss: 1.4491, Train: 0.5316, Test: 0.4914\n",
            "Early stopping:  0.010446971069899858\n",
            "Epoch: 084, Loss: 1.4426, Train: 0.5338, Test: 0.4936\n",
            "Early stopping:  0.010333699007816917\n",
            "Epoch: 085, Loss: 1.4364, Train: 0.5359, Test: 0.4931\n",
            "Early stopping:  0.010034904539915585\n",
            "Epoch: 086, Loss: 1.4301, Train: 0.5389, Test: 0.4965\n",
            "Early stopping:  0.01002699067362668\n",
            "Epoch: 087, Loss: 1.4240, Train: 0.5393, Test: 0.4948\n",
            "Early stopping:  0.009908100248531563\n",
            "PREDICTIONS -> tensor([ 9,  0, 11,  ...,  8,  6,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.44      0.61      0.51       379\n",
            "         capital_goods       0.32      0.17      0.22       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.46      0.40      0.43       396\n",
            " consumer_non-cyclical       0.70      0.29      0.41       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.56      0.55      0.55       384\n",
            "            healthcare       0.79      0.19      0.30       159\n",
            "              services       0.49      0.80      0.60      1038\n",
            "            technology       0.55      0.14      0.22       198\n",
            "        transportation       0.58      0.68      0.62       202\n",
            "             utilities       0.59      0.17      0.26       113\n",
            "\n",
            "              accuracy                           0.49      3527\n",
            "             macro avg       0.46      0.33      0.34      3527\n",
            "          weighted avg       0.49      0.49      0.45      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 50.2559, Train: 0.2802, Test: 0.2827\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 213.2916, Train: 0.0516, Test: 0.0502\n",
            "Early stopping:  115.28367725812396\n",
            "Epoch: 003, Loss: 497.4585, Train: 0.1206, Test: 0.1171\n",
            "Early stopping:  226.3189543696719\n",
            "Epoch: 004, Loss: 473.8097, Train: 0.0643, Test: 0.0629\n",
            "Early stopping:  215.0868718762411\n",
            "Epoch: 005, Loss: 552.6711, Train: 0.1195, Test: 0.1239\n",
            "Early stopping:  215.8721189343559\n",
            "Epoch: 006, Loss: 559.1595, Train: 0.1301, Test: 0.1245\n",
            "Early stopping:  142.19424333934006\n",
            "Epoch: 007, Loss: 481.5386, Train: 0.0786, Test: 0.0825\n",
            "Early stopping:  40.22349067914232\n",
            "Epoch: 008, Loss: 371.8305, Train: 0.1112, Test: 0.1029\n",
            "Early stopping:  75.80287045770957\n",
            "Epoch: 009, Loss: 223.5132, Train: 0.3062, Test: 0.2986\n",
            "Early stopping:  141.51623145189356\n",
            "Epoch: 010, Loss: 193.7690, Train: 0.3085, Test: 0.3000\n",
            "Early stopping:  158.63462478997292\n",
            "Epoch: 011, Loss: 175.2820, Train: 0.3051, Test: 0.2986\n",
            "Early stopping:  132.49629856940712\n",
            "Epoch: 012, Loss: 124.2304, Train: 0.1009, Test: 0.0947\n",
            "Early stopping:  93.40578750715012\n",
            "Epoch: 013, Loss: 103.2001, Train: 0.0605, Test: 0.0536\n",
            "Early stopping:  49.58213666594117\n",
            "Epoch: 014, Loss: 91.8692, Train: 0.0786, Test: 0.0689\n",
            "Early stopping:  44.8003153470818\n",
            "Epoch: 015, Loss: 68.9840, Train: 0.1159, Test: 0.1046\n",
            "Early stopping:  40.263875983626164\n",
            "Epoch: 016, Loss: 50.1865, Train: 0.1779, Test: 0.1783\n",
            "Early stopping:  28.938103905686024\n",
            "Epoch: 017, Loss: 37.7101, Train: 0.2144, Test: 0.2078\n",
            "Early stopping:  27.45902491958185\n",
            "Epoch: 018, Loss: 26.9522, Train: 0.3323, Test: 0.3314\n",
            "Early stopping:  25.804944558954972\n",
            "Epoch: 019, Loss: 16.7713, Train: 0.3397, Test: 0.3374\n",
            "Early stopping:  20.364320911432756\n",
            "Epoch: 020, Loss: 8.4249, Train: 0.2813, Test: 0.2852\n",
            "Early stopping:  16.559665945109597\n",
            "Epoch: 021, Loss: 4.5788, Train: 0.2206, Test: 0.2169\n",
            "Early stopping:  13.583191899241578\n",
            "Epoch: 022, Loss: 4.6214, Train: 0.2081, Test: 0.1993\n",
            "Early stopping:  9.595178189141773\n",
            "Epoch: 023, Loss: 4.4823, Train: 0.2055, Test: 0.1948\n",
            "Early stopping:  5.299966599609122\n",
            "Epoch: 024, Loss: 4.0792, Train: 0.2043, Test: 0.1993\n",
            "Early stopping:  1.7947840784872293\n",
            "Epoch: 025, Loss: 3.5838, Train: 0.2282, Test: 0.2291\n",
            "Early stopping:  0.43906735455734874\n",
            "Epoch: 026, Loss: 3.1180, Train: 0.2820, Test: 0.2779\n",
            "Early stopping:  0.6271847530683945\n",
            "Epoch: 027, Loss: 2.7718, Train: 0.3134, Test: 0.3207\n",
            "Early stopping:  0.6939727871972298\n",
            "Epoch: 028, Loss: 2.5404, Train: 0.3265, Test: 0.3348\n",
            "Early stopping:  0.621248919930195\n",
            "Epoch: 029, Loss: 2.3972, Train: 0.3374, Test: 0.3451\n",
            "Early stopping:  0.47754261231009887\n",
            "Epoch: 030, Loss: 2.3277, Train: 0.3425, Test: 0.3575\n",
            "Early stopping:  0.3209066701515019\n",
            "Epoch: 031, Loss: 2.2827, Train: 0.3501, Test: 0.3570\n",
            "Early stopping:  0.19779892721919295\n",
            "Epoch: 032, Loss: 2.2290, Train: 0.3544, Test: 0.3592\n",
            "Early stopping:  0.1204133807207697\n",
            "Epoch: 033, Loss: 2.1673, Train: 0.3535, Test: 0.3561\n",
            "Early stopping:  0.088473537900354\n",
            "Epoch: 034, Loss: 2.1164, Train: 0.3516, Test: 0.3561\n",
            "Early stopping:  0.08515173731444883\n",
            "Epoch: 035, Loss: 2.0950, Train: 0.3444, Test: 0.3459\n",
            "Early stopping:  0.07802701165788313\n",
            "Epoch: 036, Loss: 2.0950, Train: 0.3371, Test: 0.3323\n",
            "Early stopping:  0.05759996227834869\n",
            "Epoch: 037, Loss: 2.1014, Train: 0.3346, Test: 0.3261\n",
            "Early stopping:  0.030497516453229845\n",
            "Epoch: 038, Loss: 2.1008, Train: 0.3457, Test: 0.3351\n",
            "Early stopping:  0.008755057181471123\n",
            "PREDICTIONS -> tensor([ 3,  8,  5,  ..., 11,  5,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       1.00      0.01      0.01       379\n",
            "         capital_goods       1.00      0.02      0.03       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.27      0.51      0.35       396\n",
            " consumer_non-cyclical       0.47      0.25      0.32       223\n",
            "                energy       0.09      0.18      0.12       141\n",
            "             financial       0.48      0.15      0.23       384\n",
            "            healthcare       0.00      0.00      0.00       159\n",
            "              services       0.48      0.60      0.53      1038\n",
            "            technology       0.07      0.18      0.10       198\n",
            "        transportation       0.66      0.51      0.57       202\n",
            "             utilities       0.28      0.67      0.39       113\n",
            "\n",
            "              accuracy                           0.34      3527\n",
            "             macro avg       0.40      0.26      0.22      3527\n",
            "          weighted avg       0.49      0.34      0.30      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 92.7249, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 512.7358, Train: 0.1125, Test: 0.1137\n",
            "Early stopping:  296.9926112106709\n",
            "Epoch: 003, Loss: 416.4617, Train: 0.1006, Test: 0.1007\n",
            "Early stopping:  220.03162164602486\n",
            "Epoch: 004, Loss: 253.3805, Train: 0.0694, Test: 0.0672\n",
            "Early stopping:  184.87708105517626\n",
            "Epoch: 005, Loss: 275.1383, Train: 0.2032, Test: 0.2005\n",
            "Early stopping:  161.29590475067886\n",
            "Epoch: 006, Loss: 195.7294, Train: 0.2917, Test: 0.2892\n",
            "Early stopping:  130.1677695630042\n",
            "Epoch: 007, Loss: 223.6003, Train: 0.2348, Test: 0.2362\n",
            "Early stopping:  85.70552724611774\n",
            "Epoch: 008, Loss: 214.1259, Train: 0.0688, Test: 0.0689\n",
            "Early stopping:  31.712992994341242\n",
            "Epoch: 009, Loss: 238.1100, Train: 0.1633, Test: 0.1559\n",
            "Early stopping:  29.868403463795286\n",
            "Epoch: 010, Loss: 188.3376, Train: 0.1945, Test: 0.1840\n",
            "Early stopping:  20.282130126560507\n",
            "Epoch: 011, Loss: 160.0364, Train: 0.1667, Test: 0.1687\n",
            "Early stopping:  30.922320917982024\n",
            "Epoch: 012, Loss: 136.0637, Train: 0.2724, Test: 0.2733\n",
            "Early stopping:  40.83753696616046\n",
            "Epoch: 013, Loss: 107.9226, Train: 0.2826, Test: 0.2838\n",
            "Early stopping:  50.008751696578074\n",
            "Epoch: 014, Loss: 71.4789, Train: 0.1811, Test: 0.1730\n",
            "Early stopping:  45.32122873308557\n",
            "Epoch: 015, Loss: 55.1303, Train: 0.2195, Test: 0.2115\n",
            "Early stopping:  43.608511808890924\n",
            "Epoch: 016, Loss: 29.5183, Train: 0.1618, Test: 0.1523\n",
            "Early stopping:  42.31525947733615\n",
            "Epoch: 017, Loss: 23.0187, Train: 0.1643, Test: 0.1542\n",
            "Early stopping:  34.3342734650977\n",
            "Epoch: 018, Loss: 16.2881, Train: 0.1922, Test: 0.1891\n",
            "Early stopping:  23.32468710484253\n",
            "Epoch: 019, Loss: 7.5065, Train: 0.2214, Test: 0.2192\n",
            "Early stopping:  18.067557974862716\n",
            "Epoch: 020, Loss: 2.9937, Train: 0.2233, Test: 0.2084\n",
            "Early stopping:  10.877730711160453\n",
            "Epoch: 021, Loss: 2.5089, Train: 0.2384, Test: 0.2223\n",
            "Early stopping:  8.935543090289668\n",
            "Epoch: 022, Loss: 2.3458, Train: 0.2567, Test: 0.2461\n",
            "Early stopping:  5.961397170403745\n",
            "Epoch: 023, Loss: 2.2732, Train: 0.2853, Test: 0.2745\n",
            "Early stopping:  2.2430180294323234\n",
            "Epoch: 024, Loss: 2.2413, Train: 0.3287, Test: 0.3187\n",
            "Early stopping:  0.30911177564697234\n",
            "Epoch: 025, Loss: 2.2717, Train: 0.3180, Test: 0.3124\n",
            "Early stopping:  0.10806399702794502\n",
            "Epoch: 026, Loss: 2.2367, Train: 0.3028, Test: 0.2940\n",
            "Early stopping:  0.043638274858592556\n",
            "Epoch: 027, Loss: 2.2336, Train: 0.3009, Test: 0.2883\n",
            "Early stopping:  0.01949534591776036\n",
            "Epoch: 028, Loss: 2.2315, Train: 0.3072, Test: 0.2912\n",
            "Early stopping:  0.016477398478430494\n",
            "Epoch: 029, Loss: 2.2199, Train: 0.3293, Test: 0.3088\n",
            "Early stopping:  0.019519454034800408\n",
            "Epoch: 030, Loss: 2.1957, Train: 0.3482, Test: 0.3292\n",
            "Early stopping:  0.01679647396009741\n",
            "Epoch: 031, Loss: 2.1559, Train: 0.3707, Test: 0.3618\n",
            "Early stopping:  0.032455434654297136\n",
            "Epoch: 032, Loss: 2.1171, Train: 0.3762, Test: 0.3655\n",
            "Early stopping:  0.04729824992952123\n",
            "Epoch: 033, Loss: 2.1268, Train: 0.3794, Test: 0.3743\n",
            "Early stopping:  0.04408402708946125\n",
            "Epoch: 034, Loss: 2.0803, Train: 0.3820, Test: 0.3655\n",
            "Early stopping:  0.043307832269387114\n",
            "Epoch: 035, Loss: 2.0557, Train: 0.3807, Test: 0.3640\n",
            "Early stopping:  0.03945397566318858\n",
            "Epoch: 036, Loss: 2.0517, Train: 0.3820, Test: 0.3604\n",
            "Early stopping:  0.034471463769474235\n",
            "Epoch: 037, Loss: 2.0364, Train: 0.3845, Test: 0.3612\n",
            "Early stopping:  0.035349703383614546\n",
            "Epoch: 038, Loss: 2.0121, Train: 0.3896, Test: 0.3726\n",
            "Early stopping:  0.02518246873710274\n",
            "Epoch: 039, Loss: 1.9931, Train: 0.3898, Test: 0.3808\n",
            "Early stopping:  0.0267163037457079\n",
            "Epoch: 040, Loss: 1.9893, Train: 0.3955, Test: 0.3862\n",
            "Early stopping:  0.02710892265801235\n",
            "Epoch: 041, Loss: 1.9758, Train: 0.3983, Test: 0.3842\n",
            "Early stopping:  0.023504615099123875\n",
            "Epoch: 042, Loss: 1.9547, Train: 0.4013, Test: 0.3802\n",
            "Early stopping:  0.021336472138392567\n",
            "Epoch: 043, Loss: 1.9433, Train: 0.3983, Test: 0.3782\n",
            "Early stopping:  0.021673159904799722\n",
            "Epoch: 044, Loss: 1.9372, Train: 0.3992, Test: 0.3808\n",
            "Early stopping:  0.021980191815537335\n",
            "Epoch: 045, Loss: 1.9240, Train: 0.4057, Test: 0.3867\n",
            "Early stopping:  0.01951760116377236\n",
            "Epoch: 046, Loss: 1.9049, Train: 0.4078, Test: 0.3930\n",
            "Early stopping:  0.01915746596646548\n",
            "Epoch: 047, Loss: 1.8925, Train: 0.4129, Test: 0.3967\n",
            "Early stopping:  0.021470447360920332\n",
            "Epoch: 048, Loss: 1.8849, Train: 0.4187, Test: 0.4006\n",
            "Early stopping:  0.021760925034509243\n",
            "Epoch: 049, Loss: 1.8689, Train: 0.4198, Test: 0.4066\n",
            "Early stopping:  0.020807594535297992\n",
            "Epoch: 050, Loss: 1.8527, Train: 0.4208, Test: 0.4063\n",
            "Early stopping:  0.020392756069492406\n",
            "Epoch: 051, Loss: 1.8440, Train: 0.4216, Test: 0.4074\n",
            "Early stopping:  0.02055646453136059\n",
            "Epoch: 052, Loss: 1.8348, Train: 0.4234, Test: 0.4105\n",
            "Early stopping:  0.01997989781529298\n",
            "Epoch: 053, Loss: 1.8207, Train: 0.4282, Test: 0.4137\n",
            "Early stopping:  0.01817163213977874\n",
            "Epoch: 054, Loss: 1.8074, Train: 0.4295, Test: 0.4210\n",
            "Early stopping:  0.018120807855503723\n",
            "Epoch: 055, Loss: 1.7994, Train: 0.4327, Test: 0.4244\n",
            "Early stopping:  0.018508021472067777\n",
            "Epoch: 056, Loss: 1.7875, Train: 0.4391, Test: 0.4256\n",
            "Early stopping:  0.018407212263697133\n",
            "Epoch: 057, Loss: 1.7725, Train: 0.4425, Test: 0.4239\n",
            "Early stopping:  0.018497802045102825\n",
            "Epoch: 058, Loss: 1.7638, Train: 0.4473, Test: 0.4312\n",
            "Early stopping:  0.018146987771376355\n",
            "Epoch: 059, Loss: 1.7522, Train: 0.4482, Test: 0.4361\n",
            "Early stopping:  0.018722581204093883\n",
            "Epoch: 060, Loss: 1.7391, Train: 0.4543, Test: 0.4392\n",
            "Early stopping:  0.01855865508565093\n",
            "Epoch: 061, Loss: 1.7296, Train: 0.4569, Test: 0.4415\n",
            "Early stopping:  0.01749966530229858\n",
            "Epoch: 062, Loss: 1.7179, Train: 0.4607, Test: 0.4437\n",
            "Early stopping:  0.018104938932287158\n",
            "Epoch: 063, Loss: 1.7069, Train: 0.4592, Test: 0.4434\n",
            "Early stopping:  0.017688658128892087\n",
            "Epoch: 064, Loss: 1.6978, Train: 0.4614, Test: 0.4474\n",
            "Early stopping:  0.016645346411141248\n",
            "Epoch: 065, Loss: 1.6878, Train: 0.4645, Test: 0.4494\n",
            "Early stopping:  0.01642896172190363\n",
            "Epoch: 066, Loss: 1.6799, Train: 0.4656, Test: 0.4500\n",
            "Early stopping:  0.01508866947333868\n",
            "Epoch: 067, Loss: 1.6700, Train: 0.4707, Test: 0.4514\n",
            "Early stopping:  0.014517834372812247\n",
            "Epoch: 068, Loss: 1.6613, Train: 0.4756, Test: 0.4522\n",
            "Early stopping:  0.014360947973799441\n",
            "Epoch: 069, Loss: 1.6515, Train: 0.4807, Test: 0.4556\n",
            "Early stopping:  0.014422373648919698\n",
            "Epoch: 070, Loss: 1.6403, Train: 0.4841, Test: 0.4610\n",
            "Early stopping:  0.01546291777081933\n",
            "Epoch: 071, Loss: 1.6253, Train: 0.4892, Test: 0.4596\n",
            "Early stopping:  0.017561922622596395\n",
            "Epoch: 072, Loss: 1.6130, Train: 0.4943, Test: 0.4630\n",
            "Early stopping:  0.019453428763880398\n",
            "Epoch: 073, Loss: 1.5958, Train: 0.4962, Test: 0.4681\n",
            "Early stopping:  0.021979515229843316\n",
            "Epoch: 074, Loss: 1.5823, Train: 0.5023, Test: 0.4726\n",
            "Early stopping:  0.023024393820913838\n",
            "Epoch: 075, Loss: 1.5681, Train: 0.5032, Test: 0.4749\n",
            "Early stopping:  0.022964887657281427\n",
            "Epoch: 076, Loss: 1.5553, Train: 0.5093, Test: 0.4814\n",
            "Early stopping:  0.022660472720288743\n",
            "Epoch: 077, Loss: 1.5419, Train: 0.5108, Test: 0.4792\n",
            "Early stopping:  0.021314319360775772\n",
            "Epoch: 078, Loss: 1.5292, Train: 0.5136, Test: 0.4854\n",
            "Early stopping:  0.02094973539501661\n",
            "Epoch: 079, Loss: 1.5163, Train: 0.5108, Test: 0.4857\n",
            "Early stopping:  0.02051460743099374\n",
            "Epoch: 080, Loss: 1.5067, Train: 0.5125, Test: 0.4880\n",
            "Early stopping:  0.01944128876686665\n",
            "Epoch: 081, Loss: 1.4968, Train: 0.5157, Test: 0.4882\n",
            "Early stopping:  0.017864054693863173\n",
            "Epoch: 082, Loss: 1.4879, Train: 0.5191, Test: 0.4922\n",
            "Early stopping:  0.016185788663034457\n",
            "Epoch: 083, Loss: 1.4772, Train: 0.5185, Test: 0.4931\n",
            "Early stopping:  0.015333336602182935\n",
            "Epoch: 084, Loss: 1.4684, Train: 0.5219, Test: 0.4959\n",
            "Early stopping:  0.015196869631350118\n",
            "Epoch: 085, Loss: 1.4597, Train: 0.5259, Test: 0.4953\n",
            "Early stopping:  0.014802556139970224\n",
            "Epoch: 086, Loss: 1.4528, Train: 0.5274, Test: 0.4956\n",
            "Early stopping:  0.013913998465763671\n",
            "Epoch: 087, Loss: 1.4436, Train: 0.5278, Test: 0.5010\n",
            "Early stopping:  0.013111026972892598\n",
            "Epoch: 088, Loss: 1.4362, Train: 0.5321, Test: 0.5024\n",
            "Early stopping:  0.012750805194382011\n",
            "Epoch: 089, Loss: 1.4272, Train: 0.5325, Test: 0.5047\n",
            "Early stopping:  0.012917558106214732\n",
            "Epoch: 090, Loss: 1.4206, Train: 0.5355, Test: 0.5086\n",
            "Early stopping:  0.012796353811871376\n",
            "Epoch: 091, Loss: 1.4159, Train: 0.5365, Test: 0.5140\n",
            "Early stopping:  0.01129553060036159\n",
            "Epoch: 092, Loss: 1.4091, Train: 0.5384, Test: 0.5092\n",
            "Early stopping:  0.010431613780129028\n",
            "Epoch: 093, Loss: 1.4017, Train: 0.5452, Test: 0.5149\n",
            "Early stopping:  0.009905278735253709\n",
            "PREDICTIONS -> tensor([1, 0, 0,  ..., 5, 5, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.41      0.63      0.50       379\n",
            "         capital_goods       0.32      0.09      0.14       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.48      0.34      0.40       396\n",
            " consumer_non-cyclical       0.77      0.26      0.39       223\n",
            "                energy       0.46      0.52      0.49       141\n",
            "             financial       0.71      0.60      0.65       384\n",
            "            healthcare       0.70      0.41      0.52       159\n",
            "              services       0.51      0.83      0.63      1038\n",
            "            technology       1.00      0.01      0.01       198\n",
            "        transportation       0.56      0.61      0.59       202\n",
            "             utilities       0.22      0.04      0.07       113\n",
            "\n",
            "              accuracy                           0.51      3527\n",
            "             macro avg       0.51      0.36      0.36      3527\n",
            "          weighted avg       0.54      0.51      0.47      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 84.5876, Train: 0.2866, Test: 0.2869\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 347.0593, Train: 0.0155, Test: 0.0136\n",
            "Early stopping:  185.5955261556411\n",
            "Epoch: 003, Loss: 382.7686, Train: 0.1091, Test: 0.1086\n",
            "Early stopping:  162.82835556425655\n",
            "Epoch: 004, Loss: 509.5534, Train: 0.0586, Test: 0.0559\n",
            "Early stopping:  178.45473818542675\n",
            "Epoch: 005, Loss: 323.3261, Train: 0.0522, Test: 0.0491\n",
            "Early stopping:  154.58435945864736\n",
            "Epoch: 006, Loss: 114.3234, Train: 0.2442, Test: 0.2498\n",
            "Early stopping:  142.9421007984073\n",
            "Epoch: 007, Loss: 112.0660, Train: 0.3161, Test: 0.3085\n",
            "Early stopping:  173.5154545835161\n",
            "Epoch: 008, Loss: 102.2476, Train: 0.1422, Test: 0.1367\n",
            "Early stopping:  180.58483621456605\n",
            "Epoch: 009, Loss: 105.0861, Train: 0.1331, Test: 0.1250\n",
            "Early stopping:  96.23064829024358\n",
            "Epoch: 010, Loss: 111.6503, Train: 0.1505, Test: 0.1350\n",
            "Early stopping:  5.139270342087919\n",
            "Epoch: 011, Loss: 94.2705, Train: 0.3047, Test: 0.2988\n",
            "Early stopping:  7.3627339887603895\n",
            "Epoch: 012, Loss: 67.9800, Train: 0.2628, Test: 0.2617\n",
            "Early stopping:  16.987693896804924\n",
            "Epoch: 013, Loss: 67.3481, Train: 0.2773, Test: 0.2745\n",
            "Early stopping:  20.67525727461639\n",
            "Epoch: 014, Loss: 53.4423, Train: 0.3049, Test: 0.2949\n",
            "Early stopping:  23.50326598074968\n",
            "Epoch: 015, Loss: 36.0611, Train: 0.2013, Test: 0.1925\n",
            "Early stopping:  21.419853409836517\n",
            "Epoch: 016, Loss: 42.5459, Train: 0.2098, Test: 0.1976\n",
            "Early stopping:  14.366184790638442\n",
            "Epoch: 017, Loss: 35.7617, Train: 0.1862, Test: 0.1684\n",
            "Early stopping:  13.42890208961083\n",
            "Epoch: 018, Loss: 24.3493, Train: 0.1839, Test: 0.1843\n",
            "Early stopping:  10.645514691865852\n",
            "Epoch: 019, Loss: 12.4350, Train: 0.1669, Test: 0.1684\n",
            "Early stopping:  11.911398341387796\n",
            "Epoch: 020, Loss: 10.5604, Train: 0.1456, Test: 0.1412\n",
            "Early stopping:  14.056946923102188\n",
            "Epoch: 021, Loss: 8.5672, Train: 0.1342, Test: 0.1406\n",
            "Early stopping:  11.516544671269244\n",
            "Epoch: 022, Loss: 6.3183, Train: 0.1707, Test: 0.1718\n",
            "Early stopping:  7.032732713395509\n",
            "Epoch: 023, Loss: 4.8555, Train: 0.2146, Test: 0.2067\n",
            "Early stopping:  3.072666905207125\n",
            "Epoch: 024, Loss: 3.7669, Train: 0.2340, Test: 0.2401\n",
            "Early stopping:  2.7602320008983816\n",
            "Epoch: 025, Loss: 3.1227, Train: 0.2488, Test: 0.2600\n",
            "Early stopping:  2.1792766212940293\n",
            "Epoch: 026, Loss: 2.7316, Train: 0.2900, Test: 0.3020\n",
            "Early stopping:  1.450260565012638\n",
            "Epoch: 027, Loss: 2.3753, Train: 0.3412, Test: 0.3448\n",
            "Early stopping:  0.9775691561340312\n",
            "Epoch: 028, Loss: 2.1669, Train: 0.3749, Test: 0.3777\n",
            "Early stopping:  0.6363141366879846\n",
            "Epoch: 029, Loss: 2.0872, Train: 0.3871, Test: 0.3969\n",
            "Early stopping:  0.4296120805906559\n",
            "Epoch: 030, Loss: 2.0449, Train: 0.3947, Test: 0.4009\n",
            "Early stopping:  0.2820626162839138\n",
            "Epoch: 031, Loss: 2.0188, Train: 0.3911, Test: 0.4012\n",
            "Early stopping:  0.14369387589800214\n",
            "Epoch: 032, Loss: 2.0132, Train: 0.3866, Test: 0.3944\n",
            "Early stopping:  0.06343165335981357\n",
            "Epoch: 033, Loss: 2.0242, Train: 0.3822, Test: 0.3913\n",
            "Early stopping:  0.030149711482578927\n",
            "Epoch: 034, Loss: 2.0309, Train: 0.3843, Test: 0.3921\n",
            "Early stopping:  0.012239445271991702\n",
            "Epoch: 035, Loss: 2.0144, Train: 0.3870, Test: 0.3989\n",
            "Early stopping:  0.0073298642198928775\n",
            "PREDICTIONS -> tensor([3, 6, 3,  ..., 8, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.60      0.16      0.26       379\n",
            "         capital_goods       0.23      0.03      0.05       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.26      0.16      0.20       396\n",
            " consumer_non-cyclical       0.58      0.22      0.32       223\n",
            "                energy       0.41      0.19      0.26       141\n",
            "             financial       0.37      0.73      0.49       384\n",
            "            healthcare       0.69      0.11      0.19       159\n",
            "              services       0.41      0.75      0.53      1038\n",
            "            technology       0.17      0.17      0.17       198\n",
            "        transportation       0.76      0.40      0.53       202\n",
            "             utilities       0.00      0.00      0.00       113\n",
            "\n",
            "              accuracy                           0.40      3527\n",
            "             macro avg       0.37      0.24      0.25      3527\n",
            "          weighted avg       0.41      0.40      0.34      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 69.7165, Train: 0.1076, Test: 0.1103\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 293.5366, Train: 0.2940, Test: 0.2940\n",
            "Early stopping:  158.2646934973473\n",
            "Epoch: 003, Loss: 452.5253, Train: 0.1185, Test: 0.1191\n",
            "Early stopping:  192.31721515118787\n",
            "Epoch: 004, Loss: 592.6407, Train: 0.0921, Test: 0.0879\n",
            "Early stopping:  224.43649755170554\n",
            "Epoch: 005, Loss: 354.8824, Train: 0.1221, Test: 0.1213\n",
            "Early stopping:  194.37167778983212\n",
            "Epoch: 006, Loss: 431.6418, Train: 0.2756, Test: 0.2730\n",
            "Early stopping:  113.03127766437396\n",
            "Epoch: 007, Loss: 244.1101, Train: 0.0524, Test: 0.0493\n",
            "Early stopping:  128.50942747442213\n",
            "Epoch: 008, Loss: 308.2895, Train: 0.1340, Test: 0.1457\n",
            "Early stopping:  134.09236808035317\n",
            "Epoch: 009, Loss: 252.2430, Train: 0.2767, Test: 0.2790\n",
            "Early stopping:  77.70537613341129\n",
            "Epoch: 010, Loss: 220.7060, Train: 0.1025, Test: 0.0950\n",
            "Early stopping:  84.73528782649923\n",
            "Epoch: 011, Loss: 196.3994, Train: 0.1123, Test: 0.1103\n",
            "Early stopping:  41.85965544764048\n",
            "Epoch: 012, Loss: 153.8337, Train: 0.1826, Test: 0.1715\n",
            "Early stopping:  58.249523940794575\n",
            "Epoch: 013, Loss: 96.3410, Train: 0.2136, Test: 0.2053\n",
            "Early stopping:  60.7293755909968\n",
            "Epoch: 014, Loss: 63.0173, Train: 0.2250, Test: 0.2107\n",
            "Early stopping:  66.20906368299187\n",
            "Epoch: 015, Loss: 47.5846, Train: 0.2527, Test: 0.2373\n",
            "Early stopping:  62.58107311532581\n",
            "Epoch: 016, Loss: 25.8478, Train: 0.2817, Test: 0.2770\n",
            "Early stopping:  49.885601891818155\n",
            "Epoch: 017, Loss: 13.0422, Train: 0.3187, Test: 0.3235\n",
            "Early stopping:  32.666943278282034\n",
            "Epoch: 018, Loss: 7.4127, Train: 0.3180, Test: 0.3190\n",
            "Early stopping:  23.477769683062053\n",
            "Epoch: 019, Loss: 4.6070, Train: 0.2964, Test: 0.3028\n",
            "Early stopping:  17.59580306633915\n",
            "Epoch: 020, Loss: 3.6296, Train: 0.2764, Test: 0.2750\n",
            "Early stopping:  9.1193042883627\n",
            "Epoch: 021, Loss: 3.1744, Train: 0.2694, Test: 0.2648\n",
            "Early stopping:  4.07536769785965\n",
            "Epoch: 022, Loss: 2.8839, Train: 0.2690, Test: 0.2628\n",
            "Early stopping:  1.8368541709223103\n",
            "Epoch: 023, Loss: 2.6574, Train: 0.2735, Test: 0.2711\n",
            "Early stopping:  0.770999418885529\n",
            "Epoch: 024, Loss: 2.4639, Train: 0.2894, Test: 0.2793\n",
            "Early stopping:  0.4576726042943168\n",
            "Epoch: 025, Loss: 2.3156, Train: 0.2958, Test: 0.2883\n",
            "Early stopping:  0.34066142288043094\n",
            "Epoch: 026, Loss: 2.2328, Train: 0.2909, Test: 0.2883\n",
            "Early stopping:  0.2637486998670414\n",
            "Epoch: 027, Loss: 2.2004, Train: 0.2922, Test: 0.2875\n",
            "Early stopping:  0.18830631000994902\n",
            "Epoch: 028, Loss: 2.1842, Train: 0.2871, Test: 0.2864\n",
            "Early stopping:  0.114899221215179\n",
            "Epoch: 029, Loss: 2.1725, Train: 0.2894, Test: 0.2815\n",
            "Early stopping:  0.057510235356565846\n",
            "Epoch: 030, Loss: 2.1629, Train: 0.2919, Test: 0.2779\n",
            "Early stopping:  0.027478540624360316\n",
            "Epoch: 031, Loss: 2.1518, Train: 0.2991, Test: 0.2835\n",
            "Early stopping:  0.01885602734537118\n",
            "Epoch: 032, Loss: 2.1380, Train: 0.3053, Test: 0.2923\n",
            "Early stopping:  0.017906957660806695\n",
            "Epoch: 033, Loss: 2.1210, Train: 0.3268, Test: 0.3113\n",
            "Early stopping:  0.020320039670505118\n",
            "Epoch: 034, Loss: 2.1025, Train: 0.3395, Test: 0.3278\n",
            "Early stopping:  0.02407094887896803\n",
            "Epoch: 035, Loss: 2.0866, Train: 0.3550, Test: 0.3402\n",
            "Early stopping:  0.02627751458943971\n",
            "Epoch: 036, Loss: 2.0706, Train: 0.3616, Test: 0.3448\n",
            "Early stopping:  0.026791581222309718\n",
            "Epoch: 037, Loss: 2.0530, Train: 0.3671, Test: 0.3516\n",
            "Early stopping:  0.026561925871798054\n",
            "Epoch: 038, Loss: 2.0338, Train: 0.3760, Test: 0.3595\n",
            "Early stopping:  0.027048704802027635\n",
            "Epoch: 039, Loss: 2.0205, Train: 0.3862, Test: 0.3692\n",
            "Early stopping:  0.02673168521759834\n",
            "Epoch: 040, Loss: 2.0027, Train: 0.3964, Test: 0.3748\n",
            "Early stopping:  0.02663400833766179\n",
            "Epoch: 041, Loss: 1.9865, Train: 0.4051, Test: 0.3836\n",
            "Early stopping:  0.02599055321273411\n",
            "Epoch: 042, Loss: 1.9656, Train: 0.4136, Test: 0.3890\n",
            "Early stopping:  0.027032377780765334\n",
            "Epoch: 043, Loss: 1.9485, Train: 0.4229, Test: 0.3995\n",
            "Early stopping:  0.028674281206437724\n",
            "Epoch: 044, Loss: 1.9315, Train: 0.4308, Test: 0.4026\n",
            "Early stopping:  0.028537026204494057\n",
            "Epoch: 045, Loss: 1.9199, Train: 0.4318, Test: 0.4040\n",
            "Early stopping:  0.02656408918064273\n",
            "Epoch: 046, Loss: 1.9064, Train: 0.4365, Test: 0.4094\n",
            "Early stopping:  0.02330238945943935\n",
            "Epoch: 047, Loss: 1.8965, Train: 0.4380, Test: 0.4128\n",
            "Early stopping:  0.0204892832025184\n",
            "Epoch: 048, Loss: 1.8853, Train: 0.4374, Test: 0.4185\n",
            "Early stopping:  0.0183481262201498\n",
            "Epoch: 049, Loss: 1.8733, Train: 0.4386, Test: 0.4188\n",
            "Early stopping:  0.01807957412229111\n",
            "Epoch: 050, Loss: 1.8632, Train: 0.4425, Test: 0.4205\n",
            "Early stopping:  0.017348596159953034\n",
            "Epoch: 051, Loss: 1.8478, Train: 0.4473, Test: 0.4270\n",
            "Early stopping:  0.018925789719347323\n",
            "Epoch: 052, Loss: 1.8352, Train: 0.4526, Test: 0.4273\n",
            "Early stopping:  0.019898791842652284\n",
            "Epoch: 053, Loss: 1.8201, Train: 0.4531, Test: 0.4307\n",
            "Early stopping:  0.021292268469882754\n",
            "Epoch: 054, Loss: 1.8051, Train: 0.4573, Test: 0.4310\n",
            "Early stopping:  0.022756323134285838\n",
            "Epoch: 055, Loss: 1.7906, Train: 0.4607, Test: 0.4358\n",
            "Early stopping:  0.02288245999504171\n",
            "Epoch: 056, Loss: 1.7754, Train: 0.4609, Test: 0.4383\n",
            "Early stopping:  0.023590454908291824\n",
            "Epoch: 057, Loss: 1.7609, Train: 0.4645, Test: 0.4443\n",
            "Early stopping:  0.02341246453417689\n",
            "Epoch: 058, Loss: 1.7487, Train: 0.4639, Test: 0.4451\n",
            "Early stopping:  0.0225523956296985\n",
            "Epoch: 059, Loss: 1.7369, Train: 0.4607, Test: 0.4466\n",
            "Early stopping:  0.021247040706945827\n",
            "Epoch: 060, Loss: 1.7261, Train: 0.4597, Test: 0.4446\n",
            "Early stopping:  0.019437426954303665\n",
            "Epoch: 061, Loss: 1.7160, Train: 0.4643, Test: 0.4519\n",
            "Early stopping:  0.017784541941127538\n",
            "Epoch: 062, Loss: 1.7058, Train: 0.4673, Test: 0.4525\n",
            "Early stopping:  0.01683917003391526\n",
            "Epoch: 063, Loss: 1.6971, Train: 0.4699, Test: 0.4508\n",
            "Early stopping:  0.01578989680900094\n",
            "Epoch: 064, Loss: 1.6868, Train: 0.4720, Test: 0.4525\n",
            "Early stopping:  0.01541767667582978\n",
            "Epoch: 065, Loss: 1.6776, Train: 0.4760, Test: 0.4568\n",
            "Early stopping:  0.01516860410324379\n",
            "Epoch: 066, Loss: 1.6669, Train: 0.4786, Test: 0.4587\n",
            "Early stopping:  0.015413500642612793\n",
            "Epoch: 067, Loss: 1.6567, Train: 0.4781, Test: 0.4613\n",
            "Early stopping:  0.015906037620294588\n",
            "Epoch: 068, Loss: 1.6463, Train: 0.4803, Test: 0.4624\n",
            "Early stopping:  0.016113869245874397\n",
            "Epoch: 069, Loss: 1.6366, Train: 0.4841, Test: 0.4673\n",
            "Early stopping:  0.016221259038114874\n",
            "Epoch: 070, Loss: 1.6298, Train: 0.4824, Test: 0.4664\n",
            "Early stopping:  0.014933798138910239\n",
            "Epoch: 071, Loss: 1.6199, Train: 0.4853, Test: 0.4684\n",
            "Early stopping:  0.01428390148161992\n",
            "Epoch: 072, Loss: 1.6131, Train: 0.4879, Test: 0.4752\n",
            "Early stopping:  0.013147359699967337\n",
            "Epoch: 073, Loss: 1.6086, Train: 0.4877, Test: 0.4746\n",
            "Early stopping:  0.011564095835865911\n",
            "Epoch: 074, Loss: 1.5980, Train: 0.4894, Test: 0.4755\n",
            "Early stopping:  0.01196710584946541\n",
            "Epoch: 075, Loss: 1.5912, Train: 0.4919, Test: 0.4794\n",
            "Early stopping:  0.011564953836633935\n",
            "Epoch: 076, Loss: 1.5846, Train: 0.4883, Test: 0.4794\n",
            "Early stopping:  0.011842661351679834\n",
            "Epoch: 077, Loss: 1.5739, Train: 0.4922, Test: 0.4811\n",
            "Early stopping:  0.01313847092552714\n",
            "Epoch: 078, Loss: 1.5655, Train: 0.4966, Test: 0.4831\n",
            "Early stopping:  0.013059964214537118\n",
            "Epoch: 079, Loss: 1.5581, Train: 0.4957, Test: 0.4817\n",
            "Early stopping:  0.013544483896428445\n",
            "Epoch: 080, Loss: 1.5514, Train: 0.5006, Test: 0.4880\n",
            "Early stopping:  0.01308607185511283\n",
            "Epoch: 081, Loss: 1.5468, Train: 0.5017, Test: 0.4888\n",
            "Early stopping:  0.01087641995826016\n",
            "Epoch: 082, Loss: 1.5368, Train: 0.5043, Test: 0.4882\n",
            "Early stopping:  0.010899945326892464\n",
            "Epoch: 083, Loss: 1.5334, Train: 0.5096, Test: 0.4976\n",
            "Early stopping:  0.010190884123646009\n",
            "Epoch: 084, Loss: 1.5314, Train: 0.5110, Test: 0.4993\n",
            "Early stopping:  0.00869825181727094\n",
            "PREDICTIONS -> tensor([9, 0, 8,  ..., 5, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.44      0.65      0.53       379\n",
            "         capital_goods       0.21      0.05      0.08       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.46      0.35      0.40       396\n",
            " consumer_non-cyclical       0.79      0.35      0.48       223\n",
            "                energy       0.57      0.37      0.45       141\n",
            "             financial       0.62      0.58      0.60       384\n",
            "            healthcare       0.79      0.19      0.31       159\n",
            "              services       0.46      0.79      0.58      1038\n",
            "            technology       0.48      0.16      0.24       198\n",
            "        transportation       0.68      0.52      0.59       202\n",
            "             utilities       0.70      0.23      0.35       113\n",
            "\n",
            "              accuracy                           0.50      3527\n",
            "             macro avg       0.52      0.35      0.38      3527\n",
            "          weighted avg       0.51      0.50      0.46      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 103.9409, Train: 0.1130, Test: 0.1120\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 250.4422, Train: 0.0450, Test: 0.0471\n",
            "Early stopping:  103.59204437486298\n",
            "Epoch: 003, Loss: 331.3148, Train: 0.3087, Test: 0.3068\n",
            "Early stopping:  115.25474575142303\n",
            "Epoch: 004, Loss: 320.7521, Train: 0.1229, Test: 0.1239\n",
            "Early stopping:  104.78712829078678\n",
            "Epoch: 005, Loss: 399.8470, Train: 0.0707, Test: 0.0700\n",
            "Early stopping:  112.38304743350012\n",
            "Epoch: 006, Loss: 416.7873, Train: 0.0461, Test: 0.0434\n",
            "Early stopping:  66.84156097050135\n",
            "Epoch: 007, Loss: 399.0981, Train: 0.3013, Test: 0.2946\n",
            "Early stopping:  44.116671871763444\n",
            "Epoch: 008, Loss: 395.5215, Train: 0.1684, Test: 0.1667\n",
            "Early stopping:  37.61082805136162\n",
            "Epoch: 009, Loss: 392.9959, Train: 0.1123, Test: 0.1106\n",
            "Early stopping:  9.329417485437716\n",
            "Epoch: 010, Loss: 370.2709, Train: 0.3045, Test: 0.2980\n",
            "Early stopping:  16.63788503553064\n",
            "Epoch: 011, Loss: 324.4173, Train: 0.2355, Test: 0.2282\n",
            "Early stopping:  31.208989620168012\n",
            "Epoch: 012, Loss: 272.5867, Train: 0.0573, Test: 0.0581\n",
            "Early stopping:  52.37447383376068\n",
            "Epoch: 013, Loss: 284.1076, Train: 0.1735, Test: 0.1732\n",
            "Early stopping:  52.482066687298065\n",
            "Epoch: 014, Loss: 189.5125, Train: 0.3119, Test: 0.3130\n",
            "Early stopping:  67.16969021649459\n",
            "Epoch: 015, Loss: 141.5138, Train: 0.2597, Test: 0.2532\n",
            "Early stopping:  74.75412609263476\n",
            "Epoch: 016, Loss: 114.6107, Train: 0.3312, Test: 0.3244\n",
            "Early stopping:  76.09828101800963\n",
            "Epoch: 017, Loss: 83.5157, Train: 0.3172, Test: 0.3059\n",
            "Early stopping:  78.24687145172096\n",
            "Epoch: 018, Loss: 63.4028, Train: 0.3198, Test: 0.3042\n",
            "Early stopping:  49.61636079339172\n",
            "Epoch: 019, Loss: 50.1032, Train: 0.2089, Test: 0.2058\n",
            "Early stopping:  37.41186995866664\n",
            "Epoch: 020, Loss: 46.7935, Train: 0.1756, Test: 0.1738\n",
            "Early stopping:  28.00241211044186\n",
            "Epoch: 021, Loss: 39.9951, Train: 0.1735, Test: 0.1744\n",
            "Early stopping:  17.209755465768144\n",
            "Epoch: 022, Loss: 26.5526, Train: 0.1991, Test: 0.2039\n",
            "Early stopping:  13.533234613148299\n",
            "Epoch: 023, Loss: 13.8037, Train: 0.2543, Test: 0.2526\n",
            "Early stopping:  15.097776069600227\n",
            "Epoch: 024, Loss: 5.0836, Train: 0.3140, Test: 0.3056\n",
            "Early stopping:  17.41973019324967\n",
            "Epoch: 025, Loss: 3.9317, Train: 0.3089, Test: 0.3042\n",
            "Early stopping:  15.323267901731723\n",
            "Epoch: 026, Loss: 3.5670, Train: 0.2955, Test: 0.2923\n",
            "Early stopping:  9.862925080629589\n",
            "Epoch: 027, Loss: 3.2129, Train: 0.2854, Test: 0.2810\n",
            "Early stopping:  4.462916131426382\n",
            "Epoch: 028, Loss: 2.9063, Train: 0.2707, Test: 0.2696\n",
            "Early stopping:  0.8433362971216647\n",
            "Epoch: 029, Loss: 2.6655, Train: 0.2599, Test: 0.2608\n",
            "Early stopping:  0.5065184493002372\n",
            "Epoch: 030, Loss: 2.4912, Train: 0.2399, Test: 0.2518\n",
            "Early stopping:  0.4305469321996861\n",
            "Epoch: 031, Loss: 2.3822, Train: 0.2263, Test: 0.2387\n",
            "Early stopping:  0.33408707283979144\n",
            "Epoch: 032, Loss: 2.3246, Train: 0.2197, Test: 0.2243\n",
            "Early stopping:  0.2359113002581023\n",
            "Epoch: 033, Loss: 2.2968, Train: 0.2176, Test: 0.2206\n",
            "Early stopping:  0.15026371947479983\n",
            "Epoch: 034, Loss: 2.2881, Train: 0.2138, Test: 0.2166\n",
            "Early stopping:  0.08376819804641801\n",
            "Epoch: 035, Loss: 2.2873, Train: 0.2026, Test: 0.2092\n",
            "Early stopping:  0.04007410023291931\n",
            "Epoch: 036, Loss: 2.2881, Train: 0.1974, Test: 0.2019\n",
            "Early stopping:  0.015926895262381505\n",
            "Epoch: 037, Loss: 2.2884, Train: 0.1932, Test: 0.1979\n",
            "Early stopping:  0.003973714902783309\n",
            "PREDICTIONS -> tensor([ 1, 11,  7,  ...,  7,  4,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.00      0.00      0.00       379\n",
            "         capital_goods       0.05      0.05      0.05       254\n",
            "conglomerates_industry       0.00      0.00      0.00        40\n",
            "     consumer_cyclical       0.00      0.00      0.00       396\n",
            " consumer_non-cyclical       0.15      0.25      0.18       223\n",
            "                energy       0.00      0.00      0.00       141\n",
            "             financial       0.75      0.02      0.03       384\n",
            "            healthcare       0.13      0.57      0.22       159\n",
            "              services       0.62      0.34      0.44      1038\n",
            "            technology       0.00      0.00      0.00       198\n",
            "        transportation       0.10      0.75      0.18       202\n",
            "             utilities       0.24      0.22      0.23       113\n",
            "\n",
            "              accuracy                           0.20      3527\n",
            "             macro avg       0.17      0.18      0.11      3527\n",
            "          weighted avg       0.30      0.20      0.18      3527\n",
            "\n",
            "time: 1min 48s (started: 2024-10-16 21:45:52 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9yik9PrgWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da71e3f-174e-4d7f-c0f5-8c2f065de8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 444 ms (started: 2024-10-16 21:47:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0785SeKgWjM"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OSBKRV8gWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef02b5f-72c0-4cb9-c8dc-ca178c5ee1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4796, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2712, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.14737067050319208\n",
            "Epoch: 003, Loss: 2.1561, Train: 0.2945, Test: 0.2943\n",
            "Early stopping:  0.16398403374456275\n",
            "Epoch: 004, Loss: 2.1208, Train: 0.3106, Test: 0.3073\n",
            "Early stopping:  0.1617375491900917\n",
            "Epoch: 005, Loss: 2.0653, Train: 0.3629, Test: 0.3643\n",
            "Early stopping:  0.16419509364065837\n",
            "Epoch: 006, Loss: 2.0068, Train: 0.3949, Test: 0.3907\n",
            "Early stopping:  0.09985075942851447\n",
            "Epoch: 007, Loss: 1.9412, Train: 0.3883, Test: 0.3859\n",
            "Early stopping:  0.08644665502710538\n",
            "Epoch: 008, Loss: 1.8808, Train: 0.3881, Test: 0.3825\n",
            "Early stopping:  0.09556796339180844\n",
            "Epoch: 009, Loss: 1.8284, Train: 0.4006, Test: 0.3924\n",
            "Early stopping:  0.0948983547380959\n",
            "Epoch: 010, Loss: 1.7752, Train: 0.4295, Test: 0.4225\n",
            "Early stopping:  0.0911842719401617\n",
            "Epoch: 011, Loss: 1.7155, Train: 0.4652, Test: 0.4573\n",
            "Early stopping:  0.08808319514733319\n",
            "Epoch: 012, Loss: 1.6554, Train: 0.4866, Test: 0.4707\n",
            "Early stopping:  0.08919382440449594\n",
            "Epoch: 013, Loss: 1.5999, Train: 0.5026, Test: 0.4820\n",
            "Early stopping:  0.09122616815574042\n",
            "Epoch: 014, Loss: 1.5451, Train: 0.5129, Test: 0.4999\n",
            "Early stopping:  0.09105368447390708\n",
            "Epoch: 015, Loss: 1.4920, Train: 0.5306, Test: 0.5135\n",
            "Early stopping:  0.088157388272814\n",
            "Epoch: 016, Loss: 1.4414, Train: 0.5514, Test: 0.5367\n",
            "Early stopping:  0.08474876322876758\n",
            "Epoch: 017, Loss: 1.3899, Train: 0.5779, Test: 0.5529\n",
            "Early stopping:  0.08283244944340133\n",
            "Epoch: 018, Loss: 1.3437, Train: 0.5924, Test: 0.5614\n",
            "Early stopping:  0.07984415460679707\n",
            "Epoch: 019, Loss: 1.3049, Train: 0.5998, Test: 0.5758\n",
            "Early stopping:  0.07469601253638285\n",
            "Epoch: 020, Loss: 1.2668, Train: 0.6055, Test: 0.5832\n",
            "Early stopping:  0.0687921537612854\n",
            "Epoch: 021, Loss: 1.2309, Train: 0.6185, Test: 0.5909\n",
            "Early stopping:  0.062490893406904585\n",
            "Epoch: 022, Loss: 1.1973, Train: 0.6284, Test: 0.6022\n",
            "Early stopping:  0.05804426654925003\n",
            "Epoch: 023, Loss: 1.1669, Train: 0.6374, Test: 0.6065\n",
            "Early stopping:  0.05468304557612113\n",
            "Epoch: 024, Loss: 1.1401, Train: 0.6446, Test: 0.6096\n",
            "Early stopping:  0.0502690360571175\n",
            "Epoch: 025, Loss: 1.1132, Train: 0.6474, Test: 0.6141\n",
            "Early stopping:  0.04632657414414994\n",
            "Epoch: 026, Loss: 1.0885, Train: 0.6575, Test: 0.6226\n",
            "Early stopping:  0.042920562358605356\n",
            "Epoch: 027, Loss: 1.0645, Train: 0.6665, Test: 0.6286\n",
            "Early stopping:  0.04058101567956343\n",
            "Epoch: 028, Loss: 1.0425, Train: 0.6739, Test: 0.6311\n",
            "Early stopping:  0.03860963792835409\n",
            "Epoch: 029, Loss: 1.0199, Train: 0.6773, Test: 0.6377\n",
            "Early stopping:  0.036801911859526346\n",
            "Epoch: 030, Loss: 0.9982, Train: 0.6841, Test: 0.6396\n",
            "Early stopping:  0.035586517058821016\n",
            "Epoch: 031, Loss: 0.9778, Train: 0.6962, Test: 0.6464\n",
            "Early stopping:  0.034404312503155385\n",
            "Epoch: 032, Loss: 0.9578, Train: 0.7055, Test: 0.6504\n",
            "Early stopping:  0.03345339803222768\n",
            "Epoch: 033, Loss: 0.9373, Train: 0.7117, Test: 0.6558\n",
            "Early stopping:  0.032521927984008776\n",
            "Epoch: 034, Loss: 0.9188, Train: 0.7176, Test: 0.6589\n",
            "Early stopping:  0.0315440956870099\n",
            "Epoch: 035, Loss: 0.9006, Train: 0.7251, Test: 0.6629\n",
            "Early stopping:  0.03058119806782\n",
            "Epoch: 036, Loss: 0.8826, Train: 0.7318, Test: 0.6646\n",
            "Early stopping:  0.02957540594561675\n",
            "Epoch: 037, Loss: 0.8651, Train: 0.7399, Test: 0.6720\n",
            "Early stopping:  0.028536434905704604\n",
            "Epoch: 038, Loss: 0.8494, Train: 0.7465, Test: 0.6759\n",
            "Early stopping:  0.02755628432492347\n",
            "Epoch: 039, Loss: 0.8332, Train: 0.7556, Test: 0.6819\n",
            "Early stopping:  0.026582697706555733\n",
            "Epoch: 040, Loss: 0.8171, Train: 0.7641, Test: 0.6870\n",
            "Early stopping:  0.02576066669226919\n",
            "Epoch: 041, Loss: 0.8021, Train: 0.7709, Test: 0.6929\n",
            "Early stopping:  0.02503399489761483\n",
            "Epoch: 042, Loss: 0.7865, Train: 0.7756, Test: 0.6955\n",
            "Early stopping:  0.024823081277884115\n",
            "Epoch: 043, Loss: 0.7709, Train: 0.7800, Test: 0.7006\n",
            "Early stopping:  0.024528609434135207\n",
            "Epoch: 044, Loss: 0.7560, Train: 0.7849, Test: 0.7012\n",
            "Early stopping:  0.02425712458635837\n",
            "Epoch: 045, Loss: 0.7408, Train: 0.7932, Test: 0.7060\n",
            "Early stopping:  0.02419942217200269\n",
            "Epoch: 046, Loss: 0.7260, Train: 0.7955, Test: 0.7091\n",
            "Early stopping:  0.0238923349657564\n",
            "Epoch: 047, Loss: 0.7114, Train: 0.8019, Test: 0.7142\n",
            "Early stopping:  0.023580718147177056\n",
            "Epoch: 048, Loss: 0.6968, Train: 0.8055, Test: 0.7162\n",
            "Early stopping:  0.023368556820259036\n",
            "Epoch: 049, Loss: 0.6825, Train: 0.8091, Test: 0.7173\n",
            "Early stopping:  0.02306180559000137\n",
            "Epoch: 050, Loss: 0.6684, Train: 0.8130, Test: 0.7210\n",
            "Early stopping:  0.02277533228144524\n",
            "Epoch: 051, Loss: 0.6546, Train: 0.8197, Test: 0.7267\n",
            "Early stopping:  0.022461679803730365\n",
            "Epoch: 052, Loss: 0.6409, Train: 0.8216, Test: 0.7281\n",
            "Early stopping:  0.022086568882086037\n",
            "Epoch: 053, Loss: 0.6278, Train: 0.8289, Test: 0.7329\n",
            "Early stopping:  0.021645023471288754\n",
            "Epoch: 054, Loss: 0.6150, Train: 0.8282, Test: 0.7292\n",
            "Early stopping:  0.02114053629081364\n",
            "Epoch: 055, Loss: 0.6033, Train: 0.8376, Test: 0.7363\n",
            "Early stopping:  0.020331865954291244\n",
            "Epoch: 056, Loss: 0.5935, Train: 0.8302, Test: 0.7284\n",
            "Early stopping:  0.01890491537902054\n",
            "Epoch: 057, Loss: 0.5839, Train: 0.8474, Test: 0.7389\n",
            "Early stopping:  0.01730845334243687\n",
            "Epoch: 058, Loss: 0.5703, Train: 0.8473, Test: 0.7386\n",
            "Early stopping:  0.017218877610381555\n",
            "Epoch: 059, Loss: 0.5553, Train: 0.8459, Test: 0.7377\n",
            "Early stopping:  0.018955880069647504\n",
            "Epoch: 060, Loss: 0.5461, Train: 0.8599, Test: 0.7434\n",
            "Early stopping:  0.01957872592107324\n",
            "Epoch: 061, Loss: 0.5385, Train: 0.8527, Test: 0.7392\n",
            "Early stopping:  0.01836788581715649\n",
            "Epoch: 062, Loss: 0.5266, Train: 0.8626, Test: 0.7454\n",
            "Early stopping:  0.016592357375628954\n",
            "Epoch: 063, Loss: 0.5142, Train: 0.8703, Test: 0.7440\n",
            "Early stopping:  0.016135191000213666\n",
            "Epoch: 064, Loss: 0.5050, Train: 0.8660, Test: 0.7426\n",
            "Early stopping:  0.016864290297276062\n",
            "Epoch: 065, Loss: 0.4972, Train: 0.8773, Test: 0.7434\n",
            "Early stopping:  0.01653289482956718\n",
            "Epoch: 066, Loss: 0.4881, Train: 0.8783, Test: 0.7460\n",
            "Early stopping:  0.014910362582452971\n",
            "Epoch: 067, Loss: 0.4773, Train: 0.8815, Test: 0.7471\n",
            "Early stopping:  0.014371909097372421\n",
            "Epoch: 068, Loss: 0.4669, Train: 0.8896, Test: 0.7465\n",
            "Early stopping:  0.015237461786242006\n",
            "Epoch: 069, Loss: 0.4584, Train: 0.8856, Test: 0.7502\n",
            "Early stopping:  0.015624151542231929\n",
            "Epoch: 070, Loss: 0.4519, Train: 0.8955, Test: 0.7468\n",
            "Early stopping:  0.014516860648633742\n",
            "Epoch: 071, Loss: 0.4450, Train: 0.8930, Test: 0.7502\n",
            "Early stopping:  0.01264790374539586\n",
            "Epoch: 072, Loss: 0.4353, Train: 0.9028, Test: 0.7502\n",
            "Early stopping:  0.01213886471648571\n",
            "Epoch: 073, Loss: 0.4249, Train: 0.9002, Test: 0.7545\n",
            "Early stopping:  0.013298551232999826\n",
            "Epoch: 074, Loss: 0.4157, Train: 0.9055, Test: 0.7502\n",
            "Early stopping:  0.01463777022032707\n",
            "Epoch: 075, Loss: 0.4079, Train: 0.9091, Test: 0.7530\n",
            "Early stopping:  0.014850572233660979\n",
            "Epoch: 076, Loss: 0.4011, Train: 0.9060, Test: 0.7539\n",
            "Early stopping:  0.013539368454441524\n",
            "Epoch: 077, Loss: 0.3949, Train: 0.9159, Test: 0.7502\n",
            "Early stopping:  0.01184011108583006\n",
            "Epoch: 078, Loss: 0.3909, Train: 0.9038, Test: 0.7525\n",
            "Early stopping:  0.009966496725972836\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.76      0.78       379\n",
            "         capital_goods       0.73      0.62      0.67       254\n",
            "conglomerates_industry       0.96      0.62      0.76        40\n",
            "     consumer_cyclical       0.72      0.65      0.69       396\n",
            " consumer_non-cyclical       0.78      0.67      0.72       223\n",
            "                energy       0.89      0.77      0.82       141\n",
            "             financial       0.81      0.77      0.79       384\n",
            "            healthcare       0.82      0.74      0.78       159\n",
            "              services       0.68      0.85      0.76      1038\n",
            "            technology       0.78      0.59      0.67       198\n",
            "        transportation       0.80      0.82      0.81       202\n",
            "             utilities       0.87      0.77      0.82       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.81      0.72      0.76      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4643, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2429, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1565618866390538\n",
            "Epoch: 003, Loss: 2.1766, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15062761349601292\n",
            "Epoch: 004, Loss: 2.1275, Train: 0.3083, Test: 0.3076\n",
            "Early stopping:  0.14868709208781172\n",
            "Epoch: 005, Loss: 2.0512, Train: 0.3713, Test: 0.3655\n",
            "Early stopping:  0.15719637257211555\n",
            "Epoch: 006, Loss: 1.9920, Train: 0.4121, Test: 0.4006\n",
            "Early stopping:  0.09933419785741644\n",
            "Epoch: 007, Loss: 1.9287, Train: 0.4159, Test: 0.4003\n",
            "Early stopping:  0.09999250813637287\n",
            "Epoch: 008, Loss: 1.8577, Train: 0.4149, Test: 0.4018\n",
            "Early stopping:  0.104771728873557\n",
            "Epoch: 009, Loss: 1.7886, Train: 0.4236, Test: 0.4151\n",
            "Early stopping:  0.10434017302491316\n",
            "Epoch: 010, Loss: 1.7240, Train: 0.4518, Test: 0.4406\n",
            "Early stopping:  0.10688572495726448\n",
            "Epoch: 011, Loss: 1.6599, Train: 0.4749, Test: 0.4596\n",
            "Early stopping:  0.10616701152793534\n",
            "Epoch: 012, Loss: 1.6012, Train: 0.4958, Test: 0.4743\n",
            "Early stopping:  0.10149928940170388\n",
            "Epoch: 013, Loss: 1.5459, Train: 0.5191, Test: 0.4973\n",
            "Early stopping:  0.09622796193488353\n",
            "Epoch: 014, Loss: 1.4880, Train: 0.5512, Test: 0.5293\n",
            "Early stopping:  0.09270567955666908\n",
            "Epoch: 015, Loss: 1.4388, Train: 0.5718, Test: 0.5517\n",
            "Early stopping:  0.08786195997360945\n",
            "Epoch: 016, Loss: 1.3936, Train: 0.5873, Test: 0.5628\n",
            "Early stopping:  0.08271100767317303\n",
            "Epoch: 017, Loss: 1.3444, Train: 0.5953, Test: 0.5682\n",
            "Early stopping:  0.07872003652493677\n",
            "Epoch: 018, Loss: 1.2996, Train: 0.6000, Test: 0.5716\n",
            "Early stopping:  0.07448230390537594\n",
            "Epoch: 019, Loss: 1.2612, Train: 0.6062, Test: 0.5790\n",
            "Early stopping:  0.07107212521708621\n",
            "Epoch: 020, Loss: 1.2263, Train: 0.6142, Test: 0.5886\n",
            "Early stopping:  0.06622580843053154\n",
            "Epoch: 021, Loss: 1.1951, Train: 0.6261, Test: 0.5965\n",
            "Early stopping:  0.0589503747712708\n",
            "Epoch: 022, Loss: 1.1652, Train: 0.6361, Test: 0.6073\n",
            "Early stopping:  0.05301527515509163\n",
            "Epoch: 023, Loss: 1.1407, Train: 0.6425, Test: 0.6090\n",
            "Early stopping:  0.04784847988228518\n",
            "Epoch: 024, Loss: 1.1166, Train: 0.6478, Test: 0.6158\n",
            "Early stopping:  0.04338462574608042\n",
            "Epoch: 025, Loss: 1.0905, Train: 0.6531, Test: 0.6255\n",
            "Early stopping:  0.04082012021185173\n",
            "Epoch: 026, Loss: 1.0657, Train: 0.6611, Test: 0.6306\n",
            "Early stopping:  0.03939449341792806\n",
            "Epoch: 027, Loss: 1.0420, Train: 0.6690, Test: 0.6388\n",
            "Early stopping:  0.03924945913358641\n",
            "Epoch: 028, Loss: 1.0188, Train: 0.6824, Test: 0.6487\n",
            "Early stopping:  0.03861321266236789\n",
            "Epoch: 029, Loss: 0.9961, Train: 0.6911, Test: 0.6487\n",
            "Early stopping:  0.03728873141067009\n",
            "Epoch: 030, Loss: 0.9754, Train: 0.6994, Test: 0.6541\n",
            "Early stopping:  0.03584923637236639\n",
            "Epoch: 031, Loss: 0.9536, Train: 0.7064, Test: 0.6618\n",
            "Early stopping:  0.03482335353838901\n",
            "Epoch: 032, Loss: 0.9338, Train: 0.7138, Test: 0.6669\n",
            "Early stopping:  0.033604687105493455\n",
            "Epoch: 033, Loss: 0.9146, Train: 0.7202, Test: 0.6720\n",
            "Early stopping:  0.03235963599423161\n",
            "Epoch: 034, Loss: 0.8963, Train: 0.7293, Test: 0.6745\n",
            "Early stopping:  0.03119536462182764\n",
            "Epoch: 035, Loss: 0.8785, Train: 0.7389, Test: 0.6776\n",
            "Early stopping:  0.029673342339708003\n",
            "Epoch: 036, Loss: 0.8622, Train: 0.7456, Test: 0.6807\n",
            "Early stopping:  0.028348624812811026\n",
            "Epoch: 037, Loss: 0.8453, Train: 0.7535, Test: 0.6824\n",
            "Early stopping:  0.02731535248969299\n",
            "Epoch: 038, Loss: 0.8292, Train: 0.7620, Test: 0.6822\n",
            "Early stopping:  0.026468756462761176\n",
            "Epoch: 039, Loss: 0.8137, Train: 0.7681, Test: 0.6873\n",
            "Early stopping:  0.025715790653507598\n",
            "Epoch: 040, Loss: 0.7978, Train: 0.7732, Test: 0.6921\n",
            "Early stopping:  0.025347547049100787\n",
            "Epoch: 041, Loss: 0.7831, Train: 0.7767, Test: 0.6921\n",
            "Early stopping:  0.024630853870806574\n",
            "Epoch: 042, Loss: 0.7680, Train: 0.7822, Test: 0.6966\n",
            "Early stopping:  0.024199536630788176\n",
            "Epoch: 043, Loss: 0.7531, Train: 0.7856, Test: 0.7026\n",
            "Early stopping:  0.02388907336294063\n",
            "Epoch: 044, Loss: 0.7386, Train: 0.7930, Test: 0.7040\n",
            "Early stopping:  0.023485824382926294\n",
            "Epoch: 045, Loss: 0.7240, Train: 0.7949, Test: 0.7029\n",
            "Early stopping:  0.02332763831815544\n",
            "Epoch: 046, Loss: 0.7100, Train: 0.8021, Test: 0.7071\n",
            "Early stopping:  0.02293439688394202\n",
            "Epoch: 047, Loss: 0.6958, Train: 0.8068, Test: 0.7108\n",
            "Early stopping:  0.022615037176921562\n",
            "Epoch: 048, Loss: 0.6820, Train: 0.8106, Test: 0.7142\n",
            "Early stopping:  0.022334956935792998\n",
            "Epoch: 049, Loss: 0.6683, Train: 0.8151, Test: 0.7193\n",
            "Early stopping:  0.022037149987776915\n",
            "Epoch: 050, Loss: 0.6549, Train: 0.8163, Test: 0.7190\n",
            "Early stopping:  0.021776696423631822\n",
            "Epoch: 051, Loss: 0.6421, Train: 0.8244, Test: 0.7238\n",
            "Early stopping:  0.021287295945123474\n",
            "Epoch: 052, Loss: 0.6303, Train: 0.8219, Test: 0.7255\n",
            "Early stopping:  0.02050698759157652\n",
            "Epoch: 053, Loss: 0.6202, Train: 0.8338, Test: 0.7292\n",
            "Early stopping:  0.019128674020376577\n",
            "Epoch: 054, Loss: 0.6096, Train: 0.8302, Test: 0.7298\n",
            "Early stopping:  0.01780179636854634\n",
            "Epoch: 055, Loss: 0.5954, Train: 0.8412, Test: 0.7326\n",
            "Early stopping:  0.018075688051240605\n",
            "Epoch: 056, Loss: 0.5801, Train: 0.8457, Test: 0.7358\n",
            "Early stopping:  0.019906650298935045\n",
            "Epoch: 057, Loss: 0.5693, Train: 0.8423, Test: 0.7326\n",
            "Early stopping:  0.020813705435993837\n",
            "Epoch: 058, Loss: 0.5615, Train: 0.8544, Test: 0.7403\n",
            "Early stopping:  0.01949603817025392\n",
            "Epoch: 059, Loss: 0.5509, Train: 0.8539, Test: 0.7369\n",
            "Early stopping:  0.017135240280394703\n",
            "Epoch: 060, Loss: 0.5371, Train: 0.8588, Test: 0.7403\n",
            "Early stopping:  0.016557702600239806\n",
            "Epoch: 061, Loss: 0.5250, Train: 0.8654, Test: 0.7451\n",
            "Early stopping:  0.017943586941938944\n",
            "Epoch: 062, Loss: 0.5163, Train: 0.8609, Test: 0.7426\n",
            "Early stopping:  0.01842544510805678\n",
            "Epoch: 063, Loss: 0.5083, Train: 0.8722, Test: 0.7454\n",
            "Early stopping:  0.01687219271829195\n",
            "Epoch: 064, Loss: 0.4981, Train: 0.8720, Test: 0.7465\n",
            "Early stopping:  0.015027244472293423\n",
            "Epoch: 065, Loss: 0.4860, Train: 0.8800, Test: 0.7516\n",
            "Early stopping:  0.015260157418317035\n",
            "Epoch: 066, Loss: 0.4749, Train: 0.8824, Test: 0.7522\n",
            "Early stopping:  0.016678542784552473\n",
            "Epoch: 067, Loss: 0.4661, Train: 0.8832, Test: 0.7479\n",
            "Early stopping:  0.017049164010566963\n",
            "Epoch: 068, Loss: 0.4587, Train: 0.8938, Test: 0.7530\n",
            "Early stopping:  0.01568304076182781\n",
            "Epoch: 069, Loss: 0.4514, Train: 0.8868, Test: 0.7511\n",
            "Early stopping:  0.01354983384438408\n",
            "Epoch: 070, Loss: 0.4438, Train: 0.8974, Test: 0.7530\n",
            "Early stopping:  0.012148685040566865\n",
            "Epoch: 071, Loss: 0.4348, Train: 0.8951, Test: 0.7519\n",
            "Early stopping:  0.012249281510909892\n",
            "Epoch: 072, Loss: 0.4258, Train: 0.9049, Test: 0.7584\n",
            "Early stopping:  0.013054007634170283\n",
            "Epoch: 073, Loss: 0.4145, Train: 0.9091, Test: 0.7593\n",
            "Early stopping:  0.014577212043442584\n",
            "Epoch: 074, Loss: 0.4051, Train: 0.9076, Test: 0.7570\n",
            "Early stopping:  0.015462163000670907\n",
            "Epoch: 075, Loss: 0.3991, Train: 0.9100, Test: 0.7573\n",
            "Early stopping:  0.01464607592127005\n",
            "Epoch: 076, Loss: 0.3955, Train: 0.9015, Test: 0.7533\n",
            "Early stopping:  0.012272619711058256\n",
            "Epoch: 077, Loss: 0.3930, Train: 0.9142, Test: 0.7539\n",
            "Early stopping:  0.008586252131288481\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.71      0.79      0.74       379\n",
            "         capital_goods       0.74      0.65      0.69       254\n",
            "conglomerates_industry       1.00      0.60      0.75        40\n",
            "     consumer_cyclical       0.66      0.77      0.71       396\n",
            " consumer_non-cyclical       0.74      0.71      0.73       223\n",
            "                energy       0.81      0.81      0.81       141\n",
            "             financial       0.78      0.78      0.78       384\n",
            "            healthcare       0.83      0.75      0.79       159\n",
            "              services       0.76      0.78      0.77      1038\n",
            "            technology       0.78      0.57      0.66       198\n",
            "        transportation       0.84      0.82      0.83       202\n",
            "             utilities       0.83      0.78      0.80       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.79      0.73      0.75      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4447, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2419, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.14338610744497216\n",
            "Epoch: 003, Loss: 2.1878, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.13540126066031424\n",
            "Epoch: 004, Loss: 2.1277, Train: 0.2983, Test: 0.2977\n",
            "Early stopping:  0.13758419472978792\n",
            "Epoch: 005, Loss: 2.0554, Train: 0.3346, Test: 0.3292\n",
            "Early stopping:  0.14767664303667596\n",
            "Epoch: 006, Loss: 2.0014, Train: 0.3762, Test: 0.3669\n",
            "Early stopping:  0.09710306542922419\n",
            "Epoch: 007, Loss: 1.9502, Train: 0.3883, Test: 0.3796\n",
            "Early stopping:  0.09531462878250786\n",
            "Epoch: 008, Loss: 1.8895, Train: 0.3962, Test: 0.3853\n",
            "Early stopping:  0.09211158009950514\n",
            "Epoch: 009, Loss: 1.8239, Train: 0.4093, Test: 0.3998\n",
            "Early stopping:  0.09102621078014739\n",
            "Epoch: 010, Loss: 1.7601, Train: 0.4319, Test: 0.4188\n",
            "Early stopping:  0.09637567699699062\n",
            "Epoch: 011, Loss: 1.6972, Train: 0.4546, Test: 0.4440\n",
            "Early stopping:  0.10048097001229653\n",
            "Epoch: 012, Loss: 1.6338, Train: 0.4760, Test: 0.4656\n",
            "Early stopping:  0.10092259103139305\n",
            "Epoch: 013, Loss: 1.5740, Train: 0.4972, Test: 0.4800\n",
            "Early stopping:  0.09900621463618287\n",
            "Epoch: 014, Loss: 1.5176, Train: 0.5312, Test: 0.5050\n",
            "Early stopping:  0.0961826197308719\n",
            "Epoch: 015, Loss: 1.4620, Train: 0.5586, Test: 0.5248\n",
            "Early stopping:  0.09277796252326143\n",
            "Epoch: 016, Loss: 1.4084, Train: 0.5749, Test: 0.5461\n",
            "Early stopping:  0.0889900607439005\n",
            "Epoch: 017, Loss: 1.3597, Train: 0.5830, Test: 0.5588\n",
            "Early stopping:  0.08505822102698228\n",
            "Epoch: 018, Loss: 1.3155, Train: 0.5930, Test: 0.5713\n",
            "Early stopping:  0.0801840249056034\n",
            "Epoch: 019, Loss: 1.2740, Train: 0.6043, Test: 0.5807\n",
            "Early stopping:  0.07422488785114499\n",
            "Epoch: 020, Loss: 1.2354, Train: 0.6132, Test: 0.5889\n",
            "Early stopping:  0.06832113516263671\n",
            "Epoch: 021, Loss: 1.2001, Train: 0.6244, Test: 0.5965\n",
            "Early stopping:  0.06318462557751615\n",
            "Epoch: 022, Loss: 1.1697, Train: 0.6350, Test: 0.6016\n",
            "Early stopping:  0.05787021054986536\n",
            "Epoch: 023, Loss: 1.1420, Train: 0.6423, Test: 0.6096\n",
            "Early stopping:  0.052260647156947046\n",
            "Epoch: 024, Loss: 1.1135, Train: 0.6484, Test: 0.6195\n",
            "Early stopping:  0.04780858166087619\n",
            "Epoch: 025, Loss: 1.0875, Train: 0.6588, Test: 0.6246\n",
            "Early stopping:  0.04453101884809211\n",
            "Epoch: 026, Loss: 1.0635, Train: 0.6675, Test: 0.6337\n",
            "Early stopping:  0.04223361892036685\n",
            "Epoch: 027, Loss: 1.0387, Train: 0.6767, Test: 0.6362\n",
            "Early stopping:  0.0405965607594987\n",
            "Epoch: 028, Loss: 1.0152, Train: 0.6841, Test: 0.6445\n",
            "Early stopping:  0.03879187626436146\n",
            "Epoch: 029, Loss: 0.9931, Train: 0.6907, Test: 0.6501\n",
            "Early stopping:  0.03749298541248161\n",
            "Epoch: 030, Loss: 0.9716, Train: 0.6994, Test: 0.6538\n",
            "Early stopping:  0.03629056319070289\n",
            "Epoch: 031, Loss: 0.9506, Train: 0.7079, Test: 0.6569\n",
            "Early stopping:  0.03474893810429116\n",
            "Epoch: 032, Loss: 0.9307, Train: 0.7183, Test: 0.6637\n",
            "Early stopping:  0.03344704177717373\n",
            "Epoch: 033, Loss: 0.9120, Train: 0.7270, Test: 0.6669\n",
            "Early stopping:  0.032113425070010075\n",
            "Epoch: 034, Loss: 0.8934, Train: 0.7350, Test: 0.6688\n",
            "Early stopping:  0.030823811387672945\n",
            "Epoch: 035, Loss: 0.8758, Train: 0.7388, Test: 0.6717\n",
            "Early stopping:  0.029546253400641948\n",
            "Epoch: 036, Loss: 0.8591, Train: 0.7452, Test: 0.6739\n",
            "Early stopping:  0.028377410133683983\n",
            "Epoch: 037, Loss: 0.8428, Train: 0.7539, Test: 0.6799\n",
            "Early stopping:  0.027335438908210306\n",
            "Epoch: 038, Loss: 0.8268, Train: 0.7614, Test: 0.6859\n",
            "Early stopping:  0.026307804111919166\n",
            "Epoch: 039, Loss: 0.8114, Train: 0.7664, Test: 0.6887\n",
            "Early stopping:  0.02548067676863103\n",
            "Epoch: 040, Loss: 0.7961, Train: 0.7718, Test: 0.6932\n",
            "Early stopping:  0.024872227736800957\n",
            "Epoch: 041, Loss: 0.7813, Train: 0.7766, Test: 0.6972\n",
            "Early stopping:  0.02430010192762595\n",
            "Epoch: 042, Loss: 0.7666, Train: 0.7826, Test: 0.7017\n",
            "Early stopping:  0.023809770407741165\n",
            "Epoch: 043, Loss: 0.7523, Train: 0.7858, Test: 0.7085\n",
            "Early stopping:  0.02335318696747318\n",
            "Epoch: 044, Loss: 0.7382, Train: 0.7898, Test: 0.7105\n",
            "Early stopping:  0.022900825437851605\n",
            "Epoch: 045, Loss: 0.7242, Train: 0.7951, Test: 0.7108\n",
            "Early stopping:  0.022530600358895782\n",
            "Epoch: 046, Loss: 0.7106, Train: 0.8006, Test: 0.7148\n",
            "Early stopping:  0.022157772908387992\n",
            "Epoch: 047, Loss: 0.6969, Train: 0.8057, Test: 0.7159\n",
            "Early stopping:  0.02190062763279893\n",
            "Epoch: 048, Loss: 0.6835, Train: 0.8100, Test: 0.7196\n",
            "Early stopping:  0.021598439493373516\n",
            "Epoch: 049, Loss: 0.6703, Train: 0.8146, Test: 0.7230\n",
            "Early stopping:  0.021311855220787303\n",
            "Epoch: 050, Loss: 0.6572, Train: 0.8221, Test: 0.7241\n",
            "Early stopping:  0.021062940786566637\n",
            "Epoch: 051, Loss: 0.6445, Train: 0.8263, Test: 0.7270\n",
            "Early stopping:  0.02073791606017886\n",
            "Epoch: 052, Loss: 0.6317, Train: 0.8295, Test: 0.7284\n",
            "Early stopping:  0.020481530141025397\n",
            "Epoch: 053, Loss: 0.6191, Train: 0.8310, Test: 0.7301\n",
            "Early stopping:  0.020240734968810438\n",
            "Epoch: 054, Loss: 0.6071, Train: 0.8386, Test: 0.7324\n",
            "Early stopping:  0.01987800248261288\n",
            "Epoch: 055, Loss: 0.5989, Train: 0.8216, Test: 0.7236\n",
            "Early stopping:  0.018367625359382595\n",
            "Epoch: 056, Loss: 0.6040, Train: 0.8348, Test: 0.7270\n",
            "Early stopping:  0.013210758709755145\n",
            "Epoch: 057, Loss: 0.5940, Train: 0.8471, Test: 0.7380\n",
            "Early stopping:  0.009508842473461563\n",
            "PREDICTIONS -> tensor([ 9,  0,  1,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.77      0.76      0.76       379\n",
            "         capital_goods       0.66      0.62      0.64       254\n",
            "conglomerates_industry       1.00      0.12      0.22        40\n",
            "     consumer_cyclical       0.71      0.72      0.72       396\n",
            " consumer_non-cyclical       0.74      0.63      0.68       223\n",
            "                energy       0.86      0.77      0.81       141\n",
            "             financial       0.81      0.76      0.78       384\n",
            "            healthcare       0.87      0.73      0.79       159\n",
            "              services       0.69      0.82      0.75      1038\n",
            "            technology       0.66      0.58      0.61       198\n",
            "        transportation       0.84      0.80      0.82       202\n",
            "             utilities       0.88      0.75      0.81       113\n",
            "\n",
            "              accuracy                           0.74      3527\n",
            "             macro avg       0.79      0.67      0.70      3527\n",
            "          weighted avg       0.75      0.74      0.74      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4762, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2514, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1589698203881782\n",
            "Epoch: 003, Loss: 2.1678, Train: 0.2945, Test: 0.2943\n",
            "Early stopping:  0.15951480751500044\n",
            "Epoch: 004, Loss: 2.1183, Train: 0.3272, Test: 0.3215\n",
            "Early stopping:  0.1583468052471634\n",
            "Epoch: 005, Loss: 2.0520, Train: 0.3766, Test: 0.3706\n",
            "Early stopping:  0.16407265015294917\n",
            "Epoch: 006, Loss: 1.9947, Train: 0.3817, Test: 0.3779\n",
            "Early stopping:  0.09978270294779544\n",
            "Epoch: 007, Loss: 1.9331, Train: 0.3815, Test: 0.3740\n",
            "Early stopping:  0.09384556738952272\n",
            "Epoch: 008, Loss: 1.8740, Train: 0.3805, Test: 0.3745\n",
            "Early stopping:  0.09610471011420152\n",
            "Epoch: 009, Loss: 1.8181, Train: 0.4074, Test: 0.4006\n",
            "Early stopping:  0.09307037357211666\n",
            "Epoch: 010, Loss: 1.7558, Train: 0.4514, Test: 0.4409\n",
            "Early stopping:  0.09372418966788894\n",
            "Epoch: 011, Loss: 1.6877, Train: 0.4820, Test: 0.4701\n",
            "Early stopping:  0.09633546992967179\n",
            "Epoch: 012, Loss: 1.6238, Train: 0.5002, Test: 0.4871\n",
            "Early stopping:  0.09977854566783817\n",
            "Epoch: 013, Loss: 1.5661, Train: 0.5129, Test: 0.4970\n",
            "Early stopping:  0.10059802735414831\n",
            "Epoch: 014, Loss: 1.5084, Train: 0.5231, Test: 0.5081\n",
            "Early stopping:  0.0975481072456691\n",
            "Epoch: 015, Loss: 1.4509, Train: 0.5427, Test: 0.5265\n",
            "Early stopping:  0.09316025144547342\n",
            "Epoch: 016, Loss: 1.3962, Train: 0.5703, Test: 0.5498\n",
            "Early stopping:  0.09020020938752074\n",
            "Epoch: 017, Loss: 1.3457, Train: 0.5938, Test: 0.5744\n",
            "Early stopping:  0.08746727491649674\n",
            "Epoch: 018, Loss: 1.3017, Train: 0.6076, Test: 0.5886\n",
            "Early stopping:  0.0821047396767582\n",
            "Epoch: 019, Loss: 1.2612, Train: 0.6159, Test: 0.5923\n",
            "Early stopping:  0.07507505576321753\n",
            "Epoch: 020, Loss: 1.2226, Train: 0.6217, Test: 0.5963\n",
            "Early stopping:  0.06836628818436799\n",
            "Epoch: 021, Loss: 1.1872, Train: 0.6263, Test: 0.6011\n",
            "Early stopping:  0.06268952017642\n",
            "Epoch: 022, Loss: 1.1565, Train: 0.6357, Test: 0.6050\n",
            "Early stopping:  0.057691592782406174\n",
            "Epoch: 023, Loss: 1.1292, Train: 0.6452, Test: 0.6116\n",
            "Early stopping:  0.05229033000660015\n",
            "Epoch: 024, Loss: 1.1003, Train: 0.6552, Test: 0.6215\n",
            "Early stopping:  0.047872865086708516\n",
            "Epoch: 025, Loss: 1.0742, Train: 0.6633, Test: 0.6277\n",
            "Early stopping:  0.04462123117741232\n",
            "Epoch: 026, Loss: 1.0499, Train: 0.6739, Test: 0.6328\n",
            "Early stopping:  0.04242478944402471\n",
            "Epoch: 027, Loss: 1.0265, Train: 0.6841, Test: 0.6399\n",
            "Early stopping:  0.04050173573351146\n",
            "Epoch: 028, Loss: 1.0022, Train: 0.6900, Test: 0.6453\n",
            "Early stopping:  0.03857440085444774\n",
            "Epoch: 029, Loss: 0.9800, Train: 0.6972, Test: 0.6490\n",
            "Early stopping:  0.037331522906467525\n",
            "Epoch: 030, Loss: 0.9592, Train: 0.7043, Test: 0.6541\n",
            "Early stopping:  0.03605925064335624\n",
            "Epoch: 031, Loss: 0.9381, Train: 0.7176, Test: 0.6612\n",
            "Early stopping:  0.03475837908801804\n",
            "Epoch: 032, Loss: 0.9180, Train: 0.7255, Test: 0.6637\n",
            "Early stopping:  0.03327383799316099\n",
            "Epoch: 033, Loss: 0.9004, Train: 0.7333, Test: 0.6688\n",
            "Early stopping:  0.031720818213011946\n",
            "Epoch: 034, Loss: 0.8823, Train: 0.7403, Test: 0.6711\n",
            "Early stopping:  0.030314223776519555\n",
            "Epoch: 035, Loss: 0.8644, Train: 0.7439, Test: 0.6734\n",
            "Early stopping:  0.02895425136793885\n",
            "Epoch: 036, Loss: 0.8480, Train: 0.7541, Test: 0.6799\n",
            "Early stopping:  0.027820240859666383\n",
            "Epoch: 037, Loss: 0.8315, Train: 0.7607, Test: 0.6830\n",
            "Early stopping:  0.027225285688643835\n",
            "Epoch: 038, Loss: 0.8154, Train: 0.7643, Test: 0.6876\n",
            "Early stopping:  0.02637349606213328\n",
            "Epoch: 039, Loss: 0.8003, Train: 0.7720, Test: 0.6927\n",
            "Early stopping:  0.025427266402273226\n",
            "Epoch: 040, Loss: 0.7852, Train: 0.7771, Test: 0.6972\n",
            "Early stopping:  0.024768460604252158\n",
            "Epoch: 041, Loss: 0.7701, Train: 0.7820, Test: 0.6989\n",
            "Early stopping:  0.024181354585177263\n",
            "Epoch: 042, Loss: 0.7553, Train: 0.7885, Test: 0.7026\n",
            "Early stopping:  0.023779378296489684\n",
            "Epoch: 043, Loss: 0.7404, Train: 0.7909, Test: 0.7083\n",
            "Early stopping:  0.023694177813407297\n",
            "Epoch: 044, Loss: 0.7258, Train: 0.7979, Test: 0.7134\n",
            "Early stopping:  0.023499305432377663\n",
            "Epoch: 045, Loss: 0.7115, Train: 0.8013, Test: 0.7148\n",
            "Early stopping:  0.023193591408010715\n",
            "Epoch: 046, Loss: 0.6972, Train: 0.8043, Test: 0.7176\n",
            "Early stopping:  0.022929474446169964\n",
            "Epoch: 047, Loss: 0.6833, Train: 0.8129, Test: 0.7190\n",
            "Early stopping:  0.02254327068081393\n",
            "Epoch: 048, Loss: 0.6698, Train: 0.8119, Test: 0.7227\n",
            "Early stopping:  0.02216182156182934\n",
            "Epoch: 049, Loss: 0.6569, Train: 0.8187, Test: 0.7199\n",
            "Early stopping:  0.021615137150634\n",
            "Epoch: 050, Loss: 0.6461, Train: 0.8151, Test: 0.7261\n",
            "Early stopping:  0.020369971387171122\n",
            "Epoch: 051, Loss: 0.6365, Train: 0.8276, Test: 0.7261\n",
            "Early stopping:  0.018613698988824205\n",
            "Epoch: 052, Loss: 0.6249, Train: 0.8257, Test: 0.7298\n",
            "Early stopping:  0.017458733777561863\n",
            "Epoch: 053, Loss: 0.6084, Train: 0.8344, Test: 0.7352\n",
            "Early stopping:  0.018807401417182373\n",
            "Epoch: 054, Loss: 0.5948, Train: 0.8412, Test: 0.7335\n",
            "Early stopping:  0.02076600350572438\n",
            "Epoch: 055, Loss: 0.5873, Train: 0.8372, Test: 0.7372\n",
            "Early stopping:  0.020431817582398346\n",
            "Epoch: 056, Loss: 0.5771, Train: 0.8503, Test: 0.7397\n",
            "Early stopping:  0.018639234838615824\n",
            "Epoch: 057, Loss: 0.5623, Train: 0.8533, Test: 0.7397\n",
            "Early stopping:  0.01747704560740187\n",
            "Epoch: 058, Loss: 0.5510, Train: 0.8490, Test: 0.7443\n",
            "Early stopping:  0.017917642302081864\n",
            "Epoch: 059, Loss: 0.5431, Train: 0.8628, Test: 0.7400\n",
            "Early stopping:  0.018193577696231786\n",
            "Epoch: 060, Loss: 0.5332, Train: 0.8601, Test: 0.7451\n",
            "Early stopping:  0.017033057291995373\n",
            "Epoch: 061, Loss: 0.5210, Train: 0.8667, Test: 0.7457\n",
            "Early stopping:  0.01590382932777591\n",
            "Epoch: 062, Loss: 0.5092, Train: 0.8728, Test: 0.7445\n",
            "Early stopping:  0.01677840005538324\n",
            "Epoch: 063, Loss: 0.4996, Train: 0.8684, Test: 0.7499\n",
            "Early stopping:  0.017576391345580075\n",
            "Epoch: 064, Loss: 0.4918, Train: 0.8819, Test: 0.7448\n",
            "Early stopping:  0.016546034305737953\n",
            "Epoch: 065, Loss: 0.4823, Train: 0.8779, Test: 0.7525\n",
            "Early stopping:  0.015008582720784052\n",
            "Epoch: 066, Loss: 0.4706, Train: 0.8862, Test: 0.7496\n",
            "Early stopping:  0.014967599352892805\n",
            "Epoch: 067, Loss: 0.4594, Train: 0.8885, Test: 0.7508\n",
            "Early stopping:  0.016122164811710808\n",
            "Epoch: 068, Loss: 0.4501, Train: 0.8870, Test: 0.7539\n",
            "Early stopping:  0.016825673033584475\n",
            "Epoch: 069, Loss: 0.4414, Train: 0.9000, Test: 0.7533\n",
            "Early stopping:  0.016221374440416332\n",
            "Epoch: 070, Loss: 0.4330, Train: 0.8936, Test: 0.7536\n",
            "Early stopping:  0.014746666440027348\n",
            "Epoch: 071, Loss: 0.4261, Train: 0.9042, Test: 0.7556\n",
            "Early stopping:  0.013264104440716908\n",
            "Epoch: 072, Loss: 0.4231, Train: 0.8870, Test: 0.7505\n",
            "Early stopping:  0.01111355198629882\n",
            "Epoch: 073, Loss: 0.4254, Train: 0.9028, Test: 0.7494\n",
            "Early stopping:  0.007461293286847119\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.80      0.76       379\n",
            "         capital_goods       0.68      0.71      0.69       254\n",
            "conglomerates_industry       0.95      0.53      0.68        40\n",
            "     consumer_cyclical       0.65      0.73      0.69       396\n",
            " consumer_non-cyclical       0.77      0.67      0.72       223\n",
            "                energy       0.84      0.80      0.82       141\n",
            "             financial       0.86      0.72      0.79       384\n",
            "            healthcare       0.81      0.81      0.81       159\n",
            "              services       0.75      0.76      0.76      1038\n",
            "            technology       0.63      0.65      0.64       198\n",
            "        transportation       0.82      0.83      0.82       202\n",
            "             utilities       0.89      0.78      0.83       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.78      0.73      0.75      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5155, Train: 0.2943, Test: 0.2935\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2811, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16577569348571736\n",
            "Epoch: 003, Loss: 2.1579, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.18166523482559868\n",
            "Epoch: 004, Loss: 2.1524, Train: 0.2960, Test: 0.2949\n",
            "Early stopping:  0.1699083406680465\n",
            "Epoch: 005, Loss: 2.0857, Train: 0.3202, Test: 0.3184\n",
            "Early stopping:  0.17014122642042548\n",
            "Epoch: 006, Loss: 2.0126, Train: 0.3688, Test: 0.3595\n",
            "Early stopping:  0.0994123422977462\n",
            "Epoch: 007, Loss: 1.9567, Train: 0.4028, Test: 0.3916\n",
            "Early stopping:  0.08781970404536137\n",
            "Epoch: 008, Loss: 1.9020, Train: 0.4170, Test: 0.4029\n",
            "Early stopping:  0.09978437686574688\n",
            "Epoch: 009, Loss: 1.8417, Train: 0.4257, Test: 0.4094\n",
            "Early stopping:  0.0948096614852074\n",
            "Epoch: 010, Loss: 1.7783, Train: 0.4318, Test: 0.4165\n",
            "Early stopping:  0.09231159863513798\n",
            "Epoch: 011, Loss: 1.7182, Train: 0.4442, Test: 0.4346\n",
            "Early stopping:  0.09498794812754666\n",
            "Epoch: 012, Loss: 1.6616, Train: 0.4675, Test: 0.4559\n",
            "Early stopping:  0.09554580185452367\n",
            "Epoch: 013, Loss: 1.6044, Train: 0.4881, Test: 0.4786\n",
            "Early stopping:  0.09352500008268363\n",
            "Epoch: 014, Loss: 1.5484, Train: 0.5108, Test: 0.4962\n",
            "Early stopping:  0.09073320239375533\n",
            "Epoch: 015, Loss: 1.4949, Train: 0.5314, Test: 0.5160\n",
            "Early stopping:  0.08852638639373595\n",
            "Epoch: 016, Loss: 1.4435, Train: 0.5531, Test: 0.5319\n",
            "Early stopping:  0.08628516374944438\n",
            "Epoch: 017, Loss: 1.3971, Train: 0.5696, Test: 0.5452\n",
            "Early stopping:  0.08218632093164223\n",
            "Epoch: 018, Loss: 1.3545, Train: 0.5820, Test: 0.5543\n",
            "Early stopping:  0.0768650922852722\n",
            "Epoch: 019, Loss: 1.3120, Train: 0.5890, Test: 0.5682\n",
            "Early stopping:  0.07197667104001923\n",
            "Epoch: 020, Loss: 1.2732, Train: 0.6006, Test: 0.5795\n",
            "Early stopping:  0.0673473402222496\n",
            "Epoch: 021, Loss: 1.2404, Train: 0.6119, Test: 0.5900\n",
            "Early stopping:  0.062468515314184105\n",
            "Epoch: 022, Loss: 1.2075, Train: 0.6229, Test: 0.5954\n",
            "Early stopping:  0.05790363390126389\n",
            "Epoch: 023, Loss: 1.1758, Train: 0.6308, Test: 0.6042\n",
            "Early stopping:  0.05348146741653788\n",
            "Epoch: 024, Loss: 1.1474, Train: 0.6433, Test: 0.6133\n",
            "Early stopping:  0.05000321823126087\n",
            "Epoch: 025, Loss: 1.1196, Train: 0.6501, Test: 0.6192\n",
            "Early stopping:  0.04776580327168043\n",
            "Epoch: 026, Loss: 1.0940, Train: 0.6571, Test: 0.6215\n",
            "Early stopping:  0.044831280618992636\n",
            "Epoch: 027, Loss: 1.0690, Train: 0.6664, Test: 0.6283\n",
            "Early stopping:  0.04224707590204689\n",
            "Epoch: 028, Loss: 1.0447, Train: 0.6735, Test: 0.6365\n",
            "Early stopping:  0.04048157922264831\n",
            "Epoch: 029, Loss: 1.0232, Train: 0.6800, Test: 0.6391\n",
            "Early stopping:  0.03827900542304028\n",
            "Epoch: 030, Loss: 1.0026, Train: 0.6905, Test: 0.6442\n",
            "Early stopping:  0.036188343868194396\n",
            "Epoch: 031, Loss: 0.9819, Train: 0.6998, Test: 0.6496\n",
            "Early stopping:  0.03422464301539068\n",
            "Epoch: 032, Loss: 0.9612, Train: 0.7068, Test: 0.6552\n",
            "Early stopping:  0.032922417613418575\n",
            "Epoch: 033, Loss: 0.9428, Train: 0.7159, Test: 0.6586\n",
            "Early stopping:  0.031970657063325136\n",
            "Epoch: 034, Loss: 0.9237, Train: 0.7244, Test: 0.6618\n",
            "Early stopping:  0.031127076933061937\n",
            "Epoch: 035, Loss: 0.9049, Train: 0.7321, Test: 0.6680\n",
            "Early stopping:  0.030298706806140798\n",
            "Epoch: 036, Loss: 0.8879, Train: 0.7382, Test: 0.6714\n",
            "Early stopping:  0.02918530668922137\n",
            "Epoch: 037, Loss: 0.8709, Train: 0.7440, Test: 0.6725\n",
            "Early stopping:  0.02839669044392245\n",
            "Epoch: 038, Loss: 0.8546, Train: 0.7478, Test: 0.6768\n",
            "Early stopping:  0.027232515311993985\n",
            "Epoch: 039, Loss: 0.8391, Train: 0.7565, Test: 0.6819\n",
            "Early stopping:  0.026070435945481817\n",
            "Epoch: 040, Loss: 0.8233, Train: 0.7618, Test: 0.6864\n",
            "Early stopping:  0.02547428603782871\n",
            "Epoch: 041, Loss: 0.8084, Train: 0.7675, Test: 0.6910\n",
            "Early stopping:  0.024729288131115967\n",
            "Epoch: 042, Loss: 0.7936, Train: 0.7728, Test: 0.6932\n",
            "Early stopping:  0.024156542644300498\n",
            "Epoch: 043, Loss: 0.7787, Train: 0.7766, Test: 0.6986\n",
            "Early stopping:  0.02380462455481561\n",
            "Epoch: 044, Loss: 0.7643, Train: 0.7819, Test: 0.7037\n",
            "Early stopping:  0.023353354133912457\n",
            "Epoch: 045, Loss: 0.7500, Train: 0.7868, Test: 0.7074\n",
            "Early stopping:  0.023106467477852208\n",
            "Epoch: 046, Loss: 0.7359, Train: 0.7890, Test: 0.7074\n",
            "Early stopping:  0.022772911136353092\n",
            "Epoch: 047, Loss: 0.7221, Train: 0.7943, Test: 0.7102\n",
            "Early stopping:  0.022374151648289698\n",
            "Epoch: 048, Loss: 0.7086, Train: 0.8002, Test: 0.7153\n",
            "Early stopping:  0.02203747215984467\n",
            "Epoch: 049, Loss: 0.6951, Train: 0.8066, Test: 0.7204\n",
            "Early stopping:  0.02168855391795334\n",
            "Epoch: 050, Loss: 0.6821, Train: 0.8119, Test: 0.7241\n",
            "Early stopping:  0.02131231847694335\n",
            "Epoch: 051, Loss: 0.6691, Train: 0.8170, Test: 0.7233\n",
            "Early stopping:  0.02096616660891515\n",
            "Epoch: 052, Loss: 0.6563, Train: 0.8206, Test: 0.7250\n",
            "Early stopping:  0.020632275363166995\n",
            "Epoch: 053, Loss: 0.6438, Train: 0.8248, Test: 0.7275\n",
            "Early stopping:  0.020280048610165993\n",
            "Epoch: 054, Loss: 0.6314, Train: 0.8291, Test: 0.7309\n",
            "Early stopping:  0.02000937133408826\n",
            "Epoch: 055, Loss: 0.6194, Train: 0.8331, Test: 0.7292\n",
            "Early stopping:  0.01966258370751682\n",
            "Epoch: 056, Loss: 0.6076, Train: 0.8336, Test: 0.7343\n",
            "Early stopping:  0.01927272010349681\n",
            "Epoch: 057, Loss: 0.5968, Train: 0.8420, Test: 0.7324\n",
            "Early stopping:  0.018646564993331463\n",
            "Epoch: 058, Loss: 0.5868, Train: 0.8382, Test: 0.7346\n",
            "Early stopping:  0.0176785096990539\n",
            "Epoch: 059, Loss: 0.5775, Train: 0.8518, Test: 0.7363\n",
            "Early stopping:  0.016543821925387066\n",
            "Epoch: 060, Loss: 0.5655, Train: 0.8516, Test: 0.7380\n",
            "Early stopping:  0.016391543659213685\n",
            "Epoch: 061, Loss: 0.5528, Train: 0.8548, Test: 0.7411\n",
            "Early stopping:  0.017325858574601262\n",
            "Epoch: 062, Loss: 0.5422, Train: 0.8607, Test: 0.7403\n",
            "Early stopping:  0.018054639788415386\n",
            "Epoch: 063, Loss: 0.5334, Train: 0.8569, Test: 0.7420\n",
            "Early stopping:  0.017665296178881663\n",
            "Epoch: 064, Loss: 0.5244, Train: 0.8664, Test: 0.7462\n",
            "Early stopping:  0.016095446243557746\n",
            "Epoch: 065, Loss: 0.5138, Train: 0.8658, Test: 0.7440\n",
            "Early stopping:  0.015137976824541018\n",
            "Epoch: 066, Loss: 0.5033, Train: 0.8703, Test: 0.7505\n",
            "Early stopping:  0.015408084546760821\n",
            "Epoch: 067, Loss: 0.4933, Train: 0.8758, Test: 0.7505\n",
            "Early stopping:  0.016030558415328713\n",
            "Epoch: 068, Loss: 0.4836, Train: 0.8756, Test: 0.7505\n",
            "Early stopping:  0.01616068755657349\n",
            "Epoch: 069, Loss: 0.4746, Train: 0.8815, Test: 0.7542\n",
            "Early stopping:  0.015512252494227682\n",
            "Epoch: 070, Loss: 0.4666, Train: 0.8796, Test: 0.7479\n",
            "Early stopping:  0.014559544006770236\n",
            "Epoch: 071, Loss: 0.4599, Train: 0.8877, Test: 0.7562\n",
            "Early stopping:  0.013289766669559175\n",
            "Epoch: 072, Loss: 0.4516, Train: 0.8877, Test: 0.7528\n",
            "Early stopping:  0.012448307672964034\n",
            "Epoch: 073, Loss: 0.4407, Train: 0.8958, Test: 0.7590\n",
            "Early stopping:  0.013146417123803248\n",
            "Epoch: 074, Loss: 0.4296, Train: 0.8968, Test: 0.7573\n",
            "Early stopping:  0.014822105783997334\n",
            "Epoch: 075, Loss: 0.4218, Train: 0.8994, Test: 0.7584\n",
            "Early stopping:  0.01554916238375151\n",
            "Epoch: 076, Loss: 0.4149, Train: 0.9015, Test: 0.7618\n",
            "Early stopping:  0.014683983978631653\n",
            "Epoch: 077, Loss: 0.4068, Train: 0.9026, Test: 0.7596\n",
            "Early stopping:  0.013093629187218528\n",
            "Epoch: 078, Loss: 0.3984, Train: 0.9093, Test: 0.7610\n",
            "Early stopping:  0.012231350493245592\n",
            "Epoch: 079, Loss: 0.3925, Train: 0.9025, Test: 0.7593\n",
            "Early stopping:  0.011888329063701962\n",
            "Epoch: 080, Loss: 0.3886, Train: 0.9147, Test: 0.7604\n",
            "Early stopping:  0.010682648292447092\n",
            "Epoch: 081, Loss: 0.3816, Train: 0.9115, Test: 0.7630\n",
            "Early stopping:  0.009588255719147176\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.77      0.80      0.78       379\n",
            "         capital_goods       0.76      0.61      0.68       254\n",
            "conglomerates_industry       0.91      0.72      0.81        40\n",
            "     consumer_cyclical       0.77      0.68      0.72       396\n",
            " consumer_non-cyclical       0.78      0.67      0.72       223\n",
            "                energy       0.87      0.78      0.82       141\n",
            "             financial       0.83      0.76      0.79       384\n",
            "            healthcare       0.86      0.79      0.82       159\n",
            "              services       0.69      0.86      0.77      1038\n",
            "            technology       0.75      0.59      0.66       198\n",
            "        transportation       0.85      0.80      0.83       202\n",
            "             utilities       0.87      0.80      0.83       113\n",
            "\n",
            "              accuracy                           0.76      3527\n",
            "             macro avg       0.81      0.74      0.77      3527\n",
            "          weighted avg       0.77      0.76      0.76      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4858, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2727, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15072437953290094\n",
            "Epoch: 003, Loss: 2.1626, Train: 0.2951, Test: 0.2943\n",
            "Early stopping:  0.16431916408077998\n",
            "Epoch: 004, Loss: 2.1137, Train: 0.3250, Test: 0.3193\n",
            "Early stopping:  0.16536185951729096\n",
            "Epoch: 005, Loss: 2.0361, Train: 0.3800, Test: 0.3782\n",
            "Early stopping:  0.17440450022918555\n",
            "Epoch: 006, Loss: 1.9668, Train: 0.4045, Test: 0.3981\n",
            "Early stopping:  0.11747106348777461\n",
            "Epoch: 007, Loss: 1.9040, Train: 0.4130, Test: 0.4057\n",
            "Early stopping:  0.10522046698735658\n",
            "Epoch: 008, Loss: 1.8402, Train: 0.4229, Test: 0.4148\n",
            "Early stopping:  0.10747955994362325\n",
            "Epoch: 009, Loss: 1.7721, Train: 0.4454, Test: 0.4355\n",
            "Early stopping:  0.10351242158576397\n",
            "Epoch: 010, Loss: 1.7024, Train: 0.4713, Test: 0.4644\n",
            "Early stopping:  0.10449344610540447\n",
            "Epoch: 011, Loss: 1.6369, Train: 0.4902, Test: 0.4797\n",
            "Early stopping:  0.10626440321434678\n",
            "Epoch: 012, Loss: 1.5793, Train: 0.5053, Test: 0.4925\n",
            "Early stopping:  0.10395747853571015\n",
            "Epoch: 013, Loss: 1.5237, Train: 0.5223, Test: 0.5149\n",
            "Early stopping:  0.09814819189864464\n",
            "Epoch: 014, Loss: 1.4655, Train: 0.5467, Test: 0.5379\n",
            "Early stopping:  0.09285020421789979\n",
            "Epoch: 015, Loss: 1.4122, Train: 0.5715, Test: 0.5532\n",
            "Early stopping:  0.08905367848702028\n",
            "Epoch: 016, Loss: 1.3668, Train: 0.5875, Test: 0.5719\n",
            "Early stopping:  0.08491864639180656\n",
            "Epoch: 017, Loss: 1.3277, Train: 0.5981, Test: 0.5798\n",
            "Early stopping:  0.07783680506557626\n",
            "Epoch: 018, Loss: 1.2912, Train: 0.6074, Test: 0.5878\n",
            "Early stopping:  0.0687201386445035\n",
            "Epoch: 019, Loss: 1.2526, Train: 0.6091, Test: 0.5923\n",
            "Early stopping:  0.06248977777803486\n",
            "Epoch: 020, Loss: 1.2183, Train: 0.6144, Test: 0.5971\n",
            "Early stopping:  0.05884554261149511\n",
            "Epoch: 021, Loss: 1.1899, Train: 0.6238, Test: 0.6073\n",
            "Early stopping:  0.055176210865808505\n",
            "Epoch: 022, Loss: 1.1610, Train: 0.6388, Test: 0.6144\n",
            "Early stopping:  0.051184614346156246\n",
            "Epoch: 023, Loss: 1.1330, Train: 0.6535, Test: 0.6272\n",
            "Early stopping:  0.046917523008247725\n",
            "Epoch: 024, Loss: 1.1061, Train: 0.6637, Test: 0.6360\n",
            "Early stopping:  0.04448540417684884\n",
            "Epoch: 025, Loss: 1.0821, Train: 0.6733, Test: 0.6428\n",
            "Early stopping:  0.04280616340592689\n",
            "Epoch: 026, Loss: 1.0579, Train: 0.6798, Test: 0.6467\n",
            "Early stopping:  0.040689406518165785\n",
            "Epoch: 027, Loss: 1.0333, Train: 0.6820, Test: 0.6453\n",
            "Early stopping:  0.039141438319834576\n",
            "Epoch: 028, Loss: 1.0125, Train: 0.6911, Test: 0.6487\n",
            "Early stopping:  0.03731518495370257\n",
            "Epoch: 029, Loss: 0.9930, Train: 0.6992, Test: 0.6564\n",
            "Early stopping:  0.03538852261645082\n",
            "Epoch: 030, Loss: 0.9731, Train: 0.7081, Test: 0.6581\n",
            "Early stopping:  0.03321703539580257\n",
            "Epoch: 031, Loss: 0.9530, Train: 0.7157, Test: 0.6643\n",
            "Early stopping:  0.031625927208715036\n",
            "Epoch: 032, Loss: 0.9351, Train: 0.7206, Test: 0.6671\n",
            "Early stopping:  0.0308002745028847\n",
            "Epoch: 033, Loss: 0.9162, Train: 0.7261, Test: 0.6705\n",
            "Early stopping:  0.03029642233288213\n",
            "Epoch: 034, Loss: 0.8976, Train: 0.7327, Test: 0.6700\n",
            "Early stopping:  0.0296959404912005\n",
            "Epoch: 035, Loss: 0.8798, Train: 0.7388, Test: 0.6705\n",
            "Early stopping:  0.02906354366535528\n",
            "Epoch: 036, Loss: 0.8631, Train: 0.7476, Test: 0.6773\n",
            "Early stopping:  0.028537318597120068\n",
            "Epoch: 037, Loss: 0.8465, Train: 0.7524, Test: 0.6813\n",
            "Early stopping:  0.027520473067192783\n",
            "Epoch: 038, Loss: 0.8310, Train: 0.7631, Test: 0.6833\n",
            "Early stopping:  0.02635685845947957\n",
            "Epoch: 039, Loss: 0.8155, Train: 0.7688, Test: 0.6881\n",
            "Early stopping:  0.025420891389668354\n",
            "Epoch: 040, Loss: 0.8001, Train: 0.7730, Test: 0.6918\n",
            "Early stopping:  0.024829574160671906\n",
            "Epoch: 041, Loss: 0.7849, Train: 0.7781, Test: 0.6963\n",
            "Early stopping:  0.024362962884922963\n",
            "Epoch: 042, Loss: 0.7698, Train: 0.7879, Test: 0.7000\n",
            "Early stopping:  0.024186062106601965\n",
            "Epoch: 043, Loss: 0.7549, Train: 0.7907, Test: 0.7048\n",
            "Early stopping:  0.02394862645143105\n",
            "Epoch: 044, Loss: 0.7403, Train: 0.7962, Test: 0.7097\n",
            "Early stopping:  0.023656232751308633\n",
            "Epoch: 045, Loss: 0.7259, Train: 0.8004, Test: 0.7139\n",
            "Early stopping:  0.023347110671785583\n",
            "Epoch: 046, Loss: 0.7114, Train: 0.8055, Test: 0.7170\n",
            "Early stopping:  0.023072853276201555\n",
            "Epoch: 047, Loss: 0.6973, Train: 0.8106, Test: 0.7207\n",
            "Early stopping:  0.022775910061123786\n",
            "Epoch: 048, Loss: 0.6831, Train: 0.8155, Test: 0.7255\n",
            "Early stopping:  0.022575012663700454\n",
            "Epoch: 049, Loss: 0.6692, Train: 0.8183, Test: 0.7298\n",
            "Early stopping:  0.022398893795540508\n",
            "Epoch: 050, Loss: 0.6554, Train: 0.8233, Test: 0.7326\n",
            "Early stopping:  0.02215691450738283\n",
            "Epoch: 051, Loss: 0.6416, Train: 0.8265, Test: 0.7329\n",
            "Early stopping:  0.0220096357222157\n",
            "Epoch: 052, Loss: 0.6282, Train: 0.8319, Test: 0.7355\n",
            "Early stopping:  0.021716266194929387\n",
            "Epoch: 053, Loss: 0.6148, Train: 0.8357, Test: 0.7363\n",
            "Early stopping:  0.0214694601503255\n",
            "Epoch: 054, Loss: 0.6016, Train: 0.8401, Test: 0.7389\n",
            "Early stopping:  0.021247818187495736\n",
            "Epoch: 055, Loss: 0.5885, Train: 0.8442, Test: 0.7411\n",
            "Early stopping:  0.020993359439397938\n",
            "Epoch: 056, Loss: 0.5756, Train: 0.8452, Test: 0.7426\n",
            "Early stopping:  0.020805993056516186\n",
            "Epoch: 057, Loss: 0.5630, Train: 0.8522, Test: 0.7426\n",
            "Early stopping:  0.020494855809630754\n",
            "Epoch: 058, Loss: 0.5516, Train: 0.8484, Test: 0.7428\n",
            "Early stopping:  0.019839807707453784\n",
            "Epoch: 059, Loss: 0.5423, Train: 0.8605, Test: 0.7400\n",
            "Early stopping:  0.018441651034053562\n",
            "Epoch: 060, Loss: 0.5342, Train: 0.8563, Test: 0.7420\n",
            "Early stopping:  0.016433659974728426\n",
            "Epoch: 061, Loss: 0.5197, Train: 0.8667, Test: 0.7494\n",
            "Early stopping:  0.016527502925252686\n",
            "Epoch: 062, Loss: 0.5029, Train: 0.8724, Test: 0.7488\n",
            "Early stopping:  0.019221572767696937\n",
            "Epoch: 063, Loss: 0.4941, Train: 0.8709, Test: 0.7440\n",
            "Early stopping:  0.020348181121169824\n",
            "Epoch: 064, Loss: 0.4868, Train: 0.8819, Test: 0.7488\n",
            "Early stopping:  0.019311056472230706\n",
            "Epoch: 065, Loss: 0.4733, Train: 0.8845, Test: 0.7525\n",
            "Early stopping:  0.01739584457727653\n",
            "Epoch: 066, Loss: 0.4603, Train: 0.8834, Test: 0.7468\n",
            "Early stopping:  0.016870834026529568\n",
            "Epoch: 067, Loss: 0.4532, Train: 0.8941, Test: 0.7488\n",
            "Early stopping:  0.017207288957875173\n",
            "Epoch: 068, Loss: 0.4445, Train: 0.8943, Test: 0.7516\n",
            "Early stopping:  0.016704262233529478\n",
            "Epoch: 069, Loss: 0.4314, Train: 0.8972, Test: 0.7522\n",
            "Early stopping:  0.015840049912156408\n",
            "Epoch: 070, Loss: 0.4213, Train: 0.9059, Test: 0.7525\n",
            "Early stopping:  0.01588382076965651\n",
            "Epoch: 071, Loss: 0.4137, Train: 0.9008, Test: 0.7516\n",
            "Early stopping:  0.01621105734603689\n",
            "Epoch: 072, Loss: 0.4044, Train: 0.9106, Test: 0.7539\n",
            "Early stopping:  0.01554133738224696\n",
            "Epoch: 073, Loss: 0.3940, Train: 0.9121, Test: 0.7539\n",
            "Early stopping:  0.01453380465703755\n",
            "Epoch: 074, Loss: 0.3847, Train: 0.9110, Test: 0.7519\n",
            "Early stopping:  0.014729498982729168\n",
            "Epoch: 075, Loss: 0.3791, Train: 0.9113, Test: 0.7522\n",
            "Early stopping:  0.014152243053563057\n",
            "Epoch: 076, Loss: 0.3755, Train: 0.9147, Test: 0.7528\n",
            "Early stopping:  0.011726303554786119\n",
            "Epoch: 077, Loss: 0.3727, Train: 0.9166, Test: 0.7505\n",
            "Early stopping:  0.008419648487757905\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.84      0.76      0.80       379\n",
            "         capital_goods       0.73      0.64      0.68       254\n",
            "conglomerates_industry       1.00      0.30      0.46        40\n",
            "     consumer_cyclical       0.75      0.63      0.68       396\n",
            " consumer_non-cyclical       0.78      0.69      0.73       223\n",
            "                energy       0.83      0.78      0.81       141\n",
            "             financial       0.76      0.83      0.79       384\n",
            "            healthcare       0.81      0.74      0.77       159\n",
            "              services       0.70      0.83      0.76      1038\n",
            "            technology       0.67      0.59      0.63       198\n",
            "        transportation       0.82      0.85      0.83       202\n",
            "             utilities       0.82      0.80      0.81       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.79      0.70      0.73      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5310, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2759, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1804092478760938\n",
            "Epoch: 003, Loss: 2.1903, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.17724992018192298\n",
            "Epoch: 004, Loss: 2.1526, Train: 0.2975, Test: 0.2969\n",
            "Early stopping:  0.17036532078681968\n",
            "Epoch: 005, Loss: 2.0738, Train: 0.3401, Test: 0.3357\n",
            "Early stopping:  0.17576298212854397\n",
            "Epoch: 006, Loss: 2.0204, Train: 0.3837, Test: 0.3731\n",
            "Early stopping:  0.09973819814881231\n",
            "Epoch: 007, Loss: 1.9799, Train: 0.3892, Test: 0.3782\n",
            "Early stopping:  0.08800854401256339\n",
            "Epoch: 008, Loss: 1.9272, Train: 0.3875, Test: 0.3743\n",
            "Early stopping:  0.08676496867056606\n",
            "Epoch: 009, Loss: 1.8639, Train: 0.3904, Test: 0.3794\n",
            "Early stopping:  0.0813462802541144\n",
            "Epoch: 010, Loss: 1.8020, Train: 0.4072, Test: 0.3969\n",
            "Early stopping:  0.0877212726708516\n",
            "Epoch: 011, Loss: 1.7438, Train: 0.4329, Test: 0.4242\n",
            "Early stopping:  0.09450047092262022\n",
            "Epoch: 012, Loss: 1.6832, Train: 0.4628, Test: 0.4562\n",
            "Early stopping:  0.09615172120115856\n",
            "Epoch: 013, Loss: 1.6238, Train: 0.4887, Test: 0.4803\n",
            "Early stopping:  0.09471097351166857\n",
            "Epoch: 014, Loss: 1.5686, Train: 0.5123, Test: 0.4996\n",
            "Early stopping:  0.09282122541965247\n",
            "Epoch: 015, Loss: 1.5131, Train: 0.5374, Test: 0.5177\n",
            "Early stopping:  0.09111706820549596\n",
            "Epoch: 016, Loss: 1.4574, Train: 0.5558, Test: 0.5396\n",
            "Early stopping:  0.0889275087702055\n",
            "Epoch: 017, Loss: 1.4087, Train: 0.5722, Test: 0.5602\n",
            "Early stopping:  0.08561937588018877\n",
            "Epoch: 018, Loss: 1.3651, Train: 0.5868, Test: 0.5744\n",
            "Early stopping:  0.08095354423847771\n",
            "Epoch: 019, Loss: 1.3210, Train: 0.6021, Test: 0.5861\n",
            "Early stopping:  0.0754350716671638\n",
            "Epoch: 020, Loss: 1.2791, Train: 0.6093, Test: 0.5906\n",
            "Early stopping:  0.07028065730177624\n",
            "Epoch: 021, Loss: 1.2426, Train: 0.6163, Test: 0.5988\n",
            "Early stopping:  0.06615086257148825\n",
            "Epoch: 022, Loss: 1.2091, Train: 0.6219, Test: 0.6011\n",
            "Early stopping:  0.06182760970376792\n",
            "Epoch: 023, Loss: 1.1762, Train: 0.6282, Test: 0.6070\n",
            "Early stopping:  0.05694055126684233\n",
            "Epoch: 024, Loss: 1.1466, Train: 0.6348, Test: 0.6133\n",
            "Early stopping:  0.05243901277691893\n",
            "Epoch: 025, Loss: 1.1211, Train: 0.6433, Test: 0.6155\n",
            "Early stopping:  0.04839253471430816\n",
            "Epoch: 026, Loss: 1.0960, Train: 0.6533, Test: 0.6223\n",
            "Early stopping:  0.04455182443796681\n",
            "Epoch: 027, Loss: 1.0704, Train: 0.6633, Test: 0.6294\n",
            "Early stopping:  0.04145995156652662\n",
            "Epoch: 028, Loss: 1.0476, Train: 0.6711, Test: 0.6351\n",
            "Early stopping:  0.03932597031074802\n",
            "Epoch: 029, Loss: 1.0248, Train: 0.6805, Test: 0.6425\n",
            "Early stopping:  0.03812271063742253\n",
            "Epoch: 030, Loss: 1.0012, Train: 0.6890, Test: 0.6453\n",
            "Early stopping:  0.03721422568909838\n",
            "Epoch: 031, Loss: 0.9811, Train: 0.6985, Test: 0.6541\n",
            "Early stopping:  0.035597793861149404\n",
            "Epoch: 032, Loss: 0.9608, Train: 0.7064, Test: 0.6595\n",
            "Early stopping:  0.03437712948379215\n",
            "Epoch: 033, Loss: 0.9401, Train: 0.7132, Test: 0.6640\n",
            "Early stopping:  0.03317355064662464\n",
            "Epoch: 034, Loss: 0.9215, Train: 0.7208, Test: 0.6674\n",
            "Early stopping:  0.03168266019089568\n",
            "Epoch: 035, Loss: 0.9030, Train: 0.7285, Test: 0.6677\n",
            "Early stopping:  0.0309184325927453\n",
            "Epoch: 036, Loss: 0.8841, Train: 0.7353, Test: 0.6717\n",
            "Early stopping:  0.030122639344922857\n",
            "Epoch: 037, Loss: 0.8667, Train: 0.7439, Test: 0.6745\n",
            "Early stopping:  0.029109892234986615\n",
            "Epoch: 038, Loss: 0.8502, Train: 0.7544, Test: 0.6799\n",
            "Early stopping:  0.028276946688992825\n",
            "Epoch: 039, Loss: 0.8333, Train: 0.7603, Test: 0.6827\n",
            "Early stopping:  0.02741562309102666\n",
            "Epoch: 040, Loss: 0.8175, Train: 0.7647, Test: 0.6876\n",
            "Early stopping:  0.02636273895502814\n",
            "Epoch: 041, Loss: 0.8025, Train: 0.7715, Test: 0.6910\n",
            "Early stopping:  0.025487823301646617\n",
            "Epoch: 042, Loss: 0.7868, Train: 0.7762, Test: 0.6952\n",
            "Early stopping:  0.024914563001616014\n",
            "Epoch: 043, Loss: 0.7720, Train: 0.7792, Test: 0.6972\n",
            "Early stopping:  0.02424006710259993\n",
            "Epoch: 044, Loss: 0.7571, Train: 0.7836, Test: 0.7006\n",
            "Early stopping:  0.023948305790177794\n",
            "Epoch: 045, Loss: 0.7423, Train: 0.7902, Test: 0.7043\n",
            "Early stopping:  0.02377381620174611\n",
            "Epoch: 046, Loss: 0.7279, Train: 0.7957, Test: 0.7071\n",
            "Early stopping:  0.023334254750445774\n",
            "Epoch: 047, Loss: 0.7139, Train: 0.8015, Test: 0.7114\n",
            "Early stopping:  0.02298575600154661\n",
            "Epoch: 048, Loss: 0.6996, Train: 0.8062, Test: 0.7105\n",
            "Early stopping:  0.022651083477068883\n",
            "Epoch: 049, Loss: 0.6858, Train: 0.8100, Test: 0.7136\n",
            "Early stopping:  0.022339010729850173\n",
            "Epoch: 050, Loss: 0.6719, Train: 0.8151, Test: 0.7176\n",
            "Early stopping:  0.02216498828034398\n",
            "Epoch: 051, Loss: 0.6582, Train: 0.8210, Test: 0.7199\n",
            "Early stopping:  0.021998799362211826\n",
            "Epoch: 052, Loss: 0.6445, Train: 0.8253, Test: 0.7241\n",
            "Early stopping:  0.02178419074668363\n",
            "Epoch: 053, Loss: 0.6311, Train: 0.8293, Test: 0.7255\n",
            "Early stopping:  0.021594110176038284\n",
            "Epoch: 054, Loss: 0.6178, Train: 0.8348, Test: 0.7298\n",
            "Early stopping:  0.021368064325835733\n",
            "Epoch: 055, Loss: 0.6049, Train: 0.8344, Test: 0.7295\n",
            "Early stopping:  0.02108337128367668\n",
            "Epoch: 056, Loss: 0.5931, Train: 0.8423, Test: 0.7298\n",
            "Early stopping:  0.020431496434312447\n",
            "Epoch: 057, Loss: 0.5834, Train: 0.8369, Test: 0.7332\n",
            "Early stopping:  0.019052503099425282\n",
            "Epoch: 058, Loss: 0.5743, Train: 0.8503, Test: 0.7343\n",
            "Early stopping:  0.017218075505806806\n",
            "Epoch: 059, Loss: 0.5597, Train: 0.8527, Test: 0.7363\n",
            "Early stopping:  0.017302783630541847\n",
            "Epoch: 060, Loss: 0.5439, Train: 0.8520, Test: 0.7360\n",
            "Early stopping:  0.019438055985723693\n",
            "Epoch: 061, Loss: 0.5349, Train: 0.8611, Test: 0.7383\n",
            "Early stopping:  0.020207909368055405\n",
            "Epoch: 062, Loss: 0.5272, Train: 0.8614, Test: 0.7411\n",
            "Early stopping:  0.01902764999502133\n",
            "Epoch: 063, Loss: 0.5137, Train: 0.8707, Test: 0.7462\n",
            "Early stopping:  0.017317507897922052\n",
            "Epoch: 064, Loss: 0.5002, Train: 0.8730, Test: 0.7443\n",
            "Early stopping:  0.01729964824743493\n",
            "Epoch: 065, Loss: 0.4921, Train: 0.8698, Test: 0.7434\n",
            "Early stopping:  0.017898169148073503\n",
            "Epoch: 066, Loss: 0.4845, Train: 0.8819, Test: 0.7491\n",
            "Early stopping:  0.017086097680125336\n",
            "Epoch: 067, Loss: 0.4723, Train: 0.8851, Test: 0.7468\n",
            "Early stopping:  0.015686963609491154\n",
            "Epoch: 068, Loss: 0.4599, Train: 0.8856, Test: 0.7454\n",
            "Early stopping:  0.015983104274735916\n",
            "Epoch: 069, Loss: 0.4516, Train: 0.8945, Test: 0.7505\n",
            "Early stopping:  0.016762267828185382\n",
            "Epoch: 070, Loss: 0.4442, Train: 0.8877, Test: 0.7471\n",
            "Early stopping:  0.01614244407420634\n",
            "Epoch: 071, Loss: 0.4352, Train: 0.8992, Test: 0.7530\n",
            "Early stopping:  0.014271367595312615\n",
            "Epoch: 072, Loss: 0.4246, Train: 0.8989, Test: 0.7542\n",
            "Early stopping:  0.013790372762993375\n",
            "Epoch: 073, Loss: 0.4140, Train: 0.9015, Test: 0.7550\n",
            "Early stopping:  0.015018116921504898\n",
            "Epoch: 074, Loss: 0.4053, Train: 0.9102, Test: 0.7570\n",
            "Early stopping:  0.01565508664393711\n",
            "Epoch: 075, Loss: 0.3986, Train: 0.9030, Test: 0.7513\n",
            "Early stopping:  0.014705687981740462\n",
            "Epoch: 076, Loss: 0.3930, Train: 0.9157, Test: 0.7547\n",
            "Early stopping:  0.012547027372253118\n",
            "Epoch: 077, Loss: 0.3859, Train: 0.9085, Test: 0.7522\n",
            "Early stopping:  0.010881663916368337\n",
            "Epoch: 078, Loss: 0.3769, Train: 0.9217, Test: 0.7533\n",
            "Early stopping:  0.01102634712782553\n",
            "Epoch: 079, Loss: 0.3670, Train: 0.9225, Test: 0.7553\n",
            "Early stopping:  0.012581260673126107\n",
            "Epoch: 080, Loss: 0.3586, Train: 0.9242, Test: 0.7547\n",
            "Early stopping:  0.013868530441279692\n",
            "Epoch: 081, Loss: 0.3509, Train: 0.9285, Test: 0.7545\n",
            "Early stopping:  0.013959167951090506\n",
            "Epoch: 082, Loss: 0.3442, Train: 0.9272, Test: 0.7525\n",
            "Early stopping:  0.012913238809839916\n",
            "Epoch: 083, Loss: 0.3393, Train: 0.9327, Test: 0.7545\n",
            "Early stopping:  0.01108018672692109\n",
            "Epoch: 084, Loss: 0.3354, Train: 0.9284, Test: 0.7502\n",
            "Early stopping:  0.009240502302734259\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.82      0.75      0.78       379\n",
            "         capital_goods       0.73      0.63      0.68       254\n",
            "conglomerates_industry       1.00      0.55      0.71        40\n",
            "     consumer_cyclical       0.73      0.70      0.72       396\n",
            " consumer_non-cyclical       0.75      0.66      0.70       223\n",
            "                energy       0.86      0.77      0.81       141\n",
            "             financial       0.78      0.78      0.78       384\n",
            "            healthcare       0.81      0.74      0.77       159\n",
            "              services       0.69      0.83      0.75      1038\n",
            "            technology       0.71      0.61      0.65       198\n",
            "        transportation       0.83      0.82      0.83       202\n",
            "             utilities       0.89      0.74      0.81       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.80      0.72      0.75      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4942, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2619, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16424677440913618\n",
            "Epoch: 003, Loss: 2.1707, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.16677301115818607\n",
            "Epoch: 004, Loss: 2.1265, Train: 0.3164, Test: 0.3153\n",
            "Early stopping:  0.16390695743265857\n",
            "Epoch: 005, Loss: 2.0596, Train: 0.3624, Test: 0.3536\n",
            "Early stopping:  0.1686791064057855\n",
            "Epoch: 006, Loss: 1.9961, Train: 0.3837, Test: 0.3751\n",
            "Early stopping:  0.10211288539076988\n",
            "Epoch: 007, Loss: 1.9338, Train: 0.3900, Test: 0.3833\n",
            "Early stopping:  0.09573520506384252\n",
            "Epoch: 008, Loss: 1.8733, Train: 0.3972, Test: 0.3805\n",
            "Early stopping:  0.0999602002567224\n",
            "Epoch: 009, Loss: 1.8156, Train: 0.4106, Test: 0.3941\n",
            "Early stopping:  0.09658006949353995\n",
            "Epoch: 010, Loss: 1.7580, Train: 0.4344, Test: 0.4230\n",
            "Early stopping:  0.09398783589369834\n",
            "Epoch: 011, Loss: 1.6945, Train: 0.4622, Test: 0.4514\n",
            "Early stopping:  0.09391708775719812\n",
            "Epoch: 012, Loss: 1.6292, Train: 0.4832, Test: 0.4687\n",
            "Early stopping:  0.09639250236813972\n",
            "Epoch: 013, Loss: 1.5712, Train: 0.4972, Test: 0.4775\n",
            "Early stopping:  0.09768576730766086\n",
            "Epoch: 014, Loss: 1.5182, Train: 0.5214, Test: 0.4996\n",
            "Early stopping:  0.09543537423795068\n",
            "Epoch: 015, Loss: 1.4641, Train: 0.5412, Test: 0.5197\n",
            "Early stopping:  0.09050479564999356\n",
            "Epoch: 016, Loss: 1.4124, Train: 0.5607, Test: 0.5418\n",
            "Early stopping:  0.08552266481275811\n",
            "Epoch: 017, Loss: 1.3662, Train: 0.5790, Test: 0.5594\n",
            "Early stopping:  0.08157072115246213\n",
            "Epoch: 018, Loss: 1.3226, Train: 0.5947, Test: 0.5699\n",
            "Early stopping:  0.07738597402258458\n",
            "Epoch: 019, Loss: 1.2820, Train: 0.6059, Test: 0.5835\n",
            "Early stopping:  0.07183390069858181\n",
            "Epoch: 020, Loss: 1.2453, Train: 0.6132, Test: 0.5906\n",
            "Early stopping:  0.06621064189013945\n",
            "Epoch: 021, Loss: 1.2096, Train: 0.6210, Test: 0.5971\n",
            "Early stopping:  0.06180893880706539\n",
            "Epoch: 022, Loss: 1.1755, Train: 0.6312, Test: 0.6073\n",
            "Early stopping:  0.05800998548433862\n",
            "Epoch: 023, Loss: 1.1442, Train: 0.6388, Test: 0.6124\n",
            "Early stopping:  0.054652672996499035\n",
            "Epoch: 024, Loss: 1.1169, Train: 0.6499, Test: 0.6206\n",
            "Early stopping:  0.051020942123280545\n",
            "Epoch: 025, Loss: 1.0908, Train: 0.6599, Test: 0.6294\n",
            "Early stopping:  0.046902821860515184\n",
            "Epoch: 026, Loss: 1.0636, Train: 0.6622, Test: 0.6343\n",
            "Early stopping:  0.04383873475400013\n",
            "Epoch: 027, Loss: 1.0402, Train: 0.6681, Test: 0.6391\n",
            "Early stopping:  0.041325546309548156\n",
            "Epoch: 028, Loss: 1.0182, Train: 0.6802, Test: 0.6464\n",
            "Early stopping:  0.039252603083047524\n",
            "Epoch: 029, Loss: 0.9946, Train: 0.6915, Test: 0.6476\n",
            "Early stopping:  0.03763567096183688\n",
            "Epoch: 030, Loss: 0.9721, Train: 0.6977, Test: 0.6530\n",
            "Early stopping:  0.03614843772633284\n",
            "Epoch: 031, Loss: 0.9511, Train: 0.7053, Test: 0.6544\n",
            "Early stopping:  0.03546212087162971\n",
            "Epoch: 032, Loss: 0.9301, Train: 0.7157, Test: 0.6601\n",
            "Early stopping:  0.034737076805741625\n",
            "Epoch: 033, Loss: 0.9090, Train: 0.7240, Test: 0.6660\n",
            "Early stopping:  0.03371187316372117\n",
            "Epoch: 034, Loss: 0.8900, Train: 0.7336, Test: 0.6686\n",
            "Early stopping:  0.03261828429093984\n",
            "Epoch: 035, Loss: 0.8713, Train: 0.7401, Test: 0.6756\n",
            "Early stopping:  0.03161483162326977\n",
            "Epoch: 036, Loss: 0.8534, Train: 0.7503, Test: 0.6799\n",
            "Early stopping:  0.03026016014524825\n",
            "Epoch: 037, Loss: 0.8363, Train: 0.7573, Test: 0.6830\n",
            "Early stopping:  0.0287846951190133\n",
            "Epoch: 038, Loss: 0.8199, Train: 0.7618, Test: 0.6842\n",
            "Early stopping:  0.027708063603003198\n",
            "Epoch: 039, Loss: 0.8041, Train: 0.7690, Test: 0.6895\n",
            "Early stopping:  0.026530762448824248\n",
            "Epoch: 040, Loss: 0.7886, Train: 0.7756, Test: 0.6944\n",
            "Early stopping:  0.025597039110719803\n",
            "Epoch: 041, Loss: 0.7731, Train: 0.7807, Test: 0.6980\n",
            "Early stopping:  0.024948150049991014\n",
            "Epoch: 042, Loss: 0.7586, Train: 0.7837, Test: 0.7037\n",
            "Early stopping:  0.02430260443701751\n",
            "Epoch: 043, Loss: 0.7433, Train: 0.7881, Test: 0.7065\n",
            "Early stopping:  0.023979132491483587\n",
            "Epoch: 044, Loss: 0.7285, Train: 0.7926, Test: 0.7071\n",
            "Early stopping:  0.023695128342749568\n",
            "Epoch: 045, Loss: 0.7141, Train: 0.7981, Test: 0.7097\n",
            "Early stopping:  0.0234261363964544\n",
            "Epoch: 046, Loss: 0.6994, Train: 0.8021, Test: 0.7100\n",
            "Early stopping:  0.023325890217628476\n",
            "Epoch: 047, Loss: 0.6851, Train: 0.8066, Test: 0.7131\n",
            "Early stopping:  0.023001951757211852\n",
            "Epoch: 048, Loss: 0.6710, Train: 0.8102, Test: 0.7131\n",
            "Early stopping:  0.022784873293321413\n",
            "Epoch: 049, Loss: 0.6571, Train: 0.8166, Test: 0.7156\n",
            "Early stopping:  0.022512233534076453\n",
            "Epoch: 050, Loss: 0.6432, Train: 0.8221, Test: 0.7190\n",
            "Early stopping:  0.022195949422629827\n",
            "Epoch: 051, Loss: 0.6299, Train: 0.8257, Test: 0.7233\n",
            "Early stopping:  0.0218512985709134\n",
            "Epoch: 052, Loss: 0.6167, Train: 0.8329, Test: 0.7255\n",
            "Early stopping:  0.021471445576201835\n",
            "Epoch: 053, Loss: 0.6037, Train: 0.8371, Test: 0.7312\n",
            "Early stopping:  0.021073612437191823\n",
            "Epoch: 054, Loss: 0.5909, Train: 0.8439, Test: 0.7315\n",
            "Early stopping:  0.020678589653206944\n",
            "Epoch: 055, Loss: 0.5786, Train: 0.8435, Test: 0.7369\n",
            "Early stopping:  0.020297790496700177\n",
            "Epoch: 056, Loss: 0.5674, Train: 0.8533, Test: 0.7386\n",
            "Early stopping:  0.019563887726434703\n",
            "Epoch: 057, Loss: 0.5586, Train: 0.8429, Test: 0.7428\n",
            "Early stopping:  0.018025117753708846\n",
            "Epoch: 058, Loss: 0.5518, Train: 0.8573, Test: 0.7440\n",
            "Early stopping:  0.01563182651643645\n",
            "Epoch: 059, Loss: 0.5361, Train: 0.8637, Test: 0.7479\n",
            "Early stopping:  0.016057199784489353\n",
            "Epoch: 060, Loss: 0.5204, Train: 0.8605, Test: 0.7479\n",
            "Early stopping:  0.01872057283324502\n",
            "Epoch: 061, Loss: 0.5141, Train: 0.8730, Test: 0.7485\n",
            "Early stopping:  0.0192689400633949\n",
            "Epoch: 062, Loss: 0.5056, Train: 0.8718, Test: 0.7505\n",
            "Early stopping:  0.018400362099522772\n",
            "Epoch: 063, Loss: 0.4904, Train: 0.8745, Test: 0.7496\n",
            "Early stopping:  0.016968206747021826\n",
            "Epoch: 064, Loss: 0.4810, Train: 0.8828, Test: 0.7516\n",
            "Early stopping:  0.016375915797739282\n",
            "Epoch: 065, Loss: 0.4743, Train: 0.8826, Test: 0.7550\n",
            "Early stopping:  0.01662047814384449\n",
            "Epoch: 066, Loss: 0.4625, Train: 0.8892, Test: 0.7528\n",
            "Early stopping:  0.016337472550474925\n",
            "Epoch: 067, Loss: 0.4518, Train: 0.8919, Test: 0.7516\n",
            "Early stopping:  0.015215295064197467\n",
            "Epoch: 068, Loss: 0.4451, Train: 0.8936, Test: 0.7542\n",
            "Early stopping:  0.014984168161868107\n",
            "Epoch: 069, Loss: 0.4354, Train: 0.8994, Test: 0.7556\n",
            "Early stopping:  0.015115141916387374\n",
            "Epoch: 070, Loss: 0.4238, Train: 0.9011, Test: 0.7519\n",
            "Early stopping:  0.014877148101416344\n",
            "Epoch: 071, Loss: 0.4167, Train: 0.9032, Test: 0.7553\n",
            "Early stopping:  0.014506252265717577\n",
            "Epoch: 072, Loss: 0.4096, Train: 0.9064, Test: 0.7545\n",
            "Early stopping:  0.014253527227127171\n",
            "Epoch: 073, Loss: 0.3988, Train: 0.9123, Test: 0.7562\n",
            "Early stopping:  0.01386488508990844\n",
            "Epoch: 074, Loss: 0.3898, Train: 0.9123, Test: 0.7542\n",
            "Early stopping:  0.013614840909237518\n",
            "Epoch: 075, Loss: 0.3833, Train: 0.9163, Test: 0.7562\n",
            "Early stopping:  0.013746879562103386\n",
            "Epoch: 076, Loss: 0.3752, Train: 0.9197, Test: 0.7567\n",
            "Early stopping:  0.013395132486511334\n",
            "Epoch: 077, Loss: 0.3666, Train: 0.9265, Test: 0.7582\n",
            "Early stopping:  0.012516205431262058\n",
            "Epoch: 078, Loss: 0.3602, Train: 0.9164, Test: 0.7556\n",
            "Early stopping:  0.012007518821836285\n",
            "Epoch: 079, Loss: 0.3589, Train: 0.9250, Test: 0.7533\n",
            "Early stopping:  0.010304535173164167\n",
            "Epoch: 080, Loss: 0.3600, Train: 0.9060, Test: 0.7513\n",
            "Early stopping:  0.006837538977227886\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.82      0.73      0.77       379\n",
            "         capital_goods       0.81      0.63      0.71       254\n",
            "conglomerates_industry       0.96      0.65      0.78        40\n",
            "     consumer_cyclical       0.73      0.68      0.70       396\n",
            " consumer_non-cyclical       0.77      0.68      0.72       223\n",
            "                energy       0.87      0.78      0.82       141\n",
            "             financial       0.84      0.75      0.79       384\n",
            "            healthcare       0.82      0.70      0.76       159\n",
            "              services       0.66      0.87      0.75      1038\n",
            "            technology       0.81      0.56      0.66       198\n",
            "        transportation       0.87      0.80      0.83       202\n",
            "             utilities       0.86      0.75      0.80       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.82      0.71      0.76      3527\n",
            "          weighted avg       0.77      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5084, Train: 0.2941, Test: 0.2937\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2911, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.15363099479360623\n",
            "Epoch: 003, Loss: 2.1635, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.17440102879186514\n",
            "Epoch: 004, Loss: 2.1416, Train: 0.2991, Test: 0.2986\n",
            "Early stopping:  0.16830967110272965\n",
            "Epoch: 005, Loss: 2.0911, Train: 0.3306, Test: 0.3255\n",
            "Early stopping:  0.1676213177391105\n",
            "Epoch: 006, Loss: 2.0267, Train: 0.3701, Test: 0.3646\n",
            "Early stopping:  0.0982241424484684\n",
            "Epoch: 007, Loss: 1.9674, Train: 0.3902, Test: 0.3805\n",
            "Early stopping:  0.08121051026100073\n",
            "Epoch: 008, Loss: 1.9095, Train: 0.3977, Test: 0.3873\n",
            "Early stopping:  0.09297462341188123\n",
            "Epoch: 009, Loss: 1.8531, Train: 0.4042, Test: 0.3930\n",
            "Early stopping:  0.0938013566309631\n",
            "Epoch: 010, Loss: 1.7993, Train: 0.4123, Test: 0.4037\n",
            "Early stopping:  0.08999003877342733\n",
            "Epoch: 011, Loss: 1.7465, Train: 0.4333, Test: 0.4239\n",
            "Early stopping:  0.08730420178326463\n",
            "Epoch: 012, Loss: 1.6913, Train: 0.4563, Test: 0.4491\n",
            "Early stopping:  0.08587273878656129\n",
            "Epoch: 013, Loss: 1.6347, Train: 0.4854, Test: 0.4746\n",
            "Early stopping:  0.08615669908917817\n",
            "Epoch: 014, Loss: 1.5817, Train: 0.5036, Test: 0.4894\n",
            "Early stopping:  0.08649981800496329\n",
            "Epoch: 015, Loss: 1.5318, Train: 0.5225, Test: 0.5069\n",
            "Early stopping:  0.08523711384018463\n",
            "Epoch: 016, Loss: 1.4801, Train: 0.5310, Test: 0.5223\n",
            "Early stopping:  0.08308037198344709\n",
            "Epoch: 017, Loss: 1.4305, Train: 0.5431, Test: 0.5322\n",
            "Early stopping:  0.08065003772514617\n",
            "Epoch: 018, Loss: 1.3866, Train: 0.5558, Test: 0.5449\n",
            "Early stopping:  0.07776406569204955\n",
            "Epoch: 019, Loss: 1.3435, Train: 0.5767, Test: 0.5557\n",
            "Early stopping:  0.07440803835183767\n",
            "Epoch: 020, Loss: 1.3016, Train: 0.5940, Test: 0.5673\n",
            "Early stopping:  0.0702342756258324\n",
            "Epoch: 021, Loss: 1.2627, Train: 0.6095, Test: 0.5764\n",
            "Early stopping:  0.06648741033628101\n",
            "Epoch: 022, Loss: 1.2264, Train: 0.6210, Test: 0.5943\n",
            "Early stopping:  0.06345520514452387\n",
            "Epoch: 023, Loss: 1.1958, Train: 0.6291, Test: 0.6022\n",
            "Early stopping:  0.058684640414304726\n",
            "Epoch: 024, Loss: 1.1659, Train: 0.6389, Test: 0.6065\n",
            "Early stopping:  0.05358719917557808\n",
            "Epoch: 025, Loss: 1.1352, Train: 0.6491, Test: 0.6124\n",
            "Early stopping:  0.04992432289275453\n",
            "Epoch: 026, Loss: 1.1088, Train: 0.6571, Test: 0.6209\n",
            "Early stopping:  0.04678241748516445\n",
            "Epoch: 027, Loss: 1.0824, Train: 0.6597, Test: 0.6246\n",
            "Early stopping:  0.0449379552749965\n",
            "Epoch: 028, Loss: 1.0578, Train: 0.6665, Test: 0.6289\n",
            "Early stopping:  0.04256407629808733\n",
            "Epoch: 029, Loss: 1.0335, Train: 0.6771, Test: 0.6343\n",
            "Early stopping:  0.040243214308435506\n",
            "Epoch: 030, Loss: 1.0099, Train: 0.6887, Test: 0.6402\n",
            "Early stopping:  0.0390046179452522\n",
            "Epoch: 031, Loss: 0.9882, Train: 0.6964, Test: 0.6473\n",
            "Early stopping:  0.03737345807738644\n",
            "Epoch: 032, Loss: 0.9661, Train: 0.7026, Test: 0.6507\n",
            "Early stopping:  0.03619376830688581\n",
            "Epoch: 033, Loss: 0.9463, Train: 0.7163, Test: 0.6552\n",
            "Early stopping:  0.034517686795387796\n",
            "Epoch: 034, Loss: 0.9259, Train: 0.7238, Test: 0.6578\n",
            "Early stopping:  0.03320414965391913\n",
            "Epoch: 035, Loss: 0.9066, Train: 0.7301, Test: 0.6609\n",
            "Early stopping:  0.03216430210024651\n",
            "Epoch: 036, Loss: 0.8880, Train: 0.7342, Test: 0.6657\n",
            "Early stopping:  0.030978210625168263\n",
            "Epoch: 037, Loss: 0.8709, Train: 0.7440, Test: 0.6717\n",
            "Early stopping:  0.02984083088140526\n",
            "Epoch: 038, Loss: 0.8535, Train: 0.7529, Test: 0.6748\n",
            "Early stopping:  0.028526929828591275\n",
            "Epoch: 039, Loss: 0.8369, Train: 0.7586, Test: 0.6810\n",
            "Early stopping:  0.027479209268700285\n",
            "Epoch: 040, Loss: 0.8208, Train: 0.7635, Test: 0.6842\n",
            "Early stopping:  0.02664307749498642\n",
            "Epoch: 041, Loss: 0.8048, Train: 0.7705, Test: 0.6915\n",
            "Early stopping:  0.02610541357084779\n",
            "Epoch: 042, Loss: 0.7889, Train: 0.7773, Test: 0.6924\n",
            "Early stopping:  0.025530080208358646\n",
            "Epoch: 043, Loss: 0.7733, Train: 0.7807, Test: 0.6963\n",
            "Early stopping:  0.025150306519829232\n",
            "Epoch: 044, Loss: 0.7579, Train: 0.7877, Test: 0.7009\n",
            "Early stopping:  0.024853432427691835\n",
            "Epoch: 045, Loss: 0.7426, Train: 0.7936, Test: 0.7037\n",
            "Early stopping:  0.02458018407610736\n",
            "Epoch: 046, Loss: 0.7276, Train: 0.7968, Test: 0.7051\n",
            "Early stopping:  0.024260517680001956\n",
            "Epoch: 047, Loss: 0.7130, Train: 0.7985, Test: 0.7057\n",
            "Early stopping:  0.023867539094203614\n",
            "Epoch: 048, Loss: 0.6986, Train: 0.8045, Test: 0.7094\n",
            "Early stopping:  0.02342441589830381\n",
            "Epoch: 049, Loss: 0.6845, Train: 0.8079, Test: 0.7117\n",
            "Early stopping:  0.022950075126423955\n",
            "Epoch: 050, Loss: 0.6708, Train: 0.8119, Test: 0.7156\n",
            "Early stopping:  0.02246288799723203\n",
            "Epoch: 051, Loss: 0.6572, Train: 0.8191, Test: 0.7187\n",
            "Early stopping:  0.02204158598294918\n",
            "Epoch: 052, Loss: 0.6441, Train: 0.8227, Test: 0.7219\n",
            "Early stopping:  0.02154236231614998\n",
            "Epoch: 053, Loss: 0.6311, Train: 0.8289, Test: 0.7272\n",
            "Early stopping:  0.021114309646998206\n",
            "Epoch: 054, Loss: 0.6183, Train: 0.8335, Test: 0.7275\n",
            "Early stopping:  0.02073138173000824\n",
            "Epoch: 055, Loss: 0.6057, Train: 0.8378, Test: 0.7312\n",
            "Early stopping:  0.020363491758401606\n",
            "Epoch: 056, Loss: 0.5932, Train: 0.8414, Test: 0.7301\n",
            "Early stopping:  0.02011157662432909\n",
            "Epoch: 057, Loss: 0.5808, Train: 0.8463, Test: 0.7346\n",
            "Early stopping:  0.019877729462965094\n",
            "Epoch: 058, Loss: 0.5685, Train: 0.8495, Test: 0.7375\n",
            "Early stopping:  0.01968656717800418\n",
            "Epoch: 059, Loss: 0.5566, Train: 0.8552, Test: 0.7366\n",
            "Early stopping:  0.01941964434974823\n",
            "Epoch: 060, Loss: 0.5452, Train: 0.8499, Test: 0.7366\n",
            "Early stopping:  0.018983348301812506\n",
            "Epoch: 061, Loss: 0.5360, Train: 0.8614, Test: 0.7358\n",
            "Early stopping:  0.017864703791886944\n",
            "Epoch: 062, Loss: 0.5303, Train: 0.8541, Test: 0.7392\n",
            "Early stopping:  0.015477963688046048\n",
            "Epoch: 063, Loss: 0.5222, Train: 0.8698, Test: 0.7428\n",
            "Early stopping:  0.013352799005510244\n",
            "Epoch: 064, Loss: 0.5044, Train: 0.8762, Test: 0.7443\n",
            "Early stopping:  0.015457598708263817\n",
            "Epoch: 065, Loss: 0.4915, Train: 0.8686, Test: 0.7414\n",
            "Early stopping:  0.018521971055469006\n",
            "Epoch: 066, Loss: 0.4876, Train: 0.8836, Test: 0.7448\n",
            "Early stopping:  0.018658661398050433\n",
            "Epoch: 067, Loss: 0.4764, Train: 0.8870, Test: 0.7474\n",
            "Early stopping:  0.017522092900517792\n",
            "Epoch: 068, Loss: 0.4630, Train: 0.8830, Test: 0.7465\n",
            "Early stopping:  0.015673715570071682\n",
            "Epoch: 069, Loss: 0.4580, Train: 0.8958, Test: 0.7471\n",
            "Early stopping:  0.014730343657119267\n",
            "Epoch: 070, Loss: 0.4480, Train: 0.8964, Test: 0.7479\n",
            "Early stopping:  0.015604674020118158\n",
            "Epoch: 071, Loss: 0.4366, Train: 0.8947, Test: 0.7511\n",
            "Early stopping:  0.015089800467300185\n",
            "Epoch: 072, Loss: 0.4318, Train: 0.9036, Test: 0.7471\n",
            "Early stopping:  0.01339486941290257\n",
            "Epoch: 073, Loss: 0.4205, Train: 0.9034, Test: 0.7494\n",
            "Early stopping:  0.014482704512757825\n",
            "Epoch: 074, Loss: 0.4110, Train: 0.9032, Test: 0.7522\n",
            "Early stopping:  0.01432040745729111\n",
            "Epoch: 075, Loss: 0.4071, Train: 0.9098, Test: 0.7508\n",
            "Early stopping:  0.012762962957770164\n",
            "Epoch: 076, Loss: 0.3955, Train: 0.9115, Test: 0.7494\n",
            "Early stopping:  0.013734864949526589\n",
            "Epoch: 077, Loss: 0.3869, Train: 0.9113, Test: 0.7539\n",
            "Early stopping:  0.013201728044355739\n",
            "Epoch: 078, Loss: 0.3820, Train: 0.9157, Test: 0.7522\n",
            "Early stopping:  0.012518579697431642\n",
            "Epoch: 079, Loss: 0.3720, Train: 0.9195, Test: 0.7516\n",
            "Early stopping:  0.013365934582766893\n",
            "Epoch: 080, Loss: 0.3648, Train: 0.9185, Test: 0.7528\n",
            "Early stopping:  0.012093951648232784\n",
            "Epoch: 081, Loss: 0.3597, Train: 0.9263, Test: 0.7539\n",
            "Early stopping:  0.011361700576028429\n",
            "Epoch: 082, Loss: 0.3518, Train: 0.9229, Test: 0.7519\n",
            "Early stopping:  0.011529692815728073\n",
            "Epoch: 083, Loss: 0.3476, Train: 0.9259, Test: 0.7539\n",
            "Early stopping:  0.009799831058853212\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.74      0.79      0.76       379\n",
            "         capital_goods       0.65      0.69      0.67       254\n",
            "conglomerates_industry       0.96      0.65      0.78        40\n",
            "     consumer_cyclical       0.66      0.71      0.69       396\n",
            " consumer_non-cyclical       0.77      0.64      0.70       223\n",
            "                energy       0.87      0.80      0.83       141\n",
            "             financial       0.88      0.77      0.82       384\n",
            "            healthcare       0.77      0.79      0.78       159\n",
            "              services       0.74      0.78      0.76      1038\n",
            "            technology       0.69      0.68      0.68       198\n",
            "        transportation       0.86      0.83      0.85       202\n",
            "             utilities       0.84      0.76      0.80       113\n",
            "\n",
            "              accuracy                           0.75      3527\n",
            "             macro avg       0.79      0.74      0.76      3527\n",
            "          weighted avg       0.76      0.75      0.75      3527\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5151, Train: 0.2941, Test: 0.2943\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2801, Train: 0.2940, Test: 0.2937\n",
            "Early stopping:  0.16611168816204622\n",
            "Epoch: 003, Loss: 2.1718, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.17547812674619254\n",
            "Epoch: 004, Loss: 2.1587, Train: 0.2943, Test: 0.2943\n",
            "Early stopping:  0.1649865707212395\n",
            "Epoch: 005, Loss: 2.0818, Train: 0.3013, Test: 0.3020\n",
            "Early stopping:  0.16847263459403544\n",
            "Epoch: 006, Loss: 2.0048, Train: 0.3510, Test: 0.3428\n",
            "Early stopping:  0.10328714316593475\n",
            "Epoch: 007, Loss: 1.9589, Train: 0.3941, Test: 0.3833\n",
            "Early stopping:  0.09331862785289567\n",
            "Epoch: 008, Loss: 1.9125, Train: 0.4183, Test: 0.4046\n",
            "Early stopping:  0.09813489405242846\n",
            "Epoch: 009, Loss: 1.8498, Train: 0.4236, Test: 0.4105\n",
            "Early stopping:  0.08836112827953889\n",
            "Epoch: 010, Loss: 1.7799, Train: 0.4251, Test: 0.4091\n",
            "Early stopping:  0.08878390630612575\n",
            "Epoch: 011, Loss: 1.7179, Train: 0.4361, Test: 0.4230\n",
            "Early stopping:  0.09738356692213483\n",
            "Epoch: 012, Loss: 1.6639, Train: 0.4561, Test: 0.4505\n",
            "Early stopping:  0.09956290566751447\n",
            "Epoch: 013, Loss: 1.6092, Train: 0.4779, Test: 0.4687\n",
            "Early stopping:  0.0945952289589531\n",
            "Epoch: 014, Loss: 1.5529, Train: 0.5043, Test: 0.4933\n",
            "Early stopping:  0.08903093383371391\n",
            "Epoch: 015, Loss: 1.4974, Train: 0.5335, Test: 0.5160\n",
            "Early stopping:  0.08731510929231232\n",
            "Epoch: 016, Loss: 1.4450, Train: 0.5590, Test: 0.5361\n",
            "Early stopping:  0.0869100567062753\n",
            "Epoch: 017, Loss: 1.3982, Train: 0.5715, Test: 0.5506\n",
            "Early stopping:  0.08383456272234616\n",
            "Epoch: 018, Loss: 1.3557, Train: 0.5817, Test: 0.5546\n",
            "Early stopping:  0.07816258589788003\n",
            "Epoch: 019, Loss: 1.3131, Train: 0.5888, Test: 0.5614\n",
            "Early stopping:  0.07246617446017971\n",
            "Epoch: 020, Loss: 1.2714, Train: 0.5949, Test: 0.5668\n",
            "Early stopping:  0.06836158633826621\n",
            "Epoch: 021, Loss: 1.2355, Train: 0.6045, Test: 0.5739\n",
            "Early stopping:  0.06479970529345387\n",
            "Epoch: 022, Loss: 1.2053, Train: 0.6157, Test: 0.5838\n",
            "Early stopping:  0.05995684110421638\n",
            "Epoch: 023, Loss: 1.1760, Train: 0.6253, Test: 0.5963\n",
            "Early stopping:  0.05395766213636609\n",
            "Epoch: 024, Loss: 1.1482, Train: 0.6331, Test: 0.6028\n",
            "Early stopping:  0.04842433410910414\n",
            "Epoch: 025, Loss: 1.1235, Train: 0.6420, Test: 0.6110\n",
            "Early stopping:  0.04447525018068443\n",
            "Epoch: 026, Loss: 1.0997, Train: 0.6510, Test: 0.6223\n",
            "Early stopping:  0.04175821372265448\n",
            "Epoch: 027, Loss: 1.0764, Train: 0.6578, Test: 0.6277\n",
            "Early stopping:  0.03919273918887994\n",
            "Epoch: 028, Loss: 1.0536, Train: 0.6635, Test: 0.6297\n",
            "Early stopping:  0.03734770623763639\n",
            "Epoch: 029, Loss: 1.0311, Train: 0.6730, Test: 0.6360\n",
            "Early stopping:  0.03649920442847771\n",
            "Epoch: 030, Loss: 1.0094, Train: 0.6820, Test: 0.6436\n",
            "Early stopping:  0.03571835018353924\n",
            "Epoch: 031, Loss: 0.9884, Train: 0.6934, Test: 0.6496\n",
            "Early stopping:  0.03484853813188344\n",
            "Epoch: 032, Loss: 0.9678, Train: 0.7019, Test: 0.6552\n",
            "Early stopping:  0.03390125973613262\n",
            "Epoch: 033, Loss: 0.9478, Train: 0.7081, Test: 0.6598\n",
            "Early stopping:  0.03292517213814567\n",
            "Epoch: 034, Loss: 0.9291, Train: 0.7161, Test: 0.6657\n",
            "Early stopping:  0.03182720754772175\n",
            "Epoch: 035, Loss: 0.9111, Train: 0.7236, Test: 0.6683\n",
            "Early stopping:  0.03056695808930091\n",
            "Epoch: 036, Loss: 0.8928, Train: 0.7340, Test: 0.6725\n",
            "Early stopping:  0.029516811555087917\n",
            "Epoch: 037, Loss: 0.8759, Train: 0.7416, Test: 0.6756\n",
            "Early stopping:  0.028466542068004346\n",
            "Epoch: 038, Loss: 0.8592, Train: 0.7510, Test: 0.6793\n",
            "Early stopping:  0.027679062503528876\n",
            "Epoch: 039, Loss: 0.8421, Train: 0.7580, Test: 0.6788\n",
            "Early stopping:  0.02717011875862294\n",
            "Epoch: 040, Loss: 0.8258, Train: 0.7624, Test: 0.6833\n",
            "Early stopping:  0.026564246775050555\n",
            "Epoch: 041, Loss: 0.8098, Train: 0.7681, Test: 0.6859\n",
            "Early stopping:  0.026172550176148272\n",
            "Epoch: 042, Loss: 0.7935, Train: 0.7722, Test: 0.6955\n",
            "Early stopping:  0.025859611988632854\n",
            "Epoch: 043, Loss: 0.7782, Train: 0.7784, Test: 0.6978\n",
            "Early stopping:  0.025307160604452372\n",
            "Epoch: 044, Loss: 0.7628, Train: 0.7839, Test: 0.7017\n",
            "Early stopping:  0.024926509141628736\n",
            "Epoch: 045, Loss: 0.7471, Train: 0.7905, Test: 0.7051\n",
            "Early stopping:  0.024703495051515544\n",
            "Epoch: 046, Loss: 0.7322, Train: 0.7975, Test: 0.7094\n",
            "Early stopping:  0.02430663213875759\n",
            "Epoch: 047, Loss: 0.7172, Train: 0.7991, Test: 0.7128\n",
            "Early stopping:  0.02412526845860397\n",
            "Epoch: 048, Loss: 0.7023, Train: 0.8043, Test: 0.7162\n",
            "Early stopping:  0.023869095378066637\n",
            "Epoch: 049, Loss: 0.6879, Train: 0.8100, Test: 0.7176\n",
            "Early stopping:  0.02345796607225712\n",
            "Epoch: 050, Loss: 0.6732, Train: 0.8151, Test: 0.7176\n",
            "Early stopping:  0.02329315128713606\n",
            "Epoch: 051, Loss: 0.6589, Train: 0.8185, Test: 0.7216\n",
            "Early stopping:  0.02302619163491226\n",
            "Epoch: 052, Loss: 0.6448, Train: 0.8248, Test: 0.7253\n",
            "Early stopping:  0.022755036289236755\n",
            "Epoch: 053, Loss: 0.6307, Train: 0.8282, Test: 0.7238\n",
            "Early stopping:  0.022569366419613705\n",
            "Epoch: 054, Loss: 0.6169, Train: 0.8325, Test: 0.7275\n",
            "Early stopping:  0.022245322524901908\n",
            "Epoch: 055, Loss: 0.6030, Train: 0.8367, Test: 0.7278\n",
            "Early stopping:  0.02207885476357393\n",
            "Epoch: 056, Loss: 0.5896, Train: 0.8442, Test: 0.7315\n",
            "Early stopping:  0.021828067182234553\n",
            "Epoch: 057, Loss: 0.5765, Train: 0.8429, Test: 0.7324\n",
            "Early stopping:  0.021449346143415222\n",
            "Epoch: 058, Loss: 0.5661, Train: 0.8514, Test: 0.7355\n",
            "Early stopping:  0.02029202820613567\n",
            "Epoch: 059, Loss: 0.5589, Train: 0.8450, Test: 0.7304\n",
            "Early stopping:  0.017781575182601757\n",
            "Epoch: 060, Loss: 0.5512, Train: 0.8605, Test: 0.7369\n",
            "Early stopping:  0.015066851984317921\n",
            "Epoch: 061, Loss: 0.5293, Train: 0.8654, Test: 0.7400\n",
            "Early stopping:  0.017817383774489007\n",
            "Epoch: 062, Loss: 0.5194, Train: 0.8571, Test: 0.7369\n",
            "Early stopping:  0.019877270555208416\n",
            "Epoch: 063, Loss: 0.5149, Train: 0.8716, Test: 0.7420\n",
            "Early stopping:  0.019463487533425684\n",
            "Epoch: 064, Loss: 0.4966, Train: 0.8788, Test: 0.7437\n",
            "Early stopping:  0.020025802115267523\n",
            "Epoch: 065, Loss: 0.4880, Train: 0.8724, Test: 0.7431\n",
            "Early stopping:  0.016921763762972632\n",
            "Epoch: 066, Loss: 0.4818, Train: 0.8820, Test: 0.7443\n",
            "Early stopping:  0.016472418435446008\n",
            "Epoch: 067, Loss: 0.4659, Train: 0.8875, Test: 0.7471\n",
            "Early stopping:  0.018113339085952456\n",
            "Epoch: 068, Loss: 0.4593, Train: 0.8870, Test: 0.7465\n",
            "Early stopping:  0.015460293319789185\n",
            "Epoch: 069, Loss: 0.4515, Train: 0.8909, Test: 0.7485\n",
            "Early stopping:  0.015276585445225544\n",
            "Epoch: 070, Loss: 0.4376, Train: 0.8970, Test: 0.7508\n",
            "Early stopping:  0.016425195172353628\n",
            "Epoch: 071, Loss: 0.4321, Train: 0.8972, Test: 0.7479\n",
            "Early stopping:  0.014266285285720576\n",
            "Epoch: 072, Loss: 0.4234, Train: 0.9002, Test: 0.7491\n",
            "Early stopping:  0.014533210471740828\n",
            "Epoch: 073, Loss: 0.4115, Train: 0.9070, Test: 0.7502\n",
            "Early stopping:  0.015012227796957513\n",
            "Epoch: 074, Loss: 0.4053, Train: 0.9072, Test: 0.7508\n",
            "Early stopping:  0.013575674665654038\n",
            "Epoch: 075, Loss: 0.3974, Train: 0.9104, Test: 0.7542\n",
            "Early stopping:  0.013906316079083405\n",
            "Epoch: 076, Loss: 0.3869, Train: 0.9183, Test: 0.7525\n",
            "Early stopping:  0.013846646487408685\n",
            "Epoch: 077, Loss: 0.3797, Train: 0.9161, Test: 0.7528\n",
            "Early stopping:  0.013014897182167941\n",
            "Epoch: 078, Loss: 0.3725, Train: 0.9168, Test: 0.7565\n",
            "Early stopping:  0.013194840336878826\n",
            "Epoch: 079, Loss: 0.3641, Train: 0.9272, Test: 0.7533\n",
            "Early stopping:  0.012827033570038511\n",
            "Epoch: 080, Loss: 0.3557, Train: 0.9257, Test: 0.7525\n",
            "Early stopping:  0.012333446719444535\n",
            "Epoch: 081, Loss: 0.3482, Train: 0.9250, Test: 0.7556\n",
            "Early stopping:  0.012605392533134526\n",
            "Epoch: 082, Loss: 0.3426, Train: 0.9314, Test: 0.7505\n",
            "Early stopping:  0.01198835700701083\n",
            "Epoch: 083, Loss: 0.3344, Train: 0.9338, Test: 0.7545\n",
            "Early stopping:  0.011491539381557378\n",
            "Epoch: 084, Loss: 0.3255, Train: 0.9325, Test: 0.7550\n",
            "Early stopping:  0.011790286952094314\n",
            "Epoch: 085, Loss: 0.3204, Train: 0.9388, Test: 0.7519\n",
            "Early stopping:  0.011553303988579412\n",
            "Epoch: 086, Loss: 0.3145, Train: 0.9425, Test: 0.7556\n",
            "Early stopping:  0.011176681452535808\n",
            "Epoch: 087, Loss: 0.3066, Train: 0.9427, Test: 0.7547\n",
            "Early stopping:  0.010560538288230493\n",
            "Epoch: 088, Loss: 0.2993, Train: 0.9469, Test: 0.7562\n",
            "Early stopping:  0.01048979856712743\n",
            "Epoch: 089, Loss: 0.2937, Train: 0.9484, Test: 0.7565\n",
            "Early stopping:  0.010875716101860894\n",
            "Epoch: 090, Loss: 0.2874, Train: 0.9512, Test: 0.7539\n",
            "Early stopping:  0.010626954783383751\n",
            "Epoch: 091, Loss: 0.2814, Train: 0.9520, Test: 0.7556\n",
            "Early stopping:  0.009852364520324794\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.79      0.77      0.78       379\n",
            "         capital_goods       0.73      0.61      0.67       254\n",
            "conglomerates_industry       1.00      0.62      0.77        40\n",
            "     consumer_cyclical       0.69      0.76      0.72       396\n",
            " consumer_non-cyclical       0.75      0.69      0.72       223\n",
            "                energy       0.87      0.78      0.82       141\n",
            "             financial       0.80      0.78      0.79       384\n",
            "            healthcare       0.82      0.75      0.78       159\n",
            "              services       0.72      0.80      0.76      1038\n",
            "            technology       0.70      0.64      0.67       198\n",
            "        transportation       0.83      0.80      0.81       202\n",
            "             utilities       0.85      0.82      0.83       113\n",
            "\n",
            "              accuracy                           0.76      3527\n",
            "             macro avg       0.80      0.74      0.76      3527\n",
            "          weighted avg       0.76      0.76      0.76      3527\n",
            "\n",
            "time: 2min 26s (started: 2024-10-16 21:47:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ibd8tK-gWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2817256-1796-4652-d15f-7185830e02d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 451 ms (started: 2024-10-16 21:50:07 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F870Qs-hgWjM"
      },
      "source": [
        "### Training rotulated base = 80% ❎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG6XcGFCgWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a1fe7a-fb0e-4259-f933-43174f55c30a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 367 µs (started: 2024-10-16 21:50:08 +00:00)\n"
          ]
        }
      ],
      "source": [
        "rotulated_perc = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMMy3xS2gWjM"
      },
      "source": [
        "#### GCN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhev7oh-gWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf00c9e8-4b5e-4563-8cd4-93f3ec9d226d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 66.5811, Train: 0.1131, Test: 0.1128\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 430.3510, Train: 0.2802, Test: 0.2806\n",
            "Early stopping:  257.2241343121809\n",
            "Epoch: 003, Loss: 396.7217, Train: 0.0984, Test: 0.0935\n",
            "Early stopping:  201.01917972208162\n",
            "Epoch: 004, Loss: 661.6273, Train: 0.0332, Test: 0.0340\n",
            "Early stopping:  244.9823035336127\n",
            "Epoch: 005, Loss: 634.6266, Train: 0.0461, Test: 0.0454\n",
            "Early stopping:  238.94851695423657\n",
            "Epoch: 006, Loss: 577.0079, Train: 0.0740, Test: 0.0731\n",
            "Early stopping:  120.0713565643617\n",
            "Epoch: 007, Loss: 425.5712, Train: 0.2970, Test: 0.2931\n",
            "Early stopping:  121.17718099745412\n",
            "Epoch: 008, Loss: 303.9084, Train: 0.2919, Test: 0.2891\n",
            "Early stopping:  151.70556820587274\n",
            "Epoch: 009, Loss: 266.2439, Train: 0.1992, Test: 0.1859\n",
            "Early stopping:  162.45010108779596\n",
            "Epoch: 010, Loss: 216.3740, Train: 0.0963, Test: 0.0873\n",
            "Early stopping:  144.85610480631155\n",
            "Epoch: 011, Loss: 186.5578, Train: 0.0454, Test: 0.0402\n",
            "Early stopping:  93.18151045307552\n",
            "Epoch: 012, Loss: 168.5562, Train: 0.0603, Test: 0.0516\n",
            "Early stopping:  56.147858970311525\n",
            "Epoch: 013, Loss: 126.1381, Train: 0.0818, Test: 0.0777\n",
            "Early stopping:  52.48874417986081\n",
            "Epoch: 014, Loss: 79.3915, Train: 0.1886, Test: 0.1859\n",
            "Early stopping:  53.608023256402745\n",
            "Epoch: 015, Loss: 41.4229, Train: 0.2989, Test: 0.2840\n",
            "Early stopping:  60.51421772821314\n",
            "Epoch: 016, Loss: 21.6355, Train: 0.2551, Test: 0.2398\n",
            "Early stopping:  60.39446972868362\n",
            "Epoch: 017, Loss: 12.2575, Train: 0.2882, Test: 0.2795\n",
            "Early stopping:  46.83262485821904\n",
            "Epoch: 018, Loss: 5.2543, Train: 0.2623, Test: 0.2602\n",
            "Early stopping:  29.779450703527814\n",
            "Epoch: 019, Loss: 3.8247, Train: 0.2361, Test: 0.2387\n",
            "Early stopping:  15.426695227077625\n",
            "Epoch: 020, Loss: 3.4579, Train: 0.2347, Test: 0.2336\n",
            "Early stopping:  7.768324985878414\n",
            "Epoch: 021, Loss: 3.1405, Train: 0.2426, Test: 0.2443\n",
            "Early stopping:  3.8154625264516295\n",
            "Epoch: 022, Loss: 2.8306, Train: 0.2664, Test: 0.2608\n",
            "Early stopping:  0.9432595398578417\n",
            "Epoch: 023, Loss: 2.5762, Train: 0.2955, Test: 0.2868\n",
            "Early stopping:  0.49501918979609794\n",
            "Epoch: 024, Loss: 2.4066, Train: 0.3173, Test: 0.3050\n",
            "Early stopping:  0.42443618288116414\n",
            "Epoch: 025, Loss: 2.3465, Train: 0.3149, Test: 0.3050\n",
            "Early stopping:  0.3276502908080656\n",
            "Epoch: 026, Loss: 2.2911, Train: 0.3231, Test: 0.3107\n",
            "Early stopping:  0.2182336739917991\n",
            "Epoch: 027, Loss: 2.2505, Train: 0.3315, Test: 0.3175\n",
            "Early stopping:  0.1273085759873948\n",
            "Epoch: 028, Loss: 2.2196, Train: 0.3346, Test: 0.3180\n",
            "Early stopping:  0.07498139250668316\n",
            "Epoch: 029, Loss: 2.1909, Train: 0.3393, Test: 0.3220\n",
            "Early stopping:  0.06113312654674453\n",
            "Epoch: 030, Loss: 2.1663, Train: 0.3433, Test: 0.3265\n",
            "Early stopping:  0.04909455689437727\n",
            "Epoch: 031, Loss: 2.1475, Train: 0.3474, Test: 0.3390\n",
            "Early stopping:  0.041160427865461896\n",
            "Epoch: 032, Loss: 2.1303, Train: 0.3532, Test: 0.3401\n",
            "Early stopping:  0.03532423223614366\n",
            "Epoch: 033, Loss: 2.1139, Train: 0.3555, Test: 0.3362\n",
            "Early stopping:  0.03017613240906731\n",
            "Epoch: 034, Loss: 2.0982, Train: 0.3573, Test: 0.3339\n",
            "Early stopping:  0.02688923104961706\n",
            "Epoch: 035, Loss: 2.0834, Train: 0.3603, Test: 0.3396\n",
            "Early stopping:  0.025365520005216687\n",
            "Epoch: 036, Loss: 2.0663, Train: 0.3614, Test: 0.3413\n",
            "Early stopping:  0.02505007561861092\n",
            "Epoch: 037, Loss: 2.0462, Train: 0.3642, Test: 0.3481\n",
            "Early stopping:  0.026492663253024667\n",
            "Epoch: 038, Loss: 2.0296, Train: 0.3672, Test: 0.3526\n",
            "Early stopping:  0.027578248500981277\n",
            "Epoch: 039, Loss: 2.0118, Train: 0.3686, Test: 0.3503\n",
            "Early stopping:  0.028433759516736008\n",
            "Epoch: 040, Loss: 1.9914, Train: 0.3692, Test: 0.3515\n",
            "Early stopping:  0.029155217196946033\n",
            "Epoch: 041, Loss: 1.9742, Train: 0.3702, Test: 0.3554\n",
            "Early stopping:  0.0288434080654035\n",
            "Epoch: 042, Loss: 1.9562, Train: 0.3752, Test: 0.3622\n",
            "Early stopping:  0.029205168283098652\n",
            "Epoch: 043, Loss: 1.9399, Train: 0.3759, Test: 0.3690\n",
            "Early stopping:  0.028336110049410976\n",
            "Epoch: 044, Loss: 1.9266, Train: 0.3774, Test: 0.3690\n",
            "Early stopping:  0.025947368184534247\n",
            "Epoch: 045, Loss: 1.9124, Train: 0.3784, Test: 0.3679\n",
            "Early stopping:  0.024265258581819455\n",
            "Epoch: 046, Loss: 1.8977, Train: 0.3810, Test: 0.3673\n",
            "Early stopping:  0.022842009877689536\n",
            "Epoch: 047, Loss: 1.8827, Train: 0.3881, Test: 0.3747\n",
            "Early stopping:  0.02266917809592783\n",
            "Epoch: 048, Loss: 1.8664, Train: 0.3954, Test: 0.3815\n",
            "Early stopping:  0.023736052178072103\n",
            "Epoch: 049, Loss: 1.8523, Train: 0.4024, Test: 0.3844\n",
            "Early stopping:  0.023942473360214975\n",
            "Epoch: 050, Loss: 1.8355, Train: 0.4069, Test: 0.3878\n",
            "Early stopping:  0.024467903264570927\n",
            "Epoch: 051, Loss: 1.8210, Train: 0.4088, Test: 0.3917\n",
            "Early stopping:  0.024397469479238678\n",
            "Epoch: 052, Loss: 1.8070, Train: 0.3994, Test: 0.3793\n",
            "Early stopping:  0.023753040016035978\n",
            "Epoch: 053, Loss: 1.7837, Train: 0.3978, Test: 0.3770\n",
            "Early stopping:  0.026324225695413823\n",
            "Epoch: 054, Loss: 1.7532, Train: 0.4232, Test: 0.4059\n",
            "Early stopping:  0.03241512753563738\n",
            "Epoch: 055, Loss: 1.7296, Train: 0.4350, Test: 0.4201\n",
            "Early stopping:  0.03764306596729471\n",
            "Epoch: 056, Loss: 1.7101, Train: 0.4426, Test: 0.4325\n",
            "Early stopping:  0.039298359982927356\n",
            "Epoch: 057, Loss: 1.6905, Train: 0.4550, Test: 0.4427\n",
            "Early stopping:  0.03648199495684136\n",
            "Epoch: 058, Loss: 1.6694, Train: 0.4607, Test: 0.4490\n",
            "Early stopping:  0.0327181542723994\n",
            "Epoch: 059, Loss: 1.6522, Train: 0.4611, Test: 0.4541\n",
            "Early stopping:  0.030925713020026578\n",
            "Epoch: 060, Loss: 1.6340, Train: 0.4683, Test: 0.4626\n",
            "Early stopping:  0.030150585052563173\n",
            "Epoch: 061, Loss: 1.6129, Train: 0.4720, Test: 0.4671\n",
            "Early stopping:  0.030155599040359117\n",
            "Epoch: 062, Loss: 1.5980, Train: 0.4758, Test: 0.4734\n",
            "Early stopping:  0.028828064345373296\n",
            "Epoch: 063, Loss: 1.5812, Train: 0.4799, Test: 0.4768\n",
            "Early stopping:  0.028201543886185076\n",
            "Epoch: 064, Loss: 1.5674, Train: 0.4869, Test: 0.4739\n",
            "Early stopping:  0.026152466128061068\n",
            "Epoch: 065, Loss: 1.5542, Train: 0.4901, Test: 0.4751\n",
            "Early stopping:  0.023447588686169616\n",
            "Epoch: 066, Loss: 1.5395, Train: 0.4937, Test: 0.4751\n",
            "Early stopping:  0.022792471772380567\n",
            "Epoch: 067, Loss: 1.5261, Train: 0.4975, Test: 0.4802\n",
            "Early stopping:  0.02183016748090498\n",
            "Epoch: 068, Loss: 1.5108, Train: 0.5032, Test: 0.4847\n",
            "Early stopping:  0.022320871590439266\n",
            "Epoch: 069, Loss: 1.4965, Train: 0.5094, Test: 0.4892\n",
            "Early stopping:  0.022761264308053634\n",
            "Epoch: 070, Loss: 1.4839, Train: 0.5124, Test: 0.4892\n",
            "Early stopping:  0.022253474220415535\n",
            "Epoch: 071, Loss: 1.4702, Train: 0.5209, Test: 0.4949\n",
            "Early stopping:  0.02193351467604514\n",
            "Epoch: 072, Loss: 1.4581, Train: 0.5222, Test: 0.4989\n",
            "Early stopping:  0.02084524178692289\n",
            "Epoch: 073, Loss: 1.4459, Train: 0.5269, Test: 0.4977\n",
            "Early stopping:  0.020114146462264205\n",
            "Epoch: 074, Loss: 1.4340, Train: 0.5328, Test: 0.5096\n",
            "Early stopping:  0.019653631373477507\n",
            "Epoch: 075, Loss: 1.4231, Train: 0.5368, Test: 0.4994\n",
            "Early stopping:  0.018719445526547432\n",
            "Epoch: 076, Loss: 1.4106, Train: 0.5403, Test: 0.5096\n",
            "Early stopping:  0.01862178225568267\n",
            "Epoch: 077, Loss: 1.3987, Train: 0.5443, Test: 0.5142\n",
            "Early stopping:  0.018607149113611214\n",
            "Epoch: 078, Loss: 1.3853, Train: 0.5520, Test: 0.5176\n",
            "Early stopping:  0.019245107010392157\n",
            "Epoch: 079, Loss: 1.3740, Train: 0.5527, Test: 0.5295\n",
            "Early stopping:  0.019507377513167876\n",
            "Epoch: 080, Loss: 1.3642, Train: 0.5549, Test: 0.5244\n",
            "Early stopping:  0.018605819709168702\n",
            "Epoch: 081, Loss: 1.3594, Train: 0.5616, Test: 0.5323\n",
            "Early stopping:  0.01598395976357255\n",
            "Epoch: 082, Loss: 1.3484, Train: 0.5695, Test: 0.5306\n",
            "Early stopping:  0.014085689560460957\n",
            "Epoch: 083, Loss: 1.3336, Train: 0.5686, Test: 0.5261\n",
            "Early stopping:  0.015511608267183195\n",
            "Epoch: 084, Loss: 1.3243, Train: 0.5718, Test: 0.5357\n",
            "Early stopping:  0.01688402044453348\n",
            "Epoch: 085, Loss: 1.3118, Train: 0.5764, Test: 0.5346\n",
            "Early stopping:  0.018896289466191218\n",
            "Epoch: 086, Loss: 1.3033, Train: 0.5815, Test: 0.5385\n",
            "Early stopping:  0.01777598962408949\n",
            "Epoch: 087, Loss: 1.2956, Train: 0.5861, Test: 0.5465\n",
            "Early stopping:  0.01539477970756487\n",
            "Epoch: 088, Loss: 1.2860, Train: 0.5851, Test: 0.5510\n",
            "Early stopping:  0.014742184777583461\n",
            "Epoch: 089, Loss: 1.2805, Train: 0.5891, Test: 0.5567\n",
            "Early stopping:  0.012684528852039113\n",
            "Epoch: 090, Loss: 1.2709, Train: 0.5948, Test: 0.5471\n",
            "Early stopping:  0.01267157942643599\n",
            "Epoch: 091, Loss: 1.2666, Train: 0.5975, Test: 0.5556\n",
            "Early stopping:  0.011629615897309327\n",
            "Epoch: 092, Loss: 1.2582, Train: 0.5966, Test: 0.5601\n",
            "Early stopping:  0.01103207459984829\n",
            "Epoch: 093, Loss: 1.2527, Train: 0.5908, Test: 0.5539\n",
            "Early stopping:  0.01087345033515647\n",
            "Epoch: 094, Loss: 1.2652, Train: 0.5983, Test: 0.5629\n",
            "Early stopping:  0.007235923324860121\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.47      0.60      0.53       190\n",
            "         capital_goods       0.52      0.31      0.39       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.54      0.49      0.52       198\n",
            " consumer_non-cyclical       0.62      0.42      0.50       112\n",
            "                energy       0.68      0.32      0.44        71\n",
            "             financial       0.68      0.62      0.65       192\n",
            "            healthcare       0.73      0.58      0.65        79\n",
            "              services       0.54      0.76      0.63       519\n",
            "            technology       0.47      0.27      0.34        99\n",
            "        transportation       0.67      0.60      0.64       101\n",
            "             utilities       0.70      0.41      0.52        56\n",
            "\n",
            "              accuracy                           0.56      1764\n",
            "             macro avg       0.55      0.45      0.48      1764\n",
            "          weighted avg       0.57      0.56      0.55      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 77.4067, Train: 0.1019, Test: 0.1020\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 357.6922, Train: 0.0471, Test: 0.0522\n",
            "Early stopping:  198.19178281095006\n",
            "Epoch: 003, Loss: 474.3578, Train: 0.2837, Test: 0.2823\n",
            "Early stopping:  204.01839324394177\n",
            "Epoch: 004, Loss: 456.1940, Train: 0.2443, Test: 0.2438\n",
            "Early stopping:  183.31516172106396\n",
            "Epoch: 005, Loss: 351.5122, Train: 0.1008, Test: 0.1003\n",
            "Early stopping:  158.8198235819767\n",
            "Epoch: 006, Loss: 339.5762, Train: 0.1940, Test: 0.1956\n",
            "Early stopping:  64.01851699453388\n",
            "Epoch: 007, Loss: 248.2382, Train: 0.2105, Test: 0.2132\n",
            "Early stopping:  92.64757304069883\n",
            "Epoch: 008, Loss: 205.4154, Train: 0.2348, Test: 0.2455\n",
            "Early stopping:  97.72956749844586\n",
            "Epoch: 009, Loss: 137.1826, Train: 0.0807, Test: 0.0726\n",
            "Early stopping:  90.61377894866995\n",
            "Epoch: 010, Loss: 140.4584, Train: 0.0897, Test: 0.0833\n",
            "Early stopping:  84.14933268458549\n",
            "Epoch: 011, Loss: 106.7614, Train: 0.1629, Test: 0.1593\n",
            "Early stopping:  57.65357999908424\n",
            "Epoch: 012, Loss: 50.5031, Train: 0.3179, Test: 0.3084\n",
            "Early stopping:  56.32384296796913\n",
            "Epoch: 013, Loss: 34.6019, Train: 0.3046, Test: 0.2954\n",
            "Early stopping:  49.00521278046608\n",
            "Epoch: 014, Loss: 25.8922, Train: 0.2904, Test: 0.2766\n",
            "Early stopping:  49.701435533279565\n",
            "Epoch: 015, Loss: 17.2234, Train: 0.2217, Test: 0.2205\n",
            "Early stopping:  35.60052828357297\n",
            "Epoch: 016, Loss: 9.3899, Train: 0.1252, Test: 0.1230\n",
            "Early stopping:  15.935392606507833\n",
            "Epoch: 017, Loss: 5.0401, Train: 0.1174, Test: 0.1111\n",
            "Early stopping:  12.038300653243262\n",
            "Epoch: 018, Loss: 3.5632, Train: 0.1388, Test: 0.1293\n",
            "Early stopping:  9.306767496626106\n",
            "Epoch: 019, Loss: 3.0182, Train: 0.1520, Test: 0.1338\n",
            "Early stopping:  5.908534163541528\n",
            "Epoch: 020, Loss: 2.7193, Train: 0.1544, Test: 0.1457\n",
            "Early stopping:  2.7452019218852404\n",
            "Epoch: 021, Loss: 2.5265, Train: 0.1578, Test: 0.1587\n",
            "Early stopping:  1.0105861124626117\n",
            "Epoch: 022, Loss: 2.4231, Train: 0.1774, Test: 0.1791\n",
            "Early stopping:  0.45851463621504085\n",
            "Epoch: 023, Loss: 2.3526, Train: 0.1917, Test: 0.1978\n",
            "Early stopping:  0.26772811117403994\n",
            "Epoch: 024, Loss: 2.3186, Train: 0.2069, Test: 0.2018\n",
            "Early stopping:  0.1614134972181778\n",
            "Epoch: 025, Loss: 2.2873, Train: 0.2198, Test: 0.2160\n",
            "Early stopping:  0.0954372654754674\n",
            "Epoch: 026, Loss: 2.2481, Train: 0.2288, Test: 0.2171\n",
            "Early stopping:  0.06664280595035675\n",
            "Epoch: 027, Loss: 2.2219, Train: 0.2378, Test: 0.2285\n",
            "Early stopping:  0.05256116540285192\n",
            "Epoch: 028, Loss: 2.1824, Train: 0.2473, Test: 0.2341\n",
            "Early stopping:  0.053493054182027704\n",
            "Epoch: 029, Loss: 2.1418, Train: 0.2576, Test: 0.2460\n",
            "Early stopping:  0.05651715363295752\n",
            "Epoch: 030, Loss: 2.1122, Train: 0.2675, Test: 0.2574\n",
            "Early stopping:  0.055764581679402014\n",
            "Epoch: 031, Loss: 2.0862, Train: 0.2813, Test: 0.2670\n",
            "Early stopping:  0.05425115808787452\n",
            "Epoch: 032, Loss: 2.0676, Train: 0.3000, Test: 0.2851\n",
            "Early stopping:  0.04554911124059477\n",
            "Epoch: 033, Loss: 2.0576, Train: 0.3138, Test: 0.3039\n",
            "Early stopping:  0.03424927910962926\n",
            "Epoch: 034, Loss: 2.0314, Train: 0.3190, Test: 0.3163\n",
            "Early stopping:  0.030342105148008242\n",
            "Epoch: 035, Loss: 2.0006, Train: 0.3301, Test: 0.3260\n",
            "Early stopping:  0.03339667217417603\n",
            "Epoch: 036, Loss: 1.9753, Train: 0.3494, Test: 0.3418\n",
            "Early stopping:  0.03866820235996514\n",
            "Epoch: 037, Loss: 1.9510, Train: 0.3709, Test: 0.3583\n",
            "Early stopping:  0.04263217934353306\n",
            "Epoch: 038, Loss: 1.9295, Train: 0.3902, Test: 0.3673\n",
            "Early stopping:  0.04014597485000608\n",
            "Epoch: 039, Loss: 1.9122, Train: 0.4042, Test: 0.3815\n",
            "Early stopping:  0.035266840104911906\n",
            "Epoch: 040, Loss: 1.8920, Train: 0.4137, Test: 0.3957\n",
            "Early stopping:  0.03252822581629528\n",
            "Epoch: 041, Loss: 1.8704, Train: 0.4194, Test: 0.4002\n",
            "Early stopping:  0.03144894633359881\n",
            "Epoch: 042, Loss: 1.8532, Train: 0.4231, Test: 0.4070\n",
            "Early stopping:  0.03077679982054671\n",
            "Epoch: 043, Loss: 1.8389, Train: 0.4303, Test: 0.4201\n",
            "Early stopping:  0.029417268210881923\n",
            "Epoch: 044, Loss: 1.8220, Train: 0.4405, Test: 0.4274\n",
            "Early stopping:  0.027212058536286565\n",
            "Epoch: 045, Loss: 1.8050, Train: 0.4465, Test: 0.4342\n",
            "Early stopping:  0.025614222407129\n",
            "Epoch: 046, Loss: 1.7865, Train: 0.4519, Test: 0.4354\n",
            "Early stopping:  0.026483090771369705\n",
            "Epoch: 047, Loss: 1.7674, Train: 0.4541, Test: 0.4393\n",
            "Early stopping:  0.02824422344473465\n",
            "Epoch: 048, Loss: 1.7519, Train: 0.4578, Test: 0.4427\n",
            "Early stopping:  0.028120208003395278\n",
            "Epoch: 049, Loss: 1.7334, Train: 0.4595, Test: 0.4439\n",
            "Early stopping:  0.02813713961323816\n",
            "Epoch: 050, Loss: 1.7152, Train: 0.4585, Test: 0.4473\n",
            "Early stopping:  0.027925992135110746\n",
            "Epoch: 051, Loss: 1.7043, Train: 0.4612, Test: 0.4456\n",
            "Early stopping:  0.025823488885184504\n",
            "Epoch: 052, Loss: 1.6911, Train: 0.4656, Test: 0.4524\n",
            "Early stopping:  0.023982989329308826\n",
            "Epoch: 053, Loss: 1.6754, Train: 0.4719, Test: 0.4558\n",
            "Early stopping:  0.022202574022691227\n",
            "Epoch: 054, Loss: 1.6642, Train: 0.4751, Test: 0.4598\n",
            "Early stopping:  0.02072825534707189\n",
            "Epoch: 055, Loss: 1.6531, Train: 0.4816, Test: 0.4592\n",
            "Early stopping:  0.02051361567958184\n",
            "Epoch: 056, Loss: 1.6395, Train: 0.4853, Test: 0.4694\n",
            "Early stopping:  0.019868434755591337\n",
            "Epoch: 057, Loss: 1.6286, Train: 0.4889, Test: 0.4688\n",
            "Early stopping:  0.01872552677551944\n",
            "Epoch: 058, Loss: 1.6185, Train: 0.4916, Test: 0.4751\n",
            "Early stopping:  0.018357163346297257\n",
            "Epoch: 059, Loss: 1.6059, Train: 0.4940, Test: 0.4824\n",
            "Early stopping:  0.01826785447426609\n",
            "Epoch: 060, Loss: 1.5938, Train: 0.4945, Test: 0.4824\n",
            "Early stopping:  0.018057399571938495\n",
            "Epoch: 061, Loss: 1.5839, Train: 0.4954, Test: 0.4819\n",
            "Early stopping:  0.01803413930756102\n",
            "Epoch: 062, Loss: 1.5737, Train: 0.4970, Test: 0.4819\n",
            "Early stopping:  0.01764740998942969\n",
            "Epoch: 063, Loss: 1.5636, Train: 0.4979, Test: 0.4819\n",
            "Early stopping:  0.01655566333799067\n",
            "Epoch: 064, Loss: 1.5548, Train: 0.4975, Test: 0.4870\n",
            "Early stopping:  0.015546902852363834\n",
            "Epoch: 065, Loss: 1.5462, Train: 0.5015, Test: 0.4892\n",
            "Early stopping:  0.014924119575514516\n",
            "Epoch: 066, Loss: 1.5373, Train: 0.5028, Test: 0.4938\n",
            "Early stopping:  0.014273928628809872\n",
            "Epoch: 067, Loss: 1.5289, Train: 0.5057, Test: 0.4966\n",
            "Early stopping:  0.013755572376971644\n",
            "Epoch: 068, Loss: 1.5199, Train: 0.5089, Test: 0.4977\n",
            "Early stopping:  0.01379554112409794\n",
            "Epoch: 069, Loss: 1.5109, Train: 0.5103, Test: 0.5028\n",
            "Early stopping:  0.013934093452409224\n",
            "Epoch: 070, Loss: 1.5021, Train: 0.5138, Test: 0.5051\n",
            "Early stopping:  0.01397648616534647\n",
            "Epoch: 071, Loss: 1.4934, Train: 0.5179, Test: 0.5062\n",
            "Early stopping:  0.014016898696568228\n",
            "Epoch: 072, Loss: 1.4852, Train: 0.5228, Test: 0.5085\n",
            "Early stopping:  0.013708257651462398\n",
            "Epoch: 073, Loss: 1.4770, Train: 0.5236, Test: 0.5136\n",
            "Early stopping:  0.013374275103140484\n",
            "Epoch: 074, Loss: 1.4694, Train: 0.5270, Test: 0.5130\n",
            "Early stopping:  0.012950962528981317\n",
            "Epoch: 075, Loss: 1.4609, Train: 0.5286, Test: 0.5119\n",
            "Early stopping:  0.012788234687836093\n",
            "Epoch: 076, Loss: 1.4511, Train: 0.5243, Test: 0.5125\n",
            "Early stopping:  0.013350811683430042\n",
            "Epoch: 077, Loss: 1.4497, Train: 0.5351, Test: 0.5170\n",
            "Early stopping:  0.01173458633490712\n",
            "Epoch: 078, Loss: 1.4453, Train: 0.5349, Test: 0.5176\n",
            "Early stopping:  0.009720358518182766\n",
            "PREDICTIONS -> tensor([9, 6, 9,  ..., 5, 6, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.43      0.63      0.51       190\n",
            "         capital_goods       0.33      0.02      0.03       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.41      0.30      0.35       198\n",
            " consumer_non-cyclical       0.60      0.33      0.43       112\n",
            "                energy       0.60      0.37      0.46        71\n",
            "             financial       0.59      0.61      0.60       192\n",
            "            healthcare       0.74      0.37      0.49        79\n",
            "              services       0.50      0.79      0.62       519\n",
            "            technology       0.51      0.24      0.33        99\n",
            "        transportation       0.65      0.63      0.64       101\n",
            "             utilities       0.80      0.43      0.56        56\n",
            "\n",
            "              accuracy                           0.52      1764\n",
            "             macro avg       0.51      0.39      0.42      1764\n",
            "          weighted avg       0.52      0.52      0.48      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 76.7521, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 577.9930, Train: 0.0734, Test: 0.0737\n",
            "Early stopping:  354.43085426179175\n",
            "Epoch: 003, Loss: 359.0671, Train: 0.0734, Test: 0.0680\n",
            "Early stopping:  251.28760992144728\n",
            "Epoch: 004, Loss: 256.8612, Train: 0.1112, Test: 0.1122\n",
            "Early stopping:  209.14184731116646\n",
            "Epoch: 005, Loss: 328.2101, Train: 0.0708, Test: 0.0692\n",
            "Early stopping:  181.18349812648344\n",
            "Epoch: 006, Loss: 309.4058, Train: 0.1374, Test: 0.1400\n",
            "Early stopping:  124.02709980608027\n",
            "Epoch: 007, Loss: 342.5348, Train: 0.1123, Test: 0.1190\n",
            "Early stopping:  39.360922689647985\n",
            "Epoch: 008, Loss: 343.1085, Train: 0.0644, Test: 0.0675\n",
            "Early stopping:  35.808267752250465\n",
            "Epoch: 009, Loss: 248.3663, Train: 0.1514, Test: 0.1536\n",
            "Early stopping:  39.344080132666484\n",
            "Epoch: 010, Loss: 196.5061, Train: 0.2959, Test: 0.3005\n",
            "Early stopping:  64.05301830995579\n",
            "Epoch: 011, Loss: 182.2905, Train: 0.3034, Test: 0.3061\n",
            "Early stopping:  77.28486360436634\n",
            "Epoch: 012, Loss: 154.3801, Train: 0.1726, Test: 0.1769\n",
            "Early stopping:  74.36185784408902\n",
            "Epoch: 013, Loss: 137.3054, Train: 0.1534, Test: 0.1531\n",
            "Early stopping:  42.89464382602908\n",
            "Epoch: 014, Loss: 124.2289, Train: 0.1683, Test: 0.1650\n",
            "Early stopping:  30.20829838327562\n",
            "Epoch: 015, Loss: 92.2403, Train: 0.1450, Test: 0.1508\n",
            "Early stopping:  33.59311310891332\n",
            "Epoch: 016, Loss: 69.4754, Train: 0.2614, Test: 0.2523\n",
            "Early stopping:  34.36061331134355\n",
            "Epoch: 017, Loss: 53.4321, Train: 0.2854, Test: 0.2829\n",
            "Early stopping:  35.46309611045852\n",
            "Epoch: 018, Loss: 40.5816, Train: 0.2519, Test: 0.2613\n",
            "Early stopping:  33.15162483514128\n",
            "Epoch: 019, Loss: 31.9018, Train: 0.1791, Test: 0.1842\n",
            "Early stopping:  24.022614996782778\n",
            "Epoch: 020, Loss: 24.5566, Train: 0.1724, Test: 0.1803\n",
            "Early stopping:  17.846961574528823\n",
            "Epoch: 021, Loss: 13.2534, Train: 0.1952, Test: 0.1956\n",
            "Early stopping:  15.30562400472468\n",
            "Epoch: 022, Loss: 5.7343, Train: 0.1721, Test: 0.1774\n",
            "Early stopping:  13.997031678870956\n",
            "Epoch: 023, Loss: 4.2216, Train: 0.1791, Test: 0.1803\n",
            "Early stopping:  12.017926152214077\n",
            "Epoch: 024, Loss: 3.6252, Train: 0.1864, Test: 0.1922\n",
            "Early stopping:  8.864517158206853\n",
            "Epoch: 025, Loss: 3.1596, Train: 0.1996, Test: 0.2086\n",
            "Early stopping:  4.169930424026796\n",
            "Epoch: 026, Loss: 2.7645, Train: 0.2178, Test: 0.2171\n",
            "Early stopping:  1.1598516644588446\n",
            "Epoch: 027, Loss: 2.5218, Train: 0.2305, Test: 0.2313\n",
            "Early stopping:  0.6816324918548482\n",
            "Epoch: 028, Loss: 2.4317, Train: 0.2405, Test: 0.2540\n",
            "Early stopping:  0.4934391568498647\n",
            "Epoch: 029, Loss: 2.3811, Train: 0.2507, Test: 0.2528\n",
            "Early stopping:  0.31984607602646464\n",
            "Epoch: 030, Loss: 2.3422, Train: 0.2684, Test: 0.2727\n",
            "Early stopping:  0.16841152018443276\n",
            "Epoch: 031, Loss: 2.3081, Train: 0.2850, Test: 0.2891\n",
            "Early stopping:  0.0835830319201512\n",
            "Epoch: 032, Loss: 2.2802, Train: 0.3067, Test: 0.3005\n",
            "Early stopping:  0.05983089688011855\n",
            "Epoch: 033, Loss: 2.2567, Train: 0.3206, Test: 0.3112\n",
            "Early stopping:  0.04937492402119301\n",
            "Epoch: 034, Loss: 2.2363, Train: 0.3295, Test: 0.3203\n",
            "Early stopping:  0.041842213783256026\n",
            "Epoch: 035, Loss: 2.2191, Train: 0.3363, Test: 0.3339\n",
            "Early stopping:  0.035273663001308894\n",
            "Epoch: 036, Loss: 2.2045, Train: 0.3445, Test: 0.3401\n",
            "Early stopping:  0.03004452993400682\n",
            "Epoch: 037, Loss: 2.1914, Train: 0.3547, Test: 0.3441\n",
            "Early stopping:  0.025770575222977304\n",
            "Epoch: 038, Loss: 2.1783, Train: 0.3572, Test: 0.3441\n",
            "Early stopping:  0.022755349367708476\n",
            "Epoch: 039, Loss: 2.1645, Train: 0.3576, Test: 0.3458\n",
            "Early stopping:  0.021384664958779775\n",
            "Epoch: 040, Loss: 2.1516, Train: 0.3598, Test: 0.3441\n",
            "Early stopping:  0.020973172441651658\n",
            "Epoch: 041, Loss: 2.1409, Train: 0.3624, Test: 0.3424\n",
            "Early stopping:  0.020218136796518005\n",
            "Epoch: 042, Loss: 2.1306, Train: 0.3641, Test: 0.3458\n",
            "Early stopping:  0.018862756674474925\n",
            "Epoch: 043, Loss: 2.1191, Train: 0.3688, Test: 0.3503\n",
            "Early stopping:  0.017709789348730704\n",
            "Epoch: 044, Loss: 2.1078, Train: 0.3696, Test: 0.3549\n",
            "Early stopping:  0.017304497685559746\n",
            "Epoch: 045, Loss: 2.0978, Train: 0.3698, Test: 0.3566\n",
            "Early stopping:  0.017240738906448012\n",
            "Epoch: 046, Loss: 2.0886, Train: 0.3713, Test: 0.3588\n",
            "Early stopping:  0.016644806370063627\n",
            "Epoch: 047, Loss: 2.0799, Train: 0.3730, Test: 0.3611\n",
            "Early stopping:  0.01545195732501015\n",
            "Epoch: 048, Loss: 2.0728, Train: 0.3760, Test: 0.3634\n",
            "Early stopping:  0.01392976489708357\n",
            "Epoch: 049, Loss: 2.0589, Train: 0.3796, Test: 0.3668\n",
            "Early stopping:  0.014897888809803243\n",
            "Epoch: 050, Loss: 2.0445, Train: 0.3866, Test: 0.3713\n",
            "Early stopping:  0.01745456861815529\n",
            "Epoch: 051, Loss: 2.0252, Train: 0.3912, Test: 0.3781\n",
            "Early stopping:  0.02203138513285328\n",
            "Epoch: 052, Loss: 2.0006, Train: 0.3963, Test: 0.3895\n",
            "Early stopping:  0.028404337689578205\n",
            "Epoch: 053, Loss: 1.9694, Train: 0.3966, Test: 0.3878\n",
            "Early stopping:  0.03564456569061909\n",
            "Epoch: 054, Loss: 1.9609, Train: 0.3986, Test: 0.3906\n",
            "Early stopping:  0.03565500521765012\n",
            "Epoch: 055, Loss: 1.9519, Train: 0.4042, Test: 0.3974\n",
            "Early stopping:  0.03048135525095503\n",
            "Epoch: 056, Loss: 1.9246, Train: 0.4066, Test: 0.4031\n",
            "Early stopping:  0.02757926027067575\n",
            "Epoch: 057, Loss: 1.9128, Train: 0.4096, Test: 0.4065\n",
            "Early stopping:  0.024182854753215268\n",
            "Epoch: 058, Loss: 1.9051, Train: 0.4132, Test: 0.4076\n",
            "Early stopping:  0.02435207164165595\n",
            "Epoch: 059, Loss: 1.8925, Train: 0.4181, Test: 0.4116\n",
            "Early stopping:  0.022568698005666465\n",
            "Epoch: 060, Loss: 1.8744, Train: 0.4224, Test: 0.4161\n",
            "Early stopping:  0.01928318067227572\n",
            "Epoch: 061, Loss: 1.8564, Train: 0.4248, Test: 0.4155\n",
            "Early stopping:  0.022960496587257292\n",
            "Epoch: 062, Loss: 1.8447, Train: 0.4300, Test: 0.4178\n",
            "Early stopping:  0.024865430924874532\n",
            "Epoch: 063, Loss: 1.8325, Train: 0.4316, Test: 0.4246\n",
            "Early stopping:  0.023811664603005003\n",
            "Epoch: 064, Loss: 1.8151, Train: 0.4347, Test: 0.4257\n",
            "Early stopping:  0.02262290799723429\n",
            "Epoch: 065, Loss: 1.8024, Train: 0.4340, Test: 0.4257\n",
            "Early stopping:  0.02181013345957654\n",
            "Epoch: 066, Loss: 1.7898, Train: 0.4354, Test: 0.4286\n",
            "Early stopping:  0.022143067792468045\n",
            "Epoch: 067, Loss: 1.7799, Train: 0.4367, Test: 0.4263\n",
            "Early stopping:  0.020723946634448782\n",
            "Epoch: 068, Loss: 1.7710, Train: 0.4375, Test: 0.4303\n",
            "Early stopping:  0.017564649216472324\n",
            "Epoch: 069, Loss: 1.7571, Train: 0.4400, Test: 0.4320\n",
            "Early stopping:  0.017344344737104486\n",
            "Epoch: 070, Loss: 1.7475, Train: 0.4415, Test: 0.4297\n",
            "Early stopping:  0.017028516059019554\n",
            "Epoch: 071, Loss: 1.7357, Train: 0.4438, Test: 0.4320\n",
            "Early stopping:  0.01773061525929776\n",
            "Epoch: 072, Loss: 1.7217, Train: 0.4466, Test: 0.4354\n",
            "Early stopping:  0.019011183969533635\n",
            "Epoch: 073, Loss: 1.7118, Train: 0.4482, Test: 0.4320\n",
            "Early stopping:  0.018441542949162434\n",
            "Epoch: 074, Loss: 1.6978, Train: 0.4527, Test: 0.4320\n",
            "Early stopping:  0.019527222262379317\n",
            "Epoch: 075, Loss: 1.6835, Train: 0.4538, Test: 0.4354\n",
            "Early stopping:  0.020331631933328567\n",
            "Epoch: 076, Loss: 1.6686, Train: 0.4560, Test: 0.4354\n",
            "Early stopping:  0.021331887458879907\n",
            "Epoch: 077, Loss: 1.6543, Train: 0.4585, Test: 0.4359\n",
            "Early stopping:  0.02278606145281822\n",
            "Epoch: 078, Loss: 1.6387, Train: 0.4639, Test: 0.4422\n",
            "Early stopping:  0.023283963404258114\n",
            "Epoch: 079, Loss: 1.6240, Train: 0.4703, Test: 0.4461\n",
            "Early stopping:  0.02354176807215321\n",
            "Epoch: 080, Loss: 1.6099, Train: 0.4738, Test: 0.4450\n",
            "Early stopping:  0.023367841473894774\n",
            "Epoch: 081, Loss: 1.5980, Train: 0.4764, Test: 0.4461\n",
            "Early stopping:  0.022392554794373177\n",
            "Epoch: 082, Loss: 1.5834, Train: 0.4819, Test: 0.4495\n",
            "Early stopping:  0.02160099799203721\n",
            "Epoch: 083, Loss: 1.5717, Train: 0.4867, Test: 0.4529\n",
            "Early stopping:  0.020725907835727024\n",
            "Epoch: 084, Loss: 1.5594, Train: 0.4911, Test: 0.4569\n",
            "Early stopping:  0.020142412125791546\n",
            "Epoch: 085, Loss: 1.5464, Train: 0.4968, Test: 0.4609\n",
            "Early stopping:  0.02015888834693292\n",
            "Epoch: 086, Loss: 1.5345, Train: 0.4998, Test: 0.4643\n",
            "Early stopping:  0.019487409138158617\n",
            "Epoch: 087, Loss: 1.5208, Train: 0.5002, Test: 0.4683\n",
            "Early stopping:  0.02001908590490547\n",
            "Epoch: 088, Loss: 1.5102, Train: 0.5047, Test: 0.4722\n",
            "Early stopping:  0.019598616324887606\n",
            "Epoch: 089, Loss: 1.4968, Train: 0.5096, Test: 0.4785\n",
            "Early stopping:  0.019534817714704417\n",
            "Epoch: 090, Loss: 1.4824, Train: 0.5150, Test: 0.4796\n",
            "Early stopping:  0.02029789437136855\n",
            "Epoch: 091, Loss: 1.4694, Train: 0.5165, Test: 0.4768\n",
            "Early stopping:  0.020678440097933487\n",
            "Epoch: 092, Loss: 1.4614, Train: 0.5257, Test: 0.4853\n",
            "Early stopping:  0.019837428651150735\n",
            "Epoch: 093, Loss: 1.4516, Train: 0.5283, Test: 0.4853\n",
            "Early stopping:  0.01772938218041956\n",
            "Epoch: 094, Loss: 1.4336, Train: 0.5351, Test: 0.4904\n",
            "Early stopping:  0.01842883777782209\n",
            "Epoch: 095, Loss: 1.4179, Train: 0.5420, Test: 0.4972\n",
            "Early stopping:  0.020970252892271887\n",
            "Epoch: 096, Loss: 1.4112, Train: 0.5469, Test: 0.5023\n",
            "Early stopping:  0.021415671464756855\n",
            "Epoch: 097, Loss: 1.3949, Train: 0.5513, Test: 0.5062\n",
            "Early stopping:  0.021650191522742425\n",
            "Epoch: 098, Loss: 1.3855, Train: 0.5592, Test: 0.5108\n",
            "Early stopping:  0.01897781773002059\n",
            "Epoch: 099, Loss: 1.3749, Train: 0.5625, Test: 0.5159\n",
            "Early stopping:  0.017750883581022767\n",
            "Epoch: 100, Loss: 1.3671, Train: 0.5693, Test: 0.5198\n",
            "Early stopping:  0.017273253723705514\n",
            "Epoch: 101, Loss: 1.3482, Train: 0.5710, Test: 0.5170\n",
            "Early stopping:  0.01791120395275411\n",
            "Epoch: 102, Loss: 1.3450, Train: 0.5738, Test: 0.5187\n",
            "Early stopping:  0.017323219438589544\n",
            "Epoch: 103, Loss: 1.3340, Train: 0.5690, Test: 0.5283\n",
            "Early stopping:  0.016766483655843347\n",
            "Epoch: 104, Loss: 1.3312, Train: 0.5798, Test: 0.5261\n",
            "Early stopping:  0.014213538249575787\n",
            "Epoch: 105, Loss: 1.3151, Train: 0.5793, Test: 0.5261\n",
            "Early stopping:  0.013105943430414081\n",
            "Epoch: 106, Loss: 1.3177, Train: 0.5789, Test: 0.5363\n",
            "Early stopping:  0.012303257701455763\n",
            "Epoch: 107, Loss: 1.3065, Train: 0.5805, Test: 0.5340\n",
            "Early stopping:  0.011488279564052543\n",
            "Epoch: 108, Loss: 1.3015, Train: 0.5898, Test: 0.5312\n",
            "Early stopping:  0.011446590973055087\n",
            "Epoch: 109, Loss: 1.2893, Train: 0.5914, Test: 0.5340\n",
            "Early stopping:  0.01140445763579341\n",
            "Epoch: 110, Loss: 1.2853, Train: 0.5885, Test: 0.5425\n",
            "Early stopping:  0.01313738272775539\n",
            "Epoch: 111, Loss: 1.2761, Train: 0.5932, Test: 0.5357\n",
            "Early stopping:  0.012320010476479947\n",
            "Epoch: 112, Loss: 1.2664, Train: 0.5941, Test: 0.5300\n",
            "Early stopping:  0.01330440455384925\n",
            "Epoch: 113, Loss: 1.2659, Train: 0.5975, Test: 0.5397\n",
            "Early stopping:  0.010678358932626059\n",
            "Epoch: 114, Loss: 1.2549, Train: 0.5975, Test: 0.5454\n",
            "Early stopping:  0.011478196214337617\n",
            "Epoch: 115, Loss: 1.2582, Train: 0.5948, Test: 0.5425\n",
            "Early stopping:  0.008218767446672772\n",
            "PREDICTIONS -> tensor([8, 0, 1,  ..., 5, 5, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.49      0.57      0.53       190\n",
            "         capital_goods       0.45      0.29      0.35       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.51      0.49      0.50       198\n",
            " consumer_non-cyclical       0.71      0.43      0.53       112\n",
            "                energy       0.56      0.41      0.47        71\n",
            "             financial       0.76      0.58      0.66       192\n",
            "            healthcare       0.72      0.49      0.59        79\n",
            "              services       0.50      0.83      0.62       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.69      0.52      0.60       101\n",
            "             utilities       1.00      0.02      0.04        56\n",
            "\n",
            "              accuracy                           0.54      1764\n",
            "             macro avg       0.53      0.39      0.41      1764\n",
            "          weighted avg       0.54      0.54      0.51      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 94.1989, Train: 0.0587, Test: 0.0567\n",
            "Early stopping:  nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 568.8394, Train: 0.1112, Test: 0.1105\n",
            "Early stopping:  335.6215452213585\n",
            "Epoch: 003, Loss: 481.3213, Train: 0.0766, Test: 0.0743\n",
            "Early stopping:  252.5888657868048\n",
            "Epoch: 004, Loss: 381.7801, Train: 0.0693, Test: 0.0658\n",
            "Early stopping:  206.23801007974245\n",
            "Epoch: 005, Loss: 269.3648, Train: 0.1170, Test: 0.1111\n",
            "Early stopping:  185.51823184862945\n",
            "Epoch: 006, Loss: 222.3417, Train: 0.1164, Test: 0.1173\n",
            "Early stopping:  143.9739092198165\n",
            "Epoch: 007, Loss: 179.6852, Train: 0.0737, Test: 0.0754\n",
            "Early stopping:  123.26030532885959\n",
            "Epoch: 008, Loss: 137.7430, Train: 0.0510, Test: 0.0471\n",
            "Early stopping:  94.01271898324876\n",
            "Epoch: 009, Loss: 114.0956, Train: 0.0458, Test: 0.0420\n",
            "Early stopping:  62.84923314304386\n",
            "Epoch: 010, Loss: 76.0402, Train: 0.0431, Test: 0.0408\n",
            "Early stopping:  56.88405503351555\n",
            "Epoch: 011, Loss: 43.4887, Train: 0.0574, Test: 0.0601\n",
            "Early stopping:  52.96054189397535\n",
            "Epoch: 012, Loss: 15.6512, Train: 0.1252, Test: 0.1213\n",
            "Early stopping:  49.8813347114932\n",
            "Epoch: 013, Loss: 6.1103, Train: 0.1486, Test: 0.1361\n",
            "Early stopping:  44.52269367066282\n",
            "Epoch: 014, Loss: 3.6929, Train: 0.1803, Test: 0.1740\n",
            "Early stopping:  30.680958317340327\n",
            "Epoch: 015, Loss: 3.1726, Train: 0.1615, Test: 0.1548\n",
            "Early stopping:  17.008310172949162\n",
            "Epoch: 016, Loss: 2.9469, Train: 0.1989, Test: 0.1922\n",
            "Early stopping:  5.368889256537923\n",
            "Epoch: 017, Loss: 2.7767, Train: 0.2395, Test: 0.2347\n",
            "Early stopping:  1.3692517181597752\n",
            "Epoch: 018, Loss: 2.6754, Train: 0.2463, Test: 0.2307\n",
            "Early stopping:  0.4043575151083361\n",
            "Epoch: 019, Loss: 2.6014, Train: 0.2464, Test: 0.2234\n",
            "Early stopping:  0.22908885138886437\n",
            "Epoch: 020, Loss: 2.5413, Train: 0.2376, Test: 0.2228\n",
            "Early stopping:  0.15968212846299015\n",
            "Epoch: 021, Loss: 2.4792, Train: 0.2254, Test: 0.2115\n",
            "Early stopping:  0.11602890318333949\n",
            "Epoch: 022, Loss: 2.4220, Train: 0.2171, Test: 0.2035\n",
            "Early stopping:  0.0995794510194271\n",
            "Epoch: 023, Loss: 2.3791, Train: 0.2096, Test: 0.1990\n",
            "Early stopping:  0.08935648838988483\n",
            "Epoch: 024, Loss: 2.3385, Train: 0.1904, Test: 0.1769\n",
            "Early stopping:  0.0803304626764404\n",
            "Epoch: 025, Loss: 2.3713, Train: 0.2188, Test: 0.2052\n",
            "Early stopping:  0.054241071016793936\n",
            "Epoch: 026, Loss: 2.3197, Train: 0.3158, Test: 0.2976\n",
            "Early stopping:  0.0394553802931096\n",
            "Epoch: 027, Loss: 2.3027, Train: 0.3224, Test: 0.2999\n",
            "Early stopping:  0.03274624832856268\n",
            "Epoch: 028, Loss: 2.3138, Train: 0.3217, Test: 0.3061\n",
            "Early stopping:  0.026881127826015015\n",
            "Epoch: 029, Loss: 2.3153, Train: 0.3262, Test: 0.3084\n",
            "Early stopping:  0.02687654437441019\n",
            "Epoch: 030, Loss: 2.3102, Train: 0.3291, Test: 0.3152\n",
            "Early stopping:  0.006396616946506012\n",
            "PREDICTIONS -> tensor([ 8,  8, 11,  ...,  8,  0,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.23      0.09      0.13       190\n",
            "         capital_goods       0.10      0.06      0.07       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.00      0.00      0.00       198\n",
            " consumer_non-cyclical       0.44      0.24      0.31       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.52      0.17      0.25       192\n",
            "            healthcare       0.39      0.52      0.45        79\n",
            "              services       0.31      0.81      0.45       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.00      0.00      0.00       101\n",
            "             utilities       0.21      0.18      0.19        56\n",
            "\n",
            "              accuracy                           0.32      1764\n",
            "             macro avg       0.18      0.17      0.16      1764\n",
            "          weighted avg       0.23      0.32      0.23      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 45.2918, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 615.6483, Train: 0.1225, Test: 0.1219\n",
            "Early stopping:  403.30295602969414\n",
            "Epoch: 003, Loss: 302.2837, Train: 0.0706, Test: 0.0658\n",
            "Early stopping:  285.64219146100936\n",
            "Epoch: 004, Loss: 230.8367, Train: 0.0746, Test: 0.0760\n",
            "Early stopping:  237.5500531169995\n",
            "Epoch: 005, Loss: 162.0813, Train: 0.1133, Test: 0.1134\n",
            "Early stopping:  214.58182735603538\n",
            "Epoch: 006, Loss: 123.2329, Train: 0.2943, Test: 0.2897\n",
            "Early stopping:  196.10062197606567\n",
            "Epoch: 007, Loss: 120.0376, Train: 0.2831, Test: 0.2727\n",
            "Early stopping:  78.0716360184142\n",
            "Epoch: 008, Loss: 92.0239, Train: 0.1870, Test: 0.1848\n",
            "Early stopping:  53.76113532360069\n",
            "Epoch: 009, Loss: 68.3332, Train: 0.2155, Test: 0.2058\n",
            "Early stopping:  35.34908135452785\n",
            "Epoch: 010, Loss: 39.3156, Train: 0.0785, Test: 0.0731\n",
            "Early stopping:  35.493578896534714\n",
            "Epoch: 011, Loss: 30.0265, Train: 0.1039, Test: 0.0947\n",
            "Early stopping:  37.18529544967347\n",
            "Epoch: 012, Loss: 12.9055, Train: 0.2236, Test: 0.2211\n",
            "Early stopping:  31.54469588398871\n",
            "Epoch: 013, Loss: 3.8102, Train: 0.1781, Test: 0.1797\n",
            "Early stopping:  25.151960222801574\n",
            "Epoch: 014, Loss: 3.2792, Train: 0.1762, Test: 0.1684\n",
            "Early stopping:  16.147056704197794\n",
            "Epoch: 015, Loss: 2.9635, Train: 0.1877, Test: 0.1791\n",
            "Early stopping:  11.626659528161081\n",
            "Epoch: 016, Loss: 2.6724, Train: 0.2077, Test: 0.2080\n",
            "Early stopping:  4.369185229506306\n",
            "Epoch: 017, Loss: 2.5070, Train: 0.2281, Test: 0.2268\n",
            "Early stopping:  0.5184711503397587\n",
            "Epoch: 018, Loss: 2.4279, Train: 0.2427, Test: 0.2387\n",
            "Early stopping:  0.35088254515586986\n",
            "Epoch: 019, Loss: 2.3767, Train: 0.2512, Test: 0.2426\n",
            "Early stopping:  0.2371532644628445\n",
            "Epoch: 020, Loss: 2.3354, Train: 0.2580, Test: 0.2511\n",
            "Early stopping:  0.1330105781728171\n",
            "Epoch: 021, Loss: 2.3016, Train: 0.2622, Test: 0.2562\n",
            "Early stopping:  0.08078773703778705\n",
            "Epoch: 022, Loss: 2.2839, Train: 0.2668, Test: 0.2591\n",
            "Early stopping:  0.058276136114459795\n",
            "Epoch: 023, Loss: 2.2722, Train: 0.2688, Test: 0.2670\n",
            "Early stopping:  0.04239171145423926\n",
            "Epoch: 024, Loss: 2.2580, Train: 0.2702, Test: 0.2647\n",
            "Early stopping:  0.029867601692843547\n",
            "Epoch: 025, Loss: 2.2421, Train: 0.2751, Test: 0.2710\n",
            "Early stopping:  0.02294761950591835\n",
            "Epoch: 026, Loss: 2.2250, Train: 0.2854, Test: 0.2744\n",
            "Early stopping:  0.02346778968274778\n",
            "Epoch: 027, Loss: 2.2058, Train: 0.3013, Test: 0.2846\n",
            "Early stopping:  0.02628903876927255\n",
            "Epoch: 028, Loss: 2.1872, Train: 0.2943, Test: 0.2851\n",
            "Early stopping:  0.02816515721189056\n",
            "Epoch: 029, Loss: 2.1827, Train: 0.2985, Test: 0.2897\n",
            "Early stopping:  0.025138296709501493\n",
            "Epoch: 030, Loss: 2.1763, Train: 0.3011, Test: 0.2937\n",
            "Early stopping:  0.019838207834518087\n",
            "Epoch: 031, Loss: 2.1565, Train: 0.3071, Test: 0.2988\n",
            "Early stopping:  0.01786147109105866\n",
            "Epoch: 032, Loss: 2.1301, Train: 0.3312, Test: 0.3107\n",
            "Early stopping:  0.02351608262134628\n",
            "Epoch: 033, Loss: 2.1094, Train: 0.3272, Test: 0.3056\n",
            "Early stopping:  0.030998803463388094\n",
            "Epoch: 034, Loss: 2.0974, Train: 0.3299, Test: 0.3078\n",
            "Early stopping:  0.032614495568962384\n",
            "Epoch: 035, Loss: 2.0861, Train: 0.3332, Test: 0.3084\n",
            "Early stopping:  0.027926505191707546\n",
            "Epoch: 036, Loss: 2.0702, Train: 0.3407, Test: 0.3146\n",
            "Early stopping:  0.022767041242263426\n",
            "Epoch: 037, Loss: 2.0486, Train: 0.3556, Test: 0.3367\n",
            "Early stopping:  0.0237583861139137\n",
            "Epoch: 038, Loss: 2.0294, Train: 0.3630, Test: 0.3396\n",
            "Early stopping:  0.027594728252460947\n",
            "Epoch: 039, Loss: 2.0090, Train: 0.3614, Test: 0.3396\n",
            "Early stopping:  0.030881538337150065\n",
            "Epoch: 040, Loss: 1.9897, Train: 0.3628, Test: 0.3373\n",
            "Early stopping:  0.03173841659846297\n",
            "Epoch: 041, Loss: 1.9746, Train: 0.3712, Test: 0.3481\n",
            "Early stopping:  0.02971969173606831\n",
            "Epoch: 042, Loss: 1.9559, Train: 0.3797, Test: 0.3560\n",
            "Early stopping:  0.028719332792048594\n",
            "Epoch: 043, Loss: 1.9343, Train: 0.3893, Test: 0.3679\n",
            "Early stopping:  0.029006176972770067\n",
            "Epoch: 044, Loss: 1.9174, Train: 0.3940, Test: 0.3753\n",
            "Early stopping:  0.029291151280231575\n",
            "Epoch: 045, Loss: 1.9052, Train: 0.4031, Test: 0.3781\n",
            "Early stopping:  0.028166709457749812\n",
            "Epoch: 046, Loss: 1.8922, Train: 0.4106, Test: 0.3866\n",
            "Early stopping:  0.024935140004820302\n",
            "Epoch: 047, Loss: 1.8755, Train: 0.4147, Test: 0.3929\n",
            "Early stopping:  0.022605091511059612\n",
            "Epoch: 048, Loss: 1.8578, Train: 0.4161, Test: 0.4031\n",
            "Early stopping:  0.023624675043793668\n",
            "Epoch: 049, Loss: 1.8426, Train: 0.4150, Test: 0.4053\n",
            "Early stopping:  0.02527878715813868\n",
            "Epoch: 050, Loss: 1.8283, Train: 0.4176, Test: 0.4127\n",
            "Early stopping:  0.02542174396725335\n",
            "Epoch: 051, Loss: 1.8140, Train: 0.4205, Test: 0.4127\n",
            "Early stopping:  0.024146227068426732\n",
            "Epoch: 052, Loss: 1.8013, Train: 0.4228, Test: 0.4201\n",
            "Early stopping:  0.022397053578231844\n",
            "Epoch: 053, Loss: 1.7853, Train: 0.4263, Test: 0.4308\n",
            "Early stopping:  0.022392770641297688\n",
            "Epoch: 054, Loss: 1.7681, Train: 0.4336, Test: 0.4331\n",
            "Early stopping:  0.02361656121922615\n",
            "Epoch: 055, Loss: 1.7504, Train: 0.4361, Test: 0.4342\n",
            "Early stopping:  0.025407121762503104\n",
            "Epoch: 056, Loss: 1.7317, Train: 0.4434, Test: 0.4325\n",
            "Early stopping:  0.02754626972203026\n",
            "Epoch: 057, Loss: 1.7150, Train: 0.4480, Test: 0.4405\n",
            "Early stopping:  0.027981555750759734\n",
            "Epoch: 058, Loss: 1.6958, Train: 0.4577, Test: 0.4427\n",
            "Early stopping:  0.028453919827813316\n",
            "Epoch: 059, Loss: 1.6754, Train: 0.4686, Test: 0.4456\n",
            "Early stopping:  0.02941824262864034\n",
            "Epoch: 060, Loss: 1.6571, Train: 0.4720, Test: 0.4518\n",
            "Early stopping:  0.02987459361990226\n",
            "Epoch: 061, Loss: 1.6410, Train: 0.4792, Test: 0.4512\n",
            "Early stopping:  0.02956044155822583\n",
            "Epoch: 062, Loss: 1.6203, Train: 0.4825, Test: 0.4541\n",
            "Early stopping:  0.02933394964863939\n",
            "Epoch: 063, Loss: 1.6071, Train: 0.4850, Test: 0.4649\n",
            "Early stopping:  0.02743831167468418\n",
            "Epoch: 064, Loss: 1.5957, Train: 0.4928, Test: 0.4666\n",
            "Early stopping:  0.024920821703324665\n",
            "Epoch: 065, Loss: 1.5782, Train: 0.4958, Test: 0.4694\n",
            "Early stopping:  0.023865605022901398\n",
            "Epoch: 066, Loss: 1.5580, Train: 0.5043, Test: 0.4751\n",
            "Early stopping:  0.024454164163113332\n",
            "Epoch: 067, Loss: 1.5417, Train: 0.5066, Test: 0.4836\n",
            "Early stopping:  0.026764137727010105\n",
            "Epoch: 068, Loss: 1.5314, Train: 0.5073, Test: 0.4745\n",
            "Early stopping:  0.02625184854065158\n",
            "Epoch: 069, Loss: 1.5122, Train: 0.5089, Test: 0.4728\n",
            "Early stopping:  0.025214164249630362\n",
            "Epoch: 070, Loss: 1.5027, Train: 0.5141, Test: 0.4762\n",
            "Early stopping:  0.02224596472222754\n",
            "Epoch: 071, Loss: 1.4821, Train: 0.5155, Test: 0.4875\n",
            "Early stopping:  0.023556180837296874\n",
            "Epoch: 072, Loss: 1.4765, Train: 0.5182, Test: 0.4847\n",
            "Early stopping:  0.022413580184155894\n",
            "Epoch: 073, Loss: 1.4585, Train: 0.5232, Test: 0.4858\n",
            "Early stopping:  0.021351095409736072\n",
            "Epoch: 074, Loss: 1.4516, Train: 0.5287, Test: 0.4949\n",
            "Early stopping:  0.020220546827086912\n",
            "Epoch: 075, Loss: 1.4360, Train: 0.5298, Test: 0.4989\n",
            "Early stopping:  0.018718987317528465\n",
            "Epoch: 076, Loss: 1.4226, Train: 0.5335, Test: 0.4983\n",
            "Early stopping:  0.020732676508152373\n",
            "Epoch: 077, Loss: 1.4140, Train: 0.5374, Test: 0.5017\n",
            "Early stopping:  0.01878125772721524\n",
            "Epoch: 078, Loss: 1.3987, Train: 0.5435, Test: 0.5006\n",
            "Early stopping:  0.020313656429381335\n",
            "Epoch: 079, Loss: 1.3900, Train: 0.5457, Test: 0.5051\n",
            "Early stopping:  0.018406551212441037\n",
            "Epoch: 080, Loss: 1.3765, Train: 0.5486, Test: 0.5085\n",
            "Early stopping:  0.018439860126392246\n",
            "Epoch: 081, Loss: 1.3629, Train: 0.5511, Test: 0.5062\n",
            "Early stopping:  0.019733129457072812\n",
            "Epoch: 082, Loss: 1.3561, Train: 0.5564, Test: 0.5153\n",
            "Early stopping:  0.01786987686658003\n",
            "Epoch: 083, Loss: 1.3442, Train: 0.5603, Test: 0.5119\n",
            "Early stopping:  0.01784126633113814\n",
            "Epoch: 084, Loss: 1.3364, Train: 0.5617, Test: 0.5108\n",
            "Early stopping:  0.015726998846432798\n",
            "Epoch: 085, Loss: 1.3265, Train: 0.5657, Test: 0.5187\n",
            "Early stopping:  0.014645616061475464\n",
            "Epoch: 086, Loss: 1.3154, Train: 0.5698, Test: 0.5181\n",
            "Early stopping:  0.015669839814641477\n",
            "Epoch: 087, Loss: 1.3051, Train: 0.5756, Test: 0.5193\n",
            "Early stopping:  0.01571273034332938\n",
            "Epoch: 088, Loss: 1.2977, Train: 0.5759, Test: 0.5272\n",
            "Early stopping:  0.015665295522476353\n",
            "Epoch: 089, Loss: 1.2913, Train: 0.5790, Test: 0.5255\n",
            "Early stopping:  0.014036157688449322\n",
            "Epoch: 090, Loss: 1.2780, Train: 0.5812, Test: 0.5329\n",
            "Early stopping:  0.014117805179454011\n",
            "Epoch: 091, Loss: 1.2701, Train: 0.5868, Test: 0.5334\n",
            "Early stopping:  0.014280759549776126\n",
            "Epoch: 092, Loss: 1.2609, Train: 0.5893, Test: 0.5357\n",
            "Early stopping:  0.015057845241411898\n",
            "Epoch: 093, Loss: 1.2557, Train: 0.5894, Test: 0.5431\n",
            "Early stopping:  0.014129798723530268\n",
            "Epoch: 094, Loss: 1.2510, Train: 0.5897, Test: 0.5244\n",
            "Early stopping:  0.010926068659881804\n",
            "Epoch: 095, Loss: 1.2523, Train: 0.5921, Test: 0.5312\n",
            "Early stopping:  0.007780661987806258\n",
            "PREDICTIONS -> tensor([ 8,  0,  0,  ..., 11,  8,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.46      0.61      0.52       190\n",
            "         capital_goods       0.64      0.14      0.23       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.48      0.65      0.55       198\n",
            " consumer_non-cyclical       0.51      0.29      0.37       112\n",
            "                energy       0.70      0.30      0.42        71\n",
            "             financial       0.65      0.62      0.64       192\n",
            "            healthcare       0.49      0.47      0.48        79\n",
            "              services       0.54      0.75      0.63       519\n",
            "            technology       0.33      0.01      0.02        99\n",
            "        transportation       0.51      0.48      0.49       101\n",
            "             utilities       0.60      0.50      0.54        56\n",
            "\n",
            "              accuracy                           0.53      1764\n",
            "             macro avg       0.49      0.40      0.41      1764\n",
            "          weighted avg       0.53      0.53      0.50      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 50.4592, Train: 0.2544, Test: 0.2557\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 332.9962, Train: 0.1080, Test: 0.1083\n",
            "Early stopping:  199.7838655149332\n",
            "Epoch: 003, Loss: 795.9018, Train: 0.0581, Test: 0.0578\n",
            "Early stopping:  376.3405984649026\n",
            "Epoch: 004, Loss: 531.8702, Train: 0.1232, Test: 0.1179\n",
            "Early stopping:  315.0150245801492\n",
            "Epoch: 005, Loss: 383.2577, Train: 0.1097, Test: 0.1054\n",
            "Early stopping:  273.5375205300352\n",
            "Epoch: 006, Loss: 403.1257, Train: 0.0827, Test: 0.0845\n",
            "Early stopping:  186.3589654489051\n",
            "Epoch: 007, Loss: 375.8446, Train: 0.2700, Test: 0.2721\n",
            "Early stopping:  178.1723451769973\n",
            "Epoch: 008, Loss: 308.2159, Train: 0.3017, Test: 0.3010\n",
            "Early stopping:  81.68009145651855\n",
            "Epoch: 009, Loss: 330.3826, Train: 0.0888, Test: 0.0907\n",
            "Early stopping:  39.4031876755913\n",
            "Epoch: 010, Loss: 320.7524, Train: 0.2683, Test: 0.2749\n",
            "Early stopping:  40.15339366211964\n",
            "Epoch: 011, Loss: 250.5853, Train: 0.3048, Test: 0.3005\n",
            "Early stopping:  45.121552873929346\n",
            "Epoch: 012, Loss: 192.7700, Train: 0.3085, Test: 0.2988\n",
            "Early stopping:  58.026153911474836\n",
            "Epoch: 013, Loss: 130.7232, Train: 0.1591, Test: 0.1485\n",
            "Early stopping:  84.92196624707199\n",
            "Epoch: 014, Loss: 84.4112, Train: 0.1211, Test: 0.1179\n",
            "Early stopping:  93.89642660828643\n",
            "Epoch: 015, Loss: 60.1670, Train: 0.2002, Test: 0.1831\n",
            "Early stopping:  78.25223201261849\n",
            "Epoch: 016, Loss: 35.5385, Train: 0.1964, Test: 0.1865\n",
            "Early stopping:  62.314808617774226\n",
            "Epoch: 017, Loss: 22.0182, Train: 0.1659, Test: 0.1587\n",
            "Early stopping:  43.07770822018393\n",
            "Epoch: 018, Loss: 12.2360, Train: 0.1833, Test: 0.1695\n",
            "Early stopping:  29.3762051184038\n",
            "Epoch: 019, Loss: 7.0537, Train: 0.2215, Test: 0.2075\n",
            "Early stopping:  21.28844670616583\n",
            "Epoch: 020, Loss: 4.5040, Train: 0.2172, Test: 0.2149\n",
            "Early stopping:  12.688283282719276\n",
            "Epoch: 021, Loss: 3.6107, Train: 0.2220, Test: 0.2098\n",
            "Early stopping:  7.567690647107797\n",
            "Epoch: 022, Loss: 2.9740, Train: 0.2436, Test: 0.2268\n",
            "Early stopping:  3.777330582484026\n",
            "Epoch: 023, Loss: 2.5207, Train: 0.2622, Test: 0.2415\n",
            "Early stopping:  1.7946805017604541\n",
            "Epoch: 024, Loss: 2.2906, Train: 0.2823, Test: 0.2630\n",
            "Early stopping:  0.8953716999772322\n",
            "Epoch: 025, Loss: 2.1950, Train: 0.2939, Test: 0.2812\n",
            "Early stopping:  0.5826004190324459\n",
            "Epoch: 026, Loss: 2.1609, Train: 0.3080, Test: 0.2942\n",
            "Early stopping:  0.33586168238387765\n",
            "Epoch: 027, Loss: 2.1595, Train: 0.3236, Test: 0.3039\n",
            "Early stopping:  0.15240039729423932\n",
            "Epoch: 028, Loss: 2.1529, Train: 0.3574, Test: 0.3396\n",
            "Early stopping:  0.05762397799068232\n",
            "Epoch: 029, Loss: 2.1187, Train: 0.3659, Test: 0.3452\n",
            "Early stopping:  0.027126535245864225\n",
            "Epoch: 030, Loss: 2.0714, Train: 0.3780, Test: 0.3588\n",
            "Early stopping:  0.03833252131496195\n",
            "Epoch: 031, Loss: 2.0271, Train: 0.3834, Test: 0.3588\n",
            "Early stopping:  0.056214227581696634\n",
            "Epoch: 032, Loss: 2.0036, Train: 0.3927, Test: 0.3645\n",
            "Early stopping:  0.061999166487319915\n",
            "Epoch: 033, Loss: 1.9805, Train: 0.4000, Test: 0.3741\n",
            "Early stopping:  0.05525789353793315\n",
            "Epoch: 034, Loss: 1.9641, Train: 0.4065, Test: 0.3838\n",
            "Early stopping:  0.04205085027591061\n",
            "Epoch: 035, Loss: 1.9434, Train: 0.4208, Test: 0.3957\n",
            "Early stopping:  0.032783926462969036\n",
            "Epoch: 036, Loss: 1.9186, Train: 0.4319, Test: 0.4076\n",
            "Early stopping:  0.03280416071058637\n",
            "Epoch: 037, Loss: 1.8939, Train: 0.4404, Test: 0.4212\n",
            "Early stopping:  0.03470329522460519\n",
            "Epoch: 038, Loss: 1.8689, Train: 0.4497, Test: 0.4308\n",
            "Early stopping:  0.03796998231195582\n",
            "Epoch: 039, Loss: 1.8488, Train: 0.4543, Test: 0.4433\n",
            "Early stopping:  0.037816572863100105\n",
            "Epoch: 040, Loss: 1.8333, Train: 0.4575, Test: 0.4450\n",
            "Early stopping:  0.034264083222392876\n",
            "Epoch: 041, Loss: 1.8187, Train: 0.4558, Test: 0.4456\n",
            "Early stopping:  0.029599926233593202\n",
            "Epoch: 042, Loss: 1.8070, Train: 0.4571, Test: 0.4456\n",
            "Early stopping:  0.024449938177743692\n",
            "Epoch: 043, Loss: 1.7937, Train: 0.4598, Test: 0.4456\n",
            "Early stopping:  0.02159891828846669\n",
            "Epoch: 044, Loss: 1.7758, Train: 0.4636, Test: 0.4473\n",
            "Early stopping:  0.02220761506370377\n",
            "Epoch: 045, Loss: 1.7533, Train: 0.4728, Test: 0.4563\n",
            "Early stopping:  0.02584887593514343\n",
            "Epoch: 046, Loss: 1.7243, Train: 0.4721, Test: 0.4552\n",
            "Early stopping:  0.03287772671049852\n",
            "Epoch: 047, Loss: 1.7044, Train: 0.4771, Test: 0.4632\n",
            "Early stopping:  0.03649608059723671\n",
            "Epoch: 048, Loss: 1.6847, Train: 0.4843, Test: 0.4660\n",
            "Early stopping:  0.03664603109772177\n",
            "Epoch: 049, Loss: 1.6690, Train: 0.4904, Test: 0.4700\n",
            "Early stopping:  0.03315176106527016\n",
            "Epoch: 050, Loss: 1.6513, Train: 0.4938, Test: 0.4728\n",
            "Early stopping:  0.028722137198810057\n",
            "Epoch: 051, Loss: 1.6341, Train: 0.4981, Test: 0.4756\n",
            "Early stopping:  0.027520584153587567\n",
            "Epoch: 052, Loss: 1.6174, Train: 0.5049, Test: 0.4802\n",
            "Early stopping:  0.026817307852597134\n",
            "Epoch: 053, Loss: 1.6032, Train: 0.5104, Test: 0.4841\n",
            "Early stopping:  0.026212060865305857\n",
            "Epoch: 054, Loss: 1.5851, Train: 0.5154, Test: 0.4921\n",
            "Early stopping:  0.025833038175293793\n",
            "Epoch: 055, Loss: 1.5730, Train: 0.5211, Test: 0.4887\n",
            "Early stopping:  0.024458648760115994\n",
            "Epoch: 056, Loss: 1.5622, Train: 0.5202, Test: 0.4887\n",
            "Early stopping:  0.02229963811210079\n",
            "Epoch: 057, Loss: 1.5488, Train: 0.5218, Test: 0.4915\n",
            "Early stopping:  0.020897318006429116\n",
            "Epoch: 058, Loss: 1.5337, Train: 0.5263, Test: 0.4915\n",
            "Early stopping:  0.020117826563859274\n",
            "Epoch: 059, Loss: 1.5206, Train: 0.5255, Test: 0.4938\n",
            "Early stopping:  0.021099084314975657\n",
            "Epoch: 060, Loss: 1.5048, Train: 0.5297, Test: 0.4943\n",
            "Early stopping:  0.022620269094089474\n",
            "Epoch: 061, Loss: 1.4898, Train: 0.5357, Test: 0.4915\n",
            "Early stopping:  0.02324035192674611\n",
            "Epoch: 062, Loss: 1.4794, Train: 0.5376, Test: 0.4983\n",
            "Early stopping:  0.022107566640837486\n",
            "Epoch: 063, Loss: 1.4645, Train: 0.5368, Test: 0.5000\n",
            "Early stopping:  0.021808784923861\n",
            "Epoch: 064, Loss: 1.4576, Train: 0.5473, Test: 0.5040\n",
            "Early stopping:  0.01903896924037975\n",
            "Epoch: 065, Loss: 1.4421, Train: 0.5507, Test: 0.5085\n",
            "Early stopping:  0.018611196896554497\n",
            "Epoch: 066, Loss: 1.4349, Train: 0.5534, Test: 0.5079\n",
            "Early stopping:  0.017750732674540737\n",
            "Epoch: 067, Loss: 1.4233, Train: 0.5576, Test: 0.5147\n",
            "Early stopping:  0.01672914902178276\n",
            "Epoch: 068, Loss: 1.4133, Train: 0.5585, Test: 0.5096\n",
            "Early stopping:  0.017062315286235884\n",
            "Epoch: 069, Loss: 1.4037, Train: 0.5582, Test: 0.5164\n",
            "Early stopping:  0.0155944987740045\n",
            "Epoch: 070, Loss: 1.3935, Train: 0.5633, Test: 0.5181\n",
            "Early stopping:  0.016189025754693945\n",
            "Epoch: 071, Loss: 1.3842, Train: 0.5671, Test: 0.5204\n",
            "Early stopping:  0.015508937031061973\n",
            "Epoch: 072, Loss: 1.3763, Train: 0.5656, Test: 0.5221\n",
            "Early stopping:  0.014809658664421306\n",
            "Epoch: 073, Loss: 1.3710, Train: 0.5724, Test: 0.5261\n",
            "Early stopping:  0.013141476031282748\n",
            "Epoch: 074, Loss: 1.3560, Train: 0.5735, Test: 0.5272\n",
            "Early stopping:  0.014149557568294674\n",
            "Epoch: 075, Loss: 1.3514, Train: 0.5756, Test: 0.5312\n",
            "Early stopping:  0.013786470836346544\n",
            "Epoch: 076, Loss: 1.3407, Train: 0.5783, Test: 0.5312\n",
            "Early stopping:  0.01454418836866541\n",
            "Epoch: 077, Loss: 1.3353, Train: 0.5803, Test: 0.5340\n",
            "Early stopping:  0.013954683266868965\n",
            "Epoch: 078, Loss: 1.3252, Train: 0.5793, Test: 0.5329\n",
            "Early stopping:  0.012372209196441818\n",
            "Epoch: 079, Loss: 1.3241, Train: 0.5841, Test: 0.5374\n",
            "Early stopping:  0.011357656078052294\n",
            "Epoch: 080, Loss: 1.3107, Train: 0.5854, Test: 0.5420\n",
            "Early stopping:  0.011537500790082972\n",
            "Epoch: 081, Loss: 1.3062, Train: 0.5841, Test: 0.5374\n",
            "Early stopping:  0.011758652048986644\n",
            "Epoch: 082, Loss: 1.3034, Train: 0.5887, Test: 0.5459\n",
            "Early stopping:  0.01011050406369669\n",
            "Epoch: 083, Loss: 1.2938, Train: 0.5910, Test: 0.5465\n",
            "Early stopping:  0.011080759589734786\n",
            "Epoch: 084, Loss: 1.2844, Train: 0.5921, Test: 0.5448\n",
            "Early stopping:  0.01057937881196075\n",
            "Epoch: 085, Loss: 1.2784, Train: 0.5898, Test: 0.5471\n",
            "Early stopping:  0.011956967016735274\n",
            "Epoch: 086, Loss: 1.2760, Train: 0.5938, Test: 0.5476\n",
            "Early stopping:  0.011390976268189587\n",
            "Epoch: 087, Loss: 1.2724, Train: 0.5989, Test: 0.5510\n",
            "Early stopping:  0.008374786155393882\n",
            "PREDICTIONS -> tensor([ 9,  0,  9,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.54      0.61      0.57       190\n",
            "         capital_goods       0.44      0.06      0.10       127\n",
            "conglomerates_industry       0.25      0.10      0.14        20\n",
            "     consumer_cyclical       0.47      0.53      0.49       198\n",
            " consumer_non-cyclical       0.49      0.34      0.40       112\n",
            "                energy       0.62      0.48      0.54        71\n",
            "             financial       0.74      0.57      0.64       192\n",
            "            healthcare       0.69      0.52      0.59        79\n",
            "              services       0.52      0.77      0.62       519\n",
            "            technology       0.49      0.27      0.35        99\n",
            "        transportation       0.67      0.58      0.62       101\n",
            "             utilities       0.72      0.64      0.68        56\n",
            "\n",
            "              accuracy                           0.55      1764\n",
            "             macro avg       0.55      0.46      0.48      1764\n",
            "          weighted avg       0.55      0.55      0.53      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 74.0118, Train: 0.2939, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 448.7385, Train: 0.1070, Test: 0.1066\n",
            "Early stopping:  264.97180133040683\n",
            "Epoch: 003, Loss: 784.8141, Train: 0.0333, Test: 0.0329\n",
            "Early stopping:  355.57628393421106\n",
            "Epoch: 004, Loss: 576.5677, Train: 0.0669, Test: 0.0669\n",
            "Early stopping:  298.7301381511979\n",
            "Epoch: 005, Loss: 402.3347, Train: 0.0594, Test: 0.0607\n",
            "Early stopping:  260.5257477854493\n",
            "Epoch: 006, Loss: 380.9002, Train: 0.1110, Test: 0.1100\n",
            "Early stopping:  167.0252291432427\n",
            "Epoch: 007, Loss: 326.2361, Train: 0.0272, Test: 0.0300\n",
            "Early stopping:  187.5693159750475\n",
            "Epoch: 008, Loss: 236.7889, Train: 0.2016, Test: 0.2018\n",
            "Early stopping:  124.93427421000055\n",
            "Epoch: 009, Loss: 163.5960, Train: 0.2875, Test: 0.2800\n",
            "Early stopping:  100.35912468755913\n",
            "Epoch: 010, Loss: 126.6703, Train: 0.2392, Test: 0.2392\n",
            "Early stopping:  106.94299123339573\n",
            "Epoch: 011, Loss: 88.9866, Train: 0.1371, Test: 0.1406\n",
            "Early stopping:  94.40480367425477\n",
            "Epoch: 012, Loss: 57.1958, Train: 0.1341, Test: 0.1378\n",
            "Early stopping:  69.67459481217016\n",
            "Epoch: 013, Loss: 28.0867, Train: 0.1652, Test: 0.1701\n",
            "Early stopping:  53.91945501590818\n",
            "Epoch: 014, Loss: 12.1883, Train: 0.1439, Test: 0.1542\n",
            "Early stopping:  46.26813926506859\n",
            "Epoch: 015, Loss: 5.5574, Train: 0.1270, Test: 0.1287\n",
            "Early stopping:  34.58446744065234\n",
            "Epoch: 016, Loss: 3.9472, Train: 0.1538, Test: 0.1582\n",
            "Early stopping:  22.172592915482785\n",
            "Epoch: 017, Loss: 3.0127, Train: 0.1917, Test: 0.1882\n",
            "Early stopping:  10.434968327435087\n",
            "Epoch: 018, Loss: 2.6646, Train: 0.2060, Test: 0.2041\n",
            "Early stopping:  3.9169980653994174\n",
            "Epoch: 019, Loss: 2.5409, Train: 0.2076, Test: 0.1956\n",
            "Early stopping:  1.2528124954737951\n",
            "Epoch: 020, Loss: 2.4664, Train: 0.1847, Test: 0.1842\n",
            "Early stopping:  0.6078875713406114\n",
            "Epoch: 021, Loss: 2.4375, Train: 0.1832, Test: 0.1786\n",
            "Early stopping:  0.23411482799861857\n",
            "Epoch: 022, Loss: 2.3703, Train: 0.1914, Test: 0.1882\n",
            "Early stopping:  0.11238966437795565\n",
            "Epoch: 023, Loss: 2.3289, Train: 0.1896, Test: 0.1888\n",
            "Early stopping:  0.08283599401435933\n",
            "Epoch: 024, Loss: 2.3048, Train: 0.2050, Test: 0.2035\n",
            "Early stopping:  0.06913976031508753\n",
            "Epoch: 025, Loss: 2.2863, Train: 0.2157, Test: 0.2109\n",
            "Early stopping:  0.06024295443766255\n",
            "Epoch: 026, Loss: 2.2713, Train: 0.2225, Test: 0.2239\n",
            "Early stopping:  0.03892357205417082\n",
            "Epoch: 027, Loss: 2.2572, Train: 0.2287, Test: 0.2234\n",
            "Early stopping:  0.028165814462246055\n",
            "Epoch: 028, Loss: 2.2321, Train: 0.2339, Test: 0.2290\n",
            "Early stopping:  0.027716162060460646\n",
            "Epoch: 029, Loss: 2.2018, Train: 0.2369, Test: 0.2290\n",
            "Early stopping:  0.03339461660210976\n",
            "Epoch: 030, Loss: 2.1781, Train: 0.2396, Test: 0.2375\n",
            "Early stopping:  0.03845512260460053\n",
            "Epoch: 031, Loss: 2.1705, Train: 0.2480, Test: 0.2387\n",
            "Early stopping:  0.03653190598583136\n",
            "Epoch: 032, Loss: 2.1421, Train: 0.2542, Test: 0.2432\n",
            "Early stopping:  0.03390689698058347\n",
            "Epoch: 033, Loss: 2.1145, Train: 0.2633, Test: 0.2540\n",
            "Early stopping:  0.033776428469493795\n",
            "Epoch: 034, Loss: 2.0956, Train: 0.2846, Test: 0.2783\n",
            "Early stopping:  0.035395432284321515\n",
            "Epoch: 035, Loss: 2.0706, Train: 0.3044, Test: 0.2914\n",
            "Early stopping:  0.03905173512763133\n",
            "Epoch: 036, Loss: 2.0403, Train: 0.3455, Test: 0.3362\n",
            "Early stopping:  0.03924607304515362\n",
            "Epoch: 037, Loss: 2.0082, Train: 0.3719, Test: 0.3594\n",
            "Early stopping:  0.042575784347503975\n",
            "Epoch: 038, Loss: 1.9772, Train: 0.3824, Test: 0.3651\n",
            "Early stopping:  0.04733374265426061\n",
            "Epoch: 039, Loss: 1.9639, Train: 0.3859, Test: 0.3685\n",
            "Early stopping:  0.04408491293855061\n",
            "Epoch: 040, Loss: 1.9505, Train: 0.3943, Test: 0.3764\n",
            "Early stopping:  0.0362487475126281\n",
            "Epoch: 041, Loss: 1.9320, Train: 0.4014, Test: 0.3872\n",
            "Early stopping:  0.02874979668264248\n",
            "Epoch: 042, Loss: 1.9095, Train: 0.4076, Test: 0.3895\n",
            "Early stopping:  0.026661076400930798\n",
            "Epoch: 043, Loss: 1.8932, Train: 0.4098, Test: 0.3929\n",
            "Early stopping:  0.028911029988030554\n",
            "Epoch: 044, Loss: 1.8835, Train: 0.4095, Test: 0.3985\n",
            "Early stopping:  0.02754557235721065\n",
            "Epoch: 045, Loss: 1.8703, Train: 0.4106, Test: 0.3963\n",
            "Early stopping:  0.02391530666267319\n",
            "Epoch: 046, Loss: 1.8505, Train: 0.4115, Test: 0.3940\n",
            "Early stopping:  0.022408400711921354\n",
            "Epoch: 047, Loss: 1.8333, Train: 0.4133, Test: 0.3934\n",
            "Early stopping:  0.024385721510568004\n",
            "Epoch: 048, Loss: 1.8183, Train: 0.4144, Test: 0.3974\n",
            "Early stopping:  0.026527497021769427\n",
            "Epoch: 049, Loss: 1.7976, Train: 0.4177, Test: 0.3980\n",
            "Early stopping:  0.02813748321797275\n",
            "Epoch: 050, Loss: 1.7839, Train: 0.4185, Test: 0.3997\n",
            "Early stopping:  0.026758698306001444\n",
            "Epoch: 051, Loss: 1.7758, Train: 0.4217, Test: 0.3985\n",
            "Early stopping:  0.023868712533240858\n",
            "Epoch: 052, Loss: 1.7623, Train: 0.4218, Test: 0.4053\n",
            "Early stopping:  0.021429540347794023\n",
            "Epoch: 053, Loss: 1.7495, Train: 0.4202, Test: 0.4036\n",
            "Early stopping:  0.018682426258335095\n",
            "Epoch: 054, Loss: 1.7414, Train: 0.4245, Test: 0.4082\n",
            "Early stopping:  0.017660974095860926\n",
            "Epoch: 055, Loss: 1.7303, Train: 0.4293, Test: 0.4116\n",
            "Early stopping:  0.017759246864649397\n",
            "Epoch: 056, Loss: 1.7185, Train: 0.4349, Test: 0.4150\n",
            "Early stopping:  0.01690311177979358\n",
            "Epoch: 057, Loss: 1.7097, Train: 0.4381, Test: 0.4178\n",
            "Early stopping:  0.01620981381232998\n",
            "Epoch: 058, Loss: 1.6987, Train: 0.4401, Test: 0.4178\n",
            "Early stopping:  0.016774858918209175\n",
            "Epoch: 059, Loss: 1.6876, Train: 0.4425, Test: 0.4212\n",
            "Early stopping:  0.0166272081767816\n",
            "Epoch: 060, Loss: 1.6758, Train: 0.4446, Test: 0.4269\n",
            "Early stopping:  0.017035055155867555\n",
            "Epoch: 061, Loss: 1.6666, Train: 0.4487, Test: 0.4331\n",
            "Early stopping:  0.017276564134878894\n",
            "Epoch: 062, Loss: 1.6575, Train: 0.4534, Test: 0.4371\n",
            "Early stopping:  0.016357514182012013\n",
            "Epoch: 063, Loss: 1.6459, Train: 0.4550, Test: 0.4348\n",
            "Early stopping:  0.016098064592503212\n",
            "Epoch: 064, Loss: 1.6373, Train: 0.4575, Test: 0.4359\n",
            "Early stopping:  0.015466816442733023\n",
            "Epoch: 065, Loss: 1.6274, Train: 0.4597, Test: 0.4382\n",
            "Early stopping:  0.015621555198894573\n",
            "Epoch: 066, Loss: 1.6152, Train: 0.4595, Test: 0.4405\n",
            "Early stopping:  0.01633009260571678\n",
            "Epoch: 067, Loss: 1.6076, Train: 0.4618, Test: 0.4382\n",
            "Early stopping:  0.015621470685474663\n",
            "Epoch: 068, Loss: 1.5990, Train: 0.4638, Test: 0.4405\n",
            "Early stopping:  0.015256632676154932\n",
            "Epoch: 069, Loss: 1.5911, Train: 0.4659, Test: 0.4439\n",
            "Early stopping:  0.014099585897594292\n",
            "Epoch: 070, Loss: 1.5837, Train: 0.4696, Test: 0.4439\n",
            "Early stopping:  0.012587462391476355\n",
            "Epoch: 071, Loss: 1.5750, Train: 0.4728, Test: 0.4507\n",
            "Early stopping:  0.012741845355658999\n",
            "Epoch: 072, Loss: 1.5663, Train: 0.4747, Test: 0.4512\n",
            "Early stopping:  0.012890595903765437\n",
            "Epoch: 073, Loss: 1.5561, Train: 0.4753, Test: 0.4461\n",
            "Early stopping:  0.013826097233886843\n",
            "Epoch: 074, Loss: 1.5481, Train: 0.4778, Test: 0.4507\n",
            "Early stopping:  0.014254671499153628\n",
            "Epoch: 075, Loss: 1.5389, Train: 0.4802, Test: 0.4535\n",
            "Early stopping:  0.014305164742730112\n",
            "Epoch: 076, Loss: 1.5298, Train: 0.4858, Test: 0.4518\n",
            "Early stopping:  0.014291216843753146\n",
            "Epoch: 077, Loss: 1.5201, Train: 0.4923, Test: 0.4575\n",
            "Early stopping:  0.01430157681961844\n",
            "Epoch: 078, Loss: 1.5111, Train: 0.4943, Test: 0.4637\n",
            "Early stopping:  0.014674777183801566\n",
            "Epoch: 079, Loss: 1.5020, Train: 0.4960, Test: 0.4637\n",
            "Early stopping:  0.014621297033365982\n",
            "Epoch: 080, Loss: 1.4918, Train: 0.4995, Test: 0.4643\n",
            "Early stopping:  0.014870475800159414\n",
            "Epoch: 081, Loss: 1.4835, Train: 0.5023, Test: 0.4671\n",
            "Early stopping:  0.014628697571159395\n",
            "Epoch: 082, Loss: 1.4752, Train: 0.5057, Test: 0.4751\n",
            "Early stopping:  0.014299145717228704\n",
            "Epoch: 083, Loss: 1.4666, Train: 0.5103, Test: 0.4734\n",
            "Early stopping:  0.013833075440625883\n",
            "Epoch: 084, Loss: 1.4577, Train: 0.5117, Test: 0.4768\n",
            "Early stopping:  0.013457772572531635\n",
            "Epoch: 085, Loss: 1.4491, Train: 0.5151, Test: 0.4796\n",
            "Early stopping:  0.013635642390765696\n",
            "Epoch: 086, Loss: 1.4390, Train: 0.5169, Test: 0.4790\n",
            "Early stopping:  0.014226304924176226\n",
            "Epoch: 087, Loss: 1.4280, Train: 0.5219, Test: 0.4785\n",
            "Early stopping:  0.015197773307295154\n",
            "Epoch: 088, Loss: 1.4180, Train: 0.5240, Test: 0.4802\n",
            "Early stopping:  0.01591515478072693\n",
            "Epoch: 089, Loss: 1.4079, Train: 0.5279, Test: 0.4858\n",
            "Early stopping:  0.016341149142987357\n",
            "Epoch: 090, Loss: 1.3981, Train: 0.5338, Test: 0.4875\n",
            "Early stopping:  0.016108325434772947\n",
            "Epoch: 091, Loss: 1.3894, Train: 0.5388, Test: 0.4881\n",
            "Early stopping:  0.01536794433038895\n",
            "Epoch: 092, Loss: 1.3791, Train: 0.5435, Test: 0.4898\n",
            "Early stopping:  0.015227069518264176\n",
            "Epoch: 093, Loss: 1.3693, Train: 0.5466, Test: 0.4960\n",
            "Early stopping:  0.01519926258412999\n",
            "Epoch: 094, Loss: 1.3596, Train: 0.5511, Test: 0.4955\n",
            "Early stopping:  0.015318844080911376\n",
            "Epoch: 095, Loss: 1.3498, Train: 0.5532, Test: 0.4989\n",
            "Early stopping:  0.01559619335016229\n",
            "Epoch: 096, Loss: 1.3398, Train: 0.5582, Test: 0.5017\n",
            "Early stopping:  0.015543435945989597\n",
            "Epoch: 097, Loss: 1.3299, Train: 0.5598, Test: 0.5051\n",
            "Early stopping:  0.015607458670711704\n",
            "Epoch: 098, Loss: 1.3204, Train: 0.5653, Test: 0.5011\n",
            "Early stopping:  0.015554993023725944\n",
            "Epoch: 099, Loss: 1.3131, Train: 0.5649, Test: 0.5085\n",
            "Early stopping:  0.014695225730041181\n",
            "Epoch: 100, Loss: 1.3080, Train: 0.5701, Test: 0.5108\n",
            "Early stopping:  0.012805466326703646\n",
            "Epoch: 101, Loss: 1.2966, Train: 0.5727, Test: 0.5130\n",
            "Early stopping:  0.012588829750756009\n",
            "Epoch: 102, Loss: 1.2919, Train: 0.5717, Test: 0.5204\n",
            "Early stopping:  0.01172840717392923\n",
            "Epoch: 103, Loss: 1.2886, Train: 0.5742, Test: 0.5147\n",
            "Early stopping:  0.010535045655209347\n",
            "Epoch: 104, Loss: 1.2789, Train: 0.5786, Test: 0.5198\n",
            "Early stopping:  0.010695064730152772\n",
            "Epoch: 105, Loss: 1.2725, Train: 0.5762, Test: 0.5261\n",
            "Early stopping:  0.009822392519283891\n",
            "PREDICTIONS -> tensor([8, 0, 1,  ..., 5, 5, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.54      0.58      0.56       190\n",
            "         capital_goods       0.61      0.24      0.34       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.50      0.37      0.43       198\n",
            " consumer_non-cyclical       0.75      0.37      0.49       112\n",
            "                energy       0.54      0.51      0.52        71\n",
            "             financial       0.59      0.63      0.61       192\n",
            "            healthcare       0.63      0.48      0.55        79\n",
            "              services       0.47      0.79      0.59       519\n",
            "            technology       0.60      0.09      0.16        99\n",
            "        transportation       0.69      0.57      0.63       101\n",
            "             utilities       0.00      0.00      0.00        56\n",
            "\n",
            "              accuracy                           0.53      1764\n",
            "             macro avg       0.49      0.39      0.41      1764\n",
            "          weighted avg       0.53      0.53      0.49      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 62.4837, Train: 0.2795, Test: 0.2744\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 215.4731, Train: 0.1248, Test: 0.1202\n",
            "Early stopping:  108.1798360568564\n",
            "Epoch: 003, Loss: 342.6901, Train: 0.1080, Test: 0.1071\n",
            "Early stopping:  140.30057729840814\n",
            "Epoch: 004, Loss: 502.7337, Train: 0.1450, Test: 0.1429\n",
            "Early stopping:  187.09584155066034\n",
            "Epoch: 005, Loss: 237.3344, Train: 0.0761, Test: 0.0748\n",
            "Early stopping:  163.19398843905623\n",
            "Epoch: 006, Loss: 216.8858, Train: 0.0552, Test: 0.0544\n",
            "Early stopping:  123.34650838478247\n",
            "Epoch: 007, Loss: 271.9617, Train: 0.2955, Test: 0.3005\n",
            "Early stopping:  115.67089469432483\n",
            "Epoch: 008, Loss: 175.3894, Train: 0.3116, Test: 0.3141\n",
            "Early stopping:  128.85879251535252\n",
            "Epoch: 009, Loss: 163.6735, Train: 0.2646, Test: 0.2596\n",
            "Early stopping:  44.52917211574116\n",
            "Epoch: 010, Loss: 135.4335, Train: 0.1584, Test: 0.1599\n",
            "Early stopping:  53.12489072964038\n",
            "Epoch: 011, Loss: 132.6738, Train: 0.1679, Test: 0.1633\n",
            "Early stopping:  56.75301450941471\n",
            "Epoch: 012, Loss: 99.6427, Train: 0.2191, Test: 0.2143\n",
            "Early stopping:  29.609139304406035\n",
            "Epoch: 013, Loss: 66.9857, Train: 0.3002, Test: 0.3073\n",
            "Early stopping:  37.184067586836676\n",
            "Epoch: 014, Loss: 47.8877, Train: 0.3019, Test: 0.3050\n",
            "Early stopping:  38.950221354301426\n",
            "Epoch: 015, Loss: 30.0389, Train: 0.2405, Test: 0.2613\n",
            "Early stopping:  41.08935100329168\n",
            "Epoch: 016, Loss: 14.4221, Train: 0.1811, Test: 0.1854\n",
            "Early stopping:  33.18691226954945\n",
            "Epoch: 017, Loss: 7.1245, Train: 0.1866, Test: 0.1746\n",
            "Early stopping:  24.493348113519136\n",
            "Epoch: 018, Loss: 4.3174, Train: 0.1303, Test: 0.1304\n",
            "Early stopping:  18.155326323753826\n",
            "Epoch: 019, Loss: 3.7964, Train: 0.1347, Test: 0.1321\n",
            "Early stopping:  10.969758954322797\n",
            "Epoch: 020, Loss: 3.3405, Train: 0.1554, Test: 0.1582\n",
            "Early stopping:  4.613989038953761\n",
            "Epoch: 021, Loss: 2.9812, Train: 0.1669, Test: 0.1570\n",
            "Early stopping:  1.6500441140540432\n",
            "Epoch: 022, Loss: 2.7519, Train: 0.1680, Test: 0.1678\n",
            "Early stopping:  0.6306154345163744\n",
            "Epoch: 023, Loss: 2.6028, Train: 0.1674, Test: 0.1604\n",
            "Early stopping:  0.48092405906173347\n",
            "Epoch: 024, Loss: 2.4902, Train: 0.1691, Test: 0.1604\n",
            "Early stopping:  0.33786620915680066\n",
            "Epoch: 025, Loss: 2.4167, Train: 0.1735, Test: 0.1701\n",
            "Early stopping:  0.22486406575111523\n",
            "Epoch: 026, Loss: 2.3763, Train: 0.1752, Test: 0.1712\n",
            "Early stopping:  0.1521126014394157\n",
            "Epoch: 027, Loss: 2.3548, Train: 0.1777, Test: 0.1740\n",
            "Early stopping:  0.10069866770415581\n",
            "Epoch: 028, Loss: 2.3431, Train: 0.1816, Test: 0.1848\n",
            "Early stopping:  0.05954989784905605\n",
            "Epoch: 029, Loss: 2.3341, Train: 0.1894, Test: 0.1899\n",
            "Early stopping:  0.03295383785656575\n",
            "Epoch: 030, Loss: 2.3222, Train: 0.2294, Test: 0.2296\n",
            "Early stopping:  0.02067341397611811\n",
            "Epoch: 031, Loss: 2.3031, Train: 0.2602, Test: 0.2551\n",
            "Early stopping:  0.019851024449636198\n",
            "Epoch: 032, Loss: 2.2708, Train: 0.2826, Test: 0.2880\n",
            "Early stopping:  0.028727886376839474\n",
            "Epoch: 033, Loss: 2.2308, Train: 0.3031, Test: 0.3220\n",
            "Early stopping:  0.04183472372217779\n",
            "Epoch: 034, Loss: 2.2023, Train: 0.3267, Test: 0.3294\n",
            "Early stopping:  0.049638712664375285\n",
            "Epoch: 035, Loss: 2.2117, Train: 0.3333, Test: 0.3396\n",
            "Early stopping:  0.04232414673285973\n",
            "Epoch: 036, Loss: 2.2022, Train: 0.3305, Test: 0.3362\n",
            "Early stopping:  0.028852909886316763\n",
            "Epoch: 037, Loss: 2.1573, Train: 0.3296, Test: 0.3282\n",
            "Early stopping:  0.027012419127830683\n",
            "Epoch: 038, Loss: 2.1248, Train: 0.3258, Test: 0.3277\n",
            "Early stopping:  0.03729493356855175\n",
            "Epoch: 039, Loss: 2.1112, Train: 0.3272, Test: 0.3248\n",
            "Early stopping:  0.04494684460894289\n",
            "Epoch: 040, Loss: 2.0968, Train: 0.3393, Test: 0.3345\n",
            "Early stopping:  0.042091560818310954\n",
            "Epoch: 041, Loss: 2.0700, Train: 0.3478, Test: 0.3549\n",
            "Early stopping:  0.0324501154292159\n",
            "Epoch: 042, Loss: 2.0358, Train: 0.3625, Test: 0.3673\n",
            "Early stopping:  0.03542091563186309\n",
            "Epoch: 043, Loss: 2.0125, Train: 0.3723, Test: 0.3668\n",
            "Early stopping:  0.04118217919367728\n",
            "Epoch: 044, Loss: 2.0086, Train: 0.3760, Test: 0.3741\n",
            "Early stopping:  0.03801234793361879\n",
            "Epoch: 045, Loss: 1.9840, Train: 0.3776, Test: 0.3759\n",
            "Early stopping:  0.03241754310104986\n",
            "Epoch: 046, Loss: 1.9500, Train: 0.3817, Test: 0.3747\n",
            "Early stopping:  0.032574684589979204\n",
            "Epoch: 047, Loss: 1.9372, Train: 0.3874, Test: 0.3753\n",
            "Early stopping:  0.03393410802817096\n",
            "Epoch: 048, Loss: 1.9235, Train: 0.3930, Test: 0.3821\n",
            "Early stopping:  0.03496310586399497\n",
            "Epoch: 049, Loss: 1.8992, Train: 0.3939, Test: 0.3872\n",
            "Early stopping:  0.031533221038721984\n",
            "Epoch: 050, Loss: 1.8803, Train: 0.3947, Test: 0.3895\n",
            "Early stopping:  0.028273306119014158\n",
            "Epoch: 051, Loss: 1.8759, Train: 0.4010, Test: 0.3872\n",
            "Early stopping:  0.026684510006616428\n",
            "Epoch: 052, Loss: 1.8607, Train: 0.4071, Test: 0.3906\n",
            "Early stopping:  0.024150057506683184\n",
            "Epoch: 053, Loss: 1.8377, Train: 0.4129, Test: 0.3963\n",
            "Early stopping:  0.02301463143356611\n",
            "Epoch: 054, Loss: 1.8222, Train: 0.4171, Test: 0.4014\n",
            "Early stopping:  0.024904193118331995\n",
            "Epoch: 055, Loss: 1.8060, Train: 0.4251, Test: 0.4048\n",
            "Early stopping:  0.028268637376873348\n",
            "Epoch: 056, Loss: 1.7852, Train: 0.4306, Test: 0.4099\n",
            "Early stopping:  0.02898521049034507\n",
            "Epoch: 057, Loss: 1.7696, Train: 0.4319, Test: 0.4121\n",
            "Early stopping:  0.027460023401878506\n",
            "Epoch: 058, Loss: 1.7585, Train: 0.4374, Test: 0.4155\n",
            "Early stopping:  0.026031189796521928\n",
            "Epoch: 059, Loss: 1.7427, Train: 0.4417, Test: 0.4235\n",
            "Early stopping:  0.02435396002960523\n",
            "Epoch: 060, Loss: 1.7280, Train: 0.4429, Test: 0.4291\n",
            "Early stopping:  0.02235946082079863\n",
            "Epoch: 061, Loss: 1.7153, Train: 0.4461, Test: 0.4286\n",
            "Early stopping:  0.022022942178315106\n",
            "Epoch: 062, Loss: 1.7016, Train: 0.4492, Test: 0.4280\n",
            "Early stopping:  0.022365582947611262\n",
            "Epoch: 063, Loss: 1.6871, Train: 0.4540, Test: 0.4303\n",
            "Early stopping:  0.021766693350800375\n",
            "Epoch: 064, Loss: 1.6760, Train: 0.4568, Test: 0.4291\n",
            "Early stopping:  0.020926857349057255\n",
            "Epoch: 065, Loss: 1.6667, Train: 0.4595, Test: 0.4365\n",
            "Early stopping:  0.019491050672659292\n",
            "Epoch: 066, Loss: 1.6547, Train: 0.4638, Test: 0.4337\n",
            "Early stopping:  0.01808707402575494\n",
            "Epoch: 067, Loss: 1.6443, Train: 0.4693, Test: 0.4422\n",
            "Early stopping:  0.01689419715900705\n",
            "Epoch: 068, Loss: 1.6346, Train: 0.4753, Test: 0.4507\n",
            "Early stopping:  0.01664300815209451\n",
            "Epoch: 069, Loss: 1.6232, Train: 0.4764, Test: 0.4541\n",
            "Early stopping:  0.016962846886719646\n",
            "Epoch: 070, Loss: 1.6124, Train: 0.4794, Test: 0.4586\n",
            "Early stopping:  0.01672996026878801\n",
            "Epoch: 071, Loss: 1.6030, Train: 0.4842, Test: 0.4632\n",
            "Early stopping:  0.01659027693328121\n",
            "Epoch: 072, Loss: 1.5936, Train: 0.4883, Test: 0.4683\n",
            "Early stopping:  0.0161667839584162\n",
            "Epoch: 073, Loss: 1.5833, Train: 0.4917, Test: 0.4700\n",
            "Early stopping:  0.015572941984328252\n",
            "Epoch: 074, Loss: 1.5735, Train: 0.4940, Test: 0.4717\n",
            "Early stopping:  0.015394078172506964\n",
            "Epoch: 075, Loss: 1.5654, Train: 0.4964, Test: 0.4722\n",
            "Early stopping:  0.015068083465023759\n",
            "Epoch: 076, Loss: 1.5558, Train: 0.4991, Test: 0.4779\n",
            "Early stopping:  0.014803321323215799\n",
            "Epoch: 077, Loss: 1.5477, Train: 0.5021, Test: 0.4796\n",
            "Early stopping:  0.01406534294351672\n",
            "Epoch: 078, Loss: 1.5398, Train: 0.5073, Test: 0.4858\n",
            "Early stopping:  0.013475867619612834\n",
            "Epoch: 079, Loss: 1.5311, Train: 0.5118, Test: 0.4847\n",
            "Early stopping:  0.013386694658153106\n",
            "Epoch: 080, Loss: 1.5233, Train: 0.5118, Test: 0.4841\n",
            "Early stopping:  0.012925141571626901\n",
            "Epoch: 081, Loss: 1.5156, Train: 0.5114, Test: 0.4870\n",
            "Early stopping:  0.012768133579592505\n",
            "Epoch: 082, Loss: 1.5079, Train: 0.5124, Test: 0.4887\n",
            "Early stopping:  0.012525645232408872\n",
            "Epoch: 083, Loss: 1.5011, Train: 0.5131, Test: 0.4881\n",
            "Early stopping:  0.01190765378606815\n",
            "Epoch: 084, Loss: 1.4935, Train: 0.5142, Test: 0.4915\n",
            "Early stopping:  0.011695687641810214\n",
            "Epoch: 085, Loss: 1.4858, Train: 0.5175, Test: 0.4904\n",
            "Early stopping:  0.011709619144846488\n",
            "Epoch: 086, Loss: 1.4782, Train: 0.5192, Test: 0.4943\n",
            "Early stopping:  0.01183419524982041\n",
            "Epoch: 087, Loss: 1.4701, Train: 0.5226, Test: 0.4977\n",
            "Early stopping:  0.012253105138373441\n",
            "Epoch: 088, Loss: 1.4624, Train: 0.5259, Test: 0.5000\n",
            "Early stopping:  0.012345392857604606\n",
            "Epoch: 089, Loss: 1.4541, Train: 0.5315, Test: 0.5017\n",
            "Early stopping:  0.012518864443580819\n",
            "Epoch: 090, Loss: 1.4469, Train: 0.5344, Test: 0.5017\n",
            "Early stopping:  0.012415186117156606\n",
            "Epoch: 091, Loss: 1.4394, Train: 0.5372, Test: 0.5068\n",
            "Early stopping:  0.012160662386256096\n",
            "Epoch: 092, Loss: 1.4317, Train: 0.5396, Test: 0.5091\n",
            "Early stopping:  0.012041863879918312\n",
            "Epoch: 093, Loss: 1.4245, Train: 0.5398, Test: 0.5119\n",
            "Early stopping:  0.011785421087579556\n",
            "Epoch: 094, Loss: 1.4169, Train: 0.5435, Test: 0.5147\n",
            "Early stopping:  0.011823463584736684\n",
            "Epoch: 095, Loss: 1.4095, Train: 0.5444, Test: 0.5176\n",
            "Early stopping:  0.011775823485107402\n",
            "Epoch: 096, Loss: 1.4023, Train: 0.5449, Test: 0.5193\n",
            "Early stopping:  0.011670231563861783\n",
            "Epoch: 097, Loss: 1.3943, Train: 0.5483, Test: 0.5221\n",
            "Early stopping:  0.011848207919131057\n",
            "Epoch: 098, Loss: 1.3867, Train: 0.5500, Test: 0.5221\n",
            "Early stopping:  0.011949410303577527\n",
            "Epoch: 099, Loss: 1.3793, Train: 0.5524, Test: 0.5221\n",
            "Early stopping:  0.011994066015010485\n",
            "Epoch: 100, Loss: 1.3722, Train: 0.5544, Test: 0.5215\n",
            "Early stopping:  0.01189519000796767\n",
            "Epoch: 101, Loss: 1.3651, Train: 0.5564, Test: 0.5249\n",
            "Early stopping:  0.011541152852907197\n",
            "Epoch: 102, Loss: 1.3578, Train: 0.5575, Test: 0.5272\n",
            "Early stopping:  0.011390048377794481\n",
            "Epoch: 103, Loss: 1.3518, Train: 0.5586, Test: 0.5283\n",
            "Early stopping:  0.010990803041802145\n",
            "Epoch: 104, Loss: 1.3455, Train: 0.5610, Test: 0.5283\n",
            "Early stopping:  0.010550949248428417\n",
            "Epoch: 105, Loss: 1.3373, Train: 0.5579, Test: 0.5204\n",
            "Early stopping:  0.010753071757738113\n",
            "Epoch: 106, Loss: 1.3360, Train: 0.5673, Test: 0.5278\n",
            "Early stopping:  0.009330398810624769\n",
            "PREDICTIONS -> tensor([ 9,  0, 11,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.58      0.53       190\n",
            "         capital_goods       0.37      0.14      0.20       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.48      0.31      0.38       198\n",
            " consumer_non-cyclical       0.67      0.34      0.45       112\n",
            "                energy       0.68      0.38      0.49        71\n",
            "             financial       0.64      0.55      0.59       192\n",
            "            healthcare       0.65      0.41      0.50        79\n",
            "              services       0.50      0.80      0.61       519\n",
            "            technology       0.49      0.27      0.35        99\n",
            "        transportation       0.64      0.61      0.63       101\n",
            "             utilities       0.58      0.61      0.59        56\n",
            "\n",
            "              accuracy                           0.53      1764\n",
            "             macro avg       0.51      0.42      0.44      1764\n",
            "          weighted avg       0.53      0.53      0.50      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 46.2419, Train: 0.1087, Test: 0.1088\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 803.0211, Train: 0.1069, Test: 0.1049\n",
            "Early stopping:  535.1236486015779\n",
            "Epoch: 003, Loss: 548.5078, Train: 0.0258, Test: 0.0193\n",
            "Early stopping:  385.08929938417884\n",
            "Epoch: 004, Loss: 331.8402, Train: 0.0852, Test: 0.0833\n",
            "Early stopping:  321.49199368926236\n",
            "Epoch: 005, Loss: 156.9652, Train: 0.1090, Test: 0.0913\n",
            "Early stopping:  304.4519564375853\n",
            "Epoch: 006, Loss: 52.0546, Train: 0.1415, Test: 0.1264\n",
            "Early stopping:  302.8787471022425\n",
            "Epoch: 007, Loss: 17.1891, Train: 0.1510, Test: 0.1406\n",
            "Early stopping:  220.0728653305294\n",
            "Epoch: 008, Loss: 10.4900, Train: 0.1704, Test: 0.1633\n",
            "Early stopping:  135.29951157171618\n",
            "Epoch: 009, Loss: 7.9110, Train: 0.1717, Test: 0.1746\n",
            "Early stopping:  62.94650185985652\n",
            "Epoch: 010, Loss: 6.0298, Train: 0.1628, Test: 0.1667\n",
            "Early stopping:  19.099414941686128\n",
            "Epoch: 011, Loss: 4.7755, Train: 0.1602, Test: 0.1627\n",
            "Early stopping:  4.917994878962689\n",
            "Epoch: 012, Loss: 3.8372, Train: 0.1774, Test: 0.1837\n",
            "Early stopping:  2.652233246420077\n",
            "Epoch: 013, Loss: 3.1281, Train: 0.2344, Test: 0.2409\n",
            "Early stopping:  1.8939427979283163\n",
            "Epoch: 014, Loss: 2.6070, Train: 0.2925, Test: 0.2914\n",
            "Early stopping:  1.3620091418495122\n",
            "Epoch: 015, Loss: 2.4835, Train: 0.3102, Test: 0.3039\n",
            "Early stopping:  0.9513310271208849\n",
            "Epoch: 016, Loss: 2.4906, Train: 0.3034, Test: 0.2965\n",
            "Early stopping:  0.5824197738243396\n",
            "Epoch: 017, Loss: 2.3813, Train: 0.3031, Test: 0.2988\n",
            "Early stopping:  0.2960732043875408\n",
            "Epoch: 018, Loss: 2.3124, Train: 0.3016, Test: 0.2988\n",
            "Early stopping:  0.11283276902208648\n",
            "Epoch: 019, Loss: 2.3140, Train: 0.3031, Test: 0.2988\n",
            "Early stopping:  0.08735527246126605\n",
            "Epoch: 020, Loss: 2.3081, Train: 0.3050, Test: 0.3010\n",
            "Early stopping:  0.07837583913847793\n",
            "Epoch: 021, Loss: 2.2958, Train: 0.3087, Test: 0.3027\n",
            "Early stopping:  0.03373796869162812\n",
            "Epoch: 022, Loss: 2.2809, Train: 0.3082, Test: 0.3039\n",
            "Early stopping:  0.01391534172797219\n",
            "Epoch: 023, Loss: 2.2694, Train: 0.3087, Test: 0.3061\n",
            "Early stopping:  0.018571715705870788\n",
            "Epoch: 024, Loss: 2.2674, Train: 0.3088, Test: 0.3078\n",
            "Early stopping:  0.017438814709937172\n",
            "Epoch: 025, Loss: 2.2553, Train: 0.3095, Test: 0.3084\n",
            "Early stopping:  0.015311629951576856\n",
            "Epoch: 026, Loss: 2.2374, Train: 0.3099, Test: 0.3073\n",
            "Early stopping:  0.0164980310347581\n",
            "Epoch: 027, Loss: 2.2268, Train: 0.3115, Test: 0.3078\n",
            "Early stopping:  0.018659294312094496\n",
            "Epoch: 028, Loss: 2.2162, Train: 0.3141, Test: 0.3101\n",
            "Early stopping:  0.020778307510544262\n",
            "Epoch: 029, Loss: 2.2010, Train: 0.3149, Test: 0.3101\n",
            "Early stopping:  0.020608380390303035\n",
            "Epoch: 030, Loss: 2.1822, Train: 0.3149, Test: 0.3084\n",
            "Early stopping:  0.021757935814651425\n",
            "Epoch: 031, Loss: 2.1678, Train: 0.3159, Test: 0.3095\n",
            "Early stopping:  0.02413243642757176\n",
            "Epoch: 032, Loss: 2.1555, Train: 0.3170, Test: 0.3107\n",
            "Early stopping:  0.024518753233143425\n",
            "Epoch: 033, Loss: 2.1381, Train: 0.3186, Test: 0.3112\n",
            "Early stopping:  0.024164520465053844\n",
            "Epoch: 034, Loss: 2.1214, Train: 0.3201, Test: 0.3112\n",
            "Early stopping:  0.0239494251901459\n",
            "Epoch: 035, Loss: 2.1095, Train: 0.3203, Test: 0.3129\n",
            "Early stopping:  0.023873406848755167\n",
            "Epoch: 036, Loss: 2.0941, Train: 0.3203, Test: 0.3124\n",
            "Early stopping:  0.023962193601023956\n",
            "Epoch: 037, Loss: 2.0783, Train: 0.3203, Test: 0.3107\n",
            "Early stopping:  0.023256266317372237\n",
            "Epoch: 038, Loss: 2.0703, Train: 0.3216, Test: 0.3112\n",
            "Early stopping:  0.021212222962882506\n",
            "Epoch: 039, Loss: 2.0571, Train: 0.3231, Test: 0.3118\n",
            "Early stopping:  0.020477508312581084\n",
            "Epoch: 040, Loss: 2.0485, Train: 0.3244, Test: 0.3118\n",
            "Early stopping:  0.017884474801167085\n",
            "Epoch: 041, Loss: 2.0434, Train: 0.3247, Test: 0.3118\n",
            "Early stopping:  0.014616602890180004\n",
            "Epoch: 042, Loss: 2.0331, Train: 0.3257, Test: 0.3129\n",
            "Early stopping:  0.014060704602954619\n",
            "Epoch: 043, Loss: 2.0260, Train: 0.3257, Test: 0.3129\n",
            "Early stopping:  0.01231964440336854\n",
            "Epoch: 044, Loss: 2.0131, Train: 0.3262, Test: 0.3141\n",
            "Early stopping:  0.014104988221806447\n",
            "Epoch: 045, Loss: 2.0023, Train: 0.3275, Test: 0.3169\n",
            "Early stopping:  0.016212696180705748\n",
            "Epoch: 046, Loss: 1.9934, Train: 0.3272, Test: 0.3169\n",
            "Early stopping:  0.016341475497668293\n",
            "Epoch: 047, Loss: 1.9848, Train: 0.3271, Test: 0.3169\n",
            "Early stopping:  0.016192321718024087\n",
            "Epoch: 048, Loss: 1.9797, Train: 0.3267, Test: 0.3169\n",
            "Early stopping:  0.013447476364858407\n",
            "Epoch: 049, Loss: 1.9741, Train: 0.3260, Test: 0.3180\n",
            "Early stopping:  0.011198693593750588\n",
            "Epoch: 050, Loss: 1.9670, Train: 0.3261, Test: 0.3192\n",
            "Early stopping:  0.010072010765540637\n",
            "Epoch: 051, Loss: 1.9579, Train: 0.3265, Test: 0.3197\n",
            "Early stopping:  0.0105778902373103\n",
            "Epoch: 052, Loss: 1.9485, Train: 0.3260, Test: 0.3192\n",
            "Early stopping:  0.01247549390011834\n",
            "Epoch: 053, Loss: 1.9415, Train: 0.3264, Test: 0.3197\n",
            "Early stopping:  0.013259205990389562\n",
            "Epoch: 054, Loss: 1.9343, Train: 0.3268, Test: 0.3209\n",
            "Early stopping:  0.012973249282030917\n",
            "Epoch: 055, Loss: 1.9253, Train: 0.3281, Test: 0.3231\n",
            "Early stopping:  0.012565540123473266\n",
            "Epoch: 056, Loss: 1.9162, Train: 0.3285, Test: 0.3231\n",
            "Early stopping:  0.012788530479238976\n",
            "Epoch: 057, Loss: 1.9066, Train: 0.3309, Test: 0.3265\n",
            "Early stopping:  0.013914489978061394\n",
            "Epoch: 058, Loss: 1.8971, Train: 0.3345, Test: 0.3277\n",
            "Early stopping:  0.014735682456986985\n",
            "Epoch: 059, Loss: 1.8907, Train: 0.3356, Test: 0.3311\n",
            "Early stopping:  0.01401562150145174\n",
            "Epoch: 060, Loss: 1.8830, Train: 0.3372, Test: 0.3277\n",
            "Early stopping:  0.013052471818231227\n",
            "Epoch: 061, Loss: 1.8747, Train: 0.3372, Test: 0.3288\n",
            "Early stopping:  0.012327927166028346\n",
            "Epoch: 062, Loss: 1.8671, Train: 0.3365, Test: 0.3311\n",
            "Early stopping:  0.012037446766173993\n",
            "Epoch: 063, Loss: 1.8594, Train: 0.3352, Test: 0.3305\n",
            "Early stopping:  0.012436478967684891\n",
            "Epoch: 064, Loss: 1.8518, Train: 0.3335, Test: 0.3282\n",
            "Early stopping:  0.0123050013200958\n",
            "Epoch: 065, Loss: 1.8434, Train: 0.3322, Test: 0.3288\n",
            "Early stopping:  0.012297358835927938\n",
            "Epoch: 066, Loss: 1.8366, Train: 0.3363, Test: 0.3345\n",
            "Early stopping:  0.012165281347087185\n",
            "Epoch: 067, Loss: 1.8270, Train: 0.3435, Test: 0.3350\n",
            "Early stopping:  0.0126486418937439\n",
            "Epoch: 068, Loss: 1.8185, Train: 0.3492, Test: 0.3384\n",
            "Early stopping:  0.01314101487327504\n",
            "Epoch: 069, Loss: 1.8100, Train: 0.3506, Test: 0.3424\n",
            "Early stopping:  0.013434382781592383\n",
            "Epoch: 070, Loss: 1.8009, Train: 0.3532, Test: 0.3509\n",
            "Early stopping:  0.013970189818252902\n",
            "Epoch: 071, Loss: 1.7930, Train: 0.3579, Test: 0.3515\n",
            "Early stopping:  0.013528960764379198\n",
            "Epoch: 072, Loss: 1.7836, Train: 0.3623, Test: 0.3571\n",
            "Early stopping:  0.013730721866521255\n",
            "Epoch: 073, Loss: 1.7734, Train: 0.3672, Test: 0.3611\n",
            "Early stopping:  0.01431845849466781\n",
            "Epoch: 074, Loss: 1.7635, Train: 0.3725, Test: 0.3651\n",
            "Early stopping:  0.01492800417771251\n",
            "Epoch: 075, Loss: 1.7543, Train: 0.3746, Test: 0.3696\n",
            "Early stopping:  0.015411189479885606\n",
            "Epoch: 076, Loss: 1.7448, Train: 0.3803, Test: 0.3724\n",
            "Early stopping:  0.01529963904033449\n",
            "Epoch: 077, Loss: 1.7359, Train: 0.3854, Test: 0.3719\n",
            "Early stopping:  0.014858066968269915\n",
            "Epoch: 078, Loss: 1.7274, Train: 0.3934, Test: 0.3719\n",
            "Early stopping:  0.014360398709445584\n",
            "Epoch: 079, Loss: 1.7204, Train: 0.3970, Test: 0.3810\n",
            "Early stopping:  0.013507519168403068\n",
            "Epoch: 080, Loss: 1.7132, Train: 0.4017, Test: 0.3798\n",
            "Early stopping:  0.012439469921385208\n",
            "Epoch: 081, Loss: 1.7055, Train: 0.4073, Test: 0.3804\n",
            "Early stopping:  0.011846406765872494\n",
            "Epoch: 082, Loss: 1.6979, Train: 0.4089, Test: 0.3849\n",
            "Early stopping:  0.011679662577116012\n",
            "Epoch: 083, Loss: 1.6910, Train: 0.4116, Test: 0.3866\n",
            "Early stopping:  0.011712500095870234\n",
            "Epoch: 084, Loss: 1.6831, Train: 0.4127, Test: 0.3889\n",
            "Early stopping:  0.011818191025749644\n",
            "Epoch: 085, Loss: 1.6770, Train: 0.4160, Test: 0.3997\n",
            "Early stopping:  0.011354458426611326\n",
            "Epoch: 086, Loss: 1.6693, Train: 0.4177, Test: 0.4065\n",
            "Early stopping:  0.011275389823316091\n",
            "Epoch: 087, Loss: 1.6628, Train: 0.4195, Test: 0.4053\n",
            "Early stopping:  0.011108398190405279\n",
            "Epoch: 088, Loss: 1.6546, Train: 0.4215, Test: 0.4065\n",
            "Early stopping:  0.011277180971402227\n",
            "Epoch: 089, Loss: 1.6487, Train: 0.4244, Test: 0.4093\n",
            "Early stopping:  0.011272710452692587\n",
            "Epoch: 090, Loss: 1.6413, Train: 0.4266, Test: 0.4127\n",
            "Early stopping:  0.01108128960778377\n",
            "Epoch: 091, Loss: 1.6344, Train: 0.4275, Test: 0.4138\n",
            "Early stopping:  0.011112091347478174\n",
            "Epoch: 092, Loss: 1.6283, Train: 0.4293, Test: 0.4127\n",
            "Early stopping:  0.010597040770007287\n",
            "Epoch: 093, Loss: 1.6204, Train: 0.4319, Test: 0.4206\n",
            "Early stopping:  0.011034896941701242\n",
            "Epoch: 094, Loss: 1.6167, Train: 0.4323, Test: 0.4167\n",
            "Early stopping:  0.01004182573109606\n",
            "Epoch: 095, Loss: 1.6131, Train: 0.4340, Test: 0.4172\n",
            "Early stopping:  0.008674459454866706\n",
            "PREDICTIONS -> tensor([8, 8, 8,  ..., 8, 8, 8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.48      0.15      0.23       190\n",
            "         capital_goods       0.00      0.00      0.00       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.32      0.64      0.43       198\n",
            " consumer_non-cyclical       0.74      0.18      0.29       112\n",
            "                energy       0.00      0.00      0.00        71\n",
            "             financial       0.72      0.44      0.54       192\n",
            "            healthcare       0.72      0.48      0.58        79\n",
            "              services       0.39      0.84      0.53       519\n",
            "            technology       0.00      0.00      0.00        99\n",
            "        transportation       0.50      0.02      0.04       101\n",
            "             utilities       0.40      0.04      0.07        56\n",
            "\n",
            "              accuracy                           0.42      1764\n",
            "             macro avg       0.36      0.23      0.23      1764\n",
            "          weighted avg       0.40      0.42      0.34      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 55.1530, Train: 0.0777, Test: 0.0754\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 242.6806, Train: 0.1126, Test: 0.1156\n",
            "Early stopping:  132.60204254400875\n",
            "Epoch: 003, Loss: 324.1827, Train: 0.2963, Test: 0.2937\n",
            "Early stopping:  137.95299453981295\n",
            "Epoch: 004, Loss: 285.5003, Train: 0.1442, Test: 0.1395\n",
            "Early stopping:  119.22524268920957\n",
            "Epoch: 005, Loss: 335.7411, Train: 0.2334, Test: 0.2245\n",
            "Early stopping:  114.15419572429424\n",
            "Epoch: 006, Loss: 271.6758, Train: 0.0906, Test: 0.0822\n",
            "Early stopping:  38.19923631237197\n",
            "Epoch: 007, Loss: 354.8021, Train: 0.1489, Test: 0.1474\n",
            "Early stopping:  34.79950456330609\n",
            "Epoch: 008, Loss: 312.6161, Train: 0.3031, Test: 0.2931\n",
            "Early stopping:  34.36674400412198\n",
            "Epoch: 009, Loss: 305.7869, Train: 0.1636, Test: 0.1593\n",
            "Early stopping:  31.526301097366343\n",
            "Epoch: 010, Loss: 307.1459, Train: 0.2398, Test: 0.2347\n",
            "Early stopping:  29.613719971338313\n",
            "Epoch: 011, Loss: 235.2035, Train: 0.2681, Test: 0.2630\n",
            "Early stopping:  43.003448696537845\n",
            "Epoch: 012, Loss: 204.4415, Train: 0.3132, Test: 0.3073\n",
            "Early stopping:  49.84776473575783\n",
            "Epoch: 013, Loss: 159.0845, Train: 0.1085, Test: 0.1083\n",
            "Early stopping:  64.50583074689774\n",
            "Epoch: 014, Loss: 132.9993, Train: 0.1001, Test: 0.0998\n",
            "Early stopping:  68.1912418639887\n",
            "Epoch: 015, Loss: 100.9298, Train: 0.0943, Test: 0.0924\n",
            "Early stopping:  53.93621678425464\n",
            "Epoch: 016, Loss: 76.1939, Train: 0.2264, Test: 0.2103\n",
            "Early stopping:  50.06186990382514\n",
            "Epoch: 017, Loss: 43.6043, Train: 0.3071, Test: 0.3022\n",
            "Early stopping:  45.53892807645568\n",
            "Epoch: 018, Loss: 32.9291, Train: 0.3572, Test: 0.3418\n",
            "Early stopping:  41.12836198332245\n",
            "Epoch: 019, Loss: 23.8001, Train: 0.3705, Test: 0.3566\n",
            "Early stopping:  32.20224903002662\n",
            "Epoch: 020, Loss: 12.9953, Train: 0.3349, Test: 0.3311\n",
            "Early stopping:  24.20083835899062\n",
            "Epoch: 021, Loss: 6.9228, Train: 0.2508, Test: 0.2421\n",
            "Early stopping:  14.805552820491203\n",
            "Epoch: 022, Loss: 5.6260, Train: 0.2138, Test: 0.2035\n",
            "Early stopping:  11.677379172506082\n",
            "Epoch: 023, Loss: 5.2332, Train: 0.1986, Test: 0.1859\n",
            "Early stopping:  7.850895366935785\n",
            "Epoch: 024, Loss: 5.0129, Train: 0.2125, Test: 0.1984\n",
            "Early stopping:  3.345991856055777\n",
            "Epoch: 025, Loss: 4.4846, Train: 0.2331, Test: 0.2183\n",
            "Early stopping:  0.9178996729496582\n",
            "Epoch: 026, Loss: 3.7181, Train: 0.2946, Test: 0.2902\n",
            "Early stopping:  0.7389424599381595\n",
            "Epoch: 027, Loss: 3.0457, Train: 0.3628, Test: 0.3673\n",
            "Early stopping:  0.911735143468814\n",
            "Epoch: 028, Loss: 2.6818, Train: 0.3969, Test: 0.3878\n",
            "Early stopping:  0.9702121564133106\n",
            "Epoch: 029, Loss: 2.5088, Train: 0.4032, Test: 0.3940\n",
            "Early stopping:  0.8139614421846332\n",
            "Epoch: 030, Loss: 2.4097, Train: 0.4021, Test: 0.3889\n",
            "Early stopping:  0.5309951845322933\n",
            "Epoch: 031, Loss: 2.3264, Train: 0.4014, Test: 0.3957\n",
            "Early stopping:  0.28485390531959365\n",
            "Epoch: 032, Loss: 2.2472, Train: 0.4056, Test: 0.3951\n",
            "Early stopping:  0.16885842556486394\n",
            "Epoch: 033, Loss: 2.1858, Train: 0.3981, Test: 0.3764\n",
            "Early stopping:  0.12830420692216812\n",
            "Epoch: 034, Loss: 2.1466, Train: 0.3818, Test: 0.3594\n",
            "Early stopping:  0.10643343005480368\n",
            "Epoch: 035, Loss: 2.1225, Train: 0.3718, Test: 0.3509\n",
            "Early stopping:  0.08229481727620672\n",
            "Epoch: 036, Loss: 2.1034, Train: 0.3701, Test: 0.3464\n",
            "Early stopping:  0.05712428968985038\n",
            "Epoch: 037, Loss: 2.0866, Train: 0.3740, Test: 0.3475\n",
            "Early stopping:  0.03879635878657495\n",
            "Epoch: 038, Loss: 2.0737, Train: 0.3800, Test: 0.3532\n",
            "Early stopping:  0.028923274781437736\n",
            "Epoch: 039, Loss: 2.0650, Train: 0.3902, Test: 0.3651\n",
            "Early stopping:  0.023150670491332737\n",
            "Epoch: 040, Loss: 2.0566, Train: 0.4028, Test: 0.3759\n",
            "Early stopping:  0.01846098240111047\n",
            "Epoch: 041, Loss: 2.0448, Train: 0.4044, Test: 0.3872\n",
            "Early stopping:  0.015979410146376497\n",
            "Epoch: 042, Loss: 2.0288, Train: 0.4095, Test: 0.3917\n",
            "Early stopping:  0.017550271138025188\n",
            "Epoch: 043, Loss: 2.0207, Train: 0.4151, Test: 0.4042\n",
            "Early stopping:  0.018504581378850166\n",
            "Epoch: 044, Loss: 2.0009, Train: 0.4244, Test: 0.4070\n",
            "Early stopping:  0.02158212518818962\n",
            "Epoch: 045, Loss: 1.9820, Train: 0.4246, Test: 0.4099\n",
            "Early stopping:  0.02448742076056656\n",
            "Epoch: 046, Loss: 1.9670, Train: 0.4212, Test: 0.4059\n",
            "Early stopping:  0.025839658092742135\n",
            "Epoch: 047, Loss: 1.9509, Train: 0.4218, Test: 0.4104\n",
            "Early stopping:  0.027458954607722512\n",
            "Epoch: 048, Loss: 1.9353, Train: 0.4231, Test: 0.4184\n",
            "Early stopping:  0.025680197864724037\n",
            "Epoch: 049, Loss: 1.9181, Train: 0.4252, Test: 0.4167\n",
            "Early stopping:  0.025225744654411447\n",
            "Epoch: 050, Loss: 1.8995, Train: 0.4262, Test: 0.4240\n",
            "Early stopping:  0.026550397747819133\n",
            "Epoch: 051, Loss: 1.8817, Train: 0.4305, Test: 0.4314\n",
            "Early stopping:  0.027557925665124623\n",
            "Epoch: 052, Loss: 1.8606, Train: 0.4378, Test: 0.4286\n",
            "Early stopping:  0.029393116536447873\n",
            "Epoch: 053, Loss: 1.8374, Train: 0.4458, Test: 0.4371\n",
            "Early stopping:  0.03172161939592278\n",
            "Epoch: 054, Loss: 1.8135, Train: 0.4543, Test: 0.4433\n",
            "Early stopping:  0.03424666803992872\n",
            "Epoch: 055, Loss: 1.7890, Train: 0.4577, Test: 0.4393\n",
            "Early stopping:  0.036785228149271355\n",
            "Epoch: 056, Loss: 1.7678, Train: 0.4625, Test: 0.4365\n",
            "Early stopping:  0.037019127322814205\n",
            "Epoch: 057, Loss: 1.7443, Train: 0.4651, Test: 0.4393\n",
            "Early stopping:  0.03667272489429775\n",
            "Epoch: 058, Loss: 1.7222, Train: 0.4677, Test: 0.4427\n",
            "Early stopping:  0.03593665953747505\n",
            "Epoch: 059, Loss: 1.7035, Train: 0.4737, Test: 0.4427\n",
            "Early stopping:  0.034269928917510475\n",
            "Epoch: 060, Loss: 1.6848, Train: 0.4761, Test: 0.4473\n",
            "Early stopping:  0.03274781718189851\n",
            "Epoch: 061, Loss: 1.6714, Train: 0.4846, Test: 0.4518\n",
            "Early stopping:  0.029081181886230035\n",
            "Epoch: 062, Loss: 1.6560, Train: 0.4900, Test: 0.4620\n",
            "Early stopping:  0.026068094938867147\n",
            "Epoch: 063, Loss: 1.6402, Train: 0.4943, Test: 0.4688\n",
            "Early stopping:  0.02457681881136545\n",
            "Epoch: 064, Loss: 1.6230, Train: 0.4995, Test: 0.4756\n",
            "Early stopping:  0.024489422921357067\n",
            "Epoch: 065, Loss: 1.6061, Train: 0.5021, Test: 0.4728\n",
            "Early stopping:  0.025852973746679053\n",
            "Epoch: 066, Loss: 1.5896, Train: 0.5050, Test: 0.4819\n",
            "Early stopping:  0.026401193042452958\n",
            "Epoch: 067, Loss: 1.5757, Train: 0.5082, Test: 0.4841\n",
            "Early stopping:  0.025707359303522137\n",
            "Epoch: 068, Loss: 1.5635, Train: 0.5108, Test: 0.4841\n",
            "Early stopping:  0.02368811284786483\n",
            "Epoch: 069, Loss: 1.5518, Train: 0.5118, Test: 0.4898\n",
            "Early stopping:  0.02137316790975258\n",
            "Epoch: 070, Loss: 1.5405, Train: 0.5169, Test: 0.5000\n",
            "Early stopping:  0.019326966822829916\n",
            "Epoch: 071, Loss: 1.5298, Train: 0.5195, Test: 0.5028\n",
            "Early stopping:  0.018175836275536147\n",
            "Epoch: 072, Loss: 1.5194, Train: 0.5249, Test: 0.5045\n",
            "Early stopping:  0.017411733758802252\n",
            "Epoch: 073, Loss: 1.5093, Train: 0.5290, Test: 0.5051\n",
            "Early stopping:  0.016752637249583858\n",
            "Epoch: 074, Loss: 1.4981, Train: 0.5327, Test: 0.5057\n",
            "Early stopping:  0.016654870668724703\n",
            "Epoch: 075, Loss: 1.4874, Train: 0.5354, Test: 0.5142\n",
            "Early stopping:  0.01679859678592147\n",
            "Epoch: 076, Loss: 1.4767, Train: 0.5402, Test: 0.5193\n",
            "Early stopping:  0.016988088724329765\n",
            "Epoch: 077, Loss: 1.4665, Train: 0.5439, Test: 0.5153\n",
            "Early stopping:  0.01690594361864813\n",
            "Epoch: 078, Loss: 1.4560, Train: 0.5440, Test: 0.5119\n",
            "Early stopping:  0.01659105702661823\n",
            "Epoch: 079, Loss: 1.4457, Train: 0.5470, Test: 0.5119\n",
            "Early stopping:  0.016453556020674703\n",
            "Epoch: 080, Loss: 1.4359, Train: 0.5511, Test: 0.5176\n",
            "Early stopping:  0.01620914191331171\n",
            "Epoch: 081, Loss: 1.4264, Train: 0.5549, Test: 0.5159\n",
            "Early stopping:  0.015888697721020386\n",
            "Epoch: 082, Loss: 1.4172, Train: 0.5530, Test: 0.5164\n",
            "Early stopping:  0.01533611655046773\n",
            "Epoch: 083, Loss: 1.4081, Train: 0.5576, Test: 0.5181\n",
            "Early stopping:  0.014845639913618798\n",
            "Epoch: 084, Loss: 1.3994, Train: 0.5602, Test: 0.5227\n",
            "Early stopping:  0.014416662026143808\n",
            "Epoch: 085, Loss: 1.3907, Train: 0.5654, Test: 0.5272\n",
            "Early stopping:  0.014079373191854252\n",
            "Epoch: 086, Loss: 1.3831, Train: 0.5649, Test: 0.5255\n",
            "Early stopping:  0.01352982903712126\n",
            "Epoch: 087, Loss: 1.3765, Train: 0.5673, Test: 0.5232\n",
            "Early stopping:  0.012591927236918593\n",
            "Epoch: 088, Loss: 1.3696, Train: 0.5704, Test: 0.5255\n",
            "Early stopping:  0.011710357545111764\n",
            "Epoch: 089, Loss: 1.3590, Train: 0.5710, Test: 0.5255\n",
            "Early stopping:  0.012234225856218242\n",
            "Epoch: 090, Loss: 1.3537, Train: 0.5742, Test: 0.5300\n",
            "Early stopping:  0.01213147992531357\n",
            "Epoch: 091, Loss: 1.3485, Train: 0.5762, Test: 0.5340\n",
            "Early stopping:  0.011470240486346454\n",
            "Epoch: 092, Loss: 1.3378, Train: 0.5788, Test: 0.5374\n",
            "Early stopping:  0.011834019472845144\n",
            "Epoch: 093, Loss: 1.3317, Train: 0.5802, Test: 0.5374\n",
            "Early stopping:  0.011246789454574531\n",
            "Epoch: 094, Loss: 1.3228, Train: 0.5846, Test: 0.5397\n",
            "Early stopping:  0.012481978882624043\n",
            "Epoch: 095, Loss: 1.3146, Train: 0.5846, Test: 0.5476\n",
            "Early stopping:  0.013118939466159487\n",
            "Epoch: 096, Loss: 1.3089, Train: 0.5890, Test: 0.5420\n",
            "Early stopping:  0.011850967134463736\n",
            "Epoch: 097, Loss: 1.3005, Train: 0.5915, Test: 0.5448\n",
            "Early stopping:  0.012086211320359393\n",
            "Epoch: 098, Loss: 1.2961, Train: 0.5927, Test: 0.5465\n",
            "Early stopping:  0.010739088457309226\n",
            "Epoch: 099, Loss: 1.2858, Train: 0.5944, Test: 0.5431\n",
            "Early stopping:  0.01122794016750368\n",
            "Epoch: 100, Loss: 1.2813, Train: 0.5992, Test: 0.5505\n",
            "Early stopping:  0.011143646242719569\n",
            "Epoch: 101, Loss: 1.2725, Train: 0.6031, Test: 0.5561\n",
            "Early stopping:  0.011275916533667212\n",
            "Epoch: 102, Loss: 1.2668, Train: 0.6039, Test: 0.5539\n",
            "Early stopping:  0.011435841053205137\n",
            "Epoch: 103, Loss: 1.2633, Train: 0.6039, Test: 0.5539\n",
            "Early stopping:  0.009497335399401605\n",
            "PREDICTIONS -> tensor([ 9,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.51      0.59      0.55       190\n",
            "         capital_goods       0.60      0.19      0.29       127\n",
            "conglomerates_industry       0.00      0.00      0.00        20\n",
            "     consumer_cyclical       0.52      0.51      0.51       198\n",
            " consumer_non-cyclical       0.71      0.29      0.41       112\n",
            "                energy       0.60      0.46      0.52        71\n",
            "             financial       0.71      0.52      0.60       192\n",
            "            healthcare       0.69      0.54      0.61        79\n",
            "              services       0.51      0.81      0.63       519\n",
            "            technology       0.48      0.21      0.29        99\n",
            "        transportation       0.70      0.53      0.61       101\n",
            "             utilities       0.61      0.62      0.62        56\n",
            "\n",
            "              accuracy                           0.55      1764\n",
            "             macro avg       0.55      0.44      0.47      1764\n",
            "          weighted avg       0.57      0.55      0.53      1764\n",
            "\n",
            "time: 2min 33s (started: 2024-10-16 21:50:08 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GCN',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZHEkLMegWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c494194-728c-4fb7-bb86-070288b07f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 518 ms (started: 2024-10-16 21:52:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzR84Wt2gWjM"
      },
      "source": [
        "#### Second Network - GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPcoCua_gWjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1286811b-6ec6-4bbf-cfec-fba0900cfbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================================\n",
            "=================== MODEL 0 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5506, Train: 0.1177, Test: 0.1179\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3214, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  0.16208447249313326\n",
            "Epoch: 003, Loss: 2.1771, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.18837555777004578\n",
            "Epoch: 004, Loss: 2.1539, Train: 0.2970, Test: 0.2954\n",
            "Early stopping:  0.1823264740585029\n",
            "Epoch: 005, Loss: 2.0988, Train: 0.3190, Test: 0.3169\n",
            "Early stopping:  0.18189517321614337\n",
            "Epoch: 006, Loss: 2.0155, Train: 0.3774, Test: 0.3668\n",
            "Early stopping:  0.11265235905501024\n",
            "Epoch: 007, Loss: 1.9623, Train: 0.4029, Test: 0.3912\n",
            "Early stopping:  0.09113934759687246\n",
            "Epoch: 008, Loss: 1.9161, Train: 0.4173, Test: 0.3997\n",
            "Early stopping:  0.09727552699981093\n",
            "Epoch: 009, Loss: 1.8601, Train: 0.4255, Test: 0.4155\n",
            "Early stopping:  0.0917943548574747\n",
            "Epoch: 010, Loss: 1.7982, Train: 0.4266, Test: 0.4150\n",
            "Early stopping:  0.08499114633131459\n",
            "Epoch: 011, Loss: 1.7373, Train: 0.4322, Test: 0.4189\n",
            "Early stopping:  0.08993787949295139\n",
            "Epoch: 012, Loss: 1.6799, Train: 0.4431, Test: 0.4342\n",
            "Early stopping:  0.09411575533013983\n",
            "Epoch: 013, Loss: 1.6246, Train: 0.4642, Test: 0.4495\n",
            "Early stopping:  0.0932095653186466\n",
            "Epoch: 014, Loss: 1.5700, Train: 0.4867, Test: 0.4745\n",
            "Early stopping:  0.08999375034211396\n",
            "Epoch: 015, Loss: 1.5162, Train: 0.5195, Test: 0.5040\n",
            "Early stopping:  0.08728275899462408\n",
            "Epoch: 016, Loss: 1.4655, Train: 0.5520, Test: 0.5408\n",
            "Early stopping:  0.0849400834315132\n",
            "Epoch: 017, Loss: 1.4181, Train: 0.5775, Test: 0.5584\n",
            "Early stopping:  0.08186680983509999\n",
            "Epoch: 018, Loss: 1.3721, Train: 0.5880, Test: 0.5703\n",
            "Early stopping:  0.07813355368959748\n",
            "Epoch: 019, Loss: 1.3284, Train: 0.5955, Test: 0.5788\n",
            "Early stopping:  0.07419516380282695\n",
            "Epoch: 020, Loss: 1.2883, Train: 0.6031, Test: 0.5811\n",
            "Early stopping:  0.07027158455227382\n",
            "Epoch: 021, Loss: 1.2512, Train: 0.6114, Test: 0.5924\n",
            "Early stopping:  0.06609907805333198\n",
            "Epoch: 022, Loss: 1.2165, Train: 0.6257, Test: 0.5998\n",
            "Early stopping:  0.06148650154017472\n",
            "Epoch: 023, Loss: 1.1850, Train: 0.6404, Test: 0.6105\n",
            "Early stopping:  0.056751038766301096\n",
            "Epoch: 024, Loss: 1.1574, Train: 0.6471, Test: 0.6139\n",
            "Early stopping:  0.051952701668145564\n",
            "Epoch: 025, Loss: 1.1317, Train: 0.6515, Test: 0.6190\n",
            "Early stopping:  0.047230860163882636\n",
            "Epoch: 026, Loss: 1.1073, Train: 0.6545, Test: 0.6264\n",
            "Early stopping:  0.04302333554998399\n",
            "Epoch: 027, Loss: 1.0851, Train: 0.6586, Test: 0.6287\n",
            "Early stopping:  0.039549979026091114\n",
            "Epoch: 028, Loss: 1.0642, Train: 0.6640, Test: 0.6321\n",
            "Early stopping:  0.03687948625773593\n",
            "Epoch: 029, Loss: 1.0424, Train: 0.6726, Test: 0.6355\n",
            "Early stopping:  0.03506432868511924\n",
            "Epoch: 030, Loss: 1.0211, Train: 0.6823, Test: 0.6395\n",
            "Early stopping:  0.034009399820399334\n",
            "Epoch: 031, Loss: 1.0010, Train: 0.6913, Test: 0.6440\n",
            "Early stopping:  0.03340754371174171\n",
            "Epoch: 032, Loss: 0.9814, Train: 0.6974, Test: 0.6497\n",
            "Early stopping:  0.03273321684968726\n",
            "Epoch: 033, Loss: 0.9622, Train: 0.7027, Test: 0.6565\n",
            "Early stopping:  0.031660583275310204\n",
            "Epoch: 034, Loss: 0.9440, Train: 0.7085, Test: 0.6587\n",
            "Early stopping:  0.03052543950228101\n",
            "Epoch: 035, Loss: 0.9267, Train: 0.7127, Test: 0.6616\n",
            "Early stopping:  0.029420024328173488\n",
            "Epoch: 036, Loss: 0.9099, Train: 0.7200, Test: 0.6746\n",
            "Early stopping:  0.02823667382807918\n",
            "Epoch: 037, Loss: 0.8931, Train: 0.7298, Test: 0.6859\n",
            "Early stopping:  0.0272216836114726\n",
            "Epoch: 038, Loss: 0.8771, Train: 0.7368, Test: 0.6882\n",
            "Early stopping:  0.026452871951605603\n",
            "Epoch: 039, Loss: 0.8621, Train: 0.7424, Test: 0.6882\n",
            "Early stopping:  0.025612412273250555\n",
            "Epoch: 040, Loss: 0.8470, Train: 0.7482, Test: 0.6950\n",
            "Early stopping:  0.024817119719129463\n",
            "Epoch: 041, Loss: 0.8318, Train: 0.7563, Test: 0.6967\n",
            "Early stopping:  0.024156125874182543\n",
            "Epoch: 042, Loss: 0.8176, Train: 0.7612, Test: 0.6956\n",
            "Early stopping:  0.023597692882136017\n",
            "Epoch: 043, Loss: 0.8029, Train: 0.7676, Test: 0.6990\n",
            "Early stopping:  0.023358670516713868\n",
            "Epoch: 044, Loss: 0.7881, Train: 0.7707, Test: 0.7058\n",
            "Early stopping:  0.023188749635610635\n",
            "Epoch: 045, Loss: 0.7739, Train: 0.7758, Test: 0.7103\n",
            "Early stopping:  0.02297570403390164\n",
            "Epoch: 046, Loss: 0.7593, Train: 0.7812, Test: 0.7092\n",
            "Early stopping:  0.023025170423984543\n",
            "Epoch: 047, Loss: 0.7448, Train: 0.7859, Test: 0.7109\n",
            "Early stopping:  0.022928531457056214\n",
            "Epoch: 048, Loss: 0.7309, Train: 0.7896, Test: 0.7154\n",
            "Early stopping:  0.022675839682155585\n",
            "Epoch: 049, Loss: 0.7170, Train: 0.7954, Test: 0.7183\n",
            "Early stopping:  0.022504183111706863\n",
            "Epoch: 050, Loss: 0.7032, Train: 0.8018, Test: 0.7239\n",
            "Early stopping:  0.02214546832593707\n",
            "Epoch: 051, Loss: 0.6896, Train: 0.8067, Test: 0.7268\n",
            "Early stopping:  0.021831330633685232\n",
            "Epoch: 052, Loss: 0.6763, Train: 0.8093, Test: 0.7273\n",
            "Early stopping:  0.021589823620030115\n",
            "Epoch: 053, Loss: 0.6635, Train: 0.8137, Test: 0.7290\n",
            "Early stopping:  0.021162691355440726\n",
            "Epoch: 054, Loss: 0.6525, Train: 0.8097, Test: 0.7279\n",
            "Early stopping:  0.020171308694000557\n",
            "Epoch: 055, Loss: 0.6454, Train: 0.8162, Test: 0.7313\n",
            "Early stopping:  0.01786130451579586\n",
            "Epoch: 056, Loss: 0.6393, Train: 0.8211, Test: 0.7358\n",
            "Early stopping:  0.014746283988892715\n",
            "Epoch: 057, Loss: 0.6182, Train: 0.8300, Test: 0.7421\n",
            "Early stopping:  0.016897666013808252\n",
            "Epoch: 058, Loss: 0.6068, Train: 0.8257, Test: 0.7302\n",
            "Early stopping:  0.019224478780139227\n",
            "Epoch: 059, Loss: 0.6035, Train: 0.8365, Test: 0.7477\n",
            "Early stopping:  0.01892894547921057\n",
            "Epoch: 060, Loss: 0.5845, Train: 0.8385, Test: 0.7449\n",
            "Early stopping:  0.020162386818745274\n",
            "Epoch: 061, Loss: 0.5769, Train: 0.8406, Test: 0.7392\n",
            "Early stopping:  0.01688302406299672\n",
            "Epoch: 062, Loss: 0.5707, Train: 0.8473, Test: 0.7528\n",
            "Early stopping:  0.016026538246931503\n",
            "Epoch: 063, Loss: 0.5545, Train: 0.8456, Test: 0.7477\n",
            "Early stopping:  0.01803358520996633\n",
            "Epoch: 064, Loss: 0.5512, Train: 0.8551, Test: 0.7477\n",
            "Early stopping:  0.014355523318125677\n",
            "Epoch: 065, Loss: 0.5412, Train: 0.8513, Test: 0.7534\n",
            "Early stopping:  0.014631114292531406\n",
            "Epoch: 066, Loss: 0.5306, Train: 0.8585, Test: 0.7551\n",
            "Early stopping:  0.015020736172518414\n",
            "Epoch: 067, Loss: 0.5281, Train: 0.8567, Test: 0.7534\n",
            "Early stopping:  0.011827443929795666\n",
            "Epoch: 068, Loss: 0.5179, Train: 0.8646, Test: 0.7613\n",
            "Early stopping:  0.012771198767161486\n",
            "Epoch: 069, Loss: 0.5100, Train: 0.8618, Test: 0.7534\n",
            "Early stopping:  0.012021046026001575\n",
            "Epoch: 070, Loss: 0.4985, Train: 0.8694, Test: 0.7562\n",
            "Early stopping:  0.013249954166969604\n",
            "Epoch: 071, Loss: 0.4897, Train: 0.8714, Test: 0.7670\n",
            "Early stopping:  0.015255477971618409\n",
            "Epoch: 072, Loss: 0.4791, Train: 0.8684, Test: 0.7613\n",
            "Early stopping:  0.015497816223817315\n",
            "Epoch: 073, Loss: 0.4751, Train: 0.8778, Test: 0.7642\n",
            "Early stopping:  0.014211852313669794\n",
            "Epoch: 074, Loss: 0.4683, Train: 0.8769, Test: 0.7653\n",
            "Early stopping:  0.011984508007271135\n",
            "Epoch: 075, Loss: 0.4590, Train: 0.8818, Test: 0.7653\n",
            "Early stopping:  0.011531259968875308\n",
            "Epoch: 076, Loss: 0.4533, Train: 0.8840, Test: 0.7704\n",
            "Early stopping:  0.010821022897614075\n",
            "Epoch: 077, Loss: 0.4414, Train: 0.8880, Test: 0.7704\n",
            "Early stopping:  0.013120188682785711\n",
            "Epoch: 078, Loss: 0.4358, Train: 0.8922, Test: 0.7670\n",
            "Early stopping:  0.01311566284900672\n",
            "Epoch: 079, Loss: 0.4275, Train: 0.8914, Test: 0.7670\n",
            "Early stopping:  0.012797397503361566\n",
            "Epoch: 080, Loss: 0.4228, Train: 0.8965, Test: 0.7664\n",
            "Early stopping:  0.011986230765060267\n",
            "Epoch: 081, Loss: 0.4166, Train: 0.8920, Test: 0.7670\n",
            "Early stopping:  0.009925667293564778\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.79      0.69      0.74       190\n",
            "         capital_goods       0.85      0.65      0.74       127\n",
            "conglomerates_industry       1.00      0.75      0.86        20\n",
            "     consumer_cyclical       0.71      0.77      0.74       198\n",
            " consumer_non-cyclical       0.76      0.62      0.68       112\n",
            "                energy       0.93      0.80      0.86        71\n",
            "             financial       0.80      0.79      0.79       192\n",
            "            healthcare       0.89      0.71      0.79        79\n",
            "              services       0.71      0.85      0.78       519\n",
            "            technology       0.74      0.71      0.72        99\n",
            "        transportation       0.83      0.81      0.82       101\n",
            "             utilities       0.85      0.79      0.81        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.82      0.74      0.78      1764\n",
            "          weighted avg       0.78      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 1 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4862, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3007, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.13116234126505458\n",
            "Epoch: 003, Loss: 2.1770, Train: 0.2945, Test: 0.2942\n",
            "Early stopping:  0.15565988318008\n",
            "Epoch: 004, Loss: 2.1263, Train: 0.2989, Test: 0.2982\n",
            "Early stopping:  0.16019912926519653\n",
            "Epoch: 005, Loss: 2.0771, Train: 0.3190, Test: 0.3152\n",
            "Early stopping:  0.1639727636191128\n",
            "Epoch: 006, Loss: 2.0111, Train: 0.3613, Test: 0.3543\n",
            "Early stopping:  0.10947603529331879\n",
            "Epoch: 007, Loss: 1.9503, Train: 0.3926, Test: 0.3764\n",
            "Early stopping:  0.09006294597547804\n",
            "Epoch: 008, Loss: 1.8909, Train: 0.4064, Test: 0.3912\n",
            "Early stopping:  0.0945581528495446\n",
            "Epoch: 009, Loss: 1.8288, Train: 0.4191, Test: 0.3980\n",
            "Early stopping:  0.09754932185590998\n",
            "Epoch: 010, Loss: 1.7679, Train: 0.4380, Test: 0.4229\n",
            "Early stopping:  0.09614123857240887\n",
            "Epoch: 011, Loss: 1.7087, Train: 0.4616, Test: 0.4450\n",
            "Early stopping:  0.0958451231078753\n",
            "Epoch: 012, Loss: 1.6492, Train: 0.4774, Test: 0.4569\n",
            "Early stopping:  0.09542242947165948\n",
            "Epoch: 013, Loss: 1.5927, Train: 0.4930, Test: 0.4705\n",
            "Early stopping:  0.09341005505879245\n",
            "Epoch: 014, Loss: 1.5373, Train: 0.5150, Test: 0.4966\n",
            "Early stopping:  0.09125923288184137\n",
            "Epoch: 015, Loss: 1.4816, Train: 0.5406, Test: 0.5232\n",
            "Early stopping:  0.08952565503275729\n",
            "Epoch: 016, Loss: 1.4302, Train: 0.5654, Test: 0.5459\n",
            "Early stopping:  0.08685174693245926\n",
            "Epoch: 017, Loss: 1.3823, Train: 0.5779, Test: 0.5601\n",
            "Early stopping:  0.0835166465829588\n",
            "Epoch: 018, Loss: 1.3377, Train: 0.5931, Test: 0.5737\n",
            "Early stopping:  0.07887893845022415\n",
            "Epoch: 019, Loss: 1.2959, Train: 0.6030, Test: 0.5782\n",
            "Early stopping:  0.07341346068806652\n",
            "Epoch: 020, Loss: 1.2567, Train: 0.6070, Test: 0.5816\n",
            "Early stopping:  0.06859055073689617\n",
            "Epoch: 021, Loss: 1.2244, Train: 0.6178, Test: 0.5986\n",
            "Early stopping:  0.06285376824102461\n",
            "Epoch: 022, Loss: 1.1966, Train: 0.6253, Test: 0.6094\n",
            "Early stopping:  0.05611002430715042\n",
            "Epoch: 023, Loss: 1.1692, Train: 0.6345, Test: 0.6117\n",
            "Early stopping:  0.0496996547672295\n",
            "Epoch: 024, Loss: 1.1429, Train: 0.6409, Test: 0.6168\n",
            "Early stopping:  0.04474371156298803\n",
            "Epoch: 025, Loss: 1.1190, Train: 0.6481, Test: 0.6196\n",
            "Early stopping:  0.041856893069558206\n",
            "Epoch: 026, Loss: 1.0972, Train: 0.6542, Test: 0.6281\n",
            "Early stopping:  0.03943849582826782\n",
            "Epoch: 027, Loss: 1.0735, Train: 0.6633, Test: 0.6400\n",
            "Early stopping:  0.037512469265309066\n",
            "Epoch: 028, Loss: 1.0504, Train: 0.6699, Test: 0.6429\n",
            "Early stopping:  0.03643360882403784\n",
            "Epoch: 029, Loss: 1.0292, Train: 0.6766, Test: 0.6468\n",
            "Early stopping:  0.035777338961694174\n",
            "Epoch: 030, Loss: 1.0082, Train: 0.6859, Test: 0.6525\n",
            "Early stopping:  0.03514342293358618\n",
            "Epoch: 031, Loss: 0.9874, Train: 0.6937, Test: 0.6559\n",
            "Early stopping:  0.03392919142673273\n",
            "Epoch: 032, Loss: 0.9683, Train: 0.7028, Test: 0.6650\n",
            "Early stopping:  0.03260815672358835\n",
            "Epoch: 033, Loss: 0.9499, Train: 0.7099, Test: 0.6706\n",
            "Early stopping:  0.03141985226020823\n",
            "Epoch: 034, Loss: 0.9316, Train: 0.7163, Test: 0.6746\n",
            "Early stopping:  0.030155544930516524\n",
            "Epoch: 035, Loss: 0.9146, Train: 0.7234, Test: 0.6854\n",
            "Early stopping:  0.028814794796399198\n",
            "Epoch: 036, Loss: 0.8975, Train: 0.7334, Test: 0.6893\n",
            "Early stopping:  0.02795561451333213\n",
            "Epoch: 037, Loss: 0.8807, Train: 0.7411, Test: 0.6939\n",
            "Early stopping:  0.02727836208791081\n",
            "Epoch: 038, Loss: 0.8646, Train: 0.7472, Test: 0.6973\n",
            "Early stopping:  0.02656460094863269\n",
            "Epoch: 039, Loss: 0.8483, Train: 0.7510, Test: 0.6990\n",
            "Early stopping:  0.02618261425546153\n",
            "Epoch: 040, Loss: 0.8323, Train: 0.7578, Test: 0.6984\n",
            "Early stopping:  0.02574931649362856\n",
            "Epoch: 041, Loss: 0.8170, Train: 0.7602, Test: 0.7052\n",
            "Early stopping:  0.02525517320803444\n",
            "Epoch: 042, Loss: 0.8017, Train: 0.7655, Test: 0.7075\n",
            "Early stopping:  0.024850764048224814\n",
            "Epoch: 043, Loss: 0.7867, Train: 0.7700, Test: 0.7109\n",
            "Early stopping:  0.024328584044918944\n",
            "Epoch: 044, Loss: 0.7718, Train: 0.7744, Test: 0.7092\n",
            "Early stopping:  0.02391475398639531\n",
            "Epoch: 045, Loss: 0.7574, Train: 0.7807, Test: 0.7109\n",
            "Early stopping:  0.02355991885274331\n",
            "Epoch: 046, Loss: 0.7430, Train: 0.7872, Test: 0.7166\n",
            "Early stopping:  0.02318773316489988\n",
            "Epoch: 047, Loss: 0.7285, Train: 0.7940, Test: 0.7194\n",
            "Early stopping:  0.02294344559444142\n",
            "Epoch: 048, Loss: 0.7147, Train: 0.7970, Test: 0.7205\n",
            "Early stopping:  0.022633419207106212\n",
            "Epoch: 049, Loss: 0.7011, Train: 0.8016, Test: 0.7222\n",
            "Early stopping:  0.022278586279467823\n",
            "Epoch: 050, Loss: 0.6876, Train: 0.8055, Test: 0.7285\n",
            "Early stopping:  0.021869087626431673\n",
            "Epoch: 051, Loss: 0.6747, Train: 0.8134, Test: 0.7307\n",
            "Early stopping:  0.021307277556179623\n",
            "Epoch: 052, Loss: 0.6622, Train: 0.8107, Test: 0.7336\n",
            "Early stopping:  0.020766754673824397\n",
            "Epoch: 053, Loss: 0.6520, Train: 0.8248, Test: 0.7341\n",
            "Early stopping:  0.01956455902430375\n",
            "Epoch: 054, Loss: 0.6439, Train: 0.8144, Test: 0.7296\n",
            "Early stopping:  0.017466830692115735\n",
            "Epoch: 055, Loss: 0.6333, Train: 0.8334, Test: 0.7392\n",
            "Early stopping:  0.016024307685138742\n",
            "Epoch: 056, Loss: 0.6159, Train: 0.8355, Test: 0.7370\n",
            "Early stopping:  0.017784064742758687\n",
            "Epoch: 057, Loss: 0.6046, Train: 0.8262, Test: 0.7438\n",
            "Early stopping:  0.019563889181532026\n",
            "Epoch: 058, Loss: 0.5988, Train: 0.8433, Test: 0.7392\n",
            "Early stopping:  0.01905534354124173\n",
            "Epoch: 059, Loss: 0.5860, Train: 0.8421, Test: 0.7443\n",
            "Early stopping:  0.017928540473123165\n",
            "Epoch: 060, Loss: 0.5736, Train: 0.8391, Test: 0.7494\n",
            "Early stopping:  0.01645807480084642\n",
            "Epoch: 061, Loss: 0.5672, Train: 0.8524, Test: 0.7426\n",
            "Early stopping:  0.015943091959564934\n",
            "Epoch: 062, Loss: 0.5569, Train: 0.8518, Test: 0.7511\n",
            "Early stopping:  0.016301389297070888\n",
            "Epoch: 063, Loss: 0.5451, Train: 0.8530, Test: 0.7585\n",
            "Early stopping:  0.015612779477885426\n",
            "Epoch: 064, Loss: 0.5380, Train: 0.8609, Test: 0.7523\n",
            "Early stopping:  0.014822307835329892\n",
            "Epoch: 065, Loss: 0.5287, Train: 0.8613, Test: 0.7596\n",
            "Early stopping:  0.015212925978167861\n",
            "Epoch: 066, Loss: 0.5183, Train: 0.8618, Test: 0.7676\n",
            "Early stopping:  0.014825303439965848\n",
            "Epoch: 067, Loss: 0.5104, Train: 0.8698, Test: 0.7642\n",
            "Early stopping:  0.014090801162101282\n",
            "Epoch: 068, Loss: 0.5016, Train: 0.8691, Test: 0.7687\n",
            "Early stopping:  0.014414034056601547\n",
            "Epoch: 069, Loss: 0.4927, Train: 0.8715, Test: 0.7693\n",
            "Early stopping:  0.01406755095193551\n",
            "Epoch: 070, Loss: 0.4848, Train: 0.8788, Test: 0.7721\n",
            "Early stopping:  0.013415237057767697\n",
            "Epoch: 071, Loss: 0.4761, Train: 0.8771, Test: 0.7721\n",
            "Early stopping:  0.013521841474360117\n",
            "Epoch: 072, Loss: 0.4675, Train: 0.8823, Test: 0.7755\n",
            "Early stopping:  0.01341233185586094\n",
            "Epoch: 073, Loss: 0.4604, Train: 0.8850, Test: 0.7732\n",
            "Early stopping:  0.012951052601093806\n",
            "Epoch: 074, Loss: 0.4526, Train: 0.8864, Test: 0.7772\n",
            "Early stopping:  0.012681808572696471\n",
            "Epoch: 075, Loss: 0.4435, Train: 0.8918, Test: 0.7778\n",
            "Early stopping:  0.01266312778612642\n",
            "Epoch: 076, Loss: 0.4359, Train: 0.8911, Test: 0.7755\n",
            "Early stopping:  0.012660449423410316\n",
            "Epoch: 077, Loss: 0.4300, Train: 0.8952, Test: 0.7789\n",
            "Early stopping:  0.012281926190302416\n",
            "Epoch: 078, Loss: 0.4229, Train: 0.8989, Test: 0.7772\n",
            "Early stopping:  0.011536281881197277\n",
            "Epoch: 079, Loss: 0.4154, Train: 0.8947, Test: 0.7789\n",
            "Early stopping:  0.010955694579685623\n",
            "Epoch: 080, Loss: 0.4115, Train: 0.8989, Test: 0.7766\n",
            "Early stopping:  0.01008633749968517\n",
            "Epoch: 081, Loss: 0.4114, Train: 0.8930, Test: 0.7778\n",
            "Early stopping:  0.008080651048323138\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.77      0.79       190\n",
            "         capital_goods       0.75      0.69      0.72       127\n",
            "conglomerates_industry       1.00      0.85      0.92        20\n",
            "     consumer_cyclical       0.83      0.70      0.76       198\n",
            " consumer_non-cyclical       0.78      0.62      0.69       112\n",
            "                energy       0.85      0.73      0.79        71\n",
            "             financial       0.77      0.79      0.78       192\n",
            "            healthcare       0.91      0.75      0.82        79\n",
            "              services       0.70      0.88      0.78       519\n",
            "            technology       0.89      0.65      0.75        99\n",
            "        transportation       0.88      0.82      0.85       101\n",
            "             utilities       0.88      0.80      0.84        56\n",
            "\n",
            "              accuracy                           0.78      1764\n",
            "             macro avg       0.84      0.75      0.79      1764\n",
            "          weighted avg       0.79      0.78      0.78      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 2 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4621, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2578, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1444541085862382\n",
            "Epoch: 003, Loss: 2.1503, Train: 0.2946, Test: 0.2948\n",
            "Early stopping:  0.15839688546010136\n",
            "Epoch: 004, Loss: 2.1186, Train: 0.3071, Test: 0.3067\n",
            "Early stopping:  0.1551720065224271\n",
            "Epoch: 005, Loss: 2.0454, Train: 0.3547, Test: 0.3509\n",
            "Early stopping:  0.1618836380305022\n",
            "Epoch: 006, Loss: 1.9656, Train: 0.3964, Test: 0.3895\n",
            "Early stopping:  0.11015943544565679\n",
            "Epoch: 007, Loss: 1.8992, Train: 0.4100, Test: 0.4019\n",
            "Early stopping:  0.10444805748853672\n",
            "Epoch: 008, Loss: 1.8363, Train: 0.4205, Test: 0.4093\n",
            "Early stopping:  0.11248662482366598\n",
            "Epoch: 009, Loss: 1.7706, Train: 0.4385, Test: 0.4235\n",
            "Early stopping:  0.10743648963804468\n",
            "Epoch: 010, Loss: 1.7029, Train: 0.4605, Test: 0.4433\n",
            "Early stopping:  0.10340816436792866\n",
            "Epoch: 011, Loss: 1.6381, Train: 0.4763, Test: 0.4529\n",
            "Early stopping:  0.10366177858489069\n",
            "Epoch: 012, Loss: 1.5798, Train: 0.4982, Test: 0.4779\n",
            "Early stopping:  0.10211433613493091\n",
            "Epoch: 013, Loss: 1.5217, Train: 0.5226, Test: 0.4989\n",
            "Early stopping:  0.0982571016798743\n",
            "Epoch: 014, Loss: 1.4634, Train: 0.5525, Test: 0.5329\n",
            "Early stopping:  0.09415937160798463\n",
            "Epoch: 015, Loss: 1.4117, Train: 0.5781, Test: 0.5533\n",
            "Early stopping:  0.0900128347599968\n",
            "Epoch: 016, Loss: 1.3655, Train: 0.5942, Test: 0.5714\n",
            "Early stopping:  0.085247230981625\n",
            "Epoch: 017, Loss: 1.3208, Train: 0.5992, Test: 0.5765\n",
            "Early stopping:  0.07911742489657084\n",
            "Epoch: 018, Loss: 1.2776, Train: 0.6012, Test: 0.5748\n",
            "Early stopping:  0.07318900083843823\n",
            "Epoch: 019, Loss: 1.2412, Train: 0.6047, Test: 0.5771\n",
            "Early stopping:  0.06788386720973934\n",
            "Epoch: 020, Loss: 1.2114, Train: 0.6145, Test: 0.5879\n",
            "Early stopping:  0.06151690397682466\n",
            "Epoch: 021, Loss: 1.1826, Train: 0.6282, Test: 0.6032\n",
            "Early stopping:  0.05438563808489276\n",
            "Epoch: 022, Loss: 1.1556, Train: 0.6393, Test: 0.6196\n",
            "Early stopping:  0.04790401934907824\n",
            "Epoch: 023, Loss: 1.1326, Train: 0.6471, Test: 0.6253\n",
            "Early stopping:  0.04320176770593047\n",
            "Epoch: 024, Loss: 1.1101, Train: 0.6548, Test: 0.6332\n",
            "Early stopping:  0.0400204926816997\n",
            "Epoch: 025, Loss: 1.0849, Train: 0.6589, Test: 0.6264\n",
            "Early stopping:  0.03810643854100873\n",
            "Epoch: 026, Loss: 1.0621, Train: 0.6631, Test: 0.6332\n",
            "Early stopping:  0.03713751803537364\n",
            "Epoch: 027, Loss: 1.0414, Train: 0.6729, Test: 0.6423\n",
            "Early stopping:  0.0364542680295371\n",
            "Epoch: 028, Loss: 1.0199, Train: 0.6813, Test: 0.6451\n",
            "Early stopping:  0.03542521268123141\n",
            "Epoch: 029, Loss: 0.9992, Train: 0.6911, Test: 0.6508\n",
            "Early stopping:  0.033784778725751446\n",
            "Epoch: 030, Loss: 0.9808, Train: 0.6981, Test: 0.6627\n",
            "Early stopping:  0.03236861595359787\n",
            "Epoch: 031, Loss: 0.9624, Train: 0.7024, Test: 0.6604\n",
            "Early stopping:  0.031178041746472603\n",
            "Epoch: 032, Loss: 0.9443, Train: 0.7095, Test: 0.6655\n",
            "Early stopping:  0.029743209641191978\n",
            "Epoch: 033, Loss: 0.9270, Train: 0.7160, Test: 0.6712\n",
            "Early stopping:  0.028629071038017657\n",
            "Epoch: 034, Loss: 0.9103, Train: 0.7225, Test: 0.6791\n",
            "Early stopping:  0.02792217524990697\n",
            "Epoch: 035, Loss: 0.8939, Train: 0.7296, Test: 0.6842\n",
            "Early stopping:  0.027047422180093893\n",
            "Epoch: 036, Loss: 0.8778, Train: 0.7361, Test: 0.6905\n",
            "Early stopping:  0.026251417282898357\n",
            "Epoch: 037, Loss: 0.8625, Train: 0.7397, Test: 0.6910\n",
            "Early stopping:  0.02553041311720108\n",
            "Epoch: 038, Loss: 0.8476, Train: 0.7471, Test: 0.6922\n",
            "Early stopping:  0.024786293235578093\n",
            "Epoch: 039, Loss: 0.8323, Train: 0.7539, Test: 0.6984\n",
            "Early stopping:  0.024238610279199335\n",
            "Epoch: 040, Loss: 0.8179, Train: 0.7608, Test: 0.6984\n",
            "Early stopping:  0.02371752860872351\n",
            "Epoch: 041, Loss: 0.8037, Train: 0.7668, Test: 0.6984\n",
            "Early stopping:  0.023293916232174586\n",
            "Epoch: 042, Loss: 0.7890, Train: 0.7743, Test: 0.7012\n",
            "Early stopping:  0.023072661852366585\n",
            "Epoch: 043, Loss: 0.7750, Train: 0.7770, Test: 0.7035\n",
            "Early stopping:  0.022690283376232582\n",
            "Epoch: 044, Loss: 0.7610, Train: 0.7851, Test: 0.7098\n",
            "Early stopping:  0.02251259364838032\n",
            "Epoch: 045, Loss: 0.7468, Train: 0.7882, Test: 0.7080\n",
            "Early stopping:  0.02242009123497833\n",
            "Epoch: 046, Loss: 0.7333, Train: 0.7931, Test: 0.7109\n",
            "Early stopping:  0.022061940453805922\n",
            "Epoch: 047, Loss: 0.7196, Train: 0.7934, Test: 0.7137\n",
            "Early stopping:  0.021916201261605008\n",
            "Epoch: 048, Loss: 0.7063, Train: 0.7984, Test: 0.7160\n",
            "Early stopping:  0.021602553333840358\n",
            "Epoch: 049, Loss: 0.6933, Train: 0.8022, Test: 0.7194\n",
            "Early stopping:  0.02117214086603257\n",
            "Epoch: 050, Loss: 0.6804, Train: 0.8080, Test: 0.7205\n",
            "Early stopping:  0.020888092037000216\n",
            "Epoch: 051, Loss: 0.6683, Train: 0.8102, Test: 0.7228\n",
            "Early stopping:  0.020325080674047917\n",
            "Epoch: 052, Loss: 0.6573, Train: 0.8172, Test: 0.7217\n",
            "Early stopping:  0.019483547426579668\n",
            "Epoch: 053, Loss: 0.6488, Train: 0.8106, Test: 0.7217\n",
            "Early stopping:  0.017788300807714\n",
            "Epoch: 054, Loss: 0.6402, Train: 0.8262, Test: 0.7296\n",
            "Early stopping:  0.01583617491439682\n",
            "Epoch: 055, Loss: 0.6253, Train: 0.8280, Test: 0.7324\n",
            "Early stopping:  0.016394899807968315\n",
            "Epoch: 056, Loss: 0.6092, Train: 0.8266, Test: 0.7375\n",
            "Early stopping:  0.019147620706184188\n",
            "Epoch: 057, Loss: 0.6022, Train: 0.8362, Test: 0.7330\n",
            "Early stopping:  0.01978848606310811\n",
            "Epoch: 058, Loss: 0.5952, Train: 0.8369, Test: 0.7409\n",
            "Early stopping:  0.018226797950953565\n",
            "Epoch: 059, Loss: 0.5802, Train: 0.8408, Test: 0.7460\n",
            "Early stopping:  0.01667800195928201\n",
            "Epoch: 060, Loss: 0.5692, Train: 0.8470, Test: 0.7387\n",
            "Early stopping:  0.01634595788828253\n",
            "Epoch: 061, Loss: 0.5638, Train: 0.8430, Test: 0.7472\n",
            "Early stopping:  0.016447357840830557\n",
            "Epoch: 062, Loss: 0.5530, Train: 0.8500, Test: 0.7466\n",
            "Early stopping:  0.01614300721537053\n",
            "Epoch: 063, Loss: 0.5402, Train: 0.8575, Test: 0.7472\n",
            "Early stopping:  0.015348811414242478\n",
            "Epoch: 064, Loss: 0.5334, Train: 0.8516, Test: 0.7500\n",
            "Early stopping:  0.015165700855902131\n",
            "Epoch: 065, Loss: 0.5261, Train: 0.8640, Test: 0.7460\n",
            "Early stopping:  0.015139899972203334\n",
            "Epoch: 066, Loss: 0.5144, Train: 0.8663, Test: 0.7489\n",
            "Early stopping:  0.014513531721240655\n",
            "Epoch: 067, Loss: 0.5047, Train: 0.8633, Test: 0.7517\n",
            "Early stopping:  0.014310611082367834\n",
            "Epoch: 068, Loss: 0.4987, Train: 0.8724, Test: 0.7494\n",
            "Early stopping:  0.01442065668710964\n",
            "Epoch: 069, Loss: 0.4905, Train: 0.8693, Test: 0.7506\n",
            "Early stopping:  0.013837740219668515\n",
            "Epoch: 070, Loss: 0.4800, Train: 0.8757, Test: 0.7523\n",
            "Early stopping:  0.013159549767699584\n",
            "Epoch: 071, Loss: 0.4712, Train: 0.8801, Test: 0.7511\n",
            "Early stopping:  0.013603652289159539\n",
            "Epoch: 072, Loss: 0.4647, Train: 0.8776, Test: 0.7551\n",
            "Early stopping:  0.013851018268356708\n",
            "Epoch: 073, Loss: 0.4579, Train: 0.8871, Test: 0.7511\n",
            "Early stopping:  0.012797402656550877\n",
            "Epoch: 074, Loss: 0.4491, Train: 0.8869, Test: 0.7579\n",
            "Early stopping:  0.011909204317557692\n",
            "Epoch: 075, Loss: 0.4400, Train: 0.8921, Test: 0.7574\n",
            "Early stopping:  0.012377061598116242\n",
            "Epoch: 076, Loss: 0.4320, Train: 0.8959, Test: 0.7579\n",
            "Early stopping:  0.013196835677144875\n",
            "Epoch: 077, Loss: 0.4253, Train: 0.8924, Test: 0.7591\n",
            "Early stopping:  0.01305812535510653\n",
            "Epoch: 078, Loss: 0.4194, Train: 0.9009, Test: 0.7602\n",
            "Early stopping:  0.011779142335085634\n",
            "Epoch: 079, Loss: 0.4132, Train: 0.8959, Test: 0.7619\n",
            "Early stopping:  0.010476766412484486\n",
            "Epoch: 080, Loss: 0.4068, Train: 0.9064, Test: 0.7630\n",
            "Early stopping:  0.009846387082084433\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.79      0.77       190\n",
            "         capital_goods       0.77      0.69      0.73       127\n",
            "conglomerates_industry       1.00      0.70      0.82        20\n",
            "     consumer_cyclical       0.69      0.75      0.72       198\n",
            " consumer_non-cyclical       0.72      0.68      0.70       112\n",
            "                energy       0.95      0.76      0.84        71\n",
            "             financial       0.81      0.78      0.79       192\n",
            "            healthcare       0.87      0.70      0.77        79\n",
            "              services       0.74      0.80      0.77       519\n",
            "            technology       0.71      0.71      0.71        99\n",
            "        transportation       0.86      0.82      0.84       101\n",
            "             utilities       0.85      0.79      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.81      0.75      0.77      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 3 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4465, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2280, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15449584812504977\n",
            "Epoch: 003, Loss: 2.1981, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1355837980092592\n",
            "Epoch: 004, Loss: 2.1307, Train: 0.2997, Test: 0.2999\n",
            "Early stopping:  0.1366276347225352\n",
            "Epoch: 005, Loss: 2.0503, Train: 0.3576, Test: 0.3469\n",
            "Early stopping:  0.1484662398098101\n",
            "Epoch: 006, Loss: 2.0047, Train: 0.3913, Test: 0.3730\n",
            "Early stopping:  0.0948789691398314\n",
            "Epoch: 007, Loss: 1.9570, Train: 0.3964, Test: 0.3821\n",
            "Early stopping:  0.09683721571198328\n",
            "Epoch: 008, Loss: 1.8926, Train: 0.3862, Test: 0.3690\n",
            "Early stopping:  0.09052585543602369\n",
            "Epoch: 009, Loss: 1.8258, Train: 0.3838, Test: 0.3702\n",
            "Early stopping:  0.08907839503739563\n",
            "Epoch: 010, Loss: 1.7688, Train: 0.4020, Test: 0.3906\n",
            "Early stopping:  0.09545150046090915\n",
            "Epoch: 011, Loss: 1.7152, Train: 0.4339, Test: 0.4184\n",
            "Early stopping:  0.09613735905334612\n",
            "Epoch: 012, Loss: 1.6554, Train: 0.4649, Test: 0.4512\n",
            "Early stopping:  0.0925403438804891\n",
            "Epoch: 013, Loss: 1.5947, Train: 0.4933, Test: 0.4785\n",
            "Early stopping:  0.09103856640176773\n",
            "Epoch: 014, Loss: 1.5396, Train: 0.5209, Test: 0.5074\n",
            "Early stopping:  0.091552992388064\n",
            "Epoch: 015, Loss: 1.4905, Train: 0.5447, Test: 0.5249\n",
            "Early stopping:  0.0894326577016563\n",
            "Epoch: 016, Loss: 1.4412, Train: 0.5615, Test: 0.5431\n",
            "Early stopping:  0.08432714097934933\n",
            "Epoch: 017, Loss: 1.3901, Train: 0.5660, Test: 0.5442\n",
            "Early stopping:  0.08028761149024555\n",
            "Epoch: 018, Loss: 1.3456, Train: 0.5710, Test: 0.5544\n",
            "Early stopping:  0.07723900537576961\n",
            "Epoch: 019, Loss: 1.3083, Train: 0.5841, Test: 0.5612\n",
            "Early stopping:  0.07286260095721858\n",
            "Epoch: 020, Loss: 1.2702, Train: 0.6009, Test: 0.5748\n",
            "Early stopping:  0.0671661330263793\n",
            "Epoch: 021, Loss: 1.2325, Train: 0.6155, Test: 0.5930\n",
            "Early stopping:  0.06178745565254228\n",
            "Epoch: 022, Loss: 1.2020, Train: 0.6203, Test: 0.5930\n",
            "Early stopping:  0.05744070808770957\n",
            "Epoch: 023, Loss: 1.1745, Train: 0.6295, Test: 0.6043\n",
            "Early stopping:  0.053215862743187015\n",
            "Epoch: 024, Loss: 1.1458, Train: 0.6368, Test: 0.6037\n",
            "Early stopping:  0.04858374417043142\n",
            "Epoch: 025, Loss: 1.1201, Train: 0.6424, Test: 0.6128\n",
            "Early stopping:  0.0444601281621094\n",
            "Epoch: 026, Loss: 1.0967, Train: 0.6543, Test: 0.6196\n",
            "Early stopping:  0.04195025924746352\n",
            "Epoch: 027, Loss: 1.0718, Train: 0.6651, Test: 0.6281\n",
            "Early stopping:  0.04028838689368255\n",
            "Epoch: 028, Loss: 1.0493, Train: 0.6730, Test: 0.6281\n",
            "Early stopping:  0.03816129061559751\n",
            "Epoch: 029, Loss: 1.0289, Train: 0.6772, Test: 0.6355\n",
            "Early stopping:  0.03635039665956095\n",
            "Epoch: 030, Loss: 1.0063, Train: 0.6834, Test: 0.6423\n",
            "Early stopping:  0.03538854392244278\n",
            "Epoch: 031, Loss: 0.9855, Train: 0.6926, Test: 0.6468\n",
            "Early stopping:  0.03411407610308614\n",
            "Epoch: 032, Loss: 0.9658, Train: 0.7013, Test: 0.6536\n",
            "Early stopping:  0.03329979691914654\n",
            "Epoch: 033, Loss: 0.9449, Train: 0.7058, Test: 0.6587\n",
            "Early stopping:  0.03295741591415825\n",
            "Epoch: 034, Loss: 0.9254, Train: 0.7122, Test: 0.6667\n",
            "Early stopping:  0.0319748237855529\n",
            "Epoch: 035, Loss: 0.9077, Train: 0.7208, Test: 0.6757\n",
            "Early stopping:  0.030986187953315814\n",
            "Epoch: 036, Loss: 0.8898, Train: 0.7274, Test: 0.6791\n",
            "Early stopping:  0.029937952839502197\n",
            "Epoch: 037, Loss: 0.8737, Train: 0.7346, Test: 0.6831\n",
            "Early stopping:  0.02818974320162086\n",
            "Epoch: 038, Loss: 0.8577, Train: 0.7444, Test: 0.6876\n",
            "Early stopping:  0.02680912294219725\n",
            "Epoch: 039, Loss: 0.8418, Train: 0.7520, Test: 0.6893\n",
            "Early stopping:  0.025896491279406828\n",
            "Epoch: 040, Loss: 0.8273, Train: 0.7588, Test: 0.6950\n",
            "Early stopping:  0.02478421362645217\n",
            "Epoch: 041, Loss: 0.8121, Train: 0.7644, Test: 0.6995\n",
            "Early stopping:  0.02429095633371246\n",
            "Epoch: 042, Loss: 0.7981, Train: 0.7693, Test: 0.7075\n",
            "Early stopping:  0.023554650787009135\n",
            "Epoch: 043, Loss: 0.7835, Train: 0.7743, Test: 0.7103\n",
            "Early stopping:  0.02307437333982349\n",
            "Epoch: 044, Loss: 0.7691, Train: 0.7765, Test: 0.7126\n",
            "Early stopping:  0.02292576451713124\n",
            "Epoch: 045, Loss: 0.7550, Train: 0.7801, Test: 0.7160\n",
            "Early stopping:  0.022622812979538495\n",
            "Epoch: 046, Loss: 0.7410, Train: 0.7846, Test: 0.7160\n",
            "Early stopping:  0.022574121610719776\n",
            "Epoch: 047, Loss: 0.7271, Train: 0.7910, Test: 0.7234\n",
            "Early stopping:  0.022285656717171705\n",
            "Epoch: 048, Loss: 0.7133, Train: 0.7950, Test: 0.7273\n",
            "Early stopping:  0.022077941636528375\n",
            "Epoch: 049, Loss: 0.6998, Train: 0.7991, Test: 0.7307\n",
            "Early stopping:  0.021844646848427914\n",
            "Epoch: 050, Loss: 0.6865, Train: 0.8004, Test: 0.7330\n",
            "Early stopping:  0.021545842240633575\n",
            "Epoch: 051, Loss: 0.6733, Train: 0.8079, Test: 0.7375\n",
            "Early stopping:  0.02123228607218331\n",
            "Epoch: 052, Loss: 0.6604, Train: 0.8085, Test: 0.7341\n",
            "Early stopping:  0.02089707926933992\n",
            "Epoch: 053, Loss: 0.6493, Train: 0.8170, Test: 0.7381\n",
            "Early stopping:  0.02009943234324828\n",
            "Epoch: 054, Loss: 0.6421, Train: 0.8069, Test: 0.7336\n",
            "Early stopping:  0.017948915911596384\n",
            "Epoch: 055, Loss: 0.6403, Train: 0.8255, Test: 0.7415\n",
            "Early stopping:  0.013812855652423139\n",
            "Epoch: 056, Loss: 0.6202, Train: 0.8272, Test: 0.7460\n",
            "Early stopping:  0.014744035652328591\n",
            "Epoch: 057, Loss: 0.6032, Train: 0.8240, Test: 0.7426\n",
            "Early stopping:  0.01894031062290872\n",
            "Epoch: 058, Loss: 0.6024, Train: 0.8379, Test: 0.7517\n",
            "Early stopping:  0.019239178943461617\n",
            "Epoch: 059, Loss: 0.5862, Train: 0.8405, Test: 0.7500\n",
            "Early stopping:  0.020563214062784163\n",
            "Epoch: 060, Loss: 0.5749, Train: 0.8372, Test: 0.7534\n",
            "Early stopping:  0.017378708546951258\n",
            "Epoch: 061, Loss: 0.5705, Train: 0.8457, Test: 0.7523\n",
            "Early stopping:  0.015134312972639185\n",
            "Epoch: 062, Loss: 0.5566, Train: 0.8484, Test: 0.7523\n",
            "Early stopping:  0.01720958727595554\n",
            "Epoch: 063, Loss: 0.5472, Train: 0.8466, Test: 0.7591\n",
            "Early stopping:  0.015358107423122984\n",
            "Epoch: 064, Loss: 0.5419, Train: 0.8571, Test: 0.7534\n",
            "Early stopping:  0.01430231545047789\n",
            "Epoch: 065, Loss: 0.5288, Train: 0.8594, Test: 0.7557\n",
            "Early stopping:  0.01565034096308723\n",
            "Epoch: 066, Loss: 0.5204, Train: 0.8562, Test: 0.7636\n",
            "Early stopping:  0.014447067517543806\n",
            "Epoch: 067, Loss: 0.5156, Train: 0.8664, Test: 0.7602\n",
            "Early stopping:  0.01355302999583541\n",
            "Epoch: 068, Loss: 0.5025, Train: 0.8689, Test: 0.7630\n",
            "Early stopping:  0.01470257560390718\n",
            "Epoch: 069, Loss: 0.4949, Train: 0.8669, Test: 0.7619\n",
            "Early stopping:  0.013674199819789555\n",
            "Epoch: 070, Loss: 0.4899, Train: 0.8742, Test: 0.7642\n",
            "Early stopping:  0.013113282012651396\n",
            "Epoch: 071, Loss: 0.4788, Train: 0.8782, Test: 0.7653\n",
            "Early stopping:  0.013817692314848213\n",
            "Epoch: 072, Loss: 0.4697, Train: 0.8772, Test: 0.7670\n",
            "Early stopping:  0.013013554903170048\n",
            "Epoch: 073, Loss: 0.4643, Train: 0.8832, Test: 0.7664\n",
            "Early stopping:  0.012937728479088736\n",
            "Epoch: 074, Loss: 0.4565, Train: 0.8847, Test: 0.7704\n",
            "Early stopping:  0.01293664914192439\n",
            "Epoch: 075, Loss: 0.4470, Train: 0.8886, Test: 0.7681\n",
            "Early stopping:  0.012179772221467062\n",
            "Epoch: 076, Loss: 0.4389, Train: 0.8907, Test: 0.7710\n",
            "Early stopping:  0.012548986199210906\n",
            "Epoch: 077, Loss: 0.4336, Train: 0.8896, Test: 0.7642\n",
            "Early stopping:  0.012543017315528615\n",
            "Epoch: 078, Loss: 0.4279, Train: 0.8945, Test: 0.7761\n",
            "Early stopping:  0.011244526772689747\n",
            "Epoch: 079, Loss: 0.4200, Train: 0.8979, Test: 0.7625\n",
            "Early stopping:  0.010300784368047319\n",
            "Epoch: 080, Loss: 0.4149, Train: 0.8893, Test: 0.7749\n",
            "Early stopping:  0.009748703093770773\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.83      0.74      0.78       190\n",
            "         capital_goods       0.83      0.68      0.74       127\n",
            "conglomerates_industry       0.94      0.80      0.86        20\n",
            "     consumer_cyclical       0.81      0.73      0.77       198\n",
            " consumer_non-cyclical       0.81      0.60      0.69       112\n",
            "                energy       0.90      0.79      0.84        71\n",
            "             financial       0.80      0.73      0.77       192\n",
            "            healthcare       0.91      0.76      0.83        79\n",
            "              services       0.68      0.90      0.78       519\n",
            "            technology       0.79      0.64      0.70        99\n",
            "        transportation       0.92      0.81      0.86       101\n",
            "             utilities       0.83      0.79      0.81        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.84      0.75      0.79      1764\n",
            "          weighted avg       0.79      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 4 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5293, Train: 0.3081, Test: 0.3050\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3051, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15848614315466716\n",
            "Epoch: 003, Loss: 2.1649, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.18377795089358268\n",
            "Epoch: 004, Loss: 2.1457, Train: 0.3006, Test: 0.2993\n",
            "Early stopping:  0.1769121995721284\n",
            "Epoch: 005, Loss: 2.0801, Train: 0.3383, Test: 0.3362\n",
            "Early stopping:  0.17881109306282716\n",
            "Epoch: 006, Loss: 2.0087, Train: 0.3786, Test: 0.3730\n",
            "Early stopping:  0.11040103438933882\n",
            "Epoch: 007, Loss: 1.9571, Train: 0.4054, Test: 0.3980\n",
            "Early stopping:  0.08850725254695199\n",
            "Epoch: 008, Loss: 1.9057, Train: 0.4211, Test: 0.4076\n",
            "Early stopping:  0.09560356197740263\n",
            "Epoch: 009, Loss: 1.8469, Train: 0.4276, Test: 0.4127\n",
            "Early stopping:  0.09020041172143044\n",
            "Epoch: 010, Loss: 1.7846, Train: 0.4317, Test: 0.4161\n",
            "Early stopping:  0.08837084711202828\n",
            "Epoch: 011, Loss: 1.7246, Train: 0.4455, Test: 0.4342\n",
            "Early stopping:  0.09272075256151115\n",
            "Epoch: 012, Loss: 1.6664, Train: 0.4675, Test: 0.4563\n",
            "Early stopping:  0.0949944072738\n",
            "Epoch: 013, Loss: 1.6084, Train: 0.4910, Test: 0.4728\n",
            "Early stopping:  0.09410923893052596\n",
            "Epoch: 014, Loss: 1.5529, Train: 0.5123, Test: 0.4926\n",
            "Early stopping:  0.09165740723485416\n",
            "Epoch: 015, Loss: 1.4992, Train: 0.5368, Test: 0.5170\n",
            "Early stopping:  0.08922758898716707\n",
            "Epoch: 016, Loss: 1.4445, Train: 0.5591, Test: 0.5385\n",
            "Early stopping:  0.08743519433787768\n",
            "Epoch: 017, Loss: 1.3940, Train: 0.5738, Test: 0.5590\n",
            "Early stopping:  0.0849310908677842\n",
            "Epoch: 018, Loss: 1.3531, Train: 0.5851, Test: 0.5692\n",
            "Early stopping:  0.07991814531263448\n",
            "Epoch: 019, Loss: 1.3148, Train: 0.5953, Test: 0.5709\n",
            "Early stopping:  0.0730133996932486\n",
            "Epoch: 020, Loss: 1.2777, Train: 0.6075, Test: 0.5777\n",
            "Early stopping:  0.0654332613050504\n",
            "Epoch: 021, Loss: 1.2448, Train: 0.6143, Test: 0.5901\n",
            "Early stopping:  0.05918342637844472\n",
            "Epoch: 022, Loss: 1.2137, Train: 0.6224, Test: 0.5958\n",
            "Early stopping:  0.055212707385599344\n",
            "Epoch: 023, Loss: 1.1875, Train: 0.6281, Test: 0.6049\n",
            "Early stopping:  0.050449440401134\n",
            "Epoch: 024, Loss: 1.1631, Train: 0.6392, Test: 0.6139\n",
            "Early stopping:  0.045377450032544196\n",
            "Epoch: 025, Loss: 1.1363, Train: 0.6472, Test: 0.6219\n",
            "Early stopping:  0.04235602448738813\n",
            "Epoch: 026, Loss: 1.1118, Train: 0.6518, Test: 0.6230\n",
            "Early stopping:  0.0403416280248905\n",
            "Epoch: 027, Loss: 1.0894, Train: 0.6576, Test: 0.6304\n",
            "Early stopping:  0.0391673160810144\n",
            "Epoch: 028, Loss: 1.0685, Train: 0.6647, Test: 0.6332\n",
            "Early stopping:  0.03735660574718869\n",
            "Epoch: 029, Loss: 1.0477, Train: 0.6716, Test: 0.6400\n",
            "Early stopping:  0.03485373644825827\n",
            "Epoch: 030, Loss: 1.0269, Train: 0.6769, Test: 0.6434\n",
            "Early stopping:  0.03343552891440741\n",
            "Epoch: 031, Loss: 1.0075, Train: 0.6817, Test: 0.6508\n",
            "Early stopping:  0.0324839996621813\n",
            "Epoch: 032, Loss: 0.9882, Train: 0.6905, Test: 0.6565\n",
            "Early stopping:  0.0317645471950108\n",
            "Epoch: 033, Loss: 0.9700, Train: 0.6986, Test: 0.6627\n",
            "Early stopping:  0.030715171308134507\n",
            "Epoch: 034, Loss: 0.9518, Train: 0.7076, Test: 0.6633\n",
            "Early stopping:  0.029694041035657034\n",
            "Epoch: 035, Loss: 0.9342, Train: 0.7153, Test: 0.6672\n",
            "Early stopping:  0.028941330369732662\n",
            "Epoch: 036, Loss: 0.9174, Train: 0.7208, Test: 0.6701\n",
            "Early stopping:  0.02804815528420369\n",
            "Epoch: 037, Loss: 0.9017, Train: 0.7255, Test: 0.6729\n",
            "Early stopping:  0.027036649302608247\n",
            "Epoch: 038, Loss: 0.8863, Train: 0.7305, Test: 0.6797\n",
            "Early stopping:  0.0258524371167887\n",
            "Epoch: 039, Loss: 0.8714, Train: 0.7364, Test: 0.6786\n",
            "Early stopping:  0.02478850516819413\n",
            "Epoch: 040, Loss: 0.8567, Train: 0.7428, Test: 0.6848\n",
            "Early stopping:  0.02400614980511327\n",
            "Epoch: 041, Loss: 0.8425, Train: 0.7490, Test: 0.6888\n",
            "Early stopping:  0.023405265257024428\n",
            "Epoch: 042, Loss: 0.8278, Train: 0.7558, Test: 0.6973\n",
            "Early stopping:  0.02307784691542341\n",
            "Epoch: 043, Loss: 0.8136, Train: 0.7604, Test: 0.6967\n",
            "Early stopping:  0.02284770690512405\n",
            "Epoch: 044, Loss: 0.7997, Train: 0.7659, Test: 0.7041\n",
            "Early stopping:  0.022586499521743818\n",
            "Epoch: 045, Loss: 0.7861, Train: 0.7730, Test: 0.7052\n",
            "Early stopping:  0.022286120999846757\n",
            "Epoch: 046, Loss: 0.7726, Train: 0.7770, Test: 0.7075\n",
            "Early stopping:  0.021805113548859587\n",
            "Epoch: 047, Loss: 0.7593, Train: 0.7815, Test: 0.7098\n",
            "Early stopping:  0.02144807953097809\n",
            "Epoch: 048, Loss: 0.7458, Train: 0.7851, Test: 0.7069\n",
            "Early stopping:  0.021289187100940188\n",
            "Epoch: 049, Loss: 0.7324, Train: 0.7873, Test: 0.7109\n",
            "Early stopping:  0.021212318038917214\n",
            "Epoch: 050, Loss: 0.7195, Train: 0.7923, Test: 0.7115\n",
            "Early stopping:  0.021054829521790892\n",
            "Epoch: 051, Loss: 0.7065, Train: 0.7967, Test: 0.7171\n",
            "Early stopping:  0.02084829849225171\n",
            "Epoch: 052, Loss: 0.6939, Train: 0.8009, Test: 0.7211\n",
            "Early stopping:  0.020490646381177344\n",
            "Epoch: 053, Loss: 0.6814, Train: 0.8070, Test: 0.7222\n",
            "Early stopping:  0.020171496878953494\n",
            "Epoch: 054, Loss: 0.6688, Train: 0.8087, Test: 0.7273\n",
            "Early stopping:  0.019984009352692428\n",
            "Epoch: 055, Loss: 0.6566, Train: 0.8161, Test: 0.7256\n",
            "Early stopping:  0.019759172447347872\n",
            "Epoch: 056, Loss: 0.6445, Train: 0.8165, Test: 0.7313\n",
            "Early stopping:  0.01954980210525455\n",
            "Epoch: 057, Loss: 0.6334, Train: 0.8246, Test: 0.7324\n",
            "Early stopping:  0.01902746931192665\n",
            "Epoch: 058, Loss: 0.6240, Train: 0.8185, Test: 0.7330\n",
            "Early stopping:  0.017870085045137492\n",
            "Epoch: 059, Loss: 0.6161, Train: 0.8293, Test: 0.7375\n",
            "Early stopping:  0.01610967479265717\n",
            "Epoch: 060, Loss: 0.6040, Train: 0.8333, Test: 0.7398\n",
            "Early stopping:  0.015596834726977407\n",
            "Epoch: 061, Loss: 0.5881, Train: 0.8357, Test: 0.7443\n",
            "Early stopping:  0.0176716366693875\n",
            "Epoch: 062, Loss: 0.5774, Train: 0.8398, Test: 0.7409\n",
            "Early stopping:  0.019249450955056053\n",
            "Epoch: 063, Loss: 0.5715, Train: 0.8398, Test: 0.7483\n",
            "Early stopping:  0.01848643719269788\n",
            "Epoch: 064, Loss: 0.5607, Train: 0.8506, Test: 0.7494\n",
            "Early stopping:  0.016488634047081625\n",
            "Epoch: 065, Loss: 0.5462, Train: 0.8538, Test: 0.7511\n",
            "Early stopping:  0.016028488124117495\n",
            "Epoch: 066, Loss: 0.5374, Train: 0.8501, Test: 0.7579\n",
            "Early stopping:  0.01678540345271201\n",
            "Epoch: 067, Loss: 0.5309, Train: 0.8639, Test: 0.7585\n",
            "Early stopping:  0.016687318315700552\n",
            "Epoch: 068, Loss: 0.5201, Train: 0.8613, Test: 0.7579\n",
            "Early stopping:  0.015407304988855137\n",
            "Epoch: 069, Loss: 0.5085, Train: 0.8666, Test: 0.7557\n",
            "Early stopping:  0.014765293165629577\n",
            "Epoch: 070, Loss: 0.4990, Train: 0.8730, Test: 0.7602\n",
            "Early stopping:  0.015754182646710337\n",
            "Epoch: 071, Loss: 0.4917, Train: 0.8677, Test: 0.7596\n",
            "Early stopping:  0.015787930687054738\n",
            "Epoch: 072, Loss: 0.4845, Train: 0.8805, Test: 0.7602\n",
            "Early stopping:  0.014004570220122138\n",
            "Epoch: 073, Loss: 0.4739, Train: 0.8815, Test: 0.7602\n",
            "Early stopping:  0.013254224935213962\n",
            "Epoch: 074, Loss: 0.4629, Train: 0.8843, Test: 0.7585\n",
            "Early stopping:  0.014327139417093508\n",
            "Epoch: 075, Loss: 0.4544, Train: 0.8901, Test: 0.7562\n",
            "Early stopping:  0.015241002217175115\n",
            "Epoch: 076, Loss: 0.4477, Train: 0.8847, Test: 0.7602\n",
            "Early stopping:  0.014782563575370842\n",
            "Epoch: 077, Loss: 0.4414, Train: 0.8939, Test: 0.7557\n",
            "Early stopping:  0.012776559142018886\n",
            "Epoch: 078, Loss: 0.4345, Train: 0.8911, Test: 0.7619\n",
            "Early stopping:  0.0110466041197882\n",
            "Epoch: 079, Loss: 0.4282, Train: 0.8981, Test: 0.7602\n",
            "Early stopping:  0.010374729358374671\n",
            "Epoch: 080, Loss: 0.4199, Train: 0.8985, Test: 0.7625\n",
            "Early stopping:  0.01090249993224225\n",
            "Epoch: 081, Loss: 0.4089, Train: 0.9046, Test: 0.7602\n",
            "Early stopping:  0.012683990608714765\n",
            "Epoch: 082, Loss: 0.3994, Train: 0.9053, Test: 0.7608\n",
            "Early stopping:  0.014208319896289989\n",
            "Epoch: 083, Loss: 0.3933, Train: 0.9064, Test: 0.7619\n",
            "Early stopping:  0.014341298283886176\n",
            "Epoch: 084, Loss: 0.3872, Train: 0.9114, Test: 0.7659\n",
            "Early stopping:  0.012912257629562201\n",
            "Epoch: 085, Loss: 0.3820, Train: 0.9032, Test: 0.7591\n",
            "Early stopping:  0.010508681772991173\n",
            "Epoch: 086, Loss: 0.3797, Train: 0.9131, Test: 0.7676\n",
            "Early stopping:  0.00812267839171832\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.72      0.80      0.76       190\n",
            "         capital_goods       0.76      0.71      0.73       127\n",
            "conglomerates_industry       1.00      0.75      0.86        20\n",
            "     consumer_cyclical       0.72      0.76      0.74       198\n",
            " consumer_non-cyclical       0.72      0.67      0.69       112\n",
            "                energy       0.87      0.75      0.80        71\n",
            "             financial       0.78      0.76      0.77       192\n",
            "            healthcare       0.83      0.76      0.79        79\n",
            "              services       0.77      0.80      0.78       519\n",
            "            technology       0.75      0.69      0.72        99\n",
            "        transportation       0.85      0.83      0.84       101\n",
            "             utilities       0.84      0.86      0.85        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.80      0.76      0.78      1764\n",
            "          weighted avg       0.77      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 5 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4410, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2343, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1461745429424529\n",
            "Epoch: 003, Loss: 2.1672, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.142719602946213\n",
            "Epoch: 004, Loss: 2.1233, Train: 0.3050, Test: 0.3067\n",
            "Early stopping:  0.14063316236420714\n",
            "Epoch: 005, Loss: 2.0605, Train: 0.3486, Test: 0.3418\n",
            "Early stopping:  0.1462300851679888\n",
            "Epoch: 006, Loss: 2.0013, Train: 0.3661, Test: 0.3532\n",
            "Early stopping:  0.09066224313660998\n",
            "Epoch: 007, Loss: 1.9417, Train: 0.3674, Test: 0.3577\n",
            "Early stopping:  0.09073669517439957\n",
            "Epoch: 008, Loss: 1.8852, Train: 0.3723, Test: 0.3617\n",
            "Early stopping:  0.09410689617725139\n",
            "Epoch: 009, Loss: 1.8285, Train: 0.3983, Test: 0.3866\n",
            "Early stopping:  0.09173695829190813\n",
            "Epoch: 010, Loss: 1.7655, Train: 0.4363, Test: 0.4223\n",
            "Early stopping:  0.0924986418576121\n",
            "Epoch: 011, Loss: 1.6977, Train: 0.4717, Test: 0.4484\n",
            "Early stopping:  0.09615012981250734\n",
            "Epoch: 012, Loss: 1.6322, Train: 0.4900, Test: 0.4773\n",
            "Early stopping:  0.10074287913381758\n",
            "Epoch: 013, Loss: 1.5713, Train: 0.5082, Test: 0.4875\n",
            "Early stopping:  0.10242557141284805\n",
            "Epoch: 014, Loss: 1.5114, Train: 0.5287, Test: 0.5051\n",
            "Early stopping:  0.1003745755587574\n",
            "Epoch: 015, Loss: 1.4543, Train: 0.5508, Test: 0.5272\n",
            "Early stopping:  0.09609056399470126\n",
            "Epoch: 016, Loss: 1.4019, Train: 0.5779, Test: 0.5516\n",
            "Early stopping:  0.09134152291153691\n",
            "Epoch: 017, Loss: 1.3522, Train: 0.5935, Test: 0.5680\n",
            "Early stopping:  0.08666361601820495\n",
            "Epoch: 018, Loss: 1.3097, Train: 0.6033, Test: 0.5788\n",
            "Early stopping:  0.08006321336957598\n",
            "Epoch: 019, Loss: 1.2723, Train: 0.6105, Test: 0.5828\n",
            "Early stopping:  0.07233129539107504\n",
            "Epoch: 020, Loss: 1.2355, Train: 0.6151, Test: 0.5924\n",
            "Early stopping:  0.06540834368750979\n",
            "Epoch: 021, Loss: 1.2031, Train: 0.6257, Test: 0.5964\n",
            "Early stopping:  0.058941359877159934\n",
            "Epoch: 022, Loss: 1.1750, Train: 0.6365, Test: 0.6032\n",
            "Early stopping:  0.05361834697371988\n",
            "Epoch: 023, Loss: 1.1488, Train: 0.6447, Test: 0.6105\n",
            "Early stopping:  0.048740068307931275\n",
            "Epoch: 024, Loss: 1.1201, Train: 0.6519, Test: 0.6241\n",
            "Early stopping:  0.04509732909559935\n",
            "Epoch: 025, Loss: 1.0940, Train: 0.6597, Test: 0.6344\n",
            "Early stopping:  0.04318643320094849\n",
            "Epoch: 026, Loss: 1.0700, Train: 0.6688, Test: 0.6434\n",
            "Early stopping:  0.04188909702575703\n",
            "Epoch: 027, Loss: 1.0454, Train: 0.6742, Test: 0.6463\n",
            "Early stopping:  0.04063464794311791\n",
            "Epoch: 028, Loss: 1.0227, Train: 0.6787, Test: 0.6531\n",
            "Early stopping:  0.03848675257279023\n",
            "Epoch: 029, Loss: 1.0014, Train: 0.6889, Test: 0.6565\n",
            "Early stopping:  0.03675802068303101\n",
            "Epoch: 030, Loss: 0.9800, Train: 0.6949, Test: 0.6627\n",
            "Early stopping:  0.03541458912998326\n",
            "Epoch: 031, Loss: 0.9591, Train: 0.7038, Test: 0.6655\n",
            "Early stopping:  0.034039084146575735\n",
            "Epoch: 032, Loss: 0.9402, Train: 0.7088, Test: 0.6678\n",
            "Early stopping:  0.032793431971589404\n",
            "Epoch: 033, Loss: 0.9208, Train: 0.7166, Test: 0.6723\n",
            "Early stopping:  0.031801911301896184\n",
            "Epoch: 034, Loss: 0.9024, Train: 0.7256, Test: 0.6723\n",
            "Early stopping:  0.03062890398122263\n",
            "Epoch: 035, Loss: 0.8855, Train: 0.7302, Test: 0.6814\n",
            "Early stopping:  0.02926783141156828\n",
            "Epoch: 036, Loss: 0.8684, Train: 0.7386, Test: 0.6831\n",
            "Early stopping:  0.02827723396064524\n",
            "Epoch: 037, Loss: 0.8521, Train: 0.7473, Test: 0.6899\n",
            "Early stopping:  0.02708726027662016\n",
            "Epoch: 038, Loss: 0.8365, Train: 0.7530, Test: 0.6922\n",
            "Early stopping:  0.026113457612380098\n",
            "Epoch: 039, Loss: 0.8209, Train: 0.7591, Test: 0.6978\n",
            "Early stopping:  0.025482029320754416\n",
            "Epoch: 040, Loss: 0.8056, Train: 0.7655, Test: 0.6984\n",
            "Early stopping:  0.024796539295681923\n",
            "Epoch: 041, Loss: 0.7909, Train: 0.7707, Test: 0.7001\n",
            "Early stopping:  0.02423377008286978\n",
            "Epoch: 042, Loss: 0.7765, Train: 0.7754, Test: 0.7058\n",
            "Early stopping:  0.023738017059453116\n",
            "Epoch: 043, Loss: 0.7625, Train: 0.7817, Test: 0.7137\n",
            "Early stopping:  0.023087586491724824\n",
            "Epoch: 044, Loss: 0.7486, Train: 0.7843, Test: 0.7166\n",
            "Early stopping:  0.022544153149229477\n",
            "Epoch: 045, Loss: 0.7347, Train: 0.7880, Test: 0.7149\n",
            "Early stopping:  0.022187261033281887\n",
            "Epoch: 046, Loss: 0.7212, Train: 0.7934, Test: 0.7154\n",
            "Early stopping:  0.02186709687663433\n",
            "Epoch: 047, Loss: 0.7079, Train: 0.7963, Test: 0.7194\n",
            "Early stopping:  0.021610499117145923\n",
            "Epoch: 048, Loss: 0.6950, Train: 0.8029, Test: 0.7217\n",
            "Early stopping:  0.02119607408850059\n",
            "Epoch: 049, Loss: 0.6824, Train: 0.8052, Test: 0.7239\n",
            "Early stopping:  0.02068369952435546\n",
            "Epoch: 050, Loss: 0.6705, Train: 0.8103, Test: 0.7319\n",
            "Early stopping:  0.020055189647624656\n",
            "Epoch: 051, Loss: 0.6598, Train: 0.8111, Test: 0.7262\n",
            "Early stopping:  0.01906714125919581\n",
            "Epoch: 052, Loss: 0.6497, Train: 0.8189, Test: 0.7319\n",
            "Early stopping:  0.017915616563144145\n",
            "Epoch: 053, Loss: 0.6378, Train: 0.8198, Test: 0.7347\n",
            "Early stopping:  0.01742453287164713\n",
            "Epoch: 054, Loss: 0.6265, Train: 0.8248, Test: 0.7347\n",
            "Early stopping:  0.01741394608896894\n",
            "Epoch: 055, Loss: 0.6168, Train: 0.8297, Test: 0.7421\n",
            "Early stopping:  0.017295842174672896\n",
            "Epoch: 056, Loss: 0.6079, Train: 0.8350, Test: 0.7375\n",
            "Early stopping:  0.016554729165835735\n",
            "Epoch: 057, Loss: 0.5949, Train: 0.8404, Test: 0.7404\n",
            "Early stopping:  0.01653183826319096\n",
            "Epoch: 058, Loss: 0.5835, Train: 0.8422, Test: 0.7443\n",
            "Early stopping:  0.017111654857911305\n",
            "Epoch: 059, Loss: 0.5753, Train: 0.8429, Test: 0.7426\n",
            "Early stopping:  0.01702404541392924\n",
            "Epoch: 060, Loss: 0.5661, Train: 0.8462, Test: 0.7500\n",
            "Early stopping:  0.016386107009746832\n",
            "Epoch: 061, Loss: 0.5559, Train: 0.8531, Test: 0.7494\n",
            "Early stopping:  0.015095403391869591\n",
            "Epoch: 062, Loss: 0.5469, Train: 0.8483, Test: 0.7540\n",
            "Early stopping:  0.014648496299539066\n",
            "Epoch: 063, Loss: 0.5404, Train: 0.8589, Test: 0.7596\n",
            "Early stopping:  0.014116381869885685\n",
            "Epoch: 064, Loss: 0.5329, Train: 0.8518, Test: 0.7579\n",
            "Early stopping:  0.012994313568088372\n",
            "Epoch: 065, Loss: 0.5255, Train: 0.8602, Test: 0.7540\n",
            "Early stopping:  0.011822063811369425\n",
            "Epoch: 066, Loss: 0.5194, Train: 0.8578, Test: 0.7585\n",
            "Early stopping:  0.011070365098259064\n",
            "Epoch: 067, Loss: 0.5120, Train: 0.8679, Test: 0.7545\n",
            "Early stopping:  0.011149100290641381\n",
            "Epoch: 068, Loss: 0.4976, Train: 0.8679, Test: 0.7551\n",
            "Early stopping:  0.013512107881706476\n",
            "Epoch: 069, Loss: 0.4905, Train: 0.8662, Test: 0.7619\n",
            "Early stopping:  0.014650303121351103\n",
            "Epoch: 070, Loss: 0.4876, Train: 0.8771, Test: 0.7574\n",
            "Early stopping:  0.013757183062280827\n",
            "Epoch: 071, Loss: 0.4787, Train: 0.8721, Test: 0.7608\n",
            "Early stopping:  0.012458959435149205\n",
            "Epoch: 072, Loss: 0.4718, Train: 0.8818, Test: 0.7664\n",
            "Early stopping:  0.010150920716401177\n",
            "Epoch: 073, Loss: 0.4655, Train: 0.8808, Test: 0.7602\n",
            "Early stopping:  0.010506744215428323\n",
            "Epoch: 074, Loss: 0.4567, Train: 0.8837, Test: 0.7647\n",
            "Early stopping:  0.011888849749623894\n",
            "Epoch: 075, Loss: 0.4486, Train: 0.8884, Test: 0.7693\n",
            "Early stopping:  0.011916600162949067\n",
            "Epoch: 076, Loss: 0.4422, Train: 0.8884, Test: 0.7630\n",
            "Early stopping:  0.012048825403793866\n",
            "Epoch: 077, Loss: 0.4361, Train: 0.8928, Test: 0.7653\n",
            "Early stopping:  0.011634695085162834\n",
            "Epoch: 078, Loss: 0.4304, Train: 0.8897, Test: 0.7687\n",
            "Early stopping:  0.010325915666887772\n",
            "Epoch: 079, Loss: 0.4246, Train: 0.8959, Test: 0.7647\n",
            "Early stopping:  0.009477501516397627\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.80      0.75      0.77       190\n",
            "         capital_goods       0.68      0.76      0.72       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.69      0.75      0.72       198\n",
            " consumer_non-cyclical       0.75      0.66      0.70       112\n",
            "                energy       0.90      0.75      0.82        71\n",
            "             financial       0.81      0.76      0.78       192\n",
            "            healthcare       0.84      0.75      0.79        79\n",
            "              services       0.75      0.80      0.78       519\n",
            "            technology       0.74      0.71      0.72        99\n",
            "        transportation       0.84      0.81      0.82       101\n",
            "             utilities       0.84      0.82      0.83        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.80      0.76      0.78      1764\n",
            "          weighted avg       0.77      0.76      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 6 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5652, Train: 0.2935, Test: 0.2925\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3483, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.15339918712679632\n",
            "Epoch: 003, Loss: 2.2019, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1827679151457743\n",
            "Epoch: 004, Loss: 2.1666, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.18109841444394606\n",
            "Epoch: 005, Loss: 2.1386, Train: 0.3011, Test: 0.3027\n",
            "Early stopping:  0.1766860085014939\n",
            "Epoch: 006, Loss: 2.0725, Train: 0.3326, Test: 0.3260\n",
            "Early stopping:  0.10257939000652541\n",
            "Epoch: 007, Loss: 2.0129, Train: 0.3718, Test: 0.3571\n",
            "Early stopping:  0.0757392438866122\n",
            "Epoch: 008, Loss: 1.9643, Train: 0.3889, Test: 0.3736\n",
            "Early stopping:  0.08434093752166968\n",
            "Epoch: 009, Loss: 1.9173, Train: 0.3932, Test: 0.3804\n",
            "Early stopping:  0.08731462798339061\n",
            "Epoch: 010, Loss: 1.8701, Train: 0.3971, Test: 0.3878\n",
            "Early stopping:  0.07922168440954303\n",
            "Epoch: 011, Loss: 1.8208, Train: 0.4124, Test: 0.3957\n",
            "Early stopping:  0.0756573824117661\n",
            "Epoch: 012, Loss: 1.7666, Train: 0.4357, Test: 0.4178\n",
            "Early stopping:  0.07781913822348661\n",
            "Epoch: 013, Loss: 1.7105, Train: 0.4563, Test: 0.4342\n",
            "Early stopping:  0.08182374665095227\n",
            "Epoch: 014, Loss: 1.6594, Train: 0.4726, Test: 0.4529\n",
            "Early stopping:  0.08409181889354936\n",
            "Epoch: 015, Loss: 1.6130, Train: 0.4839, Test: 0.4609\n",
            "Early stopping:  0.08272314881471411\n",
            "Epoch: 016, Loss: 1.5649, Train: 0.4977, Test: 0.4796\n",
            "Early stopping:  0.07926228386520924\n",
            "Epoch: 017, Loss: 1.5164, Train: 0.5111, Test: 0.5028\n",
            "Early stopping:  0.07631997683374371\n",
            "Epoch: 018, Loss: 1.4709, Train: 0.5375, Test: 0.5227\n",
            "Early stopping:  0.07489081705246475\n",
            "Epoch: 019, Loss: 1.4265, Train: 0.5606, Test: 0.5402\n",
            "Early stopping:  0.07385528237733241\n",
            "Epoch: 020, Loss: 1.3843, Train: 0.5809, Test: 0.5567\n",
            "Early stopping:  0.07134350895514618\n",
            "Epoch: 021, Loss: 1.3476, Train: 0.5931, Test: 0.5641\n",
            "Early stopping:  0.0671246674604721\n",
            "Epoch: 022, Loss: 1.3133, Train: 0.5983, Test: 0.5748\n",
            "Early stopping:  0.06241630953855381\n",
            "Epoch: 023, Loss: 1.2780, Train: 0.6030, Test: 0.5884\n",
            "Early stopping:  0.05824707472401673\n",
            "Epoch: 024, Loss: 1.2445, Train: 0.6102, Test: 0.5930\n",
            "Early stopping:  0.05521309803978311\n",
            "Epoch: 025, Loss: 1.2147, Train: 0.6197, Test: 0.5992\n",
            "Early stopping:  0.0529176024187615\n",
            "Epoch: 026, Loss: 1.1877, Train: 0.6302, Test: 0.6066\n",
            "Early stopping:  0.04979881964181098\n",
            "Epoch: 027, Loss: 1.1619, Train: 0.6375, Test: 0.6162\n",
            "Early stopping:  0.045776081494793994\n",
            "Epoch: 028, Loss: 1.1360, Train: 0.6411, Test: 0.6219\n",
            "Early stopping:  0.042697023405109655\n",
            "Epoch: 029, Loss: 1.1124, Train: 0.6481, Test: 0.6241\n",
            "Early stopping:  0.04052019887146672\n",
            "Epoch: 030, Loss: 1.0920, Train: 0.6562, Test: 0.6304\n",
            "Early stopping:  0.03811050642392078\n",
            "Epoch: 031, Loss: 1.0722, Train: 0.6652, Test: 0.6366\n",
            "Early stopping:  0.03535628807623453\n",
            "Epoch: 032, Loss: 1.0510, Train: 0.6719, Test: 0.6372\n",
            "Early stopping:  0.0332355877368401\n",
            "Epoch: 033, Loss: 1.0316, Train: 0.6813, Test: 0.6434\n",
            "Early stopping:  0.032035329571736736\n",
            "Epoch: 034, Loss: 1.0136, Train: 0.6875, Test: 0.6463\n",
            "Early stopping:  0.031240861208920757\n",
            "Epoch: 035, Loss: 0.9949, Train: 0.6947, Test: 0.6468\n",
            "Early stopping:  0.03037712282341825\n",
            "Epoch: 036, Loss: 0.9766, Train: 0.7007, Test: 0.6570\n",
            "Early stopping:  0.02934843218057709\n",
            "Epoch: 037, Loss: 0.9595, Train: 0.7062, Test: 0.6621\n",
            "Early stopping:  0.02865653773276761\n",
            "Epoch: 038, Loss: 0.9434, Train: 0.7149, Test: 0.6695\n",
            "Early stopping:  0.02780383200619959\n",
            "Epoch: 039, Loss: 0.9273, Train: 0.7183, Test: 0.6701\n",
            "Early stopping:  0.026648977739151248\n",
            "Epoch: 040, Loss: 0.9112, Train: 0.7230, Test: 0.6757\n",
            "Early stopping:  0.025769722275815823\n",
            "Epoch: 041, Loss: 0.8960, Train: 0.7282, Test: 0.6763\n",
            "Early stopping:  0.025179404011964324\n",
            "Epoch: 042, Loss: 0.8810, Train: 0.7351, Test: 0.6842\n",
            "Early stopping:  0.024688852248166936\n",
            "Epoch: 043, Loss: 0.8658, Train: 0.7427, Test: 0.6910\n",
            "Early stopping:  0.02422862110843793\n",
            "Epoch: 044, Loss: 0.8513, Train: 0.7495, Test: 0.6950\n",
            "Early stopping:  0.023722673133489688\n",
            "Epoch: 045, Loss: 0.8371, Train: 0.7530, Test: 0.6973\n",
            "Early stopping:  0.02333869548467325\n",
            "Epoch: 046, Loss: 0.8224, Train: 0.7591, Test: 0.7007\n",
            "Early stopping:  0.023051625428794176\n",
            "Epoch: 047, Loss: 0.8083, Train: 0.7645, Test: 0.7041\n",
            "Early stopping:  0.0227266889900116\n",
            "Epoch: 048, Loss: 0.7945, Train: 0.7688, Test: 0.7058\n",
            "Early stopping:  0.02249965978850262\n",
            "Epoch: 049, Loss: 0.7807, Train: 0.7743, Test: 0.7092\n",
            "Early stopping:  0.022256660053884658\n",
            "Epoch: 050, Loss: 0.7669, Train: 0.7800, Test: 0.7092\n",
            "Early stopping:  0.02193353267736765\n",
            "Epoch: 051, Loss: 0.7532, Train: 0.7849, Test: 0.7115\n",
            "Early stopping:  0.021808571706785032\n",
            "Epoch: 052, Loss: 0.7397, Train: 0.7892, Test: 0.7166\n",
            "Early stopping:  0.021675340048644742\n",
            "Epoch: 053, Loss: 0.7263, Train: 0.7926, Test: 0.7211\n",
            "Early stopping:  0.0214849792609646\n",
            "Epoch: 054, Loss: 0.7130, Train: 0.7991, Test: 0.7222\n",
            "Early stopping:  0.02129997920624378\n",
            "Epoch: 055, Loss: 0.7001, Train: 0.8019, Test: 0.7217\n",
            "Early stopping:  0.02100961884187026\n",
            "Epoch: 056, Loss: 0.6876, Train: 0.8086, Test: 0.7239\n",
            "Early stopping:  0.02063515005779047\n",
            "Epoch: 057, Loss: 0.6780, Train: 0.7987, Test: 0.7222\n",
            "Early stopping:  0.019334830258934187\n",
            "Epoch: 058, Loss: 0.6743, Train: 0.8127, Test: 0.7268\n",
            "Early stopping:  0.01599455065045696\n",
            "Epoch: 059, Loss: 0.6638, Train: 0.8192, Test: 0.7296\n",
            "Early stopping:  0.013758186246108655\n",
            "Epoch: 060, Loss: 0.6406, Train: 0.8150, Test: 0.7336\n",
            "Early stopping:  0.017951729136277\n",
            "Epoch: 061, Loss: 0.6368, Train: 0.8246, Test: 0.7285\n",
            "Early stopping:  0.019052626079457912\n",
            "Epoch: 062, Loss: 0.6288, Train: 0.8314, Test: 0.7353\n",
            "Early stopping:  0.01928326402683393\n",
            "Epoch: 063, Loss: 0.6093, Train: 0.8223, Test: 0.7392\n",
            "Early stopping:  0.019717878355593608\n",
            "Epoch: 064, Loss: 0.6076, Train: 0.8377, Test: 0.7375\n",
            "Early stopping:  0.0153623977417967\n",
            "Epoch: 065, Loss: 0.5953, Train: 0.8428, Test: 0.7449\n",
            "Early stopping:  0.016860302300322313\n",
            "Epoch: 066, Loss: 0.5815, Train: 0.8330, Test: 0.7443\n",
            "Early stopping:  0.01757420730613078\n",
            "Epoch: 067, Loss: 0.5792, Train: 0.8477, Test: 0.7466\n",
            "Early stopping:  0.014075855804934359\n",
            "Epoch: 068, Loss: 0.5649, Train: 0.8521, Test: 0.7449\n",
            "Early stopping:  0.01631375438616458\n",
            "Epoch: 069, Loss: 0.5560, Train: 0.8457, Test: 0.7472\n",
            "Early stopping:  0.015302087174693191\n",
            "Epoch: 070, Loss: 0.5509, Train: 0.8595, Test: 0.7534\n",
            "Early stopping:  0.013647305200496966\n",
            "Epoch: 071, Loss: 0.5375, Train: 0.8628, Test: 0.7523\n",
            "Early stopping:  0.015604106639081709\n",
            "Epoch: 072, Loss: 0.5312, Train: 0.8576, Test: 0.7506\n",
            "Early stopping:  0.01369969379352509\n",
            "Epoch: 073, Loss: 0.5245, Train: 0.8679, Test: 0.7494\n",
            "Early stopping:  0.013233887531338013\n",
            "Epoch: 074, Loss: 0.5127, Train: 0.8693, Test: 0.7557\n",
            "Early stopping:  0.014282233820707408\n",
            "Epoch: 075, Loss: 0.5068, Train: 0.8663, Test: 0.7523\n",
            "Early stopping:  0.012716242105508738\n",
            "Epoch: 076, Loss: 0.5001, Train: 0.8740, Test: 0.7568\n",
            "Early stopping:  0.01272008114677695\n",
            "Epoch: 077, Loss: 0.4894, Train: 0.8748, Test: 0.7579\n",
            "Early stopping:  0.013185198915860324\n",
            "Epoch: 078, Loss: 0.4828, Train: 0.8758, Test: 0.7579\n",
            "Early stopping:  0.012277711662945721\n",
            "Epoch: 079, Loss: 0.4768, Train: 0.8798, Test: 0.7619\n",
            "Early stopping:  0.012275828379274732\n",
            "Epoch: 080, Loss: 0.4678, Train: 0.8818, Test: 0.7625\n",
            "Early stopping:  0.012265564732693624\n",
            "Epoch: 081, Loss: 0.4599, Train: 0.8810, Test: 0.7625\n",
            "Early stopping:  0.01173838712569807\n",
            "Epoch: 082, Loss: 0.4537, Train: 0.8867, Test: 0.7630\n",
            "Early stopping:  0.011887254460846045\n",
            "Epoch: 083, Loss: 0.4474, Train: 0.8876, Test: 0.7642\n",
            "Early stopping:  0.011564020544221882\n",
            "Epoch: 084, Loss: 0.4401, Train: 0.8915, Test: 0.7670\n",
            "Early stopping:  0.010720597193768533\n",
            "Epoch: 085, Loss: 0.4318, Train: 0.8947, Test: 0.7681\n",
            "Early stopping:  0.011046226699623598\n",
            "Epoch: 086, Loss: 0.4247, Train: 0.8954, Test: 0.7659\n",
            "Early stopping:  0.011646774279119459\n",
            "Epoch: 087, Loss: 0.4194, Train: 0.9002, Test: 0.7659\n",
            "Early stopping:  0.011325364107640953\n",
            "Epoch: 088, Loss: 0.4143, Train: 0.8976, Test: 0.7698\n",
            "Early stopping:  0.010196870030478065\n",
            "Epoch: 089, Loss: 0.4090, Train: 0.9036, Test: 0.7653\n",
            "Early stopping:  0.008892800271143982\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.77      0.79      0.78       190\n",
            "         capital_goods       0.77      0.69      0.73       127\n",
            "conglomerates_industry       1.00      0.70      0.82        20\n",
            "     consumer_cyclical       0.67      0.75      0.71       198\n",
            " consumer_non-cyclical       0.72      0.61      0.66       112\n",
            "                energy       0.91      0.75      0.82        71\n",
            "             financial       0.79      0.76      0.78       192\n",
            "            healthcare       0.91      0.75      0.82        79\n",
            "              services       0.74      0.81      0.77       519\n",
            "            technology       0.76      0.72      0.74        99\n",
            "        transportation       0.88      0.84      0.86       101\n",
            "             utilities       0.83      0.80      0.82        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.81      0.75      0.78      1764\n",
            "          weighted avg       0.77      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 7 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5439, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3081, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.16677035911057447\n",
            "Epoch: 003, Loss: 2.1805, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.18437771674891834\n",
            "Epoch: 004, Loss: 2.1599, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1765069034159969\n",
            "Epoch: 005, Loss: 2.1188, Train: 0.3014, Test: 0.3016\n",
            "Early stopping:  0.17261229487562701\n",
            "Epoch: 006, Loss: 2.0516, Train: 0.3430, Test: 0.3345\n",
            "Early stopping:  0.09449667665578865\n",
            "Epoch: 007, Loss: 1.9894, Train: 0.3757, Test: 0.3662\n",
            "Early stopping:  0.07904988288355269\n",
            "Epoch: 008, Loss: 1.9365, Train: 0.3841, Test: 0.3736\n",
            "Early stopping:  0.0913153551234735\n",
            "Epoch: 009, Loss: 1.8884, Train: 0.3869, Test: 0.3719\n",
            "Early stopping:  0.09127904146749438\n",
            "Epoch: 010, Loss: 1.8386, Train: 0.3960, Test: 0.3815\n",
            "Early stopping:  0.08344633602829878\n",
            "Epoch: 011, Loss: 1.7839, Train: 0.4166, Test: 0.4070\n",
            "Early stopping:  0.0804744717839967\n",
            "Epoch: 012, Loss: 1.7250, Train: 0.4475, Test: 0.4348\n",
            "Early stopping:  0.08347479893535478\n",
            "Epoch: 013, Loss: 1.6667, Train: 0.4724, Test: 0.4575\n",
            "Early stopping:  0.08813553009935167\n",
            "Epoch: 014, Loss: 1.6140, Train: 0.4805, Test: 0.4739\n",
            "Early stopping:  0.08958784946957121\n",
            "Epoch: 015, Loss: 1.5640, Train: 0.4955, Test: 0.4943\n",
            "Early stopping:  0.08717626647040987\n",
            "Epoch: 016, Loss: 1.5107, Train: 0.5196, Test: 0.5074\n",
            "Early stopping:  0.08404948246821399\n",
            "Epoch: 017, Loss: 1.4572, Train: 0.5425, Test: 0.5283\n",
            "Early stopping:  0.08256903350424193\n",
            "Epoch: 018, Loss: 1.4092, Train: 0.5647, Test: 0.5465\n",
            "Early stopping:  0.08164232185715865\n",
            "Epoch: 019, Loss: 1.3657, Train: 0.5833, Test: 0.5607\n",
            "Early stopping:  0.07880864745023106\n",
            "Epoch: 020, Loss: 1.3249, Train: 0.5975, Test: 0.5692\n",
            "Early stopping:  0.07331798009499914\n",
            "Epoch: 021, Loss: 1.2864, Train: 0.6017, Test: 0.5743\n",
            "Early stopping:  0.06741856117135263\n",
            "Epoch: 022, Loss: 1.2506, Train: 0.6060, Test: 0.5811\n",
            "Early stopping:  0.06275231846504996\n",
            "Epoch: 023, Loss: 1.2196, Train: 0.6118, Test: 0.5890\n",
            "Early stopping:  0.058026939982913285\n",
            "Epoch: 024, Loss: 1.1917, Train: 0.6213, Test: 0.5935\n",
            "Early stopping:  0.05278692972440424\n",
            "Epoch: 025, Loss: 1.1650, Train: 0.6297, Test: 0.6032\n",
            "Early stopping:  0.04778490951226191\n",
            "Epoch: 026, Loss: 1.1416, Train: 0.6387, Test: 0.6105\n",
            "Early stopping:  0.04317969255919426\n",
            "Epoch: 027, Loss: 1.1202, Train: 0.6448, Test: 0.6173\n",
            "Early stopping:  0.039421906151937074\n",
            "Epoch: 028, Loss: 1.0981, Train: 0.6526, Test: 0.6253\n",
            "Early stopping:  0.036716926984398386\n",
            "Epoch: 029, Loss: 1.0757, Train: 0.6587, Test: 0.6310\n",
            "Early stopping:  0.03510222894672544\n",
            "Epoch: 030, Loss: 1.0554, Train: 0.6671, Test: 0.6344\n",
            "Early stopping:  0.03430262576294179\n",
            "Epoch: 031, Loss: 1.0361, Train: 0.6712, Test: 0.6383\n",
            "Early stopping:  0.03337002257832938\n",
            "Epoch: 032, Loss: 1.0156, Train: 0.6794, Test: 0.6406\n",
            "Early stopping:  0.03235835679910422\n",
            "Epoch: 033, Loss: 0.9964, Train: 0.6881, Test: 0.6423\n",
            "Early stopping:  0.03138553136197399\n",
            "Epoch: 034, Loss: 0.9785, Train: 0.6964, Test: 0.6434\n",
            "Early stopping:  0.03060275390019073\n",
            "Epoch: 035, Loss: 0.9611, Train: 0.7035, Test: 0.6491\n",
            "Early stopping:  0.02962512901299356\n",
            "Epoch: 036, Loss: 0.9434, Train: 0.7086, Test: 0.6514\n",
            "Early stopping:  0.028405119555329733\n",
            "Epoch: 037, Loss: 0.9270, Train: 0.7156, Test: 0.6621\n",
            "Early stopping:  0.027472649280794723\n",
            "Epoch: 038, Loss: 0.9112, Train: 0.7222, Test: 0.6678\n",
            "Early stopping:  0.026685363251119264\n",
            "Epoch: 039, Loss: 0.8961, Train: 0.7299, Test: 0.6729\n",
            "Early stopping:  0.025658415356176374\n",
            "Epoch: 040, Loss: 0.8814, Train: 0.7349, Test: 0.6763\n",
            "Early stopping:  0.02452188573736181\n",
            "Epoch: 041, Loss: 0.8667, Train: 0.7407, Test: 0.6797\n",
            "Early stopping:  0.023778942706836732\n",
            "Epoch: 042, Loss: 0.8529, Train: 0.7472, Test: 0.6842\n",
            "Early stopping:  0.023062590601198835\n",
            "Epoch: 043, Loss: 0.8390, Train: 0.7516, Test: 0.6865\n",
            "Early stopping:  0.022561477492536515\n",
            "Epoch: 044, Loss: 0.8250, Train: 0.7587, Test: 0.6927\n",
            "Early stopping:  0.02220542173826908\n",
            "Epoch: 045, Loss: 0.8115, Train: 0.7663, Test: 0.6956\n",
            "Early stopping:  0.021870162762920187\n",
            "Epoch: 046, Loss: 0.7979, Train: 0.7697, Test: 0.6973\n",
            "Early stopping:  0.021736415901448054\n",
            "Epoch: 047, Loss: 0.7847, Train: 0.7741, Test: 0.7007\n",
            "Early stopping:  0.0214484476442826\n",
            "Epoch: 048, Loss: 0.7715, Train: 0.7801, Test: 0.7029\n",
            "Early stopping:  0.021170436766807178\n",
            "Epoch: 049, Loss: 0.7584, Train: 0.7836, Test: 0.7092\n",
            "Early stopping:  0.020963860076014734\n",
            "Epoch: 050, Loss: 0.7458, Train: 0.7866, Test: 0.7126\n",
            "Early stopping:  0.02063548194869183\n",
            "Epoch: 051, Loss: 0.7329, Train: 0.7889, Test: 0.7137\n",
            "Early stopping:  0.020460440646458022\n",
            "Epoch: 052, Loss: 0.7205, Train: 0.7938, Test: 0.7171\n",
            "Early stopping:  0.020164937606693616\n",
            "Epoch: 053, Loss: 0.7082, Train: 0.7964, Test: 0.7211\n",
            "Early stopping:  0.019898954363842647\n",
            "Epoch: 054, Loss: 0.6960, Train: 0.8008, Test: 0.7234\n",
            "Early stopping:  0.01965916710640707\n",
            "Epoch: 055, Loss: 0.6839, Train: 0.8049, Test: 0.7245\n",
            "Early stopping:  0.019351518569617615\n",
            "Epoch: 056, Loss: 0.6721, Train: 0.8092, Test: 0.7256\n",
            "Early stopping:  0.01914842911022209\n",
            "Epoch: 057, Loss: 0.6603, Train: 0.8144, Test: 0.7262\n",
            "Early stopping:  0.018931287264431305\n",
            "Epoch: 058, Loss: 0.6489, Train: 0.8147, Test: 0.7290\n",
            "Early stopping:  0.01864467273289464\n",
            "Epoch: 059, Loss: 0.6384, Train: 0.8215, Test: 0.7273\n",
            "Early stopping:  0.0180749479953893\n",
            "Epoch: 060, Loss: 0.6297, Train: 0.8143, Test: 0.7251\n",
            "Early stopping:  0.016875094078588625\n",
            "Epoch: 061, Loss: 0.6232, Train: 0.8306, Test: 0.7375\n",
            "Early stopping:  0.01483807845451474\n",
            "Epoch: 062, Loss: 0.6118, Train: 0.8309, Test: 0.7313\n",
            "Early stopping:  0.01417199333387995\n",
            "Epoch: 063, Loss: 0.5956, Train: 0.8318, Test: 0.7347\n",
            "Early stopping:  0.01660098384281807\n",
            "Epoch: 064, Loss: 0.5879, Train: 0.8391, Test: 0.7443\n",
            "Early stopping:  0.01775592844781622\n",
            "Epoch: 065, Loss: 0.5816, Train: 0.8384, Test: 0.7358\n",
            "Early stopping:  0.017178160054755674\n",
            "Epoch: 066, Loss: 0.5683, Train: 0.8446, Test: 0.7426\n",
            "Early stopping:  0.01619200068529067\n",
            "Epoch: 067, Loss: 0.5581, Train: 0.8501, Test: 0.7455\n",
            "Early stopping:  0.015086712594098318\n",
            "Epoch: 068, Loss: 0.5520, Train: 0.8479, Test: 0.7438\n",
            "Early stopping:  0.0151622239387862\n",
            "Epoch: 069, Loss: 0.5417, Train: 0.8557, Test: 0.7443\n",
            "Early stopping:  0.015297202663088517\n",
            "Epoch: 070, Loss: 0.5310, Train: 0.8618, Test: 0.7523\n",
            "Early stopping:  0.014440974650469243\n",
            "Epoch: 071, Loss: 0.5240, Train: 0.8576, Test: 0.7455\n",
            "Early stopping:  0.014186965305893437\n",
            "Epoch: 072, Loss: 0.5158, Train: 0.8652, Test: 0.7500\n",
            "Early stopping:  0.014319236980134736\n",
            "Epoch: 073, Loss: 0.5062, Train: 0.8681, Test: 0.7568\n",
            "Early stopping:  0.013668906841812175\n",
            "Epoch: 074, Loss: 0.4976, Train: 0.8674, Test: 0.7517\n",
            "Early stopping:  0.013387155261703811\n",
            "Epoch: 075, Loss: 0.4903, Train: 0.8758, Test: 0.7579\n",
            "Early stopping:  0.013517494997154737\n",
            "Epoch: 076, Loss: 0.4824, Train: 0.8735, Test: 0.7579\n",
            "Early stopping:  0.013089177740842778\n",
            "Epoch: 077, Loss: 0.4743, Train: 0.8791, Test: 0.7591\n",
            "Early stopping:  0.012479889139326125\n",
            "Epoch: 078, Loss: 0.4656, Train: 0.8837, Test: 0.7636\n",
            "Early stopping:  0.012651297742969098\n",
            "Epoch: 079, Loss: 0.4576, Train: 0.8825, Test: 0.7602\n",
            "Early stopping:  0.012991362687056116\n",
            "Epoch: 080, Loss: 0.4509, Train: 0.8905, Test: 0.7636\n",
            "Early stopping:  0.01260362348661437\n",
            "Epoch: 081, Loss: 0.4437, Train: 0.8894, Test: 0.7647\n",
            "Early stopping:  0.012018529551288872\n",
            "Epoch: 082, Loss: 0.4362, Train: 0.8964, Test: 0.7659\n",
            "Early stopping:  0.011507326567798326\n",
            "Epoch: 083, Loss: 0.4276, Train: 0.8975, Test: 0.7664\n",
            "Early stopping:  0.011858513597184423\n",
            "Epoch: 084, Loss: 0.4196, Train: 0.8978, Test: 0.7676\n",
            "Early stopping:  0.012465904143195093\n",
            "Epoch: 085, Loss: 0.4129, Train: 0.9042, Test: 0.7636\n",
            "Early stopping:  0.01237756782926162\n",
            "Epoch: 086, Loss: 0.4066, Train: 0.9008, Test: 0.7698\n",
            "Early stopping:  0.011703027397629337\n",
            "Epoch: 087, Loss: 0.4007, Train: 0.9101, Test: 0.7664\n",
            "Early stopping:  0.010576173784680315\n",
            "Epoch: 088, Loss: 0.3942, Train: 0.9022, Test: 0.7664\n",
            "Early stopping:  0.009977948522245617\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  8], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.78      0.75      0.76       190\n",
            "         capital_goods       0.76      0.70      0.73       127\n",
            "conglomerates_industry       1.00      0.70      0.82        20\n",
            "     consumer_cyclical       0.73      0.70      0.71       198\n",
            " consumer_non-cyclical       0.82      0.61      0.70       112\n",
            "                energy       0.97      0.79      0.87        71\n",
            "             financial       0.85      0.73      0.79       192\n",
            "            healthcare       0.83      0.72      0.77        79\n",
            "              services       0.70      0.87      0.78       519\n",
            "            technology       0.74      0.66      0.70        99\n",
            "        transportation       0.83      0.83      0.83       101\n",
            "             utilities       0.85      0.80      0.83        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.82      0.74      0.77      1764\n",
            "          weighted avg       0.78      0.77      0.77      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 8 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.4667, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.2436, Train: 0.2941, Test: 0.2931\n",
            "Early stopping:  0.1577585199619751\n",
            "Epoch: 003, Loss: 2.1699, Train: 0.2942, Test: 0.2931\n",
            "Early stopping:  0.15454228712379192\n",
            "Epoch: 004, Loss: 2.1320, Train: 0.2962, Test: 0.2942\n",
            "Early stopping:  0.14978697892132578\n",
            "Epoch: 005, Loss: 2.0702, Train: 0.3260, Test: 0.3260\n",
            "Early stopping:  0.1533439434194394\n",
            "Epoch: 006, Loss: 2.0088, Train: 0.3621, Test: 0.3583\n",
            "Early stopping:  0.09030353683646322\n",
            "Epoch: 007, Loss: 1.9466, Train: 0.3737, Test: 0.3702\n",
            "Early stopping:  0.0903805344463942\n",
            "Epoch: 008, Loss: 1.8833, Train: 0.3872, Test: 0.3730\n",
            "Early stopping:  0.09818306197806631\n",
            "Epoch: 009, Loss: 1.8201, Train: 0.4083, Test: 0.3974\n",
            "Early stopping:  0.09893164966786465\n",
            "Epoch: 010, Loss: 1.7555, Train: 0.4426, Test: 0.4246\n",
            "Early stopping:  0.10013424639398472\n",
            "Epoch: 011, Loss: 1.6890, Train: 0.4707, Test: 0.4495\n",
            "Early stopping:  0.10167167134438285\n",
            "Epoch: 012, Loss: 1.6263, Train: 0.4896, Test: 0.4711\n",
            "Early stopping:  0.10198163899923669\n",
            "Epoch: 013, Loss: 1.5682, Train: 0.5060, Test: 0.4921\n",
            "Early stopping:  0.1001210332870688\n",
            "Epoch: 014, Loss: 1.5092, Train: 0.5249, Test: 0.5096\n",
            "Early stopping:  0.09700240200071036\n",
            "Epoch: 015, Loss: 1.4519, Train: 0.5486, Test: 0.5255\n",
            "Early stopping:  0.09351313683168748\n",
            "Epoch: 016, Loss: 1.3997, Train: 0.5687, Test: 0.5476\n",
            "Early stopping:  0.09008447304211868\n",
            "Epoch: 017, Loss: 1.3501, Train: 0.5873, Test: 0.5663\n",
            "Early stopping:  0.08633258132182804\n",
            "Epoch: 018, Loss: 1.3055, Train: 0.5999, Test: 0.5760\n",
            "Early stopping:  0.08061264130672365\n",
            "Epoch: 019, Loss: 1.2651, Train: 0.6085, Test: 0.5884\n",
            "Early stopping:  0.07404128197179308\n",
            "Epoch: 020, Loss: 1.2260, Train: 0.6162, Test: 0.5992\n",
            "Early stopping:  0.06844474321635848\n",
            "Epoch: 021, Loss: 1.1925, Train: 0.6214, Test: 0.6009\n",
            "Early stopping:  0.06251332455232991\n",
            "Epoch: 022, Loss: 1.1629, Train: 0.6325, Test: 0.5981\n",
            "Early stopping:  0.056698874874348965\n",
            "Epoch: 023, Loss: 1.1350, Train: 0.6414, Test: 0.6134\n",
            "Early stopping:  0.051267627025183574\n",
            "Epoch: 024, Loss: 1.1093, Train: 0.6475, Test: 0.6230\n",
            "Early stopping:  0.046060917761780514\n",
            "Epoch: 025, Loss: 1.0853, Train: 0.6529, Test: 0.6259\n",
            "Early stopping:  0.04240321838568925\n",
            "Epoch: 026, Loss: 1.0620, Train: 0.6641, Test: 0.6287\n",
            "Early stopping:  0.039801013187894474\n",
            "Epoch: 027, Loss: 1.0382, Train: 0.6716, Test: 0.6327\n",
            "Early stopping:  0.03807415704805049\n",
            "Epoch: 028, Loss: 1.0176, Train: 0.6818, Test: 0.6440\n",
            "Early stopping:  0.036428760577839045\n",
            "Epoch: 029, Loss: 0.9961, Train: 0.6852, Test: 0.6474\n",
            "Early stopping:  0.03524110166733755\n",
            "Epoch: 030, Loss: 0.9745, Train: 0.6947, Test: 0.6525\n",
            "Early stopping:  0.03433703408923928\n",
            "Epoch: 031, Loss: 0.9547, Train: 0.7020, Test: 0.6559\n",
            "Early stopping:  0.033228488505930256\n",
            "Epoch: 032, Loss: 0.9360, Train: 0.7115, Test: 0.6621\n",
            "Early stopping:  0.03235935214210041\n",
            "Epoch: 033, Loss: 0.9168, Train: 0.7163, Test: 0.6644\n",
            "Early stopping:  0.03116690342429679\n",
            "Epoch: 034, Loss: 0.8999, Train: 0.7249, Test: 0.6769\n",
            "Early stopping:  0.029611771794263535\n",
            "Epoch: 035, Loss: 0.8832, Train: 0.7333, Test: 0.6797\n",
            "Early stopping:  0.028349602552275678\n",
            "Epoch: 036, Loss: 0.8671, Train: 0.7395, Test: 0.6842\n",
            "Early stopping:  0.02712711431610117\n",
            "Epoch: 037, Loss: 0.8517, Train: 0.7475, Test: 0.6882\n",
            "Early stopping:  0.025777835603589785\n",
            "Epoch: 038, Loss: 0.8364, Train: 0.7561, Test: 0.7012\n",
            "Early stopping:  0.025075964494059577\n",
            "Epoch: 039, Loss: 0.8213, Train: 0.7597, Test: 0.7024\n",
            "Early stopping:  0.024440354462208005\n",
            "Epoch: 040, Loss: 0.8065, Train: 0.7653, Test: 0.7046\n",
            "Early stopping:  0.02394886485034461\n",
            "Epoch: 041, Loss: 0.7916, Train: 0.7727, Test: 0.7109\n",
            "Early stopping:  0.023723731602584005\n",
            "Epoch: 042, Loss: 0.7768, Train: 0.7774, Test: 0.7115\n",
            "Early stopping:  0.023537526540563833\n",
            "Epoch: 043, Loss: 0.7624, Train: 0.7818, Test: 0.7143\n",
            "Early stopping:  0.0233285698993281\n",
            "Epoch: 044, Loss: 0.7476, Train: 0.7853, Test: 0.7154\n",
            "Early stopping:  0.023248341915978892\n",
            "Epoch: 045, Loss: 0.7334, Train: 0.7912, Test: 0.7205\n",
            "Early stopping:  0.023023638516159482\n",
            "Epoch: 046, Loss: 0.7189, Train: 0.7965, Test: 0.7239\n",
            "Early stopping:  0.022894401585661014\n",
            "Epoch: 047, Loss: 0.7050, Train: 0.7985, Test: 0.7273\n",
            "Early stopping:  0.022679766934451097\n",
            "Epoch: 048, Loss: 0.6912, Train: 0.8036, Test: 0.7279\n",
            "Early stopping:  0.022323934417865884\n",
            "Epoch: 049, Loss: 0.6778, Train: 0.8041, Test: 0.7239\n",
            "Early stopping:  0.021942895866379184\n",
            "Epoch: 050, Loss: 0.6658, Train: 0.8140, Test: 0.7273\n",
            "Early stopping:  0.021086842746142\n",
            "Epoch: 051, Loss: 0.6567, Train: 0.8008, Test: 0.7290\n",
            "Early stopping:  0.01935679459384084\n",
            "Epoch: 052, Loss: 0.6513, Train: 0.8212, Test: 0.7313\n",
            "Early stopping:  0.016167646404309708\n",
            "Epoch: 053, Loss: 0.6375, Train: 0.8243, Test: 0.7387\n",
            "Early stopping:  0.015182427777466905\n",
            "Epoch: 054, Loss: 0.6177, Train: 0.8189, Test: 0.7398\n",
            "Early stopping:  0.018767405977808134\n",
            "Epoch: 055, Loss: 0.6117, Train: 0.8328, Test: 0.7415\n",
            "Early stopping:  0.01989299055220025\n",
            "Epoch: 056, Loss: 0.6052, Train: 0.8361, Test: 0.7455\n",
            "Early stopping:  0.01915887832362805\n",
            "Epoch: 057, Loss: 0.5877, Train: 0.8321, Test: 0.7472\n",
            "Early stopping:  0.018166746681245443\n",
            "Epoch: 058, Loss: 0.5804, Train: 0.8446, Test: 0.7460\n",
            "Early stopping:  0.015890918354014516\n",
            "Epoch: 059, Loss: 0.5743, Train: 0.8453, Test: 0.7540\n",
            "Early stopping:  0.016031331707957477\n",
            "Epoch: 060, Loss: 0.5589, Train: 0.8443, Test: 0.7517\n",
            "Early stopping:  0.017042879204569556\n",
            "Epoch: 061, Loss: 0.5521, Train: 0.8552, Test: 0.7494\n",
            "Early stopping:  0.014846101379932784\n",
            "Epoch: 062, Loss: 0.5454, Train: 0.8541, Test: 0.7551\n",
            "Early stopping:  0.014788050930845963\n",
            "Epoch: 063, Loss: 0.5323, Train: 0.8562, Test: 0.7557\n",
            "Early stopping:  0.015624683783091136\n",
            "Epoch: 064, Loss: 0.5254, Train: 0.8630, Test: 0.7557\n",
            "Early stopping:  0.013852885220095975\n",
            "Epoch: 065, Loss: 0.5186, Train: 0.8649, Test: 0.7562\n",
            "Early stopping:  0.013859898799783405\n",
            "Epoch: 066, Loss: 0.5075, Train: 0.8656, Test: 0.7545\n",
            "Early stopping:  0.014242680502590562\n",
            "Epoch: 067, Loss: 0.4999, Train: 0.8724, Test: 0.7602\n",
            "Early stopping:  0.013102504775120333\n",
            "Epoch: 068, Loss: 0.4935, Train: 0.8713, Test: 0.7568\n",
            "Early stopping:  0.01306677433722726\n",
            "Epoch: 069, Loss: 0.4846, Train: 0.8741, Test: 0.7642\n",
            "Early stopping:  0.013021692230683907\n",
            "Epoch: 070, Loss: 0.4756, Train: 0.8812, Test: 0.7653\n",
            "Early stopping:  0.012543333107458044\n",
            "Epoch: 071, Loss: 0.4688, Train: 0.8805, Test: 0.7591\n",
            "Early stopping:  0.012696291263553736\n",
            "Epoch: 072, Loss: 0.4625, Train: 0.8805, Test: 0.7670\n",
            "Early stopping:  0.01233692272645001\n",
            "Epoch: 073, Loss: 0.4533, Train: 0.8890, Test: 0.7642\n",
            "Early stopping:  0.011992818683115386\n",
            "Epoch: 074, Loss: 0.4452, Train: 0.8881, Test: 0.7596\n",
            "Early stopping:  0.012113660922260567\n",
            "Epoch: 075, Loss: 0.4397, Train: 0.8930, Test: 0.7659\n",
            "Early stopping:  0.011980442191471085\n",
            "Epoch: 076, Loss: 0.4327, Train: 0.8942, Test: 0.7619\n",
            "Early stopping:  0.011647871296643311\n",
            "Epoch: 077, Loss: 0.4248, Train: 0.8961, Test: 0.7659\n",
            "Early stopping:  0.011007404086610417\n",
            "Epoch: 078, Loss: 0.4181, Train: 0.9008, Test: 0.7653\n",
            "Early stopping:  0.0109485889728708\n",
            "Epoch: 079, Loss: 0.4116, Train: 0.9003, Test: 0.7608\n",
            "Early stopping:  0.011216510694725047\n",
            "Epoch: 080, Loss: 0.4037, Train: 0.9056, Test: 0.7676\n",
            "Early stopping:  0.011271709592964549\n",
            "Epoch: 081, Loss: 0.3964, Train: 0.9063, Test: 0.7647\n",
            "Early stopping:  0.011288082004894371\n",
            "Epoch: 082, Loss: 0.3921, Train: 0.9064, Test: 0.7693\n",
            "Early stopping:  0.010680064994709478\n",
            "Epoch: 083, Loss: 0.3876, Train: 0.9054, Test: 0.7630\n",
            "Early stopping:  0.009516398916536974\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11,  6], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.77      0.77      0.77       190\n",
            "         capital_goods       0.77      0.69      0.72       127\n",
            "conglomerates_industry       1.00      0.85      0.92        20\n",
            "     consumer_cyclical       0.71      0.71      0.71       198\n",
            " consumer_non-cyclical       0.77      0.63      0.70       112\n",
            "                energy       0.95      0.75      0.83        71\n",
            "             financial       0.84      0.74      0.79       192\n",
            "            healthcare       0.85      0.71      0.77        79\n",
            "              services       0.70      0.84      0.77       519\n",
            "            technology       0.74      0.69      0.71        99\n",
            "        transportation       0.88      0.82      0.85       101\n",
            "             utilities       0.86      0.77      0.81        56\n",
            "\n",
            "              accuracy                           0.76      1764\n",
            "             macro avg       0.82      0.75      0.78      1764\n",
            "          weighted avg       0.77      0.76      0.76      1764\n",
            "\n",
            "\n",
            "===============================================\n",
            "=================== MODEL 9 ===================\n",
            "===============================================\n",
            "\n",
            "Epoch: 001, Loss: 2.5313, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  nan\n",
            "Epoch: 002, Loss: 2.3108, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1558570227445576\n",
            "Epoch: 003, Loss: 2.1840, Train: 0.2943, Test: 0.2942\n",
            "Early stopping:  0.1757014567665605\n",
            "Epoch: 004, Loss: 2.1462, Train: 0.2966, Test: 0.2954\n",
            "Early stopping:  0.17368123095288565\n",
            "Epoch: 005, Loss: 2.0875, Train: 0.3177, Test: 0.3146\n",
            "Early stopping:  0.17628726687833018\n",
            "Epoch: 006, Loss: 2.0335, Train: 0.3536, Test: 0.3475\n",
            "Early stopping:  0.10545799242146517\n",
            "Epoch: 007, Loss: 1.9831, Train: 0.3750, Test: 0.3583\n",
            "Early stopping:  0.08151293339358717\n",
            "Epoch: 008, Loss: 1.9268, Train: 0.3718, Test: 0.3583\n",
            "Early stopping:  0.08593561608346063\n",
            "Epoch: 009, Loss: 1.8674, Train: 0.3732, Test: 0.3594\n",
            "Early stopping:  0.08652131833706796\n",
            "Epoch: 010, Loss: 1.8118, Train: 0.3881, Test: 0.3713\n",
            "Early stopping:  0.08844682427396515\n",
            "Epoch: 011, Loss: 1.7575, Train: 0.4215, Test: 0.4087\n",
            "Early stopping:  0.08955229140752316\n",
            "Epoch: 012, Loss: 1.6983, Train: 0.4648, Test: 0.4467\n",
            "Early stopping:  0.08965313464247615\n",
            "Epoch: 013, Loss: 1.6396, Train: 0.4880, Test: 0.4739\n",
            "Early stopping:  0.08999687932546267\n",
            "Epoch: 014, Loss: 1.5875, Train: 0.5055, Test: 0.4943\n",
            "Early stopping:  0.08957934442575112\n",
            "Epoch: 015, Loss: 1.5379, Train: 0.5184, Test: 0.5040\n",
            "Early stopping:  0.08702262538744879\n",
            "Epoch: 016, Loss: 1.4865, Train: 0.5320, Test: 0.5147\n",
            "Early stopping:  0.08308567962812294\n",
            "Epoch: 017, Loss: 1.4375, Train: 0.5460, Test: 0.5232\n",
            "Early stopping:  0.07987871678253496\n",
            "Epoch: 018, Loss: 1.3929, Train: 0.5684, Test: 0.5363\n",
            "Early stopping:  0.07741998707298037\n",
            "Epoch: 019, Loss: 1.3494, Train: 0.5822, Test: 0.5573\n",
            "Early stopping:  0.07444726165008937\n",
            "Epoch: 020, Loss: 1.3092, Train: 0.5973, Test: 0.5737\n",
            "Early stopping:  0.07004769780001724\n",
            "Epoch: 021, Loss: 1.2737, Train: 0.6090, Test: 0.5833\n",
            "Early stopping:  0.06509098500238182\n",
            "Epoch: 022, Loss: 1.2396, Train: 0.6141, Test: 0.5867\n",
            "Early stopping:  0.06054167957815898\n",
            "Epoch: 023, Loss: 1.2088, Train: 0.6206, Test: 0.5890\n",
            "Early stopping:  0.055546482929544144\n",
            "Epoch: 024, Loss: 1.1814, Train: 0.6254, Test: 0.5958\n",
            "Early stopping:  0.05076068838472856\n",
            "Epoch: 025, Loss: 1.1536, Train: 0.6338, Test: 0.5986\n",
            "Early stopping:  0.047244794272510986\n",
            "Epoch: 026, Loss: 1.1274, Train: 0.6447, Test: 0.6077\n",
            "Early stopping:  0.04422980688617879\n",
            "Epoch: 027, Loss: 1.1039, Train: 0.6509, Test: 0.6202\n",
            "Early stopping:  0.04172465045323584\n",
            "Epoch: 028, Loss: 1.0814, Train: 0.6584, Test: 0.6259\n",
            "Early stopping:  0.039511286143734885\n",
            "Epoch: 029, Loss: 1.0595, Train: 0.6654, Test: 0.6355\n",
            "Early stopping:  0.037052802973972246\n",
            "Epoch: 030, Loss: 1.0372, Train: 0.6730, Test: 0.6429\n",
            "Early stopping:  0.03554109056007446\n",
            "Epoch: 031, Loss: 1.0161, Train: 0.6791, Test: 0.6497\n",
            "Early stopping:  0.0347571719789541\n",
            "Epoch: 032, Loss: 0.9972, Train: 0.6872, Test: 0.6514\n",
            "Early stopping:  0.033504237138605446\n",
            "Epoch: 033, Loss: 0.9776, Train: 0.6943, Test: 0.6599\n",
            "Early stopping:  0.03222960619411093\n",
            "Epoch: 034, Loss: 0.9586, Train: 0.7015, Test: 0.6621\n",
            "Early stopping:  0.03094877241498724\n",
            "Epoch: 035, Loss: 0.9407, Train: 0.7123, Test: 0.6689\n",
            "Early stopping:  0.02994756415331841\n",
            "Epoch: 036, Loss: 0.9229, Train: 0.7193, Test: 0.6774\n",
            "Early stopping:  0.02932886229680776\n",
            "Epoch: 037, Loss: 0.9059, Train: 0.7247, Test: 0.6797\n",
            "Early stopping:  0.028321392327973857\n",
            "Epoch: 038, Loss: 0.8898, Train: 0.7305, Test: 0.6842\n",
            "Early stopping:  0.027280116456689527\n",
            "Epoch: 039, Loss: 0.8744, Train: 0.7377, Test: 0.6888\n",
            "Early stopping:  0.02622511323162144\n",
            "Epoch: 040, Loss: 0.8589, Train: 0.7449, Test: 0.6927\n",
            "Early stopping:  0.02523642690683258\n",
            "Epoch: 041, Loss: 0.8445, Train: 0.7509, Test: 0.6956\n",
            "Early stopping:  0.024319010509747233\n",
            "Epoch: 042, Loss: 0.8299, Train: 0.7554, Test: 0.6922\n",
            "Early stopping:  0.023659627758265616\n",
            "Epoch: 043, Loss: 0.8154, Train: 0.7624, Test: 0.6956\n",
            "Early stopping:  0.023254656881055387\n",
            "Epoch: 044, Loss: 0.8012, Train: 0.7658, Test: 0.6990\n",
            "Early stopping:  0.022842067374449496\n",
            "Epoch: 045, Loss: 0.7873, Train: 0.7710, Test: 0.7058\n",
            "Early stopping:  0.02263509697630053\n",
            "Epoch: 046, Loss: 0.7735, Train: 0.7758, Test: 0.7103\n",
            "Early stopping:  0.022289102125258305\n",
            "Epoch: 047, Loss: 0.7599, Train: 0.7790, Test: 0.7098\n",
            "Early stopping:  0.0219271681898778\n",
            "Epoch: 048, Loss: 0.7465, Train: 0.7834, Test: 0.7115\n",
            "Early stopping:  0.021639994933397726\n",
            "Epoch: 049, Loss: 0.7331, Train: 0.7883, Test: 0.7120\n",
            "Early stopping:  0.021388503358517944\n",
            "Epoch: 050, Loss: 0.7203, Train: 0.7937, Test: 0.7120\n",
            "Early stopping:  0.021053376028555566\n",
            "Epoch: 051, Loss: 0.7075, Train: 0.7995, Test: 0.7177\n",
            "Early stopping:  0.02069736985344747\n",
            "Epoch: 052, Loss: 0.6951, Train: 0.8011, Test: 0.7262\n",
            "Early stopping:  0.020304092700417827\n",
            "Epoch: 053, Loss: 0.6829, Train: 0.8075, Test: 0.7256\n",
            "Early stopping:  0.019876446168381048\n",
            "Epoch: 054, Loss: 0.6707, Train: 0.8093, Test: 0.7228\n",
            "Early stopping:  0.019572335515388613\n",
            "Epoch: 055, Loss: 0.6588, Train: 0.8133, Test: 0.7273\n",
            "Early stopping:  0.019250696170573053\n",
            "Epoch: 056, Loss: 0.6470, Train: 0.8194, Test: 0.7319\n",
            "Early stopping:  0.019007791689416523\n",
            "Epoch: 057, Loss: 0.6354, Train: 0.8235, Test: 0.7330\n",
            "Early stopping:  0.018766988444546613\n",
            "Epoch: 058, Loss: 0.6242, Train: 0.8293, Test: 0.7341\n",
            "Early stopping:  0.018426899195683664\n",
            "Epoch: 059, Loss: 0.6130, Train: 0.8296, Test: 0.7381\n",
            "Early stopping:  0.01810887470264301\n",
            "Epoch: 060, Loss: 0.6027, Train: 0.8387, Test: 0.7364\n",
            "Early stopping:  0.017567741498622332\n",
            "Epoch: 061, Loss: 0.5943, Train: 0.8235, Test: 0.7330\n",
            "Early stopping:  0.0164360124680546\n",
            "Epoch: 062, Loss: 0.5921, Train: 0.8409, Test: 0.7375\n",
            "Early stopping:  0.013406798554656414\n",
            "Epoch: 063, Loss: 0.5863, Train: 0.8399, Test: 0.7404\n",
            "Early stopping:  0.010390220607574104\n",
            "Epoch: 064, Loss: 0.5663, Train: 0.8447, Test: 0.7404\n",
            "Early stopping:  0.01362543927684846\n",
            "Epoch: 065, Loss: 0.5552, Train: 0.8486, Test: 0.7455\n",
            "Early stopping:  0.017189484669545486\n",
            "Epoch: 066, Loss: 0.5552, Train: 0.8501, Test: 0.7426\n",
            "Early stopping:  0.017306842872106383\n",
            "Epoch: 067, Loss: 0.5397, Train: 0.8534, Test: 0.7466\n",
            "Early stopping:  0.017237708201323445\n",
            "Epoch: 068, Loss: 0.5299, Train: 0.8585, Test: 0.7449\n",
            "Early stopping:  0.014401734551479408\n",
            "Epoch: 069, Loss: 0.5279, Train: 0.8595, Test: 0.7472\n",
            "Early stopping:  0.013211816822165014\n",
            "Epoch: 070, Loss: 0.5139, Train: 0.8636, Test: 0.7472\n",
            "Early stopping:  0.015306562951120272\n",
            "Epoch: 071, Loss: 0.5067, Train: 0.8686, Test: 0.7472\n",
            "Early stopping:  0.013211300828478401\n",
            "Epoch: 072, Loss: 0.5029, Train: 0.8707, Test: 0.7511\n",
            "Early stopping:  0.012231082633536079\n",
            "Epoch: 073, Loss: 0.4899, Train: 0.8711, Test: 0.7523\n",
            "Early stopping:  0.014029829826830927\n",
            "Epoch: 074, Loss: 0.4842, Train: 0.8786, Test: 0.7500\n",
            "Early stopping:  0.01222066494173063\n",
            "Epoch: 075, Loss: 0.4791, Train: 0.8796, Test: 0.7557\n",
            "Early stopping:  0.011869215168549987\n",
            "Epoch: 076, Loss: 0.4674, Train: 0.8792, Test: 0.7568\n",
            "Early stopping:  0.013129938895676707\n",
            "Epoch: 077, Loss: 0.4617, Train: 0.8873, Test: 0.7528\n",
            "Early stopping:  0.01169032440907987\n",
            "Epoch: 078, Loss: 0.4564, Train: 0.8883, Test: 0.7568\n",
            "Early stopping:  0.011644704654960251\n",
            "Epoch: 079, Loss: 0.4462, Train: 0.8894, Test: 0.7608\n",
            "Early stopping:  0.012272777717937512\n",
            "Epoch: 080, Loss: 0.4392, Train: 0.8954, Test: 0.7551\n",
            "Early stopping:  0.011449445853387861\n",
            "Epoch: 081, Loss: 0.4343, Train: 0.8935, Test: 0.7642\n",
            "Early stopping:  0.011488667632430852\n",
            "Epoch: 082, Loss: 0.4265, Train: 0.8999, Test: 0.7613\n",
            "Early stopping:  0.011421707988856214\n",
            "Epoch: 083, Loss: 0.4182, Train: 0.9025, Test: 0.7579\n",
            "Early stopping:  0.010882638131795477\n",
            "Epoch: 084, Loss: 0.4118, Train: 0.9003, Test: 0.7619\n",
            "Early stopping:  0.011237215596805182\n",
            "Epoch: 085, Loss: 0.4063, Train: 0.9076, Test: 0.7619\n",
            "Early stopping:  0.011226984862163458\n",
            "Epoch: 086, Loss: 0.4006, Train: 0.9068, Test: 0.7630\n",
            "Early stopping:  0.010127149708634963\n",
            "Epoch: 087, Loss: 0.3942, Train: 0.9135, Test: 0.7659\n",
            "Early stopping:  0.009372477298715794\n",
            "PREDICTIONS -> tensor([ 0,  0,  0,  ..., 11, 11, 11], device='cuda:0')\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "       basic_materials       0.75      0.76      0.75       190\n",
            "         capital_goods       0.73      0.69      0.71       127\n",
            "conglomerates_industry       1.00      0.80      0.89        20\n",
            "     consumer_cyclical       0.74      0.73      0.74       198\n",
            " consumer_non-cyclical       0.70      0.64      0.67       112\n",
            "                energy       0.88      0.75      0.81        71\n",
            "             financial       0.82      0.76      0.79       192\n",
            "            healthcare       0.86      0.76      0.81        79\n",
            "              services       0.73      0.83      0.78       519\n",
            "            technology       0.78      0.70      0.73        99\n",
            "        transportation       0.83      0.84      0.84       101\n",
            "             utilities       0.87      0.80      0.83        56\n",
            "\n",
            "              accuracy                           0.77      1764\n",
            "             macro avg       0.81      0.75      0.78      1764\n",
            "          weighted avg       0.77      0.77      0.77      1764\n",
            "\n",
            "time: 2min 37s (started: 2024-10-16 21:52:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    iteration(i,'GAT',str(rotulated_perc)+'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF_ymTsIgWjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1857e79-06aa-4c63-df70-f03b2fc510bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 471 ms (started: 2024-10-16 21:55:20 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Saving dataframes_big\n",
        "for i in range(10):\n",
        "    df_list[i][0].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_acc.pkl\") # acuracia\n",
        "    df_list[i][1].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_pre.pkl\") # precision\n",
        "    df_list[i][2].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_rec.pkl\") # recall\n",
        "    df_list[i][3].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_f1.pkl\")  # f1-score\n",
        "    df_list[i][4].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_sup.pkl\") # support\n",
        "    df_list[i][5].to_pickle(\"/content/drive/MyDrive/Mestrado_Grafos/processed_datasets/dataframes_big/\"+str(i)+\"/df_time.pkl\") # time"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "S-0aDh52nO-S",
        "SUqyo3PodvRh",
        "xtTe3rAQrhG-",
        "Pws3dFR9rkOI",
        "HHo1UacopxwC",
        "F42AmIbTm_Uk",
        "H9tMAdQPDowd",
        "QrRkgxKu2Wiw",
        "Z_kBUMGf_lWu",
        "MVfGvWUSs0z6",
        "hp2TAceotXLx",
        "bXGrNtvp_pvm",
        "olhv0tQ8Skgj",
        "_oj2BlyrsCdP",
        "LZQnu2Drr55D",
        "-pincp8wI-yV",
        "AsFgg19dw8eu",
        "KIe8Dr1oxMFW",
        "cn4axcr5xPl7",
        "on0xv-jO79gT",
        "p5dxHzRB79gY",
        "RVahHlnK79q4",
        "kdnUTbAr79q4",
        "TQ5heZBaNunS",
        "0XZr1amWNunS",
        "8sApynSrs_hd",
        "DnIjviAr5gWi",
        "eyOMuSd28UWx",
        "tkaPhP3p8NBt",
        "Y_u1h2Busrbs",
        "KRzeKXYxHrm-",
        "-hD3zLW6Hrm-",
        "71eypyB-tAcN",
        "gjeHM230HtEx",
        "F86xKwHiHtEx",
        "CWUrgueftApV",
        "DVAiqJH7Hugb",
        "s-Y5_EcxHugb",
        "dQn_F5KEXNpC",
        "q-p0Ugv8eQpg",
        "j5eCUuXkeQpg",
        "s_uP4E2seQph",
        "DNqpuD_qeQph",
        "7wvV-vByeQpi",
        "NpojxPL5eQpk",
        "-p97x2PZeQpl",
        "J4aNru07eQpl",
        "KgiblP-UeQpm",
        "RKQJ0GpkeQpn",
        "EzrLTAjVgSl9",
        "7HAAys2cgSl9",
        "Xg9zWmgSgSl9",
        "ApKs3OENgSl9",
        "mMXRTt74gSl-",
        "wtYMlDhagSl-",
        "GPHNFqI6gSl-",
        "EQn7_2oIgSl-",
        "YpQ-xPd7gSl-",
        "7LrmRjvUgSl-",
        "ZAwNmu-1gSl-",
        "SwA1YKbngSl-",
        "FkjPHHo_gSl_",
        "YXXZu4w_dcAv",
        "Oywnn6k1ezSj",
        "5gvup-2jezSk",
        "7bwipeK4ezSl",
        "tUbeMm8SezSm",
        "WmkD7gXqezSo",
        "fPZtNmU3ezSr",
        "OzuE-CP9ezSt",
        "IhPvsB_-ezSu",
        "Bvu3pgTlezSw",
        "psEyYySbezSy",
        "l1EOhFm1gWjL",
        "1AO1mrrwgWjL",
        "6-aLOM-rgWjL",
        "2BMAztIHgWjL",
        "DVfTPRH3gWjL",
        "heWth3vvgWjM",
        "_pAnFSX5gWjM",
        "d0785SeKgWjM",
        "pMMy3xS2gWjM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}